{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4ea674-59df-4889-b63b-04acb56d9015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing\n",
    "!python3 MT-Preparation/filtering/filter.py TED2020.en-tr.tr TED2020.en-tr.en tr en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9020d4-f37b-4390-91e4-70c08f468306",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 MT-Preparation/train_dev_split/train_dev_test_split.py 10000 10000 TED2020.en-tr.tr-filtered.tr TED2020.en-tr.en-filtered.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210c7a6b-1630-4f24-acdf-b5ae9ab626a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"Data-train-src.txt\", \"w\")\n",
    "with open(\"TED2020.en-tr.tr-filtered.tr.train\", 'r') as file:\n",
    "    for i in range(300000):\n",
    "        fp.write(file.readline())\n",
    "\n",
    "fp.close()\n",
    "\n",
    "fp = open(\"Data-train-src.txt\", \"a\")\n",
    "with open(\"kk_train_shuffled.txt-filtered.kk\", 'r') as file:\n",
    "    for i in range(300000):\n",
    "        fp.write(file.readline())\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1275778-89d8-4127-8a01-3ef83249d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"Data-train-tgt.txt\", \"w\")\n",
    "with open(\"TED2020.en-tr.en-filtered.en.train\", 'r') as file:\n",
    "    for i in range(300000):\n",
    "        fp.write(file.readline())\n",
    "\n",
    "fp.close()\n",
    "\n",
    "fp = open(\"Data-train-tgt.txt\", \"a\")\n",
    "with open(\"en_train_shuffled.txt-filtered.en\", 'r') as file:\n",
    "    for i in range(300000):\n",
    "        fp.write(file.readline())\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d5d45-0445-4efa-9204-a5d70a0e8d7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Kazakh and Turkish shared vocabulary\n",
    "!onmt-build-vocab --sentencepiece model_type=bpe --size 32000 --save_vocab kk_tr_shared_vocab Data-train-src.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180c40f4-2779-46e2-916c-8869eb3963f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#English side shared vocabulary\n",
    "!onmt-build-vocab --sentencepiece model_type=bpe --size 32000 --save_vocab en_shared_vocab Data-train-tgt.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "350820cc-2667-4ada-9aff-ab7cd93e99d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-11 10:10:07.047375: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-11 10:10:07.912337: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-11 10:10:07.912403: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-11 10:10:07.912411: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-11 10:10:08.969000: I onmt-main:8] Creating model directory KK-TR-EN-Shared-vocab\n",
      "2024-12-11 10:10:09.168000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-11 10:10:09.168000: I main.py:315] Using model:\n",
      "(model): MyCustomTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): WordEmbedder()\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): WordEmbedder()\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-11 10:10:09.171043: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-11 10:10:09.799158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5773 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "2024-12-11 10:10:09.802000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file: KK_tokens_valid_shared\n",
      "  eval_labels_file: KK_valid_target_tokens_shared.txt\n",
      "  source_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file: KK_tokens_train_shared\n",
      "  train_labels_file: KK_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: KK-TR-EN-Shared-vocab\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 250000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-11 10:10:10.100000: I inputter.py:316] Initialized source input layer:\n",
      "2024-12-11 10:10:10.100000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-11 10:10:10.100000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-11 10:10:10.180000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-11 10:10:10.180000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-11 10:10:10.180000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-11 10:10:10.188000: W runner.py:269] No checkpoint to restore in KK-TR-EN-Shared-vocab\n",
      "2024-12-11 10:10:10.188000: I main.py:325] Accumulate gradients of 13 iterations to reach effective batch size of 25000\n",
      "2024-12-11 10:10:10.415000: I dataset_ops.py:2542] Training on 318032 examples\n",
      "2024-12-11 10:10:10.439000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-11 10:10:36.876061: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-11 10:10:37.836456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-11 10:10:43.398000: I runner.py:310] Number of model parameters: 93326081\n",
      "2024-12-11 10:10:43.404000: I runner.py:310] Number of model weights: 260 (trainable = 260, non trainable = 0)\n",
      "2024-12-11 10:10:46.225000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-1\n",
      "2024-12-11 10:13:04.009000: I runner.py:310] Step = 100 ; steps/s = 0.72, tokens/s = 34177 (16459 source, 17718 target) ; Learning rate = 0.000009 ; Loss = 9.749730\n",
      "2024-12-11 10:15:22.617000: I runner.py:310] Step = 200 ; steps/s = 0.72, tokens/s = 34184 (16476 source, 17708 target) ; Learning rate = 0.000018 ; Loss = 8.943575\n",
      "2024-12-11 10:17:41.259000: I runner.py:310] Step = 300 ; steps/s = 0.72, tokens/s = 34164 (16458 source, 17706 target) ; Learning rate = 0.000027 ; Loss = 8.035521\n",
      "2024-12-11 10:19:59.894000: I runner.py:310] Step = 400 ; steps/s = 0.72, tokens/s = 34202 (16478 source, 17724 target) ; Learning rate = 0.000035 ; Loss = 7.286536\n",
      "2024-12-11 10:22:16.683000: I runner.py:310] Step = 500 ; steps/s = 0.73, tokens/s = 33944 (16356 source, 17588 target) ; Learning rate = 0.000044 ; Loss = 7.038347\n",
      "2024-12-11 10:24:35.353000: I runner.py:310] Step = 600 ; steps/s = 0.72, tokens/s = 34166 (16455 source, 17711 target) ; Learning rate = 0.000053 ; Loss = 6.695955\n",
      "2024-12-11 10:26:54.035000: I runner.py:310] Step = 700 ; steps/s = 0.72, tokens/s = 34183 (16465 source, 17718 target) ; Learning rate = 0.000062 ; Loss = 6.409574\n",
      "2024-12-11 10:29:12.738000: I runner.py:310] Step = 800 ; steps/s = 0.72, tokens/s = 34163 (16465 source, 17698 target) ; Learning rate = 0.000071 ; Loss = 6.152814\n",
      "2024-12-11 10:31:29.157000: I runner.py:310] Step = 900 ; steps/s = 0.73, tokens/s = 34044 (16399 source, 17645 target) ; Learning rate = 0.000080 ; Loss = 5.978446\n",
      "2024-12-11 10:33:47.832000: I runner.py:310] Step = 1000 ; steps/s = 0.72, tokens/s = 34165 (16467 source, 17698 target) ; Learning rate = 0.000088 ; Loss = 5.791309\n",
      "2024-12-11 10:36:06.498000: I runner.py:310] Step = 1100 ; steps/s = 0.72, tokens/s = 34173 (16462 source, 17711 target) ; Learning rate = 0.000097 ; Loss = 5.640049\n",
      "2024-12-11 10:38:25.188000: I runner.py:310] Step = 1200 ; steps/s = 0.72, tokens/s = 34165 (16466 source, 17699 target) ; Learning rate = 0.000106 ; Loss = 5.519980\n",
      "2024-12-11 10:40:41.594000: I runner.py:310] Step = 1300 ; steps/s = 0.73, tokens/s = 34050 (16405 source, 17645 target) ; Learning rate = 0.000115 ; Loss = 5.326896\n",
      "2024-12-11 10:43:00.270000: I runner.py:310] Step = 1400 ; steps/s = 0.72, tokens/s = 34171 (16468 source, 17703 target) ; Learning rate = 0.000124 ; Loss = 5.177765\n",
      "2024-12-11 10:45:18.934000: I runner.py:310] Step = 1500 ; steps/s = 0.72, tokens/s = 34187 (16463 source, 17724 target) ; Learning rate = 0.000133 ; Loss = 5.002269\n",
      "2024-12-11 10:47:37.620000: I runner.py:310] Step = 1600 ; steps/s = 0.72, tokens/s = 34172 (16464 source, 17708 target) ; Learning rate = 0.000142 ; Loss = 4.838598\n",
      "2024-12-11 10:49:56.309000: I runner.py:310] Step = 1700 ; steps/s = 0.72, tokens/s = 34157 (16462 source, 17695 target) ; Learning rate = 0.000150 ; Loss = 4.643578\n",
      "2024-12-11 10:52:12.742000: I runner.py:310] Step = 1800 ; steps/s = 0.73, tokens/s = 34029 (16392 source, 17637 target) ; Learning rate = 0.000159 ; Loss = 4.594382\n",
      "2024-12-11 10:54:31.399000: I runner.py:310] Step = 1900 ; steps/s = 0.72, tokens/s = 34171 (16466 source, 17705 target) ; Learning rate = 0.000168 ; Loss = 4.342511\n",
      "2024-12-11 10:56:49.899000: I runner.py:310] Step = 2000 ; steps/s = 0.72, tokens/s = 34225 (16490 source, 17735 target) ; Learning rate = 0.000177 ; Loss = 4.271236\n",
      "2024-12-11 10:59:08.239000: I runner.py:310] Step = 2100 ; steps/s = 0.72, tokens/s = 34252 (16503 source, 17749 target) ; Learning rate = 0.000186 ; Loss = 4.102647\n",
      "2024-12-11 11:01:24.352000: I runner.py:310] Step = 2200 ; steps/s = 0.73, tokens/s = 34136 (16443 source, 17693 target) ; Learning rate = 0.000195 ; Loss = 3.890965\n",
      "2024-12-11 11:03:42.751000: I runner.py:310] Step = 2300 ; steps/s = 0.72, tokens/s = 34241 (16499 source, 17742 target) ; Learning rate = 0.000203 ; Loss = 3.791400\n",
      "2024-12-11 11:06:01.182000: I runner.py:310] Step = 2400 ; steps/s = 0.72, tokens/s = 34215 (16480 source, 17735 target) ; Learning rate = 0.000212 ; Loss = 3.731165\n",
      "2024-12-11 11:08:19.614000: I runner.py:310] Step = 2500 ; steps/s = 0.72, tokens/s = 34224 (16500 source, 17724 target) ; Learning rate = 0.000221 ; Loss = 3.575126\n",
      "2024-12-11 11:10:35.721000: I runner.py:310] Step = 2600 ; steps/s = 0.73, tokens/s = 34122 (16440 source, 17682 target) ; Learning rate = 0.000230 ; Loss = 3.458020\n",
      "2024-12-11 11:12:54.129000: I runner.py:310] Step = 2700 ; steps/s = 0.72, tokens/s = 34238 (16492 source, 17746 target) ; Learning rate = 0.000239 ; Loss = 3.475771\n",
      "2024-12-11 11:15:12.798000: I runner.py:310] Step = 2800 ; steps/s = 0.72, tokens/s = 34183 (16473 source, 17710 target) ; Learning rate = 0.000248 ; Loss = 3.263836\n",
      "2024-12-11 11:17:31.470000: I runner.py:310] Step = 2900 ; steps/s = 0.72, tokens/s = 34169 (16459 source, 17710 target) ; Learning rate = 0.000256 ; Loss = 3.219796\n",
      "2024-12-11 11:19:47.908000: I runner.py:310] Step = 3000 ; steps/s = 0.73, tokens/s = 34019 (16388 source, 17631 target) ; Learning rate = 0.000265 ; Loss = 3.080098\n",
      "2024-12-11 11:22:06.624000: I runner.py:310] Step = 3100 ; steps/s = 0.72, tokens/s = 34176 (16463 source, 17713 target) ; Learning rate = 0.000274 ; Loss = 3.133751\n",
      "2024-12-11 11:24:25.293000: I runner.py:310] Step = 3200 ; steps/s = 0.72, tokens/s = 34163 (16454 source, 17709 target) ; Learning rate = 0.000283 ; Loss = 3.068349\n",
      "2024-12-11 11:26:43.997000: I runner.py:310] Step = 3300 ; steps/s = 0.72, tokens/s = 34174 (16461 source, 17713 target) ; Learning rate = 0.000292 ; Loss = 2.984479\n",
      "2024-12-11 11:29:02.694000: I runner.py:310] Step = 3400 ; steps/s = 0.72, tokens/s = 34153 (16472 source, 17681 target) ; Learning rate = 0.000301 ; Loss = 3.020948\n",
      "2024-12-11 11:31:19.084000: I runner.py:310] Step = 3500 ; steps/s = 0.73, tokens/s = 34045 (16397 source, 17648 target) ; Learning rate = 0.000309 ; Loss = 2.869680\n",
      "2024-12-11 11:33:37.779000: I runner.py:310] Step = 3600 ; steps/s = 0.72, tokens/s = 34163 (16450 source, 17713 target) ; Learning rate = 0.000318 ; Loss = 2.815751\n",
      "2024-12-11 11:35:56.476000: I runner.py:310] Step = 3700 ; steps/s = 0.72, tokens/s = 34166 (16468 source, 17698 target) ; Learning rate = 0.000327 ; Loss = 2.821021\n",
      "2024-12-11 11:38:15.155000: I runner.py:310] Step = 3800 ; steps/s = 0.72, tokens/s = 34173 (16465 source, 17708 target) ; Learning rate = 0.000336 ; Loss = 2.759434\n",
      "2024-12-11 11:40:31.561000: I runner.py:310] Step = 3900 ; steps/s = 0.73, tokens/s = 34062 (16413 source, 17649 target) ; Learning rate = 0.000345 ; Loss = 2.698734\n",
      "2024-12-11 11:42:50.277000: I runner.py:310] Step = 4000 ; steps/s = 0.72, tokens/s = 34157 (16457 source, 17700 target) ; Learning rate = 0.000354 ; Loss = 2.696731\n",
      "2024-12-11 11:45:08.976000: I runner.py:310] Step = 4100 ; steps/s = 0.72, tokens/s = 34162 (16461 source, 17701 target) ; Learning rate = 0.000362 ; Loss = 2.693198\n",
      "2024-12-11 11:47:27.661000: I runner.py:310] Step = 4200 ; steps/s = 0.72, tokens/s = 34159 (16451 source, 17708 target) ; Learning rate = 0.000371 ; Loss = 2.610962\n",
      "2024-12-11 11:49:44.060000: I runner.py:310] Step = 4300 ; steps/s = 0.73, tokens/s = 34036 (16408 source, 17628 target) ; Learning rate = 0.000380 ; Loss = 2.733527\n",
      "2024-12-11 11:52:02.768000: I runner.py:310] Step = 4400 ; steps/s = 0.72, tokens/s = 34170 (16461 source, 17709 target) ; Learning rate = 0.000389 ; Loss = 2.587862\n",
      "2024-12-11 11:54:21.463000: I runner.py:310] Step = 4500 ; steps/s = 0.72, tokens/s = 34172 (16470 source, 17702 target) ; Learning rate = 0.000398 ; Loss = 2.551741\n",
      "2024-12-11 11:56:40.153000: I runner.py:310] Step = 4600 ; steps/s = 0.72, tokens/s = 34160 (16455 source, 17705 target) ; Learning rate = 0.000407 ; Loss = 2.538768\n",
      "2024-12-11 11:58:56.535000: I runner.py:310] Step = 4700 ; steps/s = 0.73, tokens/s = 34044 (16396 source, 17648 target) ; Learning rate = 0.000416 ; Loss = 2.523369\n",
      "2024-12-11 12:01:15.191000: I runner.py:310] Step = 4800 ; steps/s = 0.72, tokens/s = 34179 (16477 source, 17702 target) ; Learning rate = 0.000424 ; Loss = 2.366382\n",
      "2024-12-11 12:03:33.826000: I runner.py:310] Step = 4900 ; steps/s = 0.72, tokens/s = 34177 (16469 source, 17708 target) ; Learning rate = 0.000433 ; Loss = 2.503278\n",
      "2024-12-11 12:05:52.204000: I runner.py:310] Step = 5000 ; steps/s = 0.72, tokens/s = 34256 (16498 source, 17758 target) ; Learning rate = 0.000442 ; Loss = 2.459738\n",
      "2024-12-11 12:08:10.574000: I runner.py:310] Step = 5100 ; steps/s = 0.72, tokens/s = 34242 (16493 source, 17749 target) ; Learning rate = 0.000451 ; Loss = 2.520109\n",
      "2024-12-11 12:10:26.661000: I runner.py:310] Step = 5200 ; steps/s = 0.73, tokens/s = 34123 (16432 source, 17691 target) ; Learning rate = 0.000460 ; Loss = 2.324572\n",
      "2024-12-11 12:12:45.056000: I runner.py:310] Step = 5300 ; steps/s = 0.72, tokens/s = 34226 (16486 source, 17740 target) ; Learning rate = 0.000469 ; Loss = 2.369324\n",
      "2024-12-11 12:15:03.598000: I runner.py:310] Step = 5400 ; steps/s = 0.72, tokens/s = 34215 (16487 source, 17728 target) ; Learning rate = 0.000477 ; Loss = 2.360646\n",
      "2024-12-11 12:17:22.312000: I runner.py:310] Step = 5500 ; steps/s = 0.72, tokens/s = 34170 (16472 source, 17698 target) ; Learning rate = 0.000486 ; Loss = 2.372438\n",
      "2024-12-11 12:19:38.729000: I runner.py:310] Step = 5600 ; steps/s = 0.73, tokens/s = 34028 (16402 source, 17626 target) ; Learning rate = 0.000495 ; Loss = 2.322939\n",
      "2024-12-11 12:21:57.452000: I runner.py:310] Step = 5700 ; steps/s = 0.72, tokens/s = 34162 (16458 source, 17704 target) ; Learning rate = 0.000504 ; Loss = 2.326732\n",
      "2024-12-11 12:24:16.151000: I runner.py:310] Step = 5800 ; steps/s = 0.72, tokens/s = 34185 (16473 source, 17712 target) ; Learning rate = 0.000513 ; Loss = 2.296480\n",
      "2024-12-11 12:26:34.821000: I runner.py:310] Step = 5900 ; steps/s = 0.72, tokens/s = 34151 (16455 source, 17696 target) ; Learning rate = 0.000522 ; Loss = 2.304628\n",
      "2024-12-11 12:28:51.203000: I runner.py:310] Step = 6000 ; steps/s = 0.73, tokens/s = 34048 (16396 source, 17652 target) ; Learning rate = 0.000530 ; Loss = 2.262595\n",
      "2024-12-11 12:31:09.906000: I runner.py:310] Step = 6100 ; steps/s = 0.72, tokens/s = 34184 (16466 source, 17718 target) ; Learning rate = 0.000539 ; Loss = 2.226943\n",
      "2024-12-11 12:33:28.608000: I runner.py:310] Step = 6200 ; steps/s = 0.72, tokens/s = 34152 (16462 source, 17690 target) ; Learning rate = 0.000548 ; Loss = 2.281102\n",
      "2024-12-11 12:35:47.307000: I runner.py:310] Step = 6300 ; steps/s = 0.72, tokens/s = 34179 (16464 source, 17715 target) ; Learning rate = 0.000557 ; Loss = 2.221749\n",
      "2024-12-11 12:38:04.012000: I runner.py:310] Step = 6400 ; steps/s = 0.73, tokens/s = 34023 (16395 source, 17628 target) ; Learning rate = 0.000566 ; Loss = 2.407824\n",
      "2024-12-11 12:40:22.399000: I runner.py:310] Step = 6500 ; steps/s = 0.72, tokens/s = 34159 (16449 source, 17710 target) ; Learning rate = 0.000575 ; Loss = 2.147919\n",
      "2024-12-11 12:42:41.110000: I runner.py:310] Step = 6600 ; steps/s = 0.72, tokens/s = 34175 (16459 source, 17716 target) ; Learning rate = 0.000583 ; Loss = 2.189025\n",
      "2024-12-11 12:44:59.830000: I runner.py:310] Step = 6700 ; steps/s = 0.72, tokens/s = 34161 (16468 source, 17693 target) ; Learning rate = 0.000592 ; Loss = 2.208680\n",
      "2024-12-11 12:47:18.537000: I runner.py:310] Step = 6800 ; steps/s = 0.72, tokens/s = 34162 (16464 source, 17698 target) ; Learning rate = 0.000601 ; Loss = 2.235984\n",
      "2024-12-11 12:49:34.979000: I runner.py:310] Step = 6900 ; steps/s = 0.73, tokens/s = 34038 (16387 source, 17651 target) ; Learning rate = 0.000610 ; Loss = 2.155521\n",
      "2024-12-11 12:51:53.648000: I runner.py:310] Step = 7000 ; steps/s = 0.72, tokens/s = 34189 (16468 source, 17721 target) ; Learning rate = 0.000619 ; Loss = 2.136424\n",
      "2024-12-11 12:54:12.344000: I runner.py:310] Step = 7100 ; steps/s = 0.72, tokens/s = 34156 (16463 source, 17693 target) ; Learning rate = 0.000628 ; Loss = 2.153209\n",
      "2024-12-11 12:56:31.047000: I runner.py:310] Step = 7200 ; steps/s = 0.72, tokens/s = 34152 (16464 source, 17688 target) ; Learning rate = 0.000636 ; Loss = 2.189146\n",
      "2024-12-11 12:58:47.485000: I runner.py:310] Step = 7300 ; steps/s = 0.73, tokens/s = 34038 (16403 source, 17635 target) ; Learning rate = 0.000645 ; Loss = 2.113774\n",
      "2024-12-11 13:01:06.198000: I runner.py:310] Step = 7400 ; steps/s = 0.72, tokens/s = 34155 (16452 source, 17703 target) ; Learning rate = 0.000654 ; Loss = 2.093533\n",
      "2024-12-11 13:03:24.865000: I runner.py:310] Step = 7500 ; steps/s = 0.72, tokens/s = 34181 (16466 source, 17715 target) ; Learning rate = 0.000663 ; Loss = 2.154761\n",
      "2024-12-11 13:05:43.469000: I runner.py:310] Step = 7600 ; steps/s = 0.72, tokens/s = 34191 (16471 source, 17720 target) ; Learning rate = 0.000672 ; Loss = 2.168848\n",
      "2024-12-11 13:07:59.544000: I runner.py:310] Step = 7700 ; steps/s = 0.73, tokens/s = 34126 (16441 source, 17685 target) ; Learning rate = 0.000681 ; Loss = 2.130290\n",
      "2024-12-11 13:10:17.929000: I runner.py:310] Step = 7800 ; steps/s = 0.72, tokens/s = 34235 (16489 source, 17746 target) ; Learning rate = 0.000690 ; Loss = 2.077514\n",
      "2024-12-11 13:12:36.290000: I runner.py:310] Step = 7900 ; steps/s = 0.72, tokens/s = 34255 (16503 source, 17752 target) ; Learning rate = 0.000698 ; Loss = 2.113122\n",
      "2024-12-11 13:14:54.690000: I runner.py:310] Step = 8000 ; steps/s = 0.72, tokens/s = 34252 (16505 source, 17747 target) ; Learning rate = 0.000707 ; Loss = 2.116596\n",
      "2024-12-11 13:17:13.180000: I runner.py:310] Step = 8100 ; steps/s = 0.72, tokens/s = 34203 (16490 source, 17713 target) ; Learning rate = 0.000716 ; Loss = 2.116422\n",
      "2024-12-11 13:19:29.641000: I runner.py:310] Step = 8200 ; steps/s = 0.73, tokens/s = 34028 (16387 source, 17641 target) ; Learning rate = 0.000725 ; Loss = 2.005553\n",
      "2024-12-11 13:21:48.294000: I runner.py:310] Step = 8300 ; steps/s = 0.72, tokens/s = 34175 (16461 source, 17714 target) ; Learning rate = 0.000734 ; Loss = 2.014838\n",
      "2024-12-11 13:24:07.010000: I runner.py:310] Step = 8400 ; steps/s = 0.72, tokens/s = 34161 (16463 source, 17698 target) ; Learning rate = 0.000743 ; Loss = 2.117814\n",
      "2024-12-11 13:26:25.656000: I runner.py:310] Step = 8500 ; steps/s = 0.72, tokens/s = 34181 (16474 source, 17707 target) ; Learning rate = 0.000751 ; Loss = 2.062938\n",
      "2024-12-11 13:28:42.083000: I runner.py:310] Step = 8600 ; steps/s = 0.73, tokens/s = 34038 (16396 source, 17642 target) ; Learning rate = 0.000760 ; Loss = 2.083991\n",
      "2024-12-11 13:31:00.756000: I runner.py:310] Step = 8700 ; steps/s = 0.72, tokens/s = 34165 (16457 source, 17708 target) ; Learning rate = 0.000769 ; Loss = 1.999951\n",
      "2024-12-11 13:33:19.448000: I runner.py:310] Step = 8800 ; steps/s = 0.72, tokens/s = 34176 (16473 source, 17703 target) ; Learning rate = 0.000778 ; Loss = 1.999607\n",
      "2024-12-11 13:35:38.186000: I runner.py:310] Step = 8900 ; steps/s = 0.72, tokens/s = 34161 (16467 source, 17694 target) ; Learning rate = 0.000787 ; Loss = 2.063403\n",
      "2024-12-11 13:37:54.669000: I runner.py:310] Step = 9000 ; steps/s = 0.73, tokens/s = 34018 (16379 source, 17639 target) ; Learning rate = 0.000796 ; Loss = 2.018058\n",
      "2024-12-11 13:40:13.329000: I runner.py:310] Step = 9100 ; steps/s = 0.72, tokens/s = 34168 (16461 source, 17707 target) ; Learning rate = 0.000804 ; Loss = 1.988793\n",
      "2024-12-11 13:42:32.060000: I runner.py:310] Step = 9200 ; steps/s = 0.72, tokens/s = 34157 (16452 source, 17705 target) ; Learning rate = 0.000813 ; Loss = 2.021676\n",
      "2024-12-11 13:44:50.732000: I runner.py:310] Step = 9300 ; steps/s = 0.72, tokens/s = 34183 (16472 source, 17711 target) ; Learning rate = 0.000822 ; Loss = 2.019188\n",
      "2024-12-11 13:47:07.160000: I runner.py:310] Step = 9400 ; steps/s = 0.73, tokens/s = 34035 (16403 source, 17632 target) ; Learning rate = 0.000831 ; Loss = 1.986867\n",
      "2024-12-11 13:49:25.830000: I runner.py:310] Step = 9500 ; steps/s = 0.72, tokens/s = 34164 (16462 source, 17702 target) ; Learning rate = 0.000840 ; Loss = 1.955965\n",
      "2024-12-11 13:51:44.513000: I runner.py:310] Step = 9600 ; steps/s = 0.72, tokens/s = 34157 (16460 source, 17697 target) ; Learning rate = 0.000849 ; Loss = 2.007609\n",
      "2024-12-11 13:54:03.214000: I runner.py:310] Step = 9700 ; steps/s = 0.72, tokens/s = 34171 (16463 source, 17708 target) ; Learning rate = 0.000857 ; Loss = 2.020961\n",
      "2024-12-11 13:56:21.923000: I runner.py:310] Step = 9800 ; steps/s = 0.72, tokens/s = 34172 (16460 source, 17712 target) ; Learning rate = 0.000866 ; Loss = 1.998344\n",
      "2024-12-11 13:58:38.411000: I runner.py:310] Step = 9900 ; steps/s = 0.73, tokens/s = 34025 (16386 source, 17639 target) ; Learning rate = 0.000875 ; Loss = 1.942790\n",
      "2024-12-11 14:00:57.094000: I runner.py:310] Step = 10000 ; steps/s = 0.72, tokens/s = 34159 (16462 source, 17697 target) ; Learning rate = 0.000884 ; Loss = 1.986383\n",
      "2024-12-11 14:00:59.112000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-10000\n",
      "2024-12-11 14:03:17.776000: I runner.py:310] Step = 10100 ; steps/s = 0.72, tokens/s = 34170 (16452 source, 17718 target) ; Learning rate = 0.000879 ; Loss = 1.999503\n",
      "2024-12-11 14:05:36.479000: I runner.py:310] Step = 10200 ; steps/s = 0.72, tokens/s = 34180 (16478 source, 17702 target) ; Learning rate = 0.000875 ; Loss = 2.012948\n",
      "2024-12-11 14:07:52.814000: I runner.py:310] Step = 10300 ; steps/s = 0.73, tokens/s = 34052 (16395 source, 17657 target) ; Learning rate = 0.000871 ; Loss = 1.928654\n",
      "2024-12-11 14:10:11.177000: I runner.py:310] Step = 10400 ; steps/s = 0.72, tokens/s = 34236 (16493 source, 17743 target) ; Learning rate = 0.000867 ; Loss = 1.919124\n",
      "2024-12-11 14:12:29.552000: I runner.py:310] Step = 10500 ; steps/s = 0.72, tokens/s = 34248 (16512 source, 17736 target) ; Learning rate = 0.000863 ; Loss = 1.937012\n",
      "2024-12-11 14:14:47.966000: I runner.py:310] Step = 10600 ; steps/s = 0.72, tokens/s = 34245 (16507 source, 17738 target) ; Learning rate = 0.000858 ; Loss = 1.959251\n",
      "2024-12-11 14:17:04.006000: I runner.py:310] Step = 10700 ; steps/s = 0.74, tokens/s = 34140 (16443 source, 17697 target) ; Learning rate = 0.000854 ; Loss = 1.894088\n",
      "2024-12-11 14:19:22.496000: I runner.py:310] Step = 10800 ; steps/s = 0.72, tokens/s = 34227 (16490 source, 17737 target) ; Learning rate = 0.000850 ; Loss = 1.908074\n",
      "2024-12-11 14:21:41.171000: I runner.py:310] Step = 10900 ; steps/s = 0.72, tokens/s = 34167 (16459 source, 17708 target) ; Learning rate = 0.000847 ; Loss = 1.908825\n",
      "2024-12-11 14:23:59.862000: I runner.py:310] Step = 11000 ; steps/s = 0.72, tokens/s = 34169 (16464 source, 17705 target) ; Learning rate = 0.000843 ; Loss = 1.974620\n",
      "2024-12-11 14:26:16.283000: I runner.py:310] Step = 11100 ; steps/s = 0.73, tokens/s = 34025 (16397 source, 17628 target) ; Learning rate = 0.000839 ; Loss = 1.910747\n",
      "2024-12-11 14:28:35.005000: I runner.py:310] Step = 11200 ; steps/s = 0.72, tokens/s = 34164 (16456 source, 17708 target) ; Learning rate = 0.000835 ; Loss = 1.889777\n",
      "2024-12-11 14:30:53.699000: I runner.py:310] Step = 11300 ; steps/s = 0.72, tokens/s = 34177 (16473 source, 17704 target) ; Learning rate = 0.000831 ; Loss = 1.883837\n",
      "2024-12-11 14:33:12.399000: I runner.py:310] Step = 11400 ; steps/s = 0.72, tokens/s = 34168 (16470 source, 17698 target) ; Learning rate = 0.000828 ; Loss = 1.901484\n",
      "2024-12-11 14:35:31.086000: I runner.py:310] Step = 11500 ; steps/s = 0.72, tokens/s = 34150 (16439 source, 17711 target) ; Learning rate = 0.000824 ; Loss = 1.891391\n",
      "2024-12-11 14:37:47.565000: I runner.py:310] Step = 11600 ; steps/s = 0.73, tokens/s = 34025 (16400 source, 17625 target) ; Learning rate = 0.000821 ; Loss = 1.885795\n",
      "2024-12-11 14:40:06.261000: I runner.py:310] Step = 11700 ; steps/s = 0.72, tokens/s = 34172 (16467 source, 17705 target) ; Learning rate = 0.000817 ; Loss = 1.871491\n",
      "2024-12-11 14:42:24.921000: I runner.py:310] Step = 11800 ; steps/s = 0.72, tokens/s = 34188 (16458 source, 17730 target) ; Learning rate = 0.000814 ; Loss = 1.850843\n",
      "2024-12-11 14:44:43.650000: I runner.py:310] Step = 11900 ; steps/s = 0.72, tokens/s = 34157 (16465 source, 17692 target) ; Learning rate = 0.000810 ; Loss = 1.872875\n",
      "2024-12-11 14:47:00.072000: I runner.py:310] Step = 12000 ; steps/s = 0.73, tokens/s = 34019 (16381 source, 17638 target) ; Learning rate = 0.000807 ; Loss = 1.841284\n",
      "2024-12-11 14:49:18.770000: I runner.py:310] Step = 12100 ; steps/s = 0.72, tokens/s = 34168 (16465 source, 17703 target) ; Learning rate = 0.000803 ; Loss = 1.860110\n",
      "2024-12-11 14:51:37.429000: I runner.py:310] Step = 12200 ; steps/s = 0.72, tokens/s = 34171 (16470 source, 17701 target) ; Learning rate = 0.000800 ; Loss = 1.874490\n",
      "2024-12-11 14:53:56.091000: I runner.py:310] Step = 12300 ; steps/s = 0.72, tokens/s = 34176 (16468 source, 17708 target) ; Learning rate = 0.000797 ; Loss = 1.872824\n",
      "2024-12-11 14:56:12.488000: I runner.py:310] Step = 12400 ; steps/s = 0.73, tokens/s = 34054 (16408 source, 17646 target) ; Learning rate = 0.000794 ; Loss = 1.795003\n",
      "2024-12-11 14:58:31.168000: I runner.py:310] Step = 12500 ; steps/s = 0.72, tokens/s = 34172 (16466 source, 17706 target) ; Learning rate = 0.000791 ; Loss = 1.792744\n",
      "2024-12-11 15:00:49.861000: I runner.py:310] Step = 12600 ; steps/s = 0.72, tokens/s = 34158 (16450 source, 17708 target) ; Learning rate = 0.000787 ; Loss = 1.823331\n",
      "2024-12-11 15:03:08.556000: I runner.py:310] Step = 12700 ; steps/s = 0.72, tokens/s = 34178 (16470 source, 17708 target) ; Learning rate = 0.000784 ; Loss = 1.850177\n",
      "2024-12-11 15:05:25.332000: I runner.py:310] Step = 12800 ; steps/s = 0.73, tokens/s = 34052 (16405 source, 17647 target) ; Learning rate = 0.000781 ; Loss = 1.854104\n",
      "2024-12-11 15:07:43.666000: I runner.py:310] Step = 12900 ; steps/s = 0.72, tokens/s = 34160 (16457 source, 17703 target) ; Learning rate = 0.000778 ; Loss = 1.827905\n",
      "2024-12-11 15:10:02.284000: I runner.py:310] Step = 13000 ; steps/s = 0.72, tokens/s = 34182 (16467 source, 17715 target) ; Learning rate = 0.000775 ; Loss = 1.800996\n",
      "2024-12-11 15:12:20.631000: I runner.py:310] Step = 13100 ; steps/s = 0.72, tokens/s = 34245 (16506 source, 17739 target) ; Learning rate = 0.000772 ; Loss = 1.816611\n",
      "2024-12-11 15:14:38.928000: I runner.py:310] Step = 13200 ; steps/s = 0.72, tokens/s = 34275 (16509 source, 17766 target) ; Learning rate = 0.000769 ; Loss = 1.820361\n",
      "2024-12-11 15:16:54.993000: I runner.py:310] Step = 13300 ; steps/s = 0.73, tokens/s = 34118 (16435 source, 17683 target) ; Learning rate = 0.000766 ; Loss = 1.750134\n",
      "2024-12-11 15:19:13.480000: I runner.py:310] Step = 13400 ; steps/s = 0.72, tokens/s = 34206 (16478 source, 17728 target) ; Learning rate = 0.000764 ; Loss = 1.792987\n",
      "2024-12-11 15:21:32.170000: I runner.py:310] Step = 13500 ; steps/s = 0.72, tokens/s = 34183 (16481 source, 17702 target) ; Learning rate = 0.000761 ; Loss = 1.808588\n",
      "2024-12-11 15:23:50.859000: I runner.py:310] Step = 13600 ; steps/s = 0.72, tokens/s = 34170 (16467 source, 17703 target) ; Learning rate = 0.000758 ; Loss = 1.839661\n",
      "2024-12-11 15:26:07.273000: I runner.py:310] Step = 13700 ; steps/s = 0.73, tokens/s = 34037 (16387 source, 17650 target) ; Learning rate = 0.000755 ; Loss = 1.784175\n",
      "2024-12-11 15:28:26.008000: I runner.py:310] Step = 13800 ; steps/s = 0.72, tokens/s = 34158 (16457 source, 17701 target) ; Learning rate = 0.000752 ; Loss = 1.778917\n",
      "2024-12-11 15:30:44.647000: I runner.py:310] Step = 13900 ; steps/s = 0.72, tokens/s = 34183 (16468 source, 17715 target) ; Learning rate = 0.000750 ; Loss = 1.767245\n",
      "2024-12-11 15:33:03.361000: I runner.py:310] Step = 14000 ; steps/s = 0.72, tokens/s = 34167 (16463 source, 17704 target) ; Learning rate = 0.000747 ; Loss = 1.780443\n",
      "2024-12-11 15:35:19.783000: I runner.py:310] Step = 14100 ; steps/s = 0.73, tokens/s = 34022 (16392 source, 17630 target) ; Learning rate = 0.000744 ; Loss = 1.770679\n",
      "2024-12-11 15:37:38.460000: I runner.py:310] Step = 14200 ; steps/s = 0.72, tokens/s = 34175 (16471 source, 17704 target) ; Learning rate = 0.000742 ; Loss = 1.748733\n",
      "2024-12-11 15:39:57.144000: I runner.py:310] Step = 14300 ; steps/s = 0.72, tokens/s = 34167 (16467 source, 17700 target) ; Learning rate = 0.000739 ; Loss = 1.776337\n",
      "2024-12-11 15:42:15.867000: I runner.py:310] Step = 14400 ; steps/s = 0.72, tokens/s = 34159 (16461 source, 17698 target) ; Learning rate = 0.000737 ; Loss = 1.793766\n",
      "2024-12-11 15:44:34.574000: I runner.py:310] Step = 14500 ; steps/s = 0.72, tokens/s = 34168 (16457 source, 17711 target) ; Learning rate = 0.000734 ; Loss = 1.784721\n",
      "2024-12-11 15:46:51.017000: I runner.py:310] Step = 14600 ; steps/s = 0.73, tokens/s = 34035 (16394 source, 17641 target) ; Learning rate = 0.000731 ; Loss = 1.766967\n",
      "2024-12-11 15:49:09.653000: I runner.py:310] Step = 14700 ; steps/s = 0.72, tokens/s = 34180 (16469 source, 17711 target) ; Learning rate = 0.000729 ; Loss = 1.754627\n",
      "2024-12-11 15:51:28.301000: I runner.py:310] Step = 14800 ; steps/s = 0.72, tokens/s = 34179 (16468 source, 17711 target) ; Learning rate = 0.000727 ; Loss = 1.747451\n",
      "2024-12-11 15:53:46.991000: I runner.py:310] Step = 14900 ; steps/s = 0.72, tokens/s = 34164 (16461 source, 17703 target) ; Learning rate = 0.000724 ; Loss = 1.770653\n",
      "2024-12-11 15:56:03.424000: I runner.py:310] Step = 15000 ; steps/s = 0.73, tokens/s = 34047 (16405 source, 17642 target) ; Learning rate = 0.000722 ; Loss = 1.757764\n",
      "2024-12-11 15:58:22.135000: I runner.py:310] Step = 15100 ; steps/s = 0.72, tokens/s = 34159 (16464 source, 17695 target) ; Learning rate = 0.000719 ; Loss = 1.735259\n",
      "2024-12-11 16:00:40.806000: I runner.py:310] Step = 15200 ; steps/s = 0.72, tokens/s = 34181 (16460 source, 17721 target) ; Learning rate = 0.000717 ; Loss = 1.751906\n",
      "2024-12-11 16:02:59.555000: I runner.py:310] Step = 15300 ; steps/s = 0.72, tokens/s = 34144 (16459 source, 17685 target) ; Learning rate = 0.000715 ; Loss = 1.765120\n",
      "2024-12-11 16:05:15.938000: I runner.py:310] Step = 15400 ; steps/s = 0.73, tokens/s = 34037 (16385 source, 17652 target) ; Learning rate = 0.000712 ; Loss = 1.723719\n",
      "2024-12-11 16:07:34.645000: I runner.py:310] Step = 15500 ; steps/s = 0.72, tokens/s = 34175 (16460 source, 17715 target) ; Learning rate = 0.000710 ; Loss = 1.721496\n",
      "2024-12-11 16:09:53.271000: I runner.py:310] Step = 15600 ; steps/s = 0.72, tokens/s = 34177 (16458 source, 17719 target) ; Learning rate = 0.000708 ; Loss = 1.760117\n",
      "2024-12-11 16:12:11.615000: I runner.py:310] Step = 15700 ; steps/s = 0.72, tokens/s = 34253 (16513 source, 17740 target) ; Learning rate = 0.000705 ; Loss = 1.749211\n",
      "2024-12-11 16:14:27.679000: I runner.py:310] Step = 15800 ; steps/s = 0.73, tokens/s = 34125 (16449 source, 17676 target) ; Learning rate = 0.000703 ; Loss = 1.726843\n",
      "2024-12-11 16:16:46.011000: I runner.py:310] Step = 15900 ; steps/s = 0.72, tokens/s = 34265 (16508 source, 17757 target) ; Learning rate = 0.000701 ; Loss = 1.736195\n",
      "2024-12-11 16:19:04.592000: I runner.py:310] Step = 16000 ; steps/s = 0.72, tokens/s = 34190 (16469 source, 17721 target) ; Learning rate = 0.000699 ; Loss = 1.708457\n",
      "2024-12-11 16:21:23.290000: I runner.py:310] Step = 16100 ; steps/s = 0.72, tokens/s = 34166 (16469 source, 17697 target) ; Learning rate = 0.000697 ; Loss = 1.739633\n",
      "2024-12-11 16:23:41.905000: I runner.py:310] Step = 16200 ; steps/s = 0.72, tokens/s = 34177 (16459 source, 17718 target) ; Learning rate = 0.000694 ; Loss = 1.743446\n",
      "2024-12-11 16:25:58.327000: I runner.py:310] Step = 16300 ; steps/s = 0.73, tokens/s = 34053 (16399 source, 17654 target) ; Learning rate = 0.000692 ; Loss = 1.697558\n",
      "2024-12-11 16:28:17.054000: I runner.py:310] Step = 16400 ; steps/s = 0.72, tokens/s = 34149 (16455 source, 17694 target) ; Learning rate = 0.000690 ; Loss = 1.709687\n",
      "2024-12-11 16:30:35.722000: I runner.py:310] Step = 16500 ; steps/s = 0.72, tokens/s = 34166 (16466 source, 17700 target) ; Learning rate = 0.000688 ; Loss = 1.749049\n",
      "2024-12-11 16:32:54.422000: I runner.py:310] Step = 16600 ; steps/s = 0.72, tokens/s = 34179 (16469 source, 17710 target) ; Learning rate = 0.000686 ; Loss = 1.743073\n",
      "2024-12-11 16:35:10.851000: I runner.py:310] Step = 16700 ; steps/s = 0.73, tokens/s = 34024 (16377 source, 17647 target) ; Learning rate = 0.000684 ; Loss = 1.678124\n",
      "2024-12-11 16:37:29.578000: I runner.py:310] Step = 16800 ; steps/s = 0.72, tokens/s = 34153 (16453 source, 17700 target) ; Learning rate = 0.000682 ; Loss = 1.708043\n",
      "2024-12-11 16:39:48.269000: I runner.py:310] Step = 16900 ; steps/s = 0.72, tokens/s = 34165 (16470 source, 17695 target) ; Learning rate = 0.000680 ; Loss = 1.727294\n",
      "2024-12-11 16:42:06.934000: I runner.py:310] Step = 17000 ; steps/s = 0.72, tokens/s = 34189 (16481 source, 17708 target) ; Learning rate = 0.000678 ; Loss = 1.730155\n",
      "2024-12-11 16:44:23.302000: I runner.py:310] Step = 17100 ; steps/s = 0.73, tokens/s = 34044 (16403 source, 17641 target) ; Learning rate = 0.000676 ; Loss = 1.709520\n",
      "2024-12-11 16:46:41.971000: I runner.py:310] Step = 17200 ; steps/s = 0.72, tokens/s = 34175 (16467 source, 17708 target) ; Learning rate = 0.000674 ; Loss = 1.692489\n",
      "2024-12-11 16:49:00.674000: I runner.py:310] Step = 17300 ; steps/s = 0.72, tokens/s = 34168 (16458 source, 17710 target) ; Learning rate = 0.000672 ; Loss = 1.701491\n",
      "2024-12-11 16:51:19.324000: I runner.py:310] Step = 17400 ; steps/s = 0.72, tokens/s = 34178 (16472 source, 17706 target) ; Learning rate = 0.000670 ; Loss = 1.710181\n",
      "2024-12-11 16:53:35.780000: I runner.py:310] Step = 17500 ; steps/s = 0.73, tokens/s = 34024 (16392 source, 17632 target) ; Learning rate = 0.000668 ; Loss = 1.691337\n",
      "2024-12-11 16:55:54.473000: I runner.py:310] Step = 17600 ; steps/s = 0.72, tokens/s = 34181 (16463 source, 17718 target) ; Learning rate = 0.000666 ; Loss = 1.692244\n",
      "2024-12-11 16:58:13.155000: I runner.py:310] Step = 17700 ; steps/s = 0.72, tokens/s = 34152 (16454 source, 17698 target) ; Learning rate = 0.000664 ; Loss = 1.700685\n",
      "2024-12-11 17:00:31.843000: I runner.py:310] Step = 17800 ; steps/s = 0.72, tokens/s = 34171 (16461 source, 17710 target) ; Learning rate = 0.000662 ; Loss = 1.698054\n",
      "2024-12-11 17:02:50.593000: I runner.py:310] Step = 17900 ; steps/s = 0.72, tokens/s = 34157 (16462 source, 17695 target) ; Learning rate = 0.000661 ; Loss = 1.712675\n",
      "2024-12-11 17:05:07.034000: I runner.py:310] Step = 18000 ; steps/s = 0.73, tokens/s = 34049 (16396 source, 17653 target) ; Learning rate = 0.000659 ; Loss = 1.663831\n",
      "2024-12-11 17:07:25.689000: I runner.py:310] Step = 18100 ; steps/s = 0.72, tokens/s = 34166 (16459 source, 17707 target) ; Learning rate = 0.000657 ; Loss = 1.684616\n",
      "2024-12-11 17:09:44.180000: I runner.py:310] Step = 18200 ; steps/s = 0.72, tokens/s = 34223 (16487 source, 17736 target) ; Learning rate = 0.000655 ; Loss = 1.689479\n",
      "2024-12-11 17:12:02.524000: I runner.py:310] Step = 18300 ; steps/s = 0.72, tokens/s = 34253 (16516 source, 17737 target) ; Learning rate = 0.000653 ; Loss = 1.721776\n",
      "2024-12-11 17:14:18.569000: I runner.py:310] Step = 18400 ; steps/s = 0.74, tokens/s = 34107 (16436 source, 17671 target) ; Learning rate = 0.000652 ; Loss = 1.694174\n",
      "2024-12-11 17:16:36.876000: I runner.py:310] Step = 18500 ; steps/s = 0.72, tokens/s = 34260 (16499 source, 17761 target) ; Learning rate = 0.000650 ; Loss = 1.670297\n",
      "2024-12-11 17:18:55.555000: I runner.py:310] Step = 18600 ; steps/s = 0.72, tokens/s = 34185 (16475 source, 17710 target) ; Learning rate = 0.000648 ; Loss = 1.676655\n",
      "2024-12-11 17:21:14.281000: I runner.py:310] Step = 18700 ; steps/s = 0.72, tokens/s = 34162 (16454 source, 17708 target) ; Learning rate = 0.000646 ; Loss = 1.688588\n",
      "2024-12-11 17:23:30.751000: I runner.py:310] Step = 18800 ; steps/s = 0.73, tokens/s = 34024 (16403 source, 17621 target) ; Learning rate = 0.000645 ; Loss = 1.664659\n",
      "2024-12-11 17:25:49.425000: I runner.py:310] Step = 18900 ; steps/s = 0.72, tokens/s = 34163 (16465 source, 17698 target) ; Learning rate = 0.000643 ; Loss = 1.665685\n",
      "2024-12-11 17:28:08.110000: I runner.py:310] Step = 19000 ; steps/s = 0.72, tokens/s = 34169 (16459 source, 17710 target) ; Learning rate = 0.000641 ; Loss = 1.669270\n",
      "2024-12-11 17:30:26.791000: I runner.py:310] Step = 19100 ; steps/s = 0.72, tokens/s = 34170 (16461 source, 17709 target) ; Learning rate = 0.000640 ; Loss = 1.687064\n",
      "2024-12-11 17:32:43.795000: I runner.py:310] Step = 19200 ; steps/s = 0.73, tokens/s = 34073 (16414 source, 17659 target) ; Learning rate = 0.000638 ; Loss = 1.694841\n",
      "2024-12-11 17:35:01.913000: I runner.py:310] Step = 19300 ; steps/s = 0.72, tokens/s = 34143 (16448 source, 17695 target) ; Learning rate = 0.000636 ; Loss = 1.676138\n",
      "2024-12-11 17:37:20.627000: I runner.py:310] Step = 19400 ; steps/s = 0.72, tokens/s = 34173 (16474 source, 17699 target) ; Learning rate = 0.000635 ; Loss = 1.675300\n",
      "2024-12-11 17:39:39.306000: I runner.py:310] Step = 19500 ; steps/s = 0.72, tokens/s = 34154 (16452 source, 17702 target) ; Learning rate = 0.000633 ; Loss = 1.666749\n",
      "2024-12-11 17:41:57.989000: I runner.py:310] Step = 19600 ; steps/s = 0.72, tokens/s = 34183 (16466 source, 17717 target) ; Learning rate = 0.000631 ; Loss = 1.671799\n",
      "2024-12-11 17:44:14.414000: I runner.py:310] Step = 19700 ; steps/s = 0.73, tokens/s = 34014 (16382 source, 17632 target) ; Learning rate = 0.000630 ; Loss = 1.688588\n",
      "2024-12-11 17:46:33.065000: I runner.py:310] Step = 19800 ; steps/s = 0.72, tokens/s = 34180 (16478 source, 17702 target) ; Learning rate = 0.000628 ; Loss = 1.651394\n",
      "2024-12-11 17:48:51.740000: I runner.py:310] Step = 19900 ; steps/s = 0.72, tokens/s = 34177 (16459 source, 17718 target) ; Learning rate = 0.000627 ; Loss = 1.660143\n",
      "2024-12-11 17:51:10.434000: I runner.py:310] Step = 20000 ; steps/s = 0.72, tokens/s = 34172 (16467 source, 17705 target) ; Learning rate = 0.000625 ; Loss = 1.657849\n",
      "2024-12-11 17:51:12.374000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-20000\n",
      "2024-12-11 17:53:28.834000: I runner.py:310] Step = 20100 ; steps/s = 0.73, tokens/s = 34018 (16389 source, 17629 target) ; Learning rate = 0.000623 ; Loss = 1.669005\n",
      "2024-12-11 17:55:47.495000: I runner.py:310] Step = 20200 ; steps/s = 0.72, tokens/s = 34185 (16459 source, 17726 target) ; Learning rate = 0.000622 ; Loss = 1.658652\n",
      "2024-12-11 17:58:06.176000: I runner.py:310] Step = 20300 ; steps/s = 0.72, tokens/s = 34181 (16472 source, 17709 target) ; Learning rate = 0.000620 ; Loss = 1.654613\n",
      "2024-12-11 18:00:24.852000: I runner.py:310] Step = 20400 ; steps/s = 0.72, tokens/s = 34156 (16464 source, 17692 target) ; Learning rate = 0.000619 ; Loss = 1.655252\n",
      "2024-12-11 18:02:41.329000: I runner.py:310] Step = 20500 ; steps/s = 0.73, tokens/s = 34028 (16393 source, 17635 target) ; Learning rate = 0.000617 ; Loss = 1.658405\n",
      "2024-12-11 18:05:00.053000: I runner.py:310] Step = 20600 ; steps/s = 0.72, tokens/s = 34150 (16462 source, 17688 target) ; Learning rate = 0.000616 ; Loss = 1.660667\n",
      "2024-12-11 18:07:18.764000: I runner.py:310] Step = 20700 ; steps/s = 0.72, tokens/s = 34180 (16481 source, 17699 target) ; Learning rate = 0.000614 ; Loss = 1.649819\n",
      "2024-12-11 18:09:37.183000: I runner.py:310] Step = 20800 ; steps/s = 0.72, tokens/s = 34221 (16477 source, 17744 target) ; Learning rate = 0.000613 ; Loss = 1.663264\n",
      "2024-12-11 18:11:55.547000: I runner.py:310] Step = 20900 ; steps/s = 0.72, tokens/s = 34255 (16498 source, 17757 target) ; Learning rate = 0.000611 ; Loss = 1.664798\n",
      "2024-12-11 18:14:11.581000: I runner.py:310] Step = 21000 ; steps/s = 0.74, tokens/s = 34123 (16449 source, 17674 target) ; Learning rate = 0.000610 ; Loss = 1.658001\n",
      "2024-12-11 18:16:30.064000: I runner.py:310] Step = 21100 ; steps/s = 0.72, tokens/s = 34236 (16496 source, 17740 target) ; Learning rate = 0.000608 ; Loss = 1.636995\n",
      "2024-12-11 18:18:48.730000: I runner.py:310] Step = 21200 ; steps/s = 0.72, tokens/s = 34174 (16461 source, 17713 target) ; Learning rate = 0.000607 ; Loss = 1.645287\n",
      "2024-12-11 18:21:07.423000: I runner.py:310] Step = 21300 ; steps/s = 0.72, tokens/s = 34159 (16447 source, 17712 target) ; Learning rate = 0.000606 ; Loss = 1.639658\n",
      "2024-12-11 18:23:23.858000: I runner.py:310] Step = 21400 ; steps/s = 0.73, tokens/s = 34035 (16402 source, 17633 target) ; Learning rate = 0.000604 ; Loss = 1.616107\n",
      "2024-12-11 18:25:42.562000: I runner.py:310] Step = 21500 ; steps/s = 0.72, tokens/s = 34146 (16458 source, 17688 target) ; Learning rate = 0.000603 ; Loss = 1.639584\n",
      "2024-12-11 18:28:01.271000: I runner.py:310] Step = 21600 ; steps/s = 0.72, tokens/s = 34172 (16462 source, 17710 target) ; Learning rate = 0.000601 ; Loss = 1.665104\n",
      "2024-12-11 18:30:19.965000: I runner.py:310] Step = 21700 ; steps/s = 0.72, tokens/s = 34155 (16455 source, 17700 target) ; Learning rate = 0.000600 ; Loss = 1.668546\n",
      "2024-12-11 18:32:36.381000: I runner.py:310] Step = 21800 ; steps/s = 0.73, tokens/s = 34061 (16403 source, 17658 target) ; Learning rate = 0.000599 ; Loss = 1.649645\n",
      "2024-12-11 18:34:55.067000: I runner.py:310] Step = 21900 ; steps/s = 0.72, tokens/s = 34177 (16464 source, 17713 target) ; Learning rate = 0.000597 ; Loss = 1.651392\n",
      "2024-12-11 18:37:13.771000: I runner.py:310] Step = 22000 ; steps/s = 0.72, tokens/s = 34155 (16453 source, 17702 target) ; Learning rate = 0.000596 ; Loss = 1.641127\n",
      "2024-12-11 18:39:32.433000: I runner.py:310] Step = 22100 ; steps/s = 0.72, tokens/s = 34185 (16478 source, 17707 target) ; Learning rate = 0.000595 ; Loss = 1.636656\n",
      "2024-12-11 18:41:48.815000: I runner.py:310] Step = 22200 ; steps/s = 0.73, tokens/s = 34031 (16396 source, 17635 target) ; Learning rate = 0.000593 ; Loss = 1.635927\n",
      "2024-12-11 18:44:07.493000: I runner.py:310] Step = 22300 ; steps/s = 0.72, tokens/s = 34174 (16449 source, 17725 target) ; Learning rate = 0.000592 ; Loss = 1.621450\n",
      "2024-12-11 18:46:26.175000: I runner.py:310] Step = 22400 ; steps/s = 0.72, tokens/s = 34180 (16474 source, 17706 target) ; Learning rate = 0.000591 ; Loss = 1.641258\n",
      "2024-12-11 18:48:44.874000: I runner.py:310] Step = 22500 ; steps/s = 0.72, tokens/s = 34170 (16464 source, 17706 target) ; Learning rate = 0.000589 ; Loss = 1.638641\n",
      "2024-12-11 18:51:03.575000: I runner.py:310] Step = 22600 ; steps/s = 0.72, tokens/s = 34155 (16460 source, 17695 target) ; Learning rate = 0.000588 ; Loss = 1.657556\n",
      "2024-12-11 18:53:19.971000: I runner.py:310] Step = 22700 ; steps/s = 0.73, tokens/s = 34055 (16408 source, 17647 target) ; Learning rate = 0.000587 ; Loss = 1.614725\n",
      "2024-12-11 18:55:38.657000: I runner.py:310] Step = 22800 ; steps/s = 0.72, tokens/s = 34161 (16457 source, 17704 target) ; Learning rate = 0.000585 ; Loss = 1.632219\n",
      "2024-12-11 18:57:57.371000: I runner.py:310] Step = 22900 ; steps/s = 0.72, tokens/s = 34162 (16466 source, 17696 target) ; Learning rate = 0.000584 ; Loss = 1.631794\n",
      "2024-12-11 19:00:16.083000: I runner.py:310] Step = 23000 ; steps/s = 0.72, tokens/s = 34168 (16459 source, 17709 target) ; Learning rate = 0.000583 ; Loss = 1.648412\n",
      "2024-12-11 19:02:32.558000: I runner.py:310] Step = 23100 ; steps/s = 0.73, tokens/s = 34022 (16387 source, 17635 target) ; Learning rate = 0.000582 ; Loss = 1.623782\n",
      "2024-12-11 19:04:51.224000: I runner.py:310] Step = 23200 ; steps/s = 0.72, tokens/s = 34166 (16462 source, 17704 target) ; Learning rate = 0.000580 ; Loss = 1.629666\n",
      "2024-12-11 19:07:09.823000: I runner.py:310] Step = 23300 ; steps/s = 0.72, tokens/s = 34191 (16474 source, 17717 target) ; Learning rate = 0.000579 ; Loss = 1.612939\n",
      "2024-12-11 19:09:28.118000: I runner.py:310] Step = 23400 ; steps/s = 0.72, tokens/s = 34258 (16509 source, 17749 target) ; Learning rate = 0.000578 ; Loss = 1.638464\n",
      "2024-12-11 19:11:44.140000: I runner.py:310] Step = 23500 ; steps/s = 0.74, tokens/s = 34143 (16445 source, 17698 target) ; Learning rate = 0.000577 ; Loss = 1.620060\n",
      "2024-12-11 19:14:02.507000: I runner.py:310] Step = 23600 ; steps/s = 0.72, tokens/s = 34240 (16491 source, 17749 target) ; Learning rate = 0.000575 ; Loss = 1.626735\n",
      "2024-12-11 19:16:21.205000: I runner.py:310] Step = 23700 ; steps/s = 0.72, tokens/s = 34161 (16459 source, 17702 target) ; Learning rate = 0.000574 ; Loss = 1.630878\n",
      "2024-12-11 19:18:39.917000: I runner.py:310] Step = 23800 ; steps/s = 0.72, tokens/s = 34168 (16471 source, 17697 target) ; Learning rate = 0.000573 ; Loss = 1.628351\n",
      "2024-12-11 19:20:56.374000: I runner.py:310] Step = 23900 ; steps/s = 0.73, tokens/s = 34033 (16400 source, 17633 target) ; Learning rate = 0.000572 ; Loss = 1.617891\n",
      "2024-12-11 19:23:15.096000: I runner.py:310] Step = 24000 ; steps/s = 0.72, tokens/s = 34159 (16464 source, 17695 target) ; Learning rate = 0.000571 ; Loss = 1.603274\n",
      "2024-12-11 19:25:33.809000: I runner.py:310] Step = 24100 ; steps/s = 0.72, tokens/s = 34170 (16464 source, 17706 target) ; Learning rate = 0.000569 ; Loss = 1.630077\n",
      "2024-12-11 19:27:52.481000: I runner.py:310] Step = 24200 ; steps/s = 0.72, tokens/s = 34175 (16459 source, 17716 target) ; Learning rate = 0.000568 ; Loss = 1.639130\n",
      "2024-12-11 19:30:11.172000: I runner.py:310] Step = 24300 ; steps/s = 0.72, tokens/s = 34166 (16459 source, 17707 target) ; Learning rate = 0.000567 ; Loss = 1.642941\n",
      "2024-12-11 19:32:27.626000: I runner.py:310] Step = 24400 ; steps/s = 0.73, tokens/s = 34041 (16398 source, 17643 target) ; Learning rate = 0.000566 ; Loss = 1.604793\n",
      "2024-12-11 19:34:46.330000: I runner.py:310] Step = 24500 ; steps/s = 0.72, tokens/s = 34156 (16457 source, 17699 target) ; Learning rate = 0.000565 ; Loss = 1.620788\n",
      "2024-12-11 19:37:05.060000: I runner.py:310] Step = 24600 ; steps/s = 0.72, tokens/s = 34161 (16455 source, 17706 target) ; Learning rate = 0.000564 ; Loss = 1.624492\n",
      "2024-12-11 19:39:23.735000: I runner.py:310] Step = 24700 ; steps/s = 0.72, tokens/s = 34167 (16466 source, 17701 target) ; Learning rate = 0.000562 ; Loss = 1.632996\n",
      "2024-12-11 19:41:40.188000: I runner.py:310] Step = 24800 ; steps/s = 0.73, tokens/s = 34019 (16395 source, 17624 target) ; Learning rate = 0.000561 ; Loss = 1.603142\n",
      "2024-12-11 19:43:58.898000: I runner.py:310] Step = 24900 ; steps/s = 0.72, tokens/s = 34169 (16463 source, 17706 target) ; Learning rate = 0.000560 ; Loss = 1.615283\n",
      "2024-12-11 19:46:17.569000: I runner.py:310] Step = 25000 ; steps/s = 0.72, tokens/s = 34172 (16460 source, 17712 target) ; Learning rate = 0.000559 ; Loss = 1.621183\n",
      "2024-12-11 19:48:36.245000: I runner.py:310] Step = 25100 ; steps/s = 0.72, tokens/s = 34176 (16465 source, 17711 target) ; Learning rate = 0.000558 ; Loss = 1.618175\n",
      "2024-12-11 19:50:52.685000: I runner.py:310] Step = 25200 ; steps/s = 0.73, tokens/s = 34039 (16397 source, 17642 target) ; Learning rate = 0.000557 ; Loss = 1.602629\n",
      "2024-12-11 19:53:11.416000: I runner.py:310] Step = 25300 ; steps/s = 0.72, tokens/s = 34154 (16463 source, 17691 target) ; Learning rate = 0.000556 ; Loss = 1.588351\n",
      "2024-12-11 19:55:30.083000: I runner.py:310] Step = 25400 ; steps/s = 0.72, tokens/s = 34172 (16466 source, 17706 target) ; Learning rate = 0.000555 ; Loss = 1.623898\n",
      "2024-12-11 19:57:48.781000: I runner.py:310] Step = 25500 ; steps/s = 0.72, tokens/s = 34167 (16461 source, 17706 target) ; Learning rate = 0.000553 ; Loss = 1.631777\n",
      "2024-12-11 20:00:05.986000: I runner.py:310] Step = 25600 ; steps/s = 0.73, tokens/s = 34063 (16404 source, 17659 target) ; Learning rate = 0.000552 ; Loss = 1.611374\n",
      "2024-12-11 20:02:23.935000: I runner.py:310] Step = 25700 ; steps/s = 0.72, tokens/s = 34137 (16436 source, 17701 target) ; Learning rate = 0.000551 ; Loss = 1.587982\n",
      "2024-12-11 20:04:42.606000: I runner.py:310] Step = 25800 ; steps/s = 0.72, tokens/s = 34171 (16468 source, 17703 target) ; Learning rate = 0.000550 ; Loss = 1.608171\n",
      "2024-12-11 20:07:00.970000: I runner.py:310] Step = 25900 ; steps/s = 0.72, tokens/s = 34253 (16513 source, 17740 target) ; Learning rate = 0.000549 ; Loss = 1.609124\n",
      "2024-12-11 20:09:19.338000: I runner.py:310] Step = 26000 ; steps/s = 0.72, tokens/s = 34242 (16499 source, 17743 target) ; Learning rate = 0.000548 ; Loss = 1.618660\n",
      "2024-12-11 20:11:35.381000: I runner.py:310] Step = 26100 ; steps/s = 0.74, tokens/s = 34113 (16425 source, 17688 target) ; Learning rate = 0.000547 ; Loss = 1.586697\n",
      "2024-12-11 20:13:53.932000: I runner.py:310] Step = 26200 ; steps/s = 0.72, tokens/s = 34197 (16494 source, 17703 target) ; Learning rate = 0.000546 ; Loss = 1.614986\n",
      "2024-12-11 20:16:12.608000: I runner.py:310] Step = 26300 ; steps/s = 0.72, tokens/s = 34199 (16478 source, 17721 target) ; Learning rate = 0.000545 ; Loss = 1.614111\n",
      "2024-12-11 20:18:31.363000: I runner.py:310] Step = 26400 ; steps/s = 0.72, tokens/s = 34150 (16447 source, 17703 target) ; Learning rate = 0.000544 ; Loss = 1.600224\n",
      "2024-12-11 20:20:47.788000: I runner.py:310] Step = 26500 ; steps/s = 0.73, tokens/s = 34026 (16385 source, 17641 target) ; Learning rate = 0.000543 ; Loss = 1.589991\n",
      "2024-12-11 20:23:06.558000: I runner.py:310] Step = 26600 ; steps/s = 0.72, tokens/s = 34142 (16462 source, 17680 target) ; Learning rate = 0.000542 ; Loss = 1.588991\n",
      "2024-12-11 20:25:25.237000: I runner.py:310] Step = 26700 ; steps/s = 0.72, tokens/s = 34181 (16458 source, 17723 target) ; Learning rate = 0.000541 ; Loss = 1.598390\n",
      "2024-12-11 20:27:43.884000: I runner.py:310] Step = 26800 ; steps/s = 0.72, tokens/s = 34180 (16471 source, 17709 target) ; Learning rate = 0.000540 ; Loss = 1.601096\n",
      "2024-12-11 20:30:00.350000: I runner.py:310] Step = 26900 ; steps/s = 0.73, tokens/s = 34027 (16390 source, 17637 target) ; Learning rate = 0.000539 ; Loss = 1.594510\n",
      "2024-12-11 20:32:19.021000: I runner.py:310] Step = 27000 ; steps/s = 0.72, tokens/s = 34162 (16448 source, 17714 target) ; Learning rate = 0.000538 ; Loss = 1.586802\n",
      "2024-12-11 20:34:37.750000: I runner.py:310] Step = 27100 ; steps/s = 0.72, tokens/s = 34157 (16463 source, 17694 target) ; Learning rate = 0.000537 ; Loss = 1.611341\n",
      "2024-12-11 20:36:56.482000: I runner.py:310] Step = 27200 ; steps/s = 0.72, tokens/s = 34159 (16456 source, 17703 target) ; Learning rate = 0.000536 ; Loss = 1.603799\n",
      "2024-12-11 20:39:15.159000: I runner.py:310] Step = 27300 ; steps/s = 0.72, tokens/s = 34175 (16473 source, 17702 target) ; Learning rate = 0.000535 ; Loss = 1.608817\n",
      "2024-12-11 20:41:31.568000: I runner.py:310] Step = 27400 ; steps/s = 0.73, tokens/s = 34031 (16393 source, 17638 target) ; Learning rate = 0.000534 ; Loss = 1.580132\n",
      "2024-12-11 20:43:50.308000: I runner.py:310] Step = 27500 ; steps/s = 0.72, tokens/s = 34145 (16452 source, 17693 target) ; Learning rate = 0.000533 ; Loss = 1.597947\n",
      "2024-12-11 20:46:09.029000: I runner.py:310] Step = 27600 ; steps/s = 0.72, tokens/s = 34181 (16462 source, 17719 target) ; Learning rate = 0.000532 ; Loss = 1.607158\n",
      "2024-12-11 20:48:27.714000: I runner.py:310] Step = 27700 ; steps/s = 0.72, tokens/s = 34177 (16466 source, 17711 target) ; Learning rate = 0.000531 ; Loss = 1.607980\n",
      "2024-12-11 20:50:44.176000: I runner.py:310] Step = 27800 ; steps/s = 0.73, tokens/s = 34028 (16402 source, 17626 target) ; Learning rate = 0.000530 ; Loss = 1.596547\n",
      "2024-12-11 20:53:02.890000: I runner.py:310] Step = 27900 ; steps/s = 0.72, tokens/s = 34141 (16445 source, 17696 target) ; Learning rate = 0.000529 ; Loss = 1.595415\n",
      "2024-12-11 20:55:21.583000: I runner.py:310] Step = 28000 ; steps/s = 0.72, tokens/s = 34172 (16472 source, 17700 target) ; Learning rate = 0.000528 ; Loss = 1.599087\n",
      "2024-12-11 20:57:40.251000: I runner.py:310] Step = 28100 ; steps/s = 0.72, tokens/s = 34173 (16463 source, 17710 target) ; Learning rate = 0.000527 ; Loss = 1.594261\n",
      "2024-12-11 20:59:56.701000: I runner.py:310] Step = 28200 ; steps/s = 0.73, tokens/s = 34050 (16401 source, 17649 target) ; Learning rate = 0.000526 ; Loss = 1.587219\n",
      "2024-12-11 21:02:15.407000: I runner.py:310] Step = 28300 ; steps/s = 0.72, tokens/s = 34182 (16463 source, 17719 target) ; Learning rate = 0.000525 ; Loss = 1.591228\n",
      "2024-12-11 21:04:33.935000: I runner.py:310] Step = 28400 ; steps/s = 0.72, tokens/s = 34195 (16500 source, 17695 target) ; Learning rate = 0.000524 ; Loss = 1.601261\n",
      "2024-12-11 21:06:52.297000: I runner.py:310] Step = 28500 ; steps/s = 0.72, tokens/s = 34242 (16486 source, 17756 target) ; Learning rate = 0.000524 ; Loss = 1.605052\n",
      "2024-12-11 21:09:08.321000: I runner.py:310] Step = 28600 ; steps/s = 0.74, tokens/s = 34133 (16445 source, 17688 target) ; Learning rate = 0.000523 ; Loss = 1.590000\n",
      "2024-12-11 21:11:26.703000: I runner.py:310] Step = 28700 ; steps/s = 0.72, tokens/s = 34268 (16504 source, 17764 target) ; Learning rate = 0.000522 ; Loss = 1.595645\n",
      "2024-12-11 21:13:45.387000: I runner.py:310] Step = 28800 ; steps/s = 0.72, tokens/s = 34169 (16469 source, 17700 target) ; Learning rate = 0.000521 ; Loss = 1.583851\n",
      "2024-12-11 21:16:04.064000: I runner.py:310] Step = 28900 ; steps/s = 0.72, tokens/s = 34153 (16462 source, 17691 target) ; Learning rate = 0.000520 ; Loss = 1.604375\n",
      "2024-12-11 21:18:22.749000: I runner.py:310] Step = 29000 ; steps/s = 0.72, tokens/s = 34165 (16457 source, 17708 target) ; Learning rate = 0.000519 ; Loss = 1.591602\n",
      "2024-12-11 21:20:39.212000: I runner.py:310] Step = 29100 ; steps/s = 0.73, tokens/s = 34036 (16393 source, 17643 target) ; Learning rate = 0.000518 ; Loss = 1.590263\n",
      "2024-12-11 21:22:57.890000: I runner.py:310] Step = 29200 ; steps/s = 0.72, tokens/s = 34173 (16456 source, 17717 target) ; Learning rate = 0.000517 ; Loss = 1.575760\n",
      "2024-12-11 21:25:16.585000: I runner.py:310] Step = 29300 ; steps/s = 0.72, tokens/s = 34166 (16465 source, 17701 target) ; Learning rate = 0.000516 ; Loss = 1.594181\n",
      "2024-12-11 21:27:35.274000: I runner.py:310] Step = 29400 ; steps/s = 0.72, tokens/s = 34152 (16452 source, 17700 target) ; Learning rate = 0.000515 ; Loss = 1.588718\n",
      "2024-12-11 21:29:51.734000: I runner.py:310] Step = 29500 ; steps/s = 0.73, tokens/s = 34036 (16413 source, 17623 target) ; Learning rate = 0.000515 ; Loss = 1.587512\n",
      "2024-12-11 21:32:10.420000: I runner.py:310] Step = 29600 ; steps/s = 0.72, tokens/s = 34165 (16448 source, 17717 target) ; Learning rate = 0.000514 ; Loss = 1.577631\n",
      "2024-12-11 21:34:29.161000: I runner.py:310] Step = 29700 ; steps/s = 0.72, tokens/s = 34164 (16468 source, 17696 target) ; Learning rate = 0.000513 ; Loss = 1.578052\n",
      "2024-12-11 21:36:47.866000: I runner.py:310] Step = 29800 ; steps/s = 0.72, tokens/s = 34167 (16466 source, 17701 target) ; Learning rate = 0.000512 ; Loss = 1.599967\n",
      "2024-12-11 21:39:04.284000: I runner.py:310] Step = 29900 ; steps/s = 0.73, tokens/s = 34027 (16380 source, 17647 target) ; Learning rate = 0.000511 ; Loss = 1.571928\n",
      "2024-12-11 21:41:22.988000: I runner.py:310] Step = 30000 ; steps/s = 0.72, tokens/s = 34158 (16460 source, 17698 target) ; Learning rate = 0.000510 ; Loss = 1.589256\n",
      "2024-12-11 21:41:24.997000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-30000\n",
      "2024-12-11 21:43:43.754000: I runner.py:310] Step = 30100 ; steps/s = 0.72, tokens/s = 34145 (16437 source, 17708 target) ; Learning rate = 0.000509 ; Loss = 1.583253\n",
      "2024-12-11 21:46:02.472000: I runner.py:310] Step = 30200 ; steps/s = 0.72, tokens/s = 34156 (16469 source, 17687 target) ; Learning rate = 0.000509 ; Loss = 1.593159\n",
      "2024-12-11 21:48:18.954000: I runner.py:310] Step = 30300 ; steps/s = 0.73, tokens/s = 34037 (16407 source, 17630 target) ; Learning rate = 0.000508 ; Loss = 1.590497\n",
      "2024-12-11 21:50:37.702000: I runner.py:310] Step = 30400 ; steps/s = 0.72, tokens/s = 34151 (16452 source, 17699 target) ; Learning rate = 0.000507 ; Loss = 1.567871\n",
      "2024-12-11 21:52:56.393000: I runner.py:310] Step = 30500 ; steps/s = 0.72, tokens/s = 34173 (16467 source, 17706 target) ; Learning rate = 0.000506 ; Loss = 1.579556\n",
      "2024-12-11 21:55:15.088000: I runner.py:310] Step = 30600 ; steps/s = 0.72, tokens/s = 34171 (16467 source, 17704 target) ; Learning rate = 0.000505 ; Loss = 1.593002\n",
      "2024-12-11 21:57:33.847000: I runner.py:310] Step = 30700 ; steps/s = 0.72, tokens/s = 34148 (16447 source, 17701 target) ; Learning rate = 0.000504 ; Loss = 1.604457\n",
      "2024-12-11 21:59:50.288000: I runner.py:310] Step = 30800 ; steps/s = 0.73, tokens/s = 34026 (16391 source, 17635 target) ; Learning rate = 0.000504 ; Loss = 1.591012\n",
      "2024-12-11 22:02:08.990000: I runner.py:310] Step = 30900 ; steps/s = 0.72, tokens/s = 34182 (16458 source, 17724 target) ; Learning rate = 0.000503 ; Loss = 1.574797\n",
      "2024-12-11 22:04:27.404000: I runner.py:310] Step = 31000 ; steps/s = 0.72, tokens/s = 34238 (16511 source, 17727 target) ; Learning rate = 0.000502 ; Loss = 1.577335\n",
      "2024-12-11 22:06:45.770000: I runner.py:310] Step = 31100 ; steps/s = 0.72, tokens/s = 34250 (16493 source, 17757 target) ; Learning rate = 0.000501 ; Loss = 1.585780\n",
      "2024-12-11 22:09:01.794000: I runner.py:310] Step = 31200 ; steps/s = 0.74, tokens/s = 34125 (16449 source, 17676 target) ; Learning rate = 0.000500 ; Loss = 1.581930\n",
      "2024-12-11 22:11:20.448000: I runner.py:310] Step = 31300 ; steps/s = 0.72, tokens/s = 34166 (16444 source, 17722 target) ; Learning rate = 0.000500 ; Loss = 1.571552\n",
      "2024-12-11 22:13:39.193000: I runner.py:310] Step = 31400 ; steps/s = 0.72, tokens/s = 34166 (16465 source, 17701 target) ; Learning rate = 0.000499 ; Loss = 1.589538\n",
      "2024-12-11 22:15:57.900000: I runner.py:310] Step = 31500 ; steps/s = 0.72, tokens/s = 34175 (16474 source, 17701 target) ; Learning rate = 0.000498 ; Loss = 1.604392\n",
      "2024-12-11 22:18:14.365000: I runner.py:310] Step = 31600 ; steps/s = 0.73, tokens/s = 34015 (16393 source, 17622 target) ; Learning rate = 0.000497 ; Loss = 1.575157\n",
      "2024-12-11 22:20:33.087000: I runner.py:310] Step = 31700 ; steps/s = 0.72, tokens/s = 34162 (16461 source, 17701 target) ; Learning rate = 0.000496 ; Loss = 1.574069\n",
      "2024-12-11 22:22:51.816000: I runner.py:310] Step = 31800 ; steps/s = 0.72, tokens/s = 34151 (16465 source, 17686 target) ; Learning rate = 0.000496 ; Loss = 1.576748\n",
      "2024-12-11 22:25:10.500000: I runner.py:310] Step = 31900 ; steps/s = 0.72, tokens/s = 34178 (16450 source, 17728 target) ; Learning rate = 0.000495 ; Loss = 1.576548\n",
      "2024-12-11 22:27:27.834000: I runner.py:310] Step = 32000 ; steps/s = 0.73, tokens/s = 34077 (16416 source, 17661 target) ; Learning rate = 0.000494 ; Loss = 1.574005\n",
      "2024-12-11 22:29:45.604000: I runner.py:310] Step = 32100 ; steps/s = 0.73, tokens/s = 34129 (16452 source, 17677 target) ; Learning rate = 0.000493 ; Loss = 1.557393\n",
      "2024-12-11 22:32:04.323000: I runner.py:310] Step = 32200 ; steps/s = 0.72, tokens/s = 34156 (16457 source, 17699 target) ; Learning rate = 0.000493 ; Loss = 1.576622\n",
      "2024-12-11 22:34:23.028000: I runner.py:310] Step = 32300 ; steps/s = 0.72, tokens/s = 34155 (16454 source, 17701 target) ; Learning rate = 0.000492 ; Loss = 1.583359\n",
      "2024-12-11 22:36:41.742000: I runner.py:310] Step = 32400 ; steps/s = 0.72, tokens/s = 34172 (16460 source, 17712 target) ; Learning rate = 0.000491 ; Loss = 1.589161\n",
      "2024-12-11 22:38:58.181000: I runner.py:310] Step = 32500 ; steps/s = 0.73, tokens/s = 34037 (16392 source, 17645 target) ; Learning rate = 0.000490 ; Loss = 1.559883\n",
      "2024-12-11 22:41:16.916000: I runner.py:310] Step = 32600 ; steps/s = 0.72, tokens/s = 34156 (16457 source, 17699 target) ; Learning rate = 0.000490 ; Loss = 1.577381\n",
      "2024-12-11 22:43:35.599000: I runner.py:310] Step = 32700 ; steps/s = 0.72, tokens/s = 34169 (16465 source, 17704 target) ; Learning rate = 0.000489 ; Loss = 1.569776\n",
      "2024-12-11 22:45:54.320000: I runner.py:310] Step = 32800 ; steps/s = 0.72, tokens/s = 34163 (16469 source, 17694 target) ; Learning rate = 0.000488 ; Loss = 1.576077\n",
      "2024-12-11 22:48:10.790000: I runner.py:310] Step = 32900 ; steps/s = 0.73, tokens/s = 34016 (16382 source, 17634 target) ; Learning rate = 0.000487 ; Loss = 1.574670\n",
      "2024-12-11 22:50:29.508000: I runner.py:310] Step = 33000 ; steps/s = 0.72, tokens/s = 34160 (16457 source, 17703 target) ; Learning rate = 0.000487 ; Loss = 1.561536\n",
      "2024-12-11 22:52:48.235000: I runner.py:310] Step = 33100 ; steps/s = 0.72, tokens/s = 34146 (16449 source, 17697 target) ; Learning rate = 0.000486 ; Loss = 1.554548\n",
      "2024-12-11 22:55:06.963000: I runner.py:310] Step = 33200 ; steps/s = 0.72, tokens/s = 34175 (16477 source, 17698 target) ; Learning rate = 0.000485 ; Loss = 1.563386\n",
      "2024-12-11 22:57:23.393000: I runner.py:310] Step = 33300 ; steps/s = 0.73, tokens/s = 34035 (16395 source, 17640 target) ; Learning rate = 0.000484 ; Loss = 1.580369\n",
      "2024-12-11 22:59:42.100000: I runner.py:310] Step = 33400 ; steps/s = 0.72, tokens/s = 34168 (16449 source, 17719 target) ; Learning rate = 0.000484 ; Loss = 1.562625\n",
      "2024-12-11 23:02:00.574000: I runner.py:310] Step = 33500 ; steps/s = 0.72, tokens/s = 34213 (16483 source, 17730 target) ; Learning rate = 0.000483 ; Loss = 1.568228\n",
      "2024-12-11 23:04:18.912000: I runner.py:310] Step = 33600 ; steps/s = 0.72, tokens/s = 34266 (16511 source, 17755 target) ; Learning rate = 0.000482 ; Loss = 1.570735\n",
      "2024-12-11 23:06:37.275000: I runner.py:310] Step = 33700 ; steps/s = 0.72, tokens/s = 34238 (16505 source, 17733 target) ; Learning rate = 0.000481 ; Loss = 1.559609\n",
      "2024-12-11 23:08:53.558000: I runner.py:310] Step = 33800 ; steps/s = 0.73, tokens/s = 34088 (16422 source, 17666 target) ; Learning rate = 0.000481 ; Loss = 1.552318\n",
      "2024-12-11 23:11:12.337000: I runner.py:310] Step = 33900 ; steps/s = 0.72, tokens/s = 34134 (16454 source, 17680 target) ; Learning rate = 0.000480 ; Loss = 1.571663\n",
      "2024-12-11 23:13:31.068000: I runner.py:310] Step = 34000 ; steps/s = 0.72, tokens/s = 34162 (16462 source, 17700 target) ; Learning rate = 0.000479 ; Loss = 1.583108\n",
      "2024-12-11 23:15:49.746000: I runner.py:310] Step = 34100 ; steps/s = 0.72, tokens/s = 34160 (16455 source, 17705 target) ; Learning rate = 0.000479 ; Loss = 1.577378\n",
      "2024-12-11 23:18:06.154000: I runner.py:310] Step = 34200 ; steps/s = 0.73, tokens/s = 34050 (16403 source, 17647 target) ; Learning rate = 0.000478 ; Loss = 1.575738\n",
      "2024-12-11 23:20:24.867000: I runner.py:310] Step = 34300 ; steps/s = 0.72, tokens/s = 34157 (16456 source, 17701 target) ; Learning rate = 0.000477 ; Loss = 1.556352\n",
      "2024-12-11 23:22:43.638000: I runner.py:310] Step = 34400 ; steps/s = 0.72, tokens/s = 34159 (16463 source, 17696 target) ; Learning rate = 0.000477 ; Loss = 1.569855\n",
      "2024-12-11 23:25:02.346000: I runner.py:310] Step = 34500 ; steps/s = 0.72, tokens/s = 34160 (16459 source, 17701 target) ; Learning rate = 0.000476 ; Loss = 1.569572\n",
      "2024-12-11 23:27:18.785000: I runner.py:310] Step = 34600 ; steps/s = 0.73, tokens/s = 34033 (16393 source, 17640 target) ; Learning rate = 0.000475 ; Loss = 1.557134\n",
      "2024-12-11 23:29:37.559000: I runner.py:310] Step = 34700 ; steps/s = 0.72, tokens/s = 34134 (16444 source, 17690 target) ; Learning rate = 0.000474 ; Loss = 1.565255\n",
      "2024-12-11 23:31:56.278000: I runner.py:310] Step = 34800 ; steps/s = 0.72, tokens/s = 34151 (16444 source, 17707 target) ; Learning rate = 0.000474 ; Loss = 1.572454\n",
      "2024-12-11 23:34:14.964000: I runner.py:310] Step = 34900 ; steps/s = 0.72, tokens/s = 34184 (16478 source, 17706 target) ; Learning rate = 0.000473 ; Loss = 1.570060\n",
      "2024-12-11 23:36:31.435000: I runner.py:310] Step = 35000 ; steps/s = 0.73, tokens/s = 34027 (16392 source, 17635 target) ; Learning rate = 0.000472 ; Loss = 1.556780\n",
      "2024-12-11 23:38:50.107000: I runner.py:310] Step = 35100 ; steps/s = 0.72, tokens/s = 34198 (16477 source, 17721 target) ; Learning rate = 0.000472 ; Loss = 1.555882\n",
      "2024-12-11 23:41:08.806000: I runner.py:310] Step = 35200 ; steps/s = 0.72, tokens/s = 34152 (16455 source, 17697 target) ; Learning rate = 0.000471 ; Loss = 1.560746\n",
      "2024-12-11 23:43:27.501000: I runner.py:310] Step = 35300 ; steps/s = 0.72, tokens/s = 34157 (16466 source, 17691 target) ; Learning rate = 0.000470 ; Loss = 1.573962\n",
      "2024-12-11 23:45:46.238000: I runner.py:310] Step = 35400 ; steps/s = 0.72, tokens/s = 34158 (16450 source, 17708 target) ; Learning rate = 0.000470 ; Loss = 1.567810\n",
      "2024-12-11 23:48:02.668000: I runner.py:310] Step = 35500 ; steps/s = 0.73, tokens/s = 34037 (16392 source, 17645 target) ; Learning rate = 0.000469 ; Loss = 1.555622\n",
      "2024-12-11 23:50:21.388000: I runner.py:310] Step = 35600 ; steps/s = 0.72, tokens/s = 34143 (16454 source, 17689 target) ; Learning rate = 0.000468 ; Loss = 1.563471\n",
      "2024-12-11 23:52:40.103000: I runner.py:310] Step = 35700 ; steps/s = 0.72, tokens/s = 34167 (16471 source, 17696 target) ; Learning rate = 0.000468 ; Loss = 1.573185\n",
      "2024-12-11 23:54:58.859000: I runner.py:310] Step = 35800 ; steps/s = 0.72, tokens/s = 34154 (16453 source, 17701 target) ; Learning rate = 0.000467 ; Loss = 1.568568\n",
      "2024-12-11 23:57:15.337000: I runner.py:310] Step = 35900 ; steps/s = 0.73, tokens/s = 34025 (16387 source, 17638 target) ; Learning rate = 0.000466 ; Loss = 1.551034\n",
      "2024-12-11 23:59:33.874000: I runner.py:310] Step = 36000 ; steps/s = 0.72, tokens/s = 34212 (16480 source, 17732 target) ; Learning rate = 0.000466 ; Loss = 1.551317\n",
      "2024-12-12 00:01:52.218000: I runner.py:310] Step = 36100 ; steps/s = 0.72, tokens/s = 34249 (16504 source, 17745 target) ; Learning rate = 0.000465 ; Loss = 1.573693\n",
      "2024-12-12 00:04:10.570000: I runner.py:310] Step = 36200 ; steps/s = 0.72, tokens/s = 34241 (16501 source, 17740 target) ; Learning rate = 0.000465 ; Loss = 1.563925\n",
      "2024-12-12 00:06:26.844000: I runner.py:310] Step = 36300 ; steps/s = 0.73, tokens/s = 34090 (16425 source, 17665 target) ; Learning rate = 0.000464 ; Loss = 1.551920\n",
      "2024-12-12 00:08:45.541000: I runner.py:310] Step = 36400 ; steps/s = 0.72, tokens/s = 34171 (16461 source, 17710 target) ; Learning rate = 0.000463 ; Loss = 1.554202\n",
      "2024-12-12 00:11:04.224000: I runner.py:310] Step = 36500 ; steps/s = 0.72, tokens/s = 34167 (16469 source, 17698 target) ; Learning rate = 0.000463 ; Loss = 1.559049\n",
      "2024-12-12 00:13:22.916000: I runner.py:310] Step = 36600 ; steps/s = 0.72, tokens/s = 34171 (16468 source, 17703 target) ; Learning rate = 0.000462 ; Loss = 1.553669\n",
      "2024-12-12 00:15:39.338000: I runner.py:310] Step = 36700 ; steps/s = 0.73, tokens/s = 34025 (16386 source, 17639 target) ; Learning rate = 0.000461 ; Loss = 1.551739\n",
      "2024-12-12 00:17:58.032000: I runner.py:310] Step = 36800 ; steps/s = 0.72, tokens/s = 34174 (16451 source, 17723 target) ; Learning rate = 0.000461 ; Loss = 1.560222\n",
      "2024-12-12 00:20:16.791000: I runner.py:310] Step = 36900 ; steps/s = 0.72, tokens/s = 34149 (16456 source, 17693 target) ; Learning rate = 0.000460 ; Loss = 1.558390\n",
      "2024-12-12 00:22:35.490000: I runner.py:310] Step = 37000 ; steps/s = 0.72, tokens/s = 34158 (16450 source, 17708 target) ; Learning rate = 0.000460 ; Loss = 1.547780\n",
      "2024-12-12 00:24:54.212000: I runner.py:310] Step = 37100 ; steps/s = 0.72, tokens/s = 34162 (16472 source, 17690 target) ; Learning rate = 0.000459 ; Loss = 1.561951\n",
      "2024-12-12 00:27:10.737000: I runner.py:310] Step = 37200 ; steps/s = 0.73, tokens/s = 34008 (16387 source, 17621 target) ; Learning rate = 0.000458 ; Loss = 1.561866\n",
      "2024-12-12 00:29:29.474000: I runner.py:310] Step = 37300 ; steps/s = 0.72, tokens/s = 34165 (16453 source, 17712 target) ; Learning rate = 0.000458 ; Loss = 1.548530\n",
      "2024-12-12 00:31:48.180000: I runner.py:310] Step = 37400 ; steps/s = 0.72, tokens/s = 34156 (16461 source, 17695 target) ; Learning rate = 0.000457 ; Loss = 1.555833\n",
      "2024-12-12 00:34:06.892000: I runner.py:310] Step = 37500 ; steps/s = 0.72, tokens/s = 34172 (16461 source, 17711 target) ; Learning rate = 0.000456 ; Loss = 1.555741\n",
      "2024-12-12 00:36:23.353000: I runner.py:310] Step = 37600 ; steps/s = 0.73, tokens/s = 34023 (16398 source, 17625 target) ; Learning rate = 0.000456 ; Loss = 1.535541\n",
      "2024-12-12 00:38:42.054000: I runner.py:310] Step = 37700 ; steps/s = 0.72, tokens/s = 34174 (16465 source, 17709 target) ; Learning rate = 0.000455 ; Loss = 1.554335\n",
      "2024-12-12 00:41:00.805000: I runner.py:310] Step = 37800 ; steps/s = 0.72, tokens/s = 34151 (16459 source, 17692 target) ; Learning rate = 0.000455 ; Loss = 1.562918\n",
      "2024-12-12 00:43:19.496000: I runner.py:310] Step = 37900 ; steps/s = 0.72, tokens/s = 34166 (16461 source, 17705 target) ; Learning rate = 0.000454 ; Loss = 1.562579\n",
      "2024-12-12 00:45:35.963000: I runner.py:310] Step = 38000 ; steps/s = 0.73, tokens/s = 34028 (16388 source, 17640 target) ; Learning rate = 0.000453 ; Loss = 1.557661\n",
      "2024-12-12 00:47:54.637000: I runner.py:310] Step = 38100 ; steps/s = 0.72, tokens/s = 34180 (16477 source, 17703 target) ; Learning rate = 0.000453 ; Loss = 1.555522\n",
      "2024-12-12 00:50:13.328000: I runner.py:310] Step = 38200 ; steps/s = 0.72, tokens/s = 34161 (16457 source, 17704 target) ; Learning rate = 0.000452 ; Loss = 1.553210\n",
      "2024-12-12 00:52:32.045000: I runner.py:310] Step = 38300 ; steps/s = 0.72, tokens/s = 34159 (16462 source, 17697 target) ; Learning rate = 0.000452 ; Loss = 1.555665\n",
      "2024-12-12 00:54:49.667000: I runner.py:310] Step = 38400 ; steps/s = 0.73, tokens/s = 34094 (16416 source, 17678 target) ; Learning rate = 0.000451 ; Loss = 1.558835\n",
      "2024-12-12 00:57:07.042000: I runner.py:310] Step = 38500 ; steps/s = 0.73, tokens/s = 34147 (16453 source, 17694 target) ; Learning rate = 0.000450 ; Loss = 1.539134\n",
      "2024-12-12 00:59:25.358000: I runner.py:310] Step = 38600 ; steps/s = 0.72, tokens/s = 34266 (16518 source, 17748 target) ; Learning rate = 0.000450 ; Loss = 1.553527\n",
      "2024-12-12 01:01:43.727000: I runner.py:310] Step = 38700 ; steps/s = 0.72, tokens/s = 34249 (16507 source, 17742 target) ; Learning rate = 0.000449 ; Loss = 1.553816\n",
      "2024-12-12 01:04:02.149000: I runner.py:310] Step = 38800 ; steps/s = 0.72, tokens/s = 34222 (16479 source, 17743 target) ; Learning rate = 0.000449 ; Loss = 1.561173\n",
      "2024-12-12 01:06:18.542000: I runner.py:310] Step = 38900 ; steps/s = 0.73, tokens/s = 34040 (16380 source, 17660 target) ; Learning rate = 0.000448 ; Loss = 1.565211\n",
      "2024-12-12 01:08:37.286000: I runner.py:310] Step = 39000 ; steps/s = 0.72, tokens/s = 34148 (16468 source, 17680 target) ; Learning rate = 0.000448 ; Loss = 1.551408\n",
      "2024-12-12 01:10:55.977000: I runner.py:310] Step = 39100 ; steps/s = 0.72, tokens/s = 34171 (16462 source, 17709 target) ; Learning rate = 0.000447 ; Loss = 1.550518\n",
      "2024-12-12 01:13:14.705000: I runner.py:310] Step = 39200 ; steps/s = 0.72, tokens/s = 34179 (16467 source, 17712 target) ; Learning rate = 0.000446 ; Loss = 1.545123\n",
      "2024-12-12 01:15:31.204000: I runner.py:310] Step = 39300 ; steps/s = 0.73, tokens/s = 34004 (16388 source, 17616 target) ; Learning rate = 0.000446 ; Loss = 1.541969\n",
      "2024-12-12 01:17:49.934000: I runner.py:310] Step = 39400 ; steps/s = 0.72, tokens/s = 34156 (16448 source, 17708 target) ; Learning rate = 0.000445 ; Loss = 1.545272\n",
      "2024-12-12 01:20:08.645000: I runner.py:310] Step = 39500 ; steps/s = 0.72, tokens/s = 34164 (16471 source, 17693 target) ; Learning rate = 0.000445 ; Loss = 1.549356\n",
      "2024-12-12 01:22:27.329000: I runner.py:310] Step = 39600 ; steps/s = 0.72, tokens/s = 34168 (16459 source, 17709 target) ; Learning rate = 0.000444 ; Loss = 1.548303\n",
      "2024-12-12 01:24:43.747000: I runner.py:310] Step = 39700 ; steps/s = 0.73, tokens/s = 34037 (16396 source, 17641 target) ; Learning rate = 0.000444 ; Loss = 1.549548\n",
      "2024-12-12 01:27:02.496000: I runner.py:310] Step = 39800 ; steps/s = 0.72, tokens/s = 34152 (16444 source, 17708 target) ; Learning rate = 0.000443 ; Loss = 1.556457\n",
      "2024-12-12 01:29:21.207000: I runner.py:310] Step = 39900 ; steps/s = 0.72, tokens/s = 34168 (16465 source, 17703 target) ; Learning rate = 0.000442 ; Loss = 1.535965\n",
      "2024-12-12 01:31:39.880000: I runner.py:310] Step = 40000 ; steps/s = 0.72, tokens/s = 34165 (16457 source, 17708 target) ; Learning rate = 0.000442 ; Loss = 1.543372\n",
      "2024-12-12 01:31:41.937000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-40000\n",
      "2024-12-12 01:34:00.641000: I runner.py:310] Step = 40100 ; steps/s = 0.72, tokens/s = 34167 (16471 source, 17696 target) ; Learning rate = 0.000441 ; Loss = 1.559274\n",
      "2024-12-12 01:36:17.066000: I runner.py:310] Step = 40200 ; steps/s = 0.73, tokens/s = 34029 (16393 source, 17636 target) ; Learning rate = 0.000441 ; Loss = 1.551070\n",
      "2024-12-12 01:38:35.741000: I runner.py:310] Step = 40300 ; steps/s = 0.72, tokens/s = 34171 (16465 source, 17706 target) ; Learning rate = 0.000440 ; Loss = 1.543116\n",
      "2024-12-12 01:40:54.506000: I runner.py:310] Step = 40400 ; steps/s = 0.72, tokens/s = 34150 (16448 source, 17702 target) ; Learning rate = 0.000440 ; Loss = 1.555304\n",
      "2024-12-12 01:43:13.230000: I runner.py:310] Step = 40500 ; steps/s = 0.72, tokens/s = 34168 (16463 source, 17705 target) ; Learning rate = 0.000439 ; Loss = 1.550879\n",
      "2024-12-12 01:45:29.675000: I runner.py:310] Step = 40600 ; steps/s = 0.73, tokens/s = 34038 (16405 source, 17633 target) ; Learning rate = 0.000439 ; Loss = 1.543164\n",
      "2024-12-12 01:47:48.346000: I runner.py:310] Step = 40700 ; steps/s = 0.72, tokens/s = 34186 (16456 source, 17730 target) ; Learning rate = 0.000438 ; Loss = 1.538201\n",
      "2024-12-12 01:50:07.040000: I runner.py:310] Step = 40800 ; steps/s = 0.72, tokens/s = 34147 (16459 source, 17688 target) ; Learning rate = 0.000438 ; Loss = 1.545347\n",
      "2024-12-12 01:52:25.759000: I runner.py:310] Step = 40900 ; steps/s = 0.72, tokens/s = 34165 (16463 source, 17702 target) ; Learning rate = 0.000437 ; Loss = 1.549273\n",
      "2024-12-12 01:54:42.116000: I runner.py:310] Step = 41000 ; steps/s = 0.73, tokens/s = 34043 (16403 source, 17640 target) ; Learning rate = 0.000437 ; Loss = 1.534529\n",
      "2024-12-12 01:57:00.462000: I runner.py:310] Step = 41100 ; steps/s = 0.72, tokens/s = 34247 (16495 source, 17752 target) ; Learning rate = 0.000436 ; Loss = 1.543227\n",
      "2024-12-12 01:59:18.785000: I runner.py:310] Step = 41200 ; steps/s = 0.72, tokens/s = 34271 (16526 source, 17745 target) ; Learning rate = 0.000435 ; Loss = 1.560482\n",
      "2024-12-12 02:01:37.224000: I runner.py:310] Step = 41300 ; steps/s = 0.72, tokens/s = 34229 (16488 source, 17741 target) ; Learning rate = 0.000435 ; Loss = 1.550398\n",
      "2024-12-12 02:03:53.720000: I runner.py:310] Step = 41400 ; steps/s = 0.73, tokens/s = 34018 (16384 source, 17634 target) ; Learning rate = 0.000434 ; Loss = 1.546332\n",
      "2024-12-12 02:06:12.421000: I runner.py:310] Step = 41500 ; steps/s = 0.72, tokens/s = 34161 (16456 source, 17705 target) ; Learning rate = 0.000434 ; Loss = 1.544616\n",
      "2024-12-12 02:08:31.093000: I runner.py:310] Step = 41600 ; steps/s = 0.72, tokens/s = 34194 (16478 source, 17716 target) ; Learning rate = 0.000433 ; Loss = 1.536608\n",
      "2024-12-12 02:10:49.791000: I runner.py:310] Step = 41700 ; steps/s = 0.72, tokens/s = 34161 (16458 source, 17703 target) ; Learning rate = 0.000433 ; Loss = 1.548490\n",
      "2024-12-12 02:13:08.498000: I runner.py:310] Step = 41800 ; steps/s = 0.72, tokens/s = 34154 (16458 source, 17696 target) ; Learning rate = 0.000432 ; Loss = 1.551299\n",
      "2024-12-12 02:15:24.955000: I runner.py:310] Step = 41900 ; steps/s = 0.73, tokens/s = 34036 (16395 source, 17641 target) ; Learning rate = 0.000432 ; Loss = 1.528683\n",
      "2024-12-12 02:17:43.683000: I runner.py:310] Step = 42000 ; steps/s = 0.72, tokens/s = 34164 (16460 source, 17704 target) ; Learning rate = 0.000431 ; Loss = 1.555204\n",
      "2024-12-12 02:20:02.416000: I runner.py:310] Step = 42100 ; steps/s = 0.72, tokens/s = 34158 (16464 source, 17694 target) ; Learning rate = 0.000431 ; Loss = 1.543011\n",
      "2024-12-12 02:22:21.097000: I runner.py:310] Step = 42200 ; steps/s = 0.72, tokens/s = 34161 (16464 source, 17697 target) ; Learning rate = 0.000430 ; Loss = 1.551495\n",
      "2024-12-12 02:24:37.566000: I runner.py:310] Step = 42300 ; steps/s = 0.73, tokens/s = 34018 (16374 source, 17644 target) ; Learning rate = 0.000430 ; Loss = 1.544577\n",
      "2024-12-12 02:26:56.281000: I runner.py:310] Step = 42400 ; steps/s = 0.72, tokens/s = 34167 (16460 source, 17707 target) ; Learning rate = 0.000429 ; Loss = 1.539559\n",
      "2024-12-12 02:29:14.983000: I runner.py:310] Step = 42500 ; steps/s = 0.72, tokens/s = 34160 (16461 source, 17699 target) ; Learning rate = 0.000429 ; Loss = 1.537275\n",
      "2024-12-12 02:31:33.693000: I runner.py:310] Step = 42600 ; steps/s = 0.72, tokens/s = 34155 (16459 source, 17696 target) ; Learning rate = 0.000428 ; Loss = 1.540425\n",
      "2024-12-12 02:33:50.119000: I runner.py:310] Step = 42700 ; steps/s = 0.73, tokens/s = 34051 (16397 source, 17654 target) ; Learning rate = 0.000428 ; Loss = 1.547282\n",
      "2024-12-12 02:36:08.820000: I runner.py:310] Step = 42800 ; steps/s = 0.72, tokens/s = 34151 (16460 source, 17691 target) ; Learning rate = 0.000427 ; Loss = 1.543597\n",
      "2024-12-12 02:38:27.497000: I runner.py:310] Step = 42900 ; steps/s = 0.72, tokens/s = 34177 (16459 source, 17718 target) ; Learning rate = 0.000427 ; Loss = 1.536596\n",
      "2024-12-12 02:40:46.240000: I runner.py:310] Step = 43000 ; steps/s = 0.72, tokens/s = 34165 (16464 source, 17701 target) ; Learning rate = 0.000426 ; Loss = 1.542680\n",
      "2024-12-12 02:43:02.660000: I runner.py:310] Step = 43100 ; steps/s = 0.73, tokens/s = 34033 (16409 source, 17624 target) ; Learning rate = 0.000426 ; Loss = 1.536937\n",
      "2024-12-12 02:45:21.374000: I runner.py:310] Step = 43200 ; steps/s = 0.72, tokens/s = 34170 (16461 source, 17709 target) ; Learning rate = 0.000425 ; Loss = 1.531717\n",
      "2024-12-12 02:47:40.096000: I runner.py:310] Step = 43300 ; steps/s = 0.72, tokens/s = 34146 (16450 source, 17696 target) ; Learning rate = 0.000425 ; Loss = 1.545075\n",
      "2024-12-12 02:49:58.819000: I runner.py:310] Step = 43400 ; steps/s = 0.72, tokens/s = 34153 (16469 source, 17684 target) ; Learning rate = 0.000424 ; Loss = 1.543817\n",
      "2024-12-12 02:52:17.455000: I runner.py:310] Step = 43500 ; steps/s = 0.72, tokens/s = 34185 (16460 source, 17725 target) ; Learning rate = 0.000424 ; Loss = 1.556899\n",
      "2024-12-12 02:54:33.494000: I runner.py:310] Step = 43600 ; steps/s = 0.74, tokens/s = 34146 (16457 source, 17689 target) ; Learning rate = 0.000423 ; Loss = 1.550567\n",
      "2024-12-12 02:56:51.843000: I runner.py:310] Step = 43700 ; steps/s = 0.72, tokens/s = 34255 (16508 source, 17747 target) ; Learning rate = 0.000423 ; Loss = 1.541275\n",
      "2024-12-12 02:59:10.284000: I runner.py:310] Step = 43800 ; steps/s = 0.72, tokens/s = 34223 (16490 source, 17733 target) ; Learning rate = 0.000422 ; Loss = 1.536940\n",
      "2024-12-12 03:01:29.024000: I runner.py:310] Step = 43900 ; steps/s = 0.72, tokens/s = 34155 (16449 source, 17706 target) ; Learning rate = 0.000422 ; Loss = 1.541033\n",
      "2024-12-12 03:03:45.450000: I runner.py:310] Step = 44000 ; steps/s = 0.73, tokens/s = 34034 (16394 source, 17640 target) ; Learning rate = 0.000421 ; Loss = 1.544453\n",
      "2024-12-12 03:06:04.154000: I runner.py:310] Step = 44100 ; steps/s = 0.72, tokens/s = 34172 (16468 source, 17704 target) ; Learning rate = 0.000421 ; Loss = 1.538420\n",
      "2024-12-12 03:08:22.931000: I runner.py:310] Step = 44200 ; steps/s = 0.72, tokens/s = 34144 (16446 source, 17698 target) ; Learning rate = 0.000420 ; Loss = 1.534871\n",
      "2024-12-12 03:10:41.651000: I runner.py:310] Step = 44300 ; steps/s = 0.72, tokens/s = 34163 (16450 source, 17713 target) ; Learning rate = 0.000420 ; Loss = 1.539453\n",
      "2024-12-12 03:12:58.141000: I runner.py:310] Step = 44400 ; steps/s = 0.73, tokens/s = 34009 (16393 source, 17616 target) ; Learning rate = 0.000419 ; Loss = 1.536643\n",
      "2024-12-12 03:15:16.855000: I runner.py:310] Step = 44500 ; steps/s = 0.72, tokens/s = 34183 (16466 source, 17717 target) ; Learning rate = 0.000419 ; Loss = 1.531309\n",
      "2024-12-12 03:17:35.555000: I runner.py:310] Step = 44600 ; steps/s = 0.72, tokens/s = 34144 (16450 source, 17694 target) ; Learning rate = 0.000419 ; Loss = 1.541031\n",
      "2024-12-12 03:19:54.304000: I runner.py:310] Step = 44700 ; steps/s = 0.72, tokens/s = 34170 (16468 source, 17702 target) ; Learning rate = 0.000418 ; Loss = 1.542696\n",
      "2024-12-12 03:22:12.307000: I runner.py:310] Step = 44800 ; steps/s = 0.72, tokens/s = 34092 (16424 source, 17668 target) ; Learning rate = 0.000418 ; Loss = 1.550730\n",
      "2024-12-12 03:24:29.469000: I runner.py:310] Step = 44900 ; steps/s = 0.73, tokens/s = 34086 (16419 source, 17667 target) ; Learning rate = 0.000417 ; Loss = 1.541460\n",
      "2024-12-12 03:26:48.138000: I runner.py:310] Step = 45000 ; steps/s = 0.72, tokens/s = 34182 (16471 source, 17711 target) ; Learning rate = 0.000417 ; Loss = 1.534125\n",
      "2024-12-12 03:29:06.879000: I runner.py:310] Step = 45100 ; steps/s = 0.72, tokens/s = 34162 (16461 source, 17701 target) ; Learning rate = 0.000416 ; Loss = 1.534576\n",
      "2024-12-12 03:31:25.646000: I runner.py:310] Step = 45200 ; steps/s = 0.72, tokens/s = 34144 (16456 source, 17688 target) ; Learning rate = 0.000416 ; Loss = 1.544214\n",
      "2024-12-12 03:33:42.111000: I runner.py:310] Step = 45300 ; steps/s = 0.73, tokens/s = 34007 (16372 source, 17635 target) ; Learning rate = 0.000415 ; Loss = 1.544611\n",
      "2024-12-12 03:36:00.800000: I runner.py:310] Step = 45400 ; steps/s = 0.72, tokens/s = 34176 (16463 source, 17713 target) ; Learning rate = 0.000415 ; Loss = 1.537443\n",
      "2024-12-12 03:38:19.477000: I runner.py:310] Step = 45500 ; steps/s = 0.72, tokens/s = 34176 (16480 source, 17696 target) ; Learning rate = 0.000414 ; Loss = 1.537106\n",
      "2024-12-12 03:40:38.171000: I runner.py:310] Step = 45600 ; steps/s = 0.72, tokens/s = 34165 (16457 source, 17708 target) ; Learning rate = 0.000414 ; Loss = 1.534174\n",
      "2024-12-12 03:42:54.617000: I runner.py:310] Step = 45700 ; steps/s = 0.73, tokens/s = 34043 (16397 source, 17646 target) ; Learning rate = 0.000413 ; Loss = 1.544024\n",
      "2024-12-12 03:45:13.358000: I runner.py:310] Step = 45800 ; steps/s = 0.72, tokens/s = 34159 (16462 source, 17697 target) ; Learning rate = 0.000413 ; Loss = 1.532777\n",
      "2024-12-12 03:47:32.068000: I runner.py:310] Step = 45900 ; steps/s = 0.72, tokens/s = 34157 (16454 source, 17703 target) ; Learning rate = 0.000413 ; Loss = 1.524085\n",
      "2024-12-12 03:49:50.758000: I runner.py:310] Step = 46000 ; steps/s = 0.72, tokens/s = 34171 (16460 source, 17711 target) ; Learning rate = 0.000412 ; Loss = 1.530430\n",
      "2024-12-12 03:52:06.748000: I runner.py:310] Step = 46100 ; steps/s = 0.74, tokens/s = 34135 (16450 source, 17685 target) ; Learning rate = 0.000412 ; Loss = 1.533899\n",
      "2024-12-12 03:54:25.051000: I runner.py:310] Step = 46200 ; steps/s = 0.72, tokens/s = 34270 (16518 source, 17752 target) ; Learning rate = 0.000411 ; Loss = 1.537580\n",
      "2024-12-12 03:56:43.449000: I runner.py:310] Step = 46300 ; steps/s = 0.72, tokens/s = 34256 (16501 source, 17755 target) ; Learning rate = 0.000411 ; Loss = 1.534127\n",
      "2024-12-12 03:59:02.222000: I runner.py:310] Step = 46400 ; steps/s = 0.72, tokens/s = 34135 (16454 source, 17681 target) ; Learning rate = 0.000410 ; Loss = 1.537373\n",
      "2024-12-12 04:01:20.919000: I runner.py:310] Step = 46500 ; steps/s = 0.72, tokens/s = 34160 (16448 source, 17712 target) ; Learning rate = 0.000410 ; Loss = 1.534728\n",
      "2024-12-12 04:03:37.388000: I runner.py:310] Step = 46600 ; steps/s = 0.73, tokens/s = 34039 (16395 source, 17644 target) ; Learning rate = 0.000409 ; Loss = 1.544513\n",
      "2024-12-12 04:05:56.098000: I runner.py:310] Step = 46700 ; steps/s = 0.72, tokens/s = 34167 (16469 source, 17698 target) ; Learning rate = 0.000409 ; Loss = 1.533668\n",
      "2024-12-12 04:08:14.843000: I runner.py:310] Step = 46800 ; steps/s = 0.72, tokens/s = 34157 (16455 source, 17702 target) ; Learning rate = 0.000409 ; Loss = 1.530906\n",
      "2024-12-12 04:10:33.550000: I runner.py:310] Step = 46900 ; steps/s = 0.72, tokens/s = 34157 (16457 source, 17700 target) ; Learning rate = 0.000408 ; Loss = 1.537979\n",
      "2024-12-12 04:12:50.022000: I runner.py:310] Step = 47000 ; steps/s = 0.73, tokens/s = 33997 (16380 source, 17617 target) ; Learning rate = 0.000408 ; Loss = 1.522271\n",
      "2024-12-12 04:15:08.745000: I runner.py:310] Step = 47100 ; steps/s = 0.72, tokens/s = 34161 (16452 source, 17709 target) ; Learning rate = 0.000407 ; Loss = 1.528785\n",
      "2024-12-12 04:17:27.482000: I runner.py:310] Step = 47200 ; steps/s = 0.72, tokens/s = 34167 (16471 source, 17696 target) ; Learning rate = 0.000407 ; Loss = 1.534599\n",
      "2024-12-12 04:19:46.168000: I runner.py:310] Step = 47300 ; steps/s = 0.72, tokens/s = 34170 (16461 source, 17709 target) ; Learning rate = 0.000406 ; Loss = 1.536223\n",
      "2024-12-12 04:22:02.643000: I runner.py:310] Step = 47400 ; steps/s = 0.73, tokens/s = 34034 (16398 source, 17636 target) ; Learning rate = 0.000406 ; Loss = 1.533777\n",
      "2024-12-12 04:24:21.355000: I runner.py:310] Step = 47500 ; steps/s = 0.72, tokens/s = 34158 (16463 source, 17695 target) ; Learning rate = 0.000406 ; Loss = 1.533796\n",
      "2024-12-12 04:26:40.104000: I runner.py:310] Step = 47600 ; steps/s = 0.72, tokens/s = 34152 (16453 source, 17699 target) ; Learning rate = 0.000405 ; Loss = 1.529213\n",
      "2024-12-12 04:28:58.825000: I runner.py:310] Step = 47700 ; steps/s = 0.72, tokens/s = 34161 (16469 source, 17692 target) ; Learning rate = 0.000405 ; Loss = 1.530405\n",
      "2024-12-12 04:31:15.296000: I runner.py:310] Step = 47800 ; steps/s = 0.73, tokens/s = 34023 (16376 source, 17647 target) ; Learning rate = 0.000404 ; Loss = 1.533374\n",
      "2024-12-12 04:33:34.028000: I runner.py:310] Step = 47900 ; steps/s = 0.72, tokens/s = 34153 (16448 source, 17705 target) ; Learning rate = 0.000404 ; Loss = 1.534595\n",
      "2024-12-12 04:35:52.741000: I runner.py:310] Step = 48000 ; steps/s = 0.72, tokens/s = 34159 (16468 source, 17691 target) ; Learning rate = 0.000403 ; Loss = 1.520075\n",
      "2024-12-12 04:38:11.425000: I runner.py:310] Step = 48100 ; steps/s = 0.72, tokens/s = 34177 (16467 source, 17710 target) ; Learning rate = 0.000403 ; Loss = 1.530992\n",
      "2024-12-12 04:40:30.146000: I runner.py:310] Step = 48200 ; steps/s = 0.72, tokens/s = 34158 (16456 source, 17702 target) ; Learning rate = 0.000403 ; Loss = 1.532849\n",
      "2024-12-12 04:42:46.553000: I runner.py:310] Step = 48300 ; steps/s = 0.73, tokens/s = 34040 (16399 source, 17641 target) ; Learning rate = 0.000402 ; Loss = 1.524157\n",
      "2024-12-12 04:45:05.287000: I runner.py:310] Step = 48400 ; steps/s = 0.72, tokens/s = 34171 (16457 source, 17714 target) ; Learning rate = 0.000402 ; Loss = 1.540247\n",
      "2024-12-12 04:47:23.984000: I runner.py:310] Step = 48500 ; steps/s = 0.72, tokens/s = 34170 (16458 source, 17712 target) ; Learning rate = 0.000401 ; Loss = 1.534701\n",
      "2024-12-12 04:49:42.337000: I runner.py:310] Step = 48600 ; steps/s = 0.72, tokens/s = 34246 (16513 source, 17733 target) ; Learning rate = 0.000401 ; Loss = 1.540546\n",
      "2024-12-12 04:51:58.354000: I runner.py:310] Step = 48700 ; steps/s = 0.74, tokens/s = 34124 (16434 source, 17690 target) ; Learning rate = 0.000401 ; Loss = 1.519242\n",
      "2024-12-12 04:54:16.729000: I runner.py:310] Step = 48800 ; steps/s = 0.72, tokens/s = 34260 (16504 source, 17756 target) ; Learning rate = 0.000400 ; Loss = 1.530191\n",
      "2024-12-12 04:56:35.429000: I runner.py:310] Step = 48900 ; steps/s = 0.72, tokens/s = 34162 (16463 source, 17699 target) ; Learning rate = 0.000400 ; Loss = 1.534477\n",
      "2024-12-12 04:58:54.126000: I runner.py:310] Step = 49000 ; steps/s = 0.72, tokens/s = 34155 (16457 source, 17698 target) ; Learning rate = 0.000399 ; Loss = 1.539580\n",
      "2024-12-12 05:01:10.548000: I runner.py:310] Step = 49100 ; steps/s = 0.73, tokens/s = 34038 (16399 source, 17639 target) ; Learning rate = 0.000399 ; Loss = 1.522348\n",
      "2024-12-12 05:03:29.281000: I runner.py:310] Step = 49200 ; steps/s = 0.72, tokens/s = 34147 (16442 source, 17705 target) ; Learning rate = 0.000398 ; Loss = 1.530871\n",
      "2024-12-12 05:05:48.047000: I runner.py:310] Step = 49300 ; steps/s = 0.72, tokens/s = 34151 (16455 source, 17696 target) ; Learning rate = 0.000398 ; Loss = 1.533862\n",
      "2024-12-12 05:08:06.777000: I runner.py:310] Step = 49400 ; steps/s = 0.72, tokens/s = 34165 (16470 source, 17695 target) ; Learning rate = 0.000398 ; Loss = 1.534377\n",
      "2024-12-12 05:10:23.231000: I runner.py:310] Step = 49500 ; steps/s = 0.73, tokens/s = 34029 (16400 source, 17629 target) ; Learning rate = 0.000397 ; Loss = 1.520414\n",
      "2024-12-12 05:12:41.973000: I runner.py:310] Step = 49600 ; steps/s = 0.72, tokens/s = 34155 (16445 source, 17710 target) ; Learning rate = 0.000397 ; Loss = 1.522805\n",
      "2024-12-12 05:15:00.684000: I runner.py:310] Step = 49700 ; steps/s = 0.72, tokens/s = 34152 (16448 source, 17704 target) ; Learning rate = 0.000396 ; Loss = 1.520637\n",
      "2024-12-12 05:17:19.380000: I runner.py:310] Step = 49800 ; steps/s = 0.72, tokens/s = 34171 (16469 source, 17702 target) ; Learning rate = 0.000396 ; Loss = 1.532540\n",
      "2024-12-12 05:19:38.061000: I runner.py:310] Step = 49900 ; steps/s = 0.72, tokens/s = 34178 (16472 source, 17706 target) ; Learning rate = 0.000396 ; Loss = 1.532468\n",
      "2024-12-12 05:21:54.466000: I runner.py:310] Step = 50000 ; steps/s = 0.73, tokens/s = 34039 (16402 source, 17637 target) ; Learning rate = 0.000395 ; Loss = 1.539434\n",
      "2024-12-12 05:21:56.555000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-50000\n",
      "2024-12-12 05:24:15.255000: I runner.py:310] Step = 50100 ; steps/s = 0.72, tokens/s = 34172 (16458 source, 17714 target) ; Learning rate = 0.000395 ; Loss = 1.527307\n",
      "2024-12-12 05:26:34.008000: I runner.py:310] Step = 50200 ; steps/s = 0.72, tokens/s = 34158 (16462 source, 17696 target) ; Learning rate = 0.000394 ; Loss = 1.534309\n",
      "2024-12-12 05:28:52.719000: I runner.py:310] Step = 50300 ; steps/s = 0.72, tokens/s = 34157 (16461 source, 17696 target) ; Learning rate = 0.000394 ; Loss = 1.525230\n",
      "2024-12-12 05:31:09.119000: I runner.py:310] Step = 50400 ; steps/s = 0.73, tokens/s = 34039 (16394 source, 17645 target) ; Learning rate = 0.000394 ; Loss = 1.528717\n",
      "2024-12-12 05:33:27.854000: I runner.py:310] Step = 50500 ; steps/s = 0.72, tokens/s = 34167 (16464 source, 17703 target) ; Learning rate = 0.000393 ; Loss = 1.516783\n",
      "2024-12-12 05:35:46.612000: I runner.py:310] Step = 50600 ; steps/s = 0.72, tokens/s = 34159 (16458 source, 17701 target) ; Learning rate = 0.000393 ; Loss = 1.522327\n",
      "2024-12-12 05:38:05.329000: I runner.py:310] Step = 50700 ; steps/s = 0.72, tokens/s = 34156 (16458 source, 17698 target) ; Learning rate = 0.000393 ; Loss = 1.522633\n",
      "2024-12-12 05:40:21.754000: I runner.py:310] Step = 50800 ; steps/s = 0.73, tokens/s = 34024 (16390 source, 17634 target) ; Learning rate = 0.000392 ; Loss = 1.523133\n",
      "2024-12-12 05:42:40.489000: I runner.py:310] Step = 50900 ; steps/s = 0.72, tokens/s = 34166 (16454 source, 17712 target) ; Learning rate = 0.000392 ; Loss = 1.526128\n",
      "2024-12-12 05:44:59.228000: I runner.py:310] Step = 51000 ; steps/s = 0.72, tokens/s = 34144 (16454 source, 17690 target) ; Learning rate = 0.000391 ; Loss = 1.525559\n",
      "2024-12-12 05:47:17.591000: I runner.py:310] Step = 51100 ; steps/s = 0.72, tokens/s = 34245 (16505 source, 17740 target) ; Learning rate = 0.000391 ; Loss = 1.533612\n",
      "2024-12-12 05:49:35.464000: I runner.py:310] Step = 51200 ; steps/s = 0.73, tokens/s = 34226 (16489 source, 17737 target) ; Learning rate = 0.000391 ; Loss = 1.556390\n",
      "2024-12-12 05:51:52.028000: I runner.py:310] Step = 51300 ; steps/s = 0.73, tokens/s = 34160 (16446 source, 17714 target) ; Learning rate = 0.000390 ; Loss = 1.519152\n",
      "2024-12-12 05:54:10.739000: I runner.py:310] Step = 51400 ; steps/s = 0.72, tokens/s = 34167 (16455 source, 17712 target) ; Learning rate = 0.000390 ; Loss = 1.525746\n",
      "2024-12-12 05:56:29.450000: I runner.py:310] Step = 51500 ; steps/s = 0.72, tokens/s = 34162 (16474 source, 17688 target) ; Learning rate = 0.000389 ; Loss = 1.538069\n",
      "2024-12-12 05:58:48.156000: I runner.py:310] Step = 51600 ; steps/s = 0.72, tokens/s = 34159 (16460 source, 17699 target) ; Learning rate = 0.000389 ; Loss = 1.529215\n",
      "2024-12-12 06:01:04.581000: I runner.py:310] Step = 51700 ; steps/s = 0.73, tokens/s = 34026 (16384 source, 17642 target) ; Learning rate = 0.000389 ; Loss = 1.518212\n",
      "2024-12-12 06:03:23.240000: I runner.py:310] Step = 51800 ; steps/s = 0.72, tokens/s = 34182 (16470 source, 17712 target) ; Learning rate = 0.000388 ; Loss = 1.533342\n",
      "2024-12-12 06:05:41.969000: I runner.py:310] Step = 51900 ; steps/s = 0.72, tokens/s = 34164 (16462 source, 17702 target) ; Learning rate = 0.000388 ; Loss = 1.531151\n",
      "2024-12-12 06:08:00.661000: I runner.py:310] Step = 52000 ; steps/s = 0.72, tokens/s = 34157 (16469 source, 17688 target) ; Learning rate = 0.000388 ; Loss = 1.524665\n",
      "2024-12-12 06:10:17.100000: I runner.py:310] Step = 52100 ; steps/s = 0.73, tokens/s = 34037 (16389 source, 17648 target) ; Learning rate = 0.000387 ; Loss = 1.531837\n",
      "2024-12-12 06:12:35.845000: I runner.py:310] Step = 52200 ; steps/s = 0.72, tokens/s = 34165 (16462 source, 17703 target) ; Learning rate = 0.000387 ; Loss = 1.521417\n",
      "2024-12-12 06:14:54.553000: I runner.py:310] Step = 52300 ; steps/s = 0.72, tokens/s = 34155 (16459 source, 17696 target) ; Learning rate = 0.000386 ; Loss = 1.521190\n",
      "2024-12-12 06:17:13.258000: I runner.py:310] Step = 52400 ; steps/s = 0.72, tokens/s = 34175 (16467 source, 17708 target) ; Learning rate = 0.000386 ; Loss = 1.521817\n",
      "2024-12-12 06:19:29.746000: I runner.py:310] Step = 52500 ; steps/s = 0.73, tokens/s = 34003 (16383 source, 17620 target) ; Learning rate = 0.000386 ; Loss = 1.530705\n",
      "2024-12-12 06:21:48.470000: I runner.py:310] Step = 52600 ; steps/s = 0.72, tokens/s = 34166 (16454 source, 17712 target) ; Learning rate = 0.000385 ; Loss = 1.521896\n",
      "2024-12-12 06:24:07.130000: I runner.py:310] Step = 52700 ; steps/s = 0.72, tokens/s = 34173 (16473 source, 17700 target) ; Learning rate = 0.000385 ; Loss = 1.520587\n",
      "2024-12-12 06:26:25.882000: I runner.py:310] Step = 52800 ; steps/s = 0.72, tokens/s = 34145 (16448 source, 17697 target) ; Learning rate = 0.000385 ; Loss = 1.517617\n",
      "2024-12-12 06:28:44.564000: I runner.py:310] Step = 52900 ; steps/s = 0.72, tokens/s = 34179 (16471 source, 17708 target) ; Learning rate = 0.000384 ; Loss = 1.527300\n",
      "2024-12-12 06:31:01.010000: I runner.py:310] Step = 53000 ; steps/s = 0.73, tokens/s = 34021 (16385 source, 17636 target) ; Learning rate = 0.000384 ; Loss = 1.516635\n",
      "2024-12-12 06:33:19.728000: I runner.py:310] Step = 53100 ; steps/s = 0.72, tokens/s = 34162 (16471 source, 17691 target) ; Learning rate = 0.000384 ; Loss = 1.523817\n",
      "2024-12-12 06:35:38.434000: I runner.py:310] Step = 53200 ; steps/s = 0.72, tokens/s = 34175 (16462 source, 17713 target) ; Learning rate = 0.000383 ; Loss = 1.531151\n",
      "2024-12-12 06:37:57.169000: I runner.py:310] Step = 53300 ; steps/s = 0.72, tokens/s = 34160 (16451 source, 17709 target) ; Learning rate = 0.000383 ; Loss = 1.529091\n",
      "2024-12-12 06:40:13.590000: I runner.py:310] Step = 53400 ; steps/s = 0.73, tokens/s = 34030 (16393 source, 17637 target) ; Learning rate = 0.000382 ; Loss = 1.513431\n",
      "2024-12-12 06:42:32.300000: I runner.py:310] Step = 53500 ; steps/s = 0.72, tokens/s = 34180 (16468 source, 17712 target) ; Learning rate = 0.000382 ; Loss = 1.523954\n",
      "2024-12-12 06:44:50.696000: I runner.py:310] Step = 53600 ; steps/s = 0.72, tokens/s = 34231 (16502 source, 17729 target) ; Learning rate = 0.000382 ; Loss = 1.530366\n",
      "2024-12-12 06:47:09.033000: I runner.py:310] Step = 53700 ; steps/s = 0.72, tokens/s = 34249 (16507 source, 17742 target) ; Learning rate = 0.000381 ; Loss = 1.527947\n",
      "2024-12-12 06:49:25.110000: I runner.py:310] Step = 53800 ; steps/s = 0.73, tokens/s = 34130 (16435 source, 17695 target) ; Learning rate = 0.000381 ; Loss = 1.517295\n",
      "2024-12-12 06:51:43.781000: I runner.py:310] Step = 53900 ; steps/s = 0.72, tokens/s = 34165 (16455 source, 17710 target) ; Learning rate = 0.000381 ; Loss = 1.519367\n",
      "2024-12-12 06:54:02.520000: I runner.py:310] Step = 54000 ; steps/s = 0.72, tokens/s = 34154 (16460 source, 17694 target) ; Learning rate = 0.000380 ; Loss = 1.526053\n",
      "2024-12-12 06:56:21.248000: I runner.py:310] Step = 54100 ; steps/s = 0.72, tokens/s = 34167 (16461 source, 17706 target) ; Learning rate = 0.000380 ; Loss = 1.530338\n",
      "2024-12-12 06:58:37.695000: I runner.py:310] Step = 54200 ; steps/s = 0.73, tokens/s = 34027 (16395 source, 17632 target) ; Learning rate = 0.000380 ; Loss = 1.522553\n",
      "2024-12-12 07:00:56.447000: I runner.py:310] Step = 54300 ; steps/s = 0.72, tokens/s = 34148 (16456 source, 17692 target) ; Learning rate = 0.000379 ; Loss = 1.521455\n",
      "2024-12-12 07:03:15.175000: I runner.py:310] Step = 54400 ; steps/s = 0.72, tokens/s = 34171 (16458 source, 17713 target) ; Learning rate = 0.000379 ; Loss = 1.516880\n",
      "2024-12-12 07:05:33.928000: I runner.py:310] Step = 54500 ; steps/s = 0.72, tokens/s = 34146 (16452 source, 17694 target) ; Learning rate = 0.000379 ; Loss = 1.526554\n",
      "2024-12-12 07:07:52.666000: I runner.py:310] Step = 54600 ; steps/s = 0.72, tokens/s = 34150 (16454 source, 17696 target) ; Learning rate = 0.000378 ; Loss = 1.522224\n",
      "2024-12-12 07:10:09.124000: I runner.py:310] Step = 54700 ; steps/s = 0.73, tokens/s = 34044 (16403 source, 17641 target) ; Learning rate = 0.000378 ; Loss = 1.513277\n",
      "2024-12-12 07:12:27.851000: I runner.py:310] Step = 54800 ; steps/s = 0.72, tokens/s = 34154 (16457 source, 17697 target) ; Learning rate = 0.000378 ; Loss = 1.521342\n",
      "2024-12-12 07:14:46.574000: I runner.py:310] Step = 54900 ; steps/s = 0.72, tokens/s = 34157 (16448 source, 17709 target) ; Learning rate = 0.000377 ; Loss = 1.522690\n",
      "2024-12-12 07:17:05.288000: I runner.py:310] Step = 55000 ; steps/s = 0.72, tokens/s = 34163 (16469 source, 17694 target) ; Learning rate = 0.000377 ; Loss = 1.533468\n",
      "2024-12-12 07:19:21.703000: I runner.py:310] Step = 55100 ; steps/s = 0.73, tokens/s = 34033 (16391 source, 17642 target) ; Learning rate = 0.000377 ; Loss = 1.514355\n",
      "2024-12-12 07:21:40.395000: I runner.py:310] Step = 55200 ; steps/s = 0.72, tokens/s = 34159 (16467 source, 17692 target) ; Learning rate = 0.000376 ; Loss = 1.521291\n",
      "2024-12-12 07:23:59.135000: I runner.py:310] Step = 55300 ; steps/s = 0.72, tokens/s = 34175 (16459 source, 17716 target) ; Learning rate = 0.000376 ; Loss = 1.512594\n",
      "2024-12-12 07:26:17.852000: I runner.py:310] Step = 55400 ; steps/s = 0.72, tokens/s = 34148 (16451 source, 17697 target) ; Learning rate = 0.000376 ; Loss = 1.530042\n",
      "2024-12-12 07:28:34.221000: I runner.py:310] Step = 55500 ; steps/s = 0.73, tokens/s = 34057 (16413 source, 17644 target) ; Learning rate = 0.000375 ; Loss = 1.508266\n",
      "2024-12-12 07:30:52.968000: I runner.py:310] Step = 55600 ; steps/s = 0.72, tokens/s = 34155 (16459 source, 17696 target) ; Learning rate = 0.000375 ; Loss = 1.520554\n",
      "2024-12-12 07:33:11.686000: I runner.py:310] Step = 55700 ; steps/s = 0.72, tokens/s = 34148 (16442 source, 17706 target) ; Learning rate = 0.000375 ; Loss = 1.525152\n",
      "2024-12-12 07:35:30.405000: I runner.py:310] Step = 55800 ; steps/s = 0.72, tokens/s = 34168 (16468 source, 17700 target) ; Learning rate = 0.000374 ; Loss = 1.520933\n",
      "2024-12-12 07:37:46.821000: I runner.py:310] Step = 55900 ; steps/s = 0.73, tokens/s = 34043 (16406 source, 17637 target) ; Learning rate = 0.000374 ; Loss = 1.518612\n",
      "2024-12-12 07:40:05.472000: I runner.py:310] Step = 56000 ; steps/s = 0.72, tokens/s = 34193 (16466 source, 17727 target) ; Learning rate = 0.000374 ; Loss = 1.518529\n",
      "2024-12-12 07:42:23.855000: I runner.py:310] Step = 56100 ; steps/s = 0.72, tokens/s = 34249 (16504 source, 17745 target) ; Learning rate = 0.000373 ; Loss = 1.522508\n",
      "2024-12-12 07:44:42.213000: I runner.py:310] Step = 56200 ; steps/s = 0.72, tokens/s = 34243 (16503 source, 17740 target) ; Learning rate = 0.000373 ; Loss = 1.521876\n",
      "2024-12-12 07:47:00.647000: I runner.py:310] Step = 56300 ; steps/s = 0.72, tokens/s = 34219 (16482 source, 17737 target) ; Learning rate = 0.000373 ; Loss = 1.528283\n",
      "2024-12-12 07:49:17.110000: I runner.py:310] Step = 56400 ; steps/s = 0.73, tokens/s = 34016 (16396 source, 17620 target) ; Learning rate = 0.000372 ; Loss = 1.510804\n",
      "2024-12-12 07:51:35.845000: I runner.py:310] Step = 56500 ; steps/s = 0.72, tokens/s = 34153 (16459 source, 17694 target) ; Learning rate = 0.000372 ; Loss = 1.513436\n",
      "2024-12-12 07:53:54.538000: I runner.py:310] Step = 56600 ; steps/s = 0.72, tokens/s = 34173 (16466 source, 17707 target) ; Learning rate = 0.000372 ; Loss = 1.521279\n",
      "2024-12-12 07:56:13.273000: I runner.py:310] Step = 56700 ; steps/s = 0.72, tokens/s = 34174 (16456 source, 17718 target) ; Learning rate = 0.000371 ; Loss = 1.520711\n",
      "2024-12-12 07:58:29.735000: I runner.py:310] Step = 56800 ; steps/s = 0.73, tokens/s = 34018 (16381 source, 17637 target) ; Learning rate = 0.000371 ; Loss = 1.509470\n",
      "2024-12-12 08:00:48.487000: I runner.py:310] Step = 56900 ; steps/s = 0.72, tokens/s = 34155 (16452 source, 17703 target) ; Learning rate = 0.000371 ; Loss = 1.518946\n",
      "2024-12-12 08:03:07.162000: I runner.py:310] Step = 57000 ; steps/s = 0.72, tokens/s = 34171 (16469 source, 17702 target) ; Learning rate = 0.000370 ; Loss = 1.525830\n",
      "2024-12-12 08:05:25.869000: I runner.py:310] Step = 57100 ; steps/s = 0.72, tokens/s = 34162 (16453 source, 17709 target) ; Learning rate = 0.000370 ; Loss = 1.523706\n",
      "2024-12-12 08:07:42.317000: I runner.py:310] Step = 57200 ; steps/s = 0.73, tokens/s = 34030 (16408 source, 17622 target) ; Learning rate = 0.000370 ; Loss = 1.516965\n",
      "2024-12-12 08:10:01.050000: I runner.py:310] Step = 57300 ; steps/s = 0.72, tokens/s = 34163 (16455 source, 17708 target) ; Learning rate = 0.000369 ; Loss = 1.510582\n",
      "2024-12-12 08:12:19.791000: I runner.py:310] Step = 57400 ; steps/s = 0.72, tokens/s = 34152 (16461 source, 17691 target) ; Learning rate = 0.000369 ; Loss = 1.522987\n",
      "2024-12-12 08:14:38.494000: I runner.py:310] Step = 57500 ; steps/s = 0.72, tokens/s = 34171 (16463 source, 17708 target) ; Learning rate = 0.000369 ; Loss = 1.516555\n",
      "2024-12-12 08:16:56.888000: I runner.py:310] Step = 57600 ; steps/s = 0.72, tokens/s = 34138 (16443 source, 17695 target) ; Learning rate = 0.000368 ; Loss = 1.531508\n",
      "2024-12-12 08:19:13.648000: I runner.py:310] Step = 57700 ; steps/s = 0.73, tokens/s = 34052 (16410 source, 17642 target) ; Learning rate = 0.000368 ; Loss = 1.504761\n",
      "2024-12-12 08:21:32.371000: I runner.py:310] Step = 57800 ; steps/s = 0.72, tokens/s = 34160 (16455 source, 17705 target) ; Learning rate = 0.000368 ; Loss = 1.514807\n",
      "2024-12-12 08:23:51.125000: I runner.py:310] Step = 57900 ; steps/s = 0.72, tokens/s = 34158 (16461 source, 17697 target) ; Learning rate = 0.000367 ; Loss = 1.515267\n",
      "2024-12-12 08:26:09.851000: I runner.py:310] Step = 58000 ; steps/s = 0.72, tokens/s = 34158 (16459 source, 17699 target) ; Learning rate = 0.000367 ; Loss = 1.526169\n",
      "2024-12-12 08:28:26.312000: I runner.py:310] Step = 58100 ; steps/s = 0.73, tokens/s = 34015 (16385 source, 17630 target) ; Learning rate = 0.000367 ; Loss = 1.527857\n",
      "2024-12-12 08:30:45.044000: I runner.py:310] Step = 58200 ; steps/s = 0.72, tokens/s = 34161 (16462 source, 17699 target) ; Learning rate = 0.000366 ; Loss = 1.514093\n",
      "2024-12-12 08:33:03.804000: I runner.py:310] Step = 58300 ; steps/s = 0.72, tokens/s = 34141 (16442 source, 17699 target) ; Learning rate = 0.000366 ; Loss = 1.508611\n",
      "2024-12-12 08:35:22.532000: I runner.py:310] Step = 58400 ; steps/s = 0.72, tokens/s = 34165 (16464 source, 17701 target) ; Learning rate = 0.000366 ; Loss = 1.512907\n",
      "2024-12-12 08:37:38.964000: I runner.py:310] Step = 58500 ; steps/s = 0.73, tokens/s = 34046 (16409 source, 17637 target) ; Learning rate = 0.000365 ; Loss = 1.519317\n",
      "2024-12-12 08:39:57.300000: I runner.py:310] Step = 58600 ; steps/s = 0.72, tokens/s = 34246 (16489 source, 17757 target) ; Learning rate = 0.000365 ; Loss = 1.512265\n",
      "2024-12-12 08:42:15.645000: I runner.py:310] Step = 58700 ; steps/s = 0.72, tokens/s = 34259 (16495 source, 17764 target) ; Learning rate = 0.000365 ; Loss = 1.531259\n",
      "2024-12-12 08:44:34.034000: I runner.py:310] Step = 58800 ; steps/s = 0.72, tokens/s = 34239 (16511 source, 17728 target) ; Learning rate = 0.000365 ; Loss = 1.522681\n",
      "2024-12-12 08:46:50.510000: I runner.py:310] Step = 58900 ; steps/s = 0.73, tokens/s = 34028 (16396 source, 17632 target) ; Learning rate = 0.000364 ; Loss = 1.513446\n",
      "2024-12-12 08:49:09.256000: I runner.py:310] Step = 59000 ; steps/s = 0.72, tokens/s = 34150 (16449 source, 17701 target) ; Learning rate = 0.000364 ; Loss = 1.511142\n",
      "2024-12-12 08:51:27.982000: I runner.py:310] Step = 59100 ; steps/s = 0.72, tokens/s = 34166 (16452 source, 17714 target) ; Learning rate = 0.000364 ; Loss = 1.517632\n",
      "2024-12-12 08:53:46.727000: I runner.py:310] Step = 59200 ; steps/s = 0.72, tokens/s = 34149 (16464 source, 17685 target) ; Learning rate = 0.000363 ; Loss = 1.518155\n",
      "2024-12-12 08:56:05.467000: I runner.py:310] Step = 59300 ; steps/s = 0.72, tokens/s = 34159 (16465 source, 17694 target) ; Learning rate = 0.000363 ; Loss = 1.512736\n",
      "2024-12-12 08:58:21.987000: I runner.py:310] Step = 59400 ; steps/s = 0.73, tokens/s = 34005 (16355 source, 17650 target) ; Learning rate = 0.000363 ; Loss = 1.520477\n",
      "2024-12-12 09:00:40.684000: I runner.py:310] Step = 59500 ; steps/s = 0.72, tokens/s = 34153 (16457 source, 17696 target) ; Learning rate = 0.000362 ; Loss = 1.510095\n",
      "2024-12-12 09:02:59.440000: I runner.py:310] Step = 59600 ; steps/s = 0.72, tokens/s = 34163 (16463 source, 17700 target) ; Learning rate = 0.000362 ; Loss = 1.511645\n",
      "2024-12-12 09:05:18.146000: I runner.py:310] Step = 59700 ; steps/s = 0.72, tokens/s = 34173 (16477 source, 17696 target) ; Learning rate = 0.000362 ; Loss = 1.519376\n",
      "2024-12-12 09:07:34.584000: I runner.py:310] Step = 59800 ; steps/s = 0.73, tokens/s = 34035 (16397 source, 17638 target) ; Learning rate = 0.000361 ; Loss = 1.506242\n",
      "2024-12-12 09:09:53.303000: I runner.py:310] Step = 59900 ; steps/s = 0.72, tokens/s = 34159 (16448 source, 17711 target) ; Learning rate = 0.000361 ; Loss = 1.512027\n",
      "2024-12-12 09:12:12.034000: I runner.py:310] Step = 60000 ; steps/s = 0.72, tokens/s = 34158 (16463 source, 17695 target) ; Learning rate = 0.000361 ; Loss = 1.517060\n",
      "2024-12-12 09:12:14.151000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-60000\n",
      "2024-12-12 09:14:32.913000: I runner.py:310] Step = 60100 ; steps/s = 0.72, tokens/s = 34151 (16457 source, 17694 target) ; Learning rate = 0.000361 ; Loss = 1.520105\n",
      "2024-12-12 09:16:49.323000: I runner.py:310] Step = 60200 ; steps/s = 0.73, tokens/s = 34047 (16411 source, 17636 target) ; Learning rate = 0.000360 ; Loss = 1.509137\n",
      "2024-12-12 09:19:08.060000: I runner.py:310] Step = 60300 ; steps/s = 0.72, tokens/s = 34159 (16461 source, 17698 target) ; Learning rate = 0.000360 ; Loss = 1.514313\n",
      "2024-12-12 09:21:26.754000: I runner.py:310] Step = 60400 ; steps/s = 0.72, tokens/s = 34168 (16457 source, 17711 target) ; Learning rate = 0.000360 ; Loss = 1.515124\n",
      "2024-12-12 09:23:45.513000: I runner.py:310] Step = 60500 ; steps/s = 0.72, tokens/s = 34143 (16452 source, 17691 target) ; Learning rate = 0.000359 ; Loss = 1.516492\n",
      "2024-12-12 09:26:01.948000: I runner.py:310] Step = 60600 ; steps/s = 0.73, tokens/s = 34029 (16399 source, 17630 target) ; Learning rate = 0.000359 ; Loss = 1.513752\n",
      "2024-12-12 09:28:20.634000: I runner.py:310] Step = 60700 ; steps/s = 0.72, tokens/s = 34179 (16465 source, 17714 target) ; Learning rate = 0.000359 ; Loss = 1.511288\n",
      "2024-12-12 09:30:39.354000: I runner.py:310] Step = 60800 ; steps/s = 0.72, tokens/s = 34154 (16455 source, 17699 target) ; Learning rate = 0.000358 ; Loss = 1.510410\n",
      "2024-12-12 09:32:58.088000: I runner.py:310] Step = 60900 ; steps/s = 0.72, tokens/s = 34158 (16469 source, 17689 target) ; Learning rate = 0.000358 ; Loss = 1.517435\n",
      "2024-12-12 09:35:16.793000: I runner.py:310] Step = 61000 ; steps/s = 0.72, tokens/s = 34157 (16447 source, 17710 target) ; Learning rate = 0.000358 ; Loss = 1.513011\n",
      "2024-12-12 09:37:32.813000: I runner.py:310] Step = 61100 ; steps/s = 0.74, tokens/s = 34145 (16441 source, 17704 target) ; Learning rate = 0.000358 ; Loss = 1.513617\n",
      "2024-12-12 09:39:51.141000: I runner.py:310] Step = 61200 ; steps/s = 0.72, tokens/s = 34255 (16504 source, 17751 target) ; Learning rate = 0.000357 ; Loss = 1.509242\n",
      "2024-12-12 09:42:09.495000: I runner.py:310] Step = 61300 ; steps/s = 0.72, tokens/s = 34253 (16497 source, 17756 target) ; Learning rate = 0.000357 ; Loss = 1.506219\n",
      "2024-12-12 09:44:28.229000: I runner.py:310] Step = 61400 ; steps/s = 0.72, tokens/s = 34155 (16473 source, 17682 target) ; Learning rate = 0.000357 ; Loss = 1.517475\n",
      "2024-12-12 09:46:44.674000: I runner.py:310] Step = 61500 ; steps/s = 0.73, tokens/s = 34029 (16390 source, 17639 target) ; Learning rate = 0.000356 ; Loss = 1.511042\n",
      "2024-12-12 09:49:03.404000: I runner.py:310] Step = 61600 ; steps/s = 0.72, tokens/s = 34154 (16451 source, 17703 target) ; Learning rate = 0.000356 ; Loss = 1.501016\n",
      "2024-12-12 09:51:22.110000: I runner.py:310] Step = 61700 ; steps/s = 0.72, tokens/s = 34179 (16475 source, 17704 target) ; Learning rate = 0.000356 ; Loss = 1.505986\n",
      "2024-12-12 09:53:40.841000: I runner.py:310] Step = 61800 ; steps/s = 0.72, tokens/s = 34140 (16453 source, 17687 target) ; Learning rate = 0.000356 ; Loss = 1.514017\n",
      "2024-12-12 09:55:57.293000: I runner.py:310] Step = 61900 ; steps/s = 0.73, tokens/s = 34043 (16395 source, 17648 target) ; Learning rate = 0.000355 ; Loss = 1.508004\n",
      "2024-12-12 09:58:16.010000: I runner.py:310] Step = 62000 ; steps/s = 0.72, tokens/s = 34160 (16465 source, 17695 target) ; Learning rate = 0.000355 ; Loss = 1.510482\n",
      "2024-12-12 10:00:34.691000: I runner.py:310] Step = 62100 ; steps/s = 0.72, tokens/s = 34184 (16465 source, 17719 target) ; Learning rate = 0.000355 ; Loss = 1.515398\n",
      "2024-12-12 10:02:53.392000: I runner.py:310] Step = 62200 ; steps/s = 0.72, tokens/s = 34144 (16448 source, 17696 target) ; Learning rate = 0.000354 ; Loss = 1.515708\n",
      "2024-12-12 10:05:09.849000: I runner.py:310] Step = 62300 ; steps/s = 0.73, tokens/s = 34029 (16402 source, 17627 target) ; Learning rate = 0.000354 ; Loss = 1.512479\n",
      "2024-12-12 10:07:28.537000: I runner.py:310] Step = 62400 ; steps/s = 0.72, tokens/s = 34168 (16449 source, 17719 target) ; Learning rate = 0.000354 ; Loss = 1.503311\n",
      "2024-12-12 10:09:47.207000: I runner.py:310] Step = 62500 ; steps/s = 0.72, tokens/s = 34174 (16465 source, 17709 target) ; Learning rate = 0.000354 ; Loss = 1.506146\n",
      "2024-12-12 10:12:05.778000: I runner.py:310] Step = 62600 ; steps/s = 0.72, tokens/s = 34208 (16490 source, 17718 target) ; Learning rate = 0.000353 ; Loss = 1.519874\n",
      "2024-12-12 10:14:24.014000: I runner.py:310] Step = 62700 ; steps/s = 0.72, tokens/s = 34275 (16515 source, 17760 target) ; Learning rate = 0.000353 ; Loss = 1.527208\n",
      "2024-12-12 10:16:39.987000: I runner.py:310] Step = 62800 ; steps/s = 0.74, tokens/s = 34154 (16458 source, 17696 target) ; Learning rate = 0.000353 ; Loss = 1.519260\n",
      "2024-12-12 10:18:58.851000: I runner.py:310] Step = 62900 ; steps/s = 0.72, tokens/s = 34121 (16437 source, 17684 target) ; Learning rate = 0.000352 ; Loss = 1.506598\n",
      "2024-12-12 10:21:17.452000: I runner.py:310] Step = 63000 ; steps/s = 0.72, tokens/s = 34190 (16478 source, 17712 target) ; Learning rate = 0.000352 ; Loss = 1.509568\n",
      "2024-12-12 10:23:36.165000: I runner.py:310] Step = 63100 ; steps/s = 0.72, tokens/s = 34167 (16459 source, 17708 target) ; Learning rate = 0.000352 ; Loss = 1.507145\n",
      "2024-12-12 10:25:52.531000: I runner.py:310] Step = 63200 ; steps/s = 0.73, tokens/s = 34045 (16404 source, 17641 target) ; Learning rate = 0.000352 ; Loss = 1.504557\n",
      "2024-12-12 10:28:11.347000: I runner.py:310] Step = 63300 ; steps/s = 0.72, tokens/s = 34133 (16446 source, 17687 target) ; Learning rate = 0.000351 ; Loss = 1.502111\n",
      "2024-12-12 10:30:30.014000: I runner.py:310] Step = 63400 ; steps/s = 0.72, tokens/s = 34181 (16461 source, 17720 target) ; Learning rate = 0.000351 ; Loss = 1.502868\n",
      "2024-12-12 10:32:48.673000: I runner.py:310] Step = 63500 ; steps/s = 0.72, tokens/s = 34170 (16467 source, 17703 target) ; Learning rate = 0.000351 ; Loss = 1.519416\n",
      "2024-12-12 10:35:05.010000: I runner.py:310] Step = 63600 ; steps/s = 0.73, tokens/s = 34059 (16403 source, 17656 target) ; Learning rate = 0.000350 ; Loss = 1.507239\n",
      "2024-12-12 10:37:23.836000: I runner.py:310] Step = 63700 ; steps/s = 0.72, tokens/s = 34138 (16448 source, 17690 target) ; Learning rate = 0.000350 ; Loss = 1.506833\n",
      "2024-12-12 10:39:42.617000: I runner.py:310] Step = 63800 ; steps/s = 0.72, tokens/s = 34148 (16450 source, 17698 target) ; Learning rate = 0.000350 ; Loss = 1.508731\n",
      "2024-12-12 10:42:01.715000: I runner.py:310] Step = 63900 ; steps/s = 0.72, tokens/s = 34068 (16421 source, 17647 target) ; Learning rate = 0.000350 ; Loss = 1.518963\n",
      "2024-12-12 10:44:20.783000: I runner.py:310] Step = 64000 ; steps/s = 0.72, tokens/s = 34071 (16413 source, 17658 target) ; Learning rate = 0.000349 ; Loss = 1.517389\n",
      "2024-12-12 10:46:37.588000: I runner.py:310] Step = 64100 ; steps/s = 0.73, tokens/s = 33924 (16335 source, 17589 target) ; Learning rate = 0.000349 ; Loss = 1.510571\n",
      "2024-12-12 10:48:56.647000: I runner.py:310] Step = 64200 ; steps/s = 0.72, tokens/s = 34084 (16424 source, 17660 target) ; Learning rate = 0.000349 ; Loss = 1.506761\n",
      "2024-12-12 10:51:15.718000: I runner.py:310] Step = 64300 ; steps/s = 0.72, tokens/s = 34071 (16414 source, 17657 target) ; Learning rate = 0.000349 ; Loss = 1.507461\n",
      "2024-12-12 10:53:34.826000: I runner.py:310] Step = 64400 ; steps/s = 0.72, tokens/s = 34082 (16431 source, 17651 target) ; Learning rate = 0.000348 ; Loss = 1.511514\n",
      "2024-12-12 10:55:51.641000: I runner.py:310] Step = 64500 ; steps/s = 0.73, tokens/s = 33949 (16362 source, 17587 target) ; Learning rate = 0.000348 ; Loss = 1.512241\n",
      "2024-12-12 10:58:10.726000: I runner.py:310] Step = 64600 ; steps/s = 0.72, tokens/s = 34066 (16405 source, 17661 target) ; Learning rate = 0.000348 ; Loss = 1.506946\n",
      "2024-12-12 11:00:29.814000: I runner.py:310] Step = 64700 ; steps/s = 0.72, tokens/s = 34059 (16422 source, 17637 target) ; Learning rate = 0.000347 ; Loss = 1.508115\n",
      "2024-12-12 11:02:48.913000: I runner.py:310] Step = 64800 ; steps/s = 0.72, tokens/s = 34076 (16413 source, 17663 target) ; Learning rate = 0.000347 ; Loss = 1.518632\n",
      "2024-12-12 11:05:05.746000: I runner.py:310] Step = 64900 ; steps/s = 0.73, tokens/s = 33934 (16344 source, 17590 target) ; Learning rate = 0.000347 ; Loss = 1.507211\n",
      "2024-12-12 11:07:24.844000: I runner.py:310] Step = 65000 ; steps/s = 0.72, tokens/s = 34068 (16416 source, 17652 target) ; Learning rate = 0.000347 ; Loss = 1.511786\n",
      "2024-12-12 11:09:43.901000: I runner.py:310] Step = 65100 ; steps/s = 0.72, tokens/s = 34073 (16411 source, 17662 target) ; Learning rate = 0.000346 ; Loss = 1.511135\n",
      "2024-12-12 11:12:02.952000: I runner.py:310] Step = 65200 ; steps/s = 0.72, tokens/s = 34074 (16419 source, 17655 target) ; Learning rate = 0.000346 ; Loss = 1.509186\n",
      "2024-12-12 11:14:19.766000: I runner.py:310] Step = 65300 ; steps/s = 0.73, tokens/s = 33950 (16358 source, 17592 target) ; Learning rate = 0.000346 ; Loss = 1.511695\n",
      "2024-12-12 11:16:38.871000: I runner.py:310] Step = 65400 ; steps/s = 0.72, tokens/s = 34076 (16420 source, 17656 target) ; Learning rate = 0.000346 ; Loss = 1.508552\n",
      "2024-12-12 11:18:57.937000: I runner.py:310] Step = 65500 ; steps/s = 0.72, tokens/s = 34074 (16409 source, 17665 target) ; Learning rate = 0.000345 ; Loss = 1.505505\n",
      "2024-12-12 11:21:17.023000: I runner.py:310] Step = 65600 ; steps/s = 0.72, tokens/s = 34072 (16413 source, 17659 target) ; Learning rate = 0.000345 ; Loss = 1.504294\n",
      "2024-12-12 11:23:36.123000: I runner.py:310] Step = 65700 ; steps/s = 0.72, tokens/s = 34053 (16419 source, 17634 target) ; Learning rate = 0.000345 ; Loss = 1.506494\n",
      "2024-12-12 11:25:52.939000: I runner.py:310] Step = 65800 ; steps/s = 0.73, tokens/s = 33944 (16351 source, 17593 target) ; Learning rate = 0.000345 ; Loss = 1.513444\n",
      "2024-12-12 11:28:12.044000: I runner.py:310] Step = 65900 ; steps/s = 0.72, tokens/s = 34048 (16400 source, 17648 target) ; Learning rate = 0.000344 ; Loss = 1.501809\n",
      "2024-12-12 11:30:30.849000: I runner.py:310] Step = 66000 ; steps/s = 0.72, tokens/s = 34143 (16458 source, 17685 target) ; Learning rate = 0.000344 ; Loss = 1.513764\n",
      "2024-12-12 11:32:49.644000: I runner.py:310] Step = 66100 ; steps/s = 0.72, tokens/s = 34151 (16445 source, 17706 target) ; Learning rate = 0.000344 ; Loss = 1.507726\n",
      "2024-12-12 11:35:06.114000: I runner.py:310] Step = 66200 ; steps/s = 0.73, tokens/s = 34033 (16398 source, 17635 target) ; Learning rate = 0.000344 ; Loss = 1.503624\n",
      "2024-12-12 11:37:24.908000: I runner.py:310] Step = 66300 ; steps/s = 0.72, tokens/s = 34145 (16450 source, 17695 target) ; Learning rate = 0.000343 ; Loss = 1.501469\n",
      "2024-12-12 11:39:43.981000: I runner.py:310] Step = 66400 ; steps/s = 0.72, tokens/s = 34084 (16434 source, 17650 target) ; Learning rate = 0.000343 ; Loss = 1.503900\n",
      "2024-12-12 11:42:03.112000: I runner.py:310] Step = 66500 ; steps/s = 0.72, tokens/s = 34047 (16403 source, 17644 target) ; Learning rate = 0.000343 ; Loss = 1.509454\n",
      "2024-12-12 11:44:19.897000: I runner.py:310] Step = 66600 ; steps/s = 0.73, tokens/s = 33941 (16339 source, 17602 target) ; Learning rate = 0.000342 ; Loss = 1.509864\n",
      "2024-12-12 11:46:38.979000: I runner.py:310] Step = 66700 ; steps/s = 0.72, tokens/s = 34079 (16431 source, 17648 target) ; Learning rate = 0.000342 ; Loss = 1.498858\n",
      "2024-12-12 11:48:58.020000: I runner.py:310] Step = 66800 ; steps/s = 0.72, tokens/s = 34075 (16413 source, 17662 target) ; Learning rate = 0.000342 ; Loss = 1.503363\n",
      "2024-12-12 11:51:17.073000: I runner.py:310] Step = 66900 ; steps/s = 0.72, tokens/s = 34080 (16415 source, 17665 target) ; Learning rate = 0.000342 ; Loss = 1.516443\n",
      "2024-12-12 11:53:33.875000: I runner.py:310] Step = 67000 ; steps/s = 0.73, tokens/s = 33936 (16359 source, 17577 target) ; Learning rate = 0.000341 ; Loss = 1.502396\n",
      "2024-12-12 11:55:52.966000: I runner.py:310] Step = 67100 ; steps/s = 0.72, tokens/s = 34070 (16413 source, 17657 target) ; Learning rate = 0.000341 ; Loss = 1.510178\n",
      "2024-12-12 11:58:12.084000: I runner.py:310] Step = 67200 ; steps/s = 0.72, tokens/s = 34072 (16419 source, 17653 target) ; Learning rate = 0.000341 ; Loss = 1.505009\n",
      "2024-12-12 12:00:31.214000: I runner.py:310] Step = 67300 ; steps/s = 0.72, tokens/s = 34060 (16407 source, 17653 target) ; Learning rate = 0.000341 ; Loss = 1.505799\n",
      "2024-12-12 12:02:50.311000: I runner.py:310] Step = 67400 ; steps/s = 0.72, tokens/s = 34058 (16412 source, 17646 target) ; Learning rate = 0.000340 ; Loss = 1.504317\n",
      "2024-12-12 12:05:07.159000: I runner.py:310] Step = 67500 ; steps/s = 0.73, tokens/s = 33947 (16350 source, 17597 target) ; Learning rate = 0.000340 ; Loss = 1.507426\n",
      "2024-12-12 12:07:26.241000: I runner.py:310] Step = 67600 ; steps/s = 0.72, tokens/s = 34063 (16416 source, 17647 target) ; Learning rate = 0.000340 ; Loss = 1.508243\n",
      "2024-12-12 12:09:45.387000: I runner.py:310] Step = 67700 ; steps/s = 0.72, tokens/s = 34060 (16398 source, 17662 target) ; Learning rate = 0.000340 ; Loss = 1.508496\n",
      "2024-12-12 12:12:04.483000: I runner.py:310] Step = 67800 ; steps/s = 0.72, tokens/s = 34057 (16418 source, 17639 target) ; Learning rate = 0.000339 ; Loss = 1.510583\n",
      "2024-12-12 12:14:21.340000: I runner.py:310] Step = 67900 ; steps/s = 0.73, tokens/s = 33933 (16341 source, 17592 target) ; Learning rate = 0.000339 ; Loss = 1.501166\n",
      "2024-12-12 12:16:40.431000: I runner.py:310] Step = 68000 ; steps/s = 0.72, tokens/s = 34082 (16411 source, 17671 target) ; Learning rate = 0.000339 ; Loss = 1.503461\n",
      "2024-12-12 12:18:59.555000: I runner.py:310] Step = 68100 ; steps/s = 0.72, tokens/s = 34058 (16422 source, 17636 target) ; Learning rate = 0.000339 ; Loss = 1.502458\n",
      "2024-12-12 12:21:18.647000: I runner.py:310] Step = 68200 ; steps/s = 0.72, tokens/s = 34059 (16415 source, 17644 target) ; Learning rate = 0.000338 ; Loss = 1.508334\n",
      "2024-12-12 12:23:35.441000: I runner.py:310] Step = 68300 ; steps/s = 0.73, tokens/s = 33937 (16349 source, 17588 target) ; Learning rate = 0.000338 ; Loss = 1.510112\n",
      "2024-12-12 12:25:54.517000: I runner.py:310] Step = 68400 ; steps/s = 0.72, tokens/s = 34075 (16430 source, 17645 target) ; Learning rate = 0.000338 ; Loss = 1.499939\n",
      "2024-12-12 12:28:13.600000: I runner.py:310] Step = 68500 ; steps/s = 0.72, tokens/s = 34068 (16412 source, 17656 target) ; Learning rate = 0.000338 ; Loss = 1.505780\n",
      "2024-12-12 12:30:32.301000: I runner.py:310] Step = 68600 ; steps/s = 0.72, tokens/s = 34163 (16455 source, 17708 target) ; Learning rate = 0.000337 ; Loss = 1.503756\n",
      "2024-12-12 12:32:49.047000: I runner.py:310] Step = 68700 ; steps/s = 0.73, tokens/s = 33964 (16363 source, 17601 target) ; Learning rate = 0.000337 ; Loss = 1.503206\n",
      "2024-12-12 12:35:08.092000: I runner.py:310] Step = 68800 ; steps/s = 0.72, tokens/s = 34076 (16416 source, 17660 target) ; Learning rate = 0.000337 ; Loss = 1.507965\n",
      "2024-12-12 12:37:27.207000: I runner.py:310] Step = 68900 ; steps/s = 0.72, tokens/s = 34074 (16410 source, 17664 target) ; Learning rate = 0.000337 ; Loss = 1.507098\n",
      "2024-12-12 12:39:46.337000: I runner.py:310] Step = 69000 ; steps/s = 0.72, tokens/s = 34057 (16410 source, 17647 target) ; Learning rate = 0.000336 ; Loss = 1.506121\n",
      "2024-12-12 12:42:05.355000: I runner.py:310] Step = 69100 ; steps/s = 0.72, tokens/s = 34090 (16430 source, 17660 target) ; Learning rate = 0.000336 ; Loss = 1.517086\n",
      "2024-12-12 12:44:22.231000: I runner.py:310] Step = 69200 ; steps/s = 0.73, tokens/s = 33911 (16340 source, 17571 target) ; Learning rate = 0.000336 ; Loss = 1.506812\n",
      "2024-12-12 12:46:41.314000: I runner.py:310] Step = 69300 ; steps/s = 0.72, tokens/s = 34089 (16420 source, 17669 target) ; Learning rate = 0.000336 ; Loss = 1.496725\n",
      "2024-12-12 12:49:00.392000: I runner.py:310] Step = 69400 ; steps/s = 0.72, tokens/s = 34077 (16422 source, 17655 target) ; Learning rate = 0.000336 ; Loss = 1.499826\n",
      "2024-12-12 12:51:19.502000: I runner.py:310] Step = 69500 ; steps/s = 0.72, tokens/s = 34064 (16415 source, 17649 target) ; Learning rate = 0.000335 ; Loss = 1.506349\n",
      "2024-12-12 12:53:36.356000: I runner.py:310] Step = 69600 ; steps/s = 0.73, tokens/s = 33916 (16340 source, 17576 target) ; Learning rate = 0.000335 ; Loss = 1.505676\n",
      "2024-12-12 12:55:55.451000: I runner.py:310] Step = 69700 ; steps/s = 0.72, tokens/s = 34078 (16422 source, 17656 target) ; Learning rate = 0.000335 ; Loss = 1.495103\n",
      "2024-12-12 12:58:14.549000: I runner.py:310] Step = 69800 ; steps/s = 0.72, tokens/s = 34070 (16411 source, 17659 target) ; Learning rate = 0.000335 ; Loss = 1.502161\n",
      "2024-12-12 13:00:33.670000: I runner.py:310] Step = 69900 ; steps/s = 0.72, tokens/s = 34060 (16405 source, 17655 target) ; Learning rate = 0.000334 ; Loss = 1.509715\n",
      "2024-12-12 13:02:50.478000: I runner.py:310] Step = 70000 ; steps/s = 0.73, tokens/s = 33936 (16343 source, 17593 target) ; Learning rate = 0.000334 ; Loss = 1.506148\n",
      "2024-12-12 13:02:52.169000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-70000\n",
      "2024-12-12 13:05:10.835000: I runner.py:310] Step = 70100 ; steps/s = 0.72, tokens/s = 34170 (16465 source, 17705 target) ; Learning rate = 0.000334 ; Loss = 1.503597\n",
      "2024-12-12 13:07:29.780000: I runner.py:310] Step = 70200 ; steps/s = 0.72, tokens/s = 34106 (16430 source, 17676 target) ; Learning rate = 0.000334 ; Loss = 1.504779\n",
      "2024-12-12 13:09:48.838000: I runner.py:310] Step = 70300 ; steps/s = 0.72, tokens/s = 34073 (16414 source, 17659 target) ; Learning rate = 0.000333 ; Loss = 1.508720\n",
      "2024-12-12 13:12:07.925000: I runner.py:310] Step = 70400 ; steps/s = 0.72, tokens/s = 34080 (16428 source, 17652 target) ; Learning rate = 0.000333 ; Loss = 1.503805\n",
      "2024-12-12 13:14:24.736000: I runner.py:310] Step = 70500 ; steps/s = 0.73, tokens/s = 33948 (16350 source, 17598 target) ; Learning rate = 0.000333 ; Loss = 1.502668\n",
      "2024-12-12 13:16:43.850000: I runner.py:310] Step = 70600 ; steps/s = 0.72, tokens/s = 34060 (16399 source, 17661 target) ; Learning rate = 0.000333 ; Loss = 1.498357\n",
      "2024-12-12 13:19:02.932000: I runner.py:310] Step = 70700 ; steps/s = 0.72, tokens/s = 34073 (16434 source, 17639 target) ; Learning rate = 0.000332 ; Loss = 1.501773\n",
      "2024-12-12 13:21:22.031000: I runner.py:310] Step = 70800 ; steps/s = 0.72, tokens/s = 34062 (16408 source, 17654 target) ; Learning rate = 0.000332 ; Loss = 1.507233\n",
      "2024-12-12 13:23:38.443000: I runner.py:310] Step = 70900 ; steps/s = 0.73, tokens/s = 34034 (16404 source, 17630 target) ; Learning rate = 0.000332 ; Loss = 1.506373\n",
      "2024-12-12 13:25:57.401000: I runner.py:310] Step = 71000 ; steps/s = 0.72, tokens/s = 34096 (16419 source, 17677 target) ; Learning rate = 0.000332 ; Loss = 1.500745\n",
      "2024-12-12 13:28:16.466000: I runner.py:310] Step = 71100 ; steps/s = 0.72, tokens/s = 34085 (16420 source, 17665 target) ; Learning rate = 0.000331 ; Loss = 1.500745\n",
      "2024-12-12 13:30:35.559000: I runner.py:310] Step = 71200 ; steps/s = 0.72, tokens/s = 34066 (16418 source, 17648 target) ; Learning rate = 0.000331 ; Loss = 1.505240\n",
      "2024-12-12 13:32:52.324000: I runner.py:310] Step = 71300 ; steps/s = 0.73, tokens/s = 33960 (16360 source, 17600 target) ; Learning rate = 0.000331 ; Loss = 1.502274\n",
      "2024-12-12 13:35:11.423000: I runner.py:310] Step = 71400 ; steps/s = 0.72, tokens/s = 34062 (16414 source, 17648 target) ; Learning rate = 0.000331 ; Loss = 1.502241\n",
      "2024-12-12 13:37:30.537000: I runner.py:310] Step = 71500 ; steps/s = 0.72, tokens/s = 34054 (16407 source, 17647 target) ; Learning rate = 0.000331 ; Loss = 1.502103\n",
      "2024-12-12 13:39:49.596000: I runner.py:310] Step = 71600 ; steps/s = 0.72, tokens/s = 34082 (16426 source, 17656 target) ; Learning rate = 0.000330 ; Loss = 1.509458\n",
      "2024-12-12 13:42:06.393000: I runner.py:310] Step = 71700 ; steps/s = 0.73, tokens/s = 33955 (16355 source, 17600 target) ; Learning rate = 0.000330 ; Loss = 1.511385\n",
      "2024-12-12 13:44:25.473000: I runner.py:310] Step = 71800 ; steps/s = 0.72, tokens/s = 34069 (16415 source, 17654 target) ; Learning rate = 0.000330 ; Loss = 1.501894\n",
      "2024-12-12 13:46:44.574000: I runner.py:310] Step = 71900 ; steps/s = 0.72, tokens/s = 34072 (16412 source, 17660 target) ; Learning rate = 0.000330 ; Loss = 1.497712\n",
      "2024-12-12 13:49:03.683000: I runner.py:310] Step = 72000 ; steps/s = 0.72, tokens/s = 34064 (16418 source, 17646 target) ; Learning rate = 0.000329 ; Loss = 1.506138\n",
      "2024-12-12 13:51:22.795000: I runner.py:310] Step = 72100 ; steps/s = 0.72, tokens/s = 34065 (16416 source, 17649 target) ; Learning rate = 0.000329 ; Loss = 1.500064\n",
      "2024-12-12 13:53:39.561000: I runner.py:310] Step = 72200 ; steps/s = 0.73, tokens/s = 33948 (16347 source, 17601 target) ; Learning rate = 0.000329 ; Loss = 1.492339\n",
      "2024-12-12 13:55:58.654000: I runner.py:310] Step = 72300 ; steps/s = 0.72, tokens/s = 34063 (16408 source, 17655 target) ; Learning rate = 0.000329 ; Loss = 1.505689\n",
      "2024-12-12 13:58:17.726000: I runner.py:310] Step = 72400 ; steps/s = 0.72, tokens/s = 34081 (16427 source, 17654 target) ; Learning rate = 0.000328 ; Loss = 1.496330\n",
      "2024-12-12 14:00:36.814000: I runner.py:310] Step = 72500 ; steps/s = 0.72, tokens/s = 34070 (16421 source, 17649 target) ; Learning rate = 0.000328 ; Loss = 1.503151\n",
      "2024-12-12 14:02:53.597000: I runner.py:310] Step = 72600 ; steps/s = 0.73, tokens/s = 33947 (16349 source, 17598 target) ; Learning rate = 0.000328 ; Loss = 1.494621\n",
      "2024-12-12 14:05:12.688000: I runner.py:310] Step = 72700 ; steps/s = 0.72, tokens/s = 34072 (16407 source, 17665 target) ; Learning rate = 0.000328 ; Loss = 1.503112\n",
      "2024-12-12 14:07:31.782000: I runner.py:310] Step = 72800 ; steps/s = 0.72, tokens/s = 34072 (16424 source, 17648 target) ; Learning rate = 0.000328 ; Loss = 1.506033\n",
      "2024-12-12 14:09:50.846000: I runner.py:310] Step = 72900 ; steps/s = 0.72, tokens/s = 34076 (16426 source, 17650 target) ; Learning rate = 0.000327 ; Loss = 1.501997\n",
      "2024-12-12 14:12:07.674000: I runner.py:310] Step = 73000 ; steps/s = 0.73, tokens/s = 33928 (16344 source, 17584 target) ; Learning rate = 0.000327 ; Loss = 1.515297\n",
      "2024-12-12 14:14:26.704000: I runner.py:310] Step = 73100 ; steps/s = 0.72, tokens/s = 34086 (16428 source, 17658 target) ; Learning rate = 0.000327 ; Loss = 1.505631\n",
      "2024-12-12 14:16:45.488000: I runner.py:310] Step = 73200 ; steps/s = 0.72, tokens/s = 34149 (16448 source, 17701 target) ; Learning rate = 0.000327 ; Loss = 1.498540\n",
      "2024-12-12 14:19:04.441000: I runner.py:310] Step = 73300 ; steps/s = 0.72, tokens/s = 34107 (16424 source, 17683 target) ; Learning rate = 0.000326 ; Loss = 1.498298\n",
      "2024-12-12 14:21:21.281000: I runner.py:310] Step = 73400 ; steps/s = 0.73, tokens/s = 33926 (16348 source, 17578 target) ; Learning rate = 0.000326 ; Loss = 1.498780\n",
      "2024-12-12 14:23:40.365000: I runner.py:310] Step = 73500 ; steps/s = 0.72, tokens/s = 34068 (16415 source, 17653 target) ; Learning rate = 0.000326 ; Loss = 1.500432\n",
      "2024-12-12 14:25:59.416000: I runner.py:310] Step = 73600 ; steps/s = 0.72, tokens/s = 34091 (16427 source, 17664 target) ; Learning rate = 0.000326 ; Loss = 1.506065\n",
      "2024-12-12 14:28:18.540000: I runner.py:310] Step = 73700 ; steps/s = 0.72, tokens/s = 34053 (16413 source, 17640 target) ; Learning rate = 0.000326 ; Loss = 1.502678\n",
      "2024-12-12 14:30:37.675000: I runner.py:310] Step = 73800 ; steps/s = 0.72, tokens/s = 34055 (16401 source, 17654 target) ; Learning rate = 0.000325 ; Loss = 1.507101\n",
      "2024-12-12 14:32:54.486000: I runner.py:310] Step = 73900 ; steps/s = 0.73, tokens/s = 33954 (16356 source, 17598 target) ; Learning rate = 0.000325 ; Loss = 1.503832\n",
      "2024-12-12 14:35:13.595000: I runner.py:310] Step = 74000 ; steps/s = 0.72, tokens/s = 34052 (16406 source, 17646 target) ; Learning rate = 0.000325 ; Loss = 1.498845\n",
      "2024-12-12 14:37:32.707000: I runner.py:310] Step = 74100 ; steps/s = 0.72, tokens/s = 34066 (16419 source, 17647 target) ; Learning rate = 0.000325 ; Loss = 1.500082\n",
      "2024-12-12 14:39:51.829000: I runner.py:310] Step = 74200 ; steps/s = 0.72, tokens/s = 34053 (16402 source, 17651 target) ; Learning rate = 0.000324 ; Loss = 1.510716\n",
      "2024-12-12 14:42:08.683000: I runner.py:310] Step = 74300 ; steps/s = 0.73, tokens/s = 33943 (16357 source, 17586 target) ; Learning rate = 0.000324 ; Loss = 1.501495\n",
      "2024-12-12 14:44:27.711000: I runner.py:310] Step = 74400 ; steps/s = 0.72, tokens/s = 34087 (16426 source, 17661 target) ; Learning rate = 0.000324 ; Loss = 1.496421\n",
      "2024-12-12 14:46:46.764000: I runner.py:310] Step = 74500 ; steps/s = 0.72, tokens/s = 34075 (16413 source, 17662 target) ; Learning rate = 0.000324 ; Loss = 1.498844\n",
      "2024-12-12 14:49:05.828000: I runner.py:310] Step = 74600 ; steps/s = 0.72, tokens/s = 34079 (16418 source, 17661 target) ; Learning rate = 0.000324 ; Loss = 1.498715\n",
      "2024-12-12 14:51:22.668000: I runner.py:310] Step = 74700 ; steps/s = 0.73, tokens/s = 33933 (16342 source, 17591 target) ; Learning rate = 0.000323 ; Loss = 1.509494\n",
      "2024-12-12 14:53:41.741000: I runner.py:310] Step = 74800 ; steps/s = 0.72, tokens/s = 34066 (16426 source, 17640 target) ; Learning rate = 0.000323 ; Loss = 1.502130\n",
      "2024-12-12 14:56:00.829000: I runner.py:310] Step = 74900 ; steps/s = 0.72, tokens/s = 34070 (16402 source, 17668 target) ; Learning rate = 0.000323 ; Loss = 1.497164\n",
      "2024-12-12 14:58:19.928000: I runner.py:310] Step = 75000 ; steps/s = 0.72, tokens/s = 34068 (16420 source, 17648 target) ; Learning rate = 0.000323 ; Loss = 1.495354\n",
      "2024-12-12 15:00:36.810000: I runner.py:310] Step = 75100 ; steps/s = 0.73, tokens/s = 33923 (16346 source, 17577 target) ; Learning rate = 0.000323 ; Loss = 1.498768\n",
      "2024-12-12 15:02:55.873000: I runner.py:310] Step = 75200 ; steps/s = 0.72, tokens/s = 34080 (16416 source, 17664 target) ; Learning rate = 0.000322 ; Loss = 1.500516\n",
      "2024-12-12 15:05:14.935000: I runner.py:310] Step = 75300 ; steps/s = 0.72, tokens/s = 34067 (16408 source, 17659 target) ; Learning rate = 0.000322 ; Loss = 1.501148\n",
      "2024-12-12 15:07:34.005000: I runner.py:310] Step = 75400 ; steps/s = 0.72, tokens/s = 34087 (16425 source, 17662 target) ; Learning rate = 0.000322 ; Loss = 1.499690\n",
      "2024-12-12 15:09:52.855000: I runner.py:310] Step = 75500 ; steps/s = 0.72, tokens/s = 34126 (16446 source, 17680 target) ; Learning rate = 0.000322 ; Loss = 1.503250\n",
      "2024-12-12 15:12:09.422000: I runner.py:310] Step = 75600 ; steps/s = 0.73, tokens/s = 33996 (16382 source, 17614 target) ; Learning rate = 0.000321 ; Loss = 1.486813\n",
      "2024-12-12 15:14:28.563000: I runner.py:310] Step = 75700 ; steps/s = 0.72, tokens/s = 34060 (16403 source, 17657 target) ; Learning rate = 0.000321 ; Loss = 1.502962\n",
      "2024-12-12 15:16:47.609000: I runner.py:310] Step = 75800 ; steps/s = 0.72, tokens/s = 34068 (16420 source, 17648 target) ; Learning rate = 0.000321 ; Loss = 1.507342\n",
      "2024-12-12 15:19:06.694000: I runner.py:310] Step = 75900 ; steps/s = 0.72, tokens/s = 34086 (16424 source, 17662 target) ; Learning rate = 0.000321 ; Loss = 1.500702\n",
      "2024-12-12 15:21:23.555000: I runner.py:310] Step = 76000 ; steps/s = 0.73, tokens/s = 33924 (16335 source, 17589 target) ; Learning rate = 0.000321 ; Loss = 1.495638\n",
      "2024-12-12 15:23:42.610000: I runner.py:310] Step = 76100 ; steps/s = 0.72, tokens/s = 34077 (16418 source, 17659 target) ; Learning rate = 0.000320 ; Loss = 1.500620\n",
      "2024-12-12 15:26:01.688000: I runner.py:310] Step = 76200 ; steps/s = 0.72, tokens/s = 34082 (16422 source, 17660 target) ; Learning rate = 0.000320 ; Loss = 1.497725\n",
      "2024-12-12 15:28:20.802000: I runner.py:310] Step = 76300 ; steps/s = 0.72, tokens/s = 34058 (16418 source, 17640 target) ; Learning rate = 0.000320 ; Loss = 1.499857\n",
      "2024-12-12 15:30:37.616000: I runner.py:310] Step = 76400 ; steps/s = 0.73, tokens/s = 33942 (16345 source, 17597 target) ; Learning rate = 0.000320 ; Loss = 1.498442\n",
      "2024-12-12 15:32:56.711000: I runner.py:310] Step = 76500 ; steps/s = 0.72, tokens/s = 34061 (16413 source, 17648 target) ; Learning rate = 0.000320 ; Loss = 1.494659\n",
      "2024-12-12 15:35:15.809000: I runner.py:310] Step = 76600 ; steps/s = 0.72, tokens/s = 34067 (16423 source, 17644 target) ; Learning rate = 0.000319 ; Loss = 1.495245\n",
      "2024-12-12 15:37:34.904000: I runner.py:310] Step = 76700 ; steps/s = 0.72, tokens/s = 34068 (16412 source, 17656 target) ; Learning rate = 0.000319 ; Loss = 1.505385\n",
      "2024-12-12 15:39:54.010000: I runner.py:310] Step = 76800 ; steps/s = 0.72, tokens/s = 34073 (16410 source, 17663 target) ; Learning rate = 0.000319 ; Loss = 1.513126\n",
      "2024-12-12 15:42:10.788000: I runner.py:310] Step = 76900 ; steps/s = 0.73, tokens/s = 33958 (16366 source, 17592 target) ; Learning rate = 0.000319 ; Loss = 1.498771\n",
      "2024-12-12 15:44:29.896000: I runner.py:310] Step = 77000 ; steps/s = 0.72, tokens/s = 34076 (16407 source, 17669 target) ; Learning rate = 0.000319 ; Loss = 1.496002\n",
      "2024-12-12 15:46:48.945000: I runner.py:310] Step = 77100 ; steps/s = 0.72, tokens/s = 34071 (16413 source, 17658 target) ; Learning rate = 0.000318 ; Loss = 1.495742\n",
      "2024-12-12 15:49:08.098000: I runner.py:310] Step = 77200 ; steps/s = 0.72, tokens/s = 34048 (16414 source, 17634 target) ; Learning rate = 0.000318 ; Loss = 1.501292\n",
      "2024-12-12 15:51:24.970000: I runner.py:310] Step = 77300 ; steps/s = 0.73, tokens/s = 33924 (16342 source, 17582 target) ; Learning rate = 0.000318 ; Loss = 1.493830\n",
      "2024-12-12 15:53:44.040000: I runner.py:310] Step = 77400 ; steps/s = 0.72, tokens/s = 34074 (16416 source, 17658 target) ; Learning rate = 0.000318 ; Loss = 1.495429\n",
      "2024-12-12 15:56:03.115000: I runner.py:310] Step = 77500 ; steps/s = 0.72, tokens/s = 34069 (16404 source, 17665 target) ; Learning rate = 0.000317 ; Loss = 1.494749\n",
      "2024-12-12 15:58:22.189000: I runner.py:310] Step = 77600 ; steps/s = 0.72, tokens/s = 34064 (16414 source, 17650 target) ; Learning rate = 0.000317 ; Loss = 1.496456\n",
      "2024-12-12 16:00:39.054000: I runner.py:310] Step = 77700 ; steps/s = 0.73, tokens/s = 33939 (16361 source, 17578 target) ; Learning rate = 0.000317 ; Loss = 1.499868\n",
      "2024-12-12 16:02:57.951000: I runner.py:310] Step = 77800 ; steps/s = 0.72, tokens/s = 34113 (16433 source, 17680 target) ; Learning rate = 0.000317 ; Loss = 1.496655\n",
      "2024-12-12 16:05:16.618000: I runner.py:310] Step = 77900 ; steps/s = 0.72, tokens/s = 34174 (16472 source, 17702 target) ; Learning rate = 0.000317 ; Loss = 1.497097\n",
      "2024-12-12 16:07:35.320000: I runner.py:310] Step = 78000 ; steps/s = 0.72, tokens/s = 34165 (16452 source, 17713 target) ; Learning rate = 0.000316 ; Loss = 1.498693\n",
      "2024-12-12 16:09:51.769000: I runner.py:310] Step = 78100 ; steps/s = 0.73, tokens/s = 34036 (16404 source, 17632 target) ; Learning rate = 0.000316 ; Loss = 1.495337\n",
      "2024-12-12 16:12:10.546000: I runner.py:310] Step = 78200 ; steps/s = 0.72, tokens/s = 34154 (16458 source, 17696 target) ; Learning rate = 0.000316 ; Loss = 1.498817\n",
      "2024-12-12 16:14:29.614000: I runner.py:310] Step = 78300 ; steps/s = 0.72, tokens/s = 34080 (16418 source, 17662 target) ; Learning rate = 0.000316 ; Loss = 1.497744\n",
      "2024-12-12 16:16:48.700000: I runner.py:310] Step = 78400 ; steps/s = 0.72, tokens/s = 34072 (16421 source, 17651 target) ; Learning rate = 0.000316 ; Loss = 1.499694\n",
      "2024-12-12 16:19:07.810000: I runner.py:310] Step = 78500 ; steps/s = 0.72, tokens/s = 34054 (16402 source, 17652 target) ; Learning rate = 0.000315 ; Loss = 1.494809\n",
      "2024-12-12 16:21:24.669000: I runner.py:310] Step = 78600 ; steps/s = 0.73, tokens/s = 33925 (16342 source, 17583 target) ; Learning rate = 0.000315 ; Loss = 1.513768\n",
      "2024-12-12 16:23:43.781000: I runner.py:310] Step = 78700 ; steps/s = 0.72, tokens/s = 34075 (16420 source, 17655 target) ; Learning rate = 0.000315 ; Loss = 1.496833\n",
      "2024-12-12 16:26:02.845000: I runner.py:310] Step = 78800 ; steps/s = 0.72, tokens/s = 34071 (16418 source, 17653 target) ; Learning rate = 0.000315 ; Loss = 1.490941\n",
      "2024-12-12 16:28:21.909000: I runner.py:310] Step = 78900 ; steps/s = 0.72, tokens/s = 34069 (16413 source, 17656 target) ; Learning rate = 0.000315 ; Loss = 1.490680\n",
      "2024-12-12 16:30:38.805000: I runner.py:310] Step = 79000 ; steps/s = 0.73, tokens/s = 33918 (16337 source, 17581 target) ; Learning rate = 0.000314 ; Loss = 1.497210\n",
      "2024-12-12 16:32:57.893000: I runner.py:310] Step = 79100 ; steps/s = 0.72, tokens/s = 34060 (16406 source, 17654 target) ; Learning rate = 0.000314 ; Loss = 1.497275\n",
      "2024-12-12 16:35:17.015000: I runner.py:310] Step = 79200 ; steps/s = 0.72, tokens/s = 34061 (16413 source, 17648 target) ; Learning rate = 0.000314 ; Loss = 1.498099\n",
      "2024-12-12 16:37:36.090000: I runner.py:310] Step = 79300 ; steps/s = 0.72, tokens/s = 34090 (16418 source, 17672 target) ; Learning rate = 0.000314 ; Loss = 1.499728\n",
      "2024-12-12 16:39:52.925000: I runner.py:310] Step = 79400 ; steps/s = 0.73, tokens/s = 33924 (16350 source, 17574 target) ; Learning rate = 0.000314 ; Loss = 1.496228\n",
      "2024-12-12 16:42:12.034000: I runner.py:310] Step = 79500 ; steps/s = 0.72, tokens/s = 34069 (16414 source, 17655 target) ; Learning rate = 0.000313 ; Loss = 1.502571\n",
      "2024-12-12 16:44:31.115000: I runner.py:310] Step = 79600 ; steps/s = 0.72, tokens/s = 34064 (16414 source, 17650 target) ; Learning rate = 0.000313 ; Loss = 1.499267\n",
      "2024-12-12 16:46:50.205000: I runner.py:310] Step = 79700 ; steps/s = 0.72, tokens/s = 34071 (16413 source, 17658 target) ; Learning rate = 0.000313 ; Loss = 1.496529\n",
      "2024-12-12 16:49:07.052000: I runner.py:310] Step = 79800 ; steps/s = 0.73, tokens/s = 33942 (16359 source, 17583 target) ; Learning rate = 0.000313 ; Loss = 1.489744\n",
      "2024-12-12 16:51:26.158000: I runner.py:310] Step = 79900 ; steps/s = 0.72, tokens/s = 34069 (16409 source, 17660 target) ; Learning rate = 0.000313 ; Loss = 1.499637\n",
      "2024-12-12 16:53:45.243000: I runner.py:310] Step = 80000 ; steps/s = 0.72, tokens/s = 34079 (16429 source, 17650 target) ; Learning rate = 0.000312 ; Loss = 1.496692\n",
      "2024-12-12 16:53:46.976000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-80000\n",
      "2024-12-12 16:56:05.722000: I runner.py:310] Step = 80100 ; steps/s = 0.72, tokens/s = 34146 (16449 source, 17697 target) ; Learning rate = 0.000312 ; Loss = 1.490598\n",
      "2024-12-12 16:58:24.574000: I runner.py:310] Step = 80200 ; steps/s = 0.72, tokens/s = 34114 (16435 source, 17679 target) ; Learning rate = 0.000312 ; Loss = 1.493370\n",
      "2024-12-12 17:00:41.379000: I runner.py:310] Step = 80300 ; steps/s = 0.73, tokens/s = 33964 (16355 source, 17609 target) ; Learning rate = 0.000312 ; Loss = 1.486996\n",
      "2024-12-12 17:03:00.491000: I runner.py:310] Step = 80400 ; steps/s = 0.72, tokens/s = 34060 (16411 source, 17649 target) ; Learning rate = 0.000312 ; Loss = 1.500672\n",
      "2024-12-12 17:05:19.599000: I runner.py:310] Step = 80500 ; steps/s = 0.72, tokens/s = 34067 (16416 source, 17651 target) ; Learning rate = 0.000312 ; Loss = 1.499049\n",
      "2024-12-12 17:07:38.689000: I runner.py:310] Step = 80600 ; steps/s = 0.72, tokens/s = 34072 (16415 source, 17657 target) ; Learning rate = 0.000311 ; Loss = 1.502578\n",
      "2024-12-12 17:09:55.477000: I runner.py:310] Step = 80700 ; steps/s = 0.73, tokens/s = 33925 (16348 source, 17577 target) ; Learning rate = 0.000311 ; Loss = 1.503794\n",
      "2024-12-12 17:12:14.586000: I runner.py:310] Step = 80800 ; steps/s = 0.72, tokens/s = 34078 (16420 source, 17658 target) ; Learning rate = 0.000311 ; Loss = 1.496704\n",
      "2024-12-12 17:14:33.707000: I runner.py:310] Step = 80900 ; steps/s = 0.72, tokens/s = 34055 (16405 source, 17650 target) ; Learning rate = 0.000311 ; Loss = 1.494837\n",
      "2024-12-12 17:16:52.807000: I runner.py:310] Step = 81000 ; steps/s = 0.72, tokens/s = 34077 (16427 source, 17650 target) ; Learning rate = 0.000311 ; Loss = 1.496059\n",
      "2024-12-12 17:19:09.642000: I runner.py:310] Step = 81100 ; steps/s = 0.73, tokens/s = 33931 (16338 source, 17593 target) ; Learning rate = 0.000310 ; Loss = 1.494833\n",
      "2024-12-12 17:21:28.715000: I runner.py:310] Step = 81200 ; steps/s = 0.72, tokens/s = 34076 (16415 source, 17661 target) ; Learning rate = 0.000310 ; Loss = 1.493361\n",
      "2024-12-12 17:23:47.809000: I runner.py:310] Step = 81300 ; steps/s = 0.72, tokens/s = 34072 (16421 source, 17651 target) ; Learning rate = 0.000310 ; Loss = 1.491681\n",
      "2024-12-12 17:26:06.870000: I runner.py:310] Step = 81400 ; steps/s = 0.72, tokens/s = 34087 (16426 source, 17661 target) ; Learning rate = 0.000310 ; Loss = 1.499003\n",
      "2024-12-12 17:28:23.763000: I runner.py:310] Step = 81500 ; steps/s = 0.73, tokens/s = 33901 (16336 source, 17565 target) ; Learning rate = 0.000310 ; Loss = 1.496319\n",
      "2024-12-12 17:30:42.862000: I runner.py:310] Step = 81600 ; steps/s = 0.72, tokens/s = 34076 (16413 source, 17663 target) ; Learning rate = 0.000309 ; Loss = 1.496980\n",
      "2024-12-12 17:33:01.978000: I runner.py:310] Step = 81700 ; steps/s = 0.72, tokens/s = 34063 (16416 source, 17647 target) ; Learning rate = 0.000309 ; Loss = 1.494706\n",
      "2024-12-12 17:35:21.044000: I runner.py:310] Step = 81800 ; steps/s = 0.72, tokens/s = 34082 (16420 source, 17662 target) ; Learning rate = 0.000309 ; Loss = 1.491191\n",
      "2024-12-12 17:37:40.143000: I runner.py:310] Step = 81900 ; steps/s = 0.72, tokens/s = 34059 (16403 source, 17656 target) ; Learning rate = 0.000309 ; Loss = 1.501653\n",
      "2024-12-12 17:39:56.949000: I runner.py:310] Step = 82000 ; steps/s = 0.73, tokens/s = 33935 (16353 source, 17582 target) ; Learning rate = 0.000309 ; Loss = 1.491222\n",
      "2024-12-12 17:42:16.007000: I runner.py:310] Step = 82100 ; steps/s = 0.72, tokens/s = 34074 (16412 source, 17662 target) ; Learning rate = 0.000308 ; Loss = 1.500732\n",
      "2024-12-12 17:44:35.114000: I runner.py:310] Step = 82200 ; steps/s = 0.72, tokens/s = 34071 (16415 source, 17656 target) ; Learning rate = 0.000308 ; Loss = 1.497106\n",
      "2024-12-12 17:46:54.239000: I runner.py:310] Step = 82300 ; steps/s = 0.72, tokens/s = 34068 (16425 source, 17643 target) ; Learning rate = 0.000308 ; Loss = 1.501029\n",
      "2024-12-12 17:49:10.858000: I runner.py:310] Step = 82400 ; steps/s = 0.73, tokens/s = 33987 (16370 source, 17617 target) ; Learning rate = 0.000308 ; Loss = 1.495868\n",
      "2024-12-12 17:51:29.690000: I runner.py:310] Step = 82500 ; steps/s = 0.72, tokens/s = 34124 (16430 source, 17694 target) ; Learning rate = 0.000308 ; Loss = 1.492861\n",
      "2024-12-12 17:53:48.803000: I runner.py:310] Step = 82600 ; steps/s = 0.72, tokens/s = 34069 (16430 source, 17639 target) ; Learning rate = 0.000308 ; Loss = 1.493532\n",
      "2024-12-12 17:56:07.905000: I runner.py:310] Step = 82700 ; steps/s = 0.72, tokens/s = 34068 (16412 source, 17656 target) ; Learning rate = 0.000307 ; Loss = 1.492518\n",
      "2024-12-12 17:58:24.682000: I runner.py:310] Step = 82800 ; steps/s = 0.73, tokens/s = 33952 (16350 source, 17602 target) ; Learning rate = 0.000307 ; Loss = 1.496884\n",
      "2024-12-12 18:00:43.755000: I runner.py:310] Step = 82900 ; steps/s = 0.72, tokens/s = 34091 (16432 source, 17659 target) ; Learning rate = 0.000307 ; Loss = 1.496227\n",
      "2024-12-12 18:03:02.865000: I runner.py:310] Step = 83000 ; steps/s = 0.72, tokens/s = 34062 (16415 source, 17647 target) ; Learning rate = 0.000307 ; Loss = 1.492704\n",
      "2024-12-12 18:05:21.963000: I runner.py:310] Step = 83100 ; steps/s = 0.72, tokens/s = 34061 (16410 source, 17651 target) ; Learning rate = 0.000307 ; Loss = 1.501434\n",
      "2024-12-12 18:07:41.064000: I runner.py:310] Step = 83200 ; steps/s = 0.72, tokens/s = 34059 (16402 source, 17657 target) ; Learning rate = 0.000306 ; Loss = 1.497782\n",
      "2024-12-12 18:09:57.903000: I runner.py:310] Step = 83300 ; steps/s = 0.73, tokens/s = 33928 (16341 source, 17587 target) ; Learning rate = 0.000306 ; Loss = 1.500392\n",
      "2024-12-12 18:12:17.022000: I runner.py:310] Step = 83400 ; steps/s = 0.72, tokens/s = 34063 (16420 source, 17643 target) ; Learning rate = 0.000306 ; Loss = 1.496204\n",
      "2024-12-12 18:14:36.068000: I runner.py:310] Step = 83500 ; steps/s = 0.72, tokens/s = 34086 (16426 source, 17660 target) ; Learning rate = 0.000306 ; Loss = 1.493059\n",
      "2024-12-12 18:16:55.154000: I runner.py:310] Step = 83600 ; steps/s = 0.72, tokens/s = 34066 (16413 source, 17653 target) ; Learning rate = 0.000306 ; Loss = 1.493841\n",
      "2024-12-12 18:19:12.008000: I runner.py:310] Step = 83700 ; steps/s = 0.73, tokens/s = 33929 (16338 source, 17591 target) ; Learning rate = 0.000306 ; Loss = 1.495425\n",
      "2024-12-12 18:21:31.098000: I runner.py:310] Step = 83800 ; steps/s = 0.72, tokens/s = 34057 (16412 source, 17645 target) ; Learning rate = 0.000305 ; Loss = 1.492920\n",
      "2024-12-12 18:23:50.185000: I runner.py:310] Step = 83900 ; steps/s = 0.72, tokens/s = 34077 (16415 source, 17662 target) ; Learning rate = 0.000305 ; Loss = 1.493723\n",
      "2024-12-12 18:26:09.308000: I runner.py:310] Step = 84000 ; steps/s = 0.72, tokens/s = 34072 (16411 source, 17661 target) ; Learning rate = 0.000305 ; Loss = 1.496177\n",
      "2024-12-12 18:28:26.175000: I runner.py:310] Step = 84100 ; steps/s = 0.73, tokens/s = 33932 (16353 source, 17579 target) ; Learning rate = 0.000305 ; Loss = 1.493681\n",
      "2024-12-12 18:30:45.245000: I runner.py:310] Step = 84200 ; steps/s = 0.72, tokens/s = 34073 (16409 source, 17664 target) ; Learning rate = 0.000305 ; Loss = 1.489020\n",
      "2024-12-12 18:33:04.355000: I runner.py:310] Step = 84300 ; steps/s = 0.72, tokens/s = 34077 (16429 source, 17648 target) ; Learning rate = 0.000304 ; Loss = 1.492650\n",
      "2024-12-12 18:35:23.497000: I runner.py:310] Step = 84400 ; steps/s = 0.72, tokens/s = 34051 (16404 source, 17647 target) ; Learning rate = 0.000304 ; Loss = 1.494263\n",
      "2024-12-12 18:37:40.326000: I runner.py:310] Step = 84500 ; steps/s = 0.73, tokens/s = 33932 (16355 source, 17577 target) ; Learning rate = 0.000304 ; Loss = 1.490198\n",
      "2024-12-12 18:39:59.428000: I runner.py:310] Step = 84600 ; steps/s = 0.72, tokens/s = 34072 (16414 source, 17658 target) ; Learning rate = 0.000304 ; Loss = 1.494654\n",
      "2024-12-12 18:42:18.388000: I runner.py:310] Step = 84700 ; steps/s = 0.72, tokens/s = 34101 (16433 source, 17668 target) ; Learning rate = 0.000304 ; Loss = 1.486452\n",
      "2024-12-12 18:44:37.241000: I runner.py:310] Step = 84800 ; steps/s = 0.72, tokens/s = 34130 (16451 source, 17679 target) ; Learning rate = 0.000304 ; Loss = 1.497077\n",
      "2024-12-12 18:46:56.323000: I runner.py:310] Step = 84900 ; steps/s = 0.72, tokens/s = 34058 (16402 source, 17656 target) ; Learning rate = 0.000303 ; Loss = 1.493147\n",
      "2024-12-12 18:49:13.100000: I runner.py:310] Step = 85000 ; steps/s = 0.73, tokens/s = 33958 (16360 source, 17598 target) ; Learning rate = 0.000303 ; Loss = 1.496210\n",
      "2024-12-12 18:51:32.181000: I runner.py:310] Step = 85100 ; steps/s = 0.72, tokens/s = 34076 (16407 source, 17669 target) ; Learning rate = 0.000303 ; Loss = 1.492477\n",
      "2024-12-12 18:53:51.257000: I runner.py:310] Step = 85200 ; steps/s = 0.72, tokens/s = 34071 (16414 source, 17657 target) ; Learning rate = 0.000303 ; Loss = 1.490736\n",
      "2024-12-12 18:56:10.368000: I runner.py:310] Step = 85300 ; steps/s = 0.72, tokens/s = 34056 (16413 source, 17643 target) ; Learning rate = 0.000303 ; Loss = 1.491360\n",
      "2024-12-12 18:58:27.147000: I runner.py:310] Step = 85400 ; steps/s = 0.73, tokens/s = 33963 (16371 source, 17592 target) ; Learning rate = 0.000302 ; Loss = 1.494253\n",
      "2024-12-12 19:00:46.242000: I runner.py:310] Step = 85500 ; steps/s = 0.72, tokens/s = 34056 (16410 source, 17646 target) ; Learning rate = 0.000302 ; Loss = 1.491023\n",
      "2024-12-12 19:03:05.379000: I runner.py:310] Step = 85600 ; steps/s = 0.72, tokens/s = 34044 (16399 source, 17645 target) ; Learning rate = 0.000302 ; Loss = 1.499261\n",
      "2024-12-12 19:05:24.457000: I runner.py:310] Step = 85700 ; steps/s = 0.72, tokens/s = 34084 (16428 source, 17656 target) ; Learning rate = 0.000302 ; Loss = 1.489724\n",
      "2024-12-12 19:07:41.235000: I runner.py:310] Step = 85800 ; steps/s = 0.73, tokens/s = 33950 (16354 source, 17596 target) ; Learning rate = 0.000302 ; Loss = 1.497446\n",
      "2024-12-12 19:10:00.299000: I runner.py:310] Step = 85900 ; steps/s = 0.72, tokens/s = 34073 (16407 source, 17666 target) ; Learning rate = 0.000302 ; Loss = 1.500475\n",
      "2024-12-12 19:12:19.388000: I runner.py:310] Step = 86000 ; steps/s = 0.72, tokens/s = 34081 (16421 source, 17660 target) ; Learning rate = 0.000301 ; Loss = 1.489459\n",
      "2024-12-12 19:14:38.480000: I runner.py:310] Step = 86100 ; steps/s = 0.72, tokens/s = 34069 (16415 source, 17654 target) ; Learning rate = 0.000301 ; Loss = 1.495200\n",
      "2024-12-12 19:16:55.266000: I runner.py:310] Step = 86200 ; steps/s = 0.73, tokens/s = 33940 (16356 source, 17584 target) ; Learning rate = 0.000301 ; Loss = 1.488420\n",
      "2024-12-12 19:19:14.377000: I runner.py:310] Step = 86300 ; steps/s = 0.72, tokens/s = 34070 (16412 source, 17658 target) ; Learning rate = 0.000301 ; Loss = 1.501127\n",
      "2024-12-12 19:21:33.476000: I runner.py:310] Step = 86400 ; steps/s = 0.72, tokens/s = 34062 (16417 source, 17645 target) ; Learning rate = 0.000301 ; Loss = 1.487575\n",
      "2024-12-12 19:23:52.585000: I runner.py:310] Step = 86500 ; steps/s = 0.72, tokens/s = 34072 (16419 source, 17653 target) ; Learning rate = 0.000301 ; Loss = 1.495829\n",
      "2024-12-12 19:26:11.697000: I runner.py:310] Step = 86600 ; steps/s = 0.72, tokens/s = 34055 (16401 source, 17654 target) ; Learning rate = 0.000300 ; Loss = 1.492577\n",
      "2024-12-12 19:28:28.563000: I runner.py:310] Step = 86700 ; steps/s = 0.73, tokens/s = 33926 (16338 source, 17588 target) ; Learning rate = 0.000300 ; Loss = 1.484482\n",
      "2024-12-12 19:30:47.674000: I runner.py:310] Step = 86800 ; steps/s = 0.72, tokens/s = 34069 (16411 source, 17658 target) ; Learning rate = 0.000300 ; Loss = 1.489114\n",
      "2024-12-12 19:33:06.797000: I runner.py:310] Step = 86900 ; steps/s = 0.72, tokens/s = 34057 (16415 source, 17642 target) ; Learning rate = 0.000300 ; Loss = 1.495997\n",
      "2024-12-12 19:35:25.745000: I runner.py:310] Step = 87000 ; steps/s = 0.72, tokens/s = 34116 (16432 source, 17684 target) ; Learning rate = 0.000300 ; Loss = 1.497347\n",
      "2024-12-12 19:37:42.197000: I runner.py:310] Step = 87100 ; steps/s = 0.73, tokens/s = 34022 (16402 source, 17620 target) ; Learning rate = 0.000299 ; Loss = 1.490091\n",
      "2024-12-12 19:40:01.281000: I runner.py:310] Step = 87200 ; steps/s = 0.72, tokens/s = 34084 (16427 source, 17657 target) ; Learning rate = 0.000299 ; Loss = 1.489775\n",
      "2024-12-12 19:42:20.346000: I runner.py:310] Step = 87300 ; steps/s = 0.72, tokens/s = 34076 (16421 source, 17655 target) ; Learning rate = 0.000299 ; Loss = 1.496018\n",
      "2024-12-12 19:44:39.422000: I runner.py:310] Step = 87400 ; steps/s = 0.72, tokens/s = 34068 (16414 source, 17654 target) ; Learning rate = 0.000299 ; Loss = 1.492791\n",
      "2024-12-12 19:46:56.228000: I runner.py:310] Step = 87500 ; steps/s = 0.73, tokens/s = 33939 (16347 source, 17592 target) ; Learning rate = 0.000299 ; Loss = 1.492330\n",
      "2024-12-12 19:49:15.299000: I runner.py:310] Step = 87600 ; steps/s = 0.72, tokens/s = 34070 (16416 source, 17654 target) ; Learning rate = 0.000299 ; Loss = 1.488945\n",
      "2024-12-12 19:51:34.365000: I runner.py:310] Step = 87700 ; steps/s = 0.72, tokens/s = 34080 (16408 source, 17672 target) ; Learning rate = 0.000298 ; Loss = 1.488660\n",
      "2024-12-12 19:53:53.448000: I runner.py:310] Step = 87800 ; steps/s = 0.72, tokens/s = 34061 (16414 source, 17647 target) ; Learning rate = 0.000298 ; Loss = 1.491323\n",
      "2024-12-12 19:56:10.300000: I runner.py:310] Step = 87900 ; steps/s = 0.73, tokens/s = 33930 (16355 source, 17575 target) ; Learning rate = 0.000298 ; Loss = 1.489320\n",
      "2024-12-12 19:58:29.365000: I runner.py:310] Step = 88000 ; steps/s = 0.72, tokens/s = 34083 (16416 source, 17667 target) ; Learning rate = 0.000298 ; Loss = 1.485269\n",
      "2024-12-12 20:00:48.416000: I runner.py:310] Step = 88100 ; steps/s = 0.72, tokens/s = 34082 (16420 source, 17662 target) ; Learning rate = 0.000298 ; Loss = 1.491685\n",
      "2024-12-12 20:03:07.530000: I runner.py:310] Step = 88200 ; steps/s = 0.72, tokens/s = 34045 (16403 source, 17642 target) ; Learning rate = 0.000298 ; Loss = 1.498709\n",
      "2024-12-12 20:05:26.598000: I runner.py:310] Step = 88300 ; steps/s = 0.72, tokens/s = 34082 (16419 source, 17663 target) ; Learning rate = 0.000297 ; Loss = 1.493793\n",
      "2024-12-12 20:07:43.379000: I runner.py:310] Step = 88400 ; steps/s = 0.73, tokens/s = 33955 (16365 source, 17590 target) ; Learning rate = 0.000297 ; Loss = 1.498188\n",
      "2024-12-12 20:10:02.453000: I runner.py:310] Step = 88500 ; steps/s = 0.72, tokens/s = 34091 (16414 source, 17677 target) ; Learning rate = 0.000297 ; Loss = 1.482980\n",
      "2024-12-12 20:12:21.524000: I runner.py:310] Step = 88600 ; steps/s = 0.72, tokens/s = 34056 (16409 source, 17647 target) ; Learning rate = 0.000297 ; Loss = 1.493174\n",
      "2024-12-12 20:14:40.632000: I runner.py:310] Step = 88700 ; steps/s = 0.72, tokens/s = 34064 (16422 source, 17642 target) ; Learning rate = 0.000297 ; Loss = 1.481050\n",
      "2024-12-12 20:16:57.455000: I runner.py:310] Step = 88800 ; steps/s = 0.73, tokens/s = 33935 (16353 source, 17582 target) ; Learning rate = 0.000297 ; Loss = 1.485081\n",
      "2024-12-12 20:19:16.520000: I runner.py:310] Step = 88900 ; steps/s = 0.72, tokens/s = 34075 (16410 source, 17665 target) ; Learning rate = 0.000296 ; Loss = 1.494169\n",
      "2024-12-12 20:21:35.620000: I runner.py:310] Step = 89000 ; steps/s = 0.72, tokens/s = 34065 (16409 source, 17656 target) ; Learning rate = 0.000296 ; Loss = 1.490162\n",
      "2024-12-12 20:23:54.722000: I runner.py:310] Step = 89100 ; steps/s = 0.72, tokens/s = 34074 (16429 source, 17645 target) ; Learning rate = 0.000296 ; Loss = 1.492790\n",
      "2024-12-12 20:26:11.553000: I runner.py:310] Step = 89200 ; steps/s = 0.73, tokens/s = 33940 (16340 source, 17600 target) ; Learning rate = 0.000296 ; Loss = 1.486758\n",
      "2024-12-12 20:28:30.542000: I runner.py:310] Step = 89300 ; steps/s = 0.72, tokens/s = 34102 (16424 source, 17678 target) ; Learning rate = 0.000296 ; Loss = 1.487179\n",
      "2024-12-12 20:30:49.377000: I runner.py:310] Step = 89400 ; steps/s = 0.72, tokens/s = 34127 (16444 source, 17683 target) ; Learning rate = 0.000296 ; Loss = 1.487169\n",
      "2024-12-12 20:33:08.438000: I runner.py:310] Step = 89500 ; steps/s = 0.72, tokens/s = 34077 (16426 source, 17651 target) ; Learning rate = 0.000295 ; Loss = 1.491555\n",
      "2024-12-12 20:35:27.502000: I runner.py:310] Step = 89600 ; steps/s = 0.72, tokens/s = 34069 (16417 source, 17652 target) ; Learning rate = 0.000295 ; Loss = 1.496208\n",
      "2024-12-12 20:37:44.299000: I runner.py:310] Step = 89700 ; steps/s = 0.73, tokens/s = 33946 (16356 source, 17590 target) ; Learning rate = 0.000295 ; Loss = 1.493651\n",
      "2024-12-12 20:40:03.391000: I runner.py:310] Step = 89800 ; steps/s = 0.72, tokens/s = 34062 (16416 source, 17646 target) ; Learning rate = 0.000295 ; Loss = 1.489641\n",
      "2024-12-12 20:42:22.445000: I runner.py:310] Step = 89900 ; steps/s = 0.72, tokens/s = 34097 (16419 source, 17678 target) ; Learning rate = 0.000295 ; Loss = 1.490605\n",
      "2024-12-12 20:44:41.573000: I runner.py:310] Step = 90000 ; steps/s = 0.72, tokens/s = 34046 (16403 source, 17643 target) ; Learning rate = 0.000295 ; Loss = 1.494984\n",
      "2024-12-12 20:44:43.611000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-90000\n",
      "2024-12-12 20:47:00.251000: I runner.py:310] Step = 90100 ; steps/s = 0.73, tokens/s = 33974 (16372 source, 17602 target) ; Learning rate = 0.000294 ; Loss = 1.484838\n",
      "2024-12-12 20:49:19.348000: I runner.py:310] Step = 90200 ; steps/s = 0.72, tokens/s = 34074 (16413 source, 17661 target) ; Learning rate = 0.000294 ; Loss = 1.484555\n",
      "2024-12-12 20:51:38.422000: I runner.py:310] Step = 90300 ; steps/s = 0.72, tokens/s = 34063 (16413 source, 17650 target) ; Learning rate = 0.000294 ; Loss = 1.495588\n",
      "2024-12-12 20:53:57.537000: I runner.py:310] Step = 90400 ; steps/s = 0.72, tokens/s = 34063 (16415 source, 17648 target) ; Learning rate = 0.000294 ; Loss = 1.495063\n",
      "2024-12-12 20:56:14.377000: I runner.py:310] Step = 90500 ; steps/s = 0.73, tokens/s = 33954 (16359 source, 17595 target) ; Learning rate = 0.000294 ; Loss = 1.485542\n",
      "2024-12-12 20:58:33.463000: I runner.py:310] Step = 90600 ; steps/s = 0.72, tokens/s = 34083 (16418 source, 17665 target) ; Learning rate = 0.000294 ; Loss = 1.491997\n",
      "2024-12-12 21:00:52.490000: I runner.py:310] Step = 90700 ; steps/s = 0.72, tokens/s = 34073 (16408 source, 17665 target) ; Learning rate = 0.000293 ; Loss = 1.483012\n",
      "2024-12-12 21:03:11.576000: I runner.py:310] Step = 90800 ; steps/s = 0.72, tokens/s = 34054 (16412 source, 17642 target) ; Learning rate = 0.000293 ; Loss = 1.487985\n",
      "2024-12-12 21:05:28.390000: I runner.py:310] Step = 90900 ; steps/s = 0.73, tokens/s = 33949 (16363 source, 17586 target) ; Learning rate = 0.000293 ; Loss = 1.491337\n",
      "2024-12-12 21:07:47.459000: I runner.py:310] Step = 91000 ; steps/s = 0.72, tokens/s = 34083 (16426 source, 17657 target) ; Learning rate = 0.000293 ; Loss = 1.490000\n",
      "2024-12-12 21:10:06.552000: I runner.py:310] Step = 91100 ; steps/s = 0.72, tokens/s = 34076 (16413 source, 17663 target) ; Learning rate = 0.000293 ; Loss = 1.484565\n",
      "2024-12-12 21:12:25.660000: I runner.py:310] Step = 91200 ; steps/s = 0.72, tokens/s = 34065 (16406 source, 17659 target) ; Learning rate = 0.000293 ; Loss = 1.490160\n",
      "2024-12-12 21:14:44.778000: I runner.py:310] Step = 91300 ; steps/s = 0.72, tokens/s = 34048 (16408 source, 17640 target) ; Learning rate = 0.000293 ; Loss = 1.486742\n",
      "2024-12-12 21:17:01.618000: I runner.py:310] Step = 91400 ; steps/s = 0.73, tokens/s = 33940 (16353 source, 17587 target) ; Learning rate = 0.000292 ; Loss = 1.497373\n",
      "2024-12-12 21:19:20.693000: I runner.py:310] Step = 91500 ; steps/s = 0.72, tokens/s = 34062 (16410 source, 17652 target) ; Learning rate = 0.000292 ; Loss = 1.491598\n",
      "2024-12-12 21:21:39.614000: I runner.py:310] Step = 91600 ; steps/s = 0.72, tokens/s = 34113 (16440 source, 17673 target) ; Learning rate = 0.000292 ; Loss = 1.486839\n",
      "2024-12-12 21:23:58.420000: I runner.py:310] Step = 91700 ; steps/s = 0.72, tokens/s = 34148 (16447 source, 17701 target) ; Learning rate = 0.000292 ; Loss = 1.484624\n",
      "2024-12-12 21:26:15.259000: I runner.py:310] Step = 91800 ; steps/s = 0.73, tokens/s = 33924 (16349 source, 17575 target) ; Learning rate = 0.000292 ; Loss = 1.496821\n",
      "2024-12-12 21:28:34.333000: I runner.py:310] Step = 91900 ; steps/s = 0.72, tokens/s = 34075 (16418 source, 17657 target) ; Learning rate = 0.000292 ; Loss = 1.491385\n",
      "2024-12-12 21:30:53.402000: I runner.py:310] Step = 92000 ; steps/s = 0.72, tokens/s = 34070 (16414 source, 17656 target) ; Learning rate = 0.000291 ; Loss = 1.488664\n",
      "2024-12-12 21:33:12.502000: I runner.py:310] Step = 92100 ; steps/s = 0.72, tokens/s = 34073 (16419 source, 17654 target) ; Learning rate = 0.000291 ; Loss = 1.493762\n",
      "2024-12-12 21:35:29.263000: I runner.py:310] Step = 92200 ; steps/s = 0.73, tokens/s = 33953 (16349 source, 17604 target) ; Learning rate = 0.000291 ; Loss = 1.494635\n",
      "2024-12-12 21:37:48.344000: I runner.py:310] Step = 92300 ; steps/s = 0.72, tokens/s = 34060 (16405 source, 17655 target) ; Learning rate = 0.000291 ; Loss = 1.492727\n",
      "2024-12-12 21:40:07.426000: I runner.py:310] Step = 92400 ; steps/s = 0.72, tokens/s = 34074 (16422 source, 17652 target) ; Learning rate = 0.000291 ; Loss = 1.488753\n",
      "2024-12-12 21:42:26.503000: I runner.py:310] Step = 92500 ; steps/s = 0.72, tokens/s = 34074 (16430 source, 17644 target) ; Learning rate = 0.000291 ; Loss = 1.486768\n",
      "2024-12-12 21:44:43.339000: I runner.py:310] Step = 92600 ; steps/s = 0.73, tokens/s = 33941 (16345 source, 17596 target) ; Learning rate = 0.000290 ; Loss = 1.486131\n",
      "2024-12-12 21:47:02.399000: I runner.py:310] Step = 92700 ; steps/s = 0.72, tokens/s = 34096 (16424 source, 17672 target) ; Learning rate = 0.000290 ; Loss = 1.489714\n",
      "2024-12-12 21:49:21.484000: I runner.py:310] Step = 92800 ; steps/s = 0.72, tokens/s = 34058 (16421 source, 17637 target) ; Learning rate = 0.000290 ; Loss = 1.481885\n",
      "2024-12-12 21:51:40.602000: I runner.py:310] Step = 92900 ; steps/s = 0.72, tokens/s = 34065 (16408 source, 17657 target) ; Learning rate = 0.000290 ; Loss = 1.484427\n",
      "2024-12-12 21:53:59.701000: I runner.py:310] Step = 93000 ; steps/s = 0.72, tokens/s = 34054 (16405 source, 17649 target) ; Learning rate = 0.000290 ; Loss = 1.487494\n",
      "2024-12-12 21:56:16.579000: I runner.py:310] Step = 93100 ; steps/s = 0.73, tokens/s = 33936 (16337 source, 17599 target) ; Learning rate = 0.000290 ; Loss = 1.488101\n",
      "2024-12-12 21:58:35.642000: I runner.py:310] Step = 93200 ; steps/s = 0.72, tokens/s = 34070 (16408 source, 17662 target) ; Learning rate = 0.000290 ; Loss = 1.480457\n",
      "2024-12-12 22:00:54.682000: I runner.py:310] Step = 93300 ; steps/s = 0.72, tokens/s = 34082 (16428 source, 17654 target) ; Learning rate = 0.000289 ; Loss = 1.485037\n",
      "2024-12-12 22:03:13.784000: I runner.py:310] Step = 93400 ; steps/s = 0.72, tokens/s = 34068 (16419 source, 17649 target) ; Learning rate = 0.000289 ; Loss = 1.488023\n",
      "2024-12-12 22:05:30.569000: I runner.py:310] Step = 93500 ; steps/s = 0.73, tokens/s = 33953 (16367 source, 17586 target) ; Learning rate = 0.000289 ; Loss = 1.488010\n",
      "2024-12-12 22:07:49.648000: I runner.py:310] Step = 93600 ; steps/s = 0.72, tokens/s = 34072 (16418 source, 17654 target) ; Learning rate = 0.000289 ; Loss = 1.489262\n",
      "2024-12-12 22:10:08.741000: I runner.py:310] Step = 93700 ; steps/s = 0.72, tokens/s = 34072 (16422 source, 17650 target) ; Learning rate = 0.000289 ; Loss = 1.487725\n",
      "2024-12-12 22:12:27.800000: I runner.py:310] Step = 93800 ; steps/s = 0.72, tokens/s = 34057 (16408 source, 17649 target) ; Learning rate = 0.000289 ; Loss = 1.487334\n",
      "2024-12-12 22:14:44.465000: I runner.py:310] Step = 93900 ; steps/s = 0.73, tokens/s = 33997 (16363 source, 17634 target) ; Learning rate = 0.000288 ; Loss = 1.488297\n",
      "2024-12-12 22:17:03.271000: I runner.py:310] Step = 94000 ; steps/s = 0.72, tokens/s = 34147 (16455 source, 17692 target) ; Learning rate = 0.000288 ; Loss = 1.488424\n",
      "2024-12-12 22:19:22.381000: I runner.py:310] Step = 94100 ; steps/s = 0.72, tokens/s = 34056 (16406 source, 17650 target) ; Learning rate = 0.000288 ; Loss = 1.496183\n",
      "2024-12-12 22:21:41.428000: I runner.py:310] Step = 94200 ; steps/s = 0.72, tokens/s = 34076 (16423 source, 17653 target) ; Learning rate = 0.000288 ; Loss = 1.496145\n",
      "2024-12-12 22:23:58.279000: I runner.py:310] Step = 94300 ; steps/s = 0.73, tokens/s = 33925 (16348 source, 17577 target) ; Learning rate = 0.000288 ; Loss = 1.485318\n",
      "2024-12-12 22:26:17.324000: I runner.py:310] Step = 94400 ; steps/s = 0.72, tokens/s = 34078 (16405 source, 17673 target) ; Learning rate = 0.000288 ; Loss = 1.491558\n",
      "2024-12-12 22:28:36.395000: I runner.py:310] Step = 94500 ; steps/s = 0.72, tokens/s = 34074 (16421 source, 17653 target) ; Learning rate = 0.000288 ; Loss = 1.483321\n",
      "2024-12-12 22:30:55.471000: I runner.py:310] Step = 94600 ; steps/s = 0.72, tokens/s = 34082 (16431 source, 17651 target) ; Learning rate = 0.000287 ; Loss = 1.488714\n",
      "2024-12-12 22:33:14.565000: I runner.py:310] Step = 94700 ; steps/s = 0.72, tokens/s = 34079 (16415 source, 17664 target) ; Learning rate = 0.000287 ; Loss = 1.484306\n",
      "2024-12-12 22:35:31.375000: I runner.py:310] Step = 94800 ; steps/s = 0.73, tokens/s = 33920 (16332 source, 17588 target) ; Learning rate = 0.000287 ; Loss = 1.487464\n",
      "2024-12-12 22:37:50.486000: I runner.py:310] Step = 94900 ; steps/s = 0.72, tokens/s = 34081 (16426 source, 17655 target) ; Learning rate = 0.000287 ; Loss = 1.489045\n",
      "2024-12-12 22:40:09.566000: I runner.py:310] Step = 95000 ; steps/s = 0.72, tokens/s = 34049 (16406 source, 17643 target) ; Learning rate = 0.000287 ; Loss = 1.486998\n",
      "2024-12-12 22:42:28.664000: I runner.py:310] Step = 95100 ; steps/s = 0.72, tokens/s = 34085 (16420 source, 17665 target) ; Learning rate = 0.000287 ; Loss = 1.486936\n",
      "2024-12-12 22:44:45.466000: I runner.py:310] Step = 95200 ; steps/s = 0.73, tokens/s = 33955 (16367 source, 17588 target) ; Learning rate = 0.000286 ; Loss = 1.494072\n",
      "2024-12-12 22:47:04.546000: I runner.py:310] Step = 95300 ; steps/s = 0.72, tokens/s = 34072 (16406 source, 17666 target) ; Learning rate = 0.000286 ; Loss = 1.484926\n",
      "2024-12-12 22:49:23.615000: I runner.py:310] Step = 95400 ; steps/s = 0.72, tokens/s = 34075 (16405 source, 17670 target) ; Learning rate = 0.000286 ; Loss = 1.487618\n",
      "2024-12-12 22:51:42.745000: I runner.py:310] Step = 95500 ; steps/s = 0.72, tokens/s = 34065 (16430 source, 17635 target) ; Learning rate = 0.000286 ; Loss = 1.479822\n",
      "2024-12-12 22:53:59.596000: I runner.py:310] Step = 95600 ; steps/s = 0.73, tokens/s = 33916 (16344 source, 17572 target) ; Learning rate = 0.000286 ; Loss = 1.484953\n",
      "2024-12-12 22:56:18.662000: I runner.py:310] Step = 95700 ; steps/s = 0.72, tokens/s = 34075 (16417 source, 17658 target) ; Learning rate = 0.000286 ; Loss = 1.478647\n",
      "2024-12-12 22:58:37.733000: I runner.py:310] Step = 95800 ; steps/s = 0.72, tokens/s = 34074 (16413 source, 17661 target) ; Learning rate = 0.000286 ; Loss = 1.490776\n",
      "2024-12-12 23:00:56.817000: I runner.py:310] Step = 95900 ; steps/s = 0.72, tokens/s = 34062 (16416 source, 17646 target) ; Learning rate = 0.000285 ; Loss = 1.493827\n",
      "2024-12-12 23:03:15.882000: I runner.py:310] Step = 96000 ; steps/s = 0.72, tokens/s = 34084 (16421 source, 17663 target) ; Learning rate = 0.000285 ; Loss = 1.487262\n",
      "2024-12-12 23:05:32.639000: I runner.py:310] Step = 96100 ; steps/s = 0.73, tokens/s = 33959 (16356 source, 17603 target) ; Learning rate = 0.000285 ; Loss = 1.488970\n",
      "2024-12-12 23:07:51.623000: I runner.py:310] Step = 96200 ; steps/s = 0.72, tokens/s = 34103 (16433 source, 17670 target) ; Learning rate = 0.000285 ; Loss = 1.482915\n",
      "2024-12-12 23:10:10.411000: I runner.py:310] Step = 96300 ; steps/s = 0.72, tokens/s = 34136 (16448 source, 17688 target) ; Learning rate = 0.000285 ; Loss = 1.482753\n",
      "2024-12-12 23:12:29.490000: I runner.py:310] Step = 96400 ; steps/s = 0.72, tokens/s = 34064 (16410 source, 17654 target) ; Learning rate = 0.000285 ; Loss = 1.486144\n",
      "2024-12-12 23:14:46.302000: I runner.py:310] Step = 96500 ; steps/s = 0.73, tokens/s = 33943 (16350 source, 17593 target) ; Learning rate = 0.000285 ; Loss = 1.483650\n",
      "2024-12-12 23:17:05.379000: I runner.py:310] Step = 96600 ; steps/s = 0.72, tokens/s = 34083 (16422 source, 17661 target) ; Learning rate = 0.000284 ; Loss = 1.487334\n",
      "2024-12-12 23:19:24.466000: I runner.py:310] Step = 96700 ; steps/s = 0.72, tokens/s = 34072 (16418 source, 17654 target) ; Learning rate = 0.000284 ; Loss = 1.490339\n",
      "2024-12-12 23:21:43.538000: I runner.py:310] Step = 96800 ; steps/s = 0.72, tokens/s = 34072 (16414 source, 17658 target) ; Learning rate = 0.000284 ; Loss = 1.483085\n",
      "2024-12-12 23:24:00.384000: I runner.py:310] Step = 96900 ; steps/s = 0.73, tokens/s = 33931 (16353 source, 17578 target) ; Learning rate = 0.000284 ; Loss = 1.492029\n",
      "2024-12-12 23:26:19.476000: I runner.py:310] Step = 97000 ; steps/s = 0.72, tokens/s = 34075 (16415 source, 17660 target) ; Learning rate = 0.000284 ; Loss = 1.495333\n",
      "2024-12-12 23:28:38.563000: I runner.py:310] Step = 97100 ; steps/s = 0.72, tokens/s = 34058 (16420 source, 17638 target) ; Learning rate = 0.000284 ; Loss = 1.487361\n",
      "2024-12-12 23:30:57.630000: I runner.py:310] Step = 97200 ; steps/s = 0.72, tokens/s = 34075 (16416 source, 17659 target) ; Learning rate = 0.000284 ; Loss = 1.482413\n",
      "2024-12-12 23:33:14.394000: I runner.py:310] Step = 97300 ; steps/s = 0.73, tokens/s = 33950 (16354 source, 17596 target) ; Learning rate = 0.000283 ; Loss = 1.482717\n",
      "2024-12-12 23:35:33.435000: I runner.py:310] Step = 97400 ; steps/s = 0.72, tokens/s = 34084 (16427 source, 17657 target) ; Learning rate = 0.000283 ; Loss = 1.473974\n",
      "2024-12-12 23:37:52.519000: I runner.py:310] Step = 97500 ; steps/s = 0.72, tokens/s = 34070 (16409 source, 17661 target) ; Learning rate = 0.000283 ; Loss = 1.487721\n",
      "2024-12-12 23:40:11.603000: I runner.py:310] Step = 97600 ; steps/s = 0.72, tokens/s = 34060 (16405 source, 17655 target) ; Learning rate = 0.000283 ; Loss = 1.482634\n",
      "2024-12-12 23:42:30.696000: I runner.py:310] Step = 97700 ; steps/s = 0.72, tokens/s = 34080 (16430 source, 17650 target) ; Learning rate = 0.000283 ; Loss = 1.493066\n",
      "2024-12-12 23:44:47.474000: I runner.py:310] Step = 97800 ; steps/s = 0.73, tokens/s = 33958 (16348 source, 17610 target) ; Learning rate = 0.000283 ; Loss = 1.486399\n",
      "2024-12-12 23:47:06.595000: I runner.py:310] Step = 97900 ; steps/s = 0.72, tokens/s = 34055 (16401 source, 17654 target) ; Learning rate = 0.000282 ; Loss = 1.486554\n",
      "2024-12-12 23:49:25.672000: I runner.py:310] Step = 98000 ; steps/s = 0.72, tokens/s = 34072 (16419 source, 17653 target) ; Learning rate = 0.000282 ; Loss = 1.490014\n",
      "2024-12-12 23:51:44.734000: I runner.py:310] Step = 98100 ; steps/s = 0.72, tokens/s = 34088 (16433 source, 17655 target) ; Learning rate = 0.000282 ; Loss = 1.490668\n",
      "2024-12-12 23:54:01.568000: I runner.py:310] Step = 98200 ; steps/s = 0.73, tokens/s = 33932 (16347 source, 17585 target) ; Learning rate = 0.000282 ; Loss = 1.489651\n",
      "2024-12-12 23:56:20.623000: I runner.py:310] Step = 98300 ; steps/s = 0.72, tokens/s = 34085 (16420 source, 17665 target) ; Learning rate = 0.000282 ; Loss = 1.485126\n",
      "2024-12-12 23:58:39.722000: I runner.py:310] Step = 98400 ; steps/s = 0.72, tokens/s = 34053 (16410 source, 17643 target) ; Learning rate = 0.000282 ; Loss = 1.487750\n",
      "2024-12-13 00:00:58.710000: I runner.py:310] Step = 98500 ; steps/s = 0.72, tokens/s = 34091 (16425 source, 17666 target) ; Learning rate = 0.000282 ; Loss = 1.488268\n",
      "2024-12-13 00:03:15.159000: I runner.py:310] Step = 98600 ; steps/s = 0.73, tokens/s = 34034 (16395 source, 17639 target) ; Learning rate = 0.000281 ; Loss = 1.483210\n",
      "2024-12-13 00:05:34.260000: I runner.py:310] Step = 98700 ; steps/s = 0.72, tokens/s = 34066 (16406 source, 17660 target) ; Learning rate = 0.000281 ; Loss = 1.490847\n",
      "2024-12-13 00:07:53.301000: I runner.py:310] Step = 98800 ; steps/s = 0.72, tokens/s = 34085 (16425 source, 17660 target) ; Learning rate = 0.000281 ; Loss = 1.486227\n",
      "2024-12-13 00:10:12.409000: I runner.py:310] Step = 98900 ; steps/s = 0.72, tokens/s = 34057 (16418 source, 17639 target) ; Learning rate = 0.000281 ; Loss = 1.487911\n",
      "2024-12-13 00:12:29.281000: I runner.py:310] Step = 99000 ; steps/s = 0.73, tokens/s = 33931 (16343 source, 17588 target) ; Learning rate = 0.000281 ; Loss = 1.488523\n",
      "2024-12-13 00:14:48.441000: I runner.py:310] Step = 99100 ; steps/s = 0.72, tokens/s = 34048 (16401 source, 17647 target) ; Learning rate = 0.000281 ; Loss = 1.487246\n",
      "2024-12-13 00:17:07.554000: I runner.py:310] Step = 99200 ; steps/s = 0.72, tokens/s = 34078 (16423 source, 17655 target) ; Learning rate = 0.000281 ; Loss = 1.488301\n",
      "2024-12-13 00:19:26.621000: I runner.py:310] Step = 99300 ; steps/s = 0.72, tokens/s = 34066 (16418 source, 17648 target) ; Learning rate = 0.000280 ; Loss = 1.486578\n",
      "2024-12-13 00:21:45.703000: I runner.py:310] Step = 99400 ; steps/s = 0.72, tokens/s = 34066 (16412 source, 17654 target) ; Learning rate = 0.000280 ; Loss = 1.481958\n",
      "2024-12-13 00:24:02.524000: I runner.py:310] Step = 99500 ; steps/s = 0.73, tokens/s = 33944 (16352 source, 17592 target) ; Learning rate = 0.000280 ; Loss = 1.484381\n",
      "2024-12-13 00:26:21.622000: I runner.py:310] Step = 99600 ; steps/s = 0.72, tokens/s = 34067 (16415 source, 17652 target) ; Learning rate = 0.000280 ; Loss = 1.487584\n",
      "2024-12-13 00:28:40.741000: I runner.py:310] Step = 99700 ; steps/s = 0.72, tokens/s = 34058 (16413 source, 17645 target) ; Learning rate = 0.000280 ; Loss = 1.486933\n",
      "2024-12-13 00:30:59.854000: I runner.py:310] Step = 99800 ; steps/s = 0.72, tokens/s = 34068 (16415 source, 17653 target) ; Learning rate = 0.000280 ; Loss = 1.484651\n",
      "2024-12-13 00:33:16.639000: I runner.py:310] Step = 99900 ; steps/s = 0.73, tokens/s = 33945 (16342 source, 17603 target) ; Learning rate = 0.000280 ; Loss = 1.494975\n",
      "2024-12-13 00:35:35.771000: I runner.py:310] Step = 100000 ; steps/s = 0.72, tokens/s = 34057 (16405 source, 17652 target) ; Learning rate = 0.000280 ; Loss = 1.485128\n",
      "2024-12-13 00:35:37.535000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-100000\n",
      "2024-12-13 00:37:56.597000: I runner.py:310] Step = 100100 ; steps/s = 0.72, tokens/s = 34077 (16425 source, 17652 target) ; Learning rate = 0.000279 ; Loss = 1.486564\n",
      "2024-12-13 00:40:15.673000: I runner.py:310] Step = 100200 ; steps/s = 0.72, tokens/s = 34089 (16420 source, 17669 target) ; Learning rate = 0.000279 ; Loss = 1.489251\n",
      "2024-12-13 00:42:32.490000: I runner.py:310] Step = 100300 ; steps/s = 0.73, tokens/s = 33930 (16345 source, 17585 target) ; Learning rate = 0.000279 ; Loss = 1.482296\n",
      "2024-12-13 00:44:51.554000: I runner.py:310] Step = 100400 ; steps/s = 0.72, tokens/s = 34072 (16418 source, 17654 target) ; Learning rate = 0.000279 ; Loss = 1.483087\n",
      "2024-12-13 00:47:10.651000: I runner.py:310] Step = 100500 ; steps/s = 0.72, tokens/s = 34071 (16419 source, 17652 target) ; Learning rate = 0.000279 ; Loss = 1.481958\n",
      "2024-12-13 00:49:29.752000: I runner.py:310] Step = 100600 ; steps/s = 0.72, tokens/s = 34065 (16413 source, 17652 target) ; Learning rate = 0.000279 ; Loss = 1.489962\n",
      "2024-12-13 00:51:46.578000: I runner.py:310] Step = 100700 ; steps/s = 0.73, tokens/s = 33928 (16353 source, 17575 target) ; Learning rate = 0.000279 ; Loss = 1.482111\n",
      "2024-12-13 00:54:05.532000: I runner.py:310] Step = 100800 ; steps/s = 0.72, tokens/s = 34112 (16426 source, 17686 target) ; Learning rate = 0.000278 ; Loss = 1.482465\n",
      "2024-12-13 00:56:24.317000: I runner.py:310] Step = 100900 ; steps/s = 0.72, tokens/s = 34135 (16448 source, 17687 target) ; Learning rate = 0.000278 ; Loss = 1.485179\n",
      "2024-12-13 00:58:43.440000: I runner.py:310] Step = 101000 ; steps/s = 0.72, tokens/s = 34069 (16413 source, 17656 target) ; Learning rate = 0.000278 ; Loss = 1.483796\n",
      "2024-12-13 01:01:02.539000: I runner.py:310] Step = 101100 ; steps/s = 0.72, tokens/s = 34076 (16422 source, 17654 target) ; Learning rate = 0.000278 ; Loss = 1.486256\n",
      "2024-12-13 01:03:19.320000: I runner.py:310] Step = 101200 ; steps/s = 0.73, tokens/s = 33928 (16335 source, 17593 target) ; Learning rate = 0.000278 ; Loss = 1.476760\n",
      "2024-12-13 01:05:38.423000: I runner.py:310] Step = 101300 ; steps/s = 0.72, tokens/s = 34077 (16416 source, 17661 target) ; Learning rate = 0.000278 ; Loss = 1.486402\n",
      "2024-12-13 01:07:57.511000: I runner.py:310] Step = 101400 ; steps/s = 0.72, tokens/s = 34073 (16421 source, 17652 target) ; Learning rate = 0.000278 ; Loss = 1.491441\n",
      "2024-12-13 01:10:16.581000: I runner.py:310] Step = 101500 ; steps/s = 0.72, tokens/s = 34073 (16422 source, 17651 target) ; Learning rate = 0.000277 ; Loss = 1.482262\n",
      "2024-12-13 01:12:33.425000: I runner.py:310] Step = 101600 ; steps/s = 0.73, tokens/s = 33925 (16340 source, 17585 target) ; Learning rate = 0.000277 ; Loss = 1.483584\n",
      "2024-12-13 01:14:52.513000: I runner.py:310] Step = 101700 ; steps/s = 0.72, tokens/s = 34070 (16409 source, 17661 target) ; Learning rate = 0.000277 ; Loss = 1.488300\n",
      "2024-12-13 01:17:11.583000: I runner.py:310] Step = 101800 ; steps/s = 0.72, tokens/s = 34070 (16423 source, 17647 target) ; Learning rate = 0.000277 ; Loss = 1.482786\n",
      "2024-12-13 01:19:30.702000: I runner.py:310] Step = 101900 ; steps/s = 0.72, tokens/s = 34061 (16419 source, 17642 target) ; Learning rate = 0.000277 ; Loss = 1.482381\n",
      "2024-12-13 01:21:47.516000: I runner.py:310] Step = 102000 ; steps/s = 0.73, tokens/s = 33949 (16349 source, 17600 target) ; Learning rate = 0.000277 ; Loss = 1.491569\n",
      "2024-12-13 01:24:06.601000: I runner.py:310] Step = 102100 ; steps/s = 0.72, tokens/s = 34068 (16418 source, 17650 target) ; Learning rate = 0.000277 ; Loss = 1.482280\n",
      "2024-12-13 01:26:25.684000: I runner.py:310] Step = 102200 ; steps/s = 0.72, tokens/s = 34079 (16421 source, 17658 target) ; Learning rate = 0.000276 ; Loss = 1.485654\n",
      "2024-12-13 01:28:44.799000: I runner.py:310] Step = 102300 ; steps/s = 0.72, tokens/s = 34060 (16414 source, 17646 target) ; Learning rate = 0.000276 ; Loss = 1.489805\n",
      "2024-12-13 01:31:03.897000: I runner.py:310] Step = 102400 ; steps/s = 0.72, tokens/s = 34073 (16410 source, 17663 target) ; Learning rate = 0.000276 ; Loss = 1.487899\n",
      "2024-12-13 01:33:20.731000: I runner.py:310] Step = 102500 ; steps/s = 0.73, tokens/s = 33927 (16351 source, 17576 target) ; Learning rate = 0.000276 ; Loss = 1.476774\n",
      "2024-12-13 01:35:39.801000: I runner.py:310] Step = 102600 ; steps/s = 0.72, tokens/s = 34062 (16417 source, 17645 target) ; Learning rate = 0.000276 ; Loss = 1.481931\n",
      "2024-12-13 01:37:58.907000: I runner.py:310] Step = 102700 ; steps/s = 0.72, tokens/s = 34078 (16422 source, 17656 target) ; Learning rate = 0.000276 ; Loss = 1.479159\n",
      "2024-12-13 01:40:17.946000: I runner.py:310] Step = 102800 ; steps/s = 0.72, tokens/s = 34088 (16412 source, 17676 target) ; Learning rate = 0.000276 ; Loss = 1.488253\n",
      "2024-12-13 01:42:34.821000: I runner.py:310] Step = 102900 ; steps/s = 0.73, tokens/s = 33918 (16332 source, 17586 target) ; Learning rate = 0.000276 ; Loss = 1.478505\n",
      "2024-12-13 01:44:53.911000: I runner.py:310] Step = 103000 ; steps/s = 0.72, tokens/s = 34063 (16409 source, 17654 target) ; Learning rate = 0.000275 ; Loss = 1.484237\n",
      "2024-12-13 01:47:12.854000: I runner.py:310] Step = 103100 ; steps/s = 0.72, tokens/s = 34110 (16441 source, 17669 target) ; Learning rate = 0.000275 ; Loss = 1.485448\n",
      "2024-12-13 01:49:31.675000: I runner.py:310] Step = 103200 ; steps/s = 0.72, tokens/s = 34137 (16451 source, 17686 target) ; Learning rate = 0.000275 ; Loss = 1.488023\n",
      "2024-12-13 01:51:48.429000: I runner.py:310] Step = 103300 ; steps/s = 0.73, tokens/s = 33970 (16358 source, 17612 target) ; Learning rate = 0.000275 ; Loss = 1.490697\n",
      "2024-12-13 01:54:07.536000: I runner.py:310] Step = 103400 ; steps/s = 0.72, tokens/s = 34071 (16406 source, 17665 target) ; Learning rate = 0.000275 ; Loss = 1.484652\n",
      "2024-12-13 01:56:26.624000: I runner.py:310] Step = 103500 ; steps/s = 0.72, tokens/s = 34058 (16413 source, 17645 target) ; Learning rate = 0.000275 ; Loss = 1.480460\n",
      "2024-12-13 01:58:45.771000: I runner.py:310] Step = 103600 ; steps/s = 0.72, tokens/s = 34053 (16419 source, 17634 target) ; Learning rate = 0.000275 ; Loss = 1.481309\n",
      "2024-12-13 02:01:02.574000: I runner.py:310] Step = 103700 ; steps/s = 0.73, tokens/s = 33941 (16349 source, 17592 target) ; Learning rate = 0.000274 ; Loss = 1.484094\n",
      "2024-12-13 02:03:21.649000: I runner.py:310] Step = 103800 ; steps/s = 0.72, tokens/s = 34080 (16420 source, 17660 target) ; Learning rate = 0.000274 ; Loss = 1.477672\n",
      "2024-12-13 02:05:40.737000: I runner.py:310] Step = 103900 ; steps/s = 0.72, tokens/s = 34067 (16416 source, 17651 target) ; Learning rate = 0.000274 ; Loss = 1.482647\n",
      "2024-12-13 02:07:59.827000: I runner.py:310] Step = 104000 ; steps/s = 0.72, tokens/s = 34067 (16409 source, 17658 target) ; Learning rate = 0.000274 ; Loss = 1.488739\n",
      "2024-12-13 02:10:18.949000: I runner.py:310] Step = 104100 ; steps/s = 0.72, tokens/s = 34057 (16417 source, 17640 target) ; Learning rate = 0.000274 ; Loss = 1.483752\n",
      "2024-12-13 02:12:35.759000: I runner.py:310] Step = 104200 ; steps/s = 0.73, tokens/s = 33947 (16332 source, 17615 target) ; Learning rate = 0.000274 ; Loss = 1.481115\n",
      "2024-12-13 02:14:54.863000: I runner.py:310] Step = 104300 ; steps/s = 0.72, tokens/s = 34063 (16416 source, 17647 target) ; Learning rate = 0.000274 ; Loss = 1.485563\n",
      "2024-12-13 02:17:13.953000: I runner.py:310] Step = 104400 ; steps/s = 0.72, tokens/s = 34073 (16424 source, 17649 target) ; Learning rate = 0.000274 ; Loss = 1.484265\n",
      "2024-12-13 02:19:33.097000: I runner.py:310] Step = 104500 ; steps/s = 0.72, tokens/s = 34064 (16420 source, 17644 target) ; Learning rate = 0.000273 ; Loss = 1.489831\n",
      "2024-12-13 02:21:49.934000: I runner.py:310] Step = 104600 ; steps/s = 0.73, tokens/s = 33935 (16345 source, 17590 target) ; Learning rate = 0.000273 ; Loss = 1.475217\n",
      "2024-12-13 02:24:09.033000: I runner.py:310] Step = 104700 ; steps/s = 0.72, tokens/s = 34064 (16402 source, 17662 target) ; Learning rate = 0.000273 ; Loss = 1.489479\n",
      "2024-12-13 02:26:28.139000: I runner.py:310] Step = 104800 ; steps/s = 0.72, tokens/s = 34053 (16404 source, 17649 target) ; Learning rate = 0.000273 ; Loss = 1.486983\n",
      "2024-12-13 02:28:47.217000: I runner.py:310] Step = 104900 ; steps/s = 0.72, tokens/s = 34070 (16430 source, 17640 target) ; Learning rate = 0.000273 ; Loss = 1.484964\n",
      "2024-12-13 02:31:04.082000: I runner.py:310] Step = 105000 ; steps/s = 0.73, tokens/s = 33936 (16342 source, 17594 target) ; Learning rate = 0.000273 ; Loss = 1.491324\n",
      "2024-12-13 02:33:23.195000: I runner.py:310] Step = 105100 ; steps/s = 0.72, tokens/s = 34068 (16421 source, 17647 target) ; Learning rate = 0.000273 ; Loss = 1.489467\n",
      "2024-12-13 02:35:42.260000: I runner.py:310] Step = 105200 ; steps/s = 0.72, tokens/s = 34076 (16415 source, 17661 target) ; Learning rate = 0.000273 ; Loss = 1.478477\n",
      "2024-12-13 02:38:01.376000: I runner.py:310] Step = 105300 ; steps/s = 0.72, tokens/s = 34066 (16423 source, 17643 target) ; Learning rate = 0.000272 ; Loss = 1.487554\n",
      "2024-12-13 02:40:18.049000: I runner.py:310] Step = 105400 ; steps/s = 0.73, tokens/s = 33963 (16359 source, 17604 target) ; Learning rate = 0.000272 ; Loss = 1.486287\n",
      "2024-12-13 02:42:36.850000: I runner.py:310] Step = 105500 ; steps/s = 0.72, tokens/s = 34143 (16455 source, 17688 target) ; Learning rate = 0.000272 ; Loss = 1.486033\n",
      "2024-12-13 02:44:55.912000: I runner.py:310] Step = 105600 ; steps/s = 0.72, tokens/s = 34056 (16399 source, 17657 target) ; Learning rate = 0.000272 ; Loss = 1.482101\n",
      "2024-12-13 02:47:14.958000: I runner.py:310] Step = 105700 ; steps/s = 0.72, tokens/s = 34085 (16430 source, 17655 target) ; Learning rate = 0.000272 ; Loss = 1.484848\n",
      "2024-12-13 02:49:34.049000: I runner.py:310] Step = 105800 ; steps/s = 0.72, tokens/s = 34088 (16415 source, 17673 target) ; Learning rate = 0.000272 ; Loss = 1.479034\n",
      "2024-12-13 02:51:50.880000: I runner.py:310] Step = 105900 ; steps/s = 0.73, tokens/s = 33945 (16360 source, 17585 target) ; Learning rate = 0.000272 ; Loss = 1.487067\n",
      "2024-12-13 02:54:09.955000: I runner.py:310] Step = 106000 ; steps/s = 0.72, tokens/s = 34072 (16423 source, 17649 target) ; Learning rate = 0.000271 ; Loss = 1.480742\n",
      "2024-12-13 02:56:29.026000: I runner.py:310] Step = 106100 ; steps/s = 0.72, tokens/s = 34072 (16413 source, 17659 target) ; Learning rate = 0.000271 ; Loss = 1.488399\n",
      "2024-12-13 02:58:48.095000: I runner.py:310] Step = 106200 ; steps/s = 0.72, tokens/s = 34074 (16414 source, 17660 target) ; Learning rate = 0.000271 ; Loss = 1.484459\n",
      "2024-12-13 03:01:04.883000: I runner.py:310] Step = 106300 ; steps/s = 0.73, tokens/s = 33950 (16355 source, 17595 target) ; Learning rate = 0.000271 ; Loss = 1.484820\n",
      "2024-12-13 03:03:23.989000: I runner.py:310] Step = 106400 ; steps/s = 0.72, tokens/s = 34058 (16406 source, 17652 target) ; Learning rate = 0.000271 ; Loss = 1.479957\n",
      "2024-12-13 03:05:43.071000: I runner.py:310] Step = 106500 ; steps/s = 0.72, tokens/s = 34084 (16420 source, 17664 target) ; Learning rate = 0.000271 ; Loss = 1.480426\n",
      "2024-12-13 03:08:02.148000: I runner.py:310] Step = 106600 ; steps/s = 0.72, tokens/s = 34064 (16415 source, 17649 target) ; Learning rate = 0.000271 ; Loss = 1.477829\n",
      "2024-12-13 03:10:18.998000: I runner.py:310] Step = 106700 ; steps/s = 0.73, tokens/s = 33926 (16354 source, 17572 target) ; Learning rate = 0.000271 ; Loss = 1.480449\n",
      "2024-12-13 03:12:38.082000: I runner.py:310] Step = 106800 ; steps/s = 0.72, tokens/s = 34067 (16408 source, 17659 target) ; Learning rate = 0.000270 ; Loss = 1.481536\n",
      "2024-12-13 03:14:57.151000: I runner.py:310] Step = 106900 ; steps/s = 0.72, tokens/s = 34078 (16413 source, 17665 target) ; Learning rate = 0.000270 ; Loss = 1.482815\n",
      "2024-12-13 03:17:16.234000: I runner.py:310] Step = 107000 ; steps/s = 0.72, tokens/s = 34084 (16419 source, 17665 target) ; Learning rate = 0.000270 ; Loss = 1.485753\n",
      "2024-12-13 03:19:33.095000: I runner.py:310] Step = 107100 ; steps/s = 0.73, tokens/s = 33915 (16353 source, 17562 target) ; Learning rate = 0.000270 ; Loss = 1.489248\n",
      "2024-12-13 03:21:52.134000: I runner.py:310] Step = 107200 ; steps/s = 0.72, tokens/s = 34094 (16431 source, 17663 target) ; Learning rate = 0.000270 ; Loss = 1.478105\n",
      "2024-12-13 03:24:11.207000: I runner.py:310] Step = 107300 ; steps/s = 0.72, tokens/s = 34069 (16413 source, 17656 target) ; Learning rate = 0.000270 ; Loss = 1.476552\n",
      "2024-12-13 03:26:30.270000: I runner.py:310] Step = 107400 ; steps/s = 0.72, tokens/s = 34071 (16411 source, 17660 target) ; Learning rate = 0.000270 ; Loss = 1.480002\n",
      "2024-12-13 03:28:49.388000: I runner.py:310] Step = 107500 ; steps/s = 0.72, tokens/s = 34061 (16411 source, 17650 target) ; Learning rate = 0.000270 ; Loss = 1.483221\n",
      "2024-12-13 03:31:06.217000: I runner.py:310] Step = 107600 ; steps/s = 0.73, tokens/s = 33944 (16339 source, 17605 target) ; Learning rate = 0.000269 ; Loss = 1.484394\n",
      "2024-12-13 03:33:25.180000: I runner.py:310] Step = 107700 ; steps/s = 0.72, tokens/s = 34110 (16433 source, 17677 target) ; Learning rate = 0.000269 ; Loss = 1.479252\n",
      "2024-12-13 03:35:43.964000: I runner.py:310] Step = 107800 ; steps/s = 0.72, tokens/s = 34140 (16447 source, 17693 target) ; Learning rate = 0.000269 ; Loss = 1.479204\n",
      "2024-12-13 03:38:03.061000: I runner.py:310] Step = 107900 ; steps/s = 0.72, tokens/s = 34062 (16422 source, 17640 target) ; Learning rate = 0.000269 ; Loss = 1.481252\n",
      "2024-12-13 03:40:19.896000: I runner.py:310] Step = 108000 ; steps/s = 0.73, tokens/s = 33935 (16342 source, 17593 target) ; Learning rate = 0.000269 ; Loss = 1.487147\n",
      "2024-12-13 03:42:38.984000: I runner.py:310] Step = 108100 ; steps/s = 0.72, tokens/s = 34068 (16416 source, 17652 target) ; Learning rate = 0.000269 ; Loss = 1.478483\n",
      "2024-12-13 03:44:58.057000: I runner.py:310] Step = 108200 ; steps/s = 0.72, tokens/s = 34072 (16418 source, 17654 target) ; Learning rate = 0.000269 ; Loss = 1.478257\n",
      "2024-12-13 03:47:17.162000: I runner.py:310] Step = 108300 ; steps/s = 0.72, tokens/s = 34068 (16407 source, 17661 target) ; Learning rate = 0.000269 ; Loss = 1.475662\n",
      "2024-12-13 03:49:33.984000: I runner.py:310] Step = 108400 ; steps/s = 0.73, tokens/s = 33940 (16369 source, 17571 target) ; Learning rate = 0.000268 ; Loss = 1.484592\n",
      "2024-12-13 03:51:53.056000: I runner.py:310] Step = 108500 ; steps/s = 0.72, tokens/s = 34070 (16408 source, 17662 target) ; Learning rate = 0.000268 ; Loss = 1.474756\n",
      "2024-12-13 03:54:12.161000: I runner.py:310] Step = 108600 ; steps/s = 0.72, tokens/s = 34058 (16415 source, 17643 target) ; Learning rate = 0.000268 ; Loss = 1.488321\n",
      "2024-12-13 03:56:31.221000: I runner.py:310] Step = 108700 ; steps/s = 0.72, tokens/s = 34072 (16411 source, 17661 target) ; Learning rate = 0.000268 ; Loss = 1.485157\n",
      "2024-12-13 03:58:50.282000: I runner.py:310] Step = 108800 ; steps/s = 0.72, tokens/s = 34091 (16431 source, 17660 target) ; Learning rate = 0.000268 ; Loss = 1.482260\n",
      "2024-12-13 04:01:07.157000: I runner.py:310] Step = 108900 ; steps/s = 0.73, tokens/s = 33933 (16345 source, 17588 target) ; Learning rate = 0.000268 ; Loss = 1.481143\n",
      "2024-12-13 04:03:26.264000: I runner.py:310] Step = 109000 ; steps/s = 0.72, tokens/s = 34074 (16407 source, 17667 target) ; Learning rate = 0.000268 ; Loss = 1.479859\n",
      "2024-12-13 04:05:45.338000: I runner.py:310] Step = 109100 ; steps/s = 0.72, tokens/s = 34072 (16421 source, 17651 target) ; Learning rate = 0.000268 ; Loss = 1.477811\n",
      "2024-12-13 04:08:04.483000: I runner.py:310] Step = 109200 ; steps/s = 0.72, tokens/s = 34044 (16407 source, 17637 target) ; Learning rate = 0.000267 ; Loss = 1.481072\n",
      "2024-12-13 04:10:21.398000: I runner.py:310] Step = 109300 ; steps/s = 0.73, tokens/s = 33921 (16343 source, 17578 target) ; Learning rate = 0.000267 ; Loss = 1.483134\n",
      "2024-12-13 04:12:40.467000: I runner.py:310] Step = 109400 ; steps/s = 0.72, tokens/s = 34081 (16427 source, 17654 target) ; Learning rate = 0.000267 ; Loss = 1.482752\n",
      "2024-12-13 04:14:59.540000: I runner.py:310] Step = 109500 ; steps/s = 0.72, tokens/s = 34080 (16419 source, 17661 target) ; Learning rate = 0.000267 ; Loss = 1.477116\n",
      "2024-12-13 04:17:18.619000: I runner.py:310] Step = 109600 ; steps/s = 0.72, tokens/s = 34058 (16403 source, 17655 target) ; Learning rate = 0.000267 ; Loss = 1.488613\n",
      "2024-12-13 04:19:35.414000: I runner.py:310] Step = 109700 ; steps/s = 0.73, tokens/s = 33942 (16355 source, 17587 target) ; Learning rate = 0.000267 ; Loss = 1.486050\n",
      "2024-12-13 04:21:54.464000: I runner.py:310] Step = 109800 ; steps/s = 0.72, tokens/s = 34083 (16418 source, 17665 target) ; Learning rate = 0.000267 ; Loss = 1.479666\n",
      "2024-12-13 04:24:13.530000: I runner.py:310] Step = 109900 ; steps/s = 0.72, tokens/s = 34066 (16410 source, 17656 target) ; Learning rate = 0.000267 ; Loss = 1.479565\n",
      "2024-12-13 04:26:32.485000: I runner.py:310] Step = 110000 ; steps/s = 0.72, tokens/s = 34092 (16432 source, 17660 target) ; Learning rate = 0.000266 ; Loss = 1.485602\n",
      "2024-12-13 04:26:34.142000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-110000\n",
      "2024-12-13 04:28:50.249000: I runner.py:310] Step = 110100 ; steps/s = 0.73, tokens/s = 34118 (16439 source, 17679 target) ; Learning rate = 0.000266 ; Loss = 1.477642\n",
      "2024-12-13 04:31:09.225000: I runner.py:310] Step = 110200 ; steps/s = 0.72, tokens/s = 34103 (16419 source, 17684 target) ; Learning rate = 0.000266 ; Loss = 1.479335\n",
      "2024-12-13 04:33:28.332000: I runner.py:310] Step = 110300 ; steps/s = 0.72, tokens/s = 34064 (16409 source, 17655 target) ; Learning rate = 0.000266 ; Loss = 1.479274\n",
      "2024-12-13 04:35:47.419000: I runner.py:310] Step = 110400 ; steps/s = 0.72, tokens/s = 34071 (16422 source, 17649 target) ; Learning rate = 0.000266 ; Loss = 1.479982\n",
      "2024-12-13 04:38:06.513000: I runner.py:310] Step = 110500 ; steps/s = 0.72, tokens/s = 34070 (16422 source, 17648 target) ; Learning rate = 0.000266 ; Loss = 1.481349\n",
      "2024-12-13 04:40:23.316000: I runner.py:310] Step = 110600 ; steps/s = 0.73, tokens/s = 33947 (16344 source, 17603 target) ; Learning rate = 0.000266 ; Loss = 1.478240\n",
      "2024-12-13 04:42:42.404000: I runner.py:310] Step = 110700 ; steps/s = 0.72, tokens/s = 34072 (16413 source, 17659 target) ; Learning rate = 0.000266 ; Loss = 1.473981\n",
      "2024-12-13 04:45:01.505000: I runner.py:310] Step = 110800 ; steps/s = 0.72, tokens/s = 34080 (16427 source, 17653 target) ; Learning rate = 0.000266 ; Loss = 1.485937\n",
      "2024-12-13 04:47:20.627000: I runner.py:310] Step = 110900 ; steps/s = 0.72, tokens/s = 34059 (16413 source, 17646 target) ; Learning rate = 0.000265 ; Loss = 1.482903\n",
      "2024-12-13 04:49:37.481000: I runner.py:310] Step = 111000 ; steps/s = 0.73, tokens/s = 33909 (16332 source, 17577 target) ; Learning rate = 0.000265 ; Loss = 1.487432\n",
      "2024-12-13 04:51:56.531000: I runner.py:310] Step = 111100 ; steps/s = 0.72, tokens/s = 34085 (16430 source, 17655 target) ; Learning rate = 0.000265 ; Loss = 1.472510\n",
      "2024-12-13 04:54:15.684000: I runner.py:310] Step = 111200 ; steps/s = 0.72, tokens/s = 34046 (16392 source, 17654 target) ; Learning rate = 0.000265 ; Loss = 1.483380\n",
      "2024-12-13 04:56:34.827000: I runner.py:310] Step = 111300 ; steps/s = 0.72, tokens/s = 34054 (16413 source, 17641 target) ; Learning rate = 0.000265 ; Loss = 1.476834\n",
      "2024-12-13 04:58:51.689000: I runner.py:310] Step = 111400 ; steps/s = 0.73, tokens/s = 33947 (16361 source, 17586 target) ; Learning rate = 0.000265 ; Loss = 1.482145\n",
      "2024-12-13 05:01:10.755000: I runner.py:310] Step = 111500 ; steps/s = 0.72, tokens/s = 34074 (16413 source, 17661 target) ; Learning rate = 0.000265 ; Loss = 1.483346\n",
      "2024-12-13 05:03:29.839000: I runner.py:310] Step = 111600 ; steps/s = 0.72, tokens/s = 34071 (16419 source, 17652 target) ; Learning rate = 0.000265 ; Loss = 1.482387\n",
      "2024-12-13 05:05:48.986000: I runner.py:310] Step = 111700 ; steps/s = 0.72, tokens/s = 34055 (16413 source, 17642 target) ; Learning rate = 0.000264 ; Loss = 1.487896\n",
      "2024-12-13 05:08:05.834000: I runner.py:310] Step = 111800 ; steps/s = 0.73, tokens/s = 33922 (16340 source, 17582 target) ; Learning rate = 0.000264 ; Loss = 1.477900\n",
      "2024-12-13 05:10:24.947000: I runner.py:310] Step = 111900 ; steps/s = 0.72, tokens/s = 34081 (16419 source, 17662 target) ; Learning rate = 0.000264 ; Loss = 1.488806\n",
      "2024-12-13 05:12:44.017000: I runner.py:310] Step = 112000 ; steps/s = 0.72, tokens/s = 34073 (16416 source, 17657 target) ; Learning rate = 0.000264 ; Loss = 1.476617\n",
      "2024-12-13 05:15:03.072000: I runner.py:310] Step = 112100 ; steps/s = 0.72, tokens/s = 34067 (16415 source, 17652 target) ; Learning rate = 0.000264 ; Loss = 1.481349\n",
      "2024-12-13 05:17:22.174000: I runner.py:310] Step = 112200 ; steps/s = 0.72, tokens/s = 34068 (16413 source, 17655 target) ; Learning rate = 0.000264 ; Loss = 1.483888\n",
      "2024-12-13 05:19:38.918000: I runner.py:310] Step = 112300 ; steps/s = 0.73, tokens/s = 33952 (16350 source, 17602 target) ; Learning rate = 0.000264 ; Loss = 1.484157\n",
      "2024-12-13 05:21:57.666000: I runner.py:310] Step = 112400 ; steps/s = 0.72, tokens/s = 34149 (16460 source, 17689 target) ; Learning rate = 0.000264 ; Loss = 1.478778\n",
      "2024-12-13 05:24:16.806000: I runner.py:310] Step = 112500 ; steps/s = 0.72, tokens/s = 34071 (16414 source, 17657 target) ; Learning rate = 0.000264 ; Loss = 1.477780\n",
      "2024-12-13 05:26:35.893000: I runner.py:310] Step = 112600 ; steps/s = 0.72, tokens/s = 34065 (16419 source, 17646 target) ; Learning rate = 0.000263 ; Loss = 1.481998\n",
      "2024-12-13 05:28:52.702000: I runner.py:310] Step = 112700 ; steps/s = 0.73, tokens/s = 33945 (16348 source, 17597 target) ; Learning rate = 0.000263 ; Loss = 1.486842\n",
      "2024-12-13 05:31:11.851000: I runner.py:310] Step = 112800 ; steps/s = 0.72, tokens/s = 34061 (16407 source, 17654 target) ; Learning rate = 0.000263 ; Loss = 1.479585\n",
      "2024-12-13 05:33:30.964000: I runner.py:310] Step = 112900 ; steps/s = 0.72, tokens/s = 34050 (16410 source, 17640 target) ; Learning rate = 0.000263 ; Loss = 1.479571\n",
      "2024-12-13 05:35:50.115000: I runner.py:310] Step = 113000 ; steps/s = 0.72, tokens/s = 34054 (16410 source, 17644 target) ; Learning rate = 0.000263 ; Loss = 1.475791\n",
      "2024-12-13 05:38:06.916000: I runner.py:310] Step = 113100 ; steps/s = 0.73, tokens/s = 33955 (16355 source, 17600 target) ; Learning rate = 0.000263 ; Loss = 1.484722\n",
      "2024-12-13 05:40:26.012000: I runner.py:310] Step = 113200 ; steps/s = 0.72, tokens/s = 34074 (16420 source, 17654 target) ; Learning rate = 0.000263 ; Loss = 1.478165\n",
      "2024-12-13 05:42:45.098000: I runner.py:310] Step = 113300 ; steps/s = 0.72, tokens/s = 34057 (16408 source, 17649 target) ; Learning rate = 0.000263 ; Loss = 1.473918\n",
      "2024-12-13 05:45:04.199000: I runner.py:310] Step = 113400 ; steps/s = 0.72, tokens/s = 34064 (16416 source, 17648 target) ; Learning rate = 0.000262 ; Loss = 1.480486\n",
      "2024-12-13 05:47:21.233000: I runner.py:310] Step = 113500 ; steps/s = 0.73, tokens/s = 33926 (16346 source, 17580 target) ; Learning rate = 0.000262 ; Loss = 1.488950\n",
      "2024-12-13 05:49:40.233000: I runner.py:310] Step = 113600 ; steps/s = 0.72, tokens/s = 34054 (16397 source, 17657 target) ; Learning rate = 0.000262 ; Loss = 1.475564\n",
      "2024-12-13 05:51:59.343000: I runner.py:310] Step = 113700 ; steps/s = 0.72, tokens/s = 34072 (16410 source, 17662 target) ; Learning rate = 0.000262 ; Loss = 1.484708\n",
      "2024-12-13 05:54:18.428000: I runner.py:310] Step = 113800 ; steps/s = 0.72, tokens/s = 34065 (16418 source, 17647 target) ; Learning rate = 0.000262 ; Loss = 1.489296\n",
      "2024-12-13 05:56:37.515000: I runner.py:310] Step = 113900 ; steps/s = 0.72, tokens/s = 34066 (16419 source, 17647 target) ; Learning rate = 0.000262 ; Loss = 1.481736\n",
      "2024-12-13 05:58:54.376000: I runner.py:310] Step = 114000 ; steps/s = 0.73, tokens/s = 33943 (16343 source, 17600 target) ; Learning rate = 0.000262 ; Loss = 1.475670\n",
      "2024-12-13 06:01:13.440000: I runner.py:310] Step = 114100 ; steps/s = 0.72, tokens/s = 34069 (16412 source, 17657 target) ; Learning rate = 0.000262 ; Loss = 1.477383\n",
      "2024-12-13 06:03:32.546000: I runner.py:310] Step = 114200 ; steps/s = 0.72, tokens/s = 34072 (16434 source, 17638 target) ; Learning rate = 0.000262 ; Loss = 1.481907\n",
      "2024-12-13 06:05:51.649000: I runner.py:310] Step = 114300 ; steps/s = 0.72, tokens/s = 34059 (16400 source, 17659 target) ; Learning rate = 0.000261 ; Loss = 1.479711\n",
      "2024-12-13 06:08:08.491000: I runner.py:310] Step = 114400 ; steps/s = 0.73, tokens/s = 33934 (16347 source, 17587 target) ; Learning rate = 0.000261 ; Loss = 1.476691\n",
      "2024-12-13 06:10:27.639000: I runner.py:310] Step = 114500 ; steps/s = 0.72, tokens/s = 34052 (16414 source, 17638 target) ; Learning rate = 0.000261 ; Loss = 1.477836\n",
      "2024-12-13 06:12:46.637000: I runner.py:310] Step = 114600 ; steps/s = 0.72, tokens/s = 34098 (16423 source, 17675 target) ; Learning rate = 0.000261 ; Loss = 1.484438\n",
      "2024-12-13 06:15:05.443000: I runner.py:310] Step = 114700 ; steps/s = 0.72, tokens/s = 34138 (16448 source, 17690 target) ; Learning rate = 0.000261 ; Loss = 1.475709\n",
      "2024-12-13 06:17:22.295000: I runner.py:310] Step = 114800 ; steps/s = 0.73, tokens/s = 33930 (16349 source, 17581 target) ; Learning rate = 0.000261 ; Loss = 1.475148\n",
      "2024-12-13 06:19:41.400000: I runner.py:310] Step = 114900 ; steps/s = 0.72, tokens/s = 34065 (16407 source, 17658 target) ; Learning rate = 0.000261 ; Loss = 1.479188\n",
      "2024-12-13 06:22:00.501000: I runner.py:310] Step = 115000 ; steps/s = 0.72, tokens/s = 34067 (16409 source, 17658 target) ; Learning rate = 0.000261 ; Loss = 1.477482\n",
      "2024-12-13 06:24:19.580000: I runner.py:310] Step = 115100 ; steps/s = 0.72, tokens/s = 34069 (16424 source, 17645 target) ; Learning rate = 0.000261 ; Loss = 1.479636\n",
      "2024-12-13 06:26:38.685000: I runner.py:310] Step = 115200 ; steps/s = 0.72, tokens/s = 34073 (16420 source, 17653 target) ; Learning rate = 0.000260 ; Loss = 1.475695\n",
      "2024-12-13 06:28:55.399000: I runner.py:310] Step = 115300 ; steps/s = 0.73, tokens/s = 33968 (16364 source, 17604 target) ; Learning rate = 0.000260 ; Loss = 1.472126\n",
      "2024-12-13 06:31:14.517000: I runner.py:310] Step = 115400 ; steps/s = 0.72, tokens/s = 34064 (16409 source, 17655 target) ; Learning rate = 0.000260 ; Loss = 1.480911\n",
      "2024-12-13 06:33:33.651000: I runner.py:310] Step = 115500 ; steps/s = 0.72, tokens/s = 34043 (16401 source, 17642 target) ; Learning rate = 0.000260 ; Loss = 1.479681\n",
      "2024-12-13 06:35:52.814000: I runner.py:310] Step = 115600 ; steps/s = 0.72, tokens/s = 34071 (16412 source, 17659 target) ; Learning rate = 0.000260 ; Loss = 1.480960\n",
      "2024-12-13 06:38:09.601000: I runner.py:310] Step = 115700 ; steps/s = 0.73, tokens/s = 33936 (16356 source, 17580 target) ; Learning rate = 0.000260 ; Loss = 1.472348\n",
      "2024-12-13 06:40:28.673000: I runner.py:310] Step = 115800 ; steps/s = 0.72, tokens/s = 34068 (16413 source, 17655 target) ; Learning rate = 0.000260 ; Loss = 1.485938\n",
      "2024-12-13 06:42:47.829000: I runner.py:310] Step = 115900 ; steps/s = 0.72, tokens/s = 34052 (16403 source, 17649 target) ; Learning rate = 0.000260 ; Loss = 1.481600\n",
      "2024-12-13 06:45:06.956000: I runner.py:310] Step = 116000 ; steps/s = 0.72, tokens/s = 34066 (16424 source, 17642 target) ; Learning rate = 0.000260 ; Loss = 1.477651\n",
      "2024-12-13 06:47:23.740000: I runner.py:310] Step = 116100 ; steps/s = 0.73, tokens/s = 33964 (16364 source, 17600 target) ; Learning rate = 0.000259 ; Loss = 1.480498\n",
      "2024-12-13 06:49:42.842000: I runner.py:310] Step = 116200 ; steps/s = 0.72, tokens/s = 34062 (16406 source, 17656 target) ; Learning rate = 0.000259 ; Loss = 1.477315\n",
      "2024-12-13 06:52:01.964000: I runner.py:310] Step = 116300 ; steps/s = 0.72, tokens/s = 34063 (16417 source, 17646 target) ; Learning rate = 0.000259 ; Loss = 1.479743\n",
      "2024-12-13 06:54:21.002000: I runner.py:310] Step = 116400 ; steps/s = 0.72, tokens/s = 34078 (16409 source, 17669 target) ; Learning rate = 0.000259 ; Loss = 1.489530\n",
      "2024-12-13 06:56:37.884000: I runner.py:310] Step = 116500 ; steps/s = 0.73, tokens/s = 33915 (16345 source, 17570 target) ; Learning rate = 0.000259 ; Loss = 1.480297\n",
      "2024-12-13 06:58:57.024000: I runner.py:310] Step = 116600 ; steps/s = 0.72, tokens/s = 34061 (16402 source, 17659 target) ; Learning rate = 0.000259 ; Loss = 1.474601\n",
      "2024-12-13 07:01:16.118000: I runner.py:310] Step = 116700 ; steps/s = 0.72, tokens/s = 34071 (16426 source, 17645 target) ; Learning rate = 0.000259 ; Loss = 1.477756\n",
      "2024-12-13 07:03:35.175000: I runner.py:310] Step = 116800 ; steps/s = 0.72, tokens/s = 34058 (16411 source, 17647 target) ; Learning rate = 0.000259 ; Loss = 1.481448\n",
      "2024-12-13 07:05:54.241000: I runner.py:310] Step = 116900 ; steps/s = 0.72, tokens/s = 34095 (16425 source, 17670 target) ; Learning rate = 0.000259 ; Loss = 1.485030\n",
      "2024-12-13 07:08:10.711000: I runner.py:310] Step = 117000 ; steps/s = 0.73, tokens/s = 34032 (16395 source, 17637 target) ; Learning rate = 0.000258 ; Loss = 1.474980\n",
      "2024-12-13 07:10:29.870000: I runner.py:310] Step = 117100 ; steps/s = 0.72, tokens/s = 34047 (16413 source, 17634 target) ; Learning rate = 0.000258 ; Loss = 1.474547\n",
      "2024-12-13 07:12:48.969000: I runner.py:310] Step = 117200 ; steps/s = 0.72, tokens/s = 34062 (16415 source, 17647 target) ; Learning rate = 0.000258 ; Loss = 1.479368\n",
      "2024-12-13 07:15:08.059000: I runner.py:310] Step = 117300 ; steps/s = 0.72, tokens/s = 34071 (16408 source, 17663 target) ; Learning rate = 0.000258 ; Loss = 1.484298\n",
      "2024-12-13 07:17:24.897000: I runner.py:310] Step = 117400 ; steps/s = 0.73, tokens/s = 33938 (16352 source, 17586 target) ; Learning rate = 0.000258 ; Loss = 1.480679\n",
      "2024-12-13 07:19:44.000000: I runner.py:310] Step = 117500 ; steps/s = 0.72, tokens/s = 34061 (16404 source, 17657 target) ; Learning rate = 0.000258 ; Loss = 1.477069\n",
      "2024-12-13 07:22:03.091000: I runner.py:310] Step = 117600 ; steps/s = 0.72, tokens/s = 34074 (16415 source, 17659 target) ; Learning rate = 0.000258 ; Loss = 1.480311\n",
      "2024-12-13 07:24:22.238000: I runner.py:310] Step = 117700 ; steps/s = 0.72, tokens/s = 34046 (16408 source, 17638 target) ; Learning rate = 0.000258 ; Loss = 1.483125\n",
      "2024-12-13 07:26:39.084000: I runner.py:310] Step = 117800 ; steps/s = 0.73, tokens/s = 33939 (16356 source, 17583 target) ; Learning rate = 0.000258 ; Loss = 1.475398\n",
      "2024-12-13 07:28:58.211000: I runner.py:310] Step = 117900 ; steps/s = 0.72, tokens/s = 34059 (16395 source, 17664 target) ; Learning rate = 0.000257 ; Loss = 1.482054\n",
      "2024-12-13 07:31:17.308000: I runner.py:310] Step = 118000 ; steps/s = 0.72, tokens/s = 34068 (16418 source, 17650 target) ; Learning rate = 0.000257 ; Loss = 1.482861\n",
      "2024-12-13 07:33:36.458000: I runner.py:310] Step = 118100 ; steps/s = 0.72, tokens/s = 34063 (16417 source, 17646 target) ; Learning rate = 0.000257 ; Loss = 1.481611\n",
      "2024-12-13 07:35:53.309000: I runner.py:310] Step = 118200 ; steps/s = 0.73, tokens/s = 33921 (16344 source, 17577 target) ; Learning rate = 0.000257 ; Loss = 1.478565\n",
      "2024-12-13 07:38:12.399000: I runner.py:310] Step = 118300 ; steps/s = 0.72, tokens/s = 34059 (16390 source, 17669 target) ; Learning rate = 0.000257 ; Loss = 1.479929\n",
      "2024-12-13 07:40:31.516000: I runner.py:310] Step = 118400 ; steps/s = 0.72, tokens/s = 34066 (16430 source, 17636 target) ; Learning rate = 0.000257 ; Loss = 1.481242\n",
      "2024-12-13 07:42:50.631000: I runner.py:310] Step = 118500 ; steps/s = 0.72, tokens/s = 34071 (16418 source, 17653 target) ; Learning rate = 0.000257 ; Loss = 1.477079\n",
      "2024-12-13 07:45:09.754000: I runner.py:310] Step = 118600 ; steps/s = 0.72, tokens/s = 34070 (16415 source, 17655 target) ; Learning rate = 0.000257 ; Loss = 1.482476\n",
      "2024-12-13 07:47:26.580000: I runner.py:310] Step = 118700 ; steps/s = 0.73, tokens/s = 33951 (16345 source, 17606 target) ; Learning rate = 0.000257 ; Loss = 1.471255\n",
      "2024-12-13 07:49:45.703000: I runner.py:310] Step = 118800 ; steps/s = 0.72, tokens/s = 34044 (16417 source, 17627 target) ; Learning rate = 0.000256 ; Loss = 1.477171\n",
      "2024-12-13 07:52:04.775000: I runner.py:310] Step = 118900 ; steps/s = 0.72, tokens/s = 34061 (16400 source, 17661 target) ; Learning rate = 0.000256 ; Loss = 1.475780\n",
      "2024-12-13 07:54:23.877000: I runner.py:310] Step = 119000 ; steps/s = 0.72, tokens/s = 34072 (16421 source, 17651 target) ; Learning rate = 0.000256 ; Loss = 1.477873\n",
      "2024-12-13 07:56:40.730000: I runner.py:310] Step = 119100 ; steps/s = 0.73, tokens/s = 33946 (16350 source, 17596 target) ; Learning rate = 0.000256 ; Loss = 1.470617\n",
      "2024-12-13 07:58:59.753000: I runner.py:310] Step = 119200 ; steps/s = 0.72, tokens/s = 34087 (16430 source, 17657 target) ; Learning rate = 0.000256 ; Loss = 1.476934\n",
      "2024-12-13 08:01:18.585000: I runner.py:310] Step = 119300 ; steps/s = 0.72, tokens/s = 34125 (16443 source, 17682 target) ; Learning rate = 0.000256 ; Loss = 1.477674\n",
      "2024-12-13 08:03:37.700000: I runner.py:310] Step = 119400 ; steps/s = 0.72, tokens/s = 34058 (16414 source, 17644 target) ; Learning rate = 0.000256 ; Loss = 1.474937\n",
      "2024-12-13 08:05:54.601000: I runner.py:310] Step = 119500 ; steps/s = 0.73, tokens/s = 33931 (16341 source, 17590 target) ; Learning rate = 0.000256 ; Loss = 1.479894\n",
      "2024-12-13 08:08:13.707000: I runner.py:310] Step = 119600 ; steps/s = 0.72, tokens/s = 34061 (16405 source, 17656 target) ; Learning rate = 0.000256 ; Loss = 1.476024\n",
      "2024-12-13 08:10:32.786000: I runner.py:310] Step = 119700 ; steps/s = 0.72, tokens/s = 34062 (16417 source, 17645 target) ; Learning rate = 0.000255 ; Loss = 1.480032\n",
      "2024-12-13 08:12:51.900000: I runner.py:310] Step = 119800 ; steps/s = 0.72, tokens/s = 34064 (16418 source, 17646 target) ; Learning rate = 0.000255 ; Loss = 1.486814\n",
      "2024-12-13 08:15:09.026000: I runner.py:310] Step = 119900 ; steps/s = 0.73, tokens/s = 33954 (16355 source, 17599 target) ; Learning rate = 0.000255 ; Loss = 1.487087\n",
      "2024-12-13 08:17:27.829000: I runner.py:310] Step = 120000 ; steps/s = 0.72, tokens/s = 34076 (16413 source, 17663 target) ; Learning rate = 0.000255 ; Loss = 1.476600\n",
      "2024-12-13 08:17:29.552000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-120000\n",
      "2024-12-13 08:19:48.640000: I runner.py:310] Step = 120100 ; steps/s = 0.72, tokens/s = 34044 (16408 source, 17636 target) ; Learning rate = 0.000255 ; Loss = 1.480872\n",
      "2024-12-13 08:22:07.732000: I runner.py:310] Step = 120200 ; steps/s = 0.72, tokens/s = 34067 (16418 source, 17649 target) ; Learning rate = 0.000255 ; Loss = 1.476614\n",
      "2024-12-13 08:24:26.892000: I runner.py:310] Step = 120300 ; steps/s = 0.72, tokens/s = 34074 (16413 source, 17661 target) ; Learning rate = 0.000255 ; Loss = 1.484491\n",
      "2024-12-13 08:26:43.689000: I runner.py:310] Step = 120400 ; steps/s = 0.73, tokens/s = 33931 (16345 source, 17586 target) ; Learning rate = 0.000255 ; Loss = 1.467433\n",
      "2024-12-13 08:29:02.801000: I runner.py:310] Step = 120500 ; steps/s = 0.72, tokens/s = 34048 (16401 source, 17647 target) ; Learning rate = 0.000255 ; Loss = 1.479522\n",
      "2024-12-13 08:31:21.905000: I runner.py:310] Step = 120600 ; steps/s = 0.72, tokens/s = 34068 (16422 source, 17646 target) ; Learning rate = 0.000255 ; Loss = 1.476871\n",
      "2024-12-13 08:33:41.029000: I runner.py:310] Step = 120700 ; steps/s = 0.72, tokens/s = 34067 (16405 source, 17662 target) ; Learning rate = 0.000254 ; Loss = 1.472190\n",
      "2024-12-13 08:35:57.876000: I runner.py:310] Step = 120800 ; steps/s = 0.73, tokens/s = 33951 (16365 source, 17586 target) ; Learning rate = 0.000254 ; Loss = 1.478264\n",
      "2024-12-13 08:38:16.960000: I runner.py:310] Step = 120900 ; steps/s = 0.72, tokens/s = 34072 (16411 source, 17661 target) ; Learning rate = 0.000254 ; Loss = 1.473287\n",
      "2024-12-13 08:40:36.048000: I runner.py:310] Step = 121000 ; steps/s = 0.72, tokens/s = 34055 (16414 source, 17641 target) ; Learning rate = 0.000254 ; Loss = 1.470585\n",
      "2024-12-13 08:42:55.182000: I runner.py:310] Step = 121100 ; steps/s = 0.72, tokens/s = 34062 (16407 source, 17655 target) ; Learning rate = 0.000254 ; Loss = 1.475813\n",
      "2024-12-13 08:45:12.054000: I runner.py:310] Step = 121200 ; steps/s = 0.73, tokens/s = 33933 (16348 source, 17585 target) ; Learning rate = 0.000254 ; Loss = 1.478918\n",
      "2024-12-13 08:47:31.195000: I runner.py:310] Step = 121300 ; steps/s = 0.72, tokens/s = 34066 (16417 source, 17649 target) ; Learning rate = 0.000254 ; Loss = 1.474186\n",
      "2024-12-13 08:49:50.317000: I runner.py:310] Step = 121400 ; steps/s = 0.72, tokens/s = 34057 (16409 source, 17648 target) ; Learning rate = 0.000254 ; Loss = 1.477006\n",
      "2024-12-13 08:52:09.363000: I runner.py:310] Step = 121500 ; steps/s = 0.72, tokens/s = 34080 (16412 source, 17668 target) ; Learning rate = 0.000254 ; Loss = 1.473018\n",
      "2024-12-13 08:54:28.202000: I runner.py:310] Step = 121600 ; steps/s = 0.72, tokens/s = 34121 (16443 source, 17678 target) ; Learning rate = 0.000253 ; Loss = 1.473506\n",
      "2024-12-13 08:56:45.072000: I runner.py:310] Step = 121700 ; steps/s = 0.73, tokens/s = 33929 (16343 source, 17586 target) ; Learning rate = 0.000253 ; Loss = 1.476755\n",
      "2024-12-13 08:59:04.152000: I runner.py:310] Step = 121800 ; steps/s = 0.72, tokens/s = 34084 (16417 source, 17667 target) ; Learning rate = 0.000253 ; Loss = 1.474421\n",
      "2024-12-13 09:01:23.299000: I runner.py:310] Step = 121900 ; steps/s = 0.72, tokens/s = 34037 (16399 source, 17638 target) ; Learning rate = 0.000253 ; Loss = 1.479536\n",
      "2024-12-13 09:03:42.432000: I runner.py:310] Step = 122000 ; steps/s = 0.72, tokens/s = 34059 (16414 source, 17645 target) ; Learning rate = 0.000253 ; Loss = 1.479656\n",
      "2024-12-13 09:05:59.217000: I runner.py:310] Step = 122100 ; steps/s = 0.73, tokens/s = 33948 (16343 source, 17605 target) ; Learning rate = 0.000253 ; Loss = 1.477473\n",
      "2024-12-13 09:08:18.355000: I runner.py:310] Step = 122200 ; steps/s = 0.72, tokens/s = 34072 (16416 source, 17656 target) ; Learning rate = 0.000253 ; Loss = 1.481384\n",
      "2024-12-13 09:10:37.515000: I runner.py:310] Step = 122300 ; steps/s = 0.72, tokens/s = 34036 (16415 source, 17621 target) ; Learning rate = 0.000253 ; Loss = 1.471173\n",
      "2024-12-13 09:12:56.615000: I runner.py:310] Step = 122400 ; steps/s = 0.72, tokens/s = 34069 (16412 source, 17657 target) ; Learning rate = 0.000253 ; Loss = 1.476503\n",
      "2024-12-13 09:15:13.490000: I runner.py:310] Step = 122500 ; steps/s = 0.73, tokens/s = 33939 (16357 source, 17582 target) ; Learning rate = 0.000253 ; Loss = 1.472749\n",
      "2024-12-13 09:17:32.564000: I runner.py:310] Step = 122600 ; steps/s = 0.72, tokens/s = 34061 (16403 source, 17658 target) ; Learning rate = 0.000252 ; Loss = 1.473739\n",
      "2024-12-13 09:19:51.675000: I runner.py:310] Step = 122700 ; steps/s = 0.72, tokens/s = 34042 (16407 source, 17635 target) ; Learning rate = 0.000252 ; Loss = 1.477140\n",
      "2024-12-13 09:22:10.811000: I runner.py:310] Step = 122800 ; steps/s = 0.72, tokens/s = 34071 (16417 source, 17654 target) ; Learning rate = 0.000252 ; Loss = 1.480495\n",
      "2024-12-13 09:24:27.620000: I runner.py:310] Step = 122900 ; steps/s = 0.73, tokens/s = 33946 (16351 source, 17595 target) ; Learning rate = 0.000252 ; Loss = 1.475118\n",
      "2024-12-13 09:26:46.765000: I runner.py:310] Step = 123000 ; steps/s = 0.72, tokens/s = 34057 (16412 source, 17645 target) ; Learning rate = 0.000252 ; Loss = 1.486085\n",
      "2024-12-13 09:29:05.873000: I runner.py:310] Step = 123100 ; steps/s = 0.72, tokens/s = 34080 (16420 source, 17660 target) ; Learning rate = 0.000252 ; Loss = 1.476144\n",
      "2024-12-13 09:31:25.047000: I runner.py:310] Step = 123200 ; steps/s = 0.72, tokens/s = 34055 (16409 source, 17646 target) ; Learning rate = 0.000252 ; Loss = 1.477350\n",
      "2024-12-13 09:33:44.185000: I runner.py:310] Step = 123300 ; steps/s = 0.72, tokens/s = 34037 (16397 source, 17640 target) ; Learning rate = 0.000252 ; Loss = 1.478849\n",
      "2024-12-13 09:36:01.044000: I runner.py:310] Step = 123400 ; steps/s = 0.73, tokens/s = 33931 (16341 source, 17590 target) ; Learning rate = 0.000252 ; Loss = 1.469338\n",
      "2024-12-13 09:38:20.153000: I runner.py:310] Step = 123500 ; steps/s = 0.72, tokens/s = 34059 (16409 source, 17650 target) ; Learning rate = 0.000252 ; Loss = 1.473963\n",
      "2024-12-13 09:40:39.239000: I runner.py:310] Step = 123600 ; steps/s = 0.72, tokens/s = 34061 (16410 source, 17651 target) ; Learning rate = 0.000251 ; Loss = 1.476086\n",
      "2024-12-13 09:42:58.355000: I runner.py:310] Step = 123700 ; steps/s = 0.72, tokens/s = 34071 (16428 source, 17643 target) ; Learning rate = 0.000251 ; Loss = 1.478064\n",
      "2024-12-13 09:45:15.091000: I runner.py:310] Step = 123800 ; steps/s = 0.73, tokens/s = 33974 (16361 source, 17613 target) ; Learning rate = 0.000251 ; Loss = 1.477203\n",
      "2024-12-13 09:47:33.914000: I runner.py:310] Step = 123900 ; steps/s = 0.72, tokens/s = 34134 (16441 source, 17693 target) ; Learning rate = 0.000251 ; Loss = 1.472326\n",
      "2024-12-13 09:49:53.066000: I runner.py:310] Step = 124000 ; steps/s = 0.72, tokens/s = 34040 (16406 source, 17634 target) ; Learning rate = 0.000251 ; Loss = 1.478200\n",
      "2024-12-13 09:52:12.210000: I runner.py:310] Step = 124100 ; steps/s = 0.72, tokens/s = 34066 (16415 source, 17651 target) ; Learning rate = 0.000251 ; Loss = 1.474419\n",
      "2024-12-13 09:54:29.039000: I runner.py:310] Step = 124200 ; steps/s = 0.73, tokens/s = 33929 (16344 source, 17585 target) ; Learning rate = 0.000251 ; Loss = 1.469413\n",
      "2024-12-13 09:56:48.147000: I runner.py:310] Step = 124300 ; steps/s = 0.72, tokens/s = 34064 (16409 source, 17655 target) ; Learning rate = 0.000251 ; Loss = 1.478732\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Kk-En -> Tr-En (TED2020)\n",
    "!onmt-main --model kk-tr-en-shared.py --config data.yml --auto_config train --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35f6fe27-9069-471c-8313-cc0249e113d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-13 10:49:32.500231: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-13 10:49:33.331780: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-13 10:49:33.331846: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-13 10:49:33.331854: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-13 10:49:34.295000: I onmt-main:8] Creating model directory KK-TR-EN-Shared-vocab-2\n",
      "2024-12-13 10:49:34.526000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-13 10:49:34.526000: I main.py:315] Using model:\n",
      "(model): MyCustomTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): WordEmbedder()\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): WordEmbedder()\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-13 10:49:34.530856: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-13 10:49:37.458391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-13 10:49:37.458954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7738 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-12-13 10:49:37.459333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 348 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "2024-12-13 10:49:37.463000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file: TED2020_tokens_dev_shared\n",
      "  eval_labels_file: TED2020_dev_target_tokens_shared.txt\n",
      "  source_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file: TED2020_tokens_train_shared\n",
      "  train_labels_file: TED2020_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: KK-TR-EN-Shared-vocab-2\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 250000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-13 10:49:37.802000: I inputter.py:316] Initialized source input layer:\n",
      "2024-12-13 10:49:37.802000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-13 10:49:37.802000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-13 10:49:37.876000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-13 10:49:37.876000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-13 10:49:37.876000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-13 10:49:37.900000: I runner.py:269] Restored checkpoint KK-TR-EN-Shared-vocab/ckpt-100000\n",
      "2024-12-13 10:49:37.903000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "2024-12-13 10:49:37.946000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-13 10:49:38.927199: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-13 10:49:39.045000: I main.py:325] Accumulate gradients of 7 iterations to reach effective batch size of 25000\n",
      "2024-12-13 10:49:39.167000: I mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2024-12-13 10:49:39.466000: I dataset_ops.py:2542] Training on 337547 examples\n",
      "2024-12-13 10:50:47.933358: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-13 10:50:49.041837: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-13 10:50:49.346492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-13 10:50:58.751000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-13 10:50:58.774000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-13 10:51:00.374000: I cross_device_ops.py:897] batch_all_reduce: 260 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-13 10:51:03.855000: I cross_device_ops.py:897] batch_all_reduce: 260 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-13 10:51:11.411000: I runner.py:310] Number of model parameters: 93326081\n",
      "2024-12-13 10:51:11.415000: I runner.py:310] Number of model weights: 260 (trainable = 260, non trainable = 0)\n",
      "2024-12-13 10:51:11.447000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-13 10:51:11.454000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-13 10:51:13.543000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-1\n",
      "2024-12-13 10:51:14.138000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-13 10:51:14.162000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-13 10:51:14.786000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-13 10:51:14.807000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-13 10:51:15.436000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-13 10:51:15.460000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-13 10:52:14.337000: I runner.py:310] Step = 100 ; steps/s = 1.63, tokens/s = 82504 (37931 source, 44573 target) ; Learning rate = 0.000009 ; Loss = 8.103913\n",
      "2024-12-13 10:53:16.577000: I runner.py:310] Step = 200 ; steps/s = 1.61, tokens/s = 81196 (37343 source, 43853 target) ; Learning rate = 0.000018 ; Loss = 6.618755\n",
      "2024-12-13 10:54:18.567000: I runner.py:310] Step = 300 ; steps/s = 1.61, tokens/s = 81553 (37530 source, 44023 target) ; Learning rate = 0.000027 ; Loss = 6.214300\n",
      "2024-12-13 10:55:20.100000: I runner.py:310] Step = 400 ; steps/s = 1.63, tokens/s = 80734 (37116 source, 43618 target) ; Learning rate = 0.000035 ; Loss = 6.015805\n",
      "2024-12-13 10:56:21.489000: I runner.py:310] Step = 500 ; steps/s = 1.63, tokens/s = 82339 (37874 source, 44465 target) ; Learning rate = 0.000044 ; Loss = 5.816025\n",
      "2024-12-13 10:57:22.890000: I runner.py:310] Step = 600 ; steps/s = 1.63, tokens/s = 82302 (37854 source, 44448 target) ; Learning rate = 0.000053 ; Loss = 5.714592\n",
      "2024-12-13 10:58:24.010000: I runner.py:310] Step = 700 ; steps/s = 1.64, tokens/s = 81319 (37407 source, 43912 target) ; Learning rate = 0.000062 ; Loss = 5.639654\n",
      "2024-12-13 10:59:25.394000: I runner.py:310] Step = 800 ; steps/s = 1.63, tokens/s = 82345 (37885 source, 44460 target) ; Learning rate = 0.000071 ; Loss = 5.399076\n",
      "2024-12-13 11:00:26.775000: I runner.py:310] Step = 900 ; steps/s = 1.63, tokens/s = 82358 (37897 source, 44461 target) ; Learning rate = 0.000080 ; Loss = 5.493097\n",
      "2024-12-13 11:01:28.164000: I runner.py:310] Step = 1000 ; steps/s = 1.63, tokens/s = 82271 (37818 source, 44453 target) ; Learning rate = 0.000088 ; Loss = 5.274614\n",
      "2024-12-13 11:02:29.072000: I runner.py:310] Step = 1100 ; steps/s = 1.64, tokens/s = 81593 (37506 source, 44087 target) ; Learning rate = 0.000097 ; Loss = 5.318129\n",
      "2024-12-13 11:03:30.418000: I runner.py:310] Step = 1200 ; steps/s = 1.63, tokens/s = 82374 (37876 source, 44498 target) ; Learning rate = 0.000106 ; Loss = 5.190290\n",
      "2024-12-13 11:04:31.744000: I runner.py:310] Step = 1300 ; steps/s = 1.63, tokens/s = 82415 (37920 source, 44495 target) ; Learning rate = 0.000115 ; Loss = 4.845426\n",
      "2024-12-13 11:05:32.658000: I runner.py:310] Step = 1400 ; steps/s = 1.64, tokens/s = 81595 (37533 source, 44062 target) ; Learning rate = 0.000124 ; Loss = 4.474081\n",
      "2024-12-13 11:06:34.006000: I runner.py:310] Step = 1500 ; steps/s = 1.63, tokens/s = 82357 (37878 source, 44479 target) ; Learning rate = 0.000133 ; Loss = 4.156204\n",
      "2024-12-13 11:07:35.266000: I runner.py:310] Step = 1600 ; steps/s = 1.63, tokens/s = 82506 (37956 source, 44550 target) ; Learning rate = 0.000142 ; Loss = 4.173114\n",
      "2024-12-13 11:08:36.502000: I runner.py:310] Step = 1700 ; steps/s = 1.63, tokens/s = 82529 (37959 source, 44570 target) ; Learning rate = 0.000150 ; Loss = 3.977601\n",
      "2024-12-13 11:09:37.337000: I runner.py:310] Step = 1800 ; steps/s = 1.64, tokens/s = 81695 (37562 source, 44133 target) ; Learning rate = 0.000159 ; Loss = 3.872337\n",
      "2024-12-13 11:10:38.663000: I runner.py:310] Step = 1900 ; steps/s = 1.63, tokens/s = 82416 (37914 source, 44502 target) ; Learning rate = 0.000168 ; Loss = 3.797912\n",
      "2024-12-13 11:11:40.000000: I runner.py:310] Step = 2000 ; steps/s = 1.63, tokens/s = 82381 (37888 source, 44493 target) ; Learning rate = 0.000177 ; Loss = 3.761287\n",
      "2024-12-13 11:12:40.916000: I runner.py:310] Step = 2100 ; steps/s = 1.64, tokens/s = 81587 (37522 source, 44065 target) ; Learning rate = 0.000186 ; Loss = 3.698344\n",
      "2024-12-13 11:13:42.304000: I runner.py:310] Step = 2200 ; steps/s = 1.63, tokens/s = 82296 (37833 source, 44463 target) ; Learning rate = 0.000195 ; Loss = 3.700381\n",
      "2024-12-13 11:14:43.633000: I runner.py:310] Step = 2300 ; steps/s = 1.63, tokens/s = 82426 (37920 source, 44506 target) ; Learning rate = 0.000203 ; Loss = 3.565115\n",
      "2024-12-13 11:15:44.975000: I runner.py:310] Step = 2400 ; steps/s = 1.63, tokens/s = 82376 (37898 source, 44478 target) ; Learning rate = 0.000212 ; Loss = 3.557878\n",
      "2024-12-13 11:16:45.945000: I runner.py:310] Step = 2500 ; steps/s = 1.64, tokens/s = 81512 (37489 source, 44023 target) ; Learning rate = 0.000221 ; Loss = 3.474488\n",
      "2024-12-13 11:17:47.354000: I runner.py:310] Step = 2600 ; steps/s = 1.63, tokens/s = 82288 (37850 source, 44438 target) ; Learning rate = 0.000230 ; Loss = 3.413018\n",
      "2024-12-13 11:18:48.648000: I runner.py:310] Step = 2700 ; steps/s = 1.63, tokens/s = 82444 (37910 source, 44534 target) ; Learning rate = 0.000239 ; Loss = 3.438490\n",
      "2024-12-13 11:19:49.587000: I runner.py:310] Step = 2800 ; steps/s = 1.64, tokens/s = 81584 (37528 source, 44056 target) ; Learning rate = 0.000248 ; Loss = 3.431168\n",
      "2024-12-13 11:20:50.981000: I runner.py:310] Step = 2900 ; steps/s = 1.63, tokens/s = 82329 (37876 source, 44453 target) ; Learning rate = 0.000256 ; Loss = 3.451482\n",
      "2024-12-13 11:21:52.343000: I runner.py:310] Step = 3000 ; steps/s = 1.63, tokens/s = 82353 (37873 source, 44480 target) ; Learning rate = 0.000265 ; Loss = 3.334951\n",
      "2024-12-13 11:22:53.655000: I runner.py:310] Step = 3100 ; steps/s = 1.63, tokens/s = 82417 (37902 source, 44515 target) ; Learning rate = 0.000274 ; Loss = 3.274089\n",
      "2024-12-13 11:23:54.552000: I runner.py:310] Step = 3200 ; steps/s = 1.64, tokens/s = 81597 (37522 source, 44075 target) ; Learning rate = 0.000283 ; Loss = 3.276782\n",
      "2024-12-13 11:24:55.916000: I runner.py:310] Step = 3300 ; steps/s = 1.63, tokens/s = 82365 (37869 source, 44496 target) ; Learning rate = 0.000292 ; Loss = 3.190796\n",
      "2024-12-13 11:25:57.283000: I runner.py:310] Step = 3400 ; steps/s = 1.63, tokens/s = 82384 (37921 source, 44463 target) ; Learning rate = 0.000301 ; Loss = 3.242881\n",
      "2024-12-13 11:26:58.237000: I runner.py:310] Step = 3500 ; steps/s = 1.64, tokens/s = 81529 (37500 source, 44029 target) ; Learning rate = 0.000309 ; Loss = 3.223518\n",
      "2024-12-13 11:27:59.517000: I runner.py:310] Step = 3600 ; steps/s = 1.63, tokens/s = 82457 (37917 source, 44540 target) ; Learning rate = 0.000318 ; Loss = 3.065198\n",
      "2024-12-13 11:29:00.874000: I runner.py:310] Step = 3700 ; steps/s = 1.63, tokens/s = 82389 (37906 source, 44483 target) ; Learning rate = 0.000327 ; Loss = 3.089129\n",
      "2024-12-13 11:30:02.179000: I runner.py:310] Step = 3800 ; steps/s = 1.63, tokens/s = 82425 (37906 source, 44519 target) ; Learning rate = 0.000336 ; Loss = 3.260924\n",
      "2024-12-13 11:31:03.094000: I runner.py:310] Step = 3900 ; steps/s = 1.64, tokens/s = 81600 (37536 source, 44064 target) ; Learning rate = 0.000345 ; Loss = 3.062406\n",
      "2024-12-13 11:32:04.440000: I runner.py:310] Step = 4000 ; steps/s = 1.63, tokens/s = 82408 (37912 source, 44496 target) ; Learning rate = 0.000354 ; Loss = 3.108427\n",
      "2024-12-13 11:33:05.822000: I runner.py:310] Step = 4100 ; steps/s = 1.63, tokens/s = 82308 (37846 source, 44462 target) ; Learning rate = 0.000362 ; Loss = 3.180343\n",
      "2024-12-13 11:34:06.778000: I runner.py:310] Step = 4200 ; steps/s = 1.64, tokens/s = 81517 (37475 source, 44042 target) ; Learning rate = 0.000371 ; Loss = 3.009428\n",
      "2024-12-13 11:35:08.134000: I runner.py:310] Step = 4300 ; steps/s = 1.63, tokens/s = 82374 (37894 source, 44480 target) ; Learning rate = 0.000380 ; Loss = 3.081079\n",
      "2024-12-13 11:36:09.479000: I runner.py:310] Step = 4400 ; steps/s = 1.63, tokens/s = 82344 (37865 source, 44479 target) ; Learning rate = 0.000389 ; Loss = 3.045191\n",
      "2024-12-13 11:37:10.898000: I runner.py:310] Step = 4500 ; steps/s = 1.63, tokens/s = 82306 (37871 source, 44435 target) ; Learning rate = 0.000398 ; Loss = 3.032488\n",
      "2024-12-13 11:38:11.866000: I runner.py:310] Step = 4600 ; steps/s = 1.64, tokens/s = 81551 (37523 source, 44028 target) ; Learning rate = 0.000407 ; Loss = 2.952890\n",
      "2024-12-13 11:39:13.191000: I runner.py:310] Step = 4700 ; steps/s = 1.63, tokens/s = 82391 (37887 source, 44504 target) ; Learning rate = 0.000416 ; Loss = 2.896119\n",
      "2024-12-13 11:40:14.539000: I runner.py:310] Step = 4800 ; steps/s = 1.63, tokens/s = 82360 (37865 source, 44495 target) ; Learning rate = 0.000424 ; Loss = 2.968110\n",
      "2024-12-13 11:41:15.449000: I runner.py:310] Step = 4900 ; steps/s = 1.64, tokens/s = 81580 (37518 source, 44062 target) ; Learning rate = 0.000433 ; Loss = 2.873629\n",
      "2024-12-13 11:42:16.844000: I runner.py:310] Step = 5000 ; steps/s = 1.63, tokens/s = 82302 (37855 source, 44447 target) ; Learning rate = 0.000442 ; Loss = 2.920085\n",
      "2024-12-13 11:42:16.846000: I training.py:192] Running evaluation for step 5000\n",
      "2024-12-13 11:44:42.900000: I training.py:192] Evaluation result for step 5000: loss = 1.996395 ; perplexity = 7.362463\n",
      "2024-12-13 11:45:44.276000: I runner.py:310] Step = 5100 ; steps/s = 1.63, tokens/s = 82364 (37889 source, 44475 target) ; Learning rate = 0.000451 ; Loss = 2.956877\n",
      "2024-12-13 11:46:45.788000: I runner.py:310] Step = 5200 ; steps/s = 1.63, tokens/s = 82153 (37779 source, 44374 target) ; Learning rate = 0.000460 ; Loss = 2.954378\n",
      "2024-12-13 11:47:46.851000: I runner.py:310] Step = 5300 ; steps/s = 1.64, tokens/s = 81389 (37415 source, 43974 target) ; Learning rate = 0.000469 ; Loss = 2.834910\n",
      "2024-12-13 11:48:48.363000: I runner.py:310] Step = 5400 ; steps/s = 1.63, tokens/s = 82157 (37784 source, 44373 target) ; Learning rate = 0.000477 ; Loss = 2.881889\n",
      "2024-12-13 11:49:49.837000: I runner.py:310] Step = 5500 ; steps/s = 1.63, tokens/s = 82192 (37813 source, 44379 target) ; Learning rate = 0.000486 ; Loss = 2.823867\n",
      "2024-12-13 11:50:50.960000: I runner.py:310] Step = 5600 ; steps/s = 1.64, tokens/s = 81330 (37411 source, 43919 target) ; Learning rate = 0.000495 ; Loss = 2.838448\n",
      "2024-12-13 11:51:52.447000: I runner.py:310] Step = 5700 ; steps/s = 1.63, tokens/s = 82185 (37791 source, 44394 target) ; Learning rate = 0.000504 ; Loss = 2.789204\n",
      "2024-12-13 11:52:53.916000: I runner.py:310] Step = 5800 ; steps/s = 1.63, tokens/s = 82238 (37851 source, 44387 target) ; Learning rate = 0.000513 ; Loss = 2.909052\n",
      "2024-12-13 11:53:55.446000: I runner.py:310] Step = 5900 ; steps/s = 1.63, tokens/s = 82113 (37759 source, 44354 target) ; Learning rate = 0.000522 ; Loss = 2.769799\n",
      "2024-12-13 11:54:56.531000: I runner.py:310] Step = 6000 ; steps/s = 1.64, tokens/s = 81388 (37423 source, 43965 target) ; Learning rate = 0.000530 ; Loss = 2.764453\n",
      "2024-12-13 11:55:57.971000: I runner.py:310] Step = 6100 ; steps/s = 1.63, tokens/s = 82256 (37840 source, 44416 target) ; Learning rate = 0.000539 ; Loss = 2.788798\n",
      "2024-12-13 11:56:59.438000: I runner.py:310] Step = 6200 ; steps/s = 1.63, tokens/s = 82204 (37816 source, 44388 target) ; Learning rate = 0.000548 ; Loss = 2.774157\n",
      "2024-12-13 11:58:00.470000: I runner.py:310] Step = 6300 ; steps/s = 1.64, tokens/s = 81434 (37449 source, 43985 target) ; Learning rate = 0.000557 ; Loss = 2.706398\n",
      "2024-12-13 11:59:01.955000: I runner.py:310] Step = 6400 ; steps/s = 1.63, tokens/s = 82179 (37801 source, 44378 target) ; Learning rate = 0.000566 ; Loss = 2.755565\n",
      "2024-12-13 12:00:03.364000: I runner.py:310] Step = 6500 ; steps/s = 1.63, tokens/s = 82317 (37862 source, 44455 target) ; Learning rate = 0.000575 ; Loss = 2.764386\n",
      "2024-12-13 12:01:04.850000: I runner.py:310] Step = 6600 ; steps/s = 1.63, tokens/s = 82180 (37793 source, 44387 target) ; Learning rate = 0.000583 ; Loss = 2.742410\n",
      "2024-12-13 12:02:05.866000: I runner.py:310] Step = 6700 ; steps/s = 1.64, tokens/s = 81453 (37468 source, 43985 target) ; Learning rate = 0.000592 ; Loss = 2.789649\n",
      "2024-12-13 12:03:07.317000: I runner.py:310] Step = 6800 ; steps/s = 1.63, tokens/s = 82233 (37821 source, 44412 target) ; Learning rate = 0.000601 ; Loss = 2.693882\n",
      "2024-12-13 12:04:08.732000: I runner.py:310] Step = 6900 ; steps/s = 1.63, tokens/s = 82267 (37836 source, 44431 target) ; Learning rate = 0.000610 ; Loss = 2.647337\n",
      "2024-12-13 12:05:09.717000: I runner.py:310] Step = 7000 ; steps/s = 1.64, tokens/s = 81504 (37483 source, 44021 target) ; Learning rate = 0.000619 ; Loss = 2.628222\n",
      "2024-12-13 12:06:11.153000: I runner.py:310] Step = 7100 ; steps/s = 1.63, tokens/s = 82275 (37842 source, 44433 target) ; Learning rate = 0.000628 ; Loss = 2.616594\n",
      "2024-12-13 12:07:12.526000: I runner.py:310] Step = 7200 ; steps/s = 1.63, tokens/s = 82336 (37869 source, 44467 target) ; Learning rate = 0.000636 ; Loss = 2.657197\n",
      "2024-12-13 12:08:13.949000: I runner.py:310] Step = 7300 ; steps/s = 1.63, tokens/s = 82262 (37836 source, 44426 target) ; Learning rate = 0.000645 ; Loss = 2.659510\n",
      "2024-12-13 12:09:14.876000: I runner.py:310] Step = 7400 ; steps/s = 1.64, tokens/s = 81580 (37513 source, 44067 target) ; Learning rate = 0.000654 ; Loss = 2.564634\n",
      "2024-12-13 12:10:16.265000: I runner.py:310] Step = 7500 ; steps/s = 1.63, tokens/s = 82344 (37883 source, 44461 target) ; Learning rate = 0.000663 ; Loss = 2.681327\n",
      "2024-12-13 12:11:17.673000: I runner.py:310] Step = 7600 ; steps/s = 1.63, tokens/s = 82294 (37855 source, 44439 target) ; Learning rate = 0.000672 ; Loss = 2.679036\n",
      "2024-12-13 12:12:18.605000: I runner.py:310] Step = 7700 ; steps/s = 1.64, tokens/s = 81534 (37486 source, 44048 target) ; Learning rate = 0.000681 ; Loss = 2.641627\n",
      "2024-12-13 12:13:19.909000: I runner.py:310] Step = 7800 ; steps/s = 1.63, tokens/s = 82416 (37905 source, 44511 target) ; Learning rate = 0.000690 ; Loss = 2.541487\n",
      "2024-12-13 12:14:21.352000: I runner.py:310] Step = 7900 ; steps/s = 1.63, tokens/s = 82253 (37818 source, 44435 target) ; Learning rate = 0.000698 ; Loss = 2.602786\n",
      "2024-12-13 12:15:22.579000: I runner.py:310] Step = 8000 ; steps/s = 1.63, tokens/s = 82113 (37784 source, 44329 target) ; Learning rate = 0.000707 ; Loss = 2.491058\n",
      "2024-12-13 12:16:23.665000: I runner.py:310] Step = 8100 ; steps/s = 1.64, tokens/s = 81838 (37637 source, 44201 target) ; Learning rate = 0.000716 ; Loss = 2.465643\n",
      "2024-12-13 12:17:25.038000: I runner.py:310] Step = 8200 ; steps/s = 1.63, tokens/s = 82332 (37876 source, 44456 target) ; Learning rate = 0.000725 ; Loss = 2.561077\n",
      "2024-12-13 12:18:26.439000: I runner.py:310] Step = 8300 ; steps/s = 1.63, tokens/s = 82303 (37851 source, 44452 target) ; Learning rate = 0.000734 ; Loss = 2.550086\n",
      "2024-12-13 12:19:27.456000: I runner.py:310] Step = 8400 ; steps/s = 1.64, tokens/s = 81422 (37433 source, 43989 target) ; Learning rate = 0.000743 ; Loss = 2.582042\n",
      "2024-12-13 12:20:28.793000: I runner.py:310] Step = 8500 ; steps/s = 1.63, tokens/s = 82365 (37868 source, 44497 target) ; Learning rate = 0.000751 ; Loss = 2.541525\n",
      "2024-12-13 12:21:30.142000: I runner.py:310] Step = 8600 ; steps/s = 1.63, tokens/s = 82400 (37920 source, 44480 target) ; Learning rate = 0.000760 ; Loss = 2.507073\n",
      "2024-12-13 12:22:31.178000: I runner.py:310] Step = 8700 ; steps/s = 1.64, tokens/s = 81454 (37475 source, 43979 target) ; Learning rate = 0.000769 ; Loss = 2.474618\n",
      "2024-12-13 12:23:32.599000: I runner.py:310] Step = 8800 ; steps/s = 1.63, tokens/s = 82254 (37810 source, 44444 target) ; Learning rate = 0.000778 ; Loss = 2.533784\n",
      "2024-12-13 12:24:34.017000: I runner.py:310] Step = 8900 ; steps/s = 1.63, tokens/s = 82271 (37832 source, 44439 target) ; Learning rate = 0.000787 ; Loss = 2.460251\n",
      "2024-12-13 12:25:35.370000: I runner.py:310] Step = 9000 ; steps/s = 1.63, tokens/s = 82390 (37919 source, 44471 target) ; Learning rate = 0.000796 ; Loss = 2.527223\n",
      "2024-12-13 12:26:36.399000: I runner.py:310] Step = 9100 ; steps/s = 1.64, tokens/s = 81463 (37483 source, 43980 target) ; Learning rate = 0.000804 ; Loss = 2.450857\n",
      "2024-12-13 12:27:37.820000: I runner.py:310] Step = 9200 ; steps/s = 1.63, tokens/s = 82266 (37824 source, 44442 target) ; Learning rate = 0.000813 ; Loss = 2.495539\n",
      "2024-12-13 12:28:39.169000: I runner.py:310] Step = 9300 ; steps/s = 1.63, tokens/s = 82352 (37877 source, 44475 target) ; Learning rate = 0.000822 ; Loss = 2.526340\n",
      "2024-12-13 12:29:40.193000: I runner.py:310] Step = 9400 ; steps/s = 1.64, tokens/s = 81468 (37470 source, 43998 target) ; Learning rate = 0.000831 ; Loss = 2.438774\n",
      "2024-12-13 12:30:41.557000: I runner.py:310] Step = 9500 ; steps/s = 1.63, tokens/s = 82384 (37905 source, 44479 target) ; Learning rate = 0.000840 ; Loss = 2.474777\n",
      "2024-12-13 12:31:42.913000: I runner.py:310] Step = 9600 ; steps/s = 1.63, tokens/s = 82328 (37857 source, 44471 target) ; Learning rate = 0.000849 ; Loss = 2.454501\n",
      "2024-12-13 12:32:44.251000: I runner.py:310] Step = 9700 ; steps/s = 1.63, tokens/s = 82382 (37879 source, 44503 target) ; Learning rate = 0.000857 ; Loss = 2.494308\n",
      "2024-12-13 12:33:45.188000: I runner.py:310] Step = 9800 ; steps/s = 1.64, tokens/s = 81546 (37507 source, 44039 target) ; Learning rate = 0.000866 ; Loss = 2.503667\n",
      "2024-12-13 12:34:46.510000: I runner.py:310] Step = 9900 ; steps/s = 1.63, tokens/s = 82431 (37927 source, 44504 target) ; Learning rate = 0.000875 ; Loss = 2.448456\n",
      "2024-12-13 12:35:47.887000: I runner.py:310] Step = 10000 ; steps/s = 1.63, tokens/s = 82328 (37867 source, 44461 target) ; Learning rate = 0.000884 ; Loss = 2.430911\n",
      "2024-12-13 12:35:49.602000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-10000\n",
      "2024-12-13 12:35:49.602000: I training.py:192] Running evaluation for step 10000\n",
      "2024-12-13 12:37:52.875000: I training.py:192] Evaluation result for step 10000: loss = 1.975831 ; perplexity = 7.212610\n",
      "2024-12-13 12:38:53.747000: I runner.py:310] Step = 10100 ; steps/s = 1.64, tokens/s = 81710 (37572 source, 44138 target) ; Learning rate = 0.000879 ; Loss = 2.426731\n",
      "2024-12-13 12:39:55.131000: I runner.py:310] Step = 10200 ; steps/s = 1.63, tokens/s = 82310 (37846 source, 44464 target) ; Learning rate = 0.000875 ; Loss = 2.404415\n",
      "2024-12-13 12:40:56.505000: I runner.py:310] Step = 10300 ; steps/s = 1.63, tokens/s = 82318 (37857 source, 44461 target) ; Learning rate = 0.000871 ; Loss = 2.439970\n",
      "2024-12-13 12:41:57.930000: I runner.py:310] Step = 10400 ; steps/s = 1.63, tokens/s = 82271 (37843 source, 44428 target) ; Learning rate = 0.000867 ; Loss = 2.450098\n",
      "2024-12-13 12:42:58.896000: I runner.py:310] Step = 10500 ; steps/s = 1.64, tokens/s = 81522 (37487 source, 44035 target) ; Learning rate = 0.000863 ; Loss = 2.405991\n",
      "2024-12-13 12:44:00.294000: I runner.py:310] Step = 10600 ; steps/s = 1.63, tokens/s = 82305 (37857 source, 44448 target) ; Learning rate = 0.000858 ; Loss = 2.365200\n",
      "2024-12-13 12:45:01.624000: I runner.py:310] Step = 10700 ; steps/s = 1.63, tokens/s = 82422 (37906 source, 44516 target) ; Learning rate = 0.000854 ; Loss = 2.429672\n",
      "2024-12-13 12:46:02.659000: I runner.py:310] Step = 10800 ; steps/s = 1.64, tokens/s = 81440 (37474 source, 43966 target) ; Learning rate = 0.000850 ; Loss = 2.359746\n",
      "2024-12-13 12:47:04.038000: I runner.py:310] Step = 10900 ; steps/s = 1.63, tokens/s = 82352 (37883 source, 44469 target) ; Learning rate = 0.000847 ; Loss = 2.396650\n",
      "2024-12-13 12:48:05.452000: I runner.py:310] Step = 11000 ; steps/s = 1.63, tokens/s = 82284 (37848 source, 44436 target) ; Learning rate = 0.000843 ; Loss = 2.373912\n",
      "2024-12-13 12:49:06.818000: I runner.py:310] Step = 11100 ; steps/s = 1.63, tokens/s = 82342 (37865 source, 44477 target) ; Learning rate = 0.000839 ; Loss = 2.440535\n",
      "2024-12-13 12:50:07.836000: I runner.py:310] Step = 11200 ; steps/s = 1.64, tokens/s = 81437 (37458 source, 43979 target) ; Learning rate = 0.000835 ; Loss = 2.397746\n",
      "2024-12-13 12:51:09.222000: I runner.py:310] Step = 11300 ; steps/s = 1.63, tokens/s = 82311 (37837 source, 44474 target) ; Learning rate = 0.000831 ; Loss = 2.358093\n",
      "2024-12-13 12:52:10.579000: I runner.py:310] Step = 11400 ; steps/s = 1.63, tokens/s = 82398 (37933 source, 44465 target) ; Learning rate = 0.000828 ; Loss = 2.356861\n",
      "2024-12-13 12:53:11.529000: I runner.py:310] Step = 11500 ; steps/s = 1.64, tokens/s = 81503 (37465 source, 44038 target) ; Learning rate = 0.000824 ; Loss = 2.329953\n",
      "2024-12-13 12:54:12.992000: I runner.py:310] Step = 11600 ; steps/s = 1.63, tokens/s = 82205 (37799 source, 44406 target) ; Learning rate = 0.000821 ; Loss = 2.329964\n",
      "2024-12-13 12:55:14.408000: I runner.py:310] Step = 11700 ; steps/s = 1.63, tokens/s = 82296 (37848 source, 44448 target) ; Learning rate = 0.000817 ; Loss = 2.278385\n",
      "2024-12-13 12:56:15.804000: I runner.py:310] Step = 11800 ; steps/s = 1.63, tokens/s = 82318 (37867 source, 44451 target) ; Learning rate = 0.000814 ; Loss = 2.341936\n",
      "2024-12-13 12:57:16.742000: I runner.py:310] Step = 11900 ; steps/s = 1.64, tokens/s = 81570 (37522 source, 44048 target) ; Learning rate = 0.000810 ; Loss = 2.362394\n",
      "2024-12-13 12:58:18.141000: I runner.py:310] Step = 12000 ; steps/s = 1.63, tokens/s = 82317 (37856 source, 44461 target) ; Learning rate = 0.000807 ; Loss = 2.301951\n",
      "2024-12-13 12:59:19.589000: I runner.py:310] Step = 12100 ; steps/s = 1.63, tokens/s = 82210 (37802 source, 44408 target) ; Learning rate = 0.000803 ; Loss = 2.301930\n",
      "2024-12-13 13:00:20.599000: I runner.py:310] Step = 12200 ; steps/s = 1.64, tokens/s = 81488 (37499 source, 43989 target) ; Learning rate = 0.000800 ; Loss = 2.334536\n",
      "2024-12-13 13:01:22.020000: I runner.py:310] Step = 12300 ; steps/s = 1.63, tokens/s = 82268 (37832 source, 44436 target) ; Learning rate = 0.000797 ; Loss = 2.318417\n",
      "2024-12-13 13:02:23.422000: I runner.py:310] Step = 12400 ; steps/s = 1.63, tokens/s = 82316 (37871 source, 44445 target) ; Learning rate = 0.000794 ; Loss = 2.296735\n",
      "2024-12-13 13:03:24.881000: I runner.py:310] Step = 12500 ; steps/s = 1.63, tokens/s = 82226 (37815 source, 44411 target) ; Learning rate = 0.000791 ; Loss = 2.338247\n",
      "2024-12-13 13:04:25.861000: I runner.py:310] Step = 12600 ; steps/s = 1.64, tokens/s = 81496 (37489 source, 44007 target) ; Learning rate = 0.000787 ; Loss = 2.316708\n",
      "2024-12-13 13:05:27.277000: I runner.py:310] Step = 12700 ; steps/s = 1.63, tokens/s = 82265 (37834 source, 44431 target) ; Learning rate = 0.000784 ; Loss = 2.288557\n",
      "2024-12-13 13:06:28.683000: I runner.py:310] Step = 12800 ; steps/s = 1.63, tokens/s = 82314 (37863 source, 44451 target) ; Learning rate = 0.000781 ; Loss = 2.302404\n",
      "2024-12-13 13:07:29.735000: I runner.py:310] Step = 12900 ; steps/s = 1.64, tokens/s = 81411 (37444 source, 43967 target) ; Learning rate = 0.000778 ; Loss = 2.225353\n",
      "2024-12-13 13:08:31.104000: I runner.py:310] Step = 13000 ; steps/s = 1.63, tokens/s = 82351 (37875 source, 44476 target) ; Learning rate = 0.000775 ; Loss = 2.252186\n",
      "2024-12-13 13:09:32.532000: I runner.py:310] Step = 13100 ; steps/s = 1.63, tokens/s = 82245 (37807 source, 44438 target) ; Learning rate = 0.000772 ; Loss = 2.287823\n",
      "2024-12-13 13:10:33.946000: I runner.py:310] Step = 13200 ; steps/s = 1.63, tokens/s = 82307 (37870 source, 44437 target) ; Learning rate = 0.000769 ; Loss = 2.313778\n",
      "2024-12-13 13:11:34.932000: I runner.py:310] Step = 13300 ; steps/s = 1.64, tokens/s = 81514 (37500 source, 44014 target) ; Learning rate = 0.000766 ; Loss = 2.192521\n",
      "2024-12-13 13:12:36.366000: I runner.py:310] Step = 13400 ; steps/s = 1.63, tokens/s = 82257 (37826 source, 44431 target) ; Learning rate = 0.000764 ; Loss = 2.259994\n",
      "2024-12-13 13:13:37.771000: I runner.py:310] Step = 13500 ; steps/s = 1.63, tokens/s = 82289 (37837 source, 44452 target) ; Learning rate = 0.000761 ; Loss = 2.282522\n",
      "2024-12-13 13:14:38.852000: I runner.py:310] Step = 13600 ; steps/s = 1.64, tokens/s = 81367 (37429 source, 43938 target) ; Learning rate = 0.000758 ; Loss = 2.232431\n",
      "2024-12-13 13:15:40.256000: I runner.py:310] Step = 13700 ; steps/s = 1.63, tokens/s = 82324 (37890 source, 44434 target) ; Learning rate = 0.000755 ; Loss = 2.246575\n",
      "2024-12-13 13:16:41.709000: I runner.py:310] Step = 13800 ; steps/s = 1.63, tokens/s = 82206 (37790 source, 44416 target) ; Learning rate = 0.000752 ; Loss = 2.259387\n",
      "2024-12-13 13:17:43.124000: I runner.py:310] Step = 13900 ; steps/s = 1.63, tokens/s = 82276 (37837 source, 44439 target) ; Learning rate = 0.000750 ; Loss = 2.295063\n",
      "2024-12-13 13:18:44.131000: I runner.py:310] Step = 14000 ; steps/s = 1.64, tokens/s = 81451 (37466 source, 43985 target) ; Learning rate = 0.000747 ; Loss = 2.253083\n",
      "2024-12-13 13:19:45.575000: I runner.py:310] Step = 14100 ; steps/s = 1.63, tokens/s = 82251 (37827 source, 44424 target) ; Learning rate = 0.000744 ; Loss = 2.217368\n",
      "2024-12-13 13:20:46.993000: I runner.py:310] Step = 14200 ; steps/s = 1.63, tokens/s = 82271 (37844 source, 44427 target) ; Learning rate = 0.000742 ; Loss = 2.247669\n",
      "2024-12-13 13:21:48.041000: I runner.py:310] Step = 14300 ; steps/s = 1.64, tokens/s = 81429 (37443 source, 43986 target) ; Learning rate = 0.000739 ; Loss = 2.230744\n",
      "2024-12-13 13:22:49.479000: I runner.py:310] Step = 14400 ; steps/s = 1.63, tokens/s = 82257 (37830 source, 44427 target) ; Learning rate = 0.000737 ; Loss = 2.220258\n",
      "2024-12-13 13:23:50.913000: I runner.py:310] Step = 14500 ; steps/s = 1.63, tokens/s = 82252 (37840 source, 44412 target) ; Learning rate = 0.000734 ; Loss = 2.178132\n",
      "2024-12-13 13:24:52.401000: I runner.py:310] Step = 14600 ; steps/s = 1.63, tokens/s = 82198 (37806 source, 44392 target) ; Learning rate = 0.000731 ; Loss = 2.221466\n",
      "2024-12-13 13:25:53.449000: I runner.py:310] Step = 14700 ; steps/s = 1.64, tokens/s = 81438 (37459 source, 43979 target) ; Learning rate = 0.000729 ; Loss = 2.149852\n",
      "2024-12-13 13:26:54.837000: I runner.py:310] Step = 14800 ; steps/s = 1.63, tokens/s = 82303 (37853 source, 44450 target) ; Learning rate = 0.000727 ; Loss = 2.209275\n",
      "2024-12-13 13:27:56.222000: I runner.py:310] Step = 14900 ; steps/s = 1.63, tokens/s = 82331 (37863 source, 44468 target) ; Learning rate = 0.000724 ; Loss = 2.234006\n",
      "2024-12-13 13:28:57.206000: I runner.py:310] Step = 15000 ; steps/s = 1.64, tokens/s = 81489 (37483 source, 44006 target) ; Learning rate = 0.000722 ; Loss = 2.122488\n",
      "2024-12-13 13:28:57.208000: I training.py:192] Running evaluation for step 15000\n",
      "2024-12-13 13:31:07.850000: I training.py:192] Evaluation result for step 15000: loss = 2.085210 ; perplexity = 8.046278\n",
      "2024-12-13 13:32:09.047000: I runner.py:310] Step = 15100 ; steps/s = 1.63, tokens/s = 82611 (37994 source, 44617 target) ; Learning rate = 0.000719 ; Loss = 2.207050\n",
      "2024-12-13 13:33:10.503000: I runner.py:310] Step = 15200 ; steps/s = 1.63, tokens/s = 82230 (37831 source, 44399 target) ; Learning rate = 0.000717 ; Loss = 2.228015\n",
      "2024-12-13 13:34:11.909000: I runner.py:310] Step = 15300 ; steps/s = 1.63, tokens/s = 82288 (37842 source, 44446 target) ; Learning rate = 0.000715 ; Loss = 2.250037\n",
      "2024-12-13 13:35:12.933000: I runner.py:310] Step = 15400 ; steps/s = 1.64, tokens/s = 81455 (37461 source, 43994 target) ; Learning rate = 0.000712 ; Loss = 2.117652\n",
      "2024-12-13 13:36:14.364000: I runner.py:310] Step = 15500 ; steps/s = 1.63, tokens/s = 82245 (37813 source, 44432 target) ; Learning rate = 0.000710 ; Loss = 2.218137\n",
      "2024-12-13 13:37:15.751000: I runner.py:310] Step = 15600 ; steps/s = 1.63, tokens/s = 82343 (37886 source, 44457 target) ; Learning rate = 0.000708 ; Loss = 2.216356\n",
      "2024-12-13 13:38:16.876000: I runner.py:310] Step = 15700 ; steps/s = 1.64, tokens/s = 81281 (37375 source, 43906 target) ; Learning rate = 0.000705 ; Loss = 2.130285\n",
      "2024-12-13 13:39:18.284000: I runner.py:310] Step = 15800 ; steps/s = 1.63, tokens/s = 82310 (37870 source, 44440 target) ; Learning rate = 0.000703 ; Loss = 2.176213\n",
      "2024-12-13 13:40:19.693000: I runner.py:310] Step = 15900 ; steps/s = 1.63, tokens/s = 82294 (37836 source, 44458 target) ; Learning rate = 0.000701 ; Loss = 2.184367\n",
      "2024-12-13 13:41:21.055000: I runner.py:310] Step = 16000 ; steps/s = 1.63, tokens/s = 82354 (37884 source, 44470 target) ; Learning rate = 0.000699 ; Loss = 2.221701\n",
      "2024-12-13 13:42:22.039000: I runner.py:310] Step = 16100 ; steps/s = 1.64, tokens/s = 81495 (37479 source, 44016 target) ; Learning rate = 0.000697 ; Loss = 2.099304\n",
      "2024-12-13 13:43:23.405000: I runner.py:310] Step = 16200 ; steps/s = 1.63, tokens/s = 82356 (37881 source, 44475 target) ; Learning rate = 0.000694 ; Loss = 2.180597\n",
      "2024-12-13 13:44:24.784000: I runner.py:310] Step = 16300 ; steps/s = 1.63, tokens/s = 82334 (37860 source, 44474 target) ; Learning rate = 0.000692 ; Loss = 2.214736\n",
      "2024-12-13 13:45:25.773000: I runner.py:310] Step = 16400 ; steps/s = 1.64, tokens/s = 81499 (37502 source, 43997 target) ; Learning rate = 0.000690 ; Loss = 2.106091\n",
      "2024-12-13 13:46:27.221000: I runner.py:310] Step = 16500 ; steps/s = 1.63, tokens/s = 82223 (37803 source, 44420 target) ; Learning rate = 0.000688 ; Loss = 2.171474\n",
      "2024-12-13 13:47:28.618000: I runner.py:310] Step = 16600 ; steps/s = 1.63, tokens/s = 82323 (37879 source, 44444 target) ; Learning rate = 0.000686 ; Loss = 2.166438\n",
      "2024-12-13 13:48:29.700000: I runner.py:310] Step = 16700 ; steps/s = 1.64, tokens/s = 81589 (37517 source, 44072 target) ; Learning rate = 0.000684 ; Loss = 2.296591\n",
      "2024-12-13 13:49:31.085000: I runner.py:310] Step = 16800 ; steps/s = 1.63, tokens/s = 82078 (37747 source, 44331 target) ; Learning rate = 0.000682 ; Loss = 2.184188\n",
      "2024-12-13 13:50:32.509000: I runner.py:310] Step = 16900 ; steps/s = 1.63, tokens/s = 82276 (37824 source, 44452 target) ; Learning rate = 0.000680 ; Loss = 2.141990\n",
      "2024-12-13 13:51:33.930000: I runner.py:310] Step = 17000 ; steps/s = 1.63, tokens/s = 82289 (37880 source, 44409 target) ; Learning rate = 0.000678 ; Loss = 2.153542\n",
      "2024-12-13 13:52:34.951000: I runner.py:310] Step = 17100 ; steps/s = 1.64, tokens/s = 81451 (37448 source, 44003 target) ; Learning rate = 0.000676 ; Loss = 2.156598\n",
      "2024-12-13 13:53:36.364000: I runner.py:310] Step = 17200 ; steps/s = 1.63, tokens/s = 82286 (37834 source, 44452 target) ; Learning rate = 0.000674 ; Loss = 2.104367\n",
      "2024-12-13 13:54:37.839000: I runner.py:310] Step = 17300 ; steps/s = 1.63, tokens/s = 82190 (37812 source, 44378 target) ; Learning rate = 0.000672 ; Loss = 2.118000\n",
      "2024-12-13 13:55:38.898000: I runner.py:310] Step = 17400 ; steps/s = 1.64, tokens/s = 81414 (37448 source, 43966 target) ; Learning rate = 0.000670 ; Loss = 2.127889\n",
      "2024-12-13 13:56:40.376000: I runner.py:310] Step = 17500 ; steps/s = 1.63, tokens/s = 82179 (37783 source, 44396 target) ; Learning rate = 0.000668 ; Loss = 2.125709\n",
      "2024-12-13 13:57:41.769000: I runner.py:310] Step = 17600 ; steps/s = 1.63, tokens/s = 82331 (37872 source, 44459 target) ; Learning rate = 0.000666 ; Loss = 2.151007\n",
      "2024-12-13 13:58:43.167000: I runner.py:310] Step = 17700 ; steps/s = 1.63, tokens/s = 82326 (37879 source, 44447 target) ; Learning rate = 0.000664 ; Loss = 2.141379\n",
      "2024-12-13 13:59:44.254000: I runner.py:310] Step = 17800 ; steps/s = 1.64, tokens/s = 81334 (37408 source, 43926 target) ; Learning rate = 0.000662 ; Loss = 2.121302\n",
      "2024-12-13 14:00:45.617000: I runner.py:310] Step = 17900 ; steps/s = 1.63, tokens/s = 82352 (37873 source, 44479 target) ; Learning rate = 0.000661 ; Loss = 2.079367\n",
      "2024-12-13 14:01:47.016000: I runner.py:310] Step = 18000 ; steps/s = 1.63, tokens/s = 82307 (37852 source, 44455 target) ; Learning rate = 0.000659 ; Loss = 2.126332\n",
      "2024-12-13 14:02:48.069000: I runner.py:310] Step = 18100 ; steps/s = 1.64, tokens/s = 81431 (37457 source, 43974 target) ; Learning rate = 0.000657 ; Loss = 2.119477\n",
      "2024-12-13 14:03:49.504000: I runner.py:310] Step = 18200 ; steps/s = 1.63, tokens/s = 82250 (37814 source, 44436 target) ; Learning rate = 0.000655 ; Loss = 2.109835\n",
      "2024-12-13 14:04:50.953000: I runner.py:310] Step = 18300 ; steps/s = 1.63, tokens/s = 82207 (37815 source, 44392 target) ; Learning rate = 0.000653 ; Loss = 2.115119\n",
      "2024-12-13 14:05:52.350000: I runner.py:310] Step = 18400 ; steps/s = 1.63, tokens/s = 82320 (37864 source, 44456 target) ; Learning rate = 0.000652 ; Loss = 2.114424\n",
      "2024-12-13 14:06:53.361000: I runner.py:310] Step = 18500 ; steps/s = 1.64, tokens/s = 81495 (37511 source, 43984 target) ; Learning rate = 0.000650 ; Loss = 2.125641\n",
      "2024-12-13 14:07:54.749000: I runner.py:310] Step = 18600 ; steps/s = 1.63, tokens/s = 82331 (37851 source, 44480 target) ; Learning rate = 0.000648 ; Loss = 2.078664\n",
      "2024-12-13 14:08:56.217000: I runner.py:310] Step = 18700 ; steps/s = 1.63, tokens/s = 82196 (37801 source, 44395 target) ; Learning rate = 0.000646 ; Loss = 2.112201\n",
      "2024-12-13 14:09:57.223000: I runner.py:310] Step = 18800 ; steps/s = 1.64, tokens/s = 81467 (37470 source, 43997 target) ; Learning rate = 0.000645 ; Loss = 2.098546\n",
      "2024-12-13 14:10:58.646000: I runner.py:310] Step = 18900 ; steps/s = 1.63, tokens/s = 82239 (37819 source, 44420 target) ; Learning rate = 0.000643 ; Loss = 2.095655\n",
      "2024-12-13 14:12:00.070000: I runner.py:310] Step = 19000 ; steps/s = 1.63, tokens/s = 82298 (37868 source, 44430 target) ; Learning rate = 0.000641 ; Loss = 2.075984\n",
      "2024-12-13 14:13:01.480000: I runner.py:310] Step = 19100 ; steps/s = 1.63, tokens/s = 82307 (37860 source, 44447 target) ; Learning rate = 0.000640 ; Loss = 2.096488\n",
      "2024-12-13 14:14:02.469000: I runner.py:310] Step = 19200 ; steps/s = 1.64, tokens/s = 81474 (37454 source, 44020 target) ; Learning rate = 0.000638 ; Loss = 2.138148\n",
      "2024-12-13 14:15:03.918000: I runner.py:310] Step = 19300 ; steps/s = 1.63, tokens/s = 82222 (37810 source, 44412 target) ; Learning rate = 0.000636 ; Loss = 2.094814\n",
      "2024-12-13 14:16:05.311000: I runner.py:310] Step = 19400 ; steps/s = 1.63, tokens/s = 82344 (37875 source, 44469 target) ; Learning rate = 0.000635 ; Loss = 2.097416\n",
      "2024-12-13 14:17:06.321000: I runner.py:310] Step = 19500 ; steps/s = 1.64, tokens/s = 81469 (37480 source, 43989 target) ; Learning rate = 0.000633 ; Loss = 2.061321\n",
      "2024-12-13 14:18:07.801000: I runner.py:310] Step = 19600 ; steps/s = 1.63, tokens/s = 82193 (37792 source, 44401 target) ; Learning rate = 0.000631 ; Loss = 2.100607\n",
      "2024-12-13 14:19:09.234000: I runner.py:310] Step = 19700 ; steps/s = 1.63, tokens/s = 82266 (37839 source, 44427 target) ; Learning rate = 0.000630 ; Loss = 2.057991\n",
      "2024-12-13 14:20:10.595000: I runner.py:310] Step = 19800 ; steps/s = 1.63, tokens/s = 82348 (37882 source, 44466 target) ; Learning rate = 0.000628 ; Loss = 2.082356\n",
      "2024-12-13 14:21:11.646000: I runner.py:310] Step = 19900 ; steps/s = 1.64, tokens/s = 81398 (37424 source, 43974 target) ; Learning rate = 0.000627 ; Loss = 2.012474\n",
      "2024-12-13 14:22:13.053000: I runner.py:310] Step = 20000 ; steps/s = 1.63, tokens/s = 82311 (37872 source, 44439 target) ; Learning rate = 0.000625 ; Loss = 2.091095\n",
      "2024-12-13 14:22:14.810000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-20000\n",
      "2024-12-13 14:22:14.810000: I training.py:192] Running evaluation for step 20000\n",
      "2024-12-13 14:24:20.198000: I training.py:192] Evaluation result for step 20000: loss = 2.166039 ; perplexity = 8.723657\n",
      "2024-12-13 14:25:21.482000: I runner.py:310] Step = 20100 ; steps/s = 1.63, tokens/s = 82495 (37938 source, 44557 target) ; Learning rate = 0.000623 ; Loss = 2.085530\n",
      "2024-12-13 14:26:22.429000: I runner.py:310] Step = 20200 ; steps/s = 1.64, tokens/s = 81527 (37498 source, 44029 target) ; Learning rate = 0.000622 ; Loss = 2.076755\n",
      "2024-12-13 14:27:23.856000: I runner.py:310] Step = 20300 ; steps/s = 1.63, tokens/s = 82251 (37823 source, 44428 target) ; Learning rate = 0.000620 ; Loss = 2.046414\n",
      "2024-12-13 14:28:25.210000: I runner.py:310] Step = 20400 ; steps/s = 1.63, tokens/s = 82364 (37870 source, 44494 target) ; Learning rate = 0.000619 ; Loss = 2.048533\n",
      "2024-12-13 14:29:26.631000: I runner.py:310] Step = 20500 ; steps/s = 1.63, tokens/s = 82297 (37872 source, 44425 target) ; Learning rate = 0.000617 ; Loss = 2.076077\n",
      "2024-12-13 14:30:27.617000: I runner.py:310] Step = 20600 ; steps/s = 1.64, tokens/s = 81497 (37484 source, 44013 target) ; Learning rate = 0.000616 ; Loss = 2.013218\n",
      "2024-12-13 14:31:29.042000: I runner.py:310] Step = 20700 ; steps/s = 1.63, tokens/s = 82266 (37823 source, 44443 target) ; Learning rate = 0.000614 ; Loss = 2.057376\n",
      "2024-12-13 14:32:30.348000: I runner.py:310] Step = 20800 ; steps/s = 1.63, tokens/s = 82441 (37933 source, 44508 target) ; Learning rate = 0.000613 ; Loss = 2.077997\n",
      "2024-12-13 14:33:31.219000: I runner.py:310] Step = 20900 ; steps/s = 1.64, tokens/s = 81633 (37546 source, 44087 target) ; Learning rate = 0.000611 ; Loss = 2.063010\n",
      "2024-12-13 14:34:32.550000: I runner.py:310] Step = 21000 ; steps/s = 1.63, tokens/s = 82405 (37908 source, 44497 target) ; Learning rate = 0.000610 ; Loss = 2.058395\n",
      "2024-12-13 14:35:33.929000: I runner.py:310] Step = 21100 ; steps/s = 1.63, tokens/s = 82339 (37864 source, 44475 target) ; Learning rate = 0.000608 ; Loss = 2.053409\n",
      "2024-12-13 14:36:35.330000: I runner.py:310] Step = 21200 ; steps/s = 1.63, tokens/s = 82299 (37851 source, 44448 target) ; Learning rate = 0.000607 ; Loss = 2.084495\n",
      "2024-12-13 14:37:36.269000: I runner.py:310] Step = 21300 ; steps/s = 1.64, tokens/s = 81543 (37508 source, 44035 target) ; Learning rate = 0.000606 ; Loss = 2.089481\n",
      "2024-12-13 14:38:37.601000: I runner.py:310] Step = 21400 ; steps/s = 1.63, tokens/s = 82406 (37895 source, 44511 target) ; Learning rate = 0.000604 ; Loss = 2.023752\n",
      "2024-12-13 14:39:39.038000: I runner.py:310] Step = 21500 ; steps/s = 1.63, tokens/s = 82281 (37847 source, 44434 target) ; Learning rate = 0.000603 ; Loss = 2.037781\n",
      "2024-12-13 14:40:39.972000: I runner.py:310] Step = 21600 ; steps/s = 1.64, tokens/s = 81557 (37510 source, 44047 target) ; Learning rate = 0.000601 ; Loss = 2.030482\n",
      "2024-12-13 14:41:41.362000: I runner.py:310] Step = 21700 ; steps/s = 1.63, tokens/s = 82333 (37883 source, 44450 target) ; Learning rate = 0.000600 ; Loss = 2.039465\n",
      "2024-12-13 14:42:42.741000: I runner.py:310] Step = 21800 ; steps/s = 1.63, tokens/s = 82316 (37841 source, 44475 target) ; Learning rate = 0.000599 ; Loss = 2.061731\n",
      "2024-12-13 14:43:44.126000: I runner.py:310] Step = 21900 ; steps/s = 1.63, tokens/s = 82331 (37874 source, 44457 target) ; Learning rate = 0.000597 ; Loss = 2.092670\n",
      "2024-12-13 14:44:45.146000: I runner.py:310] Step = 22000 ; steps/s = 1.64, tokens/s = 81456 (37459 source, 43997 target) ; Learning rate = 0.000596 ; Loss = 2.011881\n",
      "2024-12-13 14:45:46.480000: I runner.py:310] Step = 22100 ; steps/s = 1.63, tokens/s = 82406 (37911 source, 44495 target) ; Learning rate = 0.000595 ; Loss = 2.052603\n",
      "2024-12-13 14:46:47.857000: I runner.py:310] Step = 22200 ; steps/s = 1.63, tokens/s = 82315 (37854 source, 44461 target) ; Learning rate = 0.000593 ; Loss = 2.047765\n",
      "2024-12-13 14:47:48.868000: I runner.py:310] Step = 22300 ; steps/s = 1.64, tokens/s = 81460 (37473 source, 43987 target) ; Learning rate = 0.000592 ; Loss = 2.046474\n",
      "2024-12-13 14:48:50.263000: I runner.py:310] Step = 22400 ; steps/s = 1.63, tokens/s = 82303 (37845 source, 44458 target) ; Learning rate = 0.000591 ; Loss = 2.016647\n",
      "2024-12-13 14:49:51.681000: I runner.py:310] Step = 22500 ; steps/s = 1.63, tokens/s = 82289 (37850 source, 44439 target) ; Learning rate = 0.000589 ; Loss = 2.014507\n",
      "2024-12-13 14:50:53.075000: I runner.py:310] Step = 22600 ; steps/s = 1.63, tokens/s = 82323 (37871 source, 44452 target) ; Learning rate = 0.000588 ; Loss = 2.039370\n",
      "2024-12-13 14:51:54.092000: I runner.py:310] Step = 22700 ; steps/s = 1.64, tokens/s = 81442 (37451 source, 43991 target) ; Learning rate = 0.000587 ; Loss = 2.046050\n",
      "2024-12-13 14:52:55.443000: I runner.py:310] Step = 22800 ; steps/s = 1.63, tokens/s = 82385 (37908 source, 44477 target) ; Learning rate = 0.000585 ; Loss = 2.019163\n",
      "2024-12-13 14:53:56.831000: I runner.py:310] Step = 22900 ; steps/s = 1.63, tokens/s = 82317 (37848 source, 44469 target) ; Learning rate = 0.000584 ; Loss = 2.015637\n",
      "2024-12-13 14:54:57.809000: I runner.py:310] Step = 23000 ; steps/s = 1.64, tokens/s = 81496 (37476 source, 44020 target) ; Learning rate = 0.000583 ; Loss = 2.041598\n",
      "2024-12-13 14:55:59.197000: I runner.py:310] Step = 23100 ; steps/s = 1.63, tokens/s = 82300 (37848 source, 44452 target) ; Learning rate = 0.000582 ; Loss = 2.009737\n",
      "2024-12-13 14:57:00.496000: I runner.py:310] Step = 23200 ; steps/s = 1.63, tokens/s = 82482 (37957 source, 44525 target) ; Learning rate = 0.000580 ; Loss = 2.018688\n",
      "2024-12-13 14:58:01.844000: I runner.py:310] Step = 23300 ; steps/s = 1.63, tokens/s = 82371 (37875 source, 44496 target) ; Learning rate = 0.000579 ; Loss = 2.053943\n",
      "2024-12-13 14:59:02.826000: I runner.py:310] Step = 23400 ; steps/s = 1.64, tokens/s = 81481 (37495 source, 43986 target) ; Learning rate = 0.000578 ; Loss = 2.032311\n",
      "2024-12-13 15:00:04.208000: I runner.py:310] Step = 23500 ; steps/s = 1.63, tokens/s = 82341 (37857 source, 44484 target) ; Learning rate = 0.000577 ; Loss = 2.009559\n",
      "2024-12-13 15:01:05.614000: I runner.py:310] Step = 23600 ; steps/s = 1.63, tokens/s = 82282 (37825 source, 44457 target) ; Learning rate = 0.000575 ; Loss = 2.029327\n",
      "2024-12-13 15:02:06.584000: I runner.py:310] Step = 23700 ; steps/s = 1.64, tokens/s = 81531 (37510 source, 44021 target) ; Learning rate = 0.000574 ; Loss = 2.041632\n",
      "2024-12-13 15:03:07.928000: I runner.py:310] Step = 23800 ; steps/s = 1.63, tokens/s = 82349 (37856 source, 44493 target) ; Learning rate = 0.000573 ; Loss = 1.971473\n",
      "2024-12-13 15:04:09.325000: I runner.py:310] Step = 23900 ; steps/s = 1.63, tokens/s = 82349 (37886 source, 44463 target) ; Learning rate = 0.000572 ; Loss = 2.015446\n",
      "2024-12-13 15:05:10.652000: I runner.py:310] Step = 24000 ; steps/s = 1.63, tokens/s = 82386 (37899 source, 44487 target) ; Learning rate = 0.000571 ; Loss = 2.024942\n",
      "2024-12-13 15:06:11.585000: I runner.py:310] Step = 24100 ; steps/s = 1.64, tokens/s = 81564 (37525 source, 44039 target) ; Learning rate = 0.000569 ; Loss = 1.978775\n",
      "2024-12-13 15:07:12.904000: I runner.py:310] Step = 24200 ; steps/s = 1.63, tokens/s = 82422 (37902 source, 44520 target) ; Learning rate = 0.000568 ; Loss = 2.020508\n",
      "2024-12-13 15:08:14.280000: I runner.py:310] Step = 24300 ; steps/s = 1.63, tokens/s = 82335 (37868 source, 44467 target) ; Learning rate = 0.000567 ; Loss = 2.040637\n",
      "2024-12-13 15:09:15.283000: I runner.py:310] Step = 24400 ; steps/s = 1.64, tokens/s = 81483 (37480 source, 44003 target) ; Learning rate = 0.000566 ; Loss = 2.029696\n",
      "2024-12-13 15:10:16.596000: I runner.py:310] Step = 24500 ; steps/s = 1.63, tokens/s = 82423 (37907 source, 44516 target) ; Learning rate = 0.000565 ; Loss = 2.001678\n",
      "2024-12-13 15:11:17.941000: I runner.py:310] Step = 24600 ; steps/s = 1.63, tokens/s = 82391 (37896 source, 44495 target) ; Learning rate = 0.000564 ; Loss = 2.000983\n",
      "2024-12-13 15:12:19.235000: I runner.py:310] Step = 24700 ; steps/s = 1.63, tokens/s = 82160 (37786 source, 44374 target) ; Learning rate = 0.000562 ; Loss = 1.977741\n",
      "2024-12-13 15:13:20.246000: I runner.py:310] Step = 24800 ; steps/s = 1.64, tokens/s = 81753 (37612 source, 44141 target) ; Learning rate = 0.000561 ; Loss = 1.959590\n",
      "2024-12-13 15:14:21.638000: I runner.py:310] Step = 24900 ; steps/s = 1.63, tokens/s = 82304 (37847 source, 44457 target) ; Learning rate = 0.000560 ; Loss = 2.016734\n",
      "2024-12-13 15:15:23.050000: I runner.py:310] Step = 25000 ; steps/s = 1.63, tokens/s = 82272 (37823 source, 44449 target) ; Learning rate = 0.000559 ; Loss = 2.050280\n",
      "2024-12-13 15:15:23.051000: I training.py:192] Running evaluation for step 25000\n",
      "2024-12-13 15:17:28.041000: I training.py:192] Evaluation result for step 25000: loss = 2.230018 ; perplexity = 9.300036\n",
      "2024-12-13 15:18:28.761000: I runner.py:310] Step = 25100 ; steps/s = 1.65, tokens/s = 81869 (37653 source, 44216 target) ; Learning rate = 0.000558 ; Loss = 2.037285\n",
      "2024-12-13 15:19:30.106000: I runner.py:310] Step = 25200 ; steps/s = 1.63, tokens/s = 82369 (37886 source, 44483 target) ; Learning rate = 0.000557 ; Loss = 1.974076\n",
      "2024-12-13 15:20:31.469000: I runner.py:310] Step = 25300 ; steps/s = 1.63, tokens/s = 82339 (37863 source, 44476 target) ; Learning rate = 0.000556 ; Loss = 1.978354\n",
      "2024-12-13 15:21:32.422000: I runner.py:310] Step = 25400 ; steps/s = 1.64, tokens/s = 81589 (37548 source, 44041 target) ; Learning rate = 0.000555 ; Loss = 2.002989\n",
      "2024-12-13 15:22:33.740000: I runner.py:310] Step = 25500 ; steps/s = 1.63, tokens/s = 82402 (37898 source, 44504 target) ; Learning rate = 0.000553 ; Loss = 1.995025\n",
      "2024-12-13 15:23:35.064000: I runner.py:310] Step = 25600 ; steps/s = 1.63, tokens/s = 82419 (37909 source, 44510 target) ; Learning rate = 0.000552 ; Loss = 1.973039\n",
      "2024-12-13 15:24:36.464000: I runner.py:310] Step = 25700 ; steps/s = 1.63, tokens/s = 82310 (37874 source, 44436 target) ; Learning rate = 0.000551 ; Loss = 1.981132\n",
      "2024-12-13 15:25:37.417000: I runner.py:310] Step = 25800 ; steps/s = 1.64, tokens/s = 81541 (37484 source, 44057 target) ; Learning rate = 0.000550 ; Loss = 2.048746\n",
      "2024-12-13 15:26:38.803000: I runner.py:310] Step = 25900 ; steps/s = 1.63, tokens/s = 82303 (37833 source, 44470 target) ; Learning rate = 0.000549 ; Loss = 1.974298\n",
      "2024-12-13 15:27:40.141000: I runner.py:310] Step = 26000 ; steps/s = 1.63, tokens/s = 82402 (37914 source, 44488 target) ; Learning rate = 0.000548 ; Loss = 2.000167\n",
      "2024-12-13 15:28:41.113000: I runner.py:310] Step = 26100 ; steps/s = 1.64, tokens/s = 81517 (37502 source, 44015 target) ; Learning rate = 0.000547 ; Loss = 1.976780\n",
      "2024-12-13 15:29:42.528000: I runner.py:310] Step = 26200 ; steps/s = 1.63, tokens/s = 82316 (37871 source, 44445 target) ; Learning rate = 0.000546 ; Loss = 1.964178\n",
      "2024-12-13 15:30:43.915000: I runner.py:310] Step = 26300 ; steps/s = 1.63, tokens/s = 82299 (37845 source, 44454 target) ; Learning rate = 0.000545 ; Loss = 2.008769\n",
      "2024-12-13 15:31:45.287000: I runner.py:310] Step = 26400 ; steps/s = 1.63, tokens/s = 82320 (37851 source, 44469 target) ; Learning rate = 0.000544 ; Loss = 2.010739\n",
      "2024-12-13 15:32:46.297000: I runner.py:310] Step = 26500 ; steps/s = 1.64, tokens/s = 81504 (37507 source, 43997 target) ; Learning rate = 0.000543 ; Loss = 1.932096\n",
      "2024-12-13 15:33:47.628000: I runner.py:310] Step = 26600 ; steps/s = 1.63, tokens/s = 82402 (37907 source, 44495 target) ; Learning rate = 0.000542 ; Loss = 1.994190\n",
      "2024-12-13 15:34:48.939000: I runner.py:310] Step = 26700 ; steps/s = 1.63, tokens/s = 82396 (37872 source, 44524 target) ; Learning rate = 0.000541 ; Loss = 2.014252\n",
      "2024-12-13 15:35:49.894000: I runner.py:310] Step = 26800 ; steps/s = 1.64, tokens/s = 81536 (37501 source, 44035 target) ; Learning rate = 0.000540 ; Loss = 1.964542\n",
      "2024-12-13 15:36:51.259000: I runner.py:310] Step = 26900 ; steps/s = 1.63, tokens/s = 82324 (37855 source, 44469 target) ; Learning rate = 0.000539 ; Loss = 1.988089\n",
      "2024-12-13 15:37:52.628000: I runner.py:310] Step = 27000 ; steps/s = 1.63, tokens/s = 82347 (37875 source, 44472 target) ; Learning rate = 0.000538 ; Loss = 1.960342\n",
      "2024-12-13 15:38:53.985000: I runner.py:310] Step = 27100 ; steps/s = 1.63, tokens/s = 82389 (37910 source, 44479 target) ; Learning rate = 0.000537 ; Loss = 1.962106\n",
      "2024-12-13 15:39:54.932000: I runner.py:310] Step = 27200 ; steps/s = 1.64, tokens/s = 81549 (37490 source, 44059 target) ; Learning rate = 0.000536 ; Loss = 1.925065\n",
      "2024-12-13 15:40:56.245000: I runner.py:310] Step = 27300 ; steps/s = 1.63, tokens/s = 82450 (37937 source, 44513 target) ; Learning rate = 0.000535 ; Loss = 1.994666\n",
      "2024-12-13 15:41:57.625000: I runner.py:310] Step = 27400 ; steps/s = 1.63, tokens/s = 82315 (37858 source, 44457 target) ; Learning rate = 0.000534 ; Loss = 1.998465\n",
      "2024-12-13 15:42:58.628000: I runner.py:310] Step = 27500 ; steps/s = 1.64, tokens/s = 81467 (37464 source, 44003 target) ; Learning rate = 0.000533 ; Loss = 1.959368\n",
      "2024-12-13 15:44:00.013000: I runner.py:310] Step = 27600 ; steps/s = 1.63, tokens/s = 82329 (37863 source, 44466 target) ; Learning rate = 0.000532 ; Loss = 1.950503\n",
      "2024-12-13 15:45:01.395000: I runner.py:310] Step = 27700 ; steps/s = 1.63, tokens/s = 82317 (37854 source, 44463 target) ; Learning rate = 0.000531 ; Loss = 1.995935\n",
      "2024-12-13 15:46:02.815000: I runner.py:310] Step = 27800 ; steps/s = 1.63, tokens/s = 82282 (37849 source, 44433 target) ; Learning rate = 0.000530 ; Loss = 1.987713\n",
      "2024-12-13 15:47:03.831000: I runner.py:310] Step = 27900 ; steps/s = 1.64, tokens/s = 81427 (37446 source, 43981 target) ; Learning rate = 0.000529 ; Loss = 1.994951\n",
      "2024-12-13 15:48:05.228000: I runner.py:310] Step = 28000 ; steps/s = 1.63, tokens/s = 82305 (37832 source, 44473 target) ; Learning rate = 0.000528 ; Loss = 1.971559\n",
      "2024-12-13 15:49:06.593000: I runner.py:310] Step = 28100 ; steps/s = 1.63, tokens/s = 82405 (37940 source, 44465 target) ; Learning rate = 0.000527 ; Loss = 1.972672\n",
      "2024-12-13 15:50:07.558000: I runner.py:310] Step = 28200 ; steps/s = 1.64, tokens/s = 81522 (37492 source, 44030 target) ; Learning rate = 0.000526 ; Loss = 1.973722\n",
      "2024-12-13 15:51:08.982000: I runner.py:310] Step = 28300 ; steps/s = 1.63, tokens/s = 82256 (37842 source, 44414 target) ; Learning rate = 0.000525 ; Loss = 1.959670\n",
      "2024-12-13 15:52:10.339000: I runner.py:310] Step = 28400 ; steps/s = 1.63, tokens/s = 82346 (37860 source, 44486 target) ; Learning rate = 0.000524 ; Loss = 1.952080\n",
      "2024-12-13 15:53:11.717000: I runner.py:310] Step = 28500 ; steps/s = 1.63, tokens/s = 82351 (37879 source, 44472 target) ; Learning rate = 0.000524 ; Loss = 1.966468\n",
      "2024-12-13 15:54:12.664000: I runner.py:310] Step = 28600 ; steps/s = 1.64, tokens/s = 81564 (37523 source, 44041 target) ; Learning rate = 0.000523 ; Loss = 2.004263\n",
      "2024-12-13 15:55:14.089000: I runner.py:310] Step = 28700 ; steps/s = 1.63, tokens/s = 82271 (37821 source, 44450 target) ; Learning rate = 0.000522 ; Loss = 1.937058\n",
      "2024-12-13 15:56:15.477000: I runner.py:310] Step = 28800 ; steps/s = 1.63, tokens/s = 82335 (37881 source, 44454 target) ; Learning rate = 0.000521 ; Loss = 1.957535\n",
      "2024-12-13 15:57:16.447000: I runner.py:310] Step = 28900 ; steps/s = 1.64, tokens/s = 81497 (37487 source, 44010 target) ; Learning rate = 0.000520 ; Loss = 1.932422\n",
      "2024-12-13 15:58:17.826000: I runner.py:310] Step = 29000 ; steps/s = 1.63, tokens/s = 82342 (37871 source, 44471 target) ; Learning rate = 0.000519 ; Loss = 1.945949\n",
      "2024-12-13 15:59:19.211000: I runner.py:310] Step = 29100 ; steps/s = 1.63, tokens/s = 82317 (37862 source, 44455 target) ; Learning rate = 0.000518 ; Loss = 1.961767\n",
      "2024-12-13 16:00:20.523000: I runner.py:310] Step = 29200 ; steps/s = 1.63, tokens/s = 82408 (37900 source, 44508 target) ; Learning rate = 0.000517 ; Loss = 1.982606\n",
      "2024-12-13 16:01:21.510000: I runner.py:310] Step = 29300 ; steps/s = 1.64, tokens/s = 81494 (37468 source, 44026 target) ; Learning rate = 0.000516 ; Loss = 1.927687\n",
      "2024-12-13 16:02:22.862000: I runner.py:310] Step = 29400 ; steps/s = 1.63, tokens/s = 82376 (37887 source, 44489 target) ; Learning rate = 0.000515 ; Loss = 1.957631\n",
      "2024-12-13 16:03:24.197000: I runner.py:310] Step = 29500 ; steps/s = 1.63, tokens/s = 82376 (37887 source, 44489 target) ; Learning rate = 0.000515 ; Loss = 1.977989\n",
      "2024-12-13 16:04:25.178000: I runner.py:310] Step = 29600 ; steps/s = 1.64, tokens/s = 81516 (37500 source, 44016 target) ; Learning rate = 0.000514 ; Loss = 1.926161\n",
      "2024-12-13 16:05:26.447000: I runner.py:310] Step = 29700 ; steps/s = 1.63, tokens/s = 82502 (37955 source, 44547 target) ; Learning rate = 0.000513 ; Loss = 1.947692\n",
      "2024-12-13 16:06:27.837000: I runner.py:310] Step = 29800 ; steps/s = 1.63, tokens/s = 82328 (37869 source, 44459 target) ; Learning rate = 0.000512 ; Loss = 1.970949\n",
      "2024-12-13 16:07:29.241000: I runner.py:310] Step = 29900 ; steps/s = 1.63, tokens/s = 82273 (37830 source, 44443 target) ; Learning rate = 0.000511 ; Loss = 1.981622\n",
      "2024-12-13 16:08:30.182000: I runner.py:310] Step = 30000 ; steps/s = 1.64, tokens/s = 81558 (37518 source, 44040 target) ; Learning rate = 0.000510 ; Loss = 1.900229\n",
      "2024-12-13 16:08:31.904000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-30000\n",
      "2024-12-13 16:08:31.904000: I training.py:192] Running evaluation for step 30000\n",
      "2024-12-13 16:10:33.081000: I training.py:192] Evaluation result for step 30000: loss = 2.304003 ; perplexity = 10.014194\n",
      "2024-12-13 16:11:34.347000: I runner.py:310] Step = 30100 ; steps/s = 1.63, tokens/s = 82515 (37948 source, 44567 target) ; Learning rate = 0.000509 ; Loss = 1.950569\n",
      "2024-12-13 16:12:35.752000: I runner.py:310] Step = 30200 ; steps/s = 1.63, tokens/s = 82310 (37871 source, 44439 target) ; Learning rate = 0.000509 ; Loss = 1.963228\n",
      "2024-12-13 16:13:36.713000: I runner.py:310] Step = 30300 ; steps/s = 1.64, tokens/s = 81531 (37490 source, 44041 target) ; Learning rate = 0.000508 ; Loss = 1.928519\n",
      "2024-12-13 16:14:38.049000: I runner.py:310] Step = 30400 ; steps/s = 1.63, tokens/s = 82382 (37888 source, 44494 target) ; Learning rate = 0.000507 ; Loss = 1.952530\n",
      "2024-12-13 16:15:39.417000: I runner.py:310] Step = 30500 ; steps/s = 1.63, tokens/s = 82378 (37904 source, 44474 target) ; Learning rate = 0.000506 ; Loss = 1.955101\n",
      "2024-12-13 16:16:40.803000: I runner.py:310] Step = 30600 ; steps/s = 1.63, tokens/s = 82300 (37838 source, 44462 target) ; Learning rate = 0.000505 ; Loss = 1.974109\n",
      "2024-12-13 16:17:41.766000: I runner.py:310] Step = 30700 ; steps/s = 1.64, tokens/s = 81535 (37505 source, 44030 target) ; Learning rate = 0.000504 ; Loss = 1.903310\n",
      "2024-12-13 16:18:43.207000: I runner.py:310] Step = 30800 ; steps/s = 1.63, tokens/s = 82270 (37846 source, 44424 target) ; Learning rate = 0.000504 ; Loss = 1.947917\n",
      "2024-12-13 16:19:44.541000: I runner.py:310] Step = 30900 ; steps/s = 1.63, tokens/s = 82375 (37875 source, 44500 target) ; Learning rate = 0.000503 ; Loss = 1.952175\n",
      "2024-12-13 16:20:45.539000: I runner.py:310] Step = 31000 ; steps/s = 1.64, tokens/s = 81474 (37466 source, 44008 target) ; Learning rate = 0.000502 ; Loss = 1.958887\n",
      "2024-12-13 16:21:46.920000: I runner.py:310] Step = 31100 ; steps/s = 1.63, tokens/s = 82319 (37854 source, 44465 target) ; Learning rate = 0.000501 ; Loss = 1.946408\n",
      "2024-12-13 16:22:48.203000: I runner.py:310] Step = 31200 ; steps/s = 1.63, tokens/s = 82457 (37932 source, 44525 target) ; Learning rate = 0.000500 ; Loss = 1.939428\n",
      "2024-12-13 16:23:49.590000: I runner.py:310] Step = 31300 ; steps/s = 1.63, tokens/s = 82340 (37870 source, 44470 target) ; Learning rate = 0.000500 ; Loss = 1.936277\n",
      "2024-12-13 16:24:50.563000: I runner.py:310] Step = 31400 ; steps/s = 1.64, tokens/s = 81496 (37483 source, 44013 target) ; Learning rate = 0.000499 ; Loss = 1.972017\n",
      "2024-12-13 16:25:51.955000: I runner.py:310] Step = 31500 ; steps/s = 1.63, tokens/s = 82314 (37834 source, 44480 target) ; Learning rate = 0.000498 ; Loss = 1.917789\n",
      "2024-12-13 16:26:53.289000: I runner.py:310] Step = 31600 ; steps/s = 1.63, tokens/s = 82401 (37920 source, 44481 target) ; Learning rate = 0.000497 ; Loss = 1.931359\n",
      "2024-12-13 16:27:54.356000: I runner.py:310] Step = 31700 ; steps/s = 1.64, tokens/s = 81393 (37436 source, 43957 target) ; Learning rate = 0.000496 ; Loss = 1.904711\n",
      "2024-12-13 16:28:55.691000: I runner.py:310] Step = 31800 ; steps/s = 1.63, tokens/s = 82396 (37891 source, 44505 target) ; Learning rate = 0.000496 ; Loss = 1.934293\n",
      "2024-12-13 16:29:57.055000: I runner.py:310] Step = 31900 ; steps/s = 1.63, tokens/s = 82370 (37895 source, 44475 target) ; Learning rate = 0.000495 ; Loss = 1.975070\n",
      "2024-12-13 16:30:58.420000: I runner.py:310] Step = 32000 ; steps/s = 1.63, tokens/s = 82336 (37871 source, 44465 target) ; Learning rate = 0.000494 ; Loss = 1.952798\n",
      "2024-12-13 16:31:59.387000: I runner.py:310] Step = 32100 ; steps/s = 1.64, tokens/s = 81497 (37473 source, 44024 target) ; Learning rate = 0.000493 ; Loss = 1.906312\n",
      "2024-12-13 16:33:00.825000: I runner.py:310] Step = 32200 ; steps/s = 1.63, tokens/s = 82293 (37868 source, 44425 target) ; Learning rate = 0.000493 ; Loss = 1.938623\n",
      "2024-12-13 16:34:02.289000: I runner.py:310] Step = 32300 ; steps/s = 1.63, tokens/s = 82194 (37782 source, 44412 target) ; Learning rate = 0.000492 ; Loss = 1.931900\n",
      "2024-12-13 16:35:03.283000: I runner.py:310] Step = 32400 ; steps/s = 1.64, tokens/s = 81499 (37500 source, 43999 target) ; Learning rate = 0.000491 ; Loss = 1.952383\n",
      "2024-12-13 16:36:04.724000: I runner.py:310] Step = 32500 ; steps/s = 1.63, tokens/s = 82263 (37835 source, 44428 target) ; Learning rate = 0.000490 ; Loss = 1.913339\n",
      "2024-12-13 16:37:06.128000: I runner.py:310] Step = 32600 ; steps/s = 1.63, tokens/s = 82304 (37867 source, 44437 target) ; Learning rate = 0.000490 ; Loss = 1.912758\n",
      "2024-12-13 16:38:07.503000: I runner.py:310] Step = 32700 ; steps/s = 1.63, tokens/s = 82315 (37848 source, 44467 target) ; Learning rate = 0.000489 ; Loss = 1.933116\n",
      "2024-12-13 16:39:08.439000: I runner.py:310] Step = 32800 ; steps/s = 1.64, tokens/s = 81554 (37506 source, 44048 target) ; Learning rate = 0.000488 ; Loss = 1.903552\n",
      "2024-12-13 16:40:09.831000: I runner.py:310] Step = 32900 ; steps/s = 1.63, tokens/s = 82312 (37862 source, 44450 target) ; Learning rate = 0.000487 ; Loss = 1.948484\n",
      "2024-12-13 16:41:11.266000: I runner.py:310] Step = 33000 ; steps/s = 1.63, tokens/s = 82253 (37828 source, 44425 target) ; Learning rate = 0.000487 ; Loss = 1.949509\n",
      "2024-12-13 16:42:12.227000: I runner.py:310] Step = 33100 ; steps/s = 1.64, tokens/s = 81549 (37512 source, 44037 target) ; Learning rate = 0.000486 ; Loss = 1.892712\n",
      "2024-12-13 16:43:13.642000: I runner.py:310] Step = 33200 ; steps/s = 1.63, tokens/s = 82300 (37849 source, 44451 target) ; Learning rate = 0.000485 ; Loss = 1.931045\n",
      "2024-12-13 16:44:15.001000: I runner.py:310] Step = 33300 ; steps/s = 1.63, tokens/s = 82360 (37871 source, 44489 target) ; Learning rate = 0.000484 ; Loss = 1.925572\n",
      "2024-12-13 16:45:16.186000: I runner.py:310] Step = 33400 ; steps/s = 1.63, tokens/s = 81628 (37558 source, 44070 target) ; Learning rate = 0.000484 ; Loss = 1.978871\n",
      "2024-12-13 16:46:17.421000: I runner.py:310] Step = 33500 ; steps/s = 1.63, tokens/s = 82137 (37781 source, 44356 target) ; Learning rate = 0.000483 ; Loss = 1.912167\n",
      "2024-12-13 16:47:18.782000: I runner.py:310] Step = 33600 ; steps/s = 1.63, tokens/s = 82329 (37859 source, 44470 target) ; Learning rate = 0.000482 ; Loss = 1.919903\n",
      "2024-12-13 16:48:20.166000: I runner.py:310] Step = 33700 ; steps/s = 1.63, tokens/s = 82324 (37858 source, 44466 target) ; Learning rate = 0.000481 ; Loss = 1.938067\n",
      "2024-12-13 16:49:21.177000: I runner.py:310] Step = 33800 ; steps/s = 1.64, tokens/s = 81483 (37493 source, 43990 target) ; Learning rate = 0.000481 ; Loss = 1.876046\n",
      "2024-12-13 16:50:22.538000: I runner.py:310] Step = 33900 ; steps/s = 1.63, tokens/s = 82349 (37865 source, 44484 target) ; Learning rate = 0.000480 ; Loss = 1.932912\n",
      "2024-12-13 16:51:23.923000: I runner.py:310] Step = 34000 ; steps/s = 1.63, tokens/s = 82303 (37842 source, 44461 target) ; Learning rate = 0.000479 ; Loss = 1.943050\n",
      "2024-12-13 16:52:24.923000: I runner.py:310] Step = 34100 ; steps/s = 1.64, tokens/s = 81508 (37504 source, 44004 target) ; Learning rate = 0.000479 ; Loss = 1.919083\n",
      "2024-12-13 16:53:26.298000: I runner.py:310] Step = 34200 ; steps/s = 1.63, tokens/s = 82297 (37833 source, 44464 target) ; Learning rate = 0.000478 ; Loss = 1.947986\n",
      "2024-12-13 16:54:27.761000: I runner.py:310] Step = 34300 ; steps/s = 1.63, tokens/s = 82248 (37855 source, 44393 target) ; Learning rate = 0.000477 ; Loss = 1.897856\n",
      "2024-12-13 16:55:29.225000: I runner.py:310] Step = 34400 ; steps/s = 1.63, tokens/s = 82236 (37822 source, 44414 target) ; Learning rate = 0.000477 ; Loss = 1.916434\n",
      "2024-12-13 16:56:30.238000: I runner.py:310] Step = 34500 ; steps/s = 1.64, tokens/s = 81466 (37454 source, 44012 target) ; Learning rate = 0.000476 ; Loss = 1.866793\n",
      "2024-12-13 16:57:31.652000: I runner.py:310] Step = 34600 ; steps/s = 1.63, tokens/s = 82298 (37853 source, 44445 target) ; Learning rate = 0.000475 ; Loss = 1.933053\n",
      "2024-12-13 16:58:33.049000: I runner.py:310] Step = 34700 ; steps/s = 1.63, tokens/s = 82312 (37868 source, 44444 target) ; Learning rate = 0.000474 ; Loss = 1.926340\n",
      "2024-12-13 16:59:34.054000: I runner.py:310] Step = 34800 ; steps/s = 1.64, tokens/s = 81445 (37457 source, 43988 target) ; Learning rate = 0.000474 ; Loss = 1.909366\n",
      "2024-12-13 17:00:35.399000: I runner.py:310] Step = 34900 ; steps/s = 1.63, tokens/s = 82371 (37882 source, 44489 target) ; Learning rate = 0.000473 ; Loss = 1.902826\n",
      "2024-12-13 17:01:36.742000: I runner.py:310] Step = 35000 ; steps/s = 1.63, tokens/s = 82387 (37904 source, 44483 target) ; Learning rate = 0.000472 ; Loss = 1.914688\n",
      "2024-12-13 17:01:36.743000: I training.py:192] Running evaluation for step 35000\n",
      "2024-12-13 17:03:40.577000: I training.py:192] Evaluation result for step 35000: loss = 2.328197 ; perplexity = 10.259433\n",
      "2024-12-13 17:04:41.758000: I runner.py:310] Step = 35100 ; steps/s = 1.63, tokens/s = 82634 (38013 source, 44621 target) ; Learning rate = 0.000472 ; Loss = 1.940501\n",
      "2024-12-13 17:05:42.710000: I runner.py:310] Step = 35200 ; steps/s = 1.64, tokens/s = 81519 (37481 source, 44038 target) ; Learning rate = 0.000471 ; Loss = 1.873584\n",
      "2024-12-13 17:06:44.112000: I runner.py:310] Step = 35300 ; steps/s = 1.63, tokens/s = 82318 (37853 source, 44465 target) ; Learning rate = 0.000470 ; Loss = 1.920464\n",
      "2024-12-13 17:07:45.471000: I runner.py:310] Step = 35400 ; steps/s = 1.63, tokens/s = 82340 (37861 source, 44479 target) ; Learning rate = 0.000470 ; Loss = 1.929383\n",
      "2024-12-13 17:08:46.530000: I runner.py:310] Step = 35500 ; steps/s = 1.64, tokens/s = 81401 (37447 source, 43954 target) ; Learning rate = 0.000469 ; Loss = 1.898124\n",
      "2024-12-13 17:09:47.906000: I runner.py:310] Step = 35600 ; steps/s = 1.63, tokens/s = 82349 (37893 source, 44456 target) ; Learning rate = 0.000468 ; Loss = 1.896616\n",
      "2024-12-13 17:10:49.311000: I runner.py:310] Step = 35700 ; steps/s = 1.63, tokens/s = 82290 (37835 source, 44455 target) ; Learning rate = 0.000468 ; Loss = 1.922233\n",
      "2024-12-13 17:11:50.670000: I runner.py:310] Step = 35800 ; steps/s = 1.63, tokens/s = 82363 (37882 source, 44481 target) ; Learning rate = 0.000467 ; Loss = 1.933714\n",
      "2024-12-13 17:12:51.639000: I runner.py:310] Step = 35900 ; steps/s = 1.64, tokens/s = 81500 (37475 source, 44025 target) ; Learning rate = 0.000466 ; Loss = 1.855518\n",
      "2024-12-13 17:13:53.044000: I runner.py:310] Step = 36000 ; steps/s = 1.63, tokens/s = 82304 (37849 source, 44455 target) ; Learning rate = 0.000466 ; Loss = 1.902605\n",
      "2024-12-13 17:14:54.401000: I runner.py:310] Step = 36100 ; steps/s = 1.63, tokens/s = 82355 (37877 source, 44478 target) ; Learning rate = 0.000465 ; Loss = 1.926378\n",
      "2024-12-13 17:15:55.395000: I runner.py:310] Step = 36200 ; steps/s = 1.64, tokens/s = 81500 (37497 source, 44003 target) ; Learning rate = 0.000465 ; Loss = 1.904780\n",
      "2024-12-13 17:16:56.751000: I runner.py:310] Step = 36300 ; steps/s = 1.63, tokens/s = 82347 (37859 source, 44488 target) ; Learning rate = 0.000464 ; Loss = 1.910889\n",
      "2024-12-13 17:17:58.089000: I runner.py:310] Step = 36400 ; steps/s = 1.63, tokens/s = 82405 (37911 source, 44494 target) ; Learning rate = 0.000463 ; Loss = 1.905418\n",
      "2024-12-13 17:18:59.522000: I runner.py:310] Step = 36500 ; steps/s = 1.63, tokens/s = 82262 (37845 source, 44417 target) ; Learning rate = 0.000463 ; Loss = 1.914001\n",
      "2024-12-13 17:20:00.521000: I runner.py:310] Step = 36600 ; steps/s = 1.64, tokens/s = 81500 (37488 source, 44012 target) ; Learning rate = 0.000462 ; Loss = 1.863131\n",
      "2024-12-13 17:21:01.941000: I runner.py:310] Step = 36700 ; steps/s = 1.63, tokens/s = 82279 (37849 source, 44430 target) ; Learning rate = 0.000461 ; Loss = 1.899511\n",
      "2024-12-13 17:22:03.308000: I runner.py:310] Step = 36800 ; steps/s = 1.63, tokens/s = 82344 (37871 source, 44473 target) ; Learning rate = 0.000461 ; Loss = 1.935725\n",
      "2024-12-13 17:23:04.282000: I runner.py:310] Step = 36900 ; steps/s = 1.64, tokens/s = 81512 (37485 source, 44027 target) ; Learning rate = 0.000460 ; Loss = 1.897158\n",
      "2024-12-13 17:24:05.702000: I runner.py:310] Step = 37000 ; steps/s = 1.63, tokens/s = 82254 (37810 source, 44444 target) ; Learning rate = 0.000460 ; Loss = 1.895352\n",
      "2024-12-13 17:25:07.057000: I runner.py:310] Step = 37100 ; steps/s = 1.63, tokens/s = 82383 (37904 source, 44479 target) ; Learning rate = 0.000459 ; Loss = 1.899369\n",
      "2024-12-13 17:26:08.464000: I runner.py:310] Step = 37200 ; steps/s = 1.63, tokens/s = 82291 (37852 source, 44439 target) ; Learning rate = 0.000458 ; Loss = 1.929381\n",
      "2024-12-13 17:27:09.422000: I runner.py:310] Step = 37300 ; steps/s = 1.64, tokens/s = 81553 (37529 source, 44024 target) ; Learning rate = 0.000458 ; Loss = 1.941101\n",
      "2024-12-13 17:28:10.813000: I runner.py:310] Step = 37400 ; steps/s = 1.63, tokens/s = 82325 (37857 source, 44468 target) ; Learning rate = 0.000457 ; Loss = 1.896368\n",
      "2024-12-13 17:29:12.204000: I runner.py:310] Step = 37500 ; steps/s = 1.63, tokens/s = 82300 (37850 source, 44450 target) ; Learning rate = 0.000456 ; Loss = 1.899815\n",
      "2024-12-13 17:30:13.195000: I runner.py:310] Step = 37600 ; steps/s = 1.64, tokens/s = 81476 (37461 source, 44015 target) ; Learning rate = 0.000456 ; Loss = 1.898548\n",
      "2024-12-13 17:31:14.568000: I runner.py:310] Step = 37700 ; steps/s = 1.63, tokens/s = 82339 (37870 source, 44469 target) ; Learning rate = 0.000455 ; Loss = 1.907453\n",
      "2024-12-13 17:32:15.944000: I runner.py:310] Step = 37800 ; steps/s = 1.63, tokens/s = 82334 (37862 source, 44472 target) ; Learning rate = 0.000455 ; Loss = 1.887407\n",
      "2024-12-13 17:33:17.340000: I runner.py:310] Step = 37900 ; steps/s = 1.63, tokens/s = 82323 (37882 source, 44441 target) ; Learning rate = 0.000454 ; Loss = 1.899974\n",
      "2024-12-13 17:34:18.366000: I runner.py:310] Step = 38000 ; steps/s = 1.64, tokens/s = 81436 (37443 source, 43993 target) ; Learning rate = 0.000453 ; Loss = 1.902605\n",
      "2024-12-13 17:35:19.761000: I runner.py:310] Step = 38100 ; steps/s = 1.63, tokens/s = 82296 (37843 source, 44453 target) ; Learning rate = 0.000453 ; Loss = 1.880581\n",
      "2024-12-13 17:36:21.159000: I runner.py:310] Step = 38200 ; steps/s = 1.63, tokens/s = 82320 (37871 source, 44449 target) ; Learning rate = 0.000452 ; Loss = 1.886431\n",
      "2024-12-13 17:37:22.162000: I runner.py:310] Step = 38300 ; steps/s = 1.64, tokens/s = 81491 (37483 source, 44008 target) ; Learning rate = 0.000452 ; Loss = 1.867137\n",
      "2024-12-13 17:38:23.587000: I runner.py:310] Step = 38400 ; steps/s = 1.63, tokens/s = 82292 (37860 source, 44432 target) ; Learning rate = 0.000451 ; Loss = 1.902506\n",
      "2024-12-13 17:39:24.967000: I runner.py:310] Step = 38500 ; steps/s = 1.63, tokens/s = 82313 (37855 source, 44458 target) ; Learning rate = 0.000450 ; Loss = 1.916644\n",
      "2024-12-13 17:40:26.381000: I runner.py:310] Step = 38600 ; steps/s = 1.63, tokens/s = 82284 (37849 source, 44435 target) ; Learning rate = 0.000450 ; Loss = 1.906985\n",
      "2024-12-13 17:41:27.372000: I runner.py:310] Step = 38700 ; steps/s = 1.64, tokens/s = 81479 (37468 source, 44011 target) ; Learning rate = 0.000449 ; Loss = 1.852199\n",
      "2024-12-13 17:42:28.723000: I runner.py:310] Step = 38800 ; steps/s = 1.63, tokens/s = 82357 (37867 source, 44490 target) ; Learning rate = 0.000449 ; Loss = 1.900823\n",
      "2024-12-13 17:43:30.076000: I runner.py:310] Step = 38900 ; steps/s = 1.63, tokens/s = 82386 (37898 source, 44488 target) ; Learning rate = 0.000448 ; Loss = 1.907473\n",
      "2024-12-13 17:44:31.016000: I runner.py:310] Step = 39000 ; steps/s = 1.64, tokens/s = 81567 (37531 source, 44036 target) ; Learning rate = 0.000448 ; Loss = 1.877715\n",
      "2024-12-13 17:45:32.407000: I runner.py:310] Step = 39100 ; steps/s = 1.63, tokens/s = 82327 (37867 source, 44460 target) ; Learning rate = 0.000447 ; Loss = 1.884928\n",
      "2024-12-13 17:46:33.887000: I runner.py:310] Step = 39200 ; steps/s = 1.63, tokens/s = 82157 (37762 source, 44395 target) ; Learning rate = 0.000446 ; Loss = 1.895467\n",
      "2024-12-13 17:47:35.282000: I runner.py:310] Step = 39300 ; steps/s = 1.63, tokens/s = 82329 (37869 source, 44460 target) ; Learning rate = 0.000446 ; Loss = 1.916127\n",
      "2024-12-13 17:48:36.256000: I runner.py:310] Step = 39400 ; steps/s = 1.64, tokens/s = 81514 (37495 source, 44019 target) ; Learning rate = 0.000445 ; Loss = 1.846859\n",
      "2024-12-13 17:49:37.690000: I runner.py:310] Step = 39500 ; steps/s = 1.63, tokens/s = 82263 (37839 source, 44424 target) ; Learning rate = 0.000445 ; Loss = 1.897316\n",
      "2024-12-13 17:50:39.123000: I runner.py:310] Step = 39600 ; steps/s = 1.63, tokens/s = 82245 (37826 source, 44419 target) ; Learning rate = 0.000444 ; Loss = 1.902973\n",
      "2024-12-13 17:51:40.069000: I runner.py:310] Step = 39700 ; steps/s = 1.64, tokens/s = 81565 (37521 source, 44044 target) ; Learning rate = 0.000444 ; Loss = 1.860475\n",
      "2024-12-13 17:52:41.421000: I runner.py:310] Step = 39800 ; steps/s = 1.63, tokens/s = 82398 (37903 source, 44495 target) ; Learning rate = 0.000443 ; Loss = 1.899805\n",
      "2024-12-13 17:53:42.867000: I runner.py:310] Step = 39900 ; steps/s = 1.63, tokens/s = 82241 (37836 source, 44405 target) ; Learning rate = 0.000442 ; Loss = 1.895025\n",
      "2024-12-13 17:54:44.322000: I runner.py:310] Step = 40000 ; steps/s = 1.63, tokens/s = 82200 (37790 source, 44410 target) ; Learning rate = 0.000442 ; Loss = 1.927826\n",
      "2024-12-13 17:54:46.098000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-40000\n",
      "2024-12-13 17:54:46.098000: I training.py:192] Running evaluation for step 40000\n",
      "2024-12-13 17:56:53.488000: I training.py:192] Evaluation result for step 40000: loss = 2.355104 ; perplexity = 10.539225\n",
      "2024-12-13 17:57:54.330000: I runner.py:310] Step = 40100 ; steps/s = 1.64, tokens/s = 81675 (37535 source, 44140 target) ; Learning rate = 0.000441 ; Loss = 1.915294\n",
      "2024-12-13 17:58:55.700000: I runner.py:310] Step = 40200 ; steps/s = 1.63, tokens/s = 82334 (37871 source, 44463 target) ; Learning rate = 0.000441 ; Loss = 1.865380\n",
      "2024-12-13 17:59:57.098000: I runner.py:310] Step = 40300 ; steps/s = 1.63, tokens/s = 82320 (37868 source, 44452 target) ; Learning rate = 0.000440 ; Loss = 1.878965\n",
      "2024-12-13 18:00:58.116000: I runner.py:310] Step = 40400 ; steps/s = 1.64, tokens/s = 81496 (37512 source, 43984 target) ; Learning rate = 0.000440 ; Loss = 1.850211\n",
      "2024-12-13 18:01:59.533000: I runner.py:310] Step = 40500 ; steps/s = 1.63, tokens/s = 82300 (37876 source, 44424 target) ; Learning rate = 0.000439 ; Loss = 1.883553\n",
      "2024-12-13 18:03:00.845000: I runner.py:310] Step = 40600 ; steps/s = 1.63, tokens/s = 82417 (37896 source, 44521 target) ; Learning rate = 0.000439 ; Loss = 1.890701\n",
      "2024-12-13 18:04:02.270000: I runner.py:310] Step = 40700 ; steps/s = 1.63, tokens/s = 82238 (37800 source, 44438 target) ; Learning rate = 0.000438 ; Loss = 1.891915\n",
      "2024-12-13 18:05:03.281000: I runner.py:310] Step = 40800 ; steps/s = 1.64, tokens/s = 81485 (37476 source, 44009 target) ; Learning rate = 0.000438 ; Loss = 1.848823\n",
      "2024-12-13 18:06:04.660000: I runner.py:310] Step = 40900 ; steps/s = 1.63, tokens/s = 82334 (37889 source, 44445 target) ; Learning rate = 0.000437 ; Loss = 1.881989\n",
      "2024-12-13 18:07:06.036000: I runner.py:310] Step = 41000 ; steps/s = 1.63, tokens/s = 82305 (37833 source, 44472 target) ; Learning rate = 0.000437 ; Loss = 1.892220\n",
      "2024-12-13 18:08:07.012000: I runner.py:310] Step = 41100 ; steps/s = 1.64, tokens/s = 81525 (37499 source, 44026 target) ; Learning rate = 0.000436 ; Loss = 1.842009\n",
      "2024-12-13 18:09:08.457000: I runner.py:310] Step = 41200 ; steps/s = 1.63, tokens/s = 82254 (37845 source, 44409 target) ; Learning rate = 0.000435 ; Loss = 1.884570\n",
      "2024-12-13 18:10:09.780000: I runner.py:310] Step = 41300 ; steps/s = 1.63, tokens/s = 82413 (37909 source, 44504 target) ; Learning rate = 0.000435 ; Loss = 1.890488\n",
      "2024-12-13 18:11:11.193000: I runner.py:310] Step = 41400 ; steps/s = 1.63, tokens/s = 82270 (37821 source, 44449 target) ; Learning rate = 0.000434 ; Loss = 1.903465\n",
      "2024-12-13 18:12:12.112000: I runner.py:310] Step = 41500 ; steps/s = 1.64, tokens/s = 81584 (37522 source, 44062 target) ; Learning rate = 0.000434 ; Loss = 1.853948\n",
      "2024-12-13 18:13:13.466000: I runner.py:310] Step = 41600 ; steps/s = 1.63, tokens/s = 82373 (37883 source, 44490 target) ; Learning rate = 0.000433 ; Loss = 1.910214\n",
      "2024-12-13 18:14:14.822000: I runner.py:310] Step = 41700 ; steps/s = 1.63, tokens/s = 82371 (37900 source, 44471 target) ; Learning rate = 0.000433 ; Loss = 1.907089\n",
      "2024-12-13 18:15:15.848000: I runner.py:310] Step = 41800 ; steps/s = 1.64, tokens/s = 81432 (37445 source, 43987 target) ; Learning rate = 0.000432 ; Loss = 1.879233\n",
      "2024-12-13 18:16:17.274000: I runner.py:310] Step = 41900 ; steps/s = 1.63, tokens/s = 82253 (37832 source, 44421 target) ; Learning rate = 0.000432 ; Loss = 1.876113\n",
      "2024-12-13 18:17:18.599000: I runner.py:310] Step = 42000 ; steps/s = 1.63, tokens/s = 82392 (37865 source, 44527 target) ; Learning rate = 0.000431 ; Loss = 1.872132\n",
      "2024-12-13 18:18:19.650000: I runner.py:310] Step = 42100 ; steps/s = 1.64, tokens/s = 81453 (37495 source, 43958 target) ; Learning rate = 0.000431 ; Loss = 1.866281\n",
      "2024-12-13 18:19:20.972000: I runner.py:310] Step = 42200 ; steps/s = 1.63, tokens/s = 82402 (37904 source, 44498 target) ; Learning rate = 0.000430 ; Loss = 1.864056\n",
      "2024-12-13 18:20:22.347000: I runner.py:310] Step = 42300 ; steps/s = 1.63, tokens/s = 82332 (37851 source, 44481 target) ; Learning rate = 0.000430 ; Loss = 1.883642\n",
      "2024-12-13 18:21:23.693000: I runner.py:310] Step = 42400 ; steps/s = 1.63, tokens/s = 82381 (37898 source, 44483 target) ; Learning rate = 0.000429 ; Loss = 1.889988\n",
      "2024-12-13 18:22:24.618000: I runner.py:310] Step = 42500 ; steps/s = 1.64, tokens/s = 81583 (37529 source, 44054 target) ; Learning rate = 0.000429 ; Loss = 1.877785\n",
      "2024-12-13 18:23:25.957000: I runner.py:310] Step = 42600 ; steps/s = 1.63, tokens/s = 82373 (37891 source, 44482 target) ; Learning rate = 0.000428 ; Loss = 1.858947\n",
      "2024-12-13 18:24:27.424000: I runner.py:310] Step = 42700 ; steps/s = 1.63, tokens/s = 82218 (37815 source, 44403 target) ; Learning rate = 0.000428 ; Loss = 1.853841\n",
      "2024-12-13 18:25:28.411000: I runner.py:310] Step = 42800 ; steps/s = 1.64, tokens/s = 81505 (37474 source, 44031 target) ; Learning rate = 0.000427 ; Loss = 1.879739\n",
      "2024-12-13 18:26:29.786000: I runner.py:310] Step = 42900 ; steps/s = 1.63, tokens/s = 82363 (37882 source, 44481 target) ; Learning rate = 0.000427 ; Loss = 1.865644\n",
      "2024-12-13 18:27:31.184000: I runner.py:310] Step = 43000 ; steps/s = 1.63, tokens/s = 82288 (37842 source, 44446 target) ; Learning rate = 0.000426 ; Loss = 1.860854\n",
      "2024-12-13 18:28:32.554000: I runner.py:310] Step = 43100 ; steps/s = 1.63, tokens/s = 82340 (37882 source, 44458 target) ; Learning rate = 0.000426 ; Loss = 1.876560\n",
      "2024-12-13 18:29:33.473000: I runner.py:310] Step = 43200 ; steps/s = 1.64, tokens/s = 81575 (37500 source, 44075 target) ; Learning rate = 0.000425 ; Loss = 1.894127\n",
      "2024-12-13 18:30:34.903000: I runner.py:310] Step = 43300 ; steps/s = 1.63, tokens/s = 82259 (37835 source, 44424 target) ; Learning rate = 0.000425 ; Loss = 1.845606\n",
      "2024-12-13 18:31:36.324000: I runner.py:310] Step = 43400 ; steps/s = 1.63, tokens/s = 82292 (37856 source, 44436 target) ; Learning rate = 0.000424 ; Loss = 1.860798\n",
      "2024-12-13 18:32:37.373000: I runner.py:310] Step = 43500 ; steps/s = 1.64, tokens/s = 81430 (37463 source, 43967 target) ; Learning rate = 0.000424 ; Loss = 1.866793\n",
      "2024-12-13 18:33:38.744000: I runner.py:310] Step = 43600 ; steps/s = 1.63, tokens/s = 82344 (37872 source, 44472 target) ; Learning rate = 0.000423 ; Loss = 1.868096\n",
      "2024-12-13 18:34:40.174000: I runner.py:310] Step = 43700 ; steps/s = 1.63, tokens/s = 82245 (37818 source, 44427 target) ; Learning rate = 0.000423 ; Loss = 1.886826\n",
      "2024-12-13 18:35:41.645000: I runner.py:310] Step = 43800 ; steps/s = 1.63, tokens/s = 82204 (37812 source, 44392 target) ; Learning rate = 0.000422 ; Loss = 1.871587\n",
      "2024-12-13 18:36:42.598000: I runner.py:310] Step = 43900 ; steps/s = 1.64, tokens/s = 81554 (37510 source, 44044 target) ; Learning rate = 0.000422 ; Loss = 1.894864\n",
      "2024-12-13 18:37:44.046000: I runner.py:310] Step = 44000 ; steps/s = 1.63, tokens/s = 82257 (37844 source, 44413 target) ; Learning rate = 0.000421 ; Loss = 1.847250\n",
      "2024-12-13 18:38:45.436000: I runner.py:310] Step = 44100 ; steps/s = 1.63, tokens/s = 82305 (37839 source, 44466 target) ; Learning rate = 0.000421 ; Loss = 1.858559\n",
      "2024-12-13 18:39:46.472000: I runner.py:310] Step = 44200 ; steps/s = 1.64, tokens/s = 81426 (37450 source, 43976 target) ; Learning rate = 0.000420 ; Loss = 1.859370\n",
      "2024-12-13 18:40:47.814000: I runner.py:310] Step = 44300 ; steps/s = 1.63, tokens/s = 82416 (37913 source, 44503 target) ; Learning rate = 0.000420 ; Loss = 1.853639\n",
      "2024-12-13 18:41:49.196000: I runner.py:310] Step = 44400 ; steps/s = 1.63, tokens/s = 82286 (37825 source, 44461 target) ; Learning rate = 0.000419 ; Loss = 1.882067\n",
      "2024-12-13 18:42:50.604000: I runner.py:310] Step = 44500 ; steps/s = 1.63, tokens/s = 82297 (37867 source, 44430 target) ; Learning rate = 0.000419 ; Loss = 1.876238\n",
      "2024-12-13 18:43:51.561000: I runner.py:310] Step = 44600 ; steps/s = 1.64, tokens/s = 81566 (37537 source, 44029 target) ; Learning rate = 0.000419 ; Loss = 1.878875\n",
      "2024-12-13 18:44:52.939000: I runner.py:310] Step = 44700 ; steps/s = 1.63, tokens/s = 82329 (37861 source, 44468 target) ; Learning rate = 0.000418 ; Loss = 1.839371\n",
      "2024-12-13 18:45:54.420000: I runner.py:310] Step = 44800 ; steps/s = 1.63, tokens/s = 82208 (37812 source, 44396 target) ; Learning rate = 0.000418 ; Loss = 1.849515\n",
      "2024-12-13 18:46:55.403000: I runner.py:310] Step = 44900 ; steps/s = 1.64, tokens/s = 81479 (37468 source, 44011 target) ; Learning rate = 0.000417 ; Loss = 1.854254\n",
      "2024-12-13 18:47:56.741000: I runner.py:310] Step = 45000 ; steps/s = 1.63, tokens/s = 82383 (37898 source, 44485 target) ; Learning rate = 0.000417 ; Loss = 1.861426\n",
      "2024-12-13 18:47:56.743000: I training.py:192] Running evaluation for step 45000\n",
      "2024-12-13 18:50:01.137000: I training.py:192] Evaluation result for step 45000: loss = 2.397552 ; perplexity = 10.996220\n",
      "2024-12-13 18:51:02.348000: I runner.py:310] Step = 45100 ; steps/s = 1.63, tokens/s = 82574 (37975 source, 44599 target) ; Learning rate = 0.000416 ; Loss = 1.868766\n",
      "2024-12-13 18:52:03.739000: I runner.py:310] Step = 45200 ; steps/s = 1.63, tokens/s = 82309 (37849 source, 44460 target) ; Learning rate = 0.000416 ; Loss = 1.880608\n",
      "2024-12-13 18:53:04.741000: I runner.py:310] Step = 45300 ; steps/s = 1.64, tokens/s = 81483 (37486 source, 43997 target) ; Learning rate = 0.000415 ; Loss = 1.890281\n",
      "2024-12-13 18:54:06.197000: I runner.py:310] Step = 45400 ; steps/s = 1.63, tokens/s = 82228 (37811 source, 44417 target) ; Learning rate = 0.000415 ; Loss = 1.842088\n",
      "2024-12-13 18:55:07.617000: I runner.py:310] Step = 45500 ; steps/s = 1.63, tokens/s = 82281 (37843 source, 44438 target) ; Learning rate = 0.000414 ; Loss = 1.854532\n",
      "2024-12-13 18:56:09.402000: I runner.py:310] Step = 45600 ; steps/s = 1.62, tokens/s = 80436 (36990 source, 43446 target) ; Learning rate = 0.000414 ; Loss = 1.869102\n",
      "2024-12-13 18:57:10.804000: I runner.py:310] Step = 45700 ; steps/s = 1.63, tokens/s = 82269 (37827 source, 44442 target) ; Learning rate = 0.000413 ; Loss = 1.848293\n",
      "2024-12-13 18:58:12.167000: I runner.py:310] Step = 45800 ; steps/s = 1.63, tokens/s = 82353 (37874 source, 44479 target) ; Learning rate = 0.000413 ; Loss = 1.846731\n",
      "2024-12-13 18:59:13.546000: I runner.py:310] Step = 45900 ; steps/s = 1.63, tokens/s = 82365 (37907 source, 44458 target) ; Learning rate = 0.000413 ; Loss = 1.854926\n",
      "2024-12-13 19:00:14.510000: I runner.py:310] Step = 46000 ; steps/s = 1.64, tokens/s = 81539 (37500 source, 44039 target) ; Learning rate = 0.000412 ; Loss = 1.887839\n",
      "2024-12-13 19:01:15.884000: I runner.py:310] Step = 46100 ; steps/s = 1.63, tokens/s = 82336 (37862 source, 44474 target) ; Learning rate = 0.000412 ; Loss = 1.841532\n",
      "2024-12-13 19:02:17.357000: I runner.py:310] Step = 46200 ; steps/s = 1.63, tokens/s = 82215 (37825 source, 44390 target) ; Learning rate = 0.000411 ; Loss = 1.861149\n",
      "2024-12-13 19:03:18.332000: I runner.py:310] Step = 46300 ; steps/s = 1.64, tokens/s = 81508 (37475 source, 44033 target) ; Learning rate = 0.000411 ; Loss = 1.845297\n",
      "2024-12-13 19:04:19.799000: I runner.py:310] Step = 46400 ; steps/s = 1.63, tokens/s = 82210 (37809 source, 44401 target) ; Learning rate = 0.000410 ; Loss = 1.856273\n",
      "2024-12-13 19:05:21.126000: I runner.py:310] Step = 46500 ; steps/s = 1.63, tokens/s = 82406 (37916 source, 44490 target) ; Learning rate = 0.000410 ; Loss = 1.869507\n",
      "2024-12-13 19:06:22.497000: I runner.py:310] Step = 46600 ; steps/s = 1.63, tokens/s = 82348 (37880 source, 44468 target) ; Learning rate = 0.000409 ; Loss = 1.876993\n",
      "2024-12-13 19:07:23.456000: I runner.py:310] Step = 46700 ; steps/s = 1.64, tokens/s = 81546 (37505 source, 44041 target) ; Learning rate = 0.000409 ; Loss = 1.800450\n",
      "2024-12-13 19:08:24.835000: I runner.py:310] Step = 46800 ; steps/s = 1.63, tokens/s = 82348 (37877 source, 44471 target) ; Learning rate = 0.000409 ; Loss = 1.870064\n",
      "2024-12-13 19:09:26.221000: I runner.py:310] Step = 46900 ; steps/s = 1.63, tokens/s = 82276 (37817 source, 44459 target) ; Learning rate = 0.000408 ; Loss = 1.862498\n",
      "2024-12-13 19:10:27.202000: I runner.py:310] Step = 47000 ; steps/s = 1.64, tokens/s = 81531 (37518 source, 44013 target) ; Learning rate = 0.000408 ; Loss = 1.855230\n",
      "2024-12-13 19:11:28.610000: I runner.py:310] Step = 47100 ; steps/s = 1.63, tokens/s = 82260 (37824 source, 44436 target) ; Learning rate = 0.000407 ; Loss = 1.836544\n",
      "2024-12-13 19:12:29.937000: I runner.py:310] Step = 47200 ; steps/s = 1.63, tokens/s = 82409 (37909 source, 44500 target) ; Learning rate = 0.000407 ; Loss = 1.858799\n",
      "2024-12-13 19:13:31.400000: I runner.py:310] Step = 47300 ; steps/s = 1.63, tokens/s = 82235 (37818 source, 44417 target) ; Learning rate = 0.000406 ; Loss = 1.852577\n",
      "2024-12-13 19:14:32.402000: I runner.py:310] Step = 47400 ; steps/s = 1.64, tokens/s = 81479 (37486 source, 43993 target) ; Learning rate = 0.000406 ; Loss = 1.808111\n",
      "2024-12-13 19:15:33.804000: I runner.py:310] Step = 47500 ; steps/s = 1.63, tokens/s = 82303 (37857 source, 44446 target) ; Learning rate = 0.000406 ; Loss = 1.879956\n",
      "2024-12-13 19:16:35.221000: I runner.py:310] Step = 47600 ; steps/s = 1.63, tokens/s = 82278 (37830 source, 44448 target) ; Learning rate = 0.000405 ; Loss = 1.879774\n",
      "2024-12-13 19:17:36.224000: I runner.py:310] Step = 47700 ; steps/s = 1.64, tokens/s = 81465 (37460 source, 44005 target) ; Learning rate = 0.000405 ; Loss = 1.834749\n",
      "2024-12-13 19:18:37.652000: I runner.py:310] Step = 47800 ; steps/s = 1.63, tokens/s = 82288 (37861 source, 44427 target) ; Learning rate = 0.000404 ; Loss = 1.847372\n",
      "2024-12-13 19:19:39.007000: I runner.py:310] Step = 47900 ; steps/s = 1.63, tokens/s = 82363 (37877 source, 44486 target) ; Learning rate = 0.000404 ; Loss = 1.852994\n",
      "2024-12-13 19:20:40.406000: I runner.py:310] Step = 48000 ; steps/s = 1.63, tokens/s = 82305 (37853 source, 44452 target) ; Learning rate = 0.000403 ; Loss = 1.869453\n",
      "2024-12-13 19:21:41.321000: I runner.py:310] Step = 48100 ; steps/s = 1.64, tokens/s = 81592 (37528 source, 44064 target) ; Learning rate = 0.000403 ; Loss = 1.821232\n",
      "2024-12-13 19:22:42.741000: I runner.py:310] Step = 48200 ; steps/s = 1.63, tokens/s = 82267 (37838 source, 44429 target) ; Learning rate = 0.000403 ; Loss = 1.851839\n",
      "2024-12-13 19:23:44.125000: I runner.py:310] Step = 48300 ; steps/s = 1.63, tokens/s = 82322 (37848 source, 44474 target) ; Learning rate = 0.000402 ; Loss = 1.866317\n",
      "2024-12-13 19:24:45.085000: I runner.py:310] Step = 48400 ; steps/s = 1.64, tokens/s = 81552 (37533 source, 44019 target) ; Learning rate = 0.000402 ; Loss = 1.864497\n",
      "2024-12-13 19:25:46.460000: I runner.py:310] Step = 48500 ; steps/s = 1.63, tokens/s = 82323 (37853 source, 44470 target) ; Learning rate = 0.000401 ; Loss = 1.826477\n",
      "2024-12-13 19:26:47.801000: I runner.py:310] Step = 48600 ; steps/s = 1.63, tokens/s = 82403 (37905 source, 44498 target) ; Learning rate = 0.000401 ; Loss = 1.845719\n",
      "2024-12-13 19:27:49.269000: I runner.py:310] Step = 48700 ; steps/s = 1.63, tokens/s = 82204 (37798 source, 44406 target) ; Learning rate = 0.000401 ; Loss = 1.827541\n",
      "2024-12-13 19:28:50.204000: I runner.py:310] Step = 48800 ; steps/s = 1.64, tokens/s = 81559 (37523 source, 44036 target) ; Learning rate = 0.000400 ; Loss = 1.850794\n",
      "2024-12-13 19:29:51.703000: I runner.py:310] Step = 48900 ; steps/s = 1.63, tokens/s = 82195 (37828 source, 44367 target) ; Learning rate = 0.000400 ; Loss = 1.846242\n",
      "2024-12-13 19:30:53.081000: I runner.py:310] Step = 49000 ; steps/s = 1.63, tokens/s = 82347 (37866 source, 44481 target) ; Learning rate = 0.000399 ; Loss = 1.840561\n",
      "2024-12-13 19:31:54.067000: I runner.py:310] Step = 49100 ; steps/s = 1.64, tokens/s = 81469 (37451 source, 44018 target) ; Learning rate = 0.000399 ; Loss = 1.829635\n",
      "2024-12-13 19:32:55.415000: I runner.py:310] Step = 49200 ; steps/s = 1.63, tokens/s = 82405 (37921 source, 44484 target) ; Learning rate = 0.000398 ; Loss = 1.849735\n",
      "2024-12-13 19:33:56.743000: I runner.py:310] Step = 49300 ; steps/s = 1.63, tokens/s = 82391 (37884 source, 44507 target) ; Learning rate = 0.000398 ; Loss = 1.857442\n",
      "2024-12-13 19:34:58.206000: I runner.py:310] Step = 49400 ; steps/s = 1.63, tokens/s = 82212 (37808 source, 44404 target) ; Learning rate = 0.000398 ; Loss = 1.860321\n",
      "2024-12-13 19:35:59.213000: I runner.py:310] Step = 49500 ; steps/s = 1.64, tokens/s = 81474 (37476 source, 43998 target) ; Learning rate = 0.000397 ; Loss = 1.855300\n",
      "2024-12-13 19:37:00.645000: I runner.py:310] Step = 49600 ; steps/s = 1.63, tokens/s = 82258 (37815 source, 44443 target) ; Learning rate = 0.000397 ; Loss = 1.830940\n",
      "2024-12-13 19:38:01.964000: I runner.py:310] Step = 49700 ; steps/s = 1.63, tokens/s = 82406 (37901 source, 44505 target) ; Learning rate = 0.000396 ; Loss = 1.841904\n",
      "2024-12-13 19:39:02.987000: I runner.py:310] Step = 49800 ; steps/s = 1.64, tokens/s = 81435 (37461 source, 43974 target) ; Learning rate = 0.000396 ; Loss = 1.854324\n",
      "2024-12-13 19:40:04.379000: I runner.py:310] Step = 49900 ; steps/s = 1.63, tokens/s = 82286 (37841 source, 44445 target) ; Learning rate = 0.000396 ; Loss = 1.822468\n",
      "2024-12-13 19:41:05.830000: I runner.py:310] Step = 50000 ; steps/s = 1.63, tokens/s = 82269 (37858 source, 44411 target) ; Learning rate = 0.000395 ; Loss = 1.826389\n",
      "2024-12-13 19:41:07.690000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-50000\n",
      "2024-12-13 19:41:07.690000: I training.py:192] Running evaluation for step 50000\n",
      "2024-12-13 19:43:11.590000: I training.py:192] Evaluation result for step 50000: loss = 2.428246 ; perplexity = 11.338982\n",
      "2024-12-13 19:44:12.590000: I runner.py:310] Step = 50100 ; steps/s = 1.64, tokens/s = 82093 (37749 source, 44344 target) ; Learning rate = 0.000395 ; Loss = 1.848655\n",
      "2024-12-13 19:45:13.840000: I runner.py:310] Step = 50200 ; steps/s = 1.63, tokens/s = 81900 (37666 source, 44234 target) ; Learning rate = 0.000394 ; Loss = 1.808247\n",
      "2024-12-13 19:46:15.268000: I runner.py:310] Step = 50300 ; steps/s = 1.63, tokens/s = 82270 (37837 source, 44433 target) ; Learning rate = 0.000394 ; Loss = 1.837630\n",
      "2024-12-13 19:47:16.626000: I runner.py:310] Step = 50400 ; steps/s = 1.63, tokens/s = 82382 (37896 source, 44486 target) ; Learning rate = 0.000394 ; Loss = 1.865150\n",
      "2024-12-13 19:48:17.595000: I runner.py:310] Step = 50500 ; steps/s = 1.64, tokens/s = 81497 (37478 source, 44019 target) ; Learning rate = 0.000393 ; Loss = 1.857949\n",
      "2024-12-13 19:49:18.950000: I runner.py:310] Step = 50600 ; steps/s = 1.63, tokens/s = 82368 (37886 source, 44482 target) ; Learning rate = 0.000393 ; Loss = 1.821439\n",
      "2024-12-13 19:50:20.285000: I runner.py:310] Step = 50700 ; steps/s = 1.63, tokens/s = 82417 (37915 source, 44502 target) ; Learning rate = 0.000393 ; Loss = 1.843708\n",
      "2024-12-13 19:51:21.221000: I runner.py:310] Step = 50800 ; steps/s = 1.64, tokens/s = 81559 (37510 source, 44049 target) ; Learning rate = 0.000392 ; Loss = 1.848512\n",
      "2024-12-13 19:52:22.563000: I runner.py:310] Step = 50900 ; steps/s = 1.63, tokens/s = 82380 (37875 source, 44505 target) ; Learning rate = 0.000392 ; Loss = 1.850601\n",
      "2024-12-13 19:53:24.007000: I runner.py:310] Step = 51000 ; steps/s = 1.63, tokens/s = 82233 (37830 source, 44403 target) ; Learning rate = 0.000391 ; Loss = 1.828080\n",
      "2024-12-13 19:54:25.384000: I runner.py:310] Step = 51100 ; steps/s = 1.63, tokens/s = 82319 (37849 source, 44470 target) ; Learning rate = 0.000391 ; Loss = 1.835857\n",
      "2024-12-13 19:55:26.445000: I runner.py:310] Step = 51200 ; steps/s = 1.64, tokens/s = 81413 (37458 source, 43955 target) ; Learning rate = 0.000391 ; Loss = 1.878405\n",
      "2024-12-13 19:56:27.774000: I runner.py:310] Step = 51300 ; steps/s = 1.63, tokens/s = 82405 (37917 source, 44488 target) ; Learning rate = 0.000390 ; Loss = 1.829907\n",
      "2024-12-13 19:57:29.214000: I runner.py:310] Step = 51400 ; steps/s = 1.63, tokens/s = 82241 (37808 source, 44433 target) ; Learning rate = 0.000390 ; Loss = 1.825585\n",
      "2024-12-13 19:58:30.160000: I runner.py:310] Step = 51500 ; steps/s = 1.64, tokens/s = 81571 (37522 source, 44049 target) ; Learning rate = 0.000389 ; Loss = 1.826461\n",
      "2024-12-13 19:59:31.618000: I runner.py:310] Step = 51600 ; steps/s = 1.63, tokens/s = 82222 (37805 source, 44417 target) ; Learning rate = 0.000389 ; Loss = 1.824152\n",
      "2024-12-13 20:00:32.998000: I runner.py:310] Step = 51700 ; steps/s = 1.63, tokens/s = 82321 (37862 source, 44459 target) ; Learning rate = 0.000389 ; Loss = 1.852595\n",
      "2024-12-13 20:01:34.337000: I runner.py:310] Step = 51800 ; steps/s = 1.63, tokens/s = 82367 (37878 source, 44489 target) ; Learning rate = 0.000388 ; Loss = 1.841162\n",
      "2024-12-13 20:02:35.333000: I runner.py:310] Step = 51900 ; steps/s = 1.64, tokens/s = 81498 (37489 source, 44009 target) ; Learning rate = 0.000388 ; Loss = 1.842750\n",
      "2024-12-13 20:03:36.686000: I runner.py:310] Step = 52000 ; steps/s = 1.63, tokens/s = 82400 (37906 source, 44494 target) ; Learning rate = 0.000388 ; Loss = 1.821643\n",
      "2024-12-13 20:04:38.122000: I runner.py:310] Step = 52100 ; steps/s = 1.63, tokens/s = 82255 (37841 source, 44414 target) ; Learning rate = 0.000387 ; Loss = 1.817096\n",
      "2024-12-13 20:05:39.076000: I runner.py:310] Step = 52200 ; steps/s = 1.64, tokens/s = 81531 (37497 source, 44034 target) ; Learning rate = 0.000387 ; Loss = 1.822544\n",
      "2024-12-13 20:06:40.524000: I runner.py:310] Step = 52300 ; steps/s = 1.63, tokens/s = 82248 (37828 source, 44420 target) ; Learning rate = 0.000386 ; Loss = 1.800674\n",
      "2024-12-13 20:07:41.937000: I runner.py:310] Step = 52400 ; steps/s = 1.63, tokens/s = 82314 (37868 source, 44446 target) ; Learning rate = 0.000386 ; Loss = 1.839681\n",
      "2024-12-13 20:08:43.356000: I runner.py:310] Step = 52500 ; steps/s = 1.63, tokens/s = 82247 (37819 source, 44428 target) ; Learning rate = 0.000386 ; Loss = 1.857939\n",
      "2024-12-13 20:09:44.379000: I runner.py:310] Step = 52600 ; steps/s = 1.64, tokens/s = 81433 (37433 source, 44000 target) ; Learning rate = 0.000385 ; Loss = 1.852197\n",
      "2024-12-13 20:10:45.725000: I runner.py:310] Step = 52700 ; steps/s = 1.63, tokens/s = 82349 (37868 source, 44481 target) ; Learning rate = 0.000385 ; Loss = 1.811120\n",
      "2024-12-13 20:11:47.175000: I runner.py:310] Step = 52800 ; steps/s = 1.63, tokens/s = 82256 (37848 source, 44408 target) ; Learning rate = 0.000385 ; Loss = 1.822833\n",
      "2024-12-13 20:12:48.084000: I runner.py:310] Step = 52900 ; steps/s = 1.64, tokens/s = 81595 (37541 source, 44054 target) ; Learning rate = 0.000384 ; Loss = 1.842635\n",
      "2024-12-13 20:13:49.444000: I runner.py:310] Step = 53000 ; steps/s = 1.63, tokens/s = 82360 (37870 source, 44490 target) ; Learning rate = 0.000384 ; Loss = 1.832081\n",
      "2024-12-13 20:14:50.845000: I runner.py:310] Step = 53100 ; steps/s = 1.63, tokens/s = 82324 (37869 source, 44455 target) ; Learning rate = 0.000384 ; Loss = 1.818473\n",
      "2024-12-13 20:15:52.276000: I runner.py:310] Step = 53200 ; steps/s = 1.63, tokens/s = 82265 (37851 source, 44414 target) ; Learning rate = 0.000383 ; Loss = 1.833528\n",
      "2024-12-13 20:16:53.228000: I runner.py:310] Step = 53300 ; steps/s = 1.64, tokens/s = 81513 (37483 source, 44030 target) ; Learning rate = 0.000383 ; Loss = 1.841453\n",
      "2024-12-13 20:17:54.624000: I runner.py:310] Step = 53400 ; steps/s = 1.63, tokens/s = 82317 (37864 source, 44453 target) ; Learning rate = 0.000382 ; Loss = 1.834345\n",
      "2024-12-13 20:18:55.950000: I runner.py:310] Step = 53500 ; steps/s = 1.63, tokens/s = 82406 (37897 source, 44509 target) ; Learning rate = 0.000382 ; Loss = 1.822016\n",
      "2024-12-13 20:19:56.872000: I runner.py:310] Step = 53600 ; steps/s = 1.64, tokens/s = 81578 (37516 source, 44062 target) ; Learning rate = 0.000382 ; Loss = 1.851345\n",
      "2024-12-13 20:20:58.275000: I runner.py:310] Step = 53700 ; steps/s = 1.63, tokens/s = 82314 (37865 source, 44449 target) ; Learning rate = 0.000381 ; Loss = 1.837249\n",
      "2024-12-13 20:21:59.697000: I runner.py:310] Step = 53800 ; steps/s = 1.63, tokens/s = 82261 (37829 source, 44432 target) ; Learning rate = 0.000381 ; Loss = 1.808165\n",
      "2024-12-13 20:23:01.079000: I runner.py:310] Step = 53900 ; steps/s = 1.63, tokens/s = 82341 (37872 source, 44469 target) ; Learning rate = 0.000381 ; Loss = 1.829212\n",
      "2024-12-13 20:24:02.075000: I runner.py:310] Step = 54000 ; steps/s = 1.64, tokens/s = 81507 (37486 source, 44021 target) ; Learning rate = 0.000380 ; Loss = 1.783837\n",
      "2024-12-13 20:25:03.554000: I runner.py:310] Step = 54100 ; steps/s = 1.63, tokens/s = 82192 (37793 source, 44399 target) ; Learning rate = 0.000380 ; Loss = 1.840544\n",
      "2024-12-13 20:26:04.908000: I runner.py:310] Step = 54200 ; steps/s = 1.63, tokens/s = 82352 (37896 source, 44456 target) ; Learning rate = 0.000380 ; Loss = 1.838208\n",
      "2024-12-13 20:27:05.955000: I runner.py:310] Step = 54300 ; steps/s = 1.64, tokens/s = 81422 (37446 source, 43976 target) ; Learning rate = 0.000379 ; Loss = 1.823548\n",
      "2024-12-13 20:28:07.338000: I runner.py:310] Step = 54400 ; steps/s = 1.63, tokens/s = 82305 (37839 source, 44466 target) ; Learning rate = 0.000379 ; Loss = 1.836491\n",
      "2024-12-13 20:29:08.665000: I runner.py:310] Step = 54500 ; steps/s = 1.63, tokens/s = 82396 (37903 source, 44493 target) ; Learning rate = 0.000379 ; Loss = 1.841497\n",
      "2024-12-13 20:30:10.111000: I runner.py:310] Step = 54600 ; steps/s = 1.63, tokens/s = 82269 (37844 source, 44425 target) ; Learning rate = 0.000378 ; Loss = 1.840411\n",
      "2024-12-13 20:31:11.023000: I runner.py:310] Step = 54700 ; steps/s = 1.64, tokens/s = 81622 (37558 source, 44064 target) ; Learning rate = 0.000378 ; Loss = 1.784021\n",
      "2024-12-13 20:32:12.428000: I runner.py:310] Step = 54800 ; steps/s = 1.63, tokens/s = 82280 (37842 source, 44438 target) ; Learning rate = 0.000378 ; Loss = 1.840598\n",
      "2024-12-13 20:33:13.789000: I runner.py:310] Step = 54900 ; steps/s = 1.63, tokens/s = 82352 (37866 source, 44486 target) ; Learning rate = 0.000377 ; Loss = 1.836786\n",
      "2024-12-13 20:34:14.832000: I runner.py:310] Step = 55000 ; steps/s = 1.64, tokens/s = 81411 (37438 source, 43973 target) ; Learning rate = 0.000377 ; Loss = 1.831937\n",
      "2024-12-13 20:34:14.833000: I training.py:192] Running evaluation for step 55000\n",
      "2024-12-13 20:36:19.924000: I training.py:192] Evaluation result for step 55000: loss = 2.442019 ; perplexity = 11.496226\n",
      "2024-12-13 20:37:21.073000: I runner.py:310] Step = 55100 ; steps/s = 1.64, tokens/s = 82653 (38015 source, 44638 target) ; Learning rate = 0.000377 ; Loss = 1.833503\n",
      "2024-12-13 20:38:22.420000: I runner.py:310] Step = 55200 ; steps/s = 1.63, tokens/s = 82384 (37895 source, 44489 target) ; Learning rate = 0.000376 ; Loss = 1.810417\n",
      "2024-12-13 20:39:23.908000: I runner.py:310] Step = 55300 ; steps/s = 1.63, tokens/s = 82195 (37804 source, 44391 target) ; Learning rate = 0.000376 ; Loss = 1.832141\n",
      "2024-12-13 20:40:24.816000: I runner.py:310] Step = 55400 ; steps/s = 1.64, tokens/s = 81624 (37559 source, 44065 target) ; Learning rate = 0.000376 ; Loss = 1.841555\n",
      "2024-12-13 20:41:26.269000: I runner.py:310] Step = 55500 ; steps/s = 1.63, tokens/s = 82242 (37834 source, 44408 target) ; Learning rate = 0.000375 ; Loss = 1.803950\n",
      "2024-12-13 20:42:27.561000: I runner.py:310] Step = 55600 ; steps/s = 1.63, tokens/s = 82435 (37898 source, 44537 target) ; Learning rate = 0.000375 ; Loss = 1.823144\n",
      "2024-12-13 20:43:28.628000: I runner.py:310] Step = 55700 ; steps/s = 1.64, tokens/s = 81357 (37406 source, 43951 target) ; Learning rate = 0.000375 ; Loss = 1.827960\n",
      "2024-12-13 20:44:29.974000: I runner.py:310] Step = 55800 ; steps/s = 1.63, tokens/s = 82397 (37908 source, 44489 target) ; Learning rate = 0.000374 ; Loss = 1.814659\n",
      "2024-12-13 20:45:31.431000: I runner.py:310] Step = 55900 ; steps/s = 1.63, tokens/s = 82224 (37808 source, 44416 target) ; Learning rate = 0.000374 ; Loss = 1.825565\n",
      "2024-12-13 20:46:32.811000: I runner.py:310] Step = 56000 ; steps/s = 1.63, tokens/s = 82322 (37863 source, 44459 target) ; Learning rate = 0.000374 ; Loss = 1.817240\n",
      "2024-12-13 20:47:33.804000: I runner.py:310] Step = 56100 ; steps/s = 1.64, tokens/s = 81512 (37484 source, 44028 target) ; Learning rate = 0.000373 ; Loss = 1.803203\n",
      "2024-12-13 20:48:35.249000: I runner.py:310] Step = 56200 ; steps/s = 1.63, tokens/s = 82232 (37827 source, 44405 target) ; Learning rate = 0.000373 ; Loss = 1.836746\n",
      "2024-12-13 20:49:36.654000: I runner.py:310] Step = 56300 ; steps/s = 1.63, tokens/s = 82285 (37844 source, 44441 target) ; Learning rate = 0.000373 ; Loss = 1.850480\n",
      "2024-12-13 20:50:37.687000: I runner.py:310] Step = 56400 ; steps/s = 1.64, tokens/s = 81425 (37459 source, 43966 target) ; Learning rate = 0.000372 ; Loss = 1.810181\n",
      "2024-12-13 20:51:39.005000: I runner.py:310] Step = 56500 ; steps/s = 1.63, tokens/s = 82431 (37924 source, 44507 target) ; Learning rate = 0.000372 ; Loss = 1.813829\n",
      "2024-12-13 20:52:40.414000: I runner.py:310] Step = 56600 ; steps/s = 1.63, tokens/s = 82294 (37837 source, 44457 target) ; Learning rate = 0.000372 ; Loss = 1.826073\n",
      "2024-12-13 20:53:41.797000: I runner.py:310] Step = 56700 ; steps/s = 1.63, tokens/s = 82325 (37868 source, 44457 target) ; Learning rate = 0.000371 ; Loss = 1.839924\n",
      "2024-12-13 20:54:42.831000: I runner.py:310] Step = 56800 ; steps/s = 1.64, tokens/s = 81419 (37430 source, 43989 target) ; Learning rate = 0.000371 ; Loss = 1.797314\n",
      "2024-12-13 20:55:44.262000: I runner.py:310] Step = 56900 ; steps/s = 1.63, tokens/s = 82264 (37848 source, 44416 target) ; Learning rate = 0.000371 ; Loss = 1.834490\n",
      "2024-12-13 20:56:45.641000: I runner.py:310] Step = 57000 ; steps/s = 1.63, tokens/s = 82334 (37869 source, 44465 target) ; Learning rate = 0.000370 ; Loss = 1.836126\n",
      "2024-12-13 20:57:46.696000: I runner.py:310] Step = 57100 ; steps/s = 1.64, tokens/s = 81383 (37417 source, 43966 target) ; Learning rate = 0.000370 ; Loss = 1.836577\n",
      "2024-12-13 20:58:48.071000: I runner.py:310] Step = 57200 ; steps/s = 1.63, tokens/s = 82332 (37865 source, 44467 target) ; Learning rate = 0.000370 ; Loss = 1.815619\n",
      "2024-12-13 20:59:49.549000: I runner.py:310] Step = 57300 ; steps/s = 1.63, tokens/s = 82198 (37814 source, 44384 target) ; Learning rate = 0.000369 ; Loss = 1.813162\n",
      "2024-12-13 21:00:50.926000: I runner.py:310] Step = 57400 ; steps/s = 1.63, tokens/s = 82367 (37892 source, 44475 target) ; Learning rate = 0.000369 ; Loss = 1.807779\n",
      "2024-12-13 21:01:51.899000: I runner.py:310] Step = 57500 ; steps/s = 1.64, tokens/s = 81520 (37499 source, 44021 target) ; Learning rate = 0.000369 ; Loss = 1.796126\n",
      "2024-12-13 21:02:53.237000: I runner.py:310] Step = 57600 ; steps/s = 1.63, tokens/s = 82394 (37900 source, 44494 target) ; Learning rate = 0.000368 ; Loss = 1.821269\n",
      "2024-12-13 21:03:54.702000: I runner.py:310] Step = 57700 ; steps/s = 1.63, tokens/s = 82203 (37795 source, 44408 target) ; Learning rate = 0.000368 ; Loss = 1.821720\n",
      "2024-12-13 21:04:55.688000: I runner.py:310] Step = 57800 ; steps/s = 1.64, tokens/s = 81463 (37466 source, 43997 target) ; Learning rate = 0.000368 ; Loss = 1.827765\n",
      "2024-12-13 21:05:57.085000: I runner.py:310] Step = 57900 ; steps/s = 1.63, tokens/s = 82318 (37871 source, 44447 target) ; Learning rate = 0.000367 ; Loss = 1.808237\n",
      "2024-12-13 21:06:58.551000: I runner.py:310] Step = 58000 ; steps/s = 1.63, tokens/s = 82227 (37809 source, 44418 target) ; Learning rate = 0.000367 ; Loss = 1.807313\n",
      "2024-12-13 21:07:59.916000: I runner.py:310] Step = 58100 ; steps/s = 1.63, tokens/s = 82363 (37886 source, 44477 target) ; Learning rate = 0.000367 ; Loss = 1.816507\n",
      "2024-12-13 21:09:01.039000: I runner.py:310] Step = 58200 ; steps/s = 1.64, tokens/s = 81285 (37372 source, 43913 target) ; Learning rate = 0.000366 ; Loss = 1.838924\n",
      "2024-12-13 21:10:02.407000: I runner.py:310] Step = 58300 ; steps/s = 1.63, tokens/s = 82364 (37913 source, 44451 target) ; Learning rate = 0.000366 ; Loss = 1.808996\n",
      "2024-12-13 21:11:03.866000: I runner.py:310] Step = 58400 ; steps/s = 1.63, tokens/s = 82207 (37791 source, 44416 target) ; Learning rate = 0.000366 ; Loss = 1.812159\n",
      "2024-12-13 21:12:04.814000: I runner.py:310] Step = 58500 ; steps/s = 1.64, tokens/s = 81581 (37528 source, 44053 target) ; Learning rate = 0.000365 ; Loss = 1.831825\n",
      "2024-12-13 21:13:06.301000: I runner.py:310] Step = 58600 ; steps/s = 1.63, tokens/s = 82163 (37781 source, 44382 target) ; Learning rate = 0.000365 ; Loss = 1.801936\n",
      "2024-12-13 21:14:07.646000: I runner.py:310] Step = 58700 ; steps/s = 1.63, tokens/s = 82392 (37891 source, 44501 target) ; Learning rate = 0.000365 ; Loss = 1.801898\n",
      "2024-12-13 21:15:08.610000: I runner.py:310] Step = 58800 ; steps/s = 1.64, tokens/s = 81546 (37519 source, 44027 target) ; Learning rate = 0.000365 ; Loss = 1.843185\n",
      "2024-12-13 21:16:09.969000: I runner.py:310] Step = 58900 ; steps/s = 1.63, tokens/s = 82346 (37862 source, 44484 target) ; Learning rate = 0.000364 ; Loss = 1.833428\n",
      "2024-12-13 21:17:11.362000: I runner.py:310] Step = 59000 ; steps/s = 1.63, tokens/s = 82315 (37874 source, 44441 target) ; Learning rate = 0.000364 ; Loss = 1.797185\n",
      "2024-12-13 21:18:12.805000: I runner.py:310] Step = 59100 ; steps/s = 1.63, tokens/s = 82242 (37828 source, 44414 target) ; Learning rate = 0.000364 ; Loss = 1.801977\n",
      "2024-12-13 21:19:13.758000: I runner.py:310] Step = 59200 ; steps/s = 1.64, tokens/s = 81530 (37479 source, 44051 target) ; Learning rate = 0.000363 ; Loss = 1.830252\n",
      "2024-12-13 21:20:15.154000: I runner.py:310] Step = 59300 ; steps/s = 1.63, tokens/s = 82304 (37864 source, 44440 target) ; Learning rate = 0.000363 ; Loss = 1.790668\n",
      "2024-12-13 21:21:16.496000: I runner.py:310] Step = 59400 ; steps/s = 1.63, tokens/s = 82392 (37895 source, 44497 target) ; Learning rate = 0.000363 ; Loss = 1.801420\n",
      "2024-12-13 21:22:17.533000: I runner.py:310] Step = 59500 ; steps/s = 1.64, tokens/s = 81438 (37460 source, 43978 target) ; Learning rate = 0.000362 ; Loss = 1.804958\n",
      "2024-12-13 21:23:18.951000: I runner.py:310] Step = 59600 ; steps/s = 1.63, tokens/s = 82294 (37848 source, 44446 target) ; Learning rate = 0.000362 ; Loss = 1.799015\n",
      "2024-12-13 21:24:20.341000: I runner.py:310] Step = 59700 ; steps/s = 1.63, tokens/s = 82289 (37829 source, 44460 target) ; Learning rate = 0.000362 ; Loss = 1.831220\n",
      "2024-12-13 21:25:21.763000: I runner.py:310] Step = 59800 ; steps/s = 1.63, tokens/s = 82274 (37856 source, 44418 target) ; Learning rate = 0.000361 ; Loss = 1.829216\n",
      "2024-12-13 21:26:22.732000: I runner.py:310] Step = 59900 ; steps/s = 1.64, tokens/s = 81514 (37487 source, 44027 target) ; Learning rate = 0.000361 ; Loss = 1.816915\n",
      "2024-12-13 21:27:24.179000: I runner.py:310] Step = 60000 ; steps/s = 1.63, tokens/s = 82239 (37817 source, 44422 target) ; Learning rate = 0.000361 ; Loss = 1.790889\n",
      "2024-12-13 21:27:26.115000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-60000\n",
      "2024-12-13 21:27:26.115000: I training.py:192] Running evaluation for step 60000\n",
      "2024-12-13 21:29:29.053000: I training.py:192] Evaluation result for step 60000: loss = 2.466208 ; perplexity = 11.777704\n",
      "2024-12-13 21:30:30.265000: I runner.py:310] Step = 60100 ; steps/s = 1.63, tokens/s = 82597 (37999 source, 44598 target) ; Learning rate = 0.000361 ; Loss = 1.813126\n",
      "2024-12-13 21:31:31.251000: I runner.py:310] Step = 60200 ; steps/s = 1.64, tokens/s = 81493 (37489 source, 44004 target) ; Learning rate = 0.000360 ; Loss = 1.796304\n",
      "2024-12-13 21:32:32.535000: I runner.py:310] Step = 60300 ; steps/s = 1.63, tokens/s = 82491 (37952 source, 44539 target) ; Learning rate = 0.000360 ; Loss = 1.799925\n",
      "2024-12-13 21:33:33.866000: I runner.py:310] Step = 60400 ; steps/s = 1.63, tokens/s = 82348 (37855 source, 44493 target) ; Learning rate = 0.000360 ; Loss = 1.820045\n",
      "2024-12-13 21:34:35.255000: I runner.py:310] Step = 60500 ; steps/s = 1.63, tokens/s = 82334 (37866 source, 44468 target) ; Learning rate = 0.000359 ; Loss = 1.823854\n",
      "2024-12-13 21:35:36.074000: I runner.py:310] Step = 60600 ; steps/s = 1.64, tokens/s = 81727 (37614 source, 44113 target) ; Learning rate = 0.000359 ; Loss = 1.776137\n",
      "2024-12-13 21:36:37.492000: I runner.py:310] Step = 60700 ; steps/s = 1.63, tokens/s = 82277 (37838 source, 44439 target) ; Learning rate = 0.000359 ; Loss = 1.807148\n",
      "2024-12-13 21:37:38.821000: I runner.py:310] Step = 60800 ; steps/s = 1.63, tokens/s = 82403 (37905 source, 44498 target) ; Learning rate = 0.000358 ; Loss = 1.814628\n",
      "2024-12-13 21:38:39.863000: I runner.py:310] Step = 60900 ; steps/s = 1.64, tokens/s = 81416 (37430 source, 43986 target) ; Learning rate = 0.000358 ; Loss = 1.801699\n",
      "2024-12-13 21:39:41.164000: I runner.py:310] Step = 61000 ; steps/s = 1.63, tokens/s = 82445 (37925 source, 44520 target) ; Learning rate = 0.000358 ; Loss = 1.793337\n",
      "2024-12-13 21:40:42.464000: I runner.py:310] Step = 61100 ; steps/s = 1.63, tokens/s = 82443 (37923 source, 44520 target) ; Learning rate = 0.000358 ; Loss = 1.829501\n",
      "2024-12-13 21:41:43.846000: I runner.py:310] Step = 61200 ; steps/s = 1.63, tokens/s = 82341 (37867 source, 44474 target) ; Learning rate = 0.000357 ; Loss = 1.816526\n",
      "2024-12-13 21:42:44.742000: I runner.py:310] Step = 61300 ; steps/s = 1.64, tokens/s = 81627 (37553 source, 44074 target) ; Learning rate = 0.000357 ; Loss = 1.770428\n",
      "2024-12-13 21:43:46.098000: I runner.py:310] Step = 61400 ; steps/s = 1.63, tokens/s = 82368 (37891 source, 44477 target) ; Learning rate = 0.000357 ; Loss = 1.803957\n",
      "2024-12-13 21:44:47.415000: I runner.py:310] Step = 61500 ; steps/s = 1.63, tokens/s = 82401 (37890 source, 44511 target) ; Learning rate = 0.000356 ; Loss = 1.809631\n",
      "2024-12-13 21:45:48.415000: I runner.py:310] Step = 61600 ; steps/s = 1.64, tokens/s = 81473 (37455 source, 44018 target) ; Learning rate = 0.000356 ; Loss = 1.809951\n",
      "2024-12-13 21:46:49.751000: I runner.py:310] Step = 61700 ; steps/s = 1.63, tokens/s = 82385 (37886 source, 44499 target) ; Learning rate = 0.000356 ; Loss = 1.805406\n",
      "2024-12-13 21:47:51.141000: I runner.py:310] Step = 61800 ; steps/s = 1.63, tokens/s = 82319 (37859 source, 44460 target) ; Learning rate = 0.000356 ; Loss = 1.805776\n",
      "2024-12-13 21:48:52.530000: I runner.py:310] Step = 61900 ; steps/s = 1.63, tokens/s = 82313 (37864 source, 44449 target) ; Learning rate = 0.000355 ; Loss = 1.802510\n",
      "2024-12-13 21:49:53.386000: I runner.py:310] Step = 62000 ; steps/s = 1.64, tokens/s = 81693 (37587 source, 44106 target) ; Learning rate = 0.000355 ; Loss = 1.776100\n",
      "2024-12-13 21:50:54.770000: I runner.py:310] Step = 62100 ; steps/s = 1.63, tokens/s = 82334 (37872 source, 44462 target) ; Learning rate = 0.000355 ; Loss = 1.807572\n",
      "2024-12-13 21:51:56.077000: I runner.py:310] Step = 62200 ; steps/s = 1.63, tokens/s = 82423 (37904 source, 44519 target) ; Learning rate = 0.000354 ; Loss = 1.811582\n",
      "2024-12-13 21:52:57.117000: I runner.py:310] Step = 62300 ; steps/s = 1.64, tokens/s = 81418 (37437 source, 43981 target) ; Learning rate = 0.000354 ; Loss = 1.813058\n",
      "2024-12-13 21:53:58.373000: I runner.py:310] Step = 62400 ; steps/s = 1.63, tokens/s = 82503 (37965 source, 44538 target) ; Learning rate = 0.000354 ; Loss = 1.800499\n",
      "2024-12-13 21:54:59.737000: I runner.py:310] Step = 62500 ; steps/s = 1.63, tokens/s = 82328 (37848 source, 44480 target) ; Learning rate = 0.000354 ; Loss = 1.792934\n",
      "2024-12-13 21:56:01.022000: I runner.py:310] Step = 62600 ; steps/s = 1.63, tokens/s = 82493 (37966 source, 44527 target) ; Learning rate = 0.000353 ; Loss = 1.792384\n",
      "2024-12-13 21:57:02.057000: I runner.py:310] Step = 62700 ; steps/s = 1.64, tokens/s = 81395 (37400 source, 43995 target) ; Learning rate = 0.000353 ; Loss = 1.837480\n",
      "2024-12-13 21:58:03.351000: I runner.py:310] Step = 62800 ; steps/s = 1.63, tokens/s = 82477 (37941 source, 44536 target) ; Learning rate = 0.000353 ; Loss = 1.799681\n",
      "2024-12-13 21:59:04.688000: I runner.py:310] Step = 62900 ; steps/s = 1.63, tokens/s = 82363 (37885 source, 44478 target) ; Learning rate = 0.000352 ; Loss = 1.801622\n",
      "2024-12-13 22:00:05.691000: I runner.py:310] Step = 63000 ; steps/s = 1.64, tokens/s = 81483 (37482 source, 44001 target) ; Learning rate = 0.000352 ; Loss = 1.804771\n",
      "2024-12-13 22:01:07.022000: I runner.py:310] Step = 63100 ; steps/s = 1.63, tokens/s = 82392 (37896 source, 44496 target) ; Learning rate = 0.000352 ; Loss = 1.809341\n",
      "2024-12-13 22:02:08.370000: I runner.py:310] Step = 63200 ; steps/s = 1.63, tokens/s = 82388 (37898 source, 44490 target) ; Learning rate = 0.000352 ; Loss = 1.802711\n",
      "2024-12-13 22:03:09.637000: I runner.py:310] Step = 63300 ; steps/s = 1.63, tokens/s = 82483 (37934 source, 44549 target) ; Learning rate = 0.000351 ; Loss = 1.793254\n",
      "2024-12-13 22:04:10.575000: I runner.py:310] Step = 63400 ; steps/s = 1.64, tokens/s = 81552 (37507 source, 44045 target) ; Learning rate = 0.000351 ; Loss = 1.832401\n",
      "2024-12-13 22:05:11.846000: I runner.py:310] Step = 63500 ; steps/s = 1.63, tokens/s = 82447 (37909 source, 44538 target) ; Learning rate = 0.000351 ; Loss = 1.792748\n",
      "2024-12-13 22:06:13.249000: I runner.py:310] Step = 63600 ; steps/s = 1.63, tokens/s = 82312 (37867 source, 44445 target) ; Learning rate = 0.000350 ; Loss = 1.790769\n",
      "2024-12-13 22:07:14.146000: I runner.py:310] Step = 63700 ; steps/s = 1.64, tokens/s = 81630 (37543 source, 44087 target) ; Learning rate = 0.000350 ; Loss = 1.811203\n",
      "2024-12-13 22:08:15.452000: I runner.py:310] Step = 63800 ; steps/s = 1.63, tokens/s = 82414 (37887 source, 44527 target) ; Learning rate = 0.000350 ; Loss = 1.794448\n",
      "2024-12-13 22:09:16.813000: I runner.py:310] Step = 63900 ; steps/s = 1.63, tokens/s = 82358 (37891 source, 44467 target) ; Learning rate = 0.000350 ; Loss = 1.801398\n",
      "2024-12-13 22:10:18.103000: I runner.py:310] Step = 64000 ; steps/s = 1.63, tokens/s = 82477 (37948 source, 44529 target) ; Learning rate = 0.000349 ; Loss = 1.816230\n",
      "2024-12-13 22:11:19.057000: I runner.py:310] Step = 64100 ; steps/s = 1.64, tokens/s = 81545 (37506 source, 44039 target) ; Learning rate = 0.000349 ; Loss = 1.773212\n",
      "2024-12-13 22:12:20.369000: I runner.py:310] Step = 64200 ; steps/s = 1.63, tokens/s = 82423 (37913 source, 44510 target) ; Learning rate = 0.000349 ; Loss = 1.799771\n",
      "2024-12-13 22:13:21.747000: I runner.py:310] Step = 64300 ; steps/s = 1.63, tokens/s = 82307 (37834 source, 44473 target) ; Learning rate = 0.000349 ; Loss = 1.809915\n",
      "2024-12-13 22:14:22.662000: I runner.py:310] Step = 64400 ; steps/s = 1.64, tokens/s = 81598 (37538 source, 44060 target) ; Learning rate = 0.000348 ; Loss = 1.812683\n",
      "2024-12-13 22:15:24.014000: I runner.py:310] Step = 64500 ; steps/s = 1.63, tokens/s = 82350 (37866 source, 44484 target) ; Learning rate = 0.000348 ; Loss = 1.800099\n",
      "2024-12-13 22:16:25.352000: I runner.py:310] Step = 64600 ; steps/s = 1.63, tokens/s = 82390 (37892 source, 44498 target) ; Learning rate = 0.000348 ; Loss = 1.785771\n",
      "2024-12-13 22:17:26.695000: I runner.py:310] Step = 64700 ; steps/s = 1.63, tokens/s = 82409 (37923 source, 44486 target) ; Learning rate = 0.000347 ; Loss = 1.782965\n",
      "2024-12-13 22:18:27.585000: I runner.py:310] Step = 64800 ; steps/s = 1.64, tokens/s = 81624 (37537 source, 44087 target) ; Learning rate = 0.000347 ; Loss = 1.827714\n",
      "2024-12-13 22:19:28.882000: I runner.py:310] Step = 64900 ; steps/s = 1.63, tokens/s = 82431 (37893 source, 44538 target) ; Learning rate = 0.000347 ; Loss = 1.797851\n",
      "2024-12-13 22:20:30.323000: I runner.py:310] Step = 65000 ; steps/s = 1.63, tokens/s = 82255 (37838 source, 44417 target) ; Learning rate = 0.000347 ; Loss = 1.799170\n",
      "2024-12-13 22:20:30.326000: I training.py:192] Running evaluation for step 65000\n",
      "2024-12-13 22:22:33.158000: I training.py:192] Evaluation result for step 65000: loss = 2.480661 ; perplexity = 11.949156\n",
      "2024-12-13 22:23:33.884000: I runner.py:310] Step = 65100 ; steps/s = 1.65, tokens/s = 81869 (37672 source, 44197 target) ; Learning rate = 0.000346 ; Loss = 1.809226\n",
      "2024-12-13 22:24:35.204000: I runner.py:310] Step = 65200 ; steps/s = 1.63, tokens/s = 82416 (37914 source, 44502 target) ; Learning rate = 0.000346 ; Loss = 1.787342\n",
      "2024-12-13 22:25:36.599000: I runner.py:310] Step = 65300 ; steps/s = 1.63, tokens/s = 82317 (37854 source, 44463 target) ; Learning rate = 0.000346 ; Loss = 1.788368\n",
      "2024-12-13 22:26:37.974000: I runner.py:310] Step = 65400 ; steps/s = 1.63, tokens/s = 82347 (37879 source, 44468 target) ; Learning rate = 0.000346 ; Loss = 1.804970\n",
      "2024-12-13 22:27:38.965000: I runner.py:310] Step = 65500 ; steps/s = 1.64, tokens/s = 81478 (37470 source, 44008 target) ; Learning rate = 0.000345 ; Loss = 1.784315\n",
      "2024-12-13 22:28:40.259000: I runner.py:310] Step = 65600 ; steps/s = 1.63, tokens/s = 82435 (37932 source, 44503 target) ; Learning rate = 0.000345 ; Loss = 1.808869\n",
      "2024-12-13 22:29:41.631000: I runner.py:310] Step = 65700 ; steps/s = 1.63, tokens/s = 82339 (37849 source, 44490 target) ; Learning rate = 0.000345 ; Loss = 1.790709\n",
      "2024-12-13 22:30:42.600000: I runner.py:310] Step = 65800 ; steps/s = 1.64, tokens/s = 81529 (37506 source, 44023 target) ; Learning rate = 0.000345 ; Loss = 1.775751\n",
      "2024-12-13 22:31:43.954000: I runner.py:310] Step = 65900 ; steps/s = 1.63, tokens/s = 82369 (37873 source, 44496 target) ; Learning rate = 0.000344 ; Loss = 1.814468\n",
      "2024-12-13 22:32:45.304000: I runner.py:310] Step = 66000 ; steps/s = 1.63, tokens/s = 82356 (37871 source, 44485 target) ; Learning rate = 0.000344 ; Loss = 1.804004\n",
      "2024-12-13 22:33:46.640000: I runner.py:310] Step = 66100 ; steps/s = 1.63, tokens/s = 82406 (37913 source, 44493 target) ; Learning rate = 0.000344 ; Loss = 1.803531\n",
      "2024-12-13 22:34:47.532000: I runner.py:310] Step = 66200 ; steps/s = 1.64, tokens/s = 81613 (37531 source, 44082 target) ; Learning rate = 0.000344 ; Loss = 1.810093\n",
      "2024-12-13 22:35:48.870000: I runner.py:310] Step = 66300 ; steps/s = 1.63, tokens/s = 82411 (37908 source, 44503 target) ; Learning rate = 0.000343 ; Loss = 1.790468\n",
      "2024-12-13 22:36:50.336000: I runner.py:310] Step = 66400 ; steps/s = 1.63, tokens/s = 82189 (37799 source, 44390 target) ; Learning rate = 0.000343 ; Loss = 1.790978\n",
      "2024-12-13 22:37:51.288000: I runner.py:310] Step = 66500 ; steps/s = 1.64, tokens/s = 81567 (37523 source, 44044 target) ; Learning rate = 0.000343 ; Loss = 1.787444\n",
      "2024-12-13 22:38:52.704000: I runner.py:310] Step = 66600 ; steps/s = 1.63, tokens/s = 82283 (37856 source, 44427 target) ; Learning rate = 0.000342 ; Loss = 1.797297\n",
      "2024-12-13 22:39:53.995000: I runner.py:310] Step = 66700 ; steps/s = 1.63, tokens/s = 82470 (37932 source, 44538 target) ; Learning rate = 0.000342 ; Loss = 1.800017\n",
      "2024-12-13 22:40:55.239000: I runner.py:310] Step = 66800 ; steps/s = 1.63, tokens/s = 81995 (37701 source, 44294 target) ; Learning rate = 0.000342 ; Loss = 1.775219\n",
      "2024-12-13 22:41:56.322000: I runner.py:310] Step = 66900 ; steps/s = 1.64, tokens/s = 81865 (37657 source, 44208 target) ; Learning rate = 0.000342 ; Loss = 1.795697\n",
      "2024-12-13 22:42:57.685000: I runner.py:310] Step = 67000 ; steps/s = 1.63, tokens/s = 82392 (37905 source, 44487 target) ; Learning rate = 0.000341 ; Loss = 1.790226\n",
      "2024-12-13 22:43:59.047000: I runner.py:310] Step = 67100 ; steps/s = 1.63, tokens/s = 82338 (37855 source, 44483 target) ; Learning rate = 0.000341 ; Loss = 1.788890\n",
      "2024-12-13 22:45:00.050000: I runner.py:310] Step = 67200 ; steps/s = 1.64, tokens/s = 81457 (37467 source, 43990 target) ; Learning rate = 0.000341 ; Loss = 1.830372\n",
      "2024-12-13 22:46:01.417000: I runner.py:310] Step = 67300 ; steps/s = 1.63, tokens/s = 82315 (37842 source, 44473 target) ; Learning rate = 0.000341 ; Loss = 1.775432\n",
      "2024-12-13 22:47:02.731000: I runner.py:310] Step = 67400 ; steps/s = 1.63, tokens/s = 82459 (37959 source, 44500 target) ; Learning rate = 0.000340 ; Loss = 1.781864\n",
      "2024-12-13 22:48:03.737000: I runner.py:310] Step = 67500 ; steps/s = 1.64, tokens/s = 81464 (37452 source, 44012 target) ; Learning rate = 0.000340 ; Loss = 1.787561\n",
      "2024-12-13 22:49:05.056000: I runner.py:310] Step = 67600 ; steps/s = 1.63, tokens/s = 82434 (37915 source, 44519 target) ; Learning rate = 0.000340 ; Loss = 1.781057\n",
      "2024-12-13 22:50:06.380000: I runner.py:310] Step = 67700 ; steps/s = 1.63, tokens/s = 82405 (37896 source, 44509 target) ; Learning rate = 0.000340 ; Loss = 1.799791\n",
      "2024-12-13 22:51:07.691000: I runner.py:310] Step = 67800 ; steps/s = 1.63, tokens/s = 82402 (37904 source, 44498 target) ; Learning rate = 0.000339 ; Loss = 1.791743\n",
      "2024-12-13 22:52:08.710000: I runner.py:310] Step = 67900 ; steps/s = 1.64, tokens/s = 81425 (37438 source, 43987 target) ; Learning rate = 0.000339 ; Loss = 1.807793\n",
      "2024-12-13 22:53:10.132000: I runner.py:310] Step = 68000 ; steps/s = 1.63, tokens/s = 82261 (37842 source, 44419 target) ; Learning rate = 0.000339 ; Loss = 1.778328\n",
      "2024-12-13 22:54:11.525000: I runner.py:310] Step = 68100 ; steps/s = 1.63, tokens/s = 82340 (37880 source, 44460 target) ; Learning rate = 0.000339 ; Loss = 1.790578\n",
      "2024-12-13 22:55:12.523000: I runner.py:310] Step = 68200 ; steps/s = 1.64, tokens/s = 81489 (37469 source, 44020 target) ; Learning rate = 0.000338 ; Loss = 1.778430\n",
      "2024-12-13 22:56:13.824000: I runner.py:310] Step = 68300 ; steps/s = 1.63, tokens/s = 82435 (37920 source, 44515 target) ; Learning rate = 0.000338 ; Loss = 1.779533\n",
      "2024-12-13 22:57:15.197000: I runner.py:310] Step = 68400 ; steps/s = 1.63, tokens/s = 82348 (37874 source, 44474 target) ; Learning rate = 0.000338 ; Loss = 1.814629\n",
      "2024-12-13 22:58:16.531000: I runner.py:310] Step = 68500 ; steps/s = 1.63, tokens/s = 82384 (37888 source, 44496 target) ; Learning rate = 0.000338 ; Loss = 1.798970\n",
      "2024-12-13 22:59:17.609000: I runner.py:310] Step = 68600 ; steps/s = 1.64, tokens/s = 81372 (37434 source, 43938 target) ; Learning rate = 0.000337 ; Loss = 1.801254\n",
      "2024-12-13 23:00:18.990000: I runner.py:310] Step = 68700 ; steps/s = 1.63, tokens/s = 82332 (37872 source, 44460 target) ; Learning rate = 0.000337 ; Loss = 1.773321\n",
      "2024-12-13 23:01:20.268000: I runner.py:310] Step = 68800 ; steps/s = 1.63, tokens/s = 82459 (37908 source, 44551 target) ; Learning rate = 0.000337 ; Loss = 1.785404\n",
      "2024-12-13 23:02:21.231000: I runner.py:310] Step = 68900 ; steps/s = 1.64, tokens/s = 81556 (37519 source, 44037 target) ; Learning rate = 0.000337 ; Loss = 1.797130\n",
      "2024-12-13 23:03:22.568000: I runner.py:310] Step = 69000 ; steps/s = 1.63, tokens/s = 82356 (37868 source, 44488 target) ; Learning rate = 0.000336 ; Loss = 1.806911\n",
      "2024-12-13 23:04:23.902000: I runner.py:310] Step = 69100 ; steps/s = 1.63, tokens/s = 82411 (37911 source, 44500 target) ; Learning rate = 0.000336 ; Loss = 1.786024\n",
      "2024-12-13 23:05:25.164000: I runner.py:310] Step = 69200 ; steps/s = 1.63, tokens/s = 82488 (37944 source, 44544 target) ; Learning rate = 0.000336 ; Loss = 1.777655\n",
      "2024-12-13 23:06:26.173000: I runner.py:310] Step = 69300 ; steps/s = 1.64, tokens/s = 81481 (37479 source, 44002 target) ; Learning rate = 0.000336 ; Loss = 1.777024\n",
      "2024-12-13 23:07:27.545000: I runner.py:310] Step = 69400 ; steps/s = 1.63, tokens/s = 82342 (37860 source, 44482 target) ; Learning rate = 0.000336 ; Loss = 1.779503\n",
      "2024-12-13 23:08:28.923000: I runner.py:310] Step = 69500 ; steps/s = 1.63, tokens/s = 82301 (37848 source, 44453 target) ; Learning rate = 0.000335 ; Loss = 1.802909\n",
      "2024-12-13 23:09:29.856000: I runner.py:310] Step = 69600 ; steps/s = 1.64, tokens/s = 81582 (37527 source, 44055 target) ; Learning rate = 0.000335 ; Loss = 1.784173\n",
      "2024-12-13 23:10:31.168000: I runner.py:310] Step = 69700 ; steps/s = 1.63, tokens/s = 82424 (37912 source, 44512 target) ; Learning rate = 0.000335 ; Loss = 1.787635\n",
      "2024-12-13 23:11:32.600000: I runner.py:310] Step = 69800 ; steps/s = 1.63, tokens/s = 82260 (37831 source, 44429 target) ; Learning rate = 0.000335 ; Loss = 1.790537\n",
      "2024-12-13 23:12:33.927000: I runner.py:310] Step = 69900 ; steps/s = 1.63, tokens/s = 82400 (37902 source, 44498 target) ; Learning rate = 0.000334 ; Loss = 1.799852\n",
      "2024-12-13 23:13:34.864000: I runner.py:310] Step = 70000 ; steps/s = 1.64, tokens/s = 81595 (37543 source, 44052 target) ; Learning rate = 0.000334 ; Loss = 1.748172\n",
      "2024-12-13 23:13:36.742000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-70000\n",
      "2024-12-13 23:13:36.742000: I training.py:192] Running evaluation for step 70000\n",
      "2024-12-13 23:15:40.774000: I training.py:192] Evaluation result for step 70000: loss = 2.501964 ; perplexity = 12.206446\n",
      "2024-12-13 23:16:41.898000: I runner.py:310] Step = 70100 ; steps/s = 1.64, tokens/s = 82684 (38017 source, 44667 target) ; Learning rate = 0.000334 ; Loss = 1.797554\n",
      "2024-12-13 23:17:43.271000: I runner.py:310] Step = 70200 ; steps/s = 1.63, tokens/s = 82338 (37862 source, 44476 target) ; Learning rate = 0.000334 ; Loss = 1.791438\n",
      "2024-12-13 23:18:44.238000: I runner.py:310] Step = 70300 ; steps/s = 1.64, tokens/s = 81522 (37507 source, 44015 target) ; Learning rate = 0.000333 ; Loss = 1.786340\n",
      "2024-12-13 23:19:45.505000: I runner.py:310] Step = 70400 ; steps/s = 1.63, tokens/s = 82462 (37907 source, 44555 target) ; Learning rate = 0.000333 ; Loss = 1.794080\n",
      "2024-12-13 23:20:46.890000: I runner.py:310] Step = 70500 ; steps/s = 1.63, tokens/s = 82318 (37858 source, 44460 target) ; Learning rate = 0.000333 ; Loss = 1.772263\n",
      "2024-12-13 23:21:48.261000: I runner.py:310] Step = 70600 ; steps/s = 1.63, tokens/s = 82359 (37897 source, 44462 target) ; Learning rate = 0.000333 ; Loss = 1.793795\n",
      "2024-12-13 23:22:49.259000: I runner.py:310] Step = 70700 ; steps/s = 1.64, tokens/s = 81487 (37488 source, 43999 target) ; Learning rate = 0.000332 ; Loss = 1.817316\n",
      "2024-12-13 23:23:50.535000: I runner.py:310] Step = 70800 ; steps/s = 1.63, tokens/s = 82461 (37927 source, 44534 target) ; Learning rate = 0.000332 ; Loss = 1.763579\n",
      "2024-12-13 23:24:51.890000: I runner.py:310] Step = 70900 ; steps/s = 1.63, tokens/s = 82376 (37887 source, 44489 target) ; Learning rate = 0.000332 ; Loss = 1.780633\n",
      "2024-12-13 23:25:52.767000: I runner.py:310] Step = 71000 ; steps/s = 1.64, tokens/s = 81638 (37541 source, 44097 target) ; Learning rate = 0.000332 ; Loss = 1.804345\n",
      "2024-12-13 23:26:54.124000: I runner.py:310] Step = 71100 ; steps/s = 1.63, tokens/s = 82369 (37881 source, 44488 target) ; Learning rate = 0.000331 ; Loss = 1.783208\n",
      "2024-12-13 23:27:55.474000: I runner.py:310] Step = 71200 ; steps/s = 1.63, tokens/s = 82374 (37895 source, 44479 target) ; Learning rate = 0.000331 ; Loss = 1.780182\n",
      "2024-12-13 23:28:56.832000: I runner.py:310] Step = 71300 ; steps/s = 1.63, tokens/s = 82359 (37880 source, 44479 target) ; Learning rate = 0.000331 ; Loss = 1.773499\n",
      "2024-12-13 23:29:57.857000: I runner.py:310] Step = 71400 ; steps/s = 1.64, tokens/s = 81410 (37434 source, 43976 target) ; Learning rate = 0.000331 ; Loss = 1.807024\n",
      "2024-12-13 23:30:59.228000: I runner.py:310] Step = 71500 ; steps/s = 1.63, tokens/s = 82364 (37908 source, 44456 target) ; Learning rate = 0.000331 ; Loss = 1.778810\n",
      "2024-12-13 23:32:00.620000: I runner.py:310] Step = 71600 ; steps/s = 1.63, tokens/s = 82334 (37857 source, 44477 target) ; Learning rate = 0.000330 ; Loss = 1.777974\n",
      "2024-12-13 23:33:01.566000: I runner.py:310] Step = 71700 ; steps/s = 1.64, tokens/s = 81539 (37486 source, 44053 target) ; Learning rate = 0.000330 ; Loss = 1.773996\n",
      "2024-12-13 23:34:02.923000: I runner.py:310] Step = 71800 ; steps/s = 1.63, tokens/s = 82384 (37910 source, 44474 target) ; Learning rate = 0.000330 ; Loss = 1.790821\n",
      "2024-12-13 23:35:04.285000: I runner.py:310] Step = 71900 ; steps/s = 1.63, tokens/s = 82370 (37888 source, 44482 target) ; Learning rate = 0.000330 ; Loss = 1.793902\n",
      "2024-12-13 23:36:05.676000: I runner.py:310] Step = 72000 ; steps/s = 1.63, tokens/s = 82290 (37842 source, 44448 target) ; Learning rate = 0.000329 ; Loss = 1.799350\n",
      "2024-12-13 23:37:06.631000: I runner.py:310] Step = 72100 ; steps/s = 1.64, tokens/s = 81528 (37494 source, 44034 target) ; Learning rate = 0.000329 ; Loss = 1.755654\n",
      "2024-12-13 23:38:07.931000: I runner.py:310] Step = 72200 ; steps/s = 1.63, tokens/s = 82446 (37915 source, 44531 target) ; Learning rate = 0.000329 ; Loss = 1.796896\n",
      "2024-12-13 23:39:09.291000: I runner.py:310] Step = 72300 ; steps/s = 1.63, tokens/s = 82364 (37878 source, 44486 target) ; Learning rate = 0.000329 ; Loss = 1.796073\n",
      "2024-12-13 23:40:10.198000: I runner.py:310] Step = 72400 ; steps/s = 1.64, tokens/s = 81596 (37538 source, 44058 target) ; Learning rate = 0.000328 ; Loss = 1.782846\n",
      "2024-12-13 23:41:11.540000: I runner.py:310] Step = 72500 ; steps/s = 1.63, tokens/s = 82397 (37904 source, 44493 target) ; Learning rate = 0.000328 ; Loss = 1.776991\n",
      "2024-12-13 23:42:12.857000: I runner.py:310] Step = 72600 ; steps/s = 1.63, tokens/s = 82408 (37888 source, 44520 target) ; Learning rate = 0.000328 ; Loss = 1.771508\n",
      "2024-12-13 23:43:14.258000: I runner.py:310] Step = 72700 ; steps/s = 1.63, tokens/s = 82302 (37857 source, 44445 target) ; Learning rate = 0.000328 ; Loss = 1.788513\n",
      "2024-12-13 23:44:15.164000: I runner.py:310] Step = 72800 ; steps/s = 1.64, tokens/s = 81591 (37531 source, 44060 target) ; Learning rate = 0.000328 ; Loss = 1.790651\n",
      "2024-12-13 23:45:16.569000: I runner.py:310] Step = 72900 ; steps/s = 1.63, tokens/s = 82325 (37868 source, 44457 target) ; Learning rate = 0.000327 ; Loss = 1.766877\n",
      "2024-12-13 23:46:17.803000: I runner.py:310] Step = 73000 ; steps/s = 1.63, tokens/s = 82496 (37936 source, 44560 target) ; Learning rate = 0.000327 ; Loss = 1.775448\n",
      "2024-12-13 23:47:18.714000: I runner.py:310] Step = 73100 ; steps/s = 1.64, tokens/s = 81606 (37542 source, 44064 target) ; Learning rate = 0.000327 ; Loss = 1.765246\n",
      "2024-12-13 23:48:20.070000: I runner.py:310] Step = 73200 ; steps/s = 1.63, tokens/s = 82386 (37896 source, 44490 target) ; Learning rate = 0.000327 ; Loss = 1.795415\n",
      "2024-12-13 23:49:21.414000: I runner.py:310] Step = 73300 ; steps/s = 1.63, tokens/s = 82377 (37881 source, 44496 target) ; Learning rate = 0.000326 ; Loss = 1.785838\n",
      "2024-12-13 23:50:22.831000: I runner.py:310] Step = 73400 ; steps/s = 1.63, tokens/s = 82275 (37845 source, 44430 target) ; Learning rate = 0.000326 ; Loss = 1.790595\n",
      "2024-12-13 23:51:23.680000: I runner.py:310] Step = 73500 ; steps/s = 1.64, tokens/s = 81653 (37543 source, 44110 target) ; Learning rate = 0.000326 ; Loss = 1.804195\n",
      "2024-12-13 23:52:25.053000: I runner.py:310] Step = 73600 ; steps/s = 1.63, tokens/s = 82316 (37851 source, 44465 target) ; Learning rate = 0.000326 ; Loss = 1.767238\n",
      "2024-12-13 23:53:26.435000: I runner.py:310] Step = 73700 ; steps/s = 1.63, tokens/s = 82352 (37883 source, 44469 target) ; Learning rate = 0.000326 ; Loss = 1.778864\n",
      "2024-12-13 23:54:27.420000: I runner.py:310] Step = 73800 ; steps/s = 1.64, tokens/s = 81520 (37504 source, 44016 target) ; Learning rate = 0.000325 ; Loss = 1.797453\n",
      "2024-12-13 23:55:28.733000: I runner.py:310] Step = 73900 ; steps/s = 1.63, tokens/s = 82414 (37907 source, 44507 target) ; Learning rate = 0.000325 ; Loss = 1.769075\n",
      "2024-12-13 23:56:30.028000: I runner.py:310] Step = 74000 ; steps/s = 1.63, tokens/s = 82459 (37936 source, 44523 target) ; Learning rate = 0.000325 ; Loss = 1.775982\n",
      "2024-12-13 23:57:31.404000: I runner.py:310] Step = 74100 ; steps/s = 1.63, tokens/s = 82343 (37869 source, 44474 target) ; Learning rate = 0.000325 ; Loss = 1.786257\n",
      "2024-12-13 23:58:32.342000: I runner.py:310] Step = 74200 ; steps/s = 1.64, tokens/s = 81541 (37490 source, 44051 target) ; Learning rate = 0.000324 ; Loss = 1.758618\n",
      "2024-12-13 23:59:33.755000: I runner.py:310] Step = 74300 ; steps/s = 1.63, tokens/s = 82304 (37876 source, 44428 target) ; Learning rate = 0.000324 ; Loss = 1.788790\n",
      "2024-12-14 00:00:35.088000: I runner.py:310] Step = 74400 ; steps/s = 1.63, tokens/s = 82395 (37888 source, 44507 target) ; Learning rate = 0.000324 ; Loss = 1.793695\n",
      "2024-12-14 00:01:36.050000: I runner.py:310] Step = 74500 ; steps/s = 1.64, tokens/s = 81520 (37488 source, 44032 target) ; Learning rate = 0.000324 ; Loss = 1.751209\n",
      "2024-12-14 00:02:37.374000: I runner.py:310] Step = 74600 ; steps/s = 1.63, tokens/s = 82432 (37921 source, 44511 target) ; Learning rate = 0.000324 ; Loss = 1.782988\n",
      "2024-12-14 00:03:38.746000: I runner.py:310] Step = 74700 ; steps/s = 1.63, tokens/s = 82315 (37839 source, 44476 target) ; Learning rate = 0.000323 ; Loss = 1.790173\n",
      "2024-12-14 00:04:40.094000: I runner.py:310] Step = 74800 ; steps/s = 1.63, tokens/s = 82373 (37896 source, 44477 target) ; Learning rate = 0.000323 ; Loss = 1.796994\n",
      "2024-12-14 00:05:40.993000: I runner.py:310] Step = 74900 ; steps/s = 1.64, tokens/s = 81612 (37538 source, 44074 target) ; Learning rate = 0.000323 ; Loss = 1.762567\n",
      "2024-12-14 00:06:42.397000: I runner.py:310] Step = 75000 ; steps/s = 1.63, tokens/s = 82312 (37878 source, 44434 target) ; Learning rate = 0.000323 ; Loss = 1.778847\n",
      "2024-12-14 00:06:42.399000: I training.py:192] Running evaluation for step 75000\n",
      "2024-12-14 00:08:49.438000: I training.py:192] Evaluation result for step 75000: loss = 2.505390 ; perplexity = 12.248335\n",
      "2024-12-14 00:09:50.565000: I runner.py:310] Step = 75100 ; steps/s = 1.64, tokens/s = 82682 (38016 source, 44666 target) ; Learning rate = 0.000323 ; Loss = 1.795316\n",
      "2024-12-14 00:10:51.556000: I runner.py:310] Step = 75200 ; steps/s = 1.64, tokens/s = 81501 (37487 source, 44014 target) ; Learning rate = 0.000322 ; Loss = 1.774526\n",
      "2024-12-14 00:11:52.924000: I runner.py:310] Step = 75300 ; steps/s = 1.63, tokens/s = 82345 (37872 source, 44473 target) ; Learning rate = 0.000322 ; Loss = 1.792284\n",
      "2024-12-14 00:12:54.293000: I runner.py:310] Step = 75400 ; steps/s = 1.63, tokens/s = 82339 (37864 source, 44475 target) ; Learning rate = 0.000322 ; Loss = 1.777933\n",
      "2024-12-14 00:13:55.269000: I runner.py:310] Step = 75500 ; steps/s = 1.64, tokens/s = 81673 (37562 source, 44111 target) ; Learning rate = 0.000322 ; Loss = 1.819405\n",
      "2024-12-14 00:14:56.556000: I runner.py:310] Step = 75600 ; steps/s = 1.63, tokens/s = 82308 (37866 source, 44442 target) ; Learning rate = 0.000321 ; Loss = 1.767600\n",
      "2024-12-14 00:15:58.017000: I runner.py:310] Step = 75700 ; steps/s = 1.63, tokens/s = 82215 (37812 source, 44403 target) ; Learning rate = 0.000321 ; Loss = 1.799979\n",
      "2024-12-14 00:16:59.355000: I runner.py:310] Step = 75800 ; steps/s = 1.63, tokens/s = 82368 (37875 source, 44493 target) ; Learning rate = 0.000321 ; Loss = 1.784296\n",
      "2024-12-14 00:18:00.351000: I runner.py:310] Step = 75900 ; steps/s = 1.64, tokens/s = 81482 (37472 source, 44010 target) ; Learning rate = 0.000321 ; Loss = 1.787830\n",
      "2024-12-14 00:19:01.705000: I runner.py:310] Step = 76000 ; steps/s = 1.63, tokens/s = 82382 (37890 source, 44492 target) ; Learning rate = 0.000321 ; Loss = 1.765121\n",
      "2024-12-14 00:20:03.067000: I runner.py:310] Step = 76100 ; steps/s = 1.63, tokens/s = 82351 (37878 source, 44473 target) ; Learning rate = 0.000320 ; Loss = 1.773946\n",
      "2024-12-14 00:21:03.991000: I runner.py:310] Step = 76200 ; steps/s = 1.64, tokens/s = 81575 (37522 source, 44053 target) ; Learning rate = 0.000320 ; Loss = 1.778325\n",
      "2024-12-14 00:22:05.402000: I runner.py:310] Step = 76300 ; steps/s = 1.63, tokens/s = 82298 (37850 source, 44448 target) ; Learning rate = 0.000320 ; Loss = 1.777629\n",
      "2024-12-14 00:23:06.770000: I runner.py:310] Step = 76400 ; steps/s = 1.63, tokens/s = 82342 (37872 source, 44470 target) ; Learning rate = 0.000320 ; Loss = 1.794446\n",
      "2024-12-14 00:24:08.152000: I runner.py:310] Step = 76500 ; steps/s = 1.63, tokens/s = 82328 (37871 source, 44457 target) ; Learning rate = 0.000320 ; Loss = 1.810584\n",
      "2024-12-14 00:25:09.099000: I runner.py:310] Step = 76600 ; steps/s = 1.64, tokens/s = 81552 (37508 source, 44044 target) ; Learning rate = 0.000319 ; Loss = 1.803958\n",
      "2024-12-14 00:26:10.384000: I runner.py:310] Step = 76700 ; steps/s = 1.63, tokens/s = 82466 (37925 source, 44541 target) ; Learning rate = 0.000319 ; Loss = 1.773353\n",
      "2024-12-14 00:27:11.799000: I runner.py:310] Step = 76800 ; steps/s = 1.63, tokens/s = 82269 (37834 source, 44435 target) ; Learning rate = 0.000319 ; Loss = 1.767520\n",
      "2024-12-14 00:28:12.744000: I runner.py:310] Step = 76900 ; steps/s = 1.64, tokens/s = 81558 (37518 source, 44040 target) ; Learning rate = 0.000319 ; Loss = 1.773810\n",
      "2024-12-14 00:29:14.132000: I runner.py:310] Step = 77000 ; steps/s = 1.63, tokens/s = 82310 (37858 source, 44452 target) ; Learning rate = 0.000319 ; Loss = 1.759881\n",
      "2024-12-14 00:30:15.456000: I runner.py:310] Step = 77100 ; steps/s = 1.63, tokens/s = 82432 (37920 source, 44512 target) ; Learning rate = 0.000318 ; Loss = 1.782257\n",
      "2024-12-14 00:31:16.844000: I runner.py:310] Step = 77200 ; steps/s = 1.63, tokens/s = 82283 (37829 source, 44454 target) ; Learning rate = 0.000318 ; Loss = 1.799176\n",
      "2024-12-14 00:32:17.745000: I runner.py:310] Step = 77300 ; steps/s = 1.64, tokens/s = 81625 (37560 source, 44065 target) ; Learning rate = 0.000318 ; Loss = 1.754028\n",
      "2024-12-14 00:33:19.073000: I runner.py:310] Step = 77400 ; steps/s = 1.63, tokens/s = 82429 (37903 source, 44526 target) ; Learning rate = 0.000318 ; Loss = 1.787828\n",
      "2024-12-14 00:34:20.481000: I runner.py:310] Step = 77500 ; steps/s = 1.63, tokens/s = 82293 (37856 source, 44437 target) ; Learning rate = 0.000317 ; Loss = 1.785005\n",
      "2024-12-14 00:35:21.494000: I runner.py:310] Step = 77600 ; steps/s = 1.64, tokens/s = 81436 (37443 source, 43993 target) ; Learning rate = 0.000317 ; Loss = 1.771713\n",
      "2024-12-14 00:36:22.889000: I runner.py:310] Step = 77700 ; steps/s = 1.63, tokens/s = 82285 (37824 source, 44461 target) ; Learning rate = 0.000317 ; Loss = 1.769439\n",
      "2024-12-14 00:37:24.251000: I runner.py:310] Step = 77800 ; steps/s = 1.63, tokens/s = 82356 (37890 source, 44466 target) ; Learning rate = 0.000317 ; Loss = 1.761250\n",
      "2024-12-14 00:38:25.629000: I runner.py:310] Step = 77900 ; steps/s = 1.63, tokens/s = 82354 (37876 source, 44478 target) ; Learning rate = 0.000317 ; Loss = 1.766664\n",
      "2024-12-14 00:39:26.507000: I runner.py:310] Step = 78000 ; steps/s = 1.64, tokens/s = 81672 (37595 source, 44077 target) ; Learning rate = 0.000316 ; Loss = 1.752099\n",
      "2024-12-14 00:40:27.880000: I runner.py:310] Step = 78100 ; steps/s = 1.63, tokens/s = 82331 (37872 source, 44459 target) ; Learning rate = 0.000316 ; Loss = 1.795632\n",
      "2024-12-14 00:41:29.146000: I runner.py:310] Step = 78200 ; steps/s = 1.63, tokens/s = 82489 (37931 source, 44558 target) ; Learning rate = 0.000316 ; Loss = 1.771372\n",
      "2024-12-14 00:42:30.016000: I runner.py:310] Step = 78300 ; steps/s = 1.64, tokens/s = 81638 (37538 source, 44100 target) ; Learning rate = 0.000316 ; Loss = 1.765775\n",
      "2024-12-14 00:43:31.373000: I runner.py:310] Step = 78400 ; steps/s = 1.63, tokens/s = 82354 (37879 source, 44475 target) ; Learning rate = 0.000316 ; Loss = 1.773038\n",
      "2024-12-14 00:44:32.770000: I runner.py:310] Step = 78500 ; steps/s = 1.63, tokens/s = 82282 (37832 source, 44450 target) ; Learning rate = 0.000315 ; Loss = 1.777650\n",
      "2024-12-14 00:45:34.135000: I runner.py:310] Step = 78600 ; steps/s = 1.63, tokens/s = 82376 (37896 source, 44480 target) ; Learning rate = 0.000315 ; Loss = 1.779981\n",
      "2024-12-14 00:46:35.057000: I runner.py:310] Step = 78700 ; steps/s = 1.64, tokens/s = 81600 (37537 source, 44063 target) ; Learning rate = 0.000315 ; Loss = 1.742933\n",
      "2024-12-14 00:47:36.424000: I runner.py:310] Step = 78800 ; steps/s = 1.63, tokens/s = 82350 (37886 source, 44464 target) ; Learning rate = 0.000315 ; Loss = 1.779383\n",
      "2024-12-14 00:48:37.782000: I runner.py:310] Step = 78900 ; steps/s = 1.63, tokens/s = 82344 (37854 source, 44490 target) ; Learning rate = 0.000315 ; Loss = 1.784727\n",
      "2024-12-14 00:49:38.803000: I runner.py:310] Step = 79000 ; steps/s = 1.64, tokens/s = 81453 (37463 source, 43990 target) ; Learning rate = 0.000314 ; Loss = 1.780697\n",
      "2024-12-14 00:50:40.149000: I runner.py:310] Step = 79100 ; steps/s = 1.63, tokens/s = 82352 (37870 source, 44482 target) ; Learning rate = 0.000314 ; Loss = 1.755821\n",
      "2024-12-14 00:51:41.416000: I runner.py:310] Step = 79200 ; steps/s = 1.63, tokens/s = 82490 (37947 source, 44543 target) ; Learning rate = 0.000314 ; Loss = 1.764318\n",
      "2024-12-14 00:52:42.849000: I runner.py:310] Step = 79300 ; steps/s = 1.63, tokens/s = 82274 (37841 source, 44433 target) ; Learning rate = 0.000314 ; Loss = 1.767174\n",
      "2024-12-14 00:53:43.775000: I runner.py:310] Step = 79400 ; steps/s = 1.64, tokens/s = 81559 (37502 source, 44057 target) ; Learning rate = 0.000314 ; Loss = 1.793573\n",
      "2024-12-14 00:54:45.179000: I runner.py:310] Step = 79500 ; steps/s = 1.63, tokens/s = 82308 (37849 source, 44459 target) ; Learning rate = 0.000313 ; Loss = 1.753070\n",
      "2024-12-14 00:55:46.484000: I runner.py:310] Step = 79600 ; steps/s = 1.63, tokens/s = 82453 (37945 source, 44508 target) ; Learning rate = 0.000313 ; Loss = 1.762787\n",
      "2024-12-14 00:56:47.463000: I runner.py:310] Step = 79700 ; steps/s = 1.64, tokens/s = 81500 (37478 source, 44022 target) ; Learning rate = 0.000313 ; Loss = 1.762532\n",
      "2024-12-14 00:57:48.799000: I runner.py:310] Step = 79800 ; steps/s = 1.63, tokens/s = 82402 (37899 source, 44503 target) ; Learning rate = 0.000313 ; Loss = 1.762053\n",
      "2024-12-14 00:58:50.184000: I runner.py:310] Step = 79900 ; steps/s = 1.63, tokens/s = 82341 (37885 source, 44456 target) ; Learning rate = 0.000313 ; Loss = 1.769917\n",
      "2024-12-14 00:59:51.573000: I runner.py:310] Step = 80000 ; steps/s = 1.63, tokens/s = 82306 (37851 source, 44455 target) ; Learning rate = 0.000312 ; Loss = 1.791904\n",
      "2024-12-14 00:59:53.546000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-80000\n",
      "2024-12-14 00:59:53.547000: I training.py:192] Running evaluation for step 80000\n",
      "2024-12-14 01:02:01.094000: I training.py:192] Evaluation result for step 80000: loss = 2.513790 ; perplexity = 12.351659\n",
      "2024-12-14 01:03:01.870000: I runner.py:310] Step = 80100 ; steps/s = 1.65, tokens/s = 81785 (37613 source, 44172 target) ; Learning rate = 0.000312 ; Loss = 1.765502\n",
      "2024-12-14 01:04:03.209000: I runner.py:310] Step = 80200 ; steps/s = 1.63, tokens/s = 82385 (37890 source, 44495 target) ; Learning rate = 0.000312 ; Loss = 1.765170\n",
      "2024-12-14 01:05:04.496000: I runner.py:310] Step = 80300 ; steps/s = 1.63, tokens/s = 82451 (37913 source, 44538 target) ; Learning rate = 0.000312 ; Loss = 1.784080\n",
      "2024-12-14 01:06:05.484000: I runner.py:310] Step = 80400 ; steps/s = 1.64, tokens/s = 81491 (37487 source, 44004 target) ; Learning rate = 0.000312 ; Loss = 1.765277\n",
      "2024-12-14 01:07:06.808000: I runner.py:310] Step = 80500 ; steps/s = 1.63, tokens/s = 82401 (37900 source, 44501 target) ; Learning rate = 0.000312 ; Loss = 1.774031\n",
      "2024-12-14 01:08:08.185000: I runner.py:310] Step = 80600 ; steps/s = 1.63, tokens/s = 82368 (37892 source, 44476 target) ; Learning rate = 0.000311 ; Loss = 1.770561\n",
      "2024-12-14 01:09:09.511000: I runner.py:310] Step = 80700 ; steps/s = 1.63, tokens/s = 82392 (37890 source, 44502 target) ; Learning rate = 0.000311 ; Loss = 1.795317\n",
      "2024-12-14 01:10:10.472000: I runner.py:310] Step = 80800 ; steps/s = 1.64, tokens/s = 81506 (37491 source, 44015 target) ; Learning rate = 0.000311 ; Loss = 1.775046\n",
      "2024-12-14 01:11:11.881000: I runner.py:310] Step = 80900 ; steps/s = 1.63, tokens/s = 82312 (37853 source, 44459 target) ; Learning rate = 0.000311 ; Loss = 1.749449\n",
      "2024-12-14 01:12:13.242000: I runner.py:310] Step = 81000 ; steps/s = 1.63, tokens/s = 82355 (37885 source, 44470 target) ; Learning rate = 0.000311 ; Loss = 1.757224\n",
      "2024-12-14 01:13:14.243000: I runner.py:310] Step = 81100 ; steps/s = 1.64, tokens/s = 81479 (37482 source, 43997 target) ; Learning rate = 0.000310 ; Loss = 1.750304\n",
      "2024-12-14 01:14:15.546000: I runner.py:310] Step = 81200 ; steps/s = 1.63, tokens/s = 82444 (37923 source, 44521 target) ; Learning rate = 0.000310 ; Loss = 1.770034\n",
      "2024-12-14 01:15:16.928000: I runner.py:310] Step = 81300 ; steps/s = 1.63, tokens/s = 82333 (37873 source, 44460 target) ; Learning rate = 0.000310 ; Loss = 1.789325\n",
      "2024-12-14 01:16:18.279000: I runner.py:310] Step = 81400 ; steps/s = 1.63, tokens/s = 82344 (37849 source, 44495 target) ; Learning rate = 0.000310 ; Loss = 1.783129\n",
      "2024-12-14 01:17:19.306000: I runner.py:310] Step = 81500 ; steps/s = 1.64, tokens/s = 81440 (37470 source, 43970 target) ; Learning rate = 0.000310 ; Loss = 1.749582\n",
      "2024-12-14 01:18:20.639000: I runner.py:310] Step = 81600 ; steps/s = 1.63, tokens/s = 82383 (37884 source, 44499 target) ; Learning rate = 0.000309 ; Loss = 1.773790\n",
      "2024-12-14 01:19:21.946000: I runner.py:310] Step = 81700 ; steps/s = 1.63, tokens/s = 82422 (37900 source, 44522 target) ; Learning rate = 0.000309 ; Loss = 1.780818\n",
      "2024-12-14 01:20:22.917000: I runner.py:310] Step = 81800 ; steps/s = 1.64, tokens/s = 81525 (37502 source, 44023 target) ; Learning rate = 0.000309 ; Loss = 1.781109\n",
      "2024-12-14 01:21:24.241000: I runner.py:310] Step = 81900 ; steps/s = 1.63, tokens/s = 82386 (37878 source, 44508 target) ; Learning rate = 0.000309 ; Loss = 1.756762\n",
      "2024-12-14 01:22:25.640000: I runner.py:310] Step = 82000 ; steps/s = 1.63, tokens/s = 82345 (37886 source, 44459 target) ; Learning rate = 0.000309 ; Loss = 1.766746\n",
      "2024-12-14 01:23:26.909000: I runner.py:310] Step = 82100 ; steps/s = 1.63, tokens/s = 82475 (37931 source, 44544 target) ; Learning rate = 0.000308 ; Loss = 1.768794\n",
      "2024-12-14 01:24:27.929000: I runner.py:310] Step = 82200 ; steps/s = 1.64, tokens/s = 81442 (37462 source, 43980 target) ; Learning rate = 0.000308 ; Loss = 1.749119\n",
      "2024-12-14 01:25:29.238000: I runner.py:310] Step = 82300 ; steps/s = 1.63, tokens/s = 82411 (37893 source, 44518 target) ; Learning rate = 0.000308 ; Loss = 1.789334\n",
      "2024-12-14 01:26:30.680000: I runner.py:310] Step = 82400 ; steps/s = 1.63, tokens/s = 82265 (37851 source, 44414 target) ; Learning rate = 0.000308 ; Loss = 1.789208\n",
      "2024-12-14 01:27:31.638000: I runner.py:310] Step = 82500 ; steps/s = 1.64, tokens/s = 81529 (37488 source, 44041 target) ; Learning rate = 0.000308 ; Loss = 1.758641\n",
      "2024-12-14 01:28:32.935000: I runner.py:310] Step = 82600 ; steps/s = 1.63, tokens/s = 82453 (37919 source, 44534 target) ; Learning rate = 0.000308 ; Loss = 1.766573\n",
      "2024-12-14 01:29:34.360000: I runner.py:310] Step = 82700 ; steps/s = 1.63, tokens/s = 82279 (37849 source, 44430 target) ; Learning rate = 0.000307 ; Loss = 1.770954\n",
      "2024-12-14 01:30:35.711000: I runner.py:310] Step = 82800 ; steps/s = 1.63, tokens/s = 82364 (37885 source, 44479 target) ; Learning rate = 0.000307 ; Loss = 1.783420\n",
      "2024-12-14 01:31:36.743000: I runner.py:310] Step = 82900 ; steps/s = 1.64, tokens/s = 81442 (37466 source, 43976 target) ; Learning rate = 0.000307 ; Loss = 1.754830\n",
      "2024-12-14 01:32:38.061000: I runner.py:310] Step = 83000 ; steps/s = 1.63, tokens/s = 82421 (37916 source, 44505 target) ; Learning rate = 0.000307 ; Loss = 1.763993\n",
      "2024-12-14 01:33:39.441000: I runner.py:310] Step = 83100 ; steps/s = 1.63, tokens/s = 82327 (37859 source, 44468 target) ; Learning rate = 0.000307 ; Loss = 1.779794\n",
      "2024-12-14 01:34:40.392000: I runner.py:310] Step = 83200 ; steps/s = 1.64, tokens/s = 81549 (37505 source, 44044 target) ; Learning rate = 0.000306 ; Loss = 1.748767\n",
      "2024-12-14 01:35:41.734000: I runner.py:310] Step = 83300 ; steps/s = 1.63, tokens/s = 82383 (37892 source, 44491 target) ; Learning rate = 0.000306 ; Loss = 1.773637\n",
      "2024-12-14 01:36:43.137000: I runner.py:310] Step = 83400 ; steps/s = 1.63, tokens/s = 82276 (37829 source, 44447 target) ; Learning rate = 0.000306 ; Loss = 1.771218\n",
      "2024-12-14 01:37:44.351000: I runner.py:310] Step = 83500 ; steps/s = 1.63, tokens/s = 82272 (37842 source, 44430 target) ; Learning rate = 0.000306 ; Loss = 1.772667\n",
      "2024-12-14 01:38:45.486000: I runner.py:310] Step = 83600 ; steps/s = 1.64, tokens/s = 81606 (37543 source, 44063 target) ; Learning rate = 0.000306 ; Loss = 1.747407\n",
      "2024-12-14 01:39:46.779000: I runner.py:310] Step = 83700 ; steps/s = 1.63, tokens/s = 82458 (37933 source, 44525 target) ; Learning rate = 0.000306 ; Loss = 1.773473\n",
      "2024-12-14 01:40:48.256000: I runner.py:310] Step = 83800 ; steps/s = 1.63, tokens/s = 82189 (37792 source, 44397 target) ; Learning rate = 0.000305 ; Loss = 1.772072\n",
      "2024-12-14 01:41:49.183000: I runner.py:310] Step = 83900 ; steps/s = 1.64, tokens/s = 81552 (37500 source, 44052 target) ; Learning rate = 0.000305 ; Loss = 1.738807\n",
      "2024-12-14 01:42:50.574000: I runner.py:310] Step = 84000 ; steps/s = 1.63, tokens/s = 82312 (37863 source, 44449 target) ; Learning rate = 0.000305 ; Loss = 1.759265\n",
      "2024-12-14 01:43:51.893000: I runner.py:310] Step = 84100 ; steps/s = 1.63, tokens/s = 82409 (37893 source, 44516 target) ; Learning rate = 0.000305 ; Loss = 1.778934\n",
      "2024-12-14 01:44:52.875000: I runner.py:310] Step = 84200 ; steps/s = 1.64, tokens/s = 81535 (37514 source, 44021 target) ; Learning rate = 0.000305 ; Loss = 1.768863\n",
      "2024-12-14 01:45:54.249000: I runner.py:310] Step = 84300 ; steps/s = 1.63, tokens/s = 82322 (37857 source, 44465 target) ; Learning rate = 0.000304 ; Loss = 1.775006\n",
      "2024-12-14 01:46:55.586000: I runner.py:310] Step = 84400 ; steps/s = 1.63, tokens/s = 82395 (37900 source, 44495 target) ; Learning rate = 0.000304 ; Loss = 1.752860\n",
      "2024-12-14 01:47:56.998000: I runner.py:310] Step = 84500 ; steps/s = 1.63, tokens/s = 82275 (37830 source, 44445 target) ; Learning rate = 0.000304 ; Loss = 1.760429\n",
      "2024-12-14 01:48:57.914000: I runner.py:310] Step = 84600 ; steps/s = 1.64, tokens/s = 81600 (37539 source, 44061 target) ; Learning rate = 0.000304 ; Loss = 1.726598\n",
      "2024-12-14 01:49:59.279000: I runner.py:310] Step = 84700 ; steps/s = 1.63, tokens/s = 82355 (37871 source, 44484 target) ; Learning rate = 0.000304 ; Loss = 1.773464\n",
      "2024-12-14 01:51:00.604000: I runner.py:310] Step = 84800 ; steps/s = 1.63, tokens/s = 82416 (37903 source, 44513 target) ; Learning rate = 0.000304 ; Loss = 1.771489\n",
      "2024-12-14 01:52:01.612000: I runner.py:310] Step = 84900 ; steps/s = 1.64, tokens/s = 81466 (37484 source, 43982 target) ; Learning rate = 0.000303 ; Loss = 1.759743\n",
      "2024-12-14 01:53:02.994000: I runner.py:310] Step = 85000 ; steps/s = 1.63, tokens/s = 82335 (37858 source, 44477 target) ; Learning rate = 0.000303 ; Loss = 1.773329\n",
      "2024-12-14 01:53:02.995000: I training.py:192] Running evaluation for step 85000\n",
      "2024-12-14 01:55:09.331000: I training.py:192] Evaluation result for step 85000: loss = 2.524501 ; perplexity = 12.484662\n",
      "2024-12-14 01:56:10.502000: I runner.py:310] Step = 85100 ; steps/s = 1.64, tokens/s = 82617 (37987 source, 44630 target) ; Learning rate = 0.000303 ; Loss = 1.750650\n",
      "2024-12-14 01:57:11.892000: I runner.py:310] Step = 85200 ; steps/s = 1.63, tokens/s = 82318 (37867 source, 44451 target) ; Learning rate = 0.000303 ; Loss = 1.766418\n",
      "2024-12-14 01:58:12.786000: I runner.py:310] Step = 85300 ; steps/s = 1.64, tokens/s = 81609 (37548 source, 44061 target) ; Learning rate = 0.000303 ; Loss = 1.787155\n",
      "2024-12-14 01:59:14.144000: I runner.py:310] Step = 85400 ; steps/s = 1.63, tokens/s = 82359 (37884 source, 44475 target) ; Learning rate = 0.000302 ; Loss = 1.759185\n",
      "2024-12-14 02:00:15.506000: I runner.py:310] Step = 85500 ; steps/s = 1.63, tokens/s = 82364 (37890 source, 44474 target) ; Learning rate = 0.000302 ; Loss = 1.761512\n",
      "2024-12-14 02:01:16.611000: I runner.py:310] Step = 85600 ; steps/s = 1.64, tokens/s = 81338 (37399 source, 43939 target) ; Learning rate = 0.000302 ; Loss = 1.761429\n",
      "2024-12-14 02:02:17.952000: I runner.py:310] Step = 85700 ; steps/s = 1.63, tokens/s = 82408 (37909 source, 44499 target) ; Learning rate = 0.000302 ; Loss = 1.758814\n",
      "2024-12-14 02:03:19.332000: I runner.py:310] Step = 85800 ; steps/s = 1.63, tokens/s = 82341 (37867 source, 44474 target) ; Learning rate = 0.000302 ; Loss = 1.747680\n",
      "2024-12-14 02:04:20.687000: I runner.py:310] Step = 85900 ; steps/s = 1.63, tokens/s = 82322 (37852 source, 44470 target) ; Learning rate = 0.000302 ; Loss = 1.768319\n",
      "2024-12-14 02:05:21.621000: I runner.py:310] Step = 86000 ; steps/s = 1.64, tokens/s = 81549 (37493 source, 44056 target) ; Learning rate = 0.000301 ; Loss = 1.780190\n",
      "2024-12-14 02:06:22.987000: I runner.py:310] Step = 86100 ; steps/s = 1.63, tokens/s = 82341 (37872 source, 44469 target) ; Learning rate = 0.000301 ; Loss = 1.754428\n",
      "2024-12-14 02:07:24.275000: I runner.py:310] Step = 86200 ; steps/s = 1.63, tokens/s = 82458 (37921 source, 44537 target) ; Learning rate = 0.000301 ; Loss = 1.748794\n",
      "2024-12-14 02:08:25.191000: I runner.py:310] Step = 86300 ; steps/s = 1.64, tokens/s = 81614 (37564 source, 44050 target) ; Learning rate = 0.000301 ; Loss = 1.752545\n",
      "2024-12-14 02:09:26.494000: I runner.py:310] Step = 86400 ; steps/s = 1.63, tokens/s = 82414 (37888 source, 44526 target) ; Learning rate = 0.000301 ; Loss = 1.748284\n",
      "2024-12-14 02:10:27.975000: I runner.py:310] Step = 86500 ; steps/s = 1.63, tokens/s = 82209 (37816 source, 44393 target) ; Learning rate = 0.000301 ; Loss = 1.753361\n",
      "2024-12-14 02:11:29.305000: I runner.py:310] Step = 86600 ; steps/s = 1.63, tokens/s = 82400 (37908 source, 44492 target) ; Learning rate = 0.000300 ; Loss = 1.747494\n",
      "2024-12-14 02:12:30.268000: I runner.py:310] Step = 86700 ; steps/s = 1.64, tokens/s = 81533 (37496 source, 44037 target) ; Learning rate = 0.000300 ; Loss = 1.785498\n",
      "2024-12-14 02:13:31.612000: I runner.py:310] Step = 86800 ; steps/s = 1.63, tokens/s = 82358 (37866 source, 44492 target) ; Learning rate = 0.000300 ; Loss = 1.745457\n",
      "2024-12-14 02:14:32.929000: I runner.py:310] Step = 86900 ; steps/s = 1.63, tokens/s = 82436 (37939 source, 44497 target) ; Learning rate = 0.000300 ; Loss = 1.755064\n",
      "2024-12-14 02:15:33.889000: I runner.py:310] Step = 87000 ; steps/s = 1.64, tokens/s = 81549 (37512 source, 44037 target) ; Learning rate = 0.000300 ; Loss = 1.750982\n",
      "2024-12-14 02:16:35.227000: I runner.py:310] Step = 87100 ; steps/s = 1.63, tokens/s = 82418 (37923 source, 44495 target) ; Learning rate = 0.000299 ; Loss = 1.750881\n",
      "2024-12-14 02:17:36.603000: I runner.py:310] Step = 87200 ; steps/s = 1.63, tokens/s = 82300 (37824 source, 44476 target) ; Learning rate = 0.000299 ; Loss = 1.776863\n",
      "2024-12-14 02:18:37.874000: I runner.py:310] Step = 87300 ; steps/s = 1.63, tokens/s = 82464 (37930 source, 44534 target) ; Learning rate = 0.000299 ; Loss = 1.776188\n",
      "2024-12-14 02:19:38.898000: I runner.py:310] Step = 87400 ; steps/s = 1.64, tokens/s = 81452 (37465 source, 43987 target) ; Learning rate = 0.000299 ; Loss = 1.722315\n",
      "2024-12-14 02:20:40.216000: I runner.py:310] Step = 87500 ; steps/s = 1.63, tokens/s = 82433 (37909 source, 44524 target) ; Learning rate = 0.000299 ; Loss = 1.758389\n",
      "2024-12-14 02:21:41.550000: I runner.py:310] Step = 87600 ; steps/s = 1.63, tokens/s = 82411 (37912 source, 44499 target) ; Learning rate = 0.000299 ; Loss = 1.780540\n",
      "2024-12-14 02:22:42.586000: I runner.py:310] Step = 87700 ; steps/s = 1.64, tokens/s = 81407 (37440 source, 43967 target) ; Learning rate = 0.000298 ; Loss = 1.743803\n",
      "2024-12-14 02:23:43.919000: I runner.py:310] Step = 87800 ; steps/s = 1.63, tokens/s = 82385 (37889 source, 44496 target) ; Learning rate = 0.000298 ; Loss = 1.748640\n",
      "2024-12-14 02:24:45.281000: I runner.py:310] Step = 87900 ; steps/s = 1.63, tokens/s = 82343 (37856 source, 44487 target) ; Learning rate = 0.000298 ; Loss = 1.775133\n",
      "2024-12-14 02:25:46.609000: I runner.py:310] Step = 88000 ; steps/s = 1.63, tokens/s = 82423 (37929 source, 44494 target) ; Learning rate = 0.000298 ; Loss = 1.768850\n",
      "2024-12-14 02:26:47.617000: I runner.py:310] Step = 88100 ; steps/s = 1.64, tokens/s = 81465 (37455 source, 44010 target) ; Learning rate = 0.000298 ; Loss = 1.779217\n",
      "2024-12-14 02:27:48.958000: I runner.py:310] Step = 88200 ; steps/s = 1.63, tokens/s = 82379 (37889 source, 44490 target) ; Learning rate = 0.000298 ; Loss = 1.747755\n",
      "2024-12-14 02:28:50.400000: I runner.py:310] Step = 88300 ; steps/s = 1.63, tokens/s = 82246 (37832 source, 44414 target) ; Learning rate = 0.000297 ; Loss = 1.762479\n",
      "2024-12-14 02:29:51.259000: I runner.py:310] Step = 88400 ; steps/s = 1.64, tokens/s = 81681 (37576 source, 44105 target) ; Learning rate = 0.000297 ; Loss = 1.735170\n",
      "2024-12-14 02:30:52.604000: I runner.py:310] Step = 88500 ; steps/s = 1.63, tokens/s = 82371 (37883 source, 44488 target) ; Learning rate = 0.000297 ; Loss = 1.748817\n",
      "2024-12-14 02:31:54.013000: I runner.py:310] Step = 88600 ; steps/s = 1.63, tokens/s = 82293 (37853 source, 44440 target) ; Learning rate = 0.000297 ; Loss = 1.763188\n",
      "2024-12-14 02:32:55.323000: I runner.py:310] Step = 88700 ; steps/s = 1.63, tokens/s = 82410 (37887 source, 44523 target) ; Learning rate = 0.000297 ; Loss = 1.770314\n",
      "2024-12-14 02:33:56.326000: I runner.py:310] Step = 88800 ; steps/s = 1.64, tokens/s = 81497 (37510 source, 43987 target) ; Learning rate = 0.000297 ; Loss = 1.726852\n",
      "2024-12-14 02:34:57.734000: I runner.py:310] Step = 88900 ; steps/s = 1.63, tokens/s = 82285 (37842 source, 44443 target) ; Learning rate = 0.000296 ; Loss = 1.761193\n",
      "2024-12-14 02:35:59.132000: I runner.py:310] Step = 89000 ; steps/s = 1.63, tokens/s = 82304 (37846 source, 44458 target) ; Learning rate = 0.000296 ; Loss = 1.771467\n",
      "2024-12-14 02:37:00.054000: I runner.py:310] Step = 89100 ; steps/s = 1.64, tokens/s = 81575 (37513 source, 44062 target) ; Learning rate = 0.000296 ; Loss = 1.773764\n",
      "2024-12-14 02:38:01.501000: I runner.py:310] Step = 89200 ; steps/s = 1.63, tokens/s = 82230 (37812 source, 44418 target) ; Learning rate = 0.000296 ; Loss = 1.757282\n",
      "2024-12-14 02:39:02.864000: I runner.py:310] Step = 89300 ; steps/s = 1.63, tokens/s = 82343 (37875 source, 44468 target) ; Learning rate = 0.000296 ; Loss = 1.745624\n",
      "2024-12-14 02:40:04.193000: I runner.py:310] Step = 89400 ; steps/s = 1.63, tokens/s = 82413 (37915 source, 44498 target) ; Learning rate = 0.000296 ; Loss = 1.758890\n",
      "2024-12-14 02:41:05.195000: I runner.py:310] Step = 89500 ; steps/s = 1.64, tokens/s = 81513 (37512 source, 44001 target) ; Learning rate = 0.000295 ; Loss = 1.732930\n",
      "2024-12-14 02:42:06.520000: I runner.py:310] Step = 89600 ; steps/s = 1.63, tokens/s = 82407 (37894 source, 44513 target) ; Learning rate = 0.000295 ; Loss = 1.770182\n",
      "2024-12-14 02:43:07.938000: I runner.py:310] Step = 89700 ; steps/s = 1.63, tokens/s = 82281 (37851 source, 44430 target) ; Learning rate = 0.000295 ; Loss = 1.759022\n",
      "2024-12-14 02:44:08.854000: I runner.py:310] Step = 89800 ; steps/s = 1.64, tokens/s = 81558 (37490 source, 44068 target) ; Learning rate = 0.000295 ; Loss = 1.780826\n",
      "2024-12-14 02:45:10.277000: I runner.py:310] Step = 89900 ; steps/s = 1.63, tokens/s = 82249 (37812 source, 44437 target) ; Learning rate = 0.000295 ; Loss = 1.738728\n",
      "2024-12-14 02:46:11.617000: I runner.py:310] Step = 90000 ; steps/s = 1.63, tokens/s = 82391 (37901 source, 44490 target) ; Learning rate = 0.000295 ; Loss = 1.757590\n",
      "2024-12-14 02:46:13.570000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-90000\n",
      "2024-12-14 02:46:13.570000: I training.py:192] Running evaluation for step 90000\n",
      "2024-12-14 02:48:17.414000: I training.py:192] Evaluation result for step 90000: loss = 2.543128 ; perplexity = 12.719390\n",
      "2024-12-14 02:49:18.708000: I runner.py:310] Step = 90100 ; steps/s = 1.63, tokens/s = 82477 (37939 source, 44538 target) ; Learning rate = 0.000294 ; Loss = 1.758224\n",
      "2024-12-14 02:50:19.791000: I runner.py:310] Step = 90200 ; steps/s = 1.64, tokens/s = 81360 (37424 source, 43936 target) ; Learning rate = 0.000294 ; Loss = 1.772846\n",
      "2024-12-14 02:51:21.177000: I runner.py:310] Step = 90300 ; steps/s = 1.63, tokens/s = 82354 (37894 source, 44460 target) ; Learning rate = 0.000294 ; Loss = 1.740454\n",
      "2024-12-14 02:52:22.574000: I runner.py:310] Step = 90400 ; steps/s = 1.63, tokens/s = 82293 (37835 source, 44458 target) ; Learning rate = 0.000294 ; Loss = 1.752942\n",
      "2024-12-14 02:53:23.599000: I runner.py:310] Step = 90500 ; steps/s = 1.64, tokens/s = 81433 (37448 source, 43985 target) ; Learning rate = 0.000294 ; Loss = 1.736933\n",
      "2024-12-14 02:54:25.069000: I runner.py:310] Step = 90600 ; steps/s = 1.63, tokens/s = 82211 (37803 source, 44408 target) ; Learning rate = 0.000294 ; Loss = 1.750905\n",
      "2024-12-14 02:55:26.413000: I runner.py:310] Step = 90700 ; steps/s = 1.63, tokens/s = 82381 (37906 source, 44475 target) ; Learning rate = 0.000293 ; Loss = 1.771720\n",
      "2024-12-14 02:56:27.794000: I runner.py:310] Step = 90800 ; steps/s = 1.63, tokens/s = 82350 (37876 source, 44474 target) ; Learning rate = 0.000293 ; Loss = 1.758336\n",
      "2024-12-14 02:57:28.731000: I runner.py:310] Step = 90900 ; steps/s = 1.64, tokens/s = 81542 (37480 source, 44062 target) ; Learning rate = 0.000293 ; Loss = 1.768306\n",
      "2024-12-14 02:58:30.109000: I runner.py:310] Step = 91000 ; steps/s = 1.63, tokens/s = 82348 (37885 source, 44463 target) ; Learning rate = 0.000293 ; Loss = 1.746142\n",
      "2024-12-14 02:59:31.518000: I runner.py:310] Step = 91100 ; steps/s = 1.63, tokens/s = 82318 (37885 source, 44433 target) ; Learning rate = 0.000293 ; Loss = 1.743792\n",
      "2024-12-14 03:00:32.410000: I runner.py:310] Step = 91200 ; steps/s = 1.64, tokens/s = 81607 (37521 source, 44086 target) ; Learning rate = 0.000293 ; Loss = 1.735004\n",
      "2024-12-14 03:01:33.837000: I runner.py:310] Step = 91300 ; steps/s = 1.63, tokens/s = 82257 (37823 source, 44434 target) ; Learning rate = 0.000293 ; Loss = 1.772859\n",
      "2024-12-14 03:02:35.271000: I runner.py:310] Step = 91400 ; steps/s = 1.63, tokens/s = 82272 (37850 source, 44422 target) ; Learning rate = 0.000292 ; Loss = 1.766872\n",
      "2024-12-14 03:03:36.700000: I runner.py:310] Step = 91500 ; steps/s = 1.63, tokens/s = 82254 (37833 source, 44421 target) ; Learning rate = 0.000292 ; Loss = 1.760494\n",
      "2024-12-14 03:04:37.637000: I runner.py:310] Step = 91600 ; steps/s = 1.64, tokens/s = 81559 (37502 source, 44057 target) ; Learning rate = 0.000292 ; Loss = 1.762028\n",
      "2024-12-14 03:05:39.104000: I runner.py:310] Step = 91700 ; steps/s = 1.63, tokens/s = 82209 (37796 source, 44413 target) ; Learning rate = 0.000292 ; Loss = 1.753661\n",
      "2024-12-14 03:06:40.479000: I runner.py:310] Step = 91800 ; steps/s = 1.63, tokens/s = 82336 (37885 source, 44451 target) ; Learning rate = 0.000292 ; Loss = 1.750894\n",
      "2024-12-14 03:07:41.444000: I runner.py:310] Step = 91900 ; steps/s = 1.64, tokens/s = 81523 (37500 source, 44023 target) ; Learning rate = 0.000292 ; Loss = 1.731001\n",
      "2024-12-14 03:08:42.948000: I runner.py:310] Step = 92000 ; steps/s = 1.63, tokens/s = 82164 (37793 source, 44371 target) ; Learning rate = 0.000291 ; Loss = 1.749729\n",
      "2024-12-14 03:09:44.276000: I runner.py:310] Step = 92100 ; steps/s = 1.63, tokens/s = 82402 (37888 source, 44514 target) ; Learning rate = 0.000291 ; Loss = 1.756036\n",
      "2024-12-14 03:10:45.411000: I runner.py:310] Step = 92200 ; steps/s = 1.64, tokens/s = 81723 (37598 source, 44125 target) ; Learning rate = 0.000291 ; Loss = 1.789392\n",
      "2024-12-14 03:11:46.641000: I runner.py:310] Step = 92300 ; steps/s = 1.63, tokens/s = 82137 (37777 source, 44360 target) ; Learning rate = 0.000291 ; Loss = 1.751270\n",
      "2024-12-14 03:12:48.079000: I runner.py:310] Step = 92400 ; steps/s = 1.63, tokens/s = 82247 (37830 source, 44417 target) ; Learning rate = 0.000291 ; Loss = 1.750130\n",
      "2024-12-14 03:13:49.430000: I runner.py:310] Step = 92500 ; steps/s = 1.63, tokens/s = 82370 (37887 source, 44483 target) ; Learning rate = 0.000291 ; Loss = 1.748963\n",
      "2024-12-14 03:14:50.483000: I runner.py:310] Step = 92600 ; steps/s = 1.64, tokens/s = 81414 (37442 source, 43972 target) ; Learning rate = 0.000290 ; Loss = 1.751664\n",
      "2024-12-14 03:15:51.779000: I runner.py:310] Step = 92700 ; steps/s = 1.63, tokens/s = 82452 (37926 source, 44526 target) ; Learning rate = 0.000290 ; Loss = 1.756280\n",
      "2024-12-14 03:16:53.118000: I runner.py:310] Step = 92800 ; steps/s = 1.63, tokens/s = 82387 (37896 source, 44491 target) ; Learning rate = 0.000290 ; Loss = 1.760757\n",
      "2024-12-14 03:17:54.097000: I runner.py:310] Step = 92900 ; steps/s = 1.64, tokens/s = 81502 (37481 source, 44021 target) ; Learning rate = 0.000290 ; Loss = 1.752720\n",
      "2024-12-14 03:18:55.426000: I runner.py:310] Step = 93000 ; steps/s = 1.63, tokens/s = 82405 (37910 source, 44495 target) ; Learning rate = 0.000290 ; Loss = 1.754577\n",
      "2024-12-14 03:19:56.794000: I runner.py:310] Step = 93100 ; steps/s = 1.63, tokens/s = 82355 (37878 source, 44477 target) ; Learning rate = 0.000290 ; Loss = 1.752346\n",
      "2024-12-14 03:20:58.175000: I runner.py:310] Step = 93200 ; steps/s = 1.63, tokens/s = 82321 (37862 source, 44459 target) ; Learning rate = 0.000290 ; Loss = 1.757754\n",
      "2024-12-14 03:21:59.116000: I runner.py:310] Step = 93300 ; steps/s = 1.64, tokens/s = 81565 (37522 source, 44043 target) ; Learning rate = 0.000289 ; Loss = 1.723929\n",
      "2024-12-14 03:23:00.440000: I runner.py:310] Step = 93400 ; steps/s = 1.63, tokens/s = 82418 (37919 source, 44499 target) ; Learning rate = 0.000289 ; Loss = 1.764514\n",
      "2024-12-14 03:24:01.859000: I runner.py:310] Step = 93500 ; steps/s = 1.63, tokens/s = 82248 (37800 source, 44448 target) ; Learning rate = 0.000289 ; Loss = 1.753121\n",
      "2024-12-14 03:25:02.879000: I runner.py:310] Step = 93600 ; steps/s = 1.64, tokens/s = 81459 (37477 source, 43982 target) ; Learning rate = 0.000289 ; Loss = 1.745167\n",
      "2024-12-14 03:26:04.250000: I runner.py:310] Step = 93700 ; steps/s = 1.63, tokens/s = 82361 (37882 source, 44479 target) ; Learning rate = 0.000289 ; Loss = 1.734416\n",
      "2024-12-14 03:27:05.695000: I runner.py:310] Step = 93800 ; steps/s = 1.63, tokens/s = 82279 (37864 source, 44415 target) ; Learning rate = 0.000289 ; Loss = 1.756620\n",
      "2024-12-14 03:28:07.016000: I runner.py:310] Step = 93900 ; steps/s = 1.63, tokens/s = 82362 (37859 source, 44503 target) ; Learning rate = 0.000288 ; Loss = 1.759934\n",
      "2024-12-14 03:29:07.978000: I runner.py:310] Step = 94000 ; steps/s = 1.64, tokens/s = 81517 (37481 source, 44036 target) ; Learning rate = 0.000288 ; Loss = 1.737155\n",
      "2024-12-14 03:30:09.291000: I runner.py:310] Step = 94100 ; steps/s = 1.63, tokens/s = 82417 (37908 source, 44509 target) ; Learning rate = 0.000288 ; Loss = 1.748933\n",
      "2024-12-14 03:31:10.606000: I runner.py:310] Step = 94200 ; steps/s = 1.63, tokens/s = 82431 (37920 source, 44511 target) ; Learning rate = 0.000288 ; Loss = 1.759396\n",
      "2024-12-14 03:32:11.550000: I runner.py:310] Step = 94300 ; steps/s = 1.64, tokens/s = 81552 (37507 source, 44045 target) ; Learning rate = 0.000288 ; Loss = 1.753364\n",
      "2024-12-14 03:33:12.972000: I runner.py:310] Step = 94400 ; steps/s = 1.63, tokens/s = 82303 (37864 source, 44439 target) ; Learning rate = 0.000288 ; Loss = 1.727834\n",
      "2024-12-14 03:34:15.355000: I runner.py:310] Step = 94500 ; steps/s = 1.60, tokens/s = 80998 (37242 source, 43756 target) ; Learning rate = 0.000288 ; Loss = 1.765661\n",
      "2024-12-14 03:35:17.250000: I runner.py:310] Step = 94600 ; steps/s = 1.62, tokens/s = 81639 (37555 source, 44084 target) ; Learning rate = 0.000287 ; Loss = 1.772387\n",
      "2024-12-14 03:36:18.393000: I runner.py:310] Step = 94700 ; steps/s = 1.64, tokens/s = 81281 (37381 source, 43900 target) ; Learning rate = 0.000287 ; Loss = 1.729407\n",
      "2024-12-14 03:37:20.045000: I runner.py:310] Step = 94800 ; steps/s = 1.62, tokens/s = 81972 (37678 source, 44294 target) ; Learning rate = 0.000287 ; Loss = 1.745723\n",
      "2024-12-14 03:38:21.643000: I runner.py:310] Step = 94900 ; steps/s = 1.62, tokens/s = 82039 (37750 source, 44289 target) ; Learning rate = 0.000287 ; Loss = 1.756646\n",
      "2024-12-14 03:39:22.942000: I runner.py:310] Step = 95000 ; steps/s = 1.63, tokens/s = 81073 (37291 source, 43782 target) ; Learning rate = 0.000287 ; Loss = 1.744406\n",
      "2024-12-14 03:39:22.944000: I training.py:192] Running evaluation for step 95000\n",
      "2024-12-14 03:41:30.195000: I training.py:192] Evaluation result for step 95000: loss = 2.555342 ; perplexity = 12.875698\n",
      "2024-12-14 03:42:31.590000: I runner.py:310] Step = 95100 ; steps/s = 1.63, tokens/s = 82354 (37880 source, 44474 target) ; Learning rate = 0.000287 ; Loss = 1.741288\n",
      "2024-12-14 03:43:33.081000: I runner.py:310] Step = 95200 ; steps/s = 1.63, tokens/s = 82176 (37789 source, 44387 target) ; Learning rate = 0.000286 ; Loss = 1.749942\n",
      "2024-12-14 03:44:34.604000: I runner.py:310] Step = 95300 ; steps/s = 1.63, tokens/s = 82127 (37783 source, 44344 target) ; Learning rate = 0.000286 ; Loss = 1.772350\n",
      "2024-12-14 03:45:35.724000: I runner.py:310] Step = 95400 ; steps/s = 1.64, tokens/s = 81306 (37378 source, 43928 target) ; Learning rate = 0.000286 ; Loss = 1.719899\n",
      "2024-12-14 03:46:37.278000: I runner.py:310] Step = 95500 ; steps/s = 1.62, tokens/s = 82086 (37739 source, 44347 target) ; Learning rate = 0.000286 ; Loss = 1.750484\n",
      "2024-12-14 03:47:38.855000: I runner.py:310] Step = 95600 ; steps/s = 1.62, tokens/s = 82083 (37772 source, 44311 target) ; Learning rate = 0.000286 ; Loss = 1.759506\n",
      "2024-12-14 03:48:40.040000: I runner.py:310] Step = 95700 ; steps/s = 1.63, tokens/s = 81239 (37368 source, 43871 target) ; Learning rate = 0.000286 ; Loss = 1.736042\n",
      "2024-12-14 03:49:41.580000: I runner.py:310] Step = 95800 ; steps/s = 1.63, tokens/s = 82124 (37781 source, 44343 target) ; Learning rate = 0.000286 ; Loss = 1.735520\n",
      "2024-12-14 03:50:43.098000: I runner.py:310] Step = 95900 ; steps/s = 1.63, tokens/s = 82136 (37778 source, 44358 target) ; Learning rate = 0.000285 ; Loss = 1.750756\n",
      "2024-12-14 03:51:44.621000: I runner.py:310] Step = 96000 ; steps/s = 1.63, tokens/s = 82133 (37769 source, 44364 target) ; Learning rate = 0.000285 ; Loss = 1.759051\n",
      "2024-12-14 03:52:45.684000: I runner.py:310] Step = 96100 ; steps/s = 1.64, tokens/s = 81407 (37434 source, 43973 target) ; Learning rate = 0.000285 ; Loss = 1.728314\n",
      "2024-12-14 03:53:47.272000: I runner.py:310] Step = 96200 ; steps/s = 1.62, tokens/s = 82058 (37752 source, 44306 target) ; Learning rate = 0.000285 ; Loss = 1.741275\n",
      "2024-12-14 03:54:48.738000: I runner.py:310] Step = 96300 ; steps/s = 1.63, tokens/s = 82219 (37810 source, 44409 target) ; Learning rate = 0.000285 ; Loss = 1.759349\n",
      "2024-12-14 03:55:49.796000: I runner.py:310] Step = 96400 ; steps/s = 1.64, tokens/s = 81406 (37451 source, 43955 target) ; Learning rate = 0.000285 ; Loss = 1.744712\n",
      "2024-12-14 03:56:51.357000: I runner.py:310] Step = 96500 ; steps/s = 1.62, tokens/s = 82098 (37771 source, 44327 target) ; Learning rate = 0.000285 ; Loss = 1.742948\n",
      "2024-12-14 03:57:52.910000: I runner.py:310] Step = 96600 ; steps/s = 1.62, tokens/s = 82088 (37746 source, 44342 target) ; Learning rate = 0.000284 ; Loss = 1.753943\n",
      "2024-12-14 03:58:54.449000: I runner.py:310] Step = 96700 ; steps/s = 1.63, tokens/s = 82106 (37758 source, 44348 target) ; Learning rate = 0.000284 ; Loss = 1.764674\n",
      "2024-12-14 03:59:55.724000: I runner.py:310] Step = 96800 ; steps/s = 1.63, tokens/s = 81084 (37270 source, 43814 target) ; Learning rate = 0.000284 ; Loss = 1.764201\n",
      "2024-12-14 04:00:57.189000: I runner.py:310] Step = 96900 ; steps/s = 1.63, tokens/s = 82238 (37828 source, 44410 target) ; Learning rate = 0.000284 ; Loss = 1.739061\n",
      "2024-12-14 04:01:58.564000: I runner.py:310] Step = 97000 ; steps/s = 1.63, tokens/s = 82329 (37869 source, 44460 target) ; Learning rate = 0.000284 ; Loss = 1.736834\n",
      "2024-12-14 04:02:59.573000: I runner.py:310] Step = 97100 ; steps/s = 1.64, tokens/s = 81478 (37494 source, 43984 target) ; Learning rate = 0.000284 ; Loss = 1.747275\n",
      "2024-12-14 04:04:00.988000: I runner.py:310] Step = 97200 ; steps/s = 1.63, tokens/s = 82285 (37842 source, 44443 target) ; Learning rate = 0.000284 ; Loss = 1.741572\n",
      "2024-12-14 04:05:02.386000: I runner.py:310] Step = 97300 ; steps/s = 1.63, tokens/s = 82302 (37849 source, 44453 target) ; Learning rate = 0.000283 ; Loss = 1.751831\n",
      "2024-12-14 04:06:03.777000: I runner.py:310] Step = 97400 ; steps/s = 1.63, tokens/s = 82328 (37868 source, 44460 target) ; Learning rate = 0.000283 ; Loss = 1.751998\n",
      "2024-12-14 04:07:04.736000: I runner.py:310] Step = 97500 ; steps/s = 1.64, tokens/s = 81529 (37492 source, 44037 target) ; Learning rate = 0.000283 ; Loss = 1.766036\n",
      "2024-12-14 04:08:06.148000: I runner.py:310] Step = 97600 ; steps/s = 1.63, tokens/s = 82294 (37847 source, 44447 target) ; Learning rate = 0.000283 ; Loss = 1.728334\n",
      "2024-12-14 04:09:07.498000: I runner.py:310] Step = 97700 ; steps/s = 1.63, tokens/s = 82364 (37878 source, 44486 target) ; Learning rate = 0.000283 ; Loss = 1.751022\n",
      "2024-12-14 04:10:08.481000: I runner.py:310] Step = 97800 ; steps/s = 1.64, tokens/s = 81515 (37503 source, 44012 target) ; Learning rate = 0.000283 ; Loss = 1.760555\n",
      "2024-12-14 04:11:09.846000: I runner.py:310] Step = 97900 ; steps/s = 1.63, tokens/s = 82332 (37859 source, 44473 target) ; Learning rate = 0.000282 ; Loss = 1.738375\n",
      "2024-12-14 04:12:11.229000: I runner.py:310] Step = 98000 ; steps/s = 1.63, tokens/s = 82326 (37867 source, 44459 target) ; Learning rate = 0.000282 ; Loss = 1.722932\n",
      "2024-12-14 04:13:12.655000: I runner.py:310] Step = 98100 ; steps/s = 1.63, tokens/s = 82281 (37859 source, 44422 target) ; Learning rate = 0.000282 ; Loss = 1.756706\n",
      "2024-12-14 04:14:13.657000: I runner.py:310] Step = 98200 ; steps/s = 1.64, tokens/s = 81427 (37422 source, 44005 target) ; Learning rate = 0.000282 ; Loss = 1.743826\n",
      "2024-12-14 04:15:15.029000: I runner.py:310] Step = 98300 ; steps/s = 1.63, tokens/s = 82387 (37899 source, 44488 target) ; Learning rate = 0.000282 ; Loss = 1.745103\n",
      "2024-12-14 04:16:16.351000: I runner.py:310] Step = 98400 ; steps/s = 1.63, tokens/s = 82410 (37912 source, 44498 target) ; Learning rate = 0.000282 ; Loss = 1.751557\n",
      "2024-12-14 04:17:17.380000: I runner.py:310] Step = 98500 ; steps/s = 1.64, tokens/s = 81411 (37438 source, 43973 target) ; Learning rate = 0.000282 ; Loss = 1.760049\n",
      "2024-12-14 04:18:18.789000: I runner.py:310] Step = 98600 ; steps/s = 1.63, tokens/s = 82276 (37843 source, 44433 target) ; Learning rate = 0.000281 ; Loss = 1.735758\n",
      "2024-12-14 04:19:20.168000: I runner.py:310] Step = 98700 ; steps/s = 1.63, tokens/s = 82343 (37867 source, 44476 target) ; Learning rate = 0.000281 ; Loss = 1.734238\n",
      "2024-12-14 04:20:21.606000: I runner.py:310] Step = 98800 ; steps/s = 1.63, tokens/s = 82281 (37859 source, 44422 target) ; Learning rate = 0.000281 ; Loss = 1.753042\n",
      "2024-12-14 04:21:22.495000: I runner.py:310] Step = 98900 ; steps/s = 1.64, tokens/s = 81624 (37542 source, 44082 target) ; Learning rate = 0.000281 ; Loss = 1.756587\n",
      "2024-12-14 04:22:23.916000: I runner.py:310] Step = 99000 ; steps/s = 1.63, tokens/s = 82267 (37816 source, 44451 target) ; Learning rate = 0.000281 ; Loss = 1.733411\n",
      "2024-12-14 04:23:25.313000: I runner.py:310] Step = 99100 ; steps/s = 1.63, tokens/s = 82345 (37897 source, 44448 target) ; Learning rate = 0.000281 ; Loss = 1.750525\n",
      "2024-12-14 04:24:26.325000: I runner.py:310] Step = 99200 ; steps/s = 1.64, tokens/s = 81421 (37440 source, 43981 target) ; Learning rate = 0.000281 ; Loss = 1.735062\n",
      "2024-12-14 04:25:27.689000: I runner.py:310] Step = 99300 ; steps/s = 1.63, tokens/s = 82370 (37893 source, 44477 target) ; Learning rate = 0.000280 ; Loss = 1.747805\n",
      "2024-12-14 04:26:29.037000: I runner.py:310] Step = 99400 ; steps/s = 1.63, tokens/s = 82363 (37860 source, 44503 target) ; Learning rate = 0.000280 ; Loss = 1.756476\n",
      "2024-12-14 04:27:30.492000: I runner.py:310] Step = 99500 ; steps/s = 1.63, tokens/s = 82237 (37844 source, 44393 target) ; Learning rate = 0.000280 ; Loss = 1.752938\n",
      "2024-12-14 04:28:31.412000: I runner.py:310] Step = 99600 ; steps/s = 1.64, tokens/s = 81587 (37528 source, 44059 target) ; Learning rate = 0.000280 ; Loss = 1.759189\n",
      "2024-12-14 04:29:32.783000: I runner.py:310] Step = 99700 ; steps/s = 1.63, tokens/s = 82331 (37859 source, 44472 target) ; Learning rate = 0.000280 ; Loss = 1.733015\n",
      "2024-12-14 04:30:34.087000: I runner.py:310] Step = 99800 ; steps/s = 1.63, tokens/s = 82424 (37910 source, 44514 target) ; Learning rate = 0.000280 ; Loss = 1.747181\n",
      "2024-12-14 04:31:35.142000: I runner.py:310] Step = 99900 ; steps/s = 1.64, tokens/s = 81422 (37449 source, 43973 target) ; Learning rate = 0.000280 ; Loss = 1.758363\n",
      "2024-12-14 04:32:36.472000: I runner.py:310] Step = 100000 ; steps/s = 1.63, tokens/s = 82374 (37874 source, 44500 target) ; Learning rate = 0.000280 ; Loss = 1.731060\n",
      "2024-12-14 04:32:38.527000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-100000\n",
      "2024-12-14 04:32:38.527000: I training.py:192] Running evaluation for step 100000\n",
      "2024-12-14 04:34:44.848000: I training.py:192] Evaluation result for step 100000: loss = 2.555150 ; perplexity = 12.873235\n",
      "2024-12-14 04:35:46.156000: I runner.py:310] Step = 100100 ; steps/s = 1.63, tokens/s = 82457 (37935 source, 44522 target) ; Learning rate = 0.000279 ; Loss = 1.733204\n",
      "2024-12-14 04:36:47.565000: I runner.py:310] Step = 100200 ; steps/s = 1.63, tokens/s = 82303 (37861 source, 44442 target) ; Learning rate = 0.000279 ; Loss = 1.736971\n",
      "2024-12-14 04:37:48.522000: I runner.py:310] Step = 100300 ; steps/s = 1.64, tokens/s = 81530 (37500 source, 44030 target) ; Learning rate = 0.000279 ; Loss = 1.765293\n",
      "2024-12-14 04:38:49.922000: I runner.py:310] Step = 100400 ; steps/s = 1.63, tokens/s = 82312 (37857 source, 44455 target) ; Learning rate = 0.000279 ; Loss = 1.734233\n",
      "2024-12-14 04:39:51.323000: I runner.py:310] Step = 100500 ; steps/s = 1.63, tokens/s = 82330 (37872 source, 44458 target) ; Learning rate = 0.000279 ; Loss = 1.736255\n",
      "2024-12-14 04:40:52.333000: I runner.py:310] Step = 100600 ; steps/s = 1.64, tokens/s = 81432 (37454 source, 43978 target) ; Learning rate = 0.000279 ; Loss = 1.724585\n",
      "2024-12-14 04:41:53.767000: I runner.py:310] Step = 100700 ; steps/s = 1.63, tokens/s = 82247 (37809 source, 44438 target) ; Learning rate = 0.000279 ; Loss = 1.740803\n",
      "2024-12-14 04:42:55.149000: I runner.py:310] Step = 100800 ; steps/s = 1.63, tokens/s = 82322 (37862 source, 44460 target) ; Learning rate = 0.000278 ; Loss = 1.741987\n",
      "2024-12-14 04:43:56.163000: I runner.py:310] Step = 100900 ; steps/s = 1.64, tokens/s = 81486 (37495 source, 43991 target) ; Learning rate = 0.000278 ; Loss = 1.742453\n",
      "2024-12-14 04:44:57.581000: I runner.py:310] Step = 101000 ; steps/s = 1.63, tokens/s = 82255 (37839 source, 44416 target) ; Learning rate = 0.000278 ; Loss = 1.751655\n",
      "2024-12-14 04:45:58.976000: I runner.py:310] Step = 101100 ; steps/s = 1.63, tokens/s = 82313 (37853 source, 44460 target) ; Learning rate = 0.000278 ; Loss = 1.730741\n",
      "2024-12-14 04:47:00.441000: I runner.py:310] Step = 101200 ; steps/s = 1.63, tokens/s = 82237 (37828 source, 44409 target) ; Learning rate = 0.000278 ; Loss = 1.723599\n",
      "2024-12-14 04:48:01.442000: I runner.py:310] Step = 101300 ; steps/s = 1.64, tokens/s = 81509 (37492 source, 44017 target) ; Learning rate = 0.000278 ; Loss = 1.725397\n",
      "2024-12-14 04:49:02.831000: I runner.py:310] Step = 101400 ; steps/s = 1.63, tokens/s = 82333 (37876 source, 44457 target) ; Learning rate = 0.000278 ; Loss = 1.751320\n",
      "2024-12-14 04:50:04.181000: I runner.py:310] Step = 101500 ; steps/s = 1.63, tokens/s = 82356 (37865 source, 44491 target) ; Learning rate = 0.000277 ; Loss = 1.757304\n",
      "2024-12-14 04:51:05.177000: I runner.py:310] Step = 101600 ; steps/s = 1.64, tokens/s = 81447 (37457 source, 43990 target) ; Learning rate = 0.000277 ; Loss = 1.742998\n",
      "2024-12-14 04:52:06.628000: I runner.py:310] Step = 101700 ; steps/s = 1.63, tokens/s = 82226 (37808 source, 44418 target) ; Learning rate = 0.000277 ; Loss = 1.758001\n",
      "2024-12-14 04:53:08.031000: I runner.py:310] Step = 101800 ; steps/s = 1.63, tokens/s = 82315 (37868 source, 44447 target) ; Learning rate = 0.000277 ; Loss = 1.724740\n",
      "2024-12-14 04:54:09.412000: I runner.py:310] Step = 101900 ; steps/s = 1.63, tokens/s = 82321 (37864 source, 44457 target) ; Learning rate = 0.000277 ; Loss = 1.731442\n",
      "2024-12-14 04:55:10.431000: I runner.py:310] Step = 102000 ; steps/s = 1.64, tokens/s = 81456 (37455 source, 44001 target) ; Learning rate = 0.000277 ; Loss = 1.758063\n",
      "2024-12-14 04:56:11.791000: I runner.py:310] Step = 102100 ; steps/s = 1.63, tokens/s = 82351 (37871 source, 44480 target) ; Learning rate = 0.000277 ; Loss = 1.730493\n",
      "2024-12-14 04:57:13.236000: I runner.py:310] Step = 102200 ; steps/s = 1.63, tokens/s = 82258 (37846 source, 44412 target) ; Learning rate = 0.000276 ; Loss = 1.746506\n",
      "2024-12-14 04:58:14.245000: I runner.py:310] Step = 102300 ; steps/s = 1.64, tokens/s = 81477 (37483 source, 43994 target) ; Learning rate = 0.000276 ; Loss = 1.752346\n",
      "2024-12-14 04:59:15.689000: I runner.py:310] Step = 102400 ; steps/s = 1.63, tokens/s = 82236 (37810 source, 44426 target) ; Learning rate = 0.000276 ; Loss = 1.736816\n",
      "2024-12-14 05:00:17.056000: I runner.py:310] Step = 102500 ; steps/s = 1.63, tokens/s = 82363 (37894 source, 44469 target) ; Learning rate = 0.000276 ; Loss = 1.729986\n",
      "2024-12-14 05:01:18.525000: I runner.py:310] Step = 102600 ; steps/s = 1.63, tokens/s = 82203 (37800 source, 44403 target) ; Learning rate = 0.000276 ; Loss = 1.728262\n",
      "2024-12-14 05:02:19.478000: I runner.py:310] Step = 102700 ; steps/s = 1.64, tokens/s = 81538 (37497 source, 44041 target) ; Learning rate = 0.000276 ; Loss = 1.764413\n",
      "2024-12-14 05:03:20.885000: I runner.py:310] Step = 102800 ; steps/s = 1.63, tokens/s = 82304 (37867 source, 44437 target) ; Learning rate = 0.000276 ; Loss = 1.723591\n",
      "2024-12-14 05:04:22.267000: I runner.py:310] Step = 102900 ; steps/s = 1.63, tokens/s = 82300 (37832 source, 44468 target) ; Learning rate = 0.000276 ; Loss = 1.730989\n",
      "2024-12-14 05:05:23.224000: I runner.py:310] Step = 103000 ; steps/s = 1.64, tokens/s = 81536 (37510 source, 44026 target) ; Learning rate = 0.000275 ; Loss = 1.746283\n",
      "2024-12-14 05:06:24.602000: I runner.py:310] Step = 103100 ; steps/s = 1.63, tokens/s = 82329 (37862 source, 44467 target) ; Learning rate = 0.000275 ; Loss = 1.740292\n",
      "2024-12-14 05:07:26.000000: I runner.py:310] Step = 103200 ; steps/s = 1.63, tokens/s = 82317 (37859 source, 44458 target) ; Learning rate = 0.000275 ; Loss = 1.730218\n",
      "2024-12-14 05:08:27.413000: I runner.py:310] Step = 103300 ; steps/s = 1.63, tokens/s = 82303 (37867 source, 44436 target) ; Learning rate = 0.000275 ; Loss = 1.736650\n",
      "2024-12-14 05:09:28.424000: I runner.py:310] Step = 103400 ; steps/s = 1.64, tokens/s = 81448 (37455 source, 43993 target) ; Learning rate = 0.000275 ; Loss = 1.769308\n",
      "2024-12-14 05:10:29.826000: I runner.py:310] Step = 103500 ; steps/s = 1.63, tokens/s = 82296 (37846 source, 44450 target) ; Learning rate = 0.000275 ; Loss = 1.740291\n",
      "2024-12-14 05:11:31.241000: I runner.py:310] Step = 103600 ; steps/s = 1.63, tokens/s = 82294 (37866 source, 44428 target) ; Learning rate = 0.000275 ; Loss = 1.745016\n",
      "2024-12-14 05:12:32.215000: I runner.py:310] Step = 103700 ; steps/s = 1.64, tokens/s = 81510 (37476 source, 44034 target) ; Learning rate = 0.000274 ; Loss = 1.744946\n",
      "2024-12-14 05:13:33.621000: I runner.py:310] Step = 103800 ; steps/s = 1.63, tokens/s = 82292 (37846 source, 44446 target) ; Learning rate = 0.000274 ; Loss = 1.739727\n",
      "2024-12-14 05:14:35.054000: I runner.py:310] Step = 103900 ; steps/s = 1.63, tokens/s = 82253 (37811 source, 44442 target) ; Learning rate = 0.000274 ; Loss = 1.729686\n",
      "2024-12-14 05:15:36.422000: I runner.py:310] Step = 104000 ; steps/s = 1.63, tokens/s = 82355 (37894 source, 44461 target) ; Learning rate = 0.000274 ; Loss = 1.744199\n",
      "2024-12-14 05:16:37.431000: I runner.py:310] Step = 104100 ; steps/s = 1.64, tokens/s = 81495 (37508 source, 43987 target) ; Learning rate = 0.000274 ; Loss = 1.714590\n",
      "2024-12-14 05:17:38.762000: I runner.py:310] Step = 104200 ; steps/s = 1.63, tokens/s = 82422 (37923 source, 44499 target) ; Learning rate = 0.000274 ; Loss = 1.751533\n",
      "2024-12-14 05:18:40.108000: I runner.py:310] Step = 104300 ; steps/s = 1.63, tokens/s = 82350 (37857 source, 44493 target) ; Learning rate = 0.000274 ; Loss = 1.741064\n",
      "2024-12-14 05:19:41.027000: I runner.py:310] Step = 104400 ; steps/s = 1.64, tokens/s = 81575 (37511 source, 44064 target) ; Learning rate = 0.000274 ; Loss = 1.738364\n",
      "2024-12-14 05:20:42.365000: I runner.py:310] Step = 104500 ; steps/s = 1.63, tokens/s = 82381 (37891 source, 44490 target) ; Learning rate = 0.000273 ; Loss = 1.734910\n",
      "2024-12-14 05:21:43.684000: I runner.py:310] Step = 104600 ; steps/s = 1.63, tokens/s = 82417 (37900 source, 44517 target) ; Learning rate = 0.000273 ; Loss = 1.730441\n",
      "2024-12-14 05:22:45.057000: I runner.py:310] Step = 104700 ; steps/s = 1.63, tokens/s = 82332 (37873 source, 44459 target) ; Learning rate = 0.000273 ; Loss = 1.734382\n",
      "2024-12-14 05:23:45.962000: I runner.py:310] Step = 104800 ; steps/s = 1.64, tokens/s = 81602 (37522 source, 44080 target) ; Learning rate = 0.000273 ; Loss = 1.763674\n",
      "2024-12-14 05:24:47.333000: I runner.py:310] Step = 104900 ; steps/s = 1.63, tokens/s = 82342 (37887 source, 44455 target) ; Learning rate = 0.000273 ; Loss = 1.734385\n",
      "2024-12-14 05:25:48.714000: I runner.py:310] Step = 105000 ; steps/s = 1.63, tokens/s = 82311 (37835 source, 44476 target) ; Learning rate = 0.000273 ; Loss = 1.745075\n",
      "2024-12-14 05:25:48.715000: I training.py:192] Running evaluation for step 105000\n",
      "2024-12-14 05:27:54.503000: I training.py:192] Evaluation result for step 105000: loss = 2.566836 ; perplexity = 13.024548\n",
      "2024-12-14 05:28:55.335000: I runner.py:310] Step = 105100 ; steps/s = 1.64, tokens/s = 81736 (37608 source, 44128 target) ; Learning rate = 0.000273 ; Loss = 1.730454\n",
      "2024-12-14 05:29:56.733000: I runner.py:310] Step = 105200 ; steps/s = 1.63, tokens/s = 82327 (37870 source, 44457 target) ; Learning rate = 0.000273 ; Loss = 1.719205\n",
      "2024-12-14 05:30:58.121000: I runner.py:310] Step = 105300 ; steps/s = 1.63, tokens/s = 82337 (37877 source, 44460 target) ; Learning rate = 0.000272 ; Loss = 1.752504\n",
      "2024-12-14 05:31:59.535000: I runner.py:310] Step = 105400 ; steps/s = 1.63, tokens/s = 82270 (37834 source, 44436 target) ; Learning rate = 0.000272 ; Loss = 1.759736\n",
      "2024-12-14 05:33:00.459000: I runner.py:310] Step = 105500 ; steps/s = 1.64, tokens/s = 81576 (37519 source, 44057 target) ; Learning rate = 0.000272 ; Loss = 1.714059\n",
      "2024-12-14 05:34:01.783000: I runner.py:310] Step = 105600 ; steps/s = 1.63, tokens/s = 82408 (37898 source, 44510 target) ; Learning rate = 0.000272 ; Loss = 1.744444\n",
      "2024-12-14 05:35:03.200000: I runner.py:310] Step = 105700 ; steps/s = 1.63, tokens/s = 82257 (37837 source, 44420 target) ; Learning rate = 0.000272 ; Loss = 1.746694\n",
      "2024-12-14 05:36:04.229000: I runner.py:310] Step = 105800 ; steps/s = 1.64, tokens/s = 81456 (37456 source, 44000 target) ; Learning rate = 0.000272 ; Loss = 1.744669\n",
      "2024-12-14 05:37:05.588000: I runner.py:310] Step = 105900 ; steps/s = 1.63, tokens/s = 82376 (37909 source, 44467 target) ; Learning rate = 0.000272 ; Loss = 1.737090\n",
      "2024-12-14 05:38:06.983000: I runner.py:310] Step = 106000 ; steps/s = 1.63, tokens/s = 82300 (37864 source, 44436 target) ; Learning rate = 0.000271 ; Loss = 1.736360\n",
      "2024-12-14 05:39:08.380000: I runner.py:310] Step = 106100 ; steps/s = 1.63, tokens/s = 82295 (37822 source, 44473 target) ; Learning rate = 0.000271 ; Loss = 1.744706\n",
      "2024-12-14 05:40:09.436000: I runner.py:310] Step = 106200 ; steps/s = 1.64, tokens/s = 81403 (37428 source, 43975 target) ; Learning rate = 0.000271 ; Loss = 1.710650\n",
      "2024-12-14 05:41:10.833000: I runner.py:310] Step = 106300 ; steps/s = 1.63, tokens/s = 82291 (37849 source, 44442 target) ; Learning rate = 0.000271 ; Loss = 1.767628\n",
      "2024-12-14 05:42:12.178000: I runner.py:310] Step = 106400 ; steps/s = 1.63, tokens/s = 82384 (37886 source, 44498 target) ; Learning rate = 0.000271 ; Loss = 1.754482\n",
      "2024-12-14 05:43:13.126000: I runner.py:310] Step = 106500 ; steps/s = 1.64, tokens/s = 81572 (37532 source, 44040 target) ; Learning rate = 0.000271 ; Loss = 1.728115\n",
      "2024-12-14 05:44:14.521000: I runner.py:310] Step = 106600 ; steps/s = 1.63, tokens/s = 82293 (37844 source, 44449 target) ; Learning rate = 0.000271 ; Loss = 1.738613\n",
      "2024-12-14 05:45:15.897000: I runner.py:310] Step = 106700 ; steps/s = 1.63, tokens/s = 82361 (37895 source, 44466 target) ; Learning rate = 0.000271 ; Loss = 1.737144\n",
      "2024-12-14 05:46:17.306000: I runner.py:310] Step = 106800 ; steps/s = 1.63, tokens/s = 82275 (37833 source, 44442 target) ; Learning rate = 0.000270 ; Loss = 1.750126\n",
      "2024-12-14 05:47:18.307000: I runner.py:310] Step = 106900 ; steps/s = 1.64, tokens/s = 81474 (37474 source, 44000 target) ; Learning rate = 0.000270 ; Loss = 1.746356\n",
      "2024-12-14 05:48:19.712000: I runner.py:310] Step = 107000 ; steps/s = 1.63, tokens/s = 82293 (37851 source, 44442 target) ; Learning rate = 0.000270 ; Loss = 1.734104\n",
      "2024-12-14 05:49:21.141000: I runner.py:310] Step = 107100 ; steps/s = 1.63, tokens/s = 82287 (37838 source, 44449 target) ; Learning rate = 0.000270 ; Loss = 1.722006\n",
      "2024-12-14 05:50:22.120000: I runner.py:310] Step = 107200 ; steps/s = 1.64, tokens/s = 81504 (37490 source, 44014 target) ; Learning rate = 0.000270 ; Loss = 1.728675\n",
      "2024-12-14 05:51:23.479000: I runner.py:310] Step = 107300 ; steps/s = 1.63, tokens/s = 82398 (37926 source, 44472 target) ; Learning rate = 0.000270 ; Loss = 1.736316\n",
      "2024-12-14 05:52:24.809000: I runner.py:310] Step = 107400 ; steps/s = 1.63, tokens/s = 82371 (37873 source, 44498 target) ; Learning rate = 0.000270 ; Loss = 1.753969\n",
      "2024-12-14 05:53:26.197000: I runner.py:310] Step = 107500 ; steps/s = 1.63, tokens/s = 82306 (37849 source, 44457 target) ; Learning rate = 0.000270 ; Loss = 1.742350\n",
      "2024-12-14 05:54:27.154000: I runner.py:310] Step = 107600 ; steps/s = 1.64, tokens/s = 81537 (37506 source, 44031 target) ; Learning rate = 0.000269 ; Loss = 1.749680\n",
      "2024-12-14 05:55:28.487000: I runner.py:310] Step = 107700 ; steps/s = 1.63, tokens/s = 82395 (37902 source, 44493 target) ; Learning rate = 0.000269 ; Loss = 1.732522\n",
      "2024-12-14 05:56:29.874000: I runner.py:310] Step = 107800 ; steps/s = 1.63, tokens/s = 82346 (37877 source, 44469 target) ; Learning rate = 0.000269 ; Loss = 1.720872\n",
      "2024-12-14 05:57:30.818000: I runner.py:310] Step = 107900 ; steps/s = 1.64, tokens/s = 81528 (37482 source, 44046 target) ; Learning rate = 0.000269 ; Loss = 1.730831\n",
      "2024-12-14 05:58:32.197000: I runner.py:310] Step = 108000 ; steps/s = 1.63, tokens/s = 82375 (37902 source, 44473 target) ; Learning rate = 0.000269 ; Loss = 1.733064\n",
      "2024-12-14 05:59:33.593000: I runner.py:310] Step = 108100 ; steps/s = 1.63, tokens/s = 82278 (37835 source, 44443 target) ; Learning rate = 0.000269 ; Loss = 1.732426\n",
      "2024-12-14 06:00:34.961000: I runner.py:310] Step = 108200 ; steps/s = 1.63, tokens/s = 82335 (37859 source, 44476 target) ; Learning rate = 0.000269 ; Loss = 1.746518\n",
      "2024-12-14 06:01:35.963000: I runner.py:310] Step = 108300 ; steps/s = 1.64, tokens/s = 81491 (37489 source, 44002 target) ; Learning rate = 0.000269 ; Loss = 1.724627\n",
      "2024-12-14 06:02:37.340000: I runner.py:310] Step = 108400 ; steps/s = 1.63, tokens/s = 82345 (37877 source, 44468 target) ; Learning rate = 0.000268 ; Loss = 1.728554\n",
      "2024-12-14 06:03:38.724000: I runner.py:310] Step = 108500 ; steps/s = 1.63, tokens/s = 82295 (37835 source, 44460 target) ; Learning rate = 0.000268 ; Loss = 1.747868\n",
      "2024-12-14 06:04:39.677000: I runner.py:310] Step = 108600 ; steps/s = 1.64, tokens/s = 81531 (37488 source, 44043 target) ; Learning rate = 0.000268 ; Loss = 1.739428\n",
      "2024-12-14 06:05:41.029000: I runner.py:310] Step = 108700 ; steps/s = 1.63, tokens/s = 82373 (37891 source, 44482 target) ; Learning rate = 0.000268 ; Loss = 1.716713\n",
      "2024-12-14 06:06:42.374000: I runner.py:310] Step = 108800 ; steps/s = 1.63, tokens/s = 82391 (37892 source, 44499 target) ; Learning rate = 0.000268 ; Loss = 1.726976\n",
      "2024-12-14 06:07:43.517000: I runner.py:310] Step = 108900 ; steps/s = 1.64, tokens/s = 81855 (37659 source, 44196 target) ; Learning rate = 0.000268 ; Loss = 1.740684\n",
      "2024-12-14 06:08:44.785000: I runner.py:310] Step = 109000 ; steps/s = 1.63, tokens/s = 81905 (37666 source, 44239 target) ; Learning rate = 0.000268 ; Loss = 1.720663\n",
      "2024-12-14 06:09:46.199000: I runner.py:310] Step = 109100 ; steps/s = 1.63, tokens/s = 82294 (37844 source, 44450 target) ; Learning rate = 0.000268 ; Loss = 1.739075\n",
      "2024-12-14 06:10:47.513000: I runner.py:310] Step = 109200 ; steps/s = 1.63, tokens/s = 82435 (37935 source, 44500 target) ; Learning rate = 0.000267 ; Loss = 1.744260\n",
      "2024-12-14 06:11:48.563000: I runner.py:310] Step = 109300 ; steps/s = 1.64, tokens/s = 81404 (37435 source, 43969 target) ; Learning rate = 0.000267 ; Loss = 1.749880\n",
      "2024-12-14 06:12:49.871000: I runner.py:310] Step = 109400 ; steps/s = 1.63, tokens/s = 82440 (37943 source, 44497 target) ; Learning rate = 0.000267 ; Loss = 1.725604\n",
      "2024-12-14 06:13:51.284000: I runner.py:310] Step = 109500 ; steps/s = 1.63, tokens/s = 82271 (37831 source, 44440 target) ; Learning rate = 0.000267 ; Loss = 1.728400\n",
      "2024-12-14 06:14:52.256000: I runner.py:310] Step = 109600 ; steps/s = 1.64, tokens/s = 81520 (37478 source, 44042 target) ; Learning rate = 0.000267 ; Loss = 1.734051\n",
      "2024-12-14 06:15:53.635000: I runner.py:310] Step = 109700 ; steps/s = 1.63, tokens/s = 82367 (37898 source, 44469 target) ; Learning rate = 0.000267 ; Loss = 1.724073\n",
      "2024-12-14 06:16:55.039000: I runner.py:310] Step = 109800 ; steps/s = 1.63, tokens/s = 82287 (37844 source, 44443 target) ; Learning rate = 0.000267 ; Loss = 1.746418\n",
      "2024-12-14 06:17:56.349000: I runner.py:310] Step = 109900 ; steps/s = 1.63, tokens/s = 82377 (37873 source, 44504 target) ; Learning rate = 0.000267 ; Loss = 1.747493\n",
      "2024-12-14 06:18:57.366000: I runner.py:310] Step = 110000 ; steps/s = 1.64, tokens/s = 81481 (37486 source, 43995 target) ; Learning rate = 0.000266 ; Loss = 1.714199\n",
      "2024-12-14 06:18:59.987000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-110000\n",
      "2024-12-14 06:18:59.987000: I training.py:192] Running evaluation for step 110000\n",
      "2024-12-14 06:21:08.130000: I training.py:192] Evaluation result for step 110000: loss = 2.584208 ; perplexity = 13.252786\n",
      "2024-12-14 06:22:09.397000: I runner.py:310] Step = 110100 ; steps/s = 1.63, tokens/s = 82546 (37960 source, 44586 target) ; Learning rate = 0.000266 ; Loss = 1.734523\n",
      "2024-12-14 06:23:10.818000: I runner.py:310] Step = 110200 ; steps/s = 1.63, tokens/s = 82273 (37839 source, 44434 target) ; Learning rate = 0.000266 ; Loss = 1.750503\n",
      "2024-12-14 06:24:11.844000: I runner.py:310] Step = 110300 ; steps/s = 1.64, tokens/s = 81459 (37467 source, 43992 target) ; Learning rate = 0.000266 ; Loss = 1.731946\n",
      "2024-12-14 06:25:13.270000: I runner.py:310] Step = 110400 ; steps/s = 1.63, tokens/s = 82279 (37860 source, 44419 target) ; Learning rate = 0.000266 ; Loss = 1.727720\n",
      "2024-12-14 06:26:14.683000: I runner.py:310] Step = 110500 ; steps/s = 1.63, tokens/s = 82273 (37836 source, 44437 target) ; Learning rate = 0.000266 ; Loss = 1.739210\n",
      "2024-12-14 06:27:16.036000: I runner.py:310] Step = 110600 ; steps/s = 1.63, tokens/s = 82361 (37862 source, 44499 target) ; Learning rate = 0.000266 ; Loss = 1.748621\n",
      "2024-12-14 06:28:16.979000: I runner.py:310] Step = 110700 ; steps/s = 1.64, tokens/s = 81560 (37509 source, 44051 target) ; Learning rate = 0.000266 ; Loss = 1.749132\n",
      "2024-12-14 06:29:18.373000: I runner.py:310] Step = 110800 ; steps/s = 1.63, tokens/s = 82297 (37846 source, 44451 target) ; Learning rate = 0.000266 ; Loss = 1.729866\n",
      "2024-12-14 06:30:19.826000: I runner.py:310] Step = 110900 ; steps/s = 1.63, tokens/s = 82259 (37837 source, 44422 target) ; Learning rate = 0.000265 ; Loss = 1.728400\n",
      "2024-12-14 06:31:20.786000: I runner.py:310] Step = 111000 ; steps/s = 1.64, tokens/s = 81532 (37506 source, 44026 target) ; Learning rate = 0.000265 ; Loss = 1.736168\n",
      "2024-12-14 06:32:22.130000: I runner.py:310] Step = 111100 ; steps/s = 1.63, tokens/s = 82369 (37859 source, 44510 target) ; Learning rate = 0.000265 ; Loss = 1.729917\n",
      "2024-12-14 06:33:23.552000: I runner.py:310] Step = 111200 ; steps/s = 1.63, tokens/s = 82309 (37877 source, 44432 target) ; Learning rate = 0.000265 ; Loss = 1.720465\n",
      "2024-12-14 06:34:24.950000: I runner.py:310] Step = 111300 ; steps/s = 1.63, tokens/s = 82302 (37869 source, 44433 target) ; Learning rate = 0.000265 ; Loss = 1.716238\n",
      "2024-12-14 06:35:25.948000: I runner.py:310] Step = 111400 ; steps/s = 1.64, tokens/s = 81443 (37437 source, 44006 target) ; Learning rate = 0.000265 ; Loss = 1.749415\n",
      "2024-12-14 06:36:27.347000: I runner.py:310] Step = 111500 ; steps/s = 1.63, tokens/s = 82281 (37832 source, 44449 target) ; Learning rate = 0.000265 ; Loss = 1.719419\n",
      "2024-12-14 06:37:28.680000: I runner.py:310] Step = 111600 ; steps/s = 1.63, tokens/s = 82424 (37926 source, 44498 target) ; Learning rate = 0.000265 ; Loss = 1.730353\n",
      "2024-12-14 06:38:29.730000: I runner.py:310] Step = 111700 ; steps/s = 1.64, tokens/s = 81425 (37458 source, 43967 target) ; Learning rate = 0.000264 ; Loss = 1.740439\n",
      "2024-12-14 06:39:31.135000: I runner.py:310] Step = 111800 ; steps/s = 1.63, tokens/s = 82296 (37856 source, 44440 target) ; Learning rate = 0.000264 ; Loss = 1.734874\n",
      "2024-12-14 06:40:32.508000: I runner.py:310] Step = 111900 ; steps/s = 1.63, tokens/s = 82342 (37873 source, 44469 target) ; Learning rate = 0.000264 ; Loss = 1.713211\n",
      "2024-12-14 06:41:33.987000: I runner.py:310] Step = 112000 ; steps/s = 1.63, tokens/s = 82202 (37817 source, 44385 target) ; Learning rate = 0.000264 ; Loss = 1.735330\n",
      "2024-12-14 06:42:35.022000: I runner.py:310] Step = 112100 ; steps/s = 1.64, tokens/s = 81442 (37452 source, 43990 target) ; Learning rate = 0.000264 ; Loss = 1.702131\n",
      "2024-12-14 06:43:36.392000: I runner.py:310] Step = 112200 ; steps/s = 1.63, tokens/s = 82334 (37863 source, 44471 target) ; Learning rate = 0.000264 ; Loss = 1.731987\n",
      "2024-12-14 06:44:37.771000: I runner.py:310] Step = 112300 ; steps/s = 1.63, tokens/s = 82342 (37875 source, 44467 target) ; Learning rate = 0.000264 ; Loss = 1.748487\n",
      "2024-12-14 06:45:38.805000: I runner.py:310] Step = 112400 ; steps/s = 1.64, tokens/s = 81420 (37441 source, 43979 target) ; Learning rate = 0.000264 ; Loss = 1.743370\n",
      "2024-12-14 06:46:40.185000: I runner.py:310] Step = 112500 ; steps/s = 1.63, tokens/s = 82319 (37864 source, 44455 target) ; Learning rate = 0.000264 ; Loss = 1.725008\n",
      "2024-12-14 06:47:41.595000: I runner.py:310] Step = 112600 ; steps/s = 1.63, tokens/s = 82294 (37842 source, 44452 target) ; Learning rate = 0.000263 ; Loss = 1.726749\n",
      "2024-12-14 06:48:42.992000: I runner.py:310] Step = 112700 ; steps/s = 1.63, tokens/s = 82314 (37861 source, 44453 target) ; Learning rate = 0.000263 ; Loss = 1.721147\n",
      "2024-12-14 06:49:44.013000: I runner.py:310] Step = 112800 ; steps/s = 1.64, tokens/s = 81469 (37480 source, 43989 target) ; Learning rate = 0.000263 ; Loss = 1.707290\n",
      "2024-12-14 06:50:45.349000: I runner.py:310] Step = 112900 ; steps/s = 1.63, tokens/s = 82416 (37917 source, 44499 target) ; Learning rate = 0.000263 ; Loss = 1.728625\n",
      "2024-12-14 06:51:46.822000: I runner.py:310] Step = 113000 ; steps/s = 1.63, tokens/s = 82199 (37811 source, 44388 target) ; Learning rate = 0.000263 ; Loss = 1.729271\n",
      "2024-12-14 06:52:47.721000: I runner.py:310] Step = 113100 ; steps/s = 1.64, tokens/s = 81599 (37509 source, 44090 target) ; Learning rate = 0.000263 ; Loss = 1.714083\n",
      "2024-12-14 06:53:49.070000: I runner.py:310] Step = 113200 ; steps/s = 1.63, tokens/s = 82366 (37881 source, 44485 target) ; Learning rate = 0.000263 ; Loss = 1.728861\n",
      "2024-12-14 06:54:50.440000: I runner.py:310] Step = 113300 ; steps/s = 1.63, tokens/s = 82330 (37862 source, 44468 target) ; Learning rate = 0.000263 ; Loss = 1.739530\n",
      "2024-12-14 06:55:51.860000: I runner.py:310] Step = 113400 ; steps/s = 1.63, tokens/s = 82279 (37856 source, 44423 target) ; Learning rate = 0.000262 ; Loss = 1.734283\n",
      "2024-12-14 06:56:52.785000: I runner.py:310] Step = 113500 ; steps/s = 1.64, tokens/s = 81576 (37504 source, 44072 target) ; Learning rate = 0.000262 ; Loss = 1.748502\n",
      "2024-12-14 06:57:54.117000: I runner.py:310] Step = 113600 ; steps/s = 1.63, tokens/s = 82413 (37931 source, 44482 target) ; Learning rate = 0.000262 ; Loss = 1.724708\n",
      "2024-12-14 06:58:55.508000: I runner.py:310] Step = 113700 ; steps/s = 1.63, tokens/s = 82293 (37830 source, 44463 target) ; Learning rate = 0.000262 ; Loss = 1.729785\n",
      "2024-12-14 06:59:56.595000: I runner.py:310] Step = 113800 ; steps/s = 1.64, tokens/s = 81350 (37405 source, 43945 target) ; Learning rate = 0.000262 ; Loss = 1.737767\n",
      "2024-12-14 07:00:57.976000: I runner.py:310] Step = 113900 ; steps/s = 1.63, tokens/s = 82352 (37889 source, 44463 target) ; Learning rate = 0.000262 ; Loss = 1.724670\n",
      "2024-12-14 07:01:59.428000: I runner.py:310] Step = 114000 ; steps/s = 1.63, tokens/s = 82232 (37821 source, 44411 target) ; Learning rate = 0.000262 ; Loss = 1.730619\n",
      "2024-12-14 07:03:00.822000: I runner.py:310] Step = 114100 ; steps/s = 1.63, tokens/s = 82303 (37852 source, 44451 target) ; Learning rate = 0.000262 ; Loss = 1.728867\n",
      "2024-12-14 07:04:01.809000: I runner.py:310] Step = 114200 ; steps/s = 1.64, tokens/s = 81522 (37498 source, 44024 target) ; Learning rate = 0.000262 ; Loss = 1.747657\n",
      "2024-12-14 07:05:03.232000: I runner.py:310] Step = 114300 ; steps/s = 1.63, tokens/s = 82273 (37837 source, 44436 target) ; Learning rate = 0.000261 ; Loss = 1.725287\n",
      "2024-12-14 07:06:04.697000: I runner.py:310] Step = 114400 ; steps/s = 1.63, tokens/s = 82210 (37814 source, 44396 target) ; Learning rate = 0.000261 ; Loss = 1.725634\n",
      "2024-12-14 07:07:05.739000: I runner.py:310] Step = 114500 ; steps/s = 1.64, tokens/s = 81417 (37443 source, 43974 target) ; Learning rate = 0.000261 ; Loss = 1.735908\n",
      "2024-12-14 07:08:07.157000: I runner.py:310] Step = 114600 ; steps/s = 1.63, tokens/s = 82255 (37814 source, 44441 target) ; Learning rate = 0.000261 ; Loss = 1.727918\n",
      "2024-12-14 07:09:08.503000: I runner.py:310] Step = 114700 ; steps/s = 1.63, tokens/s = 82356 (37878 source, 44478 target) ; Learning rate = 0.000261 ; Loss = 1.726006\n",
      "2024-12-14 07:10:09.853000: I runner.py:310] Step = 114800 ; steps/s = 1.63, tokens/s = 82413 (37919 source, 44494 target) ; Learning rate = 0.000261 ; Loss = 1.732563\n",
      "2024-12-14 07:11:10.812000: I runner.py:310] Step = 114900 ; steps/s = 1.64, tokens/s = 81580 (37552 source, 44028 target) ; Learning rate = 0.000261 ; Loss = 1.711859\n",
      "2024-12-14 07:12:12.211000: I runner.py:310] Step = 115000 ; steps/s = 1.63, tokens/s = 82303 (37840 source, 44463 target) ; Learning rate = 0.000261 ; Loss = 1.739892\n",
      "2024-12-14 07:12:12.212000: I training.py:192] Running evaluation for step 115000\n",
      "2024-12-14 07:14:17.395000: I training.py:192] Evaluation result for step 115000: loss = 2.582053 ; perplexity = 13.224262\n",
      "2024-12-14 07:15:18.586000: I runner.py:310] Step = 115100 ; steps/s = 1.63, tokens/s = 82578 (37973 source, 44605 target) ; Learning rate = 0.000261 ; Loss = 1.737118\n",
      "2024-12-14 07:16:19.563000: I runner.py:310] Step = 115200 ; steps/s = 1.64, tokens/s = 81494 (37475 source, 44019 target) ; Learning rate = 0.000260 ; Loss = 1.743272\n",
      "2024-12-14 07:17:20.955000: I runner.py:310] Step = 115300 ; steps/s = 1.63, tokens/s = 82334 (37860 source, 44474 target) ; Learning rate = 0.000260 ; Loss = 1.717760\n",
      "2024-12-14 07:18:22.426000: I runner.py:310] Step = 115400 ; steps/s = 1.63, tokens/s = 82193 (37800 source, 44393 target) ; Learning rate = 0.000260 ; Loss = 1.727063\n",
      "2024-12-14 07:19:23.861000: I runner.py:310] Step = 115500 ; steps/s = 1.63, tokens/s = 82262 (37851 source, 44411 target) ; Learning rate = 0.000260 ; Loss = 1.726324\n",
      "2024-12-14 07:20:24.915000: I runner.py:310] Step = 115600 ; steps/s = 1.64, tokens/s = 81383 (37419 source, 43964 target) ; Learning rate = 0.000260 ; Loss = 1.738611\n",
      "2024-12-14 07:21:26.251000: I runner.py:310] Step = 115700 ; steps/s = 1.63, tokens/s = 82395 (37897 source, 44498 target) ; Learning rate = 0.000260 ; Loss = 1.715701\n",
      "2024-12-14 07:22:27.666000: I runner.py:310] Step = 115800 ; steps/s = 1.63, tokens/s = 82307 (37867 source, 44440 target) ; Learning rate = 0.000260 ; Loss = 1.718712\n",
      "2024-12-14 07:23:28.669000: I runner.py:310] Step = 115900 ; steps/s = 1.64, tokens/s = 81474 (37480 source, 43994 target) ; Learning rate = 0.000260 ; Loss = 1.701674\n",
      "2024-12-14 07:24:30.115000: I runner.py:310] Step = 116000 ; steps/s = 1.63, tokens/s = 82261 (37847 source, 44414 target) ; Learning rate = 0.000260 ; Loss = 1.733803\n",
      "2024-12-14 07:25:31.500000: I runner.py:310] Step = 116100 ; steps/s = 1.63, tokens/s = 82292 (37832 source, 44460 target) ; Learning rate = 0.000259 ; Loss = 1.727438\n",
      "2024-12-14 07:26:32.918000: I runner.py:310] Step = 116200 ; steps/s = 1.63, tokens/s = 82280 (37843 source, 44437 target) ; Learning rate = 0.000259 ; Loss = 1.741124\n",
      "2024-12-14 07:27:33.946000: I runner.py:310] Step = 116300 ; steps/s = 1.64, tokens/s = 81432 (37451 source, 43981 target) ; Learning rate = 0.000259 ; Loss = 1.739287\n",
      "2024-12-14 07:28:35.377000: I runner.py:310] Step = 116400 ; steps/s = 1.63, tokens/s = 82267 (37832 source, 44435 target) ; Learning rate = 0.000259 ; Loss = 1.719746\n",
      "2024-12-14 07:29:36.753000: I runner.py:310] Step = 116500 ; steps/s = 1.63, tokens/s = 82332 (37880 source, 44452 target) ; Learning rate = 0.000259 ; Loss = 1.726361\n",
      "2024-12-14 07:30:37.795000: I runner.py:310] Step = 116600 ; steps/s = 1.64, tokens/s = 81444 (37461 source, 43983 target) ; Learning rate = 0.000259 ; Loss = 1.713284\n",
      "2024-12-14 07:31:39.257000: I runner.py:310] Step = 116700 ; steps/s = 1.63, tokens/s = 82224 (37820 source, 44404 target) ; Learning rate = 0.000259 ; Loss = 1.726987\n",
      "2024-12-14 07:32:40.699000: I runner.py:310] Step = 116800 ; steps/s = 1.63, tokens/s = 82247 (37832 source, 44415 target) ; Learning rate = 0.000259 ; Loss = 1.743639\n",
      "2024-12-14 07:33:42.075000: I runner.py:310] Step = 116900 ; steps/s = 1.63, tokens/s = 82328 (37854 source, 44474 target) ; Learning rate = 0.000259 ; Loss = 1.745926\n",
      "2024-12-14 07:34:42.997000: I runner.py:310] Step = 117000 ; steps/s = 1.64, tokens/s = 81593 (37537 source, 44056 target) ; Learning rate = 0.000258 ; Loss = 1.712722\n",
      "2024-12-14 07:35:44.415000: I runner.py:310] Step = 117100 ; steps/s = 1.63, tokens/s = 82258 (37817 source, 44441 target) ; Learning rate = 0.000258 ; Loss = 1.730101\n",
      "2024-12-14 07:36:45.868000: I runner.py:310] Step = 117200 ; steps/s = 1.63, tokens/s = 82250 (37846 source, 44404 target) ; Learning rate = 0.000258 ; Loss = 1.736030\n",
      "2024-12-14 07:37:46.851000: I runner.py:310] Step = 117300 ; steps/s = 1.64, tokens/s = 81486 (37463 source, 44023 target) ; Learning rate = 0.000258 ; Loss = 1.724663\n",
      "2024-12-14 07:38:48.286000: I runner.py:310] Step = 117400 ; steps/s = 1.63, tokens/s = 82245 (37826 source, 44419 target) ; Learning rate = 0.000258 ; Loss = 1.730699\n",
      "2024-12-14 07:39:49.765000: I runner.py:310] Step = 117500 ; steps/s = 1.63, tokens/s = 82219 (37823 source, 44396 target) ; Learning rate = 0.000258 ; Loss = 1.731143\n",
      "2024-12-14 07:40:50.802000: I runner.py:310] Step = 117600 ; steps/s = 1.64, tokens/s = 81438 (37458 source, 43980 target) ; Learning rate = 0.000258 ; Loss = 1.744089\n",
      "2024-12-14 07:41:52.265000: I runner.py:310] Step = 117700 ; steps/s = 1.63, tokens/s = 82202 (37795 source, 44407 target) ; Learning rate = 0.000258 ; Loss = 1.743154\n",
      "2024-12-14 07:42:53.673000: I runner.py:310] Step = 117800 ; steps/s = 1.63, tokens/s = 82295 (37860 source, 44435 target) ; Learning rate = 0.000258 ; Loss = 1.718895\n",
      "2024-12-14 07:43:55.131000: I runner.py:310] Step = 117900 ; steps/s = 1.63, tokens/s = 82225 (37809 source, 44416 target) ; Learning rate = 0.000257 ; Loss = 1.721228\n",
      "2024-12-14 07:44:56.124000: I runner.py:310] Step = 118000 ; steps/s = 1.64, tokens/s = 81482 (37498 source, 43984 target) ; Learning rate = 0.000257 ; Loss = 1.738090\n",
      "2024-12-14 07:45:57.492000: I runner.py:310] Step = 118100 ; steps/s = 1.63, tokens/s = 82299 (37816 source, 44483 target) ; Learning rate = 0.000257 ; Loss = 1.729172\n",
      "2024-12-14 07:46:58.957000: I runner.py:310] Step = 118200 ; steps/s = 1.63, tokens/s = 82237 (37833 source, 44404 target) ; Learning rate = 0.000257 ; Loss = 1.704778\n",
      "2024-12-14 07:47:59.942000: I runner.py:310] Step = 118300 ; steps/s = 1.64, tokens/s = 81542 (37525 source, 44017 target) ; Learning rate = 0.000257 ; Loss = 1.728807\n",
      "2024-12-14 07:49:01.309000: I runner.py:310] Step = 118400 ; steps/s = 1.63, tokens/s = 82360 (37869 source, 44491 target) ; Learning rate = 0.000257 ; Loss = 1.717706\n",
      "2024-12-14 07:50:02.726000: I runner.py:310] Step = 118500 ; steps/s = 1.63, tokens/s = 82280 (37842 source, 44438 target) ; Learning rate = 0.000257 ; Loss = 1.731717\n",
      "2024-12-14 07:51:04.154000: I runner.py:310] Step = 118600 ; steps/s = 1.63, tokens/s = 82264 (37839 source, 44425 target) ; Learning rate = 0.000257 ; Loss = 1.732686\n",
      "2024-12-14 07:52:05.180000: I runner.py:310] Step = 118700 ; steps/s = 1.64, tokens/s = 81427 (37451 source, 43976 target) ; Learning rate = 0.000257 ; Loss = 1.738150\n",
      "2024-12-14 07:53:06.565000: I runner.py:310] Step = 118800 ; steps/s = 1.63, tokens/s = 82309 (37840 source, 44469 target) ; Learning rate = 0.000256 ; Loss = 1.711473\n",
      "2024-12-14 07:54:07.984000: I runner.py:310] Step = 118900 ; steps/s = 1.63, tokens/s = 82305 (37874 source, 44431 target) ; Learning rate = 0.000256 ; Loss = 1.726958\n",
      "2024-12-14 07:55:09.004000: I runner.py:310] Step = 119000 ; steps/s = 1.64, tokens/s = 81459 (37469 source, 43990 target) ; Learning rate = 0.000256 ; Loss = 1.715324\n",
      "2024-12-14 07:56:10.459000: I runner.py:310] Step = 119100 ; steps/s = 1.63, tokens/s = 82237 (37828 source, 44409 target) ; Learning rate = 0.000256 ; Loss = 1.722705\n",
      "2024-12-14 07:57:11.923000: I runner.py:310] Step = 119200 ; steps/s = 1.63, tokens/s = 82221 (37815 source, 44406 target) ; Learning rate = 0.000256 ; Loss = 1.732358\n",
      "2024-12-14 07:58:13.370000: I runner.py:310] Step = 119300 ; steps/s = 1.63, tokens/s = 82227 (37817 source, 44410 target) ; Learning rate = 0.000256 ; Loss = 1.742143\n",
      "2024-12-14 07:59:14.319000: I runner.py:310] Step = 119400 ; steps/s = 1.64, tokens/s = 81541 (37491 source, 44050 target) ; Learning rate = 0.000256 ; Loss = 1.746732\n",
      "2024-12-14 08:00:15.688000: I runner.py:310] Step = 119500 ; steps/s = 1.63, tokens/s = 82364 (37903 source, 44461 target) ; Learning rate = 0.000256 ; Loss = 1.727778\n",
      "2024-12-14 08:01:17.157000: I runner.py:310] Step = 119600 ; steps/s = 1.63, tokens/s = 82209 (37815 source, 44394 target) ; Learning rate = 0.000256 ; Loss = 1.728168\n",
      "2024-12-14 08:02:18.174000: I runner.py:310] Step = 119700 ; steps/s = 1.64, tokens/s = 81455 (37454 source, 44001 target) ; Learning rate = 0.000255 ; Loss = 1.714319\n",
      "2024-12-14 08:03:19.602000: I runner.py:310] Step = 119800 ; steps/s = 1.63, tokens/s = 82266 (37834 source, 44432 target) ; Learning rate = 0.000255 ; Loss = 1.717447\n",
      "2024-12-14 08:04:21.026000: I runner.py:310] Step = 119900 ; steps/s = 1.63, tokens/s = 82263 (37837 source, 44426 target) ; Learning rate = 0.000255 ; Loss = 1.729836\n",
      "2024-12-14 08:05:22.371000: I runner.py:310] Step = 120000 ; steps/s = 1.63, tokens/s = 82392 (37901 source, 44491 target) ; Learning rate = 0.000255 ; Loss = 1.749404\n",
      "2024-12-14 08:05:24.525000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-120000\n",
      "2024-12-14 08:05:24.525000: I training.py:192] Running evaluation for step 120000\n",
      "2024-12-14 08:07:31.677000: I training.py:192] Evaluation result for step 120000: loss = 2.582030 ; perplexity = 13.223949\n",
      "2024-12-14 08:08:32.597000: I runner.py:310] Step = 120100 ; steps/s = 1.64, tokens/s = 81651 (37550 source, 44101 target) ; Learning rate = 0.000255 ; Loss = 1.703201\n",
      "2024-12-14 08:09:34.084000: I runner.py:310] Step = 120200 ; steps/s = 1.63, tokens/s = 82202 (37811 source, 44391 target) ; Learning rate = 0.000255 ; Loss = 1.723983\n",
      "2024-12-14 08:10:35.530000: I runner.py:310] Step = 120300 ; steps/s = 1.63, tokens/s = 82227 (37813 source, 44414 target) ; Learning rate = 0.000255 ; Loss = 1.747403\n",
      "2024-12-14 08:11:36.629000: I runner.py:310] Step = 120400 ; steps/s = 1.64, tokens/s = 81356 (37423 source, 43933 target) ; Learning rate = 0.000255 ; Loss = 1.722172\n",
      "2024-12-14 08:12:38.111000: I runner.py:310] Step = 120500 ; steps/s = 1.63, tokens/s = 82215 (37828 source, 44387 target) ; Learning rate = 0.000255 ; Loss = 1.723358\n",
      "2024-12-14 08:13:39.457000: I runner.py:310] Step = 120600 ; steps/s = 1.63, tokens/s = 82386 (37884 source, 44502 target) ; Learning rate = 0.000255 ; Loss = 1.725179\n",
      "2024-12-14 08:14:40.864000: I runner.py:310] Step = 120700 ; steps/s = 1.63, tokens/s = 82275 (37835 source, 44440 target) ; Learning rate = 0.000254 ; Loss = 1.740190\n",
      "2024-12-14 08:15:41.892000: I runner.py:310] Step = 120800 ; steps/s = 1.64, tokens/s = 81413 (37432 source, 43981 target) ; Learning rate = 0.000254 ; Loss = 1.745186\n",
      "2024-12-14 08:16:43.317000: I runner.py:310] Step = 120900 ; steps/s = 1.63, tokens/s = 82259 (37839 source, 44420 target) ; Learning rate = 0.000254 ; Loss = 1.726384\n",
      "2024-12-14 08:17:44.724000: I runner.py:310] Step = 121000 ; steps/s = 1.63, tokens/s = 82310 (37860 source, 44450 target) ; Learning rate = 0.000254 ; Loss = 1.724114\n",
      "2024-12-14 08:18:45.799000: I runner.py:310] Step = 121100 ; steps/s = 1.64, tokens/s = 81401 (37438 source, 43963 target) ; Learning rate = 0.000254 ; Loss = 1.726892\n",
      "2024-12-14 08:19:47.194000: I runner.py:310] Step = 121200 ; steps/s = 1.63, tokens/s = 82318 (37868 source, 44450 target) ; Learning rate = 0.000254 ; Loss = 1.709481\n",
      "2024-12-14 08:20:48.580000: I runner.py:310] Step = 121300 ; steps/s = 1.63, tokens/s = 82326 (37879 source, 44447 target) ; Learning rate = 0.000254 ; Loss = 1.727403\n",
      "2024-12-14 08:21:49.987000: I runner.py:310] Step = 121400 ; steps/s = 1.63, tokens/s = 82290 (37831 source, 44459 target) ; Learning rate = 0.000254 ; Loss = 1.730842\n",
      "2024-12-14 08:22:50.987000: I runner.py:310] Step = 121500 ; steps/s = 1.64, tokens/s = 81425 (37439 source, 43986 target) ; Learning rate = 0.000254 ; Loss = 1.746355\n",
      "2024-12-14 08:23:52.345000: I runner.py:310] Step = 121600 ; steps/s = 1.63, tokens/s = 82410 (37931 source, 44479 target) ; Learning rate = 0.000253 ; Loss = 1.717578\n",
      "2024-12-14 08:24:53.754000: I runner.py:310] Step = 121700 ; steps/s = 1.63, tokens/s = 82282 (37827 source, 44455 target) ; Learning rate = 0.000253 ; Loss = 1.718466\n",
      "2024-12-14 08:25:54.697000: I runner.py:310] Step = 121800 ; steps/s = 1.64, tokens/s = 81546 (37497 source, 44049 target) ; Learning rate = 0.000253 ; Loss = 1.723522\n",
      "2024-12-14 08:26:56.095000: I runner.py:310] Step = 121900 ; steps/s = 1.63, tokens/s = 82323 (37880 source, 44443 target) ; Learning rate = 0.000253 ; Loss = 1.721812\n",
      "2024-12-14 08:27:57.499000: I runner.py:310] Step = 122000 ; steps/s = 1.63, tokens/s = 82288 (37842 source, 44446 target) ; Learning rate = 0.000253 ; Loss = 1.728267\n",
      "2024-12-14 08:28:58.935000: I runner.py:310] Step = 122100 ; steps/s = 1.63, tokens/s = 82257 (37831 source, 44426 target) ; Learning rate = 0.000253 ; Loss = 1.729397\n",
      "2024-12-14 08:29:59.938000: I runner.py:310] Step = 122200 ; steps/s = 1.64, tokens/s = 81461 (37466 source, 43995 target) ; Learning rate = 0.000253 ; Loss = 1.706540\n",
      "2024-12-14 08:31:01.392000: I runner.py:310] Step = 122300 ; steps/s = 1.63, tokens/s = 82256 (37837 source, 44419 target) ; Learning rate = 0.000253 ; Loss = 1.740514\n",
      "2024-12-14 08:32:02.892000: I runner.py:310] Step = 122400 ; steps/s = 1.63, tokens/s = 82166 (37799 source, 44367 target) ; Learning rate = 0.000253 ; Loss = 1.721877\n",
      "2024-12-14 08:33:03.888000: I runner.py:310] Step = 122500 ; steps/s = 1.64, tokens/s = 81451 (37437 source, 44014 target) ; Learning rate = 0.000253 ; Loss = 1.710144\n",
      "2024-12-14 08:34:05.287000: I runner.py:310] Step = 122600 ; steps/s = 1.63, tokens/s = 82312 (37861 source, 44451 target) ; Learning rate = 0.000252 ; Loss = 1.716521\n",
      "2024-12-14 08:35:06.684000: I runner.py:310] Step = 122700 ; steps/s = 1.63, tokens/s = 82278 (37829 source, 44449 target) ; Learning rate = 0.000252 ; Loss = 1.721669\n",
      "2024-12-14 08:36:08.115000: I runner.py:310] Step = 122800 ; steps/s = 1.63, tokens/s = 82292 (37865 source, 44427 target) ; Learning rate = 0.000252 ; Loss = 1.735527\n",
      "2024-12-14 08:37:09.129000: I runner.py:310] Step = 122900 ; steps/s = 1.64, tokens/s = 81454 (37457 source, 43997 target) ; Learning rate = 0.000252 ; Loss = 1.734734\n",
      "2024-12-14 08:38:10.517000: I runner.py:310] Step = 123000 ; steps/s = 1.63, tokens/s = 82323 (37856 source, 44467 target) ; Learning rate = 0.000252 ; Loss = 1.713902\n",
      "2024-12-14 08:39:11.940000: I runner.py:310] Step = 123100 ; steps/s = 1.63, tokens/s = 82289 (37858 source, 44431 target) ; Learning rate = 0.000252 ; Loss = 1.713533\n",
      "2024-12-14 08:40:12.901000: I runner.py:310] Step = 123200 ; steps/s = 1.64, tokens/s = 81509 (37488 source, 44021 target) ; Learning rate = 0.000252 ; Loss = 1.737202\n",
      "2024-12-14 08:41:14.361000: I runner.py:310] Step = 123300 ; steps/s = 1.63, tokens/s = 82237 (37835 source, 44402 target) ; Learning rate = 0.000252 ; Loss = 1.711559\n",
      "2024-12-14 08:42:15.809000: I runner.py:310] Step = 123400 ; steps/s = 1.63, tokens/s = 82261 (37848 source, 44413 target) ; Learning rate = 0.000252 ; Loss = 1.703575\n",
      "2024-12-14 08:43:17.329000: I runner.py:310] Step = 123500 ; steps/s = 1.63, tokens/s = 82132 (37753 source, 44379 target) ; Learning rate = 0.000252 ; Loss = 1.715121\n",
      "2024-12-14 08:44:18.326000: I runner.py:310] Step = 123600 ; steps/s = 1.64, tokens/s = 81480 (37485 source, 43995 target) ; Learning rate = 0.000251 ; Loss = 1.713457\n",
      "2024-12-14 08:45:19.759000: I runner.py:310] Step = 123700 ; steps/s = 1.63, tokens/s = 82254 (37817 source, 44437 target) ; Learning rate = 0.000251 ; Loss = 1.721497\n",
      "2024-12-14 08:46:21.178000: I runner.py:310] Step = 123800 ; steps/s = 1.63, tokens/s = 82298 (37861 source, 44437 target) ; Learning rate = 0.000251 ; Loss = 1.727708\n",
      "2024-12-14 08:47:22.189000: I runner.py:310] Step = 123900 ; steps/s = 1.64, tokens/s = 81445 (37456 source, 43989 target) ; Learning rate = 0.000251 ; Loss = 1.704979\n",
      "2024-12-14 08:48:23.601000: I runner.py:310] Step = 124000 ; steps/s = 1.63, tokens/s = 82333 (37892 source, 44441 target) ; Learning rate = 0.000251 ; Loss = 1.724814\n",
      "2024-12-14 08:49:25.031000: I runner.py:310] Step = 124100 ; steps/s = 1.63, tokens/s = 82265 (37832 source, 44433 target) ; Learning rate = 0.000251 ; Loss = 1.732159\n",
      "2024-12-14 08:50:26.490000: I runner.py:310] Step = 124200 ; steps/s = 1.63, tokens/s = 82190 (37793 source, 44397 target) ; Learning rate = 0.000251 ; Loss = 1.726844\n",
      "2024-12-14 08:51:27.504000: I runner.py:310] Step = 124300 ; steps/s = 1.64, tokens/s = 81458 (37453 source, 44005 target) ; Learning rate = 0.000251 ; Loss = 1.708904\n",
      "2024-12-14 08:52:28.935000: I runner.py:310] Step = 124400 ; steps/s = 1.63, tokens/s = 82255 (37826 source, 44429 target) ; Learning rate = 0.000251 ; Loss = 1.729673\n",
      "2024-12-14 08:53:30.280000: I runner.py:310] Step = 124500 ; steps/s = 1.63, tokens/s = 82391 (37913 source, 44478 target) ; Learning rate = 0.000251 ; Loss = 1.731550\n",
      "2024-12-14 08:54:31.284000: I runner.py:310] Step = 124600 ; steps/s = 1.64, tokens/s = 81464 (37472 source, 43992 target) ; Learning rate = 0.000250 ; Loss = 1.738151\n",
      "2024-12-14 08:55:32.708000: I runner.py:310] Step = 124700 ; steps/s = 1.63, tokens/s = 82287 (37851 source, 44436 target) ; Learning rate = 0.000250 ; Loss = 1.710692\n",
      "2024-12-14 08:56:34.137000: I runner.py:310] Step = 124800 ; steps/s = 1.63, tokens/s = 82281 (37840 source, 44441 target) ; Learning rate = 0.000250 ; Loss = 1.718757\n",
      "2024-12-14 08:57:35.531000: I runner.py:310] Step = 124900 ; steps/s = 1.63, tokens/s = 82291 (37842 source, 44449 target) ; Learning rate = 0.000250 ; Loss = 1.721699\n",
      "2024-12-14 08:58:36.517000: I runner.py:310] Step = 125000 ; steps/s = 1.64, tokens/s = 81509 (37502 source, 44007 target) ; Learning rate = 0.000250 ; Loss = 1.718442\n",
      "2024-12-14 08:58:36.519000: I training.py:192] Running evaluation for step 125000\n",
      "2024-12-14 09:00:42.707000: I training.py:192] Evaluation result for step 125000: loss = 2.601327 ; perplexity = 13.481616\n",
      "2024-12-14 09:01:44.009000: I runner.py:310] Step = 125100 ; steps/s = 1.63, tokens/s = 82432 (37894 source, 44538 target) ; Learning rate = 0.000250 ; Loss = 1.734431\n",
      "2024-12-14 09:02:45.454000: I runner.py:310] Step = 125200 ; steps/s = 1.63, tokens/s = 82236 (37821 source, 44415 target) ; Learning rate = 0.000250 ; Loss = 1.738563\n",
      "2024-12-14 09:03:46.508000: I runner.py:310] Step = 125300 ; steps/s = 1.64, tokens/s = 81413 (37454 source, 43959 target) ; Learning rate = 0.000250 ; Loss = 1.735522\n",
      "2024-12-14 09:04:47.935000: I runner.py:310] Step = 125400 ; steps/s = 1.63, tokens/s = 82260 (37824 source, 44436 target) ; Learning rate = 0.000250 ; Loss = 1.722022\n",
      "2024-12-14 09:05:49.305000: I runner.py:310] Step = 125500 ; steps/s = 1.63, tokens/s = 82351 (37887 source, 44464 target) ; Learning rate = 0.000250 ; Loss = 1.723686\n",
      "2024-12-14 09:06:50.599000: I runner.py:310] Step = 125600 ; steps/s = 1.63, tokens/s = 81930 (37677 source, 44253 target) ; Learning rate = 0.000249 ; Loss = 1.730971\n",
      "2024-12-14 09:07:51.788000: I runner.py:310] Step = 125700 ; steps/s = 1.63, tokens/s = 81754 (37607 source, 44147 target) ; Learning rate = 0.000249 ; Loss = 1.704850\n",
      "2024-12-14 09:08:53.297000: I runner.py:310] Step = 125800 ; steps/s = 1.63, tokens/s = 82157 (37786 source, 44371 target) ; Learning rate = 0.000249 ; Loss = 1.722518\n",
      "2024-12-14 09:09:54.701000: I runner.py:310] Step = 125900 ; steps/s = 1.63, tokens/s = 82303 (37852 source, 44451 target) ; Learning rate = 0.000249 ; Loss = 1.732299\n",
      "2024-12-14 09:10:55.763000: I runner.py:310] Step = 126000 ; steps/s = 1.64, tokens/s = 81419 (37460 source, 43959 target) ; Learning rate = 0.000249 ; Loss = 1.737108\n",
      "2024-12-14 09:11:57.143000: I runner.py:310] Step = 126100 ; steps/s = 1.63, tokens/s = 82327 (37853 source, 44474 target) ; Learning rate = 0.000249 ; Loss = 1.708591\n",
      "2024-12-14 09:12:58.583000: I runner.py:310] Step = 126200 ; steps/s = 1.63, tokens/s = 82250 (37837 source, 44413 target) ; Learning rate = 0.000249 ; Loss = 1.707553\n",
      "2024-12-14 09:13:59.633000: I runner.py:310] Step = 126300 ; steps/s = 1.64, tokens/s = 81410 (37438 source, 43972 target) ; Learning rate = 0.000249 ; Loss = 1.710962\n",
      "2024-12-14 09:15:01.004000: I runner.py:310] Step = 126400 ; steps/s = 1.63, tokens/s = 82346 (37880 source, 44466 target) ; Learning rate = 0.000249 ; Loss = 1.717776\n",
      "2024-12-14 09:16:02.484000: I runner.py:310] Step = 126500 ; steps/s = 1.63, tokens/s = 82193 (37806 source, 44387 target) ; Learning rate = 0.000249 ; Loss = 1.724652\n",
      "2024-12-14 09:17:03.929000: I runner.py:310] Step = 126600 ; steps/s = 1.63, tokens/s = 82252 (37827 source, 44425 target) ; Learning rate = 0.000248 ; Loss = 1.733257\n",
      "2024-12-14 09:18:04.895000: I runner.py:310] Step = 126700 ; steps/s = 1.64, tokens/s = 81514 (37493 source, 44021 target) ; Learning rate = 0.000248 ; Loss = 1.695662\n",
      "2024-12-14 09:19:06.306000: I runner.py:310] Step = 126800 ; steps/s = 1.63, tokens/s = 82306 (37864 source, 44442 target) ; Learning rate = 0.000248 ; Loss = 1.725443\n",
      "2024-12-14 09:20:07.701000: I runner.py:310] Step = 126900 ; steps/s = 1.63, tokens/s = 82303 (37844 source, 44459 target) ; Learning rate = 0.000248 ; Loss = 1.727587\n",
      "2024-12-14 09:21:08.713000: I runner.py:310] Step = 127000 ; steps/s = 1.64, tokens/s = 81445 (37454 source, 43991 target) ; Learning rate = 0.000248 ; Loss = 1.719773\n",
      "2024-12-14 09:22:10.169000: I runner.py:310] Step = 127100 ; steps/s = 1.63, tokens/s = 82243 (37820 source, 44423 target) ; Learning rate = 0.000248 ; Loss = 1.715324\n",
      "2024-12-14 09:23:11.547000: I runner.py:310] Step = 127200 ; steps/s = 1.63, tokens/s = 82304 (37845 source, 44459 target) ; Learning rate = 0.000248 ; Loss = 1.731720\n",
      "2024-12-14 09:24:13.011000: I runner.py:310] Step = 127300 ; steps/s = 1.63, tokens/s = 82231 (37832 source, 44399 target) ; Learning rate = 0.000248 ; Loss = 1.730212\n",
      "2024-12-14 09:25:13.985000: I runner.py:310] Step = 127400 ; steps/s = 1.64, tokens/s = 81503 (37476 source, 44027 target) ; Learning rate = 0.000248 ; Loss = 1.738237\n",
      "2024-12-14 09:26:15.477000: I runner.py:310] Step = 127500 ; steps/s = 1.63, tokens/s = 82214 (37829 source, 44385 target) ; Learning rate = 0.000248 ; Loss = 1.704909\n",
      "2024-12-14 09:27:16.856000: I runner.py:310] Step = 127600 ; steps/s = 1.63, tokens/s = 82328 (37868 source, 44460 target) ; Learning rate = 0.000247 ; Loss = 1.717450\n",
      "2024-12-14 09:28:17.876000: I runner.py:310] Step = 127700 ; steps/s = 1.64, tokens/s = 81449 (37462 source, 43987 target) ; Learning rate = 0.000247 ; Loss = 1.711009\n",
      "2024-12-14 09:29:19.326000: I runner.py:310] Step = 127800 ; steps/s = 1.63, tokens/s = 82245 (37823 source, 44422 target) ; Learning rate = 0.000247 ; Loss = 1.717222\n",
      "2024-12-14 09:30:20.791000: I runner.py:310] Step = 127900 ; steps/s = 1.63, tokens/s = 82200 (37802 source, 44398 target) ; Learning rate = 0.000247 ; Loss = 1.717997\n",
      "2024-12-14 09:31:22.232000: I runner.py:310] Step = 128000 ; steps/s = 1.63, tokens/s = 82267 (37848 source, 44419 target) ; Learning rate = 0.000247 ; Loss = 1.732178\n",
      "2024-12-14 09:32:23.252000: I runner.py:310] Step = 128100 ; steps/s = 1.64, tokens/s = 81468 (37479 source, 43989 target) ; Learning rate = 0.000247 ; Loss = 1.699517\n",
      "2024-12-14 09:33:24.664000: I runner.py:310] Step = 128200 ; steps/s = 1.63, tokens/s = 82261 (37807 source, 44454 target) ; Learning rate = 0.000247 ; Loss = 1.714359\n",
      "2024-12-14 09:34:26.057000: I runner.py:310] Step = 128300 ; steps/s = 1.63, tokens/s = 82292 (37840 source, 44452 target) ; Learning rate = 0.000247 ; Loss = 1.733283\n",
      "2024-12-14 09:35:27.131000: I runner.py:310] Step = 128400 ; steps/s = 1.64, tokens/s = 81407 (37465 source, 43942 target) ; Learning rate = 0.000247 ; Loss = 1.719793\n",
      "2024-12-14 09:36:28.553000: I runner.py:310] Step = 128500 ; steps/s = 1.63, tokens/s = 82294 (37849 source, 44445 target) ; Learning rate = 0.000247 ; Loss = 1.705906\n",
      "2024-12-14 09:37:29.955000: I runner.py:310] Step = 128600 ; steps/s = 1.63, tokens/s = 82292 (37846 source, 44446 target) ; Learning rate = 0.000246 ; Loss = 1.725407\n",
      "2024-12-14 09:38:31.356000: I runner.py:310] Step = 128700 ; steps/s = 1.63, tokens/s = 82298 (37851 source, 44447 target) ; Learning rate = 0.000246 ; Loss = 1.730254\n",
      "2024-12-14 09:39:32.343000: I runner.py:310] Step = 128800 ; steps/s = 1.64, tokens/s = 81480 (37473 source, 44007 target) ; Learning rate = 0.000246 ; Loss = 1.743698\n",
      "2024-12-14 09:40:33.894000: I runner.py:310] Step = 128900 ; steps/s = 1.62, tokens/s = 82094 (37756 source, 44338 target) ; Learning rate = 0.000246 ; Loss = 1.698767\n",
      "2024-12-14 09:41:35.291000: I runner.py:310] Step = 129000 ; steps/s = 1.63, tokens/s = 82322 (37877 source, 44445 target) ; Learning rate = 0.000246 ; Loss = 1.721821\n",
      "2024-12-14 09:42:36.311000: I runner.py:310] Step = 129100 ; steps/s = 1.64, tokens/s = 81470 (37467 source, 44003 target) ; Learning rate = 0.000246 ; Loss = 1.723123\n",
      "2024-12-14 09:43:37.778000: I runner.py:310] Step = 129200 ; steps/s = 1.63, tokens/s = 82228 (37825 source, 44403 target) ; Learning rate = 0.000246 ; Loss = 1.710714\n",
      "2024-12-14 09:44:39.196000: I runner.py:310] Step = 129300 ; steps/s = 1.63, tokens/s = 82243 (37804 source, 44439 target) ; Learning rate = 0.000246 ; Loss = 1.707617\n",
      "2024-12-14 09:45:40.602000: I runner.py:310] Step = 129400 ; steps/s = 1.63, tokens/s = 82312 (37878 source, 44434 target) ; Learning rate = 0.000246 ; Loss = 1.719041\n",
      "2024-12-14 09:46:41.656000: I runner.py:310] Step = 129500 ; steps/s = 1.64, tokens/s = 81375 (37402 source, 43973 target) ; Learning rate = 0.000246 ; Loss = 1.747660\n",
      "2024-12-14 09:47:43.030000: I runner.py:310] Step = 129600 ; steps/s = 1.63, tokens/s = 82354 (37889 source, 44465 target) ; Learning rate = 0.000246 ; Loss = 1.707099\n",
      "2024-12-14 09:48:44.392000: I runner.py:310] Step = 129700 ; steps/s = 1.63, tokens/s = 82353 (37877 source, 44476 target) ; Learning rate = 0.000245 ; Loss = 1.715308\n",
      "2024-12-14 09:49:45.462000: I runner.py:310] Step = 129800 ; steps/s = 1.64, tokens/s = 81404 (37450 source, 43954 target) ; Learning rate = 0.000245 ; Loss = 1.728209\n",
      "2024-12-14 09:50:46.854000: I runner.py:310] Step = 129900 ; steps/s = 1.63, tokens/s = 82303 (37852 source, 44451 target) ; Learning rate = 0.000245 ; Loss = 1.704434\n",
      "2024-12-14 09:51:48.309000: I runner.py:310] Step = 130000 ; steps/s = 1.63, tokens/s = 82230 (37819 source, 44411 target) ; Learning rate = 0.000245 ; Loss = 1.707044\n",
      "2024-12-14 09:51:50.484000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-130000\n",
      "2024-12-14 09:51:50.484000: I training.py:192] Running evaluation for step 130000\n",
      "2024-12-14 09:53:55.349000: I training.py:192] Evaluation result for step 130000: loss = 2.603460 ; perplexity = 13.510405\n",
      "2024-12-14 09:54:56.673000: I runner.py:310] Step = 130100 ; steps/s = 1.63, tokens/s = 82437 (37917 source, 44520 target) ; Learning rate = 0.000245 ; Loss = 1.712758\n",
      "2024-12-14 09:55:57.648000: I runner.py:310] Step = 130200 ; steps/s = 1.64, tokens/s = 81508 (37482 source, 44026 target) ; Learning rate = 0.000245 ; Loss = 1.691135\n",
      "2024-12-14 09:56:59.097000: I runner.py:310] Step = 130300 ; steps/s = 1.63, tokens/s = 82259 (37857 source, 44402 target) ; Learning rate = 0.000245 ; Loss = 1.717768\n",
      "2024-12-14 09:58:00.547000: I runner.py:310] Step = 130400 ; steps/s = 1.63, tokens/s = 82206 (37788 source, 44418 target) ; Learning rate = 0.000245 ; Loss = 1.742489\n",
      "2024-12-14 09:59:01.658000: I runner.py:310] Step = 130500 ; steps/s = 1.64, tokens/s = 81348 (37425 source, 43923 target) ; Learning rate = 0.000245 ; Loss = 1.702960\n",
      "2024-12-14 10:00:03.146000: I runner.py:310] Step = 130600 ; steps/s = 1.63, tokens/s = 82206 (37801 source, 44405 target) ; Learning rate = 0.000245 ; Loss = 1.714904\n",
      "2024-12-14 10:01:04.596000: I runner.py:310] Step = 130700 ; steps/s = 1.63, tokens/s = 82213 (37803 source, 44410 target) ; Learning rate = 0.000244 ; Loss = 1.715738\n",
      "2024-12-14 10:02:05.991000: I runner.py:310] Step = 130800 ; steps/s = 1.63, tokens/s = 82315 (37867 source, 44448 target) ; Learning rate = 0.000244 ; Loss = 1.737030\n",
      "2024-12-14 10:03:06.945000: I runner.py:310] Step = 130900 ; steps/s = 1.64, tokens/s = 81568 (37538 source, 44030 target) ; Learning rate = 0.000244 ; Loss = 1.702613\n",
      "2024-12-14 10:04:08.278000: I runner.py:310] Step = 131000 ; steps/s = 1.63, tokens/s = 82391 (37892 source, 44499 target) ; Learning rate = 0.000244 ; Loss = 1.726962\n",
      "2024-12-14 10:05:09.696000: I runner.py:310] Step = 131100 ; steps/s = 1.63, tokens/s = 82261 (37835 source, 44426 target) ; Learning rate = 0.000244 ; Loss = 1.728384\n",
      "2024-12-14 10:06:10.729000: I runner.py:310] Step = 131200 ; steps/s = 1.64, tokens/s = 81452 (37461 source, 43991 target) ; Learning rate = 0.000244 ; Loss = 1.715687\n",
      "2024-12-14 10:07:12.135000: I runner.py:310] Step = 131300 ; steps/s = 1.63, tokens/s = 82316 (37872 source, 44444 target) ; Learning rate = 0.000244 ; Loss = 1.729574\n",
      "2024-12-14 10:08:13.631000: I runner.py:310] Step = 131400 ; steps/s = 1.63, tokens/s = 82161 (37779 source, 44382 target) ; Learning rate = 0.000244 ; Loss = 1.725049\n",
      "2024-12-14 10:09:15.075000: I runner.py:310] Step = 131500 ; steps/s = 1.63, tokens/s = 82223 (37809 source, 44414 target) ; Learning rate = 0.000244 ; Loss = 1.720827\n",
      "2024-12-14 10:10:16.043000: I runner.py:310] Step = 131600 ; steps/s = 1.64, tokens/s = 81540 (37508 source, 44032 target) ; Learning rate = 0.000244 ; Loss = 1.704810\n",
      "2024-12-14 10:11:17.420000: I runner.py:310] Step = 131700 ; steps/s = 1.63, tokens/s = 82341 (37873 source, 44468 target) ; Learning rate = 0.000244 ; Loss = 1.721996\n",
      "2024-12-14 10:12:18.820000: I runner.py:310] Step = 131800 ; steps/s = 1.63, tokens/s = 82286 (37845 source, 44441 target) ; Learning rate = 0.000243 ; Loss = 1.722334\n",
      "2024-12-14 10:13:19.830000: I runner.py:310] Step = 131900 ; steps/s = 1.64, tokens/s = 81454 (37448 source, 44006 target) ; Learning rate = 0.000243 ; Loss = 1.704560\n",
      "2024-12-14 10:14:21.249000: I runner.py:310] Step = 132000 ; steps/s = 1.63, tokens/s = 82286 (37846 source, 44440 target) ; Learning rate = 0.000243 ; Loss = 1.719864\n",
      "2024-12-14 10:15:22.743000: I runner.py:310] Step = 132100 ; steps/s = 1.63, tokens/s = 82206 (37815 source, 44391 target) ; Learning rate = 0.000243 ; Loss = 1.713645\n",
      "2024-12-14 10:16:24.112000: I runner.py:310] Step = 132200 ; steps/s = 1.63, tokens/s = 82322 (37865 source, 44457 target) ; Learning rate = 0.000243 ; Loss = 1.729640\n",
      "2024-12-14 10:17:25.159000: I runner.py:310] Step = 132300 ; steps/s = 1.64, tokens/s = 81422 (37459 source, 43963 target) ; Learning rate = 0.000243 ; Loss = 1.687983\n",
      "2024-12-14 10:18:26.565000: I runner.py:310] Step = 132400 ; steps/s = 1.63, tokens/s = 82324 (37876 source, 44448 target) ; Learning rate = 0.000243 ; Loss = 1.722840\n",
      "2024-12-14 10:19:28.005000: I runner.py:310] Step = 132500 ; steps/s = 1.63, tokens/s = 82229 (37809 source, 44420 target) ; Learning rate = 0.000243 ; Loss = 1.725403\n",
      "2024-12-14 10:20:29.036000: I runner.py:310] Step = 132600 ; steps/s = 1.64, tokens/s = 81430 (37450 source, 43980 target) ; Learning rate = 0.000243 ; Loss = 1.713038\n",
      "2024-12-14 10:21:30.397000: I runner.py:310] Step = 132700 ; steps/s = 1.63, tokens/s = 82347 (37852 source, 44495 target) ; Learning rate = 0.000243 ; Loss = 1.706575\n",
      "2024-12-14 10:22:31.871000: I runner.py:310] Step = 132800 ; steps/s = 1.63, tokens/s = 82196 (37808 source, 44388 target) ; Learning rate = 0.000243 ; Loss = 1.708866\n",
      "2024-12-14 10:23:33.299000: I runner.py:310] Step = 132900 ; steps/s = 1.63, tokens/s = 82270 (37844 source, 44426 target) ; Learning rate = 0.000242 ; Loss = 1.725228\n",
      "2024-12-14 10:24:34.236000: I runner.py:310] Step = 133000 ; steps/s = 1.64, tokens/s = 81564 (37522 source, 44042 target) ; Learning rate = 0.000242 ; Loss = 1.730046\n",
      "2024-12-14 10:25:35.709000: I runner.py:310] Step = 133100 ; steps/s = 1.63, tokens/s = 82187 (37786 source, 44401 target) ; Learning rate = 0.000242 ; Loss = 1.705890\n",
      "2024-12-14 10:26:37.134000: I runner.py:310] Step = 133200 ; steps/s = 1.63, tokens/s = 82288 (37852 source, 44436 target) ; Learning rate = 0.000242 ; Loss = 1.727332\n",
      "2024-12-14 10:27:38.080000: I runner.py:310] Step = 133300 ; steps/s = 1.64, tokens/s = 81577 (37524 source, 44053 target) ; Learning rate = 0.000242 ; Loss = 1.701787\n",
      "2024-12-14 10:28:39.590000: I runner.py:310] Step = 133400 ; steps/s = 1.63, tokens/s = 82175 (37807 source, 44368 target) ; Learning rate = 0.000242 ; Loss = 1.716656\n",
      "2024-12-14 10:29:41.006000: I runner.py:310] Step = 133500 ; steps/s = 1.63, tokens/s = 82276 (37842 source, 44434 target) ; Learning rate = 0.000242 ; Loss = 1.723474\n",
      "2024-12-14 10:30:42.417000: I runner.py:310] Step = 133600 ; steps/s = 1.63, tokens/s = 82247 (37818 source, 44429 target) ; Learning rate = 0.000242 ; Loss = 1.722512\n",
      "2024-12-14 10:31:43.419000: I runner.py:310] Step = 133700 ; steps/s = 1.64, tokens/s = 81463 (37467 source, 43996 target) ; Learning rate = 0.000242 ; Loss = 1.729979\n",
      "2024-12-14 10:32:44.884000: I runner.py:310] Step = 133800 ; steps/s = 1.63, tokens/s = 82241 (37813 source, 44428 target) ; Learning rate = 0.000242 ; Loss = 1.711400\n",
      "2024-12-14 10:33:46.307000: I runner.py:310] Step = 133900 ; steps/s = 1.63, tokens/s = 82272 (37849 source, 44423 target) ; Learning rate = 0.000242 ; Loss = 1.708702\n",
      "2024-12-14 10:34:47.303000: I runner.py:310] Step = 134000 ; steps/s = 1.64, tokens/s = 81501 (37497 source, 44004 target) ; Learning rate = 0.000241 ; Loss = 1.703049\n",
      "2024-12-14 10:35:48.737000: I runner.py:310] Step = 134100 ; steps/s = 1.63, tokens/s = 82273 (37845 source, 44428 target) ; Learning rate = 0.000241 ; Loss = 1.713779\n",
      "2024-12-14 10:36:50.211000: I runner.py:310] Step = 134200 ; steps/s = 1.63, tokens/s = 82190 (37785 source, 44405 target) ; Learning rate = 0.000241 ; Loss = 1.718728\n",
      "2024-12-14 10:37:51.271000: I runner.py:310] Step = 134300 ; steps/s = 1.64, tokens/s = 81521 (37498 source, 44023 target) ; Learning rate = 0.000241 ; Loss = 1.765118\n",
      "2024-12-14 10:38:52.628000: I runner.py:310] Step = 134400 ; steps/s = 1.63, tokens/s = 82221 (37808 source, 44413 target) ; Learning rate = 0.000241 ; Loss = 1.721764\n",
      "2024-12-14 10:39:54.076000: I runner.py:310] Step = 134500 ; steps/s = 1.63, tokens/s = 82231 (37807 source, 44424 target) ; Learning rate = 0.000241 ; Loss = 1.709071\n",
      "2024-12-14 10:40:55.530000: I runner.py:310] Step = 134600 ; steps/s = 1.63, tokens/s = 82228 (37832 source, 44396 target) ; Learning rate = 0.000241 ; Loss = 1.708533\n",
      "2024-12-14 10:41:56.520000: I runner.py:310] Step = 134700 ; steps/s = 1.64, tokens/s = 81514 (37497 source, 44017 target) ; Learning rate = 0.000241 ; Loss = 1.699937\n",
      "2024-12-14 10:42:57.913000: I runner.py:310] Step = 134800 ; steps/s = 1.63, tokens/s = 82288 (37849 source, 44439 target) ; Learning rate = 0.000241 ; Loss = 1.711427\n",
      "2024-12-14 10:43:59.348000: I runner.py:310] Step = 134900 ; steps/s = 1.63, tokens/s = 82267 (37840 source, 44427 target) ; Learning rate = 0.000241 ; Loss = 1.719678\n",
      "2024-12-14 10:45:00.407000: I runner.py:310] Step = 135000 ; steps/s = 1.64, tokens/s = 81410 (37442 source, 43968 target) ; Learning rate = 0.000241 ; Loss = 1.718334\n",
      "2024-12-14 10:45:00.408000: I training.py:192] Running evaluation for step 135000\n",
      "2024-12-14 10:47:03.182000: I training.py:192] Evaluation result for step 135000: loss = 2.613696 ; perplexity = 13.649407\n",
      "2024-12-14 10:48:04.342000: I runner.py:310] Step = 135100 ; steps/s = 1.64, tokens/s = 82615 (37982 source, 44633 target) ; Learning rate = 0.000240 ; Loss = 1.711621\n",
      "2024-12-14 10:49:05.783000: I runner.py:310] Step = 135200 ; steps/s = 1.63, tokens/s = 82241 (37810 source, 44431 target) ; Learning rate = 0.000240 ; Loss = 1.704522\n",
      "2024-12-14 10:50:07.229000: I runner.py:310] Step = 135300 ; steps/s = 1.63, tokens/s = 82268 (37867 source, 44401 target) ; Learning rate = 0.000240 ; Loss = 1.727775\n",
      "2024-12-14 10:51:08.306000: I runner.py:310] Step = 135400 ; steps/s = 1.64, tokens/s = 81405 (37453 source, 43952 target) ; Learning rate = 0.000240 ; Loss = 1.685179\n",
      "2024-12-14 10:52:09.775000: I runner.py:310] Step = 135500 ; steps/s = 1.63, tokens/s = 82246 (37843 source, 44403 target) ; Learning rate = 0.000240 ; Loss = 1.716434\n",
      "2024-12-14 10:53:11.190000: I runner.py:310] Step = 135600 ; steps/s = 1.63, tokens/s = 82272 (37832 source, 44440 target) ; Learning rate = 0.000240 ; Loss = 1.727957\n",
      "2024-12-14 10:54:12.269000: I runner.py:310] Step = 135700 ; steps/s = 1.64, tokens/s = 81330 (37383 source, 43947 target) ; Learning rate = 0.000240 ; Loss = 1.701973\n",
      "2024-12-14 10:55:13.657000: I runner.py:310] Step = 135800 ; steps/s = 1.63, tokens/s = 82312 (37857 source, 44455 target) ; Learning rate = 0.000240 ; Loss = 1.707197\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Kk-En -> Tr-En (TED2020)\n",
    "!onmt-main --model kk-tr-en-shared.py --config kk-tr-en-2.yml --auto_config --checkpoint_path KK-TR-EN-Shared-vocab/ckpt-100000 train --with_eval --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "418ee93e-cec4-43d7-b232-0a370dcd0c5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-14 13:41:33.726880: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-14 13:41:34.892107: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-14 13:41:34.892308: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-14 13:41:34.892322: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-14 13:41:36.555000: I main.py:308] Loading model description from KK-TR-EN-Shared-vocab/model_description.py\n",
      "2024-12-14 13:41:36.754000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-14 13:41:36.754000: I main.py:315] Using model:\n",
      "(model): MyCustomTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): WordEmbedder()\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): WordEmbedder()\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-14 13:41:36.760000: I main.py:340] Using parameters:\n",
      "data:\n",
      "  eval_features_file: KK_tokens_valid_shared\n",
      "  eval_labels_file: KK_valid_target_tokens_shared.txt\n",
      "  source_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file: KK_tokens_train_shared\n",
      "  train_labels_file: KK_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: KK-TR-EN-Shared-vocab\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 250000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-14 13:41:36.958096: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-14 13:41:37.591209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-14 13:41:37.748000: I inputter.py:316] Initialized source input layer:\n",
      "2024-12-14 13:41:37.748000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-14 13:41:37.748000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-14 13:41:37.823000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-14 13:41:37.823000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-14 13:41:37.823000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-14 13:41:37.847000: I runner.py:462] Restored checkpoint KK-TR-EN-Shared-vocab-2/ckpt-100000\n",
      "2024-12-14 13:41:37.887000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-14 13:41:38.438331: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-14 13:41:38.560000: I runner.py:471] Tracing and optimizing the inference graph...\n",
      "2024-12-14 13:41:52.464667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-14 13:41:53.382512: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-14 13:42:10.648000: I runner.py:471] 1827 predictions are buffered, but waiting for the prediction of queued line 31 to advance the output...\n",
      "2024-12-14 13:42:22.192000: I runner.py:471] 2959 predictions are buffered, but waiting for the prediction of queued line 51 to advance the output...\n",
      "2024-12-14 13:42:32.282000: I runner.py:471] 4079 predictions are buffered, but waiting for the prediction of queued line 51 to advance the output...\n",
      "2024-12-14 13:42:42.313000: I runner.py:471] 5167 predictions are buffered, but waiting for the prediction of queued line 51 to advance the output...\n",
      "2024-12-14 13:42:53.034000: I runner.py:471] 6255 predictions are buffered, but waiting for the prediction of queued line 51 to advance the output...\n",
      "2024-12-14 13:43:09.346000: I runner.py:471] 7891 predictions are buffered, but waiting for the prediction of queued line 111 to advance the output...\n",
      "2024-12-14 13:43:19.488000: I runner.py:471] 8915 predictions are buffered, but waiting for the prediction of queued line 111 to advance the output...\n",
      "2024-12-14 13:43:30.014000: I runner.py:471] 9775 predictions are buffered, but waiting for the prediction of queued line 111 to advance the output...\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 onmt-main --config kk-tr-en-2.yml --auto_config --checkpoint_path KK-TR-EN-Shared-vocab-2/ckpt-100000 infer --features_file TED2020_tokens_test_shared --predictions_file output_kk_en_tr_shared_vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c97a435-ef85-43dc-b8f7-15064c63bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 MT-Preparation/subwording/3-desubword.py en_shared_vocab.model output_kk_en_tr_shared_vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f961d50-11bc-4e9d-8d9f-1f113ca9a0f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference first sentence: It's quite widespread.\n",
      "Translated first sentence: So this is a very common topic .\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "BLEU2:  BLEU = 24.21 55.6/30.1/18.2/11.3 (BP = 1.000 ratio = 1.070 hyp_len = 211818 ref_len = 197923)\n",
      "CHRF:  chrF2 = 51.36\n"
     ]
    }
   ],
   "source": [
    "# BLEU and chrF scores\n",
    "!python3 compute-bleu.py TED2020.en-tr.en-filtered.en.test output_kk_en_tr_shared_vocab.txt.desubword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1193a4b4-bf28-4a0d-b335-1905c73b48a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama METEOR Puanı: 0.5690762834709019\n"
     ]
    }
   ],
   "source": [
    "# Average METEOR score (Ortalama METEOR Puanı)\n",
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "def read_and_tokenize_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return [nltk.word_tokenize(line.strip()) for line in lines]\n",
    "\n",
    "def calculate_meteor(reference_file, hypothesis_file):\n",
    "    references = read_and_tokenize_file(reference_file)\n",
    "    hypotheses = read_and_tokenize_file(hypothesis_file)\n",
    "    \n",
    "    if len(references) != len(hypotheses):\n",
    "        raise ValueError(\"Dosyaların satır sayıları eşleşmiyor\")\n",
    "\n",
    "    total_meteor_score = 0.0\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        total_meteor_score += meteor_score([ref], hyp)\n",
    "\n",
    "    average_meteor_score = total_meteor_score / len(references)\n",
    "    return average_meteor_score\n",
    "\n",
    "reference_file = 'TED2020.en-tr.en-filtered.en.test'\n",
    "hypothesis_file = 'output_kk_en_tr_shared_vocab.txt.desubword'\n",
    "\n",
    "score = calculate_meteor(reference_file, hypothesis_file)\n",
    "print(f\"Ortalama METEOR Puanı: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "102e1990-5b87-408b-a2a7-26d8a07a91f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-14 13:48:20.986162: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-14 13:48:21.796544: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-14 13:48:21.796612: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-14 13:48:21.796621: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-14 13:48:22.826000: I onmt-main:8] Creating model directory TR-EN-Shared-vocab\n",
      "2024-12-14 13:48:23.045000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-14 13:48:23.045000: I main.py:315] Using model:\n",
      "(model): MyCustomTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): WordEmbedder()\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): WordEmbedder()\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-14 13:48:23.048241: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-14 13:48:25.949456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-14 13:48:25.950027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7723 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-12-14 13:48:25.950490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 348 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "2024-12-14 13:48:25.954000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file: TED2020_tokens_dev_shared\n",
      "  eval_labels_file: TED2020_dev_target_tokens_shared.txt\n",
      "  source_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file: TED2020_tokens_train_shared\n",
      "  train_labels_file: TED2020_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: TR-EN-Shared-vocab\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 250000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-14 13:48:26.284000: I inputter.py:316] Initialized source input layer:\n",
      "2024-12-14 13:48:26.284000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-14 13:48:26.284000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-14 13:48:26.356000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-14 13:48:26.357000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-14 13:48:26.357000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-14 13:48:26.361000: W runner.py:269] No checkpoint to restore in TR-EN-Shared-vocab\n",
      "2024-12-14 13:48:26.364000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "2024-12-14 13:48:26.408000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-14 13:48:27.370218: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-14 13:48:27.492000: I main.py:325] Accumulate gradients of 7 iterations to reach effective batch size of 25000\n",
      "2024-12-14 13:48:27.616000: I mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2024-12-14 13:48:27.761000: I dataset_ops.py:2542] Training on 337547 examples\n",
      "2024-12-14 13:49:36.405028: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-14 13:49:37.488559: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-14 13:49:37.789768: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-14 13:49:47.262000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-14 13:49:47.285000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-14 13:49:48.911000: I cross_device_ops.py:897] batch_all_reduce: 260 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-14 13:49:53.995000: I cross_device_ops.py:897] batch_all_reduce: 260 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-14 13:49:59.926000: I runner.py:310] Number of model parameters: 93326081\n",
      "2024-12-14 13:49:59.930000: I runner.py:310] Number of model weights: 260 (trainable = 260, non trainable = 0)\n",
      "2024-12-14 13:49:59.962000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-14 13:49:59.971000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-14 13:50:02.111000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-1\n",
      "2024-12-14 13:50:02.753000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-14 13:50:02.777000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-14 13:50:03.396000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-14 13:50:03.420000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-14 13:50:04.016000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-14 13:50:04.041000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-14 13:51:03.024000: I runner.py:310] Step = 100 ; steps/s = 1.63, tokens/s = 82413 (37901 source, 44512 target) ; Learning rate = 0.000009 ; Loss = 9.801096\n",
      "2024-12-14 13:52:04.473000: I runner.py:310] Step = 200 ; steps/s = 1.63, tokens/s = 82257 (37835 source, 44422 target) ; Learning rate = 0.000018 ; Loss = 8.928352\n",
      "2024-12-14 13:53:06.202000: I runner.py:310] Step = 300 ; steps/s = 1.62, tokens/s = 81849 (37638 source, 44211 target) ; Learning rate = 0.000027 ; Loss = 7.797183\n",
      "2024-12-14 13:54:07.864000: I runner.py:310] Step = 400 ; steps/s = 1.62, tokens/s = 80630 (37106 source, 43524 target) ; Learning rate = 0.000035 ; Loss = 7.159565\n",
      "2024-12-14 13:55:09.390000: I runner.py:310] Step = 500 ; steps/s = 1.63, tokens/s = 82120 (37761 source, 44359 target) ; Learning rate = 0.000044 ; Loss = 6.774316\n",
      "2024-12-14 13:56:10.867000: I runner.py:310] Step = 600 ; steps/s = 1.63, tokens/s = 82206 (37803 source, 44403 target) ; Learning rate = 0.000053 ; Loss = 6.560862\n",
      "2024-12-14 13:57:12.162000: I runner.py:310] Step = 700 ; steps/s = 1.63, tokens/s = 81081 (37299 source, 43782 target) ; Learning rate = 0.000062 ; Loss = 6.335167\n",
      "2024-12-14 13:58:13.658000: I runner.py:310] Step = 800 ; steps/s = 1.63, tokens/s = 82168 (37782 source, 44386 target) ; Learning rate = 0.000071 ; Loss = 6.103238\n",
      "2024-12-14 13:59:15.145000: I runner.py:310] Step = 900 ; steps/s = 1.63, tokens/s = 82174 (37785 source, 44389 target) ; Learning rate = 0.000080 ; Loss = 5.869812\n",
      "2024-12-14 14:00:16.620000: I runner.py:310] Step = 1000 ; steps/s = 1.63, tokens/s = 82238 (37845 source, 44393 target) ; Learning rate = 0.000088 ; Loss = 5.926412\n",
      "2024-12-14 14:01:17.722000: I runner.py:310] Step = 1100 ; steps/s = 1.64, tokens/s = 81327 (37396 source, 43931 target) ; Learning rate = 0.000097 ; Loss = 5.704831\n",
      "2024-12-14 14:02:19.213000: I runner.py:310] Step = 1200 ; steps/s = 1.63, tokens/s = 82199 (37815 source, 44384 target) ; Learning rate = 0.000106 ; Loss = 5.515495\n",
      "2024-12-14 14:03:20.684000: I runner.py:310] Step = 1300 ; steps/s = 1.63, tokens/s = 82231 (37832 source, 44399 target) ; Learning rate = 0.000115 ; Loss = 5.492635\n",
      "2024-12-14 14:04:21.734000: I runner.py:310] Step = 1400 ; steps/s = 1.64, tokens/s = 81396 (37424 source, 43972 target) ; Learning rate = 0.000124 ; Loss = 5.318042\n",
      "2024-12-14 14:05:23.146000: I runner.py:310] Step = 1500 ; steps/s = 1.63, tokens/s = 82238 (37790 source, 44448 target) ; Learning rate = 0.000133 ; Loss = 5.054567\n",
      "2024-12-14 14:06:24.642000: I runner.py:310] Step = 1600 ; steps/s = 1.63, tokens/s = 82193 (37806 source, 44387 target) ; Learning rate = 0.000142 ; Loss = 5.173825\n",
      "2024-12-14 14:07:26.138000: I runner.py:310] Step = 1700 ; steps/s = 1.63, tokens/s = 82200 (37823 source, 44377 target) ; Learning rate = 0.000150 ; Loss = 5.188044\n",
      "2024-12-14 14:08:27.199000: I runner.py:310] Step = 1800 ; steps/s = 1.64, tokens/s = 81388 (37448 source, 43940 target) ; Learning rate = 0.000159 ; Loss = 4.804553\n",
      "2024-12-14 14:09:28.616000: I runner.py:310] Step = 1900 ; steps/s = 1.63, tokens/s = 82286 (37852 source, 44434 target) ; Learning rate = 0.000168 ; Loss = 4.683407\n",
      "2024-12-14 14:10:30.074000: I runner.py:310] Step = 2000 ; steps/s = 1.63, tokens/s = 82227 (37815 source, 44412 target) ; Learning rate = 0.000177 ; Loss = 4.819321\n",
      "2024-12-14 14:11:31.143000: I runner.py:310] Step = 2100 ; steps/s = 1.64, tokens/s = 81399 (37435 source, 43964 target) ; Learning rate = 0.000186 ; Loss = 4.464334\n",
      "2024-12-14 14:12:32.578000: I runner.py:310] Step = 2200 ; steps/s = 1.63, tokens/s = 82252 (37835 source, 44417 target) ; Learning rate = 0.000195 ; Loss = 4.469248\n",
      "2024-12-14 14:13:34.061000: I runner.py:310] Step = 2300 ; steps/s = 1.63, tokens/s = 82205 (37802 source, 44403 target) ; Learning rate = 0.000203 ; Loss = 4.450048\n",
      "2024-12-14 14:14:35.562000: I runner.py:310] Step = 2400 ; steps/s = 1.63, tokens/s = 82180 (37801 source, 44379 target) ; Learning rate = 0.000212 ; Loss = 4.392965\n",
      "2024-12-14 14:15:36.654000: I runner.py:310] Step = 2500 ; steps/s = 1.64, tokens/s = 81350 (37399 source, 43951 target) ; Learning rate = 0.000221 ; Loss = 4.280954\n",
      "2024-12-14 14:16:38.139000: I runner.py:310] Step = 2600 ; steps/s = 1.63, tokens/s = 82179 (37803 source, 44376 target) ; Learning rate = 0.000230 ; Loss = 4.053326\n",
      "2024-12-14 14:17:39.620000: I runner.py:310] Step = 2700 ; steps/s = 1.63, tokens/s = 82216 (37821 source, 44395 target) ; Learning rate = 0.000239 ; Loss = 3.996893\n",
      "2024-12-14 14:18:40.739000: I runner.py:310] Step = 2800 ; steps/s = 1.64, tokens/s = 81295 (37382 source, 43913 target) ; Learning rate = 0.000248 ; Loss = 3.839219\n",
      "2024-12-14 14:19:42.170000: I runner.py:310] Step = 2900 ; steps/s = 1.63, tokens/s = 82256 (37833 source, 44423 target) ; Learning rate = 0.000256 ; Loss = 3.801831\n",
      "2024-12-14 14:20:43.625000: I runner.py:310] Step = 3000 ; steps/s = 1.63, tokens/s = 82222 (37806 source, 44416 target) ; Learning rate = 0.000265 ; Loss = 3.748975\n",
      "2024-12-14 14:21:45.118000: I runner.py:310] Step = 3100 ; steps/s = 1.63, tokens/s = 82183 (37813 source, 44370 target) ; Learning rate = 0.000274 ; Loss = 3.831250\n",
      "2024-12-14 14:22:46.292000: I runner.py:310] Step = 3200 ; steps/s = 1.63, tokens/s = 81253 (37364 source, 43889 target) ; Learning rate = 0.000283 ; Loss = 3.734201\n",
      "2024-12-14 14:23:47.785000: I runner.py:310] Step = 3300 ; steps/s = 1.63, tokens/s = 82168 (37773 source, 44395 target) ; Learning rate = 0.000292 ; Loss = 3.627896\n",
      "2024-12-14 14:24:49.227000: I runner.py:310] Step = 3400 ; steps/s = 1.63, tokens/s = 82272 (37862 source, 44410 target) ; Learning rate = 0.000301 ; Loss = 3.622998\n",
      "2024-12-14 14:25:50.404000: I runner.py:310] Step = 3500 ; steps/s = 1.63, tokens/s = 81242 (37372 source, 43870 target) ; Learning rate = 0.000309 ; Loss = 3.685481\n",
      "2024-12-14 14:26:51.839000: I runner.py:310] Step = 3600 ; steps/s = 1.63, tokens/s = 82272 (37854 source, 44418 target) ; Learning rate = 0.000318 ; Loss = 3.577305\n",
      "2024-12-14 14:27:53.310000: I runner.py:310] Step = 3700 ; steps/s = 1.63, tokens/s = 82189 (37793 source, 44396 target) ; Learning rate = 0.000327 ; Loss = 3.605834\n",
      "2024-12-14 14:28:54.818000: I runner.py:310] Step = 3800 ; steps/s = 1.63, tokens/s = 82159 (37781 source, 44378 target) ; Learning rate = 0.000336 ; Loss = 3.474161\n",
      "2024-12-14 14:29:55.869000: I runner.py:310] Step = 3900 ; steps/s = 1.64, tokens/s = 81422 (37448 source, 43974 target) ; Learning rate = 0.000345 ; Loss = 3.490453\n",
      "2024-12-14 14:30:57.338000: I runner.py:310] Step = 4000 ; steps/s = 1.63, tokens/s = 82214 (37833 source, 44381 target) ; Learning rate = 0.000354 ; Loss = 3.401840\n",
      "2024-12-14 14:31:58.823000: I runner.py:310] Step = 4100 ; steps/s = 1.63, tokens/s = 82184 (37788 source, 44396 target) ; Learning rate = 0.000362 ; Loss = 3.393780\n",
      "2024-12-14 14:32:59.881000: I runner.py:310] Step = 4200 ; steps/s = 1.64, tokens/s = 81414 (37444 source, 43970 target) ; Learning rate = 0.000371 ; Loss = 3.286565\n",
      "2024-12-14 14:34:01.368000: I runner.py:310] Step = 4300 ; steps/s = 1.63, tokens/s = 82177 (37774 source, 44403 target) ; Learning rate = 0.000380 ; Loss = 3.311205\n",
      "2024-12-14 14:35:02.858000: I runner.py:310] Step = 4400 ; steps/s = 1.63, tokens/s = 82170 (37794 source, 44376 target) ; Learning rate = 0.000389 ; Loss = 3.385959\n",
      "2024-12-14 14:36:04.321000: I runner.py:310] Step = 4500 ; steps/s = 1.63, tokens/s = 82250 (37858 source, 44392 target) ; Learning rate = 0.000398 ; Loss = 3.271845\n",
      "2024-12-14 14:37:05.425000: I runner.py:310] Step = 4600 ; steps/s = 1.64, tokens/s = 81331 (37394 source, 43937 target) ; Learning rate = 0.000407 ; Loss = 3.178083\n",
      "2024-12-14 14:38:06.880000: I runner.py:310] Step = 4700 ; steps/s = 1.63, tokens/s = 82238 (37831 source, 44407 target) ; Learning rate = 0.000416 ; Loss = 3.177977\n",
      "2024-12-14 14:39:08.362000: I runner.py:310] Step = 4800 ; steps/s = 1.63, tokens/s = 82194 (37808 source, 44386 target) ; Learning rate = 0.000424 ; Loss = 3.323573\n",
      "2024-12-14 14:40:09.383000: I runner.py:310] Step = 4900 ; steps/s = 1.64, tokens/s = 81431 (37434 source, 43997 target) ; Learning rate = 0.000433 ; Loss = 3.137028\n",
      "2024-12-14 14:41:10.858000: I runner.py:310] Step = 5000 ; steps/s = 1.63, tokens/s = 82211 (37820 source, 44391 target) ; Learning rate = 0.000442 ; Loss = 3.181325\n",
      "2024-12-14 14:41:10.860000: I training.py:192] Running evaluation for step 5000\n",
      "2024-12-14 14:49:00.965000: I training.py:192] Evaluation result for step 5000: loss = 2.219416 ; perplexity = 9.201955\n",
      "2024-12-14 14:50:02.474000: I runner.py:310] Step = 5100 ; steps/s = 1.63, tokens/s = 82195 (37813 source, 44382 target) ; Learning rate = 0.000451 ; Loss = 3.100470\n",
      "2024-12-14 14:51:04.065000: I runner.py:310] Step = 5200 ; steps/s = 1.62, tokens/s = 82039 (37728 source, 44311 target) ; Learning rate = 0.000460 ; Loss = 3.101640\n",
      "2024-12-14 14:52:05.176000: I runner.py:310] Step = 5300 ; steps/s = 1.64, tokens/s = 81340 (37418 source, 43922 target) ; Learning rate = 0.000469 ; Loss = 3.138941\n",
      "2024-12-14 14:53:06.792000: I runner.py:310] Step = 5400 ; steps/s = 1.62, tokens/s = 82021 (37721 source, 44300 target) ; Learning rate = 0.000477 ; Loss = 3.064174\n",
      "2024-12-14 14:54:08.378000: I runner.py:310] Step = 5500 ; steps/s = 1.62, tokens/s = 82048 (37745 source, 44303 target) ; Learning rate = 0.000486 ; Loss = 2.969890\n",
      "2024-12-14 14:55:09.531000: I runner.py:310] Step = 5600 ; steps/s = 1.64, tokens/s = 81292 (37387 source, 43905 target) ; Learning rate = 0.000495 ; Loss = 2.980420\n",
      "2024-12-14 14:56:11.078000: I runner.py:310] Step = 5700 ; steps/s = 1.62, tokens/s = 82114 (37773 source, 44341 target) ; Learning rate = 0.000504 ; Loss = 3.024560\n",
      "2024-12-14 14:57:12.629000: I runner.py:310] Step = 5800 ; steps/s = 1.62, tokens/s = 82084 (37739 source, 44345 target) ; Learning rate = 0.000513 ; Loss = 3.049461\n",
      "2024-12-14 14:58:14.173000: I runner.py:310] Step = 5900 ; steps/s = 1.63, tokens/s = 82120 (37771 source, 44349 target) ; Learning rate = 0.000522 ; Loss = 3.141361\n",
      "2024-12-14 14:59:15.332000: I runner.py:310] Step = 6000 ; steps/s = 1.64, tokens/s = 81289 (37386 source, 43903 target) ; Learning rate = 0.000530 ; Loss = 2.896097\n",
      "2024-12-14 15:00:16.908000: I runner.py:310] Step = 6100 ; steps/s = 1.62, tokens/s = 82071 (37749 source, 44322 target) ; Learning rate = 0.000539 ; Loss = 3.012262\n",
      "2024-12-14 15:01:18.527000: I runner.py:310] Step = 6200 ; steps/s = 1.62, tokens/s = 81995 (37719 source, 44276 target) ; Learning rate = 0.000548 ; Loss = 2.907971\n",
      "2024-12-14 15:02:19.682000: I runner.py:310] Step = 6300 ; steps/s = 1.64, tokens/s = 81256 (37352 source, 43904 target) ; Learning rate = 0.000557 ; Loss = 2.912786\n",
      "2024-12-14 15:03:21.209000: I runner.py:310] Step = 6400 ; steps/s = 1.63, tokens/s = 82116 (37760 source, 44356 target) ; Learning rate = 0.000566 ; Loss = 2.837362\n",
      "2024-12-14 15:04:22.797000: I runner.py:310] Step = 6500 ; steps/s = 1.62, tokens/s = 82063 (37752 source, 44311 target) ; Learning rate = 0.000575 ; Loss = 2.893057\n",
      "2024-12-14 15:05:24.290000: I runner.py:310] Step = 6600 ; steps/s = 1.63, tokens/s = 82202 (37818 source, 44384 target) ; Learning rate = 0.000583 ; Loss = 2.926079\n",
      "2024-12-14 15:06:25.342000: I runner.py:310] Step = 6700 ; steps/s = 1.64, tokens/s = 81410 (37438 source, 43972 target) ; Learning rate = 0.000592 ; Loss = 2.747747\n",
      "2024-12-14 15:07:26.818000: I runner.py:310] Step = 6800 ; steps/s = 1.63, tokens/s = 82197 (37798 source, 44399 target) ; Learning rate = 0.000601 ; Loss = 2.885138\n",
      "2024-12-14 15:08:28.353000: I runner.py:310] Step = 6900 ; steps/s = 1.63, tokens/s = 82104 (37763 source, 44341 target) ; Learning rate = 0.000610 ; Loss = 2.891532\n",
      "2024-12-14 15:09:29.459000: I runner.py:310] Step = 7000 ; steps/s = 1.64, tokens/s = 81360 (37444 source, 43916 target) ; Learning rate = 0.000619 ; Loss = 2.710329\n",
      "2024-12-14 15:10:30.982000: I runner.py:310] Step = 7100 ; steps/s = 1.63, tokens/s = 82142 (37764 source, 44378 target) ; Learning rate = 0.000628 ; Loss = 2.794745\n",
      "2024-12-14 15:11:32.504000: I runner.py:310] Step = 7200 ; steps/s = 1.63, tokens/s = 82158 (37813 source, 44345 target) ; Learning rate = 0.000636 ; Loss = 2.881444\n",
      "2024-12-14 15:12:33.986000: I runner.py:310] Step = 7300 ; steps/s = 1.63, tokens/s = 82177 (37774 source, 44403 target) ; Learning rate = 0.000645 ; Loss = 2.786776\n",
      "2024-12-14 15:13:35.081000: I runner.py:310] Step = 7400 ; steps/s = 1.64, tokens/s = 81360 (37414 source, 43946 target) ; Learning rate = 0.000654 ; Loss = 2.647692\n",
      "2024-12-14 15:14:36.559000: I runner.py:310] Step = 7500 ; steps/s = 1.63, tokens/s = 82201 (37817 source, 44384 target) ; Learning rate = 0.000663 ; Loss = 2.807182\n",
      "2024-12-14 15:15:38.070000: I runner.py:310] Step = 7600 ; steps/s = 1.63, tokens/s = 82157 (37784 source, 44373 target) ; Learning rate = 0.000672 ; Loss = 2.866505\n",
      "2024-12-14 15:16:39.087000: I runner.py:310] Step = 7700 ; steps/s = 1.64, tokens/s = 81414 (37444 source, 43970 target) ; Learning rate = 0.000681 ; Loss = 2.713424\n",
      "2024-12-14 15:17:40.573000: I runner.py:310] Step = 7800 ; steps/s = 1.63, tokens/s = 82188 (37798 source, 44390 target) ; Learning rate = 0.000690 ; Loss = 2.754289\n",
      "2024-12-14 15:18:42.060000: I runner.py:310] Step = 7900 ; steps/s = 1.63, tokens/s = 82210 (37809 source, 44401 target) ; Learning rate = 0.000698 ; Loss = 2.758862\n",
      "2024-12-14 15:19:43.415000: I runner.py:310] Step = 8000 ; steps/s = 1.63, tokens/s = 81926 (37685 source, 44241 target) ; Learning rate = 0.000707 ; Loss = 2.619710\n",
      "2024-12-14 15:20:44.651000: I runner.py:310] Step = 8100 ; steps/s = 1.63, tokens/s = 81593 (37517 source, 44076 target) ; Learning rate = 0.000716 ; Loss = 2.730151\n",
      "2024-12-14 15:21:46.196000: I runner.py:310] Step = 8200 ; steps/s = 1.63, tokens/s = 82124 (37773 source, 44351 target) ; Learning rate = 0.000725 ; Loss = 2.656725\n",
      "2024-12-14 15:22:47.661000: I runner.py:310] Step = 8300 ; steps/s = 1.63, tokens/s = 82229 (37834 source, 44395 target) ; Learning rate = 0.000734 ; Loss = 2.655066\n",
      "2024-12-14 15:23:48.741000: I runner.py:310] Step = 8400 ; steps/s = 1.64, tokens/s = 81379 (37425 source, 43954 target) ; Learning rate = 0.000743 ; Loss = 2.663766\n",
      "2024-12-14 15:24:50.216000: I runner.py:310] Step = 8500 ; steps/s = 1.63, tokens/s = 82208 (37814 source, 44394 target) ; Learning rate = 0.000751 ; Loss = 2.637730\n",
      "2024-12-14 15:25:51.555000: I runner.py:310] Step = 8600 ; steps/s = 1.63, tokens/s = 82394 (37898 source, 44496 target) ; Learning rate = 0.000760 ; Loss = 2.671813\n",
      "2024-12-14 15:26:52.657000: I runner.py:310] Step = 8700 ; steps/s = 1.64, tokens/s = 81332 (37405 source, 43927 target) ; Learning rate = 0.000769 ; Loss = 2.600786\n",
      "2024-12-14 15:27:54.093000: I runner.py:310] Step = 8800 ; steps/s = 1.63, tokens/s = 82249 (37822 source, 44427 target) ; Learning rate = 0.000778 ; Loss = 2.598858\n",
      "2024-12-14 15:28:55.546000: I runner.py:310] Step = 8900 ; steps/s = 1.63, tokens/s = 82233 (37816 source, 44417 target) ; Learning rate = 0.000787 ; Loss = 2.652523\n",
      "2024-12-14 15:29:57.023000: I runner.py:310] Step = 9000 ; steps/s = 1.63, tokens/s = 82205 (37816 source, 44389 target) ; Learning rate = 0.000796 ; Loss = 2.686025\n",
      "2024-12-14 15:30:58.081000: I runner.py:310] Step = 9100 ; steps/s = 1.64, tokens/s = 81392 (37442 source, 43950 target) ; Learning rate = 0.000804 ; Loss = 2.669324\n",
      "2024-12-14 15:31:59.566000: I runner.py:310] Step = 9200 ; steps/s = 1.63, tokens/s = 82147 (37777 source, 44370 target) ; Learning rate = 0.000813 ; Loss = 2.601101\n",
      "2024-12-14 15:33:01.027000: I runner.py:310] Step = 9300 ; steps/s = 1.63, tokens/s = 82236 (37828 source, 44408 target) ; Learning rate = 0.000822 ; Loss = 2.632755\n",
      "2024-12-14 15:34:02.029000: I runner.py:310] Step = 9400 ; steps/s = 1.64, tokens/s = 81515 (37492 source, 44023 target) ; Learning rate = 0.000831 ; Loss = 2.561465\n",
      "2024-12-14 15:35:03.501000: I runner.py:310] Step = 9500 ; steps/s = 1.63, tokens/s = 82203 (37808 source, 44395 target) ; Learning rate = 0.000840 ; Loss = 2.529471\n",
      "2024-12-14 15:36:04.947000: I runner.py:310] Step = 9600 ; steps/s = 1.63, tokens/s = 82237 (37821 source, 44416 target) ; Learning rate = 0.000849 ; Loss = 2.574098\n",
      "2024-12-14 15:37:06.403000: I runner.py:310] Step = 9700 ; steps/s = 1.63, tokens/s = 82232 (37822 source, 44410 target) ; Learning rate = 0.000857 ; Loss = 2.611725\n",
      "2024-12-14 15:38:07.472000: I runner.py:310] Step = 9800 ; steps/s = 1.64, tokens/s = 81383 (37428 source, 43955 target) ; Learning rate = 0.000866 ; Loss = 2.549284\n",
      "2024-12-14 15:39:08.936000: I runner.py:310] Step = 9900 ; steps/s = 1.63, tokens/s = 82203 (37796 source, 44407 target) ; Learning rate = 0.000875 ; Loss = 2.547047\n",
      "2024-12-14 15:40:10.411000: I runner.py:310] Step = 10000 ; steps/s = 1.63, tokens/s = 82224 (37830 source, 44394 target) ; Learning rate = 0.000884 ; Loss = 2.511610\n",
      "2024-12-14 15:40:12.541000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-10000\n",
      "2024-12-14 15:40:12.541000: I training.py:192] Running evaluation for step 10000\n",
      "2024-12-14 15:42:54.435000: I training.py:192] Evaluation result for step 10000: loss = 2.086928 ; perplexity = 8.060118\n",
      "2024-12-14 15:43:55.372000: I runner.py:310] Step = 10100 ; steps/s = 1.64, tokens/s = 81582 (37538 source, 44044 target) ; Learning rate = 0.000879 ; Loss = 2.475426\n",
      "2024-12-14 15:44:56.794000: I runner.py:310] Step = 10200 ; steps/s = 1.63, tokens/s = 82297 (37857 source, 44440 target) ; Learning rate = 0.000875 ; Loss = 2.514962\n",
      "2024-12-14 15:45:58.292000: I runner.py:310] Step = 10300 ; steps/s = 1.63, tokens/s = 82158 (37784 source, 44374 target) ; Learning rate = 0.000871 ; Loss = 2.613154\n",
      "2024-12-14 15:46:59.833000: I runner.py:310] Step = 10400 ; steps/s = 1.63, tokens/s = 82109 (37760 source, 44349 target) ; Learning rate = 0.000867 ; Loss = 2.604701\n",
      "2024-12-14 15:48:00.951000: I runner.py:310] Step = 10500 ; steps/s = 1.64, tokens/s = 81302 (37389 source, 43913 target) ; Learning rate = 0.000863 ; Loss = 2.423785\n",
      "2024-12-14 15:49:02.450000: I runner.py:310] Step = 10600 ; steps/s = 1.63, tokens/s = 82214 (37823 source, 44391 target) ; Learning rate = 0.000858 ; Loss = 2.483894\n",
      "2024-12-14 15:50:03.944000: I runner.py:310] Step = 10700 ; steps/s = 1.63, tokens/s = 82178 (37807 source, 44371 target) ; Learning rate = 0.000854 ; Loss = 2.552482\n",
      "2024-12-14 15:51:04.973000: I runner.py:310] Step = 10800 ; steps/s = 1.64, tokens/s = 81396 (37408 source, 43988 target) ; Learning rate = 0.000850 ; Loss = 2.441716\n",
      "2024-12-14 15:52:06.485000: I runner.py:310] Step = 10900 ; steps/s = 1.63, tokens/s = 82147 (37777 source, 44370 target) ; Learning rate = 0.000847 ; Loss = 2.461243\n",
      "2024-12-14 15:53:07.924000: I runner.py:310] Step = 11000 ; steps/s = 1.63, tokens/s = 82255 (37833 source, 44422 target) ; Learning rate = 0.000843 ; Loss = 2.449927\n",
      "2024-12-14 15:54:09.435000: I runner.py:310] Step = 11100 ; steps/s = 1.63, tokens/s = 82169 (37796 source, 44373 target) ; Learning rate = 0.000839 ; Loss = 2.425439\n",
      "2024-12-14 15:55:10.453000: I runner.py:310] Step = 11200 ; steps/s = 1.64, tokens/s = 81461 (37489 source, 43972 target) ; Learning rate = 0.000835 ; Loss = 2.468491\n",
      "2024-12-14 15:56:11.928000: I runner.py:310] Step = 11300 ; steps/s = 1.63, tokens/s = 82203 (37804 source, 44399 target) ; Learning rate = 0.000831 ; Loss = 2.398293\n",
      "2024-12-14 15:57:13.442000: I runner.py:310] Step = 11400 ; steps/s = 1.63, tokens/s = 82165 (37787 source, 44378 target) ; Learning rate = 0.000828 ; Loss = 2.447112\n",
      "2024-12-14 15:58:14.544000: I runner.py:310] Step = 11500 ; steps/s = 1.64, tokens/s = 81342 (37402 source, 43940 target) ; Learning rate = 0.000824 ; Loss = 2.423039\n",
      "2024-12-14 15:59:16.044000: I runner.py:310] Step = 11600 ; steps/s = 1.63, tokens/s = 82156 (37770 source, 44386 target) ; Learning rate = 0.000821 ; Loss = 2.456355\n",
      "2024-12-14 16:00:17.585000: I runner.py:310] Step = 11700 ; steps/s = 1.63, tokens/s = 82112 (37761 source, 44351 target) ; Learning rate = 0.000817 ; Loss = 2.395749\n",
      "2024-12-14 16:01:19.137000: I runner.py:310] Step = 11800 ; steps/s = 1.62, tokens/s = 82110 (37777 source, 44333 target) ; Learning rate = 0.000814 ; Loss = 2.463421\n",
      "2024-12-14 16:02:20.209000: I runner.py:310] Step = 11900 ; steps/s = 1.64, tokens/s = 81405 (37457 source, 43948 target) ; Learning rate = 0.000810 ; Loss = 2.462226\n",
      "2024-12-14 16:03:21.644000: I runner.py:310] Step = 12000 ; steps/s = 1.63, tokens/s = 82250 (37818 source, 44432 target) ; Learning rate = 0.000807 ; Loss = 2.401562\n",
      "2024-12-14 16:04:23.169000: I runner.py:310] Step = 12100 ; steps/s = 1.63, tokens/s = 82154 (37804 source, 44350 target) ; Learning rate = 0.000803 ; Loss = 2.398741\n",
      "2024-12-14 16:05:24.327000: I runner.py:310] Step = 12200 ; steps/s = 1.64, tokens/s = 81260 (37366 source, 43894 target) ; Learning rate = 0.000800 ; Loss = 2.338230\n",
      "2024-12-14 16:06:25.831000: I runner.py:310] Step = 12300 ; steps/s = 1.63, tokens/s = 82156 (37780 source, 44376 target) ; Learning rate = 0.000797 ; Loss = 2.377957\n",
      "2024-12-14 16:07:27.367000: I runner.py:310] Step = 12400 ; steps/s = 1.63, tokens/s = 82100 (37747 source, 44353 target) ; Learning rate = 0.000794 ; Loss = 2.407615\n",
      "2024-12-14 16:08:28.860000: I runner.py:310] Step = 12500 ; steps/s = 1.63, tokens/s = 82198 (37829 source, 44369 target) ; Learning rate = 0.000791 ; Loss = 2.402120\n",
      "2024-12-14 16:09:29.956000: I runner.py:310] Step = 12600 ; steps/s = 1.64, tokens/s = 81310 (37371 source, 43939 target) ; Learning rate = 0.000787 ; Loss = 2.398533\n",
      "2024-12-14 16:10:31.481000: I runner.py:310] Step = 12700 ; steps/s = 1.63, tokens/s = 82158 (37809 source, 44349 target) ; Learning rate = 0.000784 ; Loss = 2.341378\n",
      "2024-12-14 16:11:33.008000: I runner.py:310] Step = 12800 ; steps/s = 1.63, tokens/s = 82152 (37779 source, 44373 target) ; Learning rate = 0.000781 ; Loss = 2.388667\n",
      "2024-12-14 16:12:34.068000: I runner.py:310] Step = 12900 ; steps/s = 1.64, tokens/s = 81409 (37447 source, 43962 target) ; Learning rate = 0.000778 ; Loss = 2.354966\n",
      "2024-12-14 16:13:35.649000: I runner.py:310] Step = 13000 ; steps/s = 1.62, tokens/s = 82049 (37729 source, 44320 target) ; Learning rate = 0.000775 ; Loss = 2.315215\n",
      "2024-12-14 16:14:37.221000: I runner.py:310] Step = 13100 ; steps/s = 1.62, tokens/s = 82092 (37757 source, 44335 target) ; Learning rate = 0.000772 ; Loss = 2.331777\n",
      "2024-12-14 16:15:38.692000: I runner.py:310] Step = 13200 ; steps/s = 1.63, tokens/s = 82203 (37807 source, 44396 target) ; Learning rate = 0.000769 ; Loss = 2.352113\n",
      "2024-12-14 16:16:39.765000: I runner.py:310] Step = 13300 ; steps/s = 1.64, tokens/s = 81366 (37433 source, 43933 target) ; Learning rate = 0.000766 ; Loss = 2.406550\n",
      "2024-12-14 16:17:41.314000: I runner.py:310] Step = 13400 ; steps/s = 1.62, tokens/s = 82145 (37780 source, 44365 target) ; Learning rate = 0.000764 ; Loss = 2.309068\n",
      "2024-12-14 16:18:42.762000: I runner.py:310] Step = 13500 ; steps/s = 1.63, tokens/s = 82233 (37827 source, 44406 target) ; Learning rate = 0.000761 ; Loss = 2.322897\n",
      "2024-12-14 16:19:43.911000: I runner.py:310] Step = 13600 ; steps/s = 1.64, tokens/s = 81256 (37362 source, 43894 target) ; Learning rate = 0.000758 ; Loss = 2.323586\n",
      "2024-12-14 16:20:45.459000: I runner.py:310] Step = 13700 ; steps/s = 1.62, tokens/s = 82115 (37760 source, 44355 target) ; Learning rate = 0.000755 ; Loss = 2.292419\n",
      "2024-12-14 16:21:47.015000: I runner.py:310] Step = 13800 ; steps/s = 1.62, tokens/s = 82120 (37778 source, 44342 target) ; Learning rate = 0.000752 ; Loss = 2.313693\n",
      "2024-12-14 16:22:48.545000: I runner.py:310] Step = 13900 ; steps/s = 1.63, tokens/s = 82102 (37764 source, 44338 target) ; Learning rate = 0.000750 ; Loss = 2.296849\n",
      "2024-12-14 16:23:49.707000: I runner.py:310] Step = 14000 ; steps/s = 1.64, tokens/s = 81243 (37359 source, 43884 target) ; Learning rate = 0.000747 ; Loss = 2.320177\n",
      "2024-12-14 16:24:51.231000: I runner.py:310] Step = 14100 ; steps/s = 1.63, tokens/s = 82150 (37780 source, 44370 target) ; Learning rate = 0.000744 ; Loss = 2.254447\n",
      "2024-12-14 16:25:52.757000: I runner.py:310] Step = 14200 ; steps/s = 1.63, tokens/s = 82127 (37774 source, 44353 target) ; Learning rate = 0.000742 ; Loss = 2.326027\n",
      "2024-12-14 16:26:53.824000: I runner.py:310] Step = 14300 ; steps/s = 1.64, tokens/s = 81418 (37457 source, 43961 target) ; Learning rate = 0.000739 ; Loss = 2.269013\n",
      "2024-12-14 16:27:55.298000: I runner.py:310] Step = 14400 ; steps/s = 1.63, tokens/s = 82192 (37791 source, 44401 target) ; Learning rate = 0.000737 ; Loss = 2.263851\n",
      "2024-12-14 16:28:56.850000: I runner.py:310] Step = 14500 ; steps/s = 1.62, tokens/s = 82113 (37772 source, 44341 target) ; Learning rate = 0.000734 ; Loss = 2.257271\n",
      "2024-12-14 16:29:58.344000: I runner.py:310] Step = 14600 ; steps/s = 1.63, tokens/s = 82192 (37813 source, 44379 target) ; Learning rate = 0.000731 ; Loss = 2.311875\n",
      "2024-12-14 16:30:59.487000: I runner.py:310] Step = 14700 ; steps/s = 1.64, tokens/s = 81247 (37348 source, 43899 target) ; Learning rate = 0.000729 ; Loss = 2.254858\n",
      "2024-12-14 16:32:00.954000: I runner.py:310] Step = 14800 ; steps/s = 1.63, tokens/s = 82233 (37835 source, 44398 target) ; Learning rate = 0.000727 ; Loss = 2.260912\n",
      "2024-12-14 16:33:02.435000: I runner.py:310] Step = 14900 ; steps/s = 1.63, tokens/s = 82212 (37820 source, 44392 target) ; Learning rate = 0.000724 ; Loss = 2.252257\n",
      "2024-12-14 16:34:03.536000: I runner.py:310] Step = 15000 ; steps/s = 1.64, tokens/s = 81337 (37412 source, 43925 target) ; Learning rate = 0.000722 ; Loss = 2.168524\n",
      "2024-12-14 16:34:03.537000: I training.py:192] Running evaluation for step 15000\n",
      "2024-12-14 16:36:15.093000: I training.py:192] Evaluation result for step 15000: loss = 2.176992 ; perplexity = 8.819734\n",
      "2024-12-14 16:37:16.438000: I runner.py:310] Step = 15100 ; steps/s = 1.63, tokens/s = 82389 (37895 source, 44494 target) ; Learning rate = 0.000719 ; Loss = 2.233816\n",
      "2024-12-14 16:38:17.963000: I runner.py:310] Step = 15200 ; steps/s = 1.63, tokens/s = 82117 (37754 source, 44363 target) ; Learning rate = 0.000717 ; Loss = 2.289498\n",
      "2024-12-14 16:39:19.539000: I runner.py:310] Step = 15300 ; steps/s = 1.62, tokens/s = 82090 (37760 source, 44330 target) ; Learning rate = 0.000715 ; Loss = 2.276910\n",
      "2024-12-14 16:40:20.623000: I runner.py:310] Step = 15400 ; steps/s = 1.64, tokens/s = 81358 (37428 source, 43930 target) ; Learning rate = 0.000712 ; Loss = 2.246106\n",
      "2024-12-14 16:41:22.086000: I runner.py:310] Step = 15500 ; steps/s = 1.63, tokens/s = 82225 (37808 source, 44417 target) ; Learning rate = 0.000710 ; Loss = 2.240883\n",
      "2024-12-14 16:42:23.605000: I runner.py:310] Step = 15600 ; steps/s = 1.63, tokens/s = 82149 (37784 source, 44365 target) ; Learning rate = 0.000708 ; Loss = 2.232914\n",
      "2024-12-14 16:43:24.646000: I runner.py:310] Step = 15700 ; steps/s = 1.64, tokens/s = 81404 (37439 source, 43965 target) ; Learning rate = 0.000705 ; Loss = 2.225573\n",
      "2024-12-14 16:44:25.991000: I runner.py:310] Step = 15800 ; steps/s = 1.63, tokens/s = 82371 (37879 source, 44492 target) ; Learning rate = 0.000703 ; Loss = 2.171764\n",
      "2024-12-14 16:45:27.424000: I runner.py:310] Step = 15900 ; steps/s = 1.63, tokens/s = 82283 (37860 source, 44423 target) ; Learning rate = 0.000701 ; Loss = 2.219489\n",
      "2024-12-14 16:46:28.917000: I runner.py:310] Step = 16000 ; steps/s = 1.63, tokens/s = 82184 (37798 source, 44386 target) ; Learning rate = 0.000699 ; Loss = 2.223221\n",
      "2024-12-14 16:47:29.994000: I runner.py:310] Step = 16100 ; steps/s = 1.64, tokens/s = 81404 (37458 source, 43946 target) ; Learning rate = 0.000697 ; Loss = 2.166740\n",
      "2024-12-14 16:48:31.454000: I runner.py:310] Step = 16200 ; steps/s = 1.63, tokens/s = 82251 (37827 source, 44424 target) ; Learning rate = 0.000694 ; Loss = 2.224724\n",
      "2024-12-14 16:49:32.944000: I runner.py:310] Step = 16300 ; steps/s = 1.63, tokens/s = 82154 (37781 source, 44373 target) ; Learning rate = 0.000692 ; Loss = 2.244094\n",
      "2024-12-14 16:50:33.994000: I runner.py:310] Step = 16400 ; steps/s = 1.64, tokens/s = 81442 (37480 source, 43962 target) ; Learning rate = 0.000690 ; Loss = 2.143627\n",
      "2024-12-14 16:51:35.382000: I runner.py:310] Step = 16500 ; steps/s = 1.63, tokens/s = 82335 (37872 source, 44463 target) ; Learning rate = 0.000688 ; Loss = 2.223025\n",
      "2024-12-14 16:52:36.903000: I runner.py:310] Step = 16600 ; steps/s = 1.63, tokens/s = 82154 (37784 source, 44370 target) ; Learning rate = 0.000686 ; Loss = 2.209890\n",
      "2024-12-14 16:53:38.103000: I runner.py:310] Step = 16700 ; steps/s = 1.63, tokens/s = 81376 (37394 source, 43982 target) ; Learning rate = 0.000684 ; Loss = 2.329308\n",
      "2024-12-14 16:54:39.477000: I runner.py:310] Step = 16800 ; steps/s = 1.63, tokens/s = 82093 (37756 source, 44337 target) ; Learning rate = 0.000682 ; Loss = 2.183604\n",
      "2024-12-14 16:55:40.964000: I runner.py:310] Step = 16900 ; steps/s = 1.63, tokens/s = 82187 (37800 source, 44387 target) ; Learning rate = 0.000680 ; Loss = 2.149700\n",
      "2024-12-14 16:56:42.386000: I runner.py:310] Step = 17000 ; steps/s = 1.63, tokens/s = 82286 (37848 source, 44438 target) ; Learning rate = 0.000678 ; Loss = 2.191656\n",
      "2024-12-14 16:57:43.427000: I runner.py:310] Step = 17100 ; steps/s = 1.64, tokens/s = 81385 (37420 source, 43965 target) ; Learning rate = 0.000676 ; Loss = 2.208597\n",
      "2024-12-14 16:58:44.887000: I runner.py:310] Step = 17200 ; steps/s = 1.63, tokens/s = 82237 (37816 source, 44421 target) ; Learning rate = 0.000674 ; Loss = 2.170534\n",
      "2024-12-14 16:59:46.356000: I runner.py:310] Step = 17300 ; steps/s = 1.63, tokens/s = 82242 (37847 source, 44395 target) ; Learning rate = 0.000672 ; Loss = 2.185500\n",
      "2024-12-14 17:00:47.430000: I runner.py:310] Step = 17400 ; steps/s = 1.64, tokens/s = 81383 (37432 source, 43951 target) ; Learning rate = 0.000670 ; Loss = 2.149329\n",
      "2024-12-14 17:01:48.912000: I runner.py:310] Step = 17500 ; steps/s = 1.63, tokens/s = 82172 (37787 source, 44385 target) ; Learning rate = 0.000668 ; Loss = 2.176369\n",
      "2024-12-14 17:02:50.445000: I runner.py:310] Step = 17600 ; steps/s = 1.63, tokens/s = 82133 (37763 source, 44370 target) ; Learning rate = 0.000666 ; Loss = 2.168046\n",
      "2024-12-14 17:03:52.044000: I runner.py:310] Step = 17700 ; steps/s = 1.62, tokens/s = 82065 (37763 source, 44302 target) ; Learning rate = 0.000664 ; Loss = 2.155950\n",
      "2024-12-14 17:04:53.127000: I runner.py:310] Step = 17800 ; steps/s = 1.64, tokens/s = 81327 (37396 source, 43931 target) ; Learning rate = 0.000662 ; Loss = 2.182234\n",
      "2024-12-14 17:05:54.524000: I runner.py:310] Step = 17900 ; steps/s = 1.63, tokens/s = 82303 (37858 source, 44445 target) ; Learning rate = 0.000661 ; Loss = 2.107413\n",
      "2024-12-14 17:06:55.999000: I runner.py:310] Step = 18000 ; steps/s = 1.63, tokens/s = 82242 (37832 source, 44410 target) ; Learning rate = 0.000659 ; Loss = 2.144329\n",
      "2024-12-14 17:07:57.165000: I runner.py:310] Step = 18100 ; steps/s = 1.64, tokens/s = 81259 (37378 source, 43881 target) ; Learning rate = 0.000657 ; Loss = 2.126118\n",
      "2024-12-14 17:08:58.681000: I runner.py:310] Step = 18200 ; steps/s = 1.63, tokens/s = 82170 (37809 source, 44361 target) ; Learning rate = 0.000655 ; Loss = 2.107296\n",
      "2024-12-14 17:10:00.149000: I runner.py:310] Step = 18300 ; steps/s = 1.63, tokens/s = 82159 (37761 source, 44398 target) ; Learning rate = 0.000653 ; Loss = 2.134326\n",
      "2024-12-14 17:11:01.636000: I runner.py:310] Step = 18400 ; steps/s = 1.63, tokens/s = 82209 (37818 source, 44391 target) ; Learning rate = 0.000652 ; Loss = 2.192799\n",
      "2024-12-14 17:12:02.701000: I runner.py:310] Step = 18500 ; steps/s = 1.64, tokens/s = 81367 (37405 source, 43962 target) ; Learning rate = 0.000650 ; Loss = 2.150404\n",
      "2024-12-14 17:13:04.249000: I runner.py:310] Step = 18600 ; steps/s = 1.62, tokens/s = 82098 (37769 source, 44329 target) ; Learning rate = 0.000648 ; Loss = 2.118545\n",
      "2024-12-14 17:14:05.770000: I runner.py:310] Step = 18700 ; steps/s = 1.63, tokens/s = 82173 (37801 source, 44372 target) ; Learning rate = 0.000646 ; Loss = 2.124777\n",
      "2024-12-14 17:15:06.884000: I runner.py:310] Step = 18800 ; steps/s = 1.64, tokens/s = 81321 (37398 source, 43923 target) ; Learning rate = 0.000645 ; Loss = 2.096018\n",
      "2024-12-14 17:16:08.379000: I runner.py:310] Step = 18900 ; steps/s = 1.63, tokens/s = 82174 (37778 source, 44396 target) ; Learning rate = 0.000643 ; Loss = 2.108807\n",
      "2024-12-14 17:17:09.839000: I runner.py:310] Step = 19000 ; steps/s = 1.63, tokens/s = 82228 (37825 source, 44403 target) ; Learning rate = 0.000641 ; Loss = 2.103573\n",
      "2024-12-14 17:18:11.303000: I runner.py:310] Step = 19100 ; steps/s = 1.63, tokens/s = 82231 (37842 source, 44389 target) ; Learning rate = 0.000640 ; Loss = 2.115418\n",
      "2024-12-14 17:19:12.392000: I runner.py:310] Step = 19200 ; steps/s = 1.64, tokens/s = 81328 (37387 source, 43941 target) ; Learning rate = 0.000638 ; Loss = 2.020041\n",
      "2024-12-14 17:20:13.857000: I runner.py:310] Step = 19300 ; steps/s = 1.63, tokens/s = 82259 (37847 source, 44412 target) ; Learning rate = 0.000636 ; Loss = 2.123551\n",
      "2024-12-14 17:21:15.301000: I runner.py:310] Step = 19400 ; steps/s = 1.63, tokens/s = 82251 (37825 source, 44426 target) ; Learning rate = 0.000635 ; Loss = 2.160899\n",
      "2024-12-14 17:22:16.437000: I runner.py:310] Step = 19500 ; steps/s = 1.64, tokens/s = 81283 (37382 source, 43901 target) ; Learning rate = 0.000633 ; Loss = 2.116840\n",
      "2024-12-14 17:23:17.983000: I runner.py:310] Step = 19600 ; steps/s = 1.62, tokens/s = 82094 (37752 source, 44342 target) ; Learning rate = 0.000631 ; Loss = 2.088465\n",
      "2024-12-14 17:24:19.547000: I runner.py:310] Step = 19700 ; steps/s = 1.62, tokens/s = 82102 (37764 source, 44338 target) ; Learning rate = 0.000630 ; Loss = 2.087051\n",
      "2024-12-14 17:25:21.051000: I runner.py:310] Step = 19800 ; steps/s = 1.63, tokens/s = 82163 (37796 source, 44367 target) ; Learning rate = 0.000628 ; Loss = 2.120443\n",
      "2024-12-14 17:26:22.172000: I runner.py:310] Step = 19900 ; steps/s = 1.64, tokens/s = 81348 (37436 source, 43912 target) ; Learning rate = 0.000627 ; Loss = 2.053349\n",
      "2024-12-14 17:27:23.741000: I runner.py:310] Step = 20000 ; steps/s = 1.62, tokens/s = 82072 (37759 source, 44313 target) ; Learning rate = 0.000625 ; Loss = 2.100803\n",
      "2024-12-14 17:27:25.783000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-20000\n",
      "2024-12-14 17:27:25.783000: I training.py:192] Running evaluation for step 20000\n",
      "2024-12-14 17:29:38.720000: I training.py:192] Evaluation result for step 20000: loss = 2.254441 ; perplexity = 9.529963\n",
      "2024-12-14 17:30:40.090000: I runner.py:310] Step = 20100 ; steps/s = 1.63, tokens/s = 82369 (37878 source, 44491 target) ; Learning rate = 0.000623 ; Loss = 2.127112\n",
      "2024-12-14 17:31:41.095000: I runner.py:310] Step = 20200 ; steps/s = 1.64, tokens/s = 81459 (37447 source, 44012 target) ; Learning rate = 0.000622 ; Loss = 2.052664\n",
      "2024-12-14 17:32:42.577000: I runner.py:310] Step = 20300 ; steps/s = 1.63, tokens/s = 82213 (37812 source, 44401 target) ; Learning rate = 0.000620 ; Loss = 2.079463\n",
      "2024-12-14 17:33:44.123000: I runner.py:310] Step = 20400 ; steps/s = 1.62, tokens/s = 82086 (37763 source, 44323 target) ; Learning rate = 0.000619 ; Loss = 2.104108\n",
      "2024-12-14 17:34:45.688000: I runner.py:310] Step = 20500 ; steps/s = 1.62, tokens/s = 82053 (37722 source, 44331 target) ; Learning rate = 0.000617 ; Loss = 2.156986\n",
      "2024-12-14 17:35:46.833000: I runner.py:310] Step = 20600 ; steps/s = 1.64, tokens/s = 81278 (37370 source, 43908 target) ; Learning rate = 0.000616 ; Loss = 2.106314\n",
      "2024-12-14 17:36:48.350000: I runner.py:310] Step = 20700 ; steps/s = 1.63, tokens/s = 82163 (37787 source, 44376 target) ; Learning rate = 0.000614 ; Loss = 2.044722\n",
      "2024-12-14 17:37:49.873000: I runner.py:310] Step = 20800 ; steps/s = 1.63, tokens/s = 82153 (37793 source, 44360 target) ; Learning rate = 0.000613 ; Loss = 2.067851\n",
      "2024-12-14 17:38:50.990000: I runner.py:310] Step = 20900 ; steps/s = 1.64, tokens/s = 81335 (37424 source, 43911 target) ; Learning rate = 0.000611 ; Loss = 2.060724\n",
      "2024-12-14 17:39:52.469000: I runner.py:310] Step = 21000 ; steps/s = 1.63, tokens/s = 82164 (37767 source, 44397 target) ; Learning rate = 0.000610 ; Loss = 2.035565\n",
      "2024-12-14 17:40:53.979000: I runner.py:310] Step = 21100 ; steps/s = 1.63, tokens/s = 82143 (37767 source, 44376 target) ; Learning rate = 0.000608 ; Loss = 2.089687\n",
      "2024-12-14 17:41:55.500000: I runner.py:310] Step = 21200 ; steps/s = 1.63, tokens/s = 82164 (37823 source, 44341 target) ; Learning rate = 0.000607 ; Loss = 2.111054\n",
      "2024-12-14 17:42:56.616000: I runner.py:310] Step = 21300 ; steps/s = 1.64, tokens/s = 81324 (37392 source, 43932 target) ; Learning rate = 0.000606 ; Loss = 2.123134\n",
      "2024-12-14 17:43:58.135000: I runner.py:310] Step = 21400 ; steps/s = 1.63, tokens/s = 82174 (37814 source, 44360 target) ; Learning rate = 0.000604 ; Loss = 2.050700\n",
      "2024-12-14 17:44:59.654000: I runner.py:310] Step = 21500 ; steps/s = 1.63, tokens/s = 82119 (37756 source, 44363 target) ; Learning rate = 0.000603 ; Loss = 2.070689\n",
      "2024-12-14 17:46:00.729000: I runner.py:310] Step = 21600 ; steps/s = 1.64, tokens/s = 81412 (37459 source, 43953 target) ; Learning rate = 0.000601 ; Loss = 2.055042\n",
      "2024-12-14 17:47:02.200000: I runner.py:310] Step = 21700 ; steps/s = 1.63, tokens/s = 82194 (37787 source, 44407 target) ; Learning rate = 0.000600 ; Loss = 2.053016\n",
      "2024-12-14 17:48:03.631000: I runner.py:310] Step = 21800 ; steps/s = 1.63, tokens/s = 82247 (37829 source, 44418 target) ; Learning rate = 0.000599 ; Loss = 2.052649\n",
      "2024-12-14 17:49:05.160000: I runner.py:310] Step = 21900 ; steps/s = 1.63, tokens/s = 82158 (37800 source, 44358 target) ; Learning rate = 0.000597 ; Loss = 2.062600\n",
      "2024-12-14 17:50:06.293000: I runner.py:310] Step = 22000 ; steps/s = 1.64, tokens/s = 81272 (37380 source, 43892 target) ; Learning rate = 0.000596 ; Loss = 2.014562\n",
      "2024-12-14 17:51:07.798000: I runner.py:310] Step = 22100 ; steps/s = 1.63, tokens/s = 82165 (37783 source, 44382 target) ; Learning rate = 0.000595 ; Loss = 2.061655\n",
      "2024-12-14 17:52:09.311000: I runner.py:310] Step = 22200 ; steps/s = 1.63, tokens/s = 82183 (37808 source, 44375 target) ; Learning rate = 0.000593 ; Loss = 2.078281\n",
      "2024-12-14 17:53:10.443000: I runner.py:310] Step = 22300 ; steps/s = 1.64, tokens/s = 81301 (37392 source, 43909 target) ; Learning rate = 0.000592 ; Loss = 2.017750\n",
      "2024-12-14 17:54:12.024000: I runner.py:310] Step = 22400 ; steps/s = 1.62, tokens/s = 82064 (37735 source, 44329 target) ; Learning rate = 0.000591 ; Loss = 2.045367\n",
      "2024-12-14 17:55:13.544000: I runner.py:310] Step = 22500 ; steps/s = 1.63, tokens/s = 82149 (37771 source, 44378 target) ; Learning rate = 0.000589 ; Loss = 2.097299\n",
      "2024-12-14 17:56:15.001000: I runner.py:310] Step = 22600 ; steps/s = 1.63, tokens/s = 82226 (37835 source, 44391 target) ; Learning rate = 0.000588 ; Loss = 2.094544\n",
      "2024-12-14 17:57:16.232000: I runner.py:310] Step = 22700 ; steps/s = 1.63, tokens/s = 81183 (37353 source, 43830 target) ; Learning rate = 0.000587 ; Loss = 2.011231\n",
      "2024-12-14 17:58:17.719000: I runner.py:310] Step = 22800 ; steps/s = 1.63, tokens/s = 82189 (37798 source, 44391 target) ; Learning rate = 0.000585 ; Loss = 2.057642\n",
      "2024-12-14 17:59:19.261000: I runner.py:310] Step = 22900 ; steps/s = 1.63, tokens/s = 82091 (37755 source, 44336 target) ; Learning rate = 0.000584 ; Loss = 2.047786\n",
      "2024-12-14 18:00:20.441000: I runner.py:310] Step = 23000 ; steps/s = 1.63, tokens/s = 81240 (37358 source, 43882 target) ; Learning rate = 0.000583 ; Loss = 2.048920\n",
      "2024-12-14 18:01:21.949000: I runner.py:310] Step = 23100 ; steps/s = 1.63, tokens/s = 82150 (37789 source, 44361 target) ; Learning rate = 0.000582 ; Loss = 2.007492\n",
      "2024-12-14 18:02:23.422000: I runner.py:310] Step = 23200 ; steps/s = 1.63, tokens/s = 82214 (37814 source, 44400 target) ; Learning rate = 0.000580 ; Loss = 2.037649\n",
      "2024-12-14 18:03:25.015000: I runner.py:310] Step = 23300 ; steps/s = 1.62, tokens/s = 82065 (37745 source, 44320 target) ; Learning rate = 0.000579 ; Loss = 2.051197\n",
      "2024-12-14 18:04:26.120000: I runner.py:310] Step = 23400 ; steps/s = 1.64, tokens/s = 81354 (37419 source, 43935 target) ; Learning rate = 0.000578 ; Loss = 2.028720\n",
      "2024-12-14 18:05:27.596000: I runner.py:310] Step = 23500 ; steps/s = 1.63, tokens/s = 82197 (37795 source, 44402 target) ; Learning rate = 0.000577 ; Loss = 2.008855\n",
      "2024-12-14 18:06:29.123000: I runner.py:310] Step = 23600 ; steps/s = 1.63, tokens/s = 82123 (37773 source, 44350 target) ; Learning rate = 0.000575 ; Loss = 2.035508\n",
      "2024-12-14 18:07:30.189000: I runner.py:310] Step = 23700 ; steps/s = 1.64, tokens/s = 81379 (37421 source, 43958 target) ; Learning rate = 0.000574 ; Loss = 1.978524\n",
      "2024-12-14 18:08:31.682000: I runner.py:310] Step = 23800 ; steps/s = 1.63, tokens/s = 82210 (37821 source, 44389 target) ; Learning rate = 0.000573 ; Loss = 2.057442\n",
      "2024-12-14 18:09:33.208000: I runner.py:310] Step = 23900 ; steps/s = 1.63, tokens/s = 82130 (37777 source, 44353 target) ; Learning rate = 0.000572 ; Loss = 2.061953\n",
      "2024-12-14 18:10:34.701000: I runner.py:310] Step = 24000 ; steps/s = 1.63, tokens/s = 82167 (37794 source, 44373 target) ; Learning rate = 0.000571 ; Loss = 2.070644\n",
      "2024-12-14 18:11:35.725000: I runner.py:310] Step = 24100 ; steps/s = 1.64, tokens/s = 81454 (37463 source, 43991 target) ; Learning rate = 0.000569 ; Loss = 1.996213\n",
      "2024-12-14 18:12:37.162000: I runner.py:310] Step = 24200 ; steps/s = 1.63, tokens/s = 82239 (37817 source, 44422 target) ; Learning rate = 0.000568 ; Loss = 2.050111\n",
      "2024-12-14 18:13:38.650000: I runner.py:310] Step = 24300 ; steps/s = 1.63, tokens/s = 82189 (37802 source, 44387 target) ; Learning rate = 0.000567 ; Loss = 2.073849\n",
      "2024-12-14 18:14:39.691000: I runner.py:310] Step = 24400 ; steps/s = 1.64, tokens/s = 81435 (37471 source, 43964 target) ; Learning rate = 0.000566 ; Loss = 1.972368\n",
      "2024-12-14 18:15:41.167000: I runner.py:310] Step = 24500 ; steps/s = 1.63, tokens/s = 82196 (37787 source, 44409 target) ; Learning rate = 0.000565 ; Loss = 2.009437\n",
      "2024-12-14 18:16:42.708000: I runner.py:310] Step = 24600 ; steps/s = 1.63, tokens/s = 82109 (37779 source, 44330 target) ; Learning rate = 0.000564 ; Loss = 2.045364\n",
      "2024-12-14 18:17:44.127000: I runner.py:310] Step = 24700 ; steps/s = 1.63, tokens/s = 82017 (37715 source, 44302 target) ; Learning rate = 0.000562 ; Loss = 2.015133\n",
      "2024-12-14 18:18:45.319000: I runner.py:310] Step = 24800 ; steps/s = 1.63, tokens/s = 81490 (37477 source, 44013 target) ; Learning rate = 0.000561 ; Loss = 1.974327\n",
      "2024-12-14 18:19:46.859000: I runner.py:310] Step = 24900 ; steps/s = 1.63, tokens/s = 82117 (37775 source, 44342 target) ; Learning rate = 0.000560 ; Loss = 2.046891\n",
      "2024-12-14 18:20:48.323000: I runner.py:310] Step = 25000 ; steps/s = 1.63, tokens/s = 82249 (37842 source, 44407 target) ; Learning rate = 0.000559 ; Loss = 2.033254\n",
      "2024-12-14 18:20:48.325000: I training.py:192] Running evaluation for step 25000\n",
      "2024-12-14 18:23:00.646000: I training.py:192] Evaluation result for step 25000: loss = 2.309426 ; perplexity = 10.068646\n",
      "2024-12-14 18:24:01.498000: I runner.py:310] Step = 25100 ; steps/s = 1.64, tokens/s = 81686 (37561 source, 44125 target) ; Learning rate = 0.000558 ; Loss = 1.985231\n",
      "2024-12-14 18:25:02.967000: I runner.py:310] Step = 25200 ; steps/s = 1.63, tokens/s = 82189 (37797 source, 44392 target) ; Learning rate = 0.000557 ; Loss = 2.012771\n",
      "2024-12-14 18:26:04.512000: I runner.py:310] Step = 25300 ; steps/s = 1.62, tokens/s = 82135 (37789 source, 44346 target) ; Learning rate = 0.000556 ; Loss = 2.025940\n",
      "2024-12-14 18:27:05.621000: I runner.py:310] Step = 25400 ; steps/s = 1.64, tokens/s = 81328 (37394 source, 43934 target) ; Learning rate = 0.000555 ; Loss = 2.006803\n",
      "2024-12-14 18:28:07.141000: I runner.py:310] Step = 25500 ; steps/s = 1.63, tokens/s = 82158 (37789 source, 44369 target) ; Learning rate = 0.000553 ; Loss = 1.992620\n",
      "2024-12-14 18:29:08.707000: I runner.py:310] Step = 25600 ; steps/s = 1.62, tokens/s = 82101 (37767 source, 44334 target) ; Learning rate = 0.000552 ; Loss = 2.008960\n",
      "2024-12-14 18:30:10.243000: I runner.py:310] Step = 25700 ; steps/s = 1.63, tokens/s = 82079 (37746 source, 44333 target) ; Learning rate = 0.000551 ; Loss = 2.025018\n",
      "2024-12-14 18:31:11.342000: I runner.py:310] Step = 25800 ; steps/s = 1.64, tokens/s = 81367 (37421 source, 43946 target) ; Learning rate = 0.000550 ; Loss = 2.031482\n",
      "2024-12-14 18:32:12.831000: I runner.py:310] Step = 25900 ; steps/s = 1.63, tokens/s = 82182 (37791 source, 44391 target) ; Learning rate = 0.000549 ; Loss = 1.979580\n",
      "2024-12-14 18:33:14.407000: I runner.py:310] Step = 26000 ; steps/s = 1.62, tokens/s = 82076 (37745 source, 44331 target) ; Learning rate = 0.000548 ; Loss = 1.999767\n",
      "2024-12-14 18:34:15.559000: I runner.py:310] Step = 26100 ; steps/s = 1.64, tokens/s = 81266 (37390 source, 43876 target) ; Learning rate = 0.000547 ; Loss = 1.987027\n",
      "2024-12-14 18:35:17.093000: I runner.py:310] Step = 26200 ; steps/s = 1.63, tokens/s = 82122 (37765 source, 44357 target) ; Learning rate = 0.000546 ; Loss = 1.959053\n",
      "2024-12-14 18:36:18.575000: I runner.py:310] Step = 26300 ; steps/s = 1.63, tokens/s = 82189 (37801 source, 44388 target) ; Learning rate = 0.000545 ; Loss = 2.015821\n",
      "2024-12-14 18:37:20.054000: I runner.py:310] Step = 26400 ; steps/s = 1.63, tokens/s = 82205 (37817 source, 44388 target) ; Learning rate = 0.000544 ; Loss = 2.045350\n",
      "2024-12-14 18:38:21.165000: I runner.py:310] Step = 26500 ; steps/s = 1.64, tokens/s = 81305 (37383 source, 43922 target) ; Learning rate = 0.000543 ; Loss = 1.945046\n",
      "2024-12-14 18:39:22.727000: I runner.py:310] Step = 26600 ; steps/s = 1.62, tokens/s = 82091 (37754 source, 44337 target) ; Learning rate = 0.000542 ; Loss = 1.986841\n",
      "2024-12-14 18:40:24.318000: I runner.py:310] Step = 26700 ; steps/s = 1.62, tokens/s = 82073 (37751 source, 44322 target) ; Learning rate = 0.000541 ; Loss = 2.010754\n",
      "2024-12-14 18:41:25.469000: I runner.py:310] Step = 26800 ; steps/s = 1.64, tokens/s = 81281 (37398 source, 43883 target) ; Learning rate = 0.000540 ; Loss = 1.976838\n",
      "2024-12-14 18:42:27.009000: I runner.py:310] Step = 26900 ; steps/s = 1.63, tokens/s = 82096 (37753 source, 44343 target) ; Learning rate = 0.000539 ; Loss = 1.999623\n",
      "2024-12-14 18:43:28.545000: I runner.py:310] Step = 27000 ; steps/s = 1.63, tokens/s = 82146 (37794 source, 44352 target) ; Learning rate = 0.000538 ; Loss = 1.982897\n",
      "2024-12-14 18:44:30.120000: I runner.py:310] Step = 27100 ; steps/s = 1.62, tokens/s = 82054 (37735 source, 44319 target) ; Learning rate = 0.000537 ; Loss = 2.008031\n",
      "2024-12-14 18:45:31.340000: I runner.py:310] Step = 27200 ; steps/s = 1.63, tokens/s = 81162 (37317 source, 43845 target) ; Learning rate = 0.000536 ; Loss = 2.005731\n",
      "2024-12-14 18:46:32.876000: I runner.py:310] Step = 27300 ; steps/s = 1.63, tokens/s = 82126 (37773 source, 44353 target) ; Learning rate = 0.000535 ; Loss = 1.957375\n",
      "2024-12-14 18:47:34.377000: I runner.py:310] Step = 27400 ; steps/s = 1.63, tokens/s = 82187 (37811 source, 44376 target) ; Learning rate = 0.000534 ; Loss = 1.971439\n",
      "2024-12-14 18:48:35.586000: I runner.py:310] Step = 27500 ; steps/s = 1.63, tokens/s = 81209 (37342 source, 43867 target) ; Learning rate = 0.000533 ; Loss = 1.984532\n",
      "2024-12-14 18:49:37.139000: I runner.py:310] Step = 27600 ; steps/s = 1.62, tokens/s = 82092 (37749 source, 44343 target) ; Learning rate = 0.000532 ; Loss = 1.972117\n",
      "2024-12-14 18:50:38.646000: I runner.py:310] Step = 27700 ; steps/s = 1.63, tokens/s = 82148 (37788 source, 44360 target) ; Learning rate = 0.000531 ; Loss = 1.971423\n",
      "2024-12-14 18:51:40.134000: I runner.py:310] Step = 27800 ; steps/s = 1.63, tokens/s = 82208 (37821 source, 44387 target) ; Learning rate = 0.000530 ; Loss = 1.986109\n",
      "2024-12-14 18:52:41.271000: I runner.py:310] Step = 27900 ; steps/s = 1.64, tokens/s = 81310 (37396 source, 43914 target) ; Learning rate = 0.000529 ; Loss = 1.979125\n",
      "2024-12-14 18:53:42.830000: I runner.py:310] Step = 28000 ; steps/s = 1.62, tokens/s = 82084 (37752 source, 44332 target) ; Learning rate = 0.000528 ; Loss = 1.967239\n",
      "2024-12-14 18:54:44.414000: I runner.py:310] Step = 28100 ; steps/s = 1.62, tokens/s = 82047 (37745 source, 44302 target) ; Learning rate = 0.000527 ; Loss = 1.977066\n",
      "2024-12-14 18:55:45.566000: I runner.py:310] Step = 28200 ; steps/s = 1.64, tokens/s = 81275 (37364 source, 43911 target) ; Learning rate = 0.000526 ; Loss = 1.967068\n",
      "2024-12-14 18:56:47.127000: I runner.py:310] Step = 28300 ; steps/s = 1.62, tokens/s = 82086 (37756 source, 44330 target) ; Learning rate = 0.000525 ; Loss = 1.978360\n",
      "2024-12-14 18:57:48.662000: I runner.py:310] Step = 28400 ; steps/s = 1.63, tokens/s = 82131 (37774 source, 44357 target) ; Learning rate = 0.000524 ; Loss = 1.975784\n",
      "2024-12-14 18:58:50.148000: I runner.py:310] Step = 28500 ; steps/s = 1.63, tokens/s = 82190 (37810 source, 44380 target) ; Learning rate = 0.000524 ; Loss = 1.965288\n",
      "2024-12-14 18:59:51.196000: I runner.py:310] Step = 28600 ; steps/s = 1.64, tokens/s = 81399 (37417 source, 43982 target) ; Learning rate = 0.000523 ; Loss = 1.911379\n",
      "2024-12-14 19:00:52.724000: I runner.py:310] Step = 28700 ; steps/s = 1.63, tokens/s = 82152 (37801 source, 44351 target) ; Learning rate = 0.000522 ; Loss = 1.980517\n",
      "2024-12-14 19:01:54.287000: I runner.py:310] Step = 28800 ; steps/s = 1.62, tokens/s = 82065 (37732 source, 44333 target) ; Learning rate = 0.000521 ; Loss = 2.000950\n",
      "2024-12-14 19:02:55.426000: I runner.py:310] Step = 28900 ; steps/s = 1.64, tokens/s = 81313 (37421 source, 43892 target) ; Learning rate = 0.000520 ; Loss = 1.966864\n",
      "2024-12-14 19:03:57.037000: I runner.py:310] Step = 29000 ; steps/s = 1.62, tokens/s = 82041 (37734 source, 44307 target) ; Learning rate = 0.000519 ; Loss = 1.975713\n",
      "2024-12-14 19:04:58.587000: I runner.py:310] Step = 29100 ; steps/s = 1.62, tokens/s = 82109 (37769 source, 44340 target) ; Learning rate = 0.000518 ; Loss = 1.962787\n",
      "2024-12-14 19:06:00.112000: I runner.py:310] Step = 29200 ; steps/s = 1.63, tokens/s = 82115 (37768 source, 44347 target) ; Learning rate = 0.000517 ; Loss = 1.969733\n",
      "2024-12-14 19:07:01.245000: I runner.py:310] Step = 29300 ; steps/s = 1.64, tokens/s = 81315 (37399 source, 43916 target) ; Learning rate = 0.000516 ; Loss = 1.913379\n",
      "2024-12-14 19:08:02.746000: I runner.py:310] Step = 29400 ; steps/s = 1.63, tokens/s = 82149 (37774 source, 44375 target) ; Learning rate = 0.000515 ; Loss = 1.964533\n",
      "2024-12-14 19:09:04.329000: I runner.py:310] Step = 29500 ; steps/s = 1.62, tokens/s = 82098 (37775 source, 44323 target) ; Learning rate = 0.000515 ; Loss = 1.993158\n",
      "2024-12-14 19:10:05.476000: I runner.py:310] Step = 29600 ; steps/s = 1.64, tokens/s = 81246 (37352 source, 43894 target) ; Learning rate = 0.000514 ; Loss = 1.955945\n",
      "2024-12-14 19:11:07.022000: I runner.py:310] Step = 29700 ; steps/s = 1.62, tokens/s = 82116 (37760 source, 44356 target) ; Learning rate = 0.000513 ; Loss = 1.936043\n",
      "2024-12-14 19:12:08.564000: I runner.py:310] Step = 29800 ; steps/s = 1.63, tokens/s = 82119 (37778 source, 44341 target) ; Learning rate = 0.000512 ; Loss = 1.943237\n",
      "2024-12-14 19:13:10.102000: I runner.py:310] Step = 29900 ; steps/s = 1.63, tokens/s = 82102 (37752 source, 44350 target) ; Learning rate = 0.000511 ; Loss = 1.972458\n",
      "2024-12-14 19:14:11.201000: I runner.py:310] Step = 30000 ; steps/s = 1.64, tokens/s = 81348 (37423 source, 43925 target) ; Learning rate = 0.000510 ; Loss = 1.982462\n",
      "2024-12-14 19:14:13.301000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-30000\n",
      "2024-12-14 19:14:13.301000: I training.py:192] Running evaluation for step 30000\n",
      "2024-12-14 19:16:19.111000: I training.py:192] Evaluation result for step 30000: loss = 2.352143 ; perplexity = 10.508060\n",
      "2024-12-14 19:17:20.530000: I runner.py:310] Step = 30100 ; steps/s = 1.63, tokens/s = 82302 (37858 source, 44444 target) ; Learning rate = 0.000509 ; Loss = 1.945387\n",
      "2024-12-14 19:18:22.038000: I runner.py:310] Step = 30200 ; steps/s = 1.63, tokens/s = 82164 (37778 source, 44386 target) ; Learning rate = 0.000509 ; Loss = 1.941324\n",
      "2024-12-14 19:19:23.201000: I runner.py:310] Step = 30300 ; steps/s = 1.64, tokens/s = 81253 (37376 source, 43877 target) ; Learning rate = 0.000508 ; Loss = 1.957639\n",
      "2024-12-14 19:20:24.743000: I runner.py:310] Step = 30400 ; steps/s = 1.63, tokens/s = 82103 (37754 source, 44349 target) ; Learning rate = 0.000507 ; Loss = 1.958679\n",
      "2024-12-14 19:21:26.298000: I runner.py:310] Step = 30500 ; steps/s = 1.62, tokens/s = 82105 (37765 source, 44340 target) ; Learning rate = 0.000506 ; Loss = 1.946605\n",
      "2024-12-14 19:22:27.854000: I runner.py:310] Step = 30600 ; steps/s = 1.62, tokens/s = 82103 (37778 source, 44325 target) ; Learning rate = 0.000505 ; Loss = 1.976768\n",
      "2024-12-14 19:23:28.955000: I runner.py:310] Step = 30700 ; steps/s = 1.64, tokens/s = 81354 (37417 source, 43937 target) ; Learning rate = 0.000504 ; Loss = 1.912210\n",
      "2024-12-14 19:24:30.497000: I runner.py:310] Step = 30800 ; steps/s = 1.63, tokens/s = 82107 (37765 source, 44342 target) ; Learning rate = 0.000504 ; Loss = 1.963402\n",
      "2024-12-14 19:25:32.010000: I runner.py:310] Step = 30900 ; steps/s = 1.63, tokens/s = 82174 (37805 source, 44369 target) ; Learning rate = 0.000503 ; Loss = 1.983281\n",
      "2024-12-14 19:26:33.060000: I runner.py:310] Step = 31000 ; steps/s = 1.64, tokens/s = 81400 (37428 source, 43972 target) ; Learning rate = 0.000502 ; Loss = 1.960622\n",
      "2024-12-14 19:27:34.593000: I runner.py:310] Step = 31100 ; steps/s = 1.63, tokens/s = 82116 (37763 source, 44353 target) ; Learning rate = 0.000501 ; Loss = 1.911685\n",
      "2024-12-14 19:28:36.176000: I runner.py:310] Step = 31200 ; steps/s = 1.62, tokens/s = 82054 (37732 source, 44322 target) ; Learning rate = 0.000500 ; Loss = 1.948495\n",
      "2024-12-14 19:29:37.754000: I runner.py:310] Step = 31300 ; steps/s = 1.62, tokens/s = 82080 (37759 source, 44321 target) ; Learning rate = 0.000500 ; Loss = 1.950564\n",
      "2024-12-14 19:30:38.837000: I runner.py:310] Step = 31400 ; steps/s = 1.64, tokens/s = 81345 (37409 source, 43936 target) ; Learning rate = 0.000499 ; Loss = 1.945672\n",
      "2024-12-14 19:31:40.409000: I runner.py:310] Step = 31500 ; steps/s = 1.62, tokens/s = 82092 (37742 source, 44350 target) ; Learning rate = 0.000498 ; Loss = 1.933631\n",
      "2024-12-14 19:32:41.949000: I runner.py:310] Step = 31600 ; steps/s = 1.63, tokens/s = 82125 (37775 source, 44350 target) ; Learning rate = 0.000497 ; Loss = 1.946983\n",
      "2024-12-14 19:33:43.089000: I runner.py:310] Step = 31700 ; steps/s = 1.64, tokens/s = 81284 (37395 source, 43889 target) ; Learning rate = 0.000496 ; Loss = 1.900556\n",
      "2024-12-14 19:34:44.644000: I runner.py:310] Step = 31800 ; steps/s = 1.62, tokens/s = 82102 (37761 source, 44341 target) ; Learning rate = 0.000496 ; Loss = 1.940737\n",
      "2024-12-14 19:35:46.140000: I runner.py:310] Step = 31900 ; steps/s = 1.63, tokens/s = 82194 (37815 source, 44379 target) ; Learning rate = 0.000495 ; Loss = 1.972139\n",
      "2024-12-14 19:36:47.633000: I runner.py:310] Step = 32000 ; steps/s = 1.63, tokens/s = 82149 (37774 source, 44375 target) ; Learning rate = 0.000494 ; Loss = 1.973127\n",
      "2024-12-14 19:37:48.771000: I runner.py:310] Step = 32100 ; steps/s = 1.64, tokens/s = 81315 (37405 source, 43910 target) ; Learning rate = 0.000493 ; Loss = 1.893961\n",
      "2024-12-14 19:38:50.358000: I runner.py:310] Step = 32200 ; steps/s = 1.62, tokens/s = 82054 (37753 source, 44301 target) ; Learning rate = 0.000493 ; Loss = 1.927314\n",
      "2024-12-14 19:39:51.818000: I runner.py:310] Step = 32300 ; steps/s = 1.63, tokens/s = 82221 (37815 source, 44406 target) ; Learning rate = 0.000492 ; Loss = 1.960946\n",
      "2024-12-14 19:40:52.995000: I runner.py:310] Step = 32400 ; steps/s = 1.63, tokens/s = 81238 (37351 source, 43887 target) ; Learning rate = 0.000491 ; Loss = 1.898478\n",
      "2024-12-14 19:41:54.500000: I runner.py:310] Step = 32500 ; steps/s = 1.63, tokens/s = 82172 (37806 source, 44366 target) ; Learning rate = 0.000490 ; Loss = 1.936434\n",
      "2024-12-14 19:42:56.092000: I runner.py:310] Step = 32600 ; steps/s = 1.62, tokens/s = 82041 (37723 source, 44318 target) ; Learning rate = 0.000490 ; Loss = 1.947133\n",
      "2024-12-14 19:43:57.719000: I runner.py:310] Step = 32700 ; steps/s = 1.62, tokens/s = 81988 (37706 source, 44282 target) ; Learning rate = 0.000489 ; Loss = 1.951395\n",
      "2024-12-14 19:44:58.850000: I runner.py:310] Step = 32800 ; steps/s = 1.64, tokens/s = 81329 (37409 source, 43920 target) ; Learning rate = 0.000488 ; Loss = 1.913190\n",
      "2024-12-14 19:46:00.299000: I runner.py:310] Step = 32900 ; steps/s = 1.63, tokens/s = 82247 (37830 source, 44417 target) ; Learning rate = 0.000487 ; Loss = 1.956381\n",
      "2024-12-14 19:47:01.820000: I runner.py:310] Step = 33000 ; steps/s = 1.63, tokens/s = 82122 (37772 source, 44350 target) ; Learning rate = 0.000487 ; Loss = 1.943320\n",
      "2024-12-14 19:48:02.943000: I runner.py:310] Step = 33100 ; steps/s = 1.64, tokens/s = 81306 (37387 source, 43919 target) ; Learning rate = 0.000486 ; Loss = 1.898003\n",
      "2024-12-14 19:49:04.428000: I runner.py:310] Step = 33200 ; steps/s = 1.63, tokens/s = 82180 (37786 source, 44394 target) ; Learning rate = 0.000485 ; Loss = 1.927377\n",
      "2024-12-14 19:50:05.946000: I runner.py:310] Step = 33300 ; steps/s = 1.63, tokens/s = 82148 (37799 source, 44349 target) ; Learning rate = 0.000484 ; Loss = 1.934075\n",
      "2024-12-14 19:51:07.217000: I runner.py:310] Step = 33400 ; steps/s = 1.63, tokens/s = 81530 (37497 source, 44033 target) ; Learning rate = 0.000484 ; Loss = 1.983080\n",
      "2024-12-14 19:52:08.583000: I runner.py:310] Step = 33500 ; steps/s = 1.63, tokens/s = 81968 (37715 source, 44253 target) ; Learning rate = 0.000483 ; Loss = 1.901983\n",
      "2024-12-14 19:53:10.061000: I runner.py:310] Step = 33600 ; steps/s = 1.63, tokens/s = 82178 (37776 source, 44402 target) ; Learning rate = 0.000482 ; Loss = 1.942284\n",
      "2024-12-14 19:54:11.569000: I runner.py:310] Step = 33700 ; steps/s = 1.63, tokens/s = 82167 (37801 source, 44366 target) ; Learning rate = 0.000481 ; Loss = 1.946828\n",
      "2024-12-14 19:55:12.740000: I runner.py:310] Step = 33800 ; steps/s = 1.63, tokens/s = 81222 (37342 source, 43880 target) ; Learning rate = 0.000481 ; Loss = 1.924036\n",
      "2024-12-14 19:56:14.300000: I runner.py:310] Step = 33900 ; steps/s = 1.62, tokens/s = 82120 (37772 source, 44348 target) ; Learning rate = 0.000480 ; Loss = 1.894435\n",
      "2024-12-14 19:57:15.829000: I runner.py:310] Step = 34000 ; steps/s = 1.63, tokens/s = 82116 (37766 source, 44350 target) ; Learning rate = 0.000479 ; Loss = 1.918513\n",
      "2024-12-14 19:58:16.876000: I runner.py:310] Step = 34100 ; steps/s = 1.64, tokens/s = 81422 (37459 source, 43963 target) ; Learning rate = 0.000479 ; Loss = 1.896467\n",
      "2024-12-14 19:59:18.312000: I runner.py:310] Step = 34200 ; steps/s = 1.63, tokens/s = 82257 (37840 source, 44417 target) ; Learning rate = 0.000478 ; Loss = 1.916630\n",
      "2024-12-14 20:00:19.801000: I runner.py:310] Step = 34300 ; steps/s = 1.63, tokens/s = 82184 (37800 source, 44384 target) ; Learning rate = 0.000477 ; Loss = 1.907165\n",
      "2024-12-14 20:01:21.297000: I runner.py:310] Step = 34400 ; steps/s = 1.63, tokens/s = 82189 (37807 source, 44382 target) ; Learning rate = 0.000477 ; Loss = 1.917939\n",
      "2024-12-14 20:02:22.373000: I runner.py:310] Step = 34500 ; steps/s = 1.64, tokens/s = 81379 (37429 source, 43950 target) ; Learning rate = 0.000476 ; Loss = 1.939742\n",
      "2024-12-14 20:03:23.870000: I runner.py:310] Step = 34600 ; steps/s = 1.63, tokens/s = 82156 (37772 source, 44384 target) ; Learning rate = 0.000475 ; Loss = 1.909268\n",
      "2024-12-14 20:04:25.319000: I runner.py:310] Step = 34700 ; steps/s = 1.63, tokens/s = 82230 (37819 source, 44411 target) ; Learning rate = 0.000474 ; Loss = 1.907475\n",
      "2024-12-14 20:05:26.409000: I runner.py:310] Step = 34800 ; steps/s = 1.64, tokens/s = 81386 (37437 source, 43949 target) ; Learning rate = 0.000474 ; Loss = 1.912536\n",
      "2024-12-14 20:06:27.910000: I runner.py:310] Step = 34900 ; steps/s = 1.63, tokens/s = 82130 (37765 source, 44365 target) ; Learning rate = 0.000473 ; Loss = 1.923091\n",
      "2024-12-14 20:07:29.394000: I runner.py:310] Step = 35000 ; steps/s = 1.63, tokens/s = 82232 (37825 source, 44407 target) ; Learning rate = 0.000472 ; Loss = 1.899052\n",
      "2024-12-14 20:07:29.395000: I training.py:192] Running evaluation for step 35000\n",
      "2024-12-14 20:09:35.074000: I training.py:192] Evaluation result for step 35000: loss = 2.411574 ; perplexity = 11.151504\n",
      "2024-12-14 20:10:36.449000: I runner.py:310] Step = 35100 ; steps/s = 1.63, tokens/s = 82340 (37870 source, 44470 target) ; Learning rate = 0.000472 ; Loss = 1.905980\n",
      "2024-12-14 20:11:37.551000: I runner.py:310] Step = 35200 ; steps/s = 1.64, tokens/s = 81367 (37433 source, 43934 target) ; Learning rate = 0.000471 ; Loss = 1.875061\n",
      "2024-12-14 20:12:39.047000: I runner.py:310] Step = 35300 ; steps/s = 1.63, tokens/s = 82190 (37797 source, 44393 target) ; Learning rate = 0.000470 ; Loss = 1.906334\n",
      "2024-12-14 20:13:40.485000: I runner.py:310] Step = 35400 ; steps/s = 1.63, tokens/s = 82221 (37809 source, 44412 target) ; Learning rate = 0.000470 ; Loss = 1.935831\n",
      "2024-12-14 20:14:41.492000: I runner.py:310] Step = 35500 ; steps/s = 1.64, tokens/s = 81467 (37479 source, 43988 target) ; Learning rate = 0.000469 ; Loss = 1.907577\n",
      "2024-12-14 20:15:42.968000: I runner.py:310] Step = 35600 ; steps/s = 1.63, tokens/s = 82198 (37814 source, 44384 target) ; Learning rate = 0.000468 ; Loss = 1.905853\n",
      "2024-12-14 20:16:44.434000: I runner.py:310] Step = 35700 ; steps/s = 1.63, tokens/s = 82206 (37804 source, 44402 target) ; Learning rate = 0.000468 ; Loss = 1.911185\n",
      "2024-12-14 20:17:45.875000: I runner.py:310] Step = 35800 ; steps/s = 1.63, tokens/s = 82276 (37852 source, 44424 target) ; Learning rate = 0.000467 ; Loss = 1.939375\n",
      "2024-12-14 20:18:46.941000: I runner.py:310] Step = 35900 ; steps/s = 1.64, tokens/s = 81408 (37448 source, 43960 target) ; Learning rate = 0.000466 ; Loss = 1.859264\n",
      "2024-12-14 20:19:48.470000: I runner.py:310] Step = 36000 ; steps/s = 1.63, tokens/s = 82123 (37796 source, 44327 target) ; Learning rate = 0.000466 ; Loss = 1.912397\n",
      "2024-12-14 20:20:49.899000: I runner.py:310] Step = 36100 ; steps/s = 1.63, tokens/s = 82251 (37810 source, 44441 target) ; Learning rate = 0.000465 ; Loss = 1.927624\n",
      "2024-12-14 20:21:50.948000: I runner.py:310] Step = 36200 ; steps/s = 1.64, tokens/s = 81393 (37403 source, 43990 target) ; Learning rate = 0.000465 ; Loss = 1.888320\n",
      "2024-12-14 20:22:52.505000: I runner.py:310] Step = 36300 ; steps/s = 1.62, tokens/s = 82047 (37705 source, 44342 target) ; Learning rate = 0.000464 ; Loss = 1.902584\n",
      "2024-12-14 20:23:53.985000: I runner.py:310] Step = 36400 ; steps/s = 1.63, tokens/s = 82223 (37840 source, 44383 target) ; Learning rate = 0.000463 ; Loss = 1.881359\n",
      "2024-12-14 20:24:55.461000: I runner.py:310] Step = 36500 ; steps/s = 1.63, tokens/s = 82220 (37823 source, 44397 target) ; Learning rate = 0.000463 ; Loss = 1.913560\n",
      "2024-12-14 20:25:56.560000: I runner.py:310] Step = 36600 ; steps/s = 1.64, tokens/s = 81346 (37420 source, 43926 target) ; Learning rate = 0.000462 ; Loss = 1.933448\n",
      "2024-12-14 20:26:58.030000: I runner.py:310] Step = 36700 ; steps/s = 1.63, tokens/s = 82206 (37788 source, 44418 target) ; Learning rate = 0.000461 ; Loss = 1.880328\n",
      "2024-12-14 20:27:59.507000: I runner.py:310] Step = 36800 ; steps/s = 1.63, tokens/s = 82181 (37795 source, 44386 target) ; Learning rate = 0.000461 ; Loss = 1.906668\n",
      "2024-12-14 20:29:00.639000: I runner.py:310] Step = 36900 ; steps/s = 1.64, tokens/s = 81340 (37442 source, 43898 target) ; Learning rate = 0.000460 ; Loss = 1.892504\n",
      "2024-12-14 20:30:02.135000: I runner.py:310] Step = 37000 ; steps/s = 1.63, tokens/s = 82171 (37794 source, 44377 target) ; Learning rate = 0.000460 ; Loss = 1.901725\n",
      "2024-12-14 20:31:03.608000: I runner.py:310] Step = 37100 ; steps/s = 1.63, tokens/s = 82233 (37836 source, 44397 target) ; Learning rate = 0.000459 ; Loss = 1.910369\n",
      "2024-12-14 20:32:05.050000: I runner.py:310] Step = 37200 ; steps/s = 1.63, tokens/s = 82226 (37805 source, 44421 target) ; Learning rate = 0.000458 ; Loss = 1.929562\n",
      "2024-12-14 20:33:06.147000: I runner.py:310] Step = 37300 ; steps/s = 1.64, tokens/s = 81361 (37422 source, 43939 target) ; Learning rate = 0.000458 ; Loss = 1.896239\n",
      "2024-12-14 20:34:07.685000: I runner.py:310] Step = 37400 ; steps/s = 1.63, tokens/s = 82119 (37770 source, 44349 target) ; Learning rate = 0.000457 ; Loss = 1.887702\n",
      "2024-12-14 20:35:09.153000: I runner.py:310] Step = 37500 ; steps/s = 1.63, tokens/s = 82235 (37836 source, 44399 target) ; Learning rate = 0.000456 ; Loss = 1.898660\n",
      "2024-12-14 20:36:10.268000: I runner.py:310] Step = 37600 ; steps/s = 1.64, tokens/s = 81303 (37377 source, 43926 target) ; Learning rate = 0.000456 ; Loss = 1.900518\n",
      "2024-12-14 20:37:11.756000: I runner.py:310] Step = 37700 ; steps/s = 1.63, tokens/s = 82187 (37798 source, 44389 target) ; Learning rate = 0.000455 ; Loss = 1.892449\n",
      "2024-12-14 20:38:13.173000: I runner.py:310] Step = 37800 ; steps/s = 1.63, tokens/s = 82298 (37867 source, 44431 target) ; Learning rate = 0.000455 ; Loss = 1.887062\n",
      "2024-12-14 20:39:14.683000: I runner.py:310] Step = 37900 ; steps/s = 1.63, tokens/s = 82130 (37771 source, 44359 target) ; Learning rate = 0.000454 ; Loss = 1.910307\n",
      "2024-12-14 20:40:15.720000: I runner.py:310] Step = 38000 ; steps/s = 1.64, tokens/s = 81412 (37433 source, 43979 target) ; Learning rate = 0.000453 ; Loss = 1.906357\n",
      "2024-12-14 20:41:17.234000: I runner.py:310] Step = 38100 ; steps/s = 1.63, tokens/s = 82125 (37766 source, 44359 target) ; Learning rate = 0.000453 ; Loss = 1.860628\n",
      "2024-12-14 20:42:18.726000: I runner.py:310] Step = 38200 ; steps/s = 1.63, tokens/s = 82219 (37825 source, 44394 target) ; Learning rate = 0.000452 ; Loss = 1.897799\n",
      "2024-12-14 20:43:19.757000: I runner.py:310] Step = 38300 ; steps/s = 1.64, tokens/s = 81440 (37449 source, 43991 target) ; Learning rate = 0.000452 ; Loss = 1.899934\n",
      "2024-12-14 20:44:21.199000: I runner.py:310] Step = 38400 ; steps/s = 1.63, tokens/s = 82256 (37836 source, 44420 target) ; Learning rate = 0.000451 ; Loss = 1.870255\n",
      "2024-12-14 20:45:22.695000: I runner.py:310] Step = 38500 ; steps/s = 1.63, tokens/s = 82156 (37782 source, 44374 target) ; Learning rate = 0.000450 ; Loss = 1.885133\n",
      "2024-12-14 20:46:24.118000: I runner.py:310] Step = 38600 ; steps/s = 1.63, tokens/s = 82286 (37858 source, 44428 target) ; Learning rate = 0.000450 ; Loss = 1.902491\n",
      "2024-12-14 20:47:25.205000: I runner.py:310] Step = 38700 ; steps/s = 1.64, tokens/s = 81354 (37403 source, 43951 target) ; Learning rate = 0.000449 ; Loss = 1.898233\n",
      "2024-12-14 20:48:26.759000: I runner.py:310] Step = 38800 ; steps/s = 1.62, tokens/s = 82092 (37765 source, 44327 target) ; Learning rate = 0.000449 ; Loss = 1.863960\n",
      "2024-12-14 20:49:28.236000: I runner.py:310] Step = 38900 ; steps/s = 1.63, tokens/s = 82198 (37795 source, 44403 target) ; Learning rate = 0.000448 ; Loss = 1.891769\n",
      "2024-12-14 20:50:29.280000: I runner.py:310] Step = 39000 ; steps/s = 1.64, tokens/s = 81431 (37457 source, 43974 target) ; Learning rate = 0.000448 ; Loss = 1.862092\n",
      "2024-12-14 20:51:30.732000: I runner.py:310] Step = 39100 ; steps/s = 1.63, tokens/s = 82261 (37845 source, 44416 target) ; Learning rate = 0.000447 ; Loss = 1.884533\n",
      "2024-12-14 20:52:32.174000: I runner.py:310] Step = 39200 ; steps/s = 1.63, tokens/s = 82267 (37835 source, 44432 target) ; Learning rate = 0.000446 ; Loss = 1.915123\n",
      "2024-12-14 20:53:33.699000: I runner.py:310] Step = 39300 ; steps/s = 1.63, tokens/s = 82119 (37771 source, 44348 target) ; Learning rate = 0.000446 ; Loss = 1.917681\n",
      "2024-12-14 20:54:35.136000: I runner.py:310] Step = 39400 ; steps/s = 1.63, tokens/s = 80911 (37221 source, 43690 target) ; Learning rate = 0.000445 ; Loss = 1.843433\n",
      "2024-12-14 20:55:36.669000: I runner.py:310] Step = 39500 ; steps/s = 1.63, tokens/s = 82132 (37779 source, 44353 target) ; Learning rate = 0.000445 ; Loss = 1.894354\n",
      "2024-12-14 20:56:38.185000: I runner.py:310] Step = 39600 ; steps/s = 1.63, tokens/s = 82132 (37763 source, 44369 target) ; Learning rate = 0.000444 ; Loss = 1.903776\n",
      "2024-12-14 20:57:39.309000: I runner.py:310] Step = 39700 ; steps/s = 1.64, tokens/s = 81319 (37409 source, 43910 target) ; Learning rate = 0.000444 ; Loss = 1.889160\n",
      "2024-12-14 20:58:40.885000: I runner.py:310] Step = 39800 ; steps/s = 1.62, tokens/s = 82053 (37735 source, 44318 target) ; Learning rate = 0.000443 ; Loss = 1.879758\n",
      "2024-12-14 20:59:42.409000: I runner.py:310] Step = 39900 ; steps/s = 1.63, tokens/s = 82155 (37793 source, 44362 target) ; Learning rate = 0.000442 ; Loss = 1.859600\n",
      "2024-12-14 21:00:44.000000: I runner.py:310] Step = 40000 ; steps/s = 1.62, tokens/s = 82028 (37717 source, 44311 target) ; Learning rate = 0.000442 ; Loss = 1.889623\n",
      "2024-12-14 21:00:45.680000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-40000\n",
      "2024-12-14 21:00:45.681000: I training.py:192] Running evaluation for step 40000\n",
      "2024-12-14 21:02:49.110000: I training.py:192] Evaluation result for step 40000: loss = 2.450165 ; perplexity = 11.590262\n",
      "2024-12-14 21:03:50.093000: I runner.py:310] Step = 40100 ; steps/s = 1.64, tokens/s = 81542 (37520 source, 44022 target) ; Learning rate = 0.000441 ; Loss = 1.846648\n",
      "2024-12-14 21:04:51.641000: I runner.py:310] Step = 40200 ; steps/s = 1.62, tokens/s = 82107 (37759 source, 44348 target) ; Learning rate = 0.000441 ; Loss = 1.886643\n",
      "2024-12-14 21:05:53.205000: I runner.py:310] Step = 40300 ; steps/s = 1.62, tokens/s = 82072 (37738 source, 44334 target) ; Learning rate = 0.000440 ; Loss = 1.905403\n",
      "2024-12-14 21:06:54.349000: I runner.py:310] Step = 40400 ; steps/s = 1.64, tokens/s = 81279 (37379 source, 43900 target) ; Learning rate = 0.000440 ; Loss = 1.842193\n",
      "2024-12-14 21:07:55.980000: I runner.py:310] Step = 40500 ; steps/s = 1.62, tokens/s = 82027 (37732 source, 44295 target) ; Learning rate = 0.000439 ; Loss = 1.864169\n",
      "2024-12-14 21:08:57.526000: I runner.py:310] Step = 40600 ; steps/s = 1.62, tokens/s = 82090 (37755 source, 44335 target) ; Learning rate = 0.000439 ; Loss = 1.890475\n",
      "2024-12-14 21:09:59.110000: I runner.py:310] Step = 40700 ; steps/s = 1.62, tokens/s = 82050 (37736 source, 44314 target) ; Learning rate = 0.000438 ; Loss = 1.904608\n",
      "2024-12-14 21:11:00.282000: I runner.py:310] Step = 40800 ; steps/s = 1.63, tokens/s = 81241 (37355 source, 43886 target) ; Learning rate = 0.000438 ; Loss = 1.874185\n",
      "2024-12-14 21:12:01.819000: I runner.py:310] Step = 40900 ; steps/s = 1.63, tokens/s = 82105 (37761 source, 44344 target) ; Learning rate = 0.000437 ; Loss = 1.876767\n",
      "2024-12-14 21:13:03.417000: I runner.py:310] Step = 41000 ; steps/s = 1.62, tokens/s = 82058 (37756 source, 44302 target) ; Learning rate = 0.000437 ; Loss = 1.869222\n",
      "2024-12-14 21:14:04.503000: I runner.py:310] Step = 41100 ; steps/s = 1.64, tokens/s = 81360 (37411 source, 43949 target) ; Learning rate = 0.000436 ; Loss = 1.891882\n",
      "2024-12-14 21:15:06.099000: I runner.py:310] Step = 41200 ; steps/s = 1.62, tokens/s = 82050 (37744 source, 44306 target) ; Learning rate = 0.000435 ; Loss = 1.849986\n",
      "2024-12-14 21:16:07.642000: I runner.py:310] Step = 41300 ; steps/s = 1.63, tokens/s = 82103 (37768 source, 44335 target) ; Learning rate = 0.000435 ; Loss = 1.858763\n",
      "2024-12-14 21:17:09.232000: I runner.py:310] Step = 41400 ; steps/s = 1.62, tokens/s = 82067 (37746 source, 44321 target) ; Learning rate = 0.000434 ; Loss = 1.878561\n",
      "2024-12-14 21:18:10.373000: I runner.py:310] Step = 41500 ; steps/s = 1.64, tokens/s = 81296 (37389 source, 43907 target) ; Learning rate = 0.000434 ; Loss = 1.871726\n",
      "2024-12-14 21:19:11.937000: I runner.py:310] Step = 41600 ; steps/s = 1.62, tokens/s = 82075 (37745 source, 44330 target) ; Learning rate = 0.000433 ; Loss = 1.878577\n",
      "2024-12-14 21:20:13.533000: I runner.py:310] Step = 41700 ; steps/s = 1.62, tokens/s = 82058 (37753 source, 44305 target) ; Learning rate = 0.000433 ; Loss = 1.883519\n",
      "2024-12-14 21:21:14.629000: I runner.py:310] Step = 41800 ; steps/s = 1.64, tokens/s = 81324 (37388 source, 43936 target) ; Learning rate = 0.000432 ; Loss = 1.885297\n",
      "2024-12-14 21:22:16.130000: I runner.py:310] Step = 41900 ; steps/s = 1.63, tokens/s = 82141 (37781 source, 44360 target) ; Learning rate = 0.000432 ; Loss = 1.853598\n",
      "2024-12-14 21:23:17.672000: I runner.py:310] Step = 42000 ; steps/s = 1.63, tokens/s = 82127 (37772 source, 44355 target) ; Learning rate = 0.000431 ; Loss = 1.874735\n",
      "2024-12-14 21:24:18.807000: I runner.py:310] Step = 42100 ; steps/s = 1.64, tokens/s = 81329 (37417 source, 43912 target) ; Learning rate = 0.000431 ; Loss = 1.877195\n",
      "2024-12-14 21:25:20.359000: I runner.py:310] Step = 42200 ; steps/s = 1.62, tokens/s = 82097 (37755 source, 44342 target) ; Learning rate = 0.000430 ; Loss = 1.873243\n",
      "2024-12-14 21:26:21.875000: I runner.py:310] Step = 42300 ; steps/s = 1.63, tokens/s = 82156 (37793 source, 44363 target) ; Learning rate = 0.000430 ; Loss = 1.865712\n",
      "2024-12-14 21:27:23.498000: I runner.py:310] Step = 42400 ; steps/s = 1.62, tokens/s = 81999 (37711 source, 44288 target) ; Learning rate = 0.000429 ; Loss = 1.864144\n",
      "2024-12-14 21:28:24.636000: I runner.py:310] Step = 42500 ; steps/s = 1.64, tokens/s = 81296 (37390 source, 43906 target) ; Learning rate = 0.000429 ; Loss = 1.831638\n",
      "2024-12-14 21:29:26.196000: I runner.py:310] Step = 42600 ; steps/s = 1.62, tokens/s = 82086 (37762 source, 44324 target) ; Learning rate = 0.000428 ; Loss = 1.876097\n",
      "2024-12-14 21:30:27.728000: I runner.py:310] Step = 42700 ; steps/s = 1.63, tokens/s = 82118 (37759 source, 44359 target) ; Learning rate = 0.000428 ; Loss = 1.900257\n",
      "2024-12-14 21:31:28.901000: I runner.py:310] Step = 42800 ; steps/s = 1.63, tokens/s = 81245 (37367 source, 43878 target) ; Learning rate = 0.000427 ; Loss = 1.858206\n",
      "2024-12-14 21:32:30.518000: I runner.py:310] Step = 42900 ; steps/s = 1.62, tokens/s = 82045 (37740 source, 44305 target) ; Learning rate = 0.000427 ; Loss = 1.849540\n",
      "2024-12-14 21:33:32.015000: I runner.py:310] Step = 43000 ; steps/s = 1.63, tokens/s = 82164 (37779 source, 44385 target) ; Learning rate = 0.000426 ; Loss = 1.893142\n",
      "2024-12-14 21:34:33.593000: I runner.py:310] Step = 43100 ; steps/s = 1.62, tokens/s = 82062 (37751 source, 44311 target) ; Learning rate = 0.000426 ; Loss = 1.879010\n",
      "2024-12-14 21:35:34.748000: I runner.py:310] Step = 43200 ; steps/s = 1.64, tokens/s = 81254 (37369 source, 43885 target) ; Learning rate = 0.000425 ; Loss = 1.889667\n",
      "2024-12-14 21:36:36.320000: I runner.py:310] Step = 43300 ; steps/s = 1.62, tokens/s = 82079 (37752 source, 44327 target) ; Learning rate = 0.000425 ; Loss = 1.854346\n",
      "2024-12-14 21:37:37.879000: I runner.py:310] Step = 43400 ; steps/s = 1.62, tokens/s = 82133 (37793 source, 44340 target) ; Learning rate = 0.000424 ; Loss = 1.850026\n",
      "2024-12-14 21:38:39.032000: I runner.py:310] Step = 43500 ; steps/s = 1.64, tokens/s = 81236 (37343 source, 43893 target) ; Learning rate = 0.000424 ; Loss = 1.860284\n",
      "2024-12-14 21:39:40.595000: I runner.py:310] Step = 43600 ; steps/s = 1.62, tokens/s = 82052 (37720 source, 44332 target) ; Learning rate = 0.000423 ; Loss = 1.866613\n",
      "2024-12-14 21:40:42.134000: I runner.py:310] Step = 43700 ; steps/s = 1.63, tokens/s = 82155 (37803 source, 44352 target) ; Learning rate = 0.000423 ; Loss = 1.853732\n",
      "2024-12-14 21:41:43.656000: I runner.py:310] Step = 43800 ; steps/s = 1.63, tokens/s = 82157 (37798 source, 44359 target) ; Learning rate = 0.000422 ; Loss = 1.854465\n",
      "2024-12-14 21:42:44.860000: I runner.py:310] Step = 43900 ; steps/s = 1.63, tokens/s = 81221 (37370 source, 43851 target) ; Learning rate = 0.000422 ; Loss = 1.823344\n",
      "2024-12-14 21:43:46.431000: I runner.py:310] Step = 44000 ; steps/s = 1.62, tokens/s = 82063 (37738 source, 44325 target) ; Learning rate = 0.000421 ; Loss = 1.866336\n",
      "2024-12-14 21:44:48.052000: I runner.py:310] Step = 44100 ; steps/s = 1.62, tokens/s = 82012 (37703 source, 44309 target) ; Learning rate = 0.000421 ; Loss = 1.878159\n",
      "2024-12-14 21:45:49.204000: I runner.py:310] Step = 44200 ; steps/s = 1.64, tokens/s = 81270 (37382 source, 43888 target) ; Learning rate = 0.000420 ; Loss = 1.865063\n",
      "2024-12-14 21:46:50.743000: I runner.py:310] Step = 44300 ; steps/s = 1.63, tokens/s = 82116 (37768 source, 44348 target) ; Learning rate = 0.000420 ; Loss = 1.872279\n",
      "2024-12-14 21:47:52.293000: I runner.py:310] Step = 44400 ; steps/s = 1.62, tokens/s = 82129 (37778 source, 44351 target) ; Learning rate = 0.000419 ; Loss = 1.844055\n",
      "2024-12-14 21:48:53.885000: I runner.py:310] Step = 44500 ; steps/s = 1.62, tokens/s = 82029 (37728 source, 44301 target) ; Learning rate = 0.000419 ; Loss = 1.863878\n",
      "2024-12-14 21:49:54.972000: I runner.py:310] Step = 44600 ; steps/s = 1.64, tokens/s = 81369 (37422 source, 43947 target) ; Learning rate = 0.000419 ; Loss = 1.816297\n",
      "2024-12-14 21:50:56.574000: I runner.py:310] Step = 44700 ; steps/s = 1.62, tokens/s = 82035 (37740 source, 44295 target) ; Learning rate = 0.000418 ; Loss = 1.868443\n",
      "2024-12-14 21:51:58.158000: I runner.py:310] Step = 44800 ; steps/s = 1.62, tokens/s = 82060 (37740 source, 44320 target) ; Learning rate = 0.000418 ; Loss = 1.861506\n",
      "2024-12-14 21:52:59.266000: I runner.py:310] Step = 44900 ; steps/s = 1.64, tokens/s = 81327 (37392 source, 43935 target) ; Learning rate = 0.000417 ; Loss = 1.865547\n",
      "2024-12-14 21:54:00.821000: I runner.py:310] Step = 45000 ; steps/s = 1.62, tokens/s = 82081 (37748 source, 44333 target) ; Learning rate = 0.000417 ; Loss = 1.868236\n",
      "2024-12-14 21:54:00.822000: I training.py:192] Running evaluation for step 45000\n",
      "2024-12-14 21:56:05.575000: I training.py:192] Evaluation result for step 45000: loss = 2.476834 ; perplexity = 11.903522\n",
      "2024-12-14 21:57:07.029000: I runner.py:310] Step = 45100 ; steps/s = 1.63, tokens/s = 82233 (37811 source, 44422 target) ; Learning rate = 0.000416 ; Loss = 1.844267\n",
      "2024-12-14 21:58:08.598000: I runner.py:310] Step = 45200 ; steps/s = 1.62, tokens/s = 82096 (37790 source, 44306 target) ; Learning rate = 0.000416 ; Loss = 1.844286\n",
      "2024-12-14 21:59:09.789000: I runner.py:310] Step = 45300 ; steps/s = 1.63, tokens/s = 81211 (37327 source, 43884 target) ; Learning rate = 0.000415 ; Loss = 1.882567\n",
      "2024-12-14 22:00:11.296000: I runner.py:310] Step = 45400 ; steps/s = 1.63, tokens/s = 82184 (37805 source, 44379 target) ; Learning rate = 0.000415 ; Loss = 1.837516\n",
      "2024-12-14 22:01:12.870000: I runner.py:310] Step = 45500 ; steps/s = 1.62, tokens/s = 82080 (37749 source, 44331 target) ; Learning rate = 0.000414 ; Loss = 1.854057\n",
      "2024-12-14 22:02:13.983000: I runner.py:310] Step = 45600 ; steps/s = 1.64, tokens/s = 81330 (37414 source, 43916 target) ; Learning rate = 0.000414 ; Loss = 1.834118\n",
      "2024-12-14 22:03:15.565000: I runner.py:310] Step = 45700 ; steps/s = 1.62, tokens/s = 82070 (37757 source, 44313 target) ; Learning rate = 0.000413 ; Loss = 1.847303\n",
      "2024-12-14 22:04:17.063000: I runner.py:310] Step = 45800 ; steps/s = 1.63, tokens/s = 82181 (37809 source, 44372 target) ; Learning rate = 0.000413 ; Loss = 1.858671\n",
      "2024-12-14 22:05:18.669000: I runner.py:310] Step = 45900 ; steps/s = 1.62, tokens/s = 82021 (37716 source, 44305 target) ; Learning rate = 0.000413 ; Loss = 1.866656\n",
      "2024-12-14 22:06:19.796000: I runner.py:310] Step = 46000 ; steps/s = 1.64, tokens/s = 81320 (37393 source, 43927 target) ; Learning rate = 0.000412 ; Loss = 1.801616\n",
      "2024-12-14 22:07:21.355000: I runner.py:310] Step = 46100 ; steps/s = 1.62, tokens/s = 82083 (37747 source, 44336 target) ; Learning rate = 0.000412 ; Loss = 1.844088\n",
      "2024-12-14 22:08:22.917000: I runner.py:310] Step = 46200 ; steps/s = 1.62, tokens/s = 82073 (37748 source, 44325 target) ; Learning rate = 0.000411 ; Loss = 1.872724\n",
      "2024-12-14 22:09:24.163000: I runner.py:310] Step = 46300 ; steps/s = 1.63, tokens/s = 81160 (37334 source, 43826 target) ; Learning rate = 0.000411 ; Loss = 1.850998\n",
      "2024-12-14 22:10:25.749000: I runner.py:310] Step = 46400 ; steps/s = 1.62, tokens/s = 82056 (37740 source, 44316 target) ; Learning rate = 0.000410 ; Loss = 1.853751\n",
      "2024-12-14 22:11:27.407000: I runner.py:310] Step = 46500 ; steps/s = 1.62, tokens/s = 81977 (37712 source, 44265 target) ; Learning rate = 0.000410 ; Loss = 1.855220\n",
      "2024-12-14 22:12:28.943000: I runner.py:310] Step = 46600 ; steps/s = 1.63, tokens/s = 82106 (37765 source, 44341 target) ; Learning rate = 0.000409 ; Loss = 1.845159\n",
      "2024-12-14 22:13:30.159000: I runner.py:310] Step = 46700 ; steps/s = 1.63, tokens/s = 81208 (37353 source, 43855 target) ; Learning rate = 0.000409 ; Loss = 1.821467\n",
      "2024-12-14 22:14:31.685000: I runner.py:310] Step = 46800 ; steps/s = 1.63, tokens/s = 82113 (37750 source, 44363 target) ; Learning rate = 0.000409 ; Loss = 1.847882\n",
      "2024-12-14 22:15:33.242000: I runner.py:310] Step = 46900 ; steps/s = 1.62, tokens/s = 82091 (37766 source, 44325 target) ; Learning rate = 0.000408 ; Loss = 1.868730\n",
      "2024-12-14 22:16:34.473000: I runner.py:310] Step = 47000 ; steps/s = 1.63, tokens/s = 81183 (37341 source, 43842 target) ; Learning rate = 0.000408 ; Loss = 1.851514\n",
      "2024-12-14 22:17:35.974000: I runner.py:310] Step = 47100 ; steps/s = 1.63, tokens/s = 82165 (37786 source, 44379 target) ; Learning rate = 0.000407 ; Loss = 1.832442\n",
      "2024-12-14 22:18:37.608000: I runner.py:310] Step = 47200 ; steps/s = 1.62, tokens/s = 81994 (37720 source, 44274 target) ; Learning rate = 0.000407 ; Loss = 1.843322\n",
      "2024-12-14 22:19:39.202000: I runner.py:310] Step = 47300 ; steps/s = 1.62, tokens/s = 82050 (37735 source, 44315 target) ; Learning rate = 0.000406 ; Loss = 1.852931\n",
      "2024-12-14 22:20:40.368000: I runner.py:310] Step = 47400 ; steps/s = 1.64, tokens/s = 81255 (37388 source, 43867 target) ; Learning rate = 0.000406 ; Loss = 1.879101\n",
      "2024-12-14 22:21:41.915000: I runner.py:310] Step = 47500 ; steps/s = 1.62, tokens/s = 82089 (37737 source, 44352 target) ; Learning rate = 0.000406 ; Loss = 1.838752\n",
      "2024-12-14 22:22:43.502000: I runner.py:310] Step = 47600 ; steps/s = 1.62, tokens/s = 82074 (37748 source, 44326 target) ; Learning rate = 0.000405 ; Loss = 1.841624\n",
      "2024-12-14 22:23:44.700000: I runner.py:310] Step = 47700 ; steps/s = 1.63, tokens/s = 81190 (37327 source, 43863 target) ; Learning rate = 0.000405 ; Loss = 1.817784\n",
      "2024-12-14 22:24:46.340000: I runner.py:310] Step = 47800 ; steps/s = 1.62, tokens/s = 81986 (37707 source, 44279 target) ; Learning rate = 0.000404 ; Loss = 1.838040\n",
      "2024-12-14 22:25:47.868000: I runner.py:310] Step = 47900 ; steps/s = 1.63, tokens/s = 82130 (37780 source, 44350 target) ; Learning rate = 0.000404 ; Loss = 1.860564\n",
      "2024-12-14 22:26:49.472000: I runner.py:310] Step = 48000 ; steps/s = 1.62, tokens/s = 82042 (37743 source, 44299 target) ; Learning rate = 0.000403 ; Loss = 1.872928\n",
      "2024-12-14 22:27:50.600000: I runner.py:310] Step = 48100 ; steps/s = 1.64, tokens/s = 81307 (37386 source, 43921 target) ; Learning rate = 0.000403 ; Loss = 1.857837\n",
      "2024-12-14 22:28:52.142000: I runner.py:310] Step = 48200 ; steps/s = 1.63, tokens/s = 82102 (37764 source, 44338 target) ; Learning rate = 0.000403 ; Loss = 1.839467\n",
      "2024-12-14 22:29:53.715000: I runner.py:310] Step = 48300 ; steps/s = 1.62, tokens/s = 82088 (37772 source, 44316 target) ; Learning rate = 0.000402 ; Loss = 1.827970\n",
      "2024-12-14 22:30:54.903000: I runner.py:310] Step = 48400 ; steps/s = 1.63, tokens/s = 81221 (37344 source, 43877 target) ; Learning rate = 0.000402 ; Loss = 1.819814\n",
      "2024-12-14 22:31:56.901000: I runner.py:310] Step = 48500 ; steps/s = 1.61, tokens/s = 81496 (37493 source, 44003 target) ; Learning rate = 0.000401 ; Loss = 1.824652\n",
      "2024-12-14 22:32:58.306000: I runner.py:310] Step = 48600 ; steps/s = 1.63, tokens/s = 82303 (37856 source, 44447 target) ; Learning rate = 0.000401 ; Loss = 1.860126\n",
      "2024-12-14 22:33:59.786000: I runner.py:310] Step = 48700 ; steps/s = 1.63, tokens/s = 82210 (37805 source, 44405 target) ; Learning rate = 0.000401 ; Loss = 1.855162\n",
      "2024-12-14 22:35:00.776000: I runner.py:310] Step = 48800 ; steps/s = 1.64, tokens/s = 81508 (37497 source, 44011 target) ; Learning rate = 0.000400 ; Loss = 1.801349\n",
      "2024-12-14 22:36:02.313000: I runner.py:310] Step = 48900 ; steps/s = 1.63, tokens/s = 82097 (37745 source, 44352 target) ; Learning rate = 0.000400 ; Loss = 1.849618\n",
      "2024-12-14 22:37:03.772000: I runner.py:310] Step = 49000 ; steps/s = 1.63, tokens/s = 82232 (37829 source, 44403 target) ; Learning rate = 0.000399 ; Loss = 1.843821\n",
      "2024-12-14 22:38:04.827000: I runner.py:310] Step = 49100 ; steps/s = 1.64, tokens/s = 81400 (37440 source, 43960 target) ; Learning rate = 0.000399 ; Loss = 1.849085\n",
      "2024-12-14 22:39:06.253000: I runner.py:310] Step = 49200 ; steps/s = 1.63, tokens/s = 82247 (37825 source, 44422 target) ; Learning rate = 0.000398 ; Loss = 1.826790\n",
      "2024-12-14 22:40:07.696000: I runner.py:310] Step = 49300 ; steps/s = 1.63, tokens/s = 82279 (37844 source, 44435 target) ; Learning rate = 0.000398 ; Loss = 1.836153\n",
      "2024-12-14 22:41:09.098000: I runner.py:310] Step = 49400 ; steps/s = 1.63, tokens/s = 82302 (37855 source, 44447 target) ; Learning rate = 0.000398 ; Loss = 1.856973\n",
      "2024-12-14 22:42:10.158000: I runner.py:310] Step = 49500 ; steps/s = 1.64, tokens/s = 81386 (37411 source, 43975 target) ; Learning rate = 0.000397 ; Loss = 1.862591\n",
      "2024-12-14 22:43:11.585000: I runner.py:310] Step = 49600 ; steps/s = 1.63, tokens/s = 82260 (37841 source, 44419 target) ; Learning rate = 0.000397 ; Loss = 1.828776\n",
      "2024-12-14 22:44:13.083000: I runner.py:310] Step = 49700 ; steps/s = 1.63, tokens/s = 82192 (37818 source, 44374 target) ; Learning rate = 0.000396 ; Loss = 1.837076\n",
      "2024-12-14 22:45:14.172000: I runner.py:310] Step = 49800 ; steps/s = 1.64, tokens/s = 81332 (37402 source, 43930 target) ; Learning rate = 0.000396 ; Loss = 1.848189\n",
      "2024-12-14 22:46:15.617000: I runner.py:310] Step = 49900 ; steps/s = 1.63, tokens/s = 82277 (37850 source, 44427 target) ; Learning rate = 0.000396 ; Loss = 1.840972\n",
      "2024-12-14 22:47:17.106000: I runner.py:310] Step = 50000 ; steps/s = 1.63, tokens/s = 82178 (37785 source, 44393 target) ; Learning rate = 0.000395 ; Loss = 1.830637\n",
      "2024-12-14 22:47:18.617000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-50000\n",
      "2024-12-14 22:47:18.617000: I training.py:192] Running evaluation for step 50000\n",
      "2024-12-14 22:49:18.226000: I training.py:192] Evaluation result for step 50000: loss = 2.504529 ; perplexity = 12.237800\n",
      "2024-12-14 22:50:19.207000: I runner.py:310] Step = 50100 ; steps/s = 1.64, tokens/s = 82113 (37768 source, 44345 target) ; Learning rate = 0.000395 ; Loss = 1.856618\n",
      "2024-12-14 22:51:20.472000: I runner.py:310] Step = 50200 ; steps/s = 1.63, tokens/s = 81923 (37689 source, 44234 target) ; Learning rate = 0.000394 ; Loss = 1.823654\n",
      "2024-12-14 22:52:21.878000: I runner.py:310] Step = 50300 ; steps/s = 1.63, tokens/s = 82316 (37868 source, 44448 target) ; Learning rate = 0.000394 ; Loss = 1.859036\n",
      "2024-12-14 22:53:23.358000: I runner.py:310] Step = 50400 ; steps/s = 1.63, tokens/s = 82163 (37786 source, 44377 target) ; Learning rate = 0.000394 ; Loss = 1.862238\n",
      "2024-12-14 22:54:24.422000: I runner.py:310] Step = 50500 ; steps/s = 1.64, tokens/s = 81401 (37433 source, 43968 target) ; Learning rate = 0.000393 ; Loss = 1.800753\n",
      "2024-12-14 22:55:25.858000: I runner.py:310] Step = 50600 ; steps/s = 1.63, tokens/s = 82254 (37829 source, 44425 target) ; Learning rate = 0.000393 ; Loss = 1.828933\n",
      "2024-12-14 22:56:27.392000: I runner.py:310] Step = 50700 ; steps/s = 1.63, tokens/s = 82109 (37764 source, 44345 target) ; Learning rate = 0.000393 ; Loss = 1.847115\n",
      "2024-12-14 22:57:28.419000: I runner.py:310] Step = 50800 ; steps/s = 1.64, tokens/s = 81443 (37457 source, 43986 target) ; Learning rate = 0.000392 ; Loss = 1.822799\n",
      "2024-12-14 22:58:29.898000: I runner.py:310] Step = 50900 ; steps/s = 1.63, tokens/s = 82182 (37796 source, 44386 target) ; Learning rate = 0.000392 ; Loss = 1.847427\n",
      "2024-12-14 22:59:31.278000: I runner.py:310] Step = 51000 ; steps/s = 1.63, tokens/s = 82356 (37892 source, 44464 target) ; Learning rate = 0.000391 ; Loss = 1.814727\n",
      "2024-12-14 23:00:32.675000: I runner.py:310] Step = 51100 ; steps/s = 1.63, tokens/s = 82295 (37848 source, 44447 target) ; Learning rate = 0.000391 ; Loss = 1.831493\n",
      "2024-12-14 23:01:33.775000: I runner.py:310] Step = 51200 ; steps/s = 1.64, tokens/s = 81323 (37380 source, 43943 target) ; Learning rate = 0.000391 ; Loss = 1.850589\n",
      "2024-12-14 23:02:35.199000: I runner.py:310] Step = 51300 ; steps/s = 1.63, tokens/s = 82258 (37811 source, 44447 target) ; Learning rate = 0.000390 ; Loss = 1.805157\n",
      "2024-12-14 23:03:36.623000: I runner.py:310] Step = 51400 ; steps/s = 1.63, tokens/s = 82288 (37873 source, 44415 target) ; Learning rate = 0.000390 ; Loss = 1.849477\n",
      "2024-12-14 23:04:37.615000: I runner.py:310] Step = 51500 ; steps/s = 1.64, tokens/s = 81520 (37505 source, 44015 target) ; Learning rate = 0.000389 ; Loss = 1.848362\n",
      "2024-12-14 23:05:39.094000: I runner.py:310] Step = 51600 ; steps/s = 1.63, tokens/s = 82169 (37796 source, 44373 target) ; Learning rate = 0.000389 ; Loss = 1.830740\n",
      "2024-12-14 23:06:40.490000: I runner.py:310] Step = 51700 ; steps/s = 1.63, tokens/s = 82331 (37874 source, 44457 target) ; Learning rate = 0.000389 ; Loss = 1.820266\n",
      "2024-12-14 23:07:42.018000: I runner.py:310] Step = 51800 ; steps/s = 1.63, tokens/s = 82142 (37772 source, 44370 target) ; Learning rate = 0.000388 ; Loss = 1.828665\n",
      "2024-12-14 23:08:43.005000: I runner.py:310] Step = 51900 ; steps/s = 1.64, tokens/s = 81484 (37463 source, 44021 target) ; Learning rate = 0.000388 ; Loss = 1.806275\n",
      "2024-12-14 23:09:44.443000: I runner.py:310] Step = 52000 ; steps/s = 1.63, tokens/s = 82277 (37861 source, 44416 target) ; Learning rate = 0.000388 ; Loss = 1.838250\n",
      "2024-12-14 23:10:45.957000: I runner.py:310] Step = 52100 ; steps/s = 1.63, tokens/s = 82158 (37790 source, 44368 target) ; Learning rate = 0.000387 ; Loss = 1.841383\n",
      "2024-12-14 23:11:46.986000: I runner.py:310] Step = 52200 ; steps/s = 1.64, tokens/s = 81421 (37436 source, 43985 target) ; Learning rate = 0.000387 ; Loss = 1.836287\n",
      "2024-12-14 23:12:48.531000: I runner.py:310] Step = 52300 ; steps/s = 1.62, tokens/s = 82092 (37756 source, 44336 target) ; Learning rate = 0.000386 ; Loss = 1.825936\n",
      "2024-12-14 23:13:49.927000: I runner.py:310] Step = 52400 ; steps/s = 1.63, tokens/s = 82304 (37854 source, 44450 target) ; Learning rate = 0.000386 ; Loss = 1.813627\n",
      "2024-12-14 23:14:51.415000: I runner.py:310] Step = 52500 ; steps/s = 1.63, tokens/s = 82180 (37800 source, 44380 target) ; Learning rate = 0.000386 ; Loss = 1.826299\n",
      "2024-12-14 23:15:52.390000: I runner.py:310] Step = 52600 ; steps/s = 1.64, tokens/s = 81553 (37526 source, 44027 target) ; Learning rate = 0.000385 ; Loss = 1.852663\n",
      "2024-12-14 23:16:53.868000: I runner.py:310] Step = 52700 ; steps/s = 1.63, tokens/s = 82159 (37763 source, 44396 target) ; Learning rate = 0.000385 ; Loss = 1.822149\n",
      "2024-12-14 23:17:55.305000: I runner.py:310] Step = 52800 ; steps/s = 1.63, tokens/s = 82299 (37869 source, 44430 target) ; Learning rate = 0.000385 ; Loss = 1.821228\n",
      "2024-12-14 23:18:56.371000: I runner.py:310] Step = 52900 ; steps/s = 1.64, tokens/s = 81378 (37420 source, 43958 target) ; Learning rate = 0.000384 ; Loss = 1.836053\n",
      "2024-12-14 23:19:57.879000: I runner.py:310] Step = 53000 ; steps/s = 1.63, tokens/s = 82154 (37787 source, 44367 target) ; Learning rate = 0.000384 ; Loss = 1.821702\n",
      "2024-12-14 23:20:59.318000: I runner.py:310] Step = 53100 ; steps/s = 1.63, tokens/s = 82256 (37833 source, 44423 target) ; Learning rate = 0.000384 ; Loss = 1.813807\n",
      "2024-12-14 23:22:00.772000: I runner.py:310] Step = 53200 ; steps/s = 1.63, tokens/s = 82229 (37824 source, 44405 target) ; Learning rate = 0.000383 ; Loss = 1.819952\n",
      "2024-12-14 23:23:01.823000: I runner.py:310] Step = 53300 ; steps/s = 1.64, tokens/s = 81417 (37450 source, 43967 target) ; Learning rate = 0.000383 ; Loss = 1.840955\n",
      "2024-12-14 23:24:03.259000: I runner.py:310] Step = 53400 ; steps/s = 1.63, tokens/s = 82235 (37804 source, 44431 target) ; Learning rate = 0.000382 ; Loss = 1.820213\n",
      "2024-12-14 23:25:04.628000: I runner.py:310] Step = 53500 ; steps/s = 1.63, tokens/s = 82344 (37870 source, 44474 target) ; Learning rate = 0.000382 ; Loss = 1.814832\n",
      "2024-12-14 23:26:05.656000: I runner.py:310] Step = 53600 ; steps/s = 1.64, tokens/s = 81464 (37482 source, 43982 target) ; Learning rate = 0.000382 ; Loss = 1.851194\n",
      "2024-12-14 23:27:07.051000: I runner.py:310] Step = 53700 ; steps/s = 1.63, tokens/s = 82309 (37853 source, 44456 target) ; Learning rate = 0.000381 ; Loss = 1.812513\n",
      "2024-12-14 23:28:08.519000: I runner.py:310] Step = 53800 ; steps/s = 1.63, tokens/s = 82201 (37790 source, 44411 target) ; Learning rate = 0.000381 ; Loss = 1.815693\n",
      "2024-12-14 23:29:09.956000: I runner.py:310] Step = 53900 ; steps/s = 1.63, tokens/s = 82277 (37867 source, 44410 target) ; Learning rate = 0.000381 ; Loss = 1.827994\n",
      "2024-12-14 23:30:10.997000: I runner.py:310] Step = 54000 ; steps/s = 1.64, tokens/s = 81384 (37414 source, 43970 target) ; Learning rate = 0.000380 ; Loss = 1.844697\n",
      "2024-12-14 23:31:12.438000: I runner.py:310] Step = 54100 ; steps/s = 1.63, tokens/s = 82244 (37817 source, 44427 target) ; Learning rate = 0.000380 ; Loss = 1.808817\n",
      "2024-12-14 23:32:13.826000: I runner.py:310] Step = 54200 ; steps/s = 1.63, tokens/s = 82333 (37861 source, 44472 target) ; Learning rate = 0.000380 ; Loss = 1.809434\n",
      "2024-12-14 23:33:14.921000: I runner.py:310] Step = 54300 ; steps/s = 1.64, tokens/s = 81366 (37449 source, 43917 target) ; Learning rate = 0.000379 ; Loss = 1.825427\n",
      "2024-12-14 23:34:16.314000: I runner.py:310] Step = 54400 ; steps/s = 1.63, tokens/s = 82324 (37870 source, 44454 target) ; Learning rate = 0.000379 ; Loss = 1.835940\n",
      "2024-12-14 23:35:17.804000: I runner.py:310] Step = 54500 ; steps/s = 1.63, tokens/s = 82166 (37769 source, 44397 target) ; Learning rate = 0.000379 ; Loss = 1.810521\n",
      "2024-12-14 23:36:19.243000: I runner.py:310] Step = 54600 ; steps/s = 1.63, tokens/s = 82258 (37848 source, 44410 target) ; Learning rate = 0.000378 ; Loss = 1.814540\n",
      "2024-12-14 23:37:20.282000: I runner.py:310] Step = 54700 ; steps/s = 1.64, tokens/s = 81434 (37451 source, 43983 target) ; Learning rate = 0.000378 ; Loss = 1.791141\n",
      "2024-12-14 23:38:21.723000: I runner.py:310] Step = 54800 ; steps/s = 1.63, tokens/s = 82240 (37822 source, 44418 target) ; Learning rate = 0.000378 ; Loss = 1.815936\n",
      "2024-12-14 23:39:23.129000: I runner.py:310] Step = 54900 ; steps/s = 1.63, tokens/s = 82323 (37877 source, 44446 target) ; Learning rate = 0.000377 ; Loss = 1.832058\n",
      "2024-12-14 23:40:24.196000: I runner.py:310] Step = 55000 ; steps/s = 1.64, tokens/s = 81343 (37393 source, 43950 target) ; Learning rate = 0.000377 ; Loss = 1.802340\n",
      "2024-12-14 23:40:24.198000: I training.py:192] Running evaluation for step 55000\n",
      "2024-12-14 23:42:22.164000: I training.py:192] Evaluation result for step 55000: loss = 2.531049 ; perplexity = 12.566685\n",
      "2024-12-14 23:43:23.379000: I runner.py:310] Step = 55100 ; steps/s = 1.63, tokens/s = 82572 (37973 source, 44599 target) ; Learning rate = 0.000377 ; Loss = 1.819997\n",
      "2024-12-14 23:44:24.857000: I runner.py:310] Step = 55200 ; steps/s = 1.63, tokens/s = 82217 (37830 source, 44387 target) ; Learning rate = 0.000376 ; Loss = 1.817510\n",
      "2024-12-14 23:45:26.275000: I runner.py:310] Step = 55300 ; steps/s = 1.63, tokens/s = 82276 (37841 source, 44435 target) ; Learning rate = 0.000376 ; Loss = 1.846734\n",
      "2024-12-14 23:46:27.285000: I runner.py:310] Step = 55400 ; steps/s = 1.64, tokens/s = 81475 (37488 source, 43987 target) ; Learning rate = 0.000376 ; Loss = 1.833259\n",
      "2024-12-14 23:47:28.724000: I runner.py:310] Step = 55500 ; steps/s = 1.63, tokens/s = 82225 (37801 source, 44424 target) ; Learning rate = 0.000375 ; Loss = 1.790114\n",
      "2024-12-14 23:48:30.105000: I runner.py:310] Step = 55600 ; steps/s = 1.63, tokens/s = 82350 (37895 source, 44455 target) ; Learning rate = 0.000375 ; Loss = 1.814221\n",
      "2024-12-14 23:49:31.149000: I runner.py:310] Step = 55700 ; steps/s = 1.64, tokens/s = 81391 (37408 source, 43983 target) ; Learning rate = 0.000375 ; Loss = 1.840114\n",
      "2024-12-14 23:50:32.578000: I runner.py:310] Step = 55800 ; steps/s = 1.63, tokens/s = 82273 (37838 source, 44435 target) ; Learning rate = 0.000374 ; Loss = 1.808475\n",
      "2024-12-14 23:51:34.054000: I runner.py:310] Step = 55900 ; steps/s = 1.63, tokens/s = 82214 (37819 source, 44395 target) ; Learning rate = 0.000374 ; Loss = 1.789835\n",
      "2024-12-14 23:52:35.474000: I runner.py:310] Step = 56000 ; steps/s = 1.63, tokens/s = 82294 (37863 source, 44431 target) ; Learning rate = 0.000374 ; Loss = 1.818517\n",
      "2024-12-14 23:53:36.543000: I runner.py:310] Step = 56100 ; steps/s = 1.64, tokens/s = 81413 (37450 source, 43963 target) ; Learning rate = 0.000373 ; Loss = 1.799138\n",
      "2024-12-14 23:54:37.962000: I runner.py:310] Step = 56200 ; steps/s = 1.63, tokens/s = 82261 (37835 source, 44426 target) ; Learning rate = 0.000373 ; Loss = 1.828705\n",
      "2024-12-14 23:55:39.366000: I runner.py:310] Step = 56300 ; steps/s = 1.63, tokens/s = 82300 (37851 source, 44449 target) ; Learning rate = 0.000373 ; Loss = 1.815563\n",
      "2024-12-14 23:56:40.403000: I runner.py:310] Step = 56400 ; steps/s = 1.64, tokens/s = 81410 (37437 source, 43973 target) ; Learning rate = 0.000372 ; Loss = 1.797784\n",
      "2024-12-14 23:57:41.787000: I runner.py:310] Step = 56500 ; steps/s = 1.63, tokens/s = 82337 (37881 source, 44456 target) ; Learning rate = 0.000372 ; Loss = 1.813120\n",
      "2024-12-14 23:58:43.244000: I runner.py:310] Step = 56600 ; steps/s = 1.63, tokens/s = 82207 (37791 source, 44416 target) ; Learning rate = 0.000372 ; Loss = 1.812340\n",
      "2024-12-14 23:59:44.631000: I runner.py:310] Step = 56700 ; steps/s = 1.63, tokens/s = 82353 (37892 source, 44461 target) ; Learning rate = 0.000371 ; Loss = 1.851178\n",
      "2024-12-15 00:00:45.701000: I runner.py:310] Step = 56800 ; steps/s = 1.64, tokens/s = 81361 (37410 source, 43951 target) ; Learning rate = 0.000371 ; Loss = 1.821591\n",
      "2024-12-15 00:01:47.118000: I runner.py:310] Step = 56900 ; steps/s = 1.63, tokens/s = 82258 (37818 source, 44440 target) ; Learning rate = 0.000371 ; Loss = 1.802286\n",
      "2024-12-15 00:02:48.587000: I runner.py:310] Step = 57000 ; steps/s = 1.63, tokens/s = 82232 (37834 source, 44398 target) ; Learning rate = 0.000370 ; Loss = 1.814446\n",
      "2024-12-15 00:03:49.619000: I runner.py:310] Step = 57100 ; steps/s = 1.64, tokens/s = 81454 (37463 source, 43991 target) ; Learning rate = 0.000370 ; Loss = 1.792686\n",
      "2024-12-15 00:04:51.006000: I runner.py:310] Step = 57200 ; steps/s = 1.63, tokens/s = 82350 (37895 source, 44455 target) ; Learning rate = 0.000370 ; Loss = 1.810473\n",
      "2024-12-15 00:05:52.490000: I runner.py:310] Step = 57300 ; steps/s = 1.63, tokens/s = 82192 (37805 source, 44387 target) ; Learning rate = 0.000369 ; Loss = 1.815193\n",
      "2024-12-15 00:06:53.965000: I runner.py:310] Step = 57400 ; steps/s = 1.63, tokens/s = 82173 (37779 source, 44394 target) ; Learning rate = 0.000369 ; Loss = 1.832311\n",
      "2024-12-15 00:07:55.040000: I runner.py:310] Step = 57500 ; steps/s = 1.64, tokens/s = 81378 (37440 source, 43938 target) ; Learning rate = 0.000369 ; Loss = 1.786525\n",
      "2024-12-15 00:08:56.422000: I runner.py:310] Step = 57600 ; steps/s = 1.63, tokens/s = 82333 (37867 source, 44466 target) ; Learning rate = 0.000368 ; Loss = 1.830519\n",
      "2024-12-15 00:09:57.967000: I runner.py:310] Step = 57700 ; steps/s = 1.62, tokens/s = 82113 (37756 source, 44357 target) ; Learning rate = 0.000368 ; Loss = 1.818885\n",
      "2024-12-15 00:10:59.012000: I runner.py:310] Step = 57800 ; steps/s = 1.64, tokens/s = 81421 (37450 source, 43971 target) ; Learning rate = 0.000368 ; Loss = 1.778996\n",
      "2024-12-15 00:12:00.472000: I runner.py:310] Step = 57900 ; steps/s = 1.63, tokens/s = 82233 (37838 source, 44395 target) ; Learning rate = 0.000367 ; Loss = 1.812287\n",
      "2024-12-15 00:13:01.866000: I runner.py:310] Step = 58000 ; steps/s = 1.63, tokens/s = 82297 (37837 source, 44460 target) ; Learning rate = 0.000367 ; Loss = 1.813518\n",
      "2024-12-15 00:14:03.292000: I runner.py:310] Step = 58100 ; steps/s = 1.63, tokens/s = 82267 (37834 source, 44433 target) ; Learning rate = 0.000367 ; Loss = 1.837693\n",
      "2024-12-15 00:15:04.300000: I runner.py:310] Step = 58200 ; steps/s = 1.64, tokens/s = 81459 (37463 source, 43996 target) ; Learning rate = 0.000366 ; Loss = 1.821076\n",
      "2024-12-15 00:16:05.671000: I runner.py:310] Step = 58300 ; steps/s = 1.63, tokens/s = 82332 (37859 source, 44473 target) ; Learning rate = 0.000366 ; Loss = 1.794437\n",
      "2024-12-15 00:17:07.055000: I runner.py:310] Step = 58400 ; steps/s = 1.63, tokens/s = 82354 (37895 source, 44459 target) ; Learning rate = 0.000366 ; Loss = 1.815856\n",
      "2024-12-15 00:18:08.075000: I runner.py:310] Step = 58500 ; steps/s = 1.64, tokens/s = 81433 (37446 source, 43987 target) ; Learning rate = 0.000365 ; Loss = 1.824847\n",
      "2024-12-15 00:19:09.586000: I runner.py:310] Step = 58600 ; steps/s = 1.63, tokens/s = 82145 (37770 source, 44375 target) ; Learning rate = 0.000365 ; Loss = 1.800142\n",
      "2024-12-15 00:20:10.938000: I runner.py:310] Step = 58700 ; steps/s = 1.63, tokens/s = 82396 (37894 source, 44502 target) ; Learning rate = 0.000365 ; Loss = 1.806052\n",
      "2024-12-15 00:21:11.976000: I runner.py:310] Step = 58800 ; steps/s = 1.64, tokens/s = 81427 (37471 source, 43956 target) ; Learning rate = 0.000365 ; Loss = 1.853350\n",
      "2024-12-15 00:22:13.491000: I runner.py:310] Step = 58900 ; steps/s = 1.63, tokens/s = 82162 (37790 source, 44372 target) ; Learning rate = 0.000364 ; Loss = 1.796639\n",
      "2024-12-15 00:23:14.928000: I runner.py:310] Step = 59000 ; steps/s = 1.63, tokens/s = 82239 (37817 source, 44422 target) ; Learning rate = 0.000364 ; Loss = 1.811882\n",
      "2024-12-15 00:24:16.458000: I runner.py:310] Step = 59100 ; steps/s = 1.63, tokens/s = 82123 (37768 source, 44355 target) ; Learning rate = 0.000364 ; Loss = 1.818682\n",
      "2024-12-15 00:25:17.500000: I runner.py:310] Step = 59200 ; steps/s = 1.64, tokens/s = 81420 (37441 source, 43979 target) ; Learning rate = 0.000363 ; Loss = 1.818871\n",
      "2024-12-15 00:26:18.971000: I runner.py:310] Step = 59300 ; steps/s = 1.63, tokens/s = 82214 (37823 source, 44391 target) ; Learning rate = 0.000363 ; Loss = 1.802549\n",
      "2024-12-15 00:27:20.349000: I runner.py:310] Step = 59400 ; steps/s = 1.63, tokens/s = 82343 (37869 source, 44474 target) ; Learning rate = 0.000363 ; Loss = 1.806933\n",
      "2024-12-15 00:28:21.406000: I runner.py:310] Step = 59500 ; steps/s = 1.64, tokens/s = 81413 (37454 source, 43959 target) ; Learning rate = 0.000362 ; Loss = 1.803727\n",
      "2024-12-15 00:29:22.823000: I runner.py:310] Step = 59600 ; steps/s = 1.63, tokens/s = 82304 (37869 source, 44435 target) ; Learning rate = 0.000362 ; Loss = 1.783092\n",
      "2024-12-15 00:30:24.201000: I runner.py:310] Step = 59700 ; steps/s = 1.63, tokens/s = 82350 (37877 source, 44473 target) ; Learning rate = 0.000362 ; Loss = 1.815778\n",
      "2024-12-15 00:31:25.742000: I runner.py:310] Step = 59800 ; steps/s = 1.63, tokens/s = 82096 (37754 source, 44342 target) ; Learning rate = 0.000361 ; Loss = 1.820925\n",
      "2024-12-15 00:32:26.764000: I runner.py:310] Step = 59900 ; steps/s = 1.64, tokens/s = 81419 (37423 source, 43996 target) ; Learning rate = 0.000361 ; Loss = 1.828226\n",
      "2024-12-15 00:33:28.208000: I runner.py:310] Step = 60000 ; steps/s = 1.63, tokens/s = 82249 (37823 source, 44426 target) ; Learning rate = 0.000361 ; Loss = 1.797908\n",
      "2024-12-15 00:33:29.703000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-60000\n",
      "2024-12-15 00:33:29.704000: I training.py:192] Running evaluation for step 60000\n",
      "2024-12-15 00:35:31.106000: I training.py:192] Evaluation result for step 60000: loss = 2.534667 ; perplexity = 12.612231\n",
      "2024-12-15 00:36:32.376000: I runner.py:310] Step = 60100 ; steps/s = 1.63, tokens/s = 82489 (37943 source, 44546 target) ; Learning rate = 0.000361 ; Loss = 1.799568\n",
      "2024-12-15 00:37:33.501000: I runner.py:310] Step = 60200 ; steps/s = 1.64, tokens/s = 81331 (37425 source, 43906 target) ; Learning rate = 0.000360 ; Loss = 1.802219\n",
      "2024-12-15 00:38:34.929000: I runner.py:310] Step = 60300 ; steps/s = 1.63, tokens/s = 82283 (37861 source, 44422 target) ; Learning rate = 0.000360 ; Loss = 1.803210\n",
      "2024-12-15 00:39:36.376000: I runner.py:310] Step = 60400 ; steps/s = 1.63, tokens/s = 82226 (37811 source, 44415 target) ; Learning rate = 0.000360 ; Loss = 1.792832\n",
      "2024-12-15 00:40:37.784000: I runner.py:310] Step = 60500 ; steps/s = 1.63, tokens/s = 82289 (37841 source, 44448 target) ; Learning rate = 0.000359 ; Loss = 1.811091\n",
      "2024-12-15 00:41:38.870000: I runner.py:310] Step = 60600 ; steps/s = 1.64, tokens/s = 81401 (37446 source, 43955 target) ; Learning rate = 0.000359 ; Loss = 1.783719\n",
      "2024-12-15 00:42:40.323000: I runner.py:310] Step = 60700 ; steps/s = 1.63, tokens/s = 82259 (37831 source, 44428 target) ; Learning rate = 0.000359 ; Loss = 1.818724\n",
      "2024-12-15 00:43:41.737000: I runner.py:310] Step = 60800 ; steps/s = 1.63, tokens/s = 82246 (37819 source, 44427 target) ; Learning rate = 0.000358 ; Loss = 1.812137\n",
      "2024-12-15 00:44:42.892000: I runner.py:310] Step = 60900 ; steps/s = 1.64, tokens/s = 81257 (37372 source, 43885 target) ; Learning rate = 0.000358 ; Loss = 1.794194\n",
      "2024-12-15 00:45:44.394000: I runner.py:310] Step = 61000 ; steps/s = 1.63, tokens/s = 82176 (37803 source, 44373 target) ; Learning rate = 0.000358 ; Loss = 1.787002\n",
      "2024-12-15 00:46:45.867000: I runner.py:310] Step = 61100 ; steps/s = 1.63, tokens/s = 82206 (37805 source, 44401 target) ; Learning rate = 0.000358 ; Loss = 1.812911\n",
      "2024-12-15 00:47:47.371000: I runner.py:310] Step = 61200 ; steps/s = 1.63, tokens/s = 82174 (37799 source, 44375 target) ; Learning rate = 0.000357 ; Loss = 1.821850\n",
      "2024-12-15 00:48:48.411000: I runner.py:310] Step = 61300 ; steps/s = 1.64, tokens/s = 81408 (37425 source, 43983 target) ; Learning rate = 0.000357 ; Loss = 1.766435\n",
      "2024-12-15 00:49:49.838000: I runner.py:310] Step = 61400 ; steps/s = 1.63, tokens/s = 82277 (37859 source, 44418 target) ; Learning rate = 0.000357 ; Loss = 1.805404\n",
      "2024-12-15 00:50:51.260000: I runner.py:310] Step = 61500 ; steps/s = 1.63, tokens/s = 82278 (37840 source, 44438 target) ; Learning rate = 0.000356 ; Loss = 1.817687\n",
      "2024-12-15 00:51:52.404000: I runner.py:310] Step = 61600 ; steps/s = 1.64, tokens/s = 81275 (37376 source, 43899 target) ; Learning rate = 0.000356 ; Loss = 1.804598\n",
      "2024-12-15 00:52:53.809000: I runner.py:310] Step = 61700 ; steps/s = 1.63, tokens/s = 82294 (37846 source, 44448 target) ; Learning rate = 0.000356 ; Loss = 1.803330\n",
      "2024-12-15 00:53:55.297000: I runner.py:310] Step = 61800 ; steps/s = 1.63, tokens/s = 82173 (37791 source, 44382 target) ; Learning rate = 0.000356 ; Loss = 1.801125\n",
      "2024-12-15 00:54:56.672000: I runner.py:310] Step = 61900 ; steps/s = 1.63, tokens/s = 82350 (37888 source, 44462 target) ; Learning rate = 0.000355 ; Loss = 1.806267\n",
      "2024-12-15 00:55:57.733000: I runner.py:310] Step = 62000 ; steps/s = 1.64, tokens/s = 81433 (37462 source, 43971 target) ; Learning rate = 0.000355 ; Loss = 1.778702\n",
      "2024-12-15 00:56:59.157000: I runner.py:310] Step = 62100 ; steps/s = 1.63, tokens/s = 82277 (37848 source, 44429 target) ; Learning rate = 0.000355 ; Loss = 1.786684\n",
      "2024-12-15 00:58:00.633000: I runner.py:310] Step = 62200 ; steps/s = 1.63, tokens/s = 82198 (37801 source, 44397 target) ; Learning rate = 0.000354 ; Loss = 1.813572\n",
      "2024-12-15 00:59:01.768000: I runner.py:310] Step = 62300 ; steps/s = 1.64, tokens/s = 81280 (37373 source, 43907 target) ; Learning rate = 0.000354 ; Loss = 1.787366\n",
      "2024-12-15 01:00:03.205000: I runner.py:310] Step = 62400 ; steps/s = 1.63, tokens/s = 82258 (37839 source, 44419 target) ; Learning rate = 0.000354 ; Loss = 1.787847\n",
      "2024-12-15 01:01:04.630000: I runner.py:310] Step = 62500 ; steps/s = 1.63, tokens/s = 82300 (37866 source, 44434 target) ; Learning rate = 0.000354 ; Loss = 1.793517\n",
      "2024-12-15 01:02:06.076000: I runner.py:310] Step = 62600 ; steps/s = 1.63, tokens/s = 82197 (37785 source, 44412 target) ; Learning rate = 0.000353 ; Loss = 1.808639\n",
      "2024-12-15 01:03:07.242000: I runner.py:310] Step = 62700 ; steps/s = 1.64, tokens/s = 81270 (37374 source, 43896 target) ; Learning rate = 0.000353 ; Loss = 1.773578\n",
      "2024-12-15 01:04:08.653000: I runner.py:310] Step = 62800 ; steps/s = 1.63, tokens/s = 82275 (37840 source, 44435 target) ; Learning rate = 0.000353 ; Loss = 1.796649\n",
      "2024-12-15 01:05:10.141000: I runner.py:310] Step = 62900 ; steps/s = 1.63, tokens/s = 82181 (37790 source, 44391 target) ; Learning rate = 0.000352 ; Loss = 1.799663\n",
      "2024-12-15 01:06:11.175000: I runner.py:310] Step = 63000 ; steps/s = 1.64, tokens/s = 81454 (37472 source, 43982 target) ; Learning rate = 0.000352 ; Loss = 1.814845\n",
      "2024-12-15 01:07:12.642000: I runner.py:310] Step = 63100 ; steps/s = 1.63, tokens/s = 82203 (37814 source, 44389 target) ; Learning rate = 0.000352 ; Loss = 1.795177\n",
      "2024-12-15 01:08:14.092000: I runner.py:310] Step = 63200 ; steps/s = 1.63, tokens/s = 82247 (37826 source, 44421 target) ; Learning rate = 0.000352 ; Loss = 1.784904\n",
      "2024-12-15 01:09:15.535000: I runner.py:310] Step = 63300 ; steps/s = 1.63, tokens/s = 82235 (37817 source, 44418 target) ; Learning rate = 0.000351 ; Loss = 1.793519\n",
      "2024-12-15 01:10:16.568000: I runner.py:310] Step = 63400 ; steps/s = 1.64, tokens/s = 81461 (37485 source, 43976 target) ; Learning rate = 0.000351 ; Loss = 1.761882\n",
      "2024-12-15 01:11:17.962000: I runner.py:310] Step = 63500 ; steps/s = 1.63, tokens/s = 82308 (37855 source, 44453 target) ; Learning rate = 0.000351 ; Loss = 1.802216\n",
      "2024-12-15 01:12:19.499000: I runner.py:310] Step = 63600 ; steps/s = 1.63, tokens/s = 82139 (37780 source, 44359 target) ; Learning rate = 0.000350 ; Loss = 1.807189\n",
      "2024-12-15 01:13:20.538000: I runner.py:310] Step = 63700 ; steps/s = 1.64, tokens/s = 81401 (37423 source, 43978 target) ; Learning rate = 0.000350 ; Loss = 1.810135\n",
      "2024-12-15 01:14:22.070000: I runner.py:310] Step = 63800 ; steps/s = 1.63, tokens/s = 82112 (37766 source, 44346 target) ; Learning rate = 0.000350 ; Loss = 1.788664\n",
      "2024-12-15 01:15:23.469000: I runner.py:310] Step = 63900 ; steps/s = 1.63, tokens/s = 82312 (37865 source, 44447 target) ; Learning rate = 0.000350 ; Loss = 1.796953\n",
      "2024-12-15 01:16:24.927000: I runner.py:310] Step = 64000 ; steps/s = 1.63, tokens/s = 82219 (37803 source, 44416 target) ; Learning rate = 0.000349 ; Loss = 1.803409\n",
      "2024-12-15 01:17:26.006000: I runner.py:310] Step = 64100 ; steps/s = 1.64, tokens/s = 81369 (37415 source, 43954 target) ; Learning rate = 0.000349 ; Loss = 1.809513\n",
      "2024-12-15 01:18:27.457000: I runner.py:310] Step = 64200 ; steps/s = 1.63, tokens/s = 82217 (37798 source, 44419 target) ; Learning rate = 0.000349 ; Loss = 1.787601\n",
      "2024-12-15 01:19:28.958000: I runner.py:310] Step = 64300 ; steps/s = 1.63, tokens/s = 82182 (37816 source, 44366 target) ; Learning rate = 0.000349 ; Loss = 1.781088\n",
      "2024-12-15 01:20:29.947000: I runner.py:310] Step = 64400 ; steps/s = 1.64, tokens/s = 81508 (37495 source, 44013 target) ; Learning rate = 0.000348 ; Loss = 1.770445\n",
      "2024-12-15 01:21:31.478000: I runner.py:310] Step = 64500 ; steps/s = 1.63, tokens/s = 82139 (37775 source, 44364 target) ; Learning rate = 0.000348 ; Loss = 1.805814\n",
      "2024-12-15 01:22:32.903000: I runner.py:310] Step = 64600 ; steps/s = 1.63, tokens/s = 82268 (37856 source, 44412 target) ; Learning rate = 0.000348 ; Loss = 1.803422\n",
      "2024-12-15 01:23:34.488000: I runner.py:310] Step = 64700 ; steps/s = 1.62, tokens/s = 82073 (37756 source, 44317 target) ; Learning rate = 0.000347 ; Loss = 1.808530\n",
      "2024-12-15 01:24:35.504000: I runner.py:310] Step = 64800 ; steps/s = 1.64, tokens/s = 81483 (37485 source, 43998 target) ; Learning rate = 0.000347 ; Loss = 1.773587\n",
      "2024-12-15 01:25:36.937000: I runner.py:310] Step = 64900 ; steps/s = 1.63, tokens/s = 82254 (37834 source, 44420 target) ; Learning rate = 0.000347 ; Loss = 1.805312\n",
      "2024-12-15 01:26:38.329000: I runner.py:310] Step = 65000 ; steps/s = 1.63, tokens/s = 82288 (37832 source, 44456 target) ; Learning rate = 0.000347 ; Loss = 1.808017\n",
      "2024-12-15 01:26:38.330000: I training.py:192] Running evaluation for step 65000\n",
      "2024-12-15 01:28:36.994000: I training.py:192] Evaluation result for step 65000: loss = 2.551539 ; perplexity = 12.826832\n",
      "2024-12-15 01:29:37.821000: I runner.py:310] Step = 65100 ; steps/s = 1.64, tokens/s = 81733 (37586 source, 44147 target) ; Learning rate = 0.000346 ; Loss = 1.814079\n",
      "2024-12-15 01:30:39.354000: I runner.py:310] Step = 65200 ; steps/s = 1.63, tokens/s = 82124 (37752 source, 44372 target) ; Learning rate = 0.000346 ; Loss = 1.775292\n",
      "2024-12-15 01:31:40.841000: I runner.py:310] Step = 65300 ; steps/s = 1.63, tokens/s = 82192 (37816 source, 44376 target) ; Learning rate = 0.000346 ; Loss = 1.788056\n",
      "2024-12-15 01:32:42.353000: I runner.py:310] Step = 65400 ; steps/s = 1.63, tokens/s = 82149 (37786 source, 44363 target) ; Learning rate = 0.000346 ; Loss = 1.784508\n",
      "2024-12-15 01:33:43.502000: I runner.py:310] Step = 65500 ; steps/s = 1.64, tokens/s = 81276 (37369 source, 43907 target) ; Learning rate = 0.000345 ; Loss = 1.765819\n",
      "2024-12-15 01:34:44.982000: I runner.py:310] Step = 65600 ; steps/s = 1.63, tokens/s = 82213 (37829 source, 44384 target) ; Learning rate = 0.000345 ; Loss = 1.775204\n",
      "2024-12-15 01:35:46.496000: I runner.py:310] Step = 65700 ; steps/s = 1.63, tokens/s = 82142 (37785 source, 44357 target) ; Learning rate = 0.000345 ; Loss = 1.805397\n",
      "2024-12-15 01:36:47.530000: I runner.py:310] Step = 65800 ; steps/s = 1.64, tokens/s = 81420 (37435 source, 43985 target) ; Learning rate = 0.000345 ; Loss = 1.817122\n",
      "2024-12-15 01:37:49.072000: I runner.py:310] Step = 65900 ; steps/s = 1.63, tokens/s = 82131 (37789 source, 44342 target) ; Learning rate = 0.000344 ; Loss = 1.785674\n",
      "2024-12-15 01:38:50.491000: I runner.py:310] Step = 66000 ; steps/s = 1.63, tokens/s = 82283 (37845 source, 44438 target) ; Learning rate = 0.000344 ; Loss = 1.786709\n",
      "2024-12-15 01:39:52.036000: I runner.py:310] Step = 66100 ; steps/s = 1.62, tokens/s = 82106 (37757 source, 44349 target) ; Learning rate = 0.000344 ; Loss = 1.783238\n",
      "2024-12-15 01:40:53.139000: I runner.py:310] Step = 66200 ; steps/s = 1.64, tokens/s = 81378 (37429 source, 43949 target) ; Learning rate = 0.000344 ; Loss = 1.773879\n",
      "2024-12-15 01:41:54.660000: I runner.py:310] Step = 66300 ; steps/s = 1.63, tokens/s = 82130 (37764 source, 44366 target) ; Learning rate = 0.000343 ; Loss = 1.797624\n",
      "2024-12-15 01:42:56.075000: I runner.py:310] Step = 66400 ; steps/s = 1.63, tokens/s = 82276 (37846 source, 44430 target) ; Learning rate = 0.000343 ; Loss = 1.796650\n",
      "2024-12-15 01:43:57.177000: I runner.py:310] Step = 66500 ; steps/s = 1.64, tokens/s = 81313 (37407 source, 43906 target) ; Learning rate = 0.000343 ; Loss = 1.798331\n",
      "2024-12-15 01:44:58.663000: I runner.py:310] Step = 66600 ; steps/s = 1.63, tokens/s = 82177 (37785 source, 44392 target) ; Learning rate = 0.000342 ; Loss = 1.784403\n",
      "2024-12-15 01:46:00.136000: I runner.py:310] Step = 66700 ; steps/s = 1.63, tokens/s = 82222 (37832 source, 44390 target) ; Learning rate = 0.000342 ; Loss = 1.788554\n",
      "2024-12-15 01:47:01.611000: I runner.py:310] Step = 66800 ; steps/s = 1.63, tokens/s = 81731 (37587 source, 44144 target) ; Learning rate = 0.000342 ; Loss = 1.771134\n",
      "2024-12-15 01:48:02.771000: I runner.py:310] Step = 66900 ; steps/s = 1.64, tokens/s = 81745 (37590 source, 44155 target) ; Learning rate = 0.000342 ; Loss = 1.780676\n",
      "2024-12-15 01:49:04.333000: I runner.py:310] Step = 67000 ; steps/s = 1.62, tokens/s = 82101 (37762 source, 44339 target) ; Learning rate = 0.000341 ; Loss = 1.779929\n",
      "2024-12-15 01:50:05.777000: I runner.py:310] Step = 67100 ; steps/s = 1.63, tokens/s = 82242 (37835 source, 44407 target) ; Learning rate = 0.000341 ; Loss = 1.772745\n",
      "2024-12-15 01:51:06.892000: I runner.py:310] Step = 67200 ; steps/s = 1.64, tokens/s = 81329 (37397 source, 43932 target) ; Learning rate = 0.000341 ; Loss = 1.779124\n",
      "2024-12-15 01:52:08.273000: I runner.py:310] Step = 67300 ; steps/s = 1.63, tokens/s = 82355 (37902 source, 44453 target) ; Learning rate = 0.000341 ; Loss = 1.792835\n",
      "2024-12-15 01:53:09.713000: I runner.py:310] Step = 67400 ; steps/s = 1.63, tokens/s = 82239 (37820 source, 44419 target) ; Learning rate = 0.000340 ; Loss = 1.799283\n",
      "2024-12-15 01:54:10.878000: I runner.py:310] Step = 67500 ; steps/s = 1.64, tokens/s = 81249 (37358 source, 43891 target) ; Learning rate = 0.000340 ; Loss = 1.775713\n",
      "2024-12-15 01:55:12.342000: I runner.py:310] Step = 67600 ; steps/s = 1.63, tokens/s = 82224 (37799 source, 44425 target) ; Learning rate = 0.000340 ; Loss = 1.780683\n",
      "2024-12-15 01:56:13.821000: I runner.py:310] Step = 67700 ; steps/s = 1.63, tokens/s = 82177 (37795 source, 44382 target) ; Learning rate = 0.000340 ; Loss = 1.802973\n",
      "2024-12-15 01:57:15.300000: I runner.py:310] Step = 67800 ; steps/s = 1.63, tokens/s = 82223 (37835 source, 44388 target) ; Learning rate = 0.000339 ; Loss = 1.803478\n",
      "2024-12-15 01:58:16.452000: I runner.py:310] Step = 67900 ; steps/s = 1.64, tokens/s = 81268 (37386 source, 43882 target) ; Learning rate = 0.000339 ; Loss = 1.786562\n",
      "2024-12-15 01:59:18.814000: I runner.py:310] Step = 68000 ; steps/s = 1.60, tokens/s = 81019 (37249 source, 43770 target) ; Learning rate = 0.000339 ; Loss = 1.776633\n",
      "2024-12-15 02:00:20.470000: I runner.py:310] Step = 68100 ; steps/s = 1.62, tokens/s = 81961 (37705 source, 44256 target) ; Learning rate = 0.000339 ; Loss = 1.792792\n",
      "2024-12-15 02:01:21.638000: I runner.py:310] Step = 68200 ; steps/s = 1.64, tokens/s = 81266 (37376 source, 43890 target) ; Learning rate = 0.000338 ; Loss = 1.780848\n",
      "2024-12-15 02:02:23.321000: I runner.py:310] Step = 68300 ; steps/s = 1.62, tokens/s = 81942 (37692 source, 44250 target) ; Learning rate = 0.000338 ; Loss = 1.772188\n",
      "2024-12-15 02:03:24.947000: I runner.py:310] Step = 68400 ; steps/s = 1.62, tokens/s = 81992 (37718 source, 44274 target) ; Learning rate = 0.000338 ; Loss = 1.796489\n",
      "2024-12-15 02:04:26.523000: I runner.py:310] Step = 68500 ; steps/s = 1.62, tokens/s = 82080 (37745 source, 44335 target) ; Learning rate = 0.000338 ; Loss = 1.807318\n",
      "2024-12-15 02:05:27.688000: I runner.py:310] Step = 68600 ; steps/s = 1.64, tokens/s = 81243 (37370 source, 43873 target) ; Learning rate = 0.000337 ; Loss = 1.809580\n",
      "2024-12-15 02:06:29.347000: I runner.py:310] Step = 68700 ; steps/s = 1.62, tokens/s = 81978 (37715 source, 44263 target) ; Learning rate = 0.000337 ; Loss = 1.753890\n",
      "2024-12-15 02:07:30.998000: I runner.py:310] Step = 68800 ; steps/s = 1.62, tokens/s = 81963 (37691 source, 44272 target) ; Learning rate = 0.000337 ; Loss = 1.784489\n",
      "2024-12-15 02:08:32.249000: I runner.py:310] Step = 68900 ; steps/s = 1.63, tokens/s = 81137 (37311 source, 43826 target) ; Learning rate = 0.000337 ; Loss = 1.787942\n",
      "2024-12-15 02:09:33.892000: I runner.py:310] Step = 69000 ; steps/s = 1.62, tokens/s = 81949 (37668 source, 44281 target) ; Learning rate = 0.000336 ; Loss = 1.781320\n",
      "2024-12-15 02:10:35.478000: I runner.py:310] Step = 69100 ; steps/s = 1.62, tokens/s = 82060 (37743 source, 44317 target) ; Learning rate = 0.000336 ; Loss = 1.782485\n",
      "2024-12-15 02:11:37.075000: I runner.py:310] Step = 69200 ; steps/s = 1.62, tokens/s = 82072 (37766 source, 44306 target) ; Learning rate = 0.000336 ; Loss = 1.797922\n",
      "2024-12-15 02:12:38.339000: I runner.py:310] Step = 69300 ; steps/s = 1.63, tokens/s = 81097 (37283 source, 43814 target) ; Learning rate = 0.000336 ; Loss = 1.812270\n",
      "2024-12-15 02:13:39.931000: I runner.py:310] Step = 69400 ; steps/s = 1.62, tokens/s = 82053 (37742 source, 44311 target) ; Learning rate = 0.000336 ; Loss = 1.782972\n",
      "2024-12-15 02:14:41.560000: I runner.py:310] Step = 69500 ; steps/s = 1.62, tokens/s = 82008 (37717 source, 44291 target) ; Learning rate = 0.000335 ; Loss = 1.776266\n",
      "2024-12-15 02:15:42.764000: I runner.py:310] Step = 69600 ; steps/s = 1.63, tokens/s = 81218 (37372 source, 43846 target) ; Learning rate = 0.000335 ; Loss = 1.767377\n",
      "2024-12-15 02:16:44.398000: I runner.py:310] Step = 69700 ; steps/s = 1.62, tokens/s = 82017 (37739 source, 44278 target) ; Learning rate = 0.000335 ; Loss = 1.783700\n",
      "2024-12-15 02:17:46.013000: I runner.py:310] Step = 69800 ; steps/s = 1.62, tokens/s = 82020 (37726 source, 44294 target) ; Learning rate = 0.000335 ; Loss = 1.799623\n",
      "2024-12-15 02:18:47.646000: I runner.py:310] Step = 69900 ; steps/s = 1.62, tokens/s = 81967 (37692 source, 44275 target) ; Learning rate = 0.000334 ; Loss = 1.782771\n",
      "2024-12-15 02:19:48.908000: I runner.py:310] Step = 70000 ; steps/s = 1.63, tokens/s = 81148 (37320 source, 43828 target) ; Learning rate = 0.000334 ; Loss = 1.751514\n",
      "2024-12-15 02:19:50.432000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-70000\n",
      "2024-12-15 02:19:50.432000: I training.py:192] Running evaluation for step 70000\n",
      "2024-12-15 02:21:54.926000: I training.py:192] Evaluation result for step 70000: loss = 2.573625 ; perplexity = 13.113278\n",
      "2024-12-15 02:22:56.396000: I runner.py:310] Step = 70100 ; steps/s = 1.63, tokens/s = 82233 (37829 source, 44404 target) ; Learning rate = 0.000334 ; Loss = 1.808974\n",
      "2024-12-15 02:23:57.945000: I runner.py:310] Step = 70200 ; steps/s = 1.62, tokens/s = 82087 (37737 source, 44350 target) ; Learning rate = 0.000334 ; Loss = 1.788670\n",
      "2024-12-15 02:24:59.090000: I runner.py:310] Step = 70300 ; steps/s = 1.64, tokens/s = 81275 (37383 source, 43892 target) ; Learning rate = 0.000333 ; Loss = 1.773024\n",
      "2024-12-15 02:26:00.742000: I runner.py:310] Step = 70400 ; steps/s = 1.62, tokens/s = 81962 (37689 source, 44273 target) ; Learning rate = 0.000333 ; Loss = 1.778165\n",
      "2024-12-15 02:27:02.366000: I runner.py:310] Step = 70500 ; steps/s = 1.62, tokens/s = 81992 (37706 source, 44286 target) ; Learning rate = 0.000333 ; Loss = 1.786931\n",
      "2024-12-15 02:28:04.049000: I runner.py:310] Step = 70600 ; steps/s = 1.62, tokens/s = 81948 (37705 source, 44243 target) ; Learning rate = 0.000333 ; Loss = 1.799345\n",
      "2024-12-15 02:29:05.219000: I runner.py:310] Step = 70700 ; steps/s = 1.63, tokens/s = 81262 (37376 source, 43886 target) ; Learning rate = 0.000332 ; Loss = 1.752567\n",
      "2024-12-15 02:30:06.840000: I runner.py:310] Step = 70800 ; steps/s = 1.62, tokens/s = 81990 (37707 source, 44283 target) ; Learning rate = 0.000332 ; Loss = 1.787398\n",
      "2024-12-15 02:31:08.464000: I runner.py:310] Step = 70900 ; steps/s = 1.62, tokens/s = 82011 (37717 source, 44294 target) ; Learning rate = 0.000332 ; Loss = 1.788420\n",
      "2024-12-15 02:32:09.694000: I runner.py:310] Step = 71000 ; steps/s = 1.63, tokens/s = 81178 (37334 source, 43844 target) ; Learning rate = 0.000332 ; Loss = 1.790305\n",
      "2024-12-15 02:33:11.329000: I runner.py:310] Step = 71100 ; steps/s = 1.62, tokens/s = 81983 (37697 source, 44286 target) ; Learning rate = 0.000331 ; Loss = 1.781815\n",
      "2024-12-15 02:34:12.982000: I runner.py:310] Step = 71200 ; steps/s = 1.62, tokens/s = 81968 (37696 source, 44272 target) ; Learning rate = 0.000331 ; Loss = 1.774882\n",
      "2024-12-15 02:35:14.607000: I runner.py:310] Step = 71300 ; steps/s = 1.62, tokens/s = 82011 (37735 source, 44276 target) ; Learning rate = 0.000331 ; Loss = 1.789105\n",
      "2024-12-15 02:36:15.802000: I runner.py:310] Step = 71400 ; steps/s = 1.63, tokens/s = 81218 (37368 source, 43850 target) ; Learning rate = 0.000331 ; Loss = 1.794172\n",
      "2024-12-15 02:37:17.393000: I runner.py:310] Step = 71500 ; steps/s = 1.62, tokens/s = 82008 (37684 source, 44324 target) ; Learning rate = 0.000331 ; Loss = 1.772936\n",
      "2024-12-15 02:38:18.977000: I runner.py:310] Step = 71600 ; steps/s = 1.62, tokens/s = 82081 (37769 source, 44312 target) ; Learning rate = 0.000330 ; Loss = 1.775818\n",
      "2024-12-15 02:39:20.153000: I runner.py:310] Step = 71700 ; steps/s = 1.63, tokens/s = 81277 (37394 source, 43883 target) ; Learning rate = 0.000330 ; Loss = 1.760576\n",
      "2024-12-15 02:40:21.748000: I runner.py:310] Step = 71800 ; steps/s = 1.62, tokens/s = 82066 (37763 source, 44303 target) ; Learning rate = 0.000330 ; Loss = 1.765565\n",
      "2024-12-15 02:41:23.389000: I runner.py:310] Step = 71900 ; steps/s = 1.62, tokens/s = 81989 (37707 source, 44282 target) ; Learning rate = 0.000330 ; Loss = 1.786805\n",
      "2024-12-15 02:42:25.021000: I runner.py:310] Step = 72000 ; steps/s = 1.62, tokens/s = 81956 (37679 source, 44277 target) ; Learning rate = 0.000329 ; Loss = 1.786038\n",
      "2024-12-15 02:43:26.231000: I runner.py:310] Step = 72100 ; steps/s = 1.63, tokens/s = 81233 (37362 source, 43871 target) ; Learning rate = 0.000329 ; Loss = 1.790917\n",
      "2024-12-15 02:44:27.809000: I runner.py:310] Step = 72200 ; steps/s = 1.62, tokens/s = 82048 (37730 source, 44318 target) ; Learning rate = 0.000329 ; Loss = 1.753952\n",
      "2024-12-15 02:45:29.411000: I runner.py:310] Step = 72300 ; steps/s = 1.62, tokens/s = 82043 (37735 source, 44308 target) ; Learning rate = 0.000329 ; Loss = 1.769282\n",
      "2024-12-15 02:46:30.605000: I runner.py:310] Step = 72400 ; steps/s = 1.63, tokens/s = 81208 (37339 source, 43869 target) ; Learning rate = 0.000328 ; Loss = 1.791477\n",
      "2024-12-15 02:47:32.191000: I runner.py:310] Step = 72500 ; steps/s = 1.62, tokens/s = 82068 (37761 source, 44307 target) ; Learning rate = 0.000328 ; Loss = 1.763239\n",
      "2024-12-15 02:48:33.807000: I runner.py:310] Step = 72600 ; steps/s = 1.62, tokens/s = 82020 (37732 source, 44288 target) ; Learning rate = 0.000328 ; Loss = 1.775820\n",
      "2024-12-15 02:49:35.364000: I runner.py:310] Step = 72700 ; steps/s = 1.62, tokens/s = 82088 (37750 source, 44338 target) ; Learning rate = 0.000328 ; Loss = 1.776862\n",
      "2024-12-15 02:50:36.595000: I runner.py:310] Step = 72800 ; steps/s = 1.63, tokens/s = 81192 (37340 source, 43852 target) ; Learning rate = 0.000328 ; Loss = 1.746001\n",
      "2024-12-15 02:51:38.192000: I runner.py:310] Step = 72900 ; steps/s = 1.62, tokens/s = 82025 (37732 source, 44293 target) ; Learning rate = 0.000327 ; Loss = 1.788038\n",
      "2024-12-15 02:52:39.802000: I runner.py:310] Step = 73000 ; steps/s = 1.62, tokens/s = 82021 (37730 source, 44291 target) ; Learning rate = 0.000327 ; Loss = 1.797650\n",
      "2024-12-15 02:53:40.978000: I runner.py:310] Step = 73100 ; steps/s = 1.63, tokens/s = 81259 (37363 source, 43896 target) ; Learning rate = 0.000327 ; Loss = 1.751928\n",
      "2024-12-15 02:54:42.639000: I runner.py:310] Step = 73200 ; steps/s = 1.62, tokens/s = 81957 (37707 source, 44250 target) ; Learning rate = 0.000327 ; Loss = 1.769128\n",
      "2024-12-15 02:55:44.232000: I runner.py:310] Step = 73300 ; steps/s = 1.62, tokens/s = 82070 (37756 source, 44314 target) ; Learning rate = 0.000326 ; Loss = 1.797122\n",
      "2024-12-15 02:56:45.891000: I runner.py:310] Step = 73400 ; steps/s = 1.62, tokens/s = 81928 (37657 source, 44271 target) ; Learning rate = 0.000326 ; Loss = 1.801068\n",
      "2024-12-15 02:57:47.125000: I runner.py:310] Step = 73500 ; steps/s = 1.63, tokens/s = 81144 (37315 source, 43829 target) ; Learning rate = 0.000326 ; Loss = 1.757426\n",
      "2024-12-15 02:58:48.717000: I runner.py:310] Step = 73600 ; steps/s = 1.62, tokens/s = 82058 (37759 source, 44299 target) ; Learning rate = 0.000326 ; Loss = 1.770279\n",
      "2024-12-15 02:59:50.206000: I runner.py:310] Step = 73700 ; steps/s = 1.63, tokens/s = 82184 (37793 source, 44391 target) ; Learning rate = 0.000326 ; Loss = 1.781824\n",
      "2024-12-15 03:00:51.429000: I runner.py:310] Step = 73800 ; steps/s = 1.63, tokens/s = 81173 (37328 source, 43845 target) ; Learning rate = 0.000325 ; Loss = 1.754355\n",
      "2024-12-15 03:01:53.054000: I runner.py:310] Step = 73900 ; steps/s = 1.62, tokens/s = 82008 (37706 source, 44302 target) ; Learning rate = 0.000325 ; Loss = 1.780910\n",
      "2024-12-15 03:02:54.688000: I runner.py:310] Step = 74000 ; steps/s = 1.62, tokens/s = 82004 (37734 source, 44270 target) ; Learning rate = 0.000325 ; Loss = 1.781062\n",
      "2024-12-15 03:03:56.306000: I runner.py:310] Step = 74100 ; steps/s = 1.62, tokens/s = 82023 (37724 source, 44299 target) ; Learning rate = 0.000325 ; Loss = 1.792900\n",
      "2024-12-15 03:04:57.485000: I runner.py:310] Step = 74200 ; steps/s = 1.63, tokens/s = 81227 (37364 source, 43863 target) ; Learning rate = 0.000324 ; Loss = 1.779368\n",
      "2024-12-15 03:05:59.043000: I runner.py:310] Step = 74300 ; steps/s = 1.62, tokens/s = 82102 (37764 source, 44338 target) ; Learning rate = 0.000324 ; Loss = 1.762224\n",
      "2024-12-15 03:07:00.681000: I runner.py:310] Step = 74400 ; steps/s = 1.62, tokens/s = 82003 (37719 source, 44284 target) ; Learning rate = 0.000324 ; Loss = 1.770972\n",
      "2024-12-15 03:08:01.896000: I runner.py:310] Step = 74500 ; steps/s = 1.63, tokens/s = 81175 (37333 source, 43842 target) ; Learning rate = 0.000324 ; Loss = 1.759199\n",
      "2024-12-15 03:09:03.551000: I runner.py:310] Step = 74600 ; steps/s = 1.62, tokens/s = 81983 (37718 source, 44265 target) ; Learning rate = 0.000324 ; Loss = 1.777197\n",
      "2024-12-15 03:10:05.164000: I runner.py:310] Step = 74700 ; steps/s = 1.62, tokens/s = 82015 (37718 source, 44297 target) ; Learning rate = 0.000323 ; Loss = 1.776269\n",
      "2024-12-15 03:11:06.818000: I runner.py:310] Step = 74800 ; steps/s = 1.62, tokens/s = 81968 (37685 source, 44283 target) ; Learning rate = 0.000323 ; Loss = 1.790209\n",
      "2024-12-15 03:12:08.003000: I runner.py:310] Step = 74900 ; steps/s = 1.63, tokens/s = 81210 (37349 source, 43861 target) ; Learning rate = 0.000323 ; Loss = 1.778262\n",
      "2024-12-15 03:13:09.620000: I runner.py:310] Step = 75000 ; steps/s = 1.62, tokens/s = 82002 (37704 source, 44298 target) ; Learning rate = 0.000323 ; Loss = 1.776299\n",
      "2024-12-15 03:13:09.621000: I training.py:192] Running evaluation for step 75000\n",
      "2024-12-15 03:15:13.137000: I training.py:192] Evaluation result for step 75000: loss = 2.586705 ; perplexity = 13.285929\n",
      "2024-12-15 03:16:14.560000: I runner.py:310] Step = 75100 ; steps/s = 1.63, tokens/s = 82304 (37862 source, 44442 target) ; Learning rate = 0.000323 ; Loss = 1.768100\n",
      "2024-12-15 03:17:15.815000: I runner.py:310] Step = 75200 ; steps/s = 1.63, tokens/s = 81179 (37350 source, 43829 target) ; Learning rate = 0.000322 ; Loss = 1.757340\n",
      "2024-12-15 03:18:17.456000: I runner.py:310] Step = 75300 ; steps/s = 1.62, tokens/s = 81978 (37695 source, 44283 target) ; Learning rate = 0.000322 ; Loss = 1.774434\n",
      "2024-12-15 03:19:19.110000: I runner.py:310] Step = 75400 ; steps/s = 1.62, tokens/s = 81964 (37701 source, 44263 target) ; Learning rate = 0.000322 ; Loss = 1.775557\n",
      "2024-12-15 03:20:20.388000: I runner.py:310] Step = 75500 ; steps/s = 1.63, tokens/s = 81274 (37381 source, 43893 target) ; Learning rate = 0.000322 ; Loss = 1.826161\n",
      "2024-12-15 03:21:21.931000: I runner.py:310] Step = 75600 ; steps/s = 1.63, tokens/s = 81952 (37690 source, 44262 target) ; Learning rate = 0.000321 ; Loss = 1.768190\n",
      "2024-12-15 03:22:23.609000: I runner.py:310] Step = 75700 ; steps/s = 1.62, tokens/s = 81927 (37681 source, 44246 target) ; Learning rate = 0.000321 ; Loss = 1.759522\n",
      "2024-12-15 03:23:25.218000: I runner.py:310] Step = 75800 ; steps/s = 1.62, tokens/s = 82022 (37724 source, 44298 target) ; Learning rate = 0.000321 ; Loss = 1.767445\n",
      "2024-12-15 03:24:26.373000: I runner.py:310] Step = 75900 ; steps/s = 1.64, tokens/s = 81255 (37378 source, 43877 target) ; Learning rate = 0.000321 ; Loss = 1.746872\n",
      "2024-12-15 03:25:27.967000: I runner.py:310] Step = 76000 ; steps/s = 1.62, tokens/s = 82052 (37736 source, 44316 target) ; Learning rate = 0.000321 ; Loss = 1.764116\n",
      "2024-12-15 03:26:29.637000: I runner.py:310] Step = 76100 ; steps/s = 1.62, tokens/s = 81962 (37700 source, 44262 target) ; Learning rate = 0.000320 ; Loss = 1.796811\n",
      "2024-12-15 03:27:30.858000: I runner.py:310] Step = 76200 ; steps/s = 1.63, tokens/s = 81180 (37336 source, 43844 target) ; Learning rate = 0.000320 ; Loss = 1.763290\n",
      "2024-12-15 03:28:32.445000: I runner.py:310] Step = 76300 ; steps/s = 1.62, tokens/s = 82034 (37714 source, 44320 target) ; Learning rate = 0.000320 ; Loss = 1.765319\n",
      "2024-12-15 03:29:34.071000: I runner.py:310] Step = 76400 ; steps/s = 1.62, tokens/s = 82001 (37718 source, 44283 target) ; Learning rate = 0.000320 ; Loss = 1.773765\n",
      "2024-12-15 03:30:35.735000: I runner.py:310] Step = 76500 ; steps/s = 1.62, tokens/s = 81975 (37705 source, 44270 target) ; Learning rate = 0.000320 ; Loss = 1.770008\n",
      "2024-12-15 03:31:36.938000: I runner.py:310] Step = 76600 ; steps/s = 1.63, tokens/s = 81205 (37354 source, 43851 target) ; Learning rate = 0.000319 ; Loss = 1.741702\n",
      "2024-12-15 03:32:38.590000: I runner.py:310] Step = 76700 ; steps/s = 1.62, tokens/s = 81966 (37705 source, 44261 target) ; Learning rate = 0.000319 ; Loss = 1.776445\n",
      "2024-12-15 03:33:40.292000: I runner.py:310] Step = 76800 ; steps/s = 1.62, tokens/s = 81893 (37665 source, 44228 target) ; Learning rate = 0.000319 ; Loss = 1.774235\n",
      "2024-12-15 03:34:41.574000: I runner.py:310] Step = 76900 ; steps/s = 1.63, tokens/s = 81112 (37299 source, 43813 target) ; Learning rate = 0.000319 ; Loss = 1.756360\n",
      "2024-12-15 03:35:43.236000: I runner.py:310] Step = 77000 ; steps/s = 1.62, tokens/s = 81993 (37733 source, 44260 target) ; Learning rate = 0.000319 ; Loss = 1.759873\n",
      "2024-12-15 03:36:44.928000: I runner.py:310] Step = 77100 ; steps/s = 1.62, tokens/s = 81890 (37656 source, 44234 target) ; Learning rate = 0.000318 ; Loss = 1.772041\n",
      "2024-12-15 03:37:46.508000: I runner.py:310] Step = 77200 ; steps/s = 1.62, tokens/s = 82070 (37750 source, 44320 target) ; Learning rate = 0.000318 ; Loss = 1.779232\n",
      "2024-12-15 03:38:47.758000: I runner.py:310] Step = 77300 ; steps/s = 1.63, tokens/s = 81127 (37288 source, 43839 target) ; Learning rate = 0.000318 ; Loss = 1.777654\n",
      "2024-12-15 03:39:49.416000: I runner.py:310] Step = 77400 ; steps/s = 1.62, tokens/s = 81940 (37678 source, 44262 target) ; Learning rate = 0.000318 ; Loss = 1.750992\n",
      "2024-12-15 03:40:51.084000: I runner.py:310] Step = 77500 ; steps/s = 1.62, tokens/s = 81981 (37730 source, 44251 target) ; Learning rate = 0.000317 ; Loss = 1.771042\n",
      "2024-12-15 03:41:52.366000: I runner.py:310] Step = 77600 ; steps/s = 1.63, tokens/s = 81111 (37304 source, 43807 target) ; Learning rate = 0.000317 ; Loss = 1.770423\n",
      "2024-12-15 03:42:54.023000: I runner.py:310] Step = 77700 ; steps/s = 1.62, tokens/s = 81955 (37692 source, 44263 target) ; Learning rate = 0.000317 ; Loss = 1.762588\n",
      "2024-12-15 03:43:55.656000: I runner.py:310] Step = 77800 ; steps/s = 1.62, tokens/s = 81974 (37692 source, 44282 target) ; Learning rate = 0.000317 ; Loss = 1.750753\n",
      "2024-12-15 03:44:57.294000: I runner.py:310] Step = 77900 ; steps/s = 1.62, tokens/s = 81998 (37724 source, 44274 target) ; Learning rate = 0.000317 ; Loss = 1.769877\n",
      "2024-12-15 03:45:58.516000: I runner.py:310] Step = 78000 ; steps/s = 1.63, tokens/s = 81204 (37351 source, 43853 target) ; Learning rate = 0.000316 ; Loss = 1.735409\n",
      "2024-12-15 03:47:00.133000: I runner.py:310] Step = 78100 ; steps/s = 1.62, tokens/s = 82007 (37716 source, 44291 target) ; Learning rate = 0.000316 ; Loss = 1.772205\n",
      "2024-12-15 03:48:01.669000: I runner.py:310] Step = 78200 ; steps/s = 1.63, tokens/s = 82124 (37781 source, 44343 target) ; Learning rate = 0.000316 ; Loss = 1.764963\n",
      "2024-12-15 03:49:02.856000: I runner.py:310] Step = 78300 ; steps/s = 1.63, tokens/s = 81222 (37345 source, 43877 target) ; Learning rate = 0.000316 ; Loss = 1.769534\n",
      "2024-12-15 03:50:04.489000: I runner.py:310] Step = 78400 ; steps/s = 1.62, tokens/s = 81998 (37713 source, 44285 target) ; Learning rate = 0.000316 ; Loss = 1.759271\n",
      "2024-12-15 03:51:06.129000: I runner.py:310] Step = 78500 ; steps/s = 1.62, tokens/s = 81987 (37710 source, 44277 target) ; Learning rate = 0.000315 ; Loss = 1.767871\n",
      "2024-12-15 03:52:07.764000: I runner.py:310] Step = 78600 ; steps/s = 1.62, tokens/s = 81985 (37711 source, 44274 target) ; Learning rate = 0.000315 ; Loss = 1.758920\n",
      "2024-12-15 03:53:08.999000: I runner.py:310] Step = 78700 ; steps/s = 1.63, tokens/s = 81174 (37326 source, 43848 target) ; Learning rate = 0.000315 ; Loss = 1.735925\n",
      "2024-12-15 03:54:10.594000: I runner.py:310] Step = 78800 ; steps/s = 1.62, tokens/s = 82058 (37751 source, 44307 target) ; Learning rate = 0.000315 ; Loss = 1.780122\n",
      "2024-12-15 03:55:12.278000: I runner.py:310] Step = 78900 ; steps/s = 1.62, tokens/s = 81930 (37684 source, 44246 target) ; Learning rate = 0.000315 ; Loss = 1.776883\n",
      "2024-12-15 03:56:13.514000: I runner.py:310] Step = 79000 ; steps/s = 1.63, tokens/s = 81150 (37328 source, 43822 target) ; Learning rate = 0.000314 ; Loss = 1.764017\n",
      "2024-12-15 03:57:15.143000: I runner.py:310] Step = 79100 ; steps/s = 1.62, tokens/s = 82001 (37719 source, 44282 target) ; Learning rate = 0.000314 ; Loss = 1.759807\n",
      "2024-12-15 03:58:16.779000: I runner.py:310] Step = 79200 ; steps/s = 1.62, tokens/s = 82004 (37710 source, 44294 target) ; Learning rate = 0.000314 ; Loss = 1.766981\n",
      "2024-12-15 03:59:18.389000: I runner.py:310] Step = 79300 ; steps/s = 1.62, tokens/s = 82019 (37723 source, 44296 target) ; Learning rate = 0.000314 ; Loss = 1.772914\n",
      "2024-12-15 04:00:19.592000: I runner.py:310] Step = 79400 ; steps/s = 1.63, tokens/s = 81226 (37367 source, 43859 target) ; Learning rate = 0.000314 ; Loss = 1.734310\n",
      "2024-12-15 04:01:21.204000: I runner.py:310] Step = 79500 ; steps/s = 1.62, tokens/s = 82011 (37717 source, 44294 target) ; Learning rate = 0.000313 ; Loss = 1.772113\n",
      "2024-12-15 04:02:22.828000: I runner.py:310] Step = 79600 ; steps/s = 1.62, tokens/s = 81997 (37705 source, 44292 target) ; Learning rate = 0.000313 ; Loss = 1.773771\n",
      "2024-12-15 04:03:24.044000: I runner.py:310] Step = 79700 ; steps/s = 1.63, tokens/s = 81171 (37325 source, 43846 target) ; Learning rate = 0.000313 ; Loss = 1.772624\n",
      "2024-12-15 04:04:25.640000: I runner.py:310] Step = 79800 ; steps/s = 1.62, tokens/s = 82059 (37747 source, 44312 target) ; Learning rate = 0.000313 ; Loss = 1.757908\n",
      "2024-12-15 04:05:27.259000: I runner.py:310] Step = 79900 ; steps/s = 1.62, tokens/s = 82022 (37729 source, 44293 target) ; Learning rate = 0.000313 ; Loss = 1.763270\n",
      "2024-12-15 04:06:28.844000: I runner.py:310] Step = 80000 ; steps/s = 1.62, tokens/s = 82056 (37742 source, 44314 target) ; Learning rate = 0.000312 ; Loss = 1.759751\n",
      "2024-12-15 04:06:30.442000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-80000\n",
      "2024-12-15 04:06:30.442000: I training.py:192] Running evaluation for step 80000\n",
      "2024-12-15 04:08:35.017000: I training.py:192] Evaluation result for step 80000: loss = 2.597014 ; perplexity = 13.423592\n",
      "2024-12-15 04:09:36.030000: I runner.py:310] Step = 80100 ; steps/s = 1.64, tokens/s = 81500 (37503 source, 43997 target) ; Learning rate = 0.000312 ; Loss = 1.732808\n",
      "2024-12-15 04:10:37.680000: I runner.py:310] Step = 80200 ; steps/s = 1.62, tokens/s = 81968 (37688 source, 44280 target) ; Learning rate = 0.000312 ; Loss = 1.764365\n",
      "2024-12-15 04:11:39.351000: I runner.py:310] Step = 80300 ; steps/s = 1.62, tokens/s = 81918 (37661 source, 44257 target) ; Learning rate = 0.000312 ; Loss = 1.768183\n",
      "2024-12-15 04:12:40.507000: I runner.py:310] Step = 80400 ; steps/s = 1.64, tokens/s = 81256 (37368 source, 43888 target) ; Learning rate = 0.000312 ; Loss = 1.777349\n",
      "2024-12-15 04:13:42.123000: I runner.py:310] Step = 80500 ; steps/s = 1.62, tokens/s = 82046 (37755 source, 44291 target) ; Learning rate = 0.000312 ; Loss = 1.764870\n",
      "2024-12-15 04:14:43.757000: I runner.py:310] Step = 80600 ; steps/s = 1.62, tokens/s = 81986 (37708 source, 44278 target) ; Learning rate = 0.000311 ; Loss = 1.756123\n",
      "2024-12-15 04:15:45.394000: I runner.py:310] Step = 80700 ; steps/s = 1.62, tokens/s = 81980 (37706 source, 44274 target) ; Learning rate = 0.000311 ; Loss = 1.751843\n",
      "2024-12-15 04:16:46.567000: I runner.py:310] Step = 80800 ; steps/s = 1.63, tokens/s = 81244 (37350 source, 43894 target) ; Learning rate = 0.000311 ; Loss = 1.786592\n",
      "2024-12-15 04:17:48.212000: I runner.py:310] Step = 80900 ; steps/s = 1.62, tokens/s = 81970 (37694 source, 44276 target) ; Learning rate = 0.000311 ; Loss = 1.758761\n",
      "2024-12-15 04:18:49.861000: I runner.py:310] Step = 81000 ; steps/s = 1.62, tokens/s = 81992 (37728 source, 44264 target) ; Learning rate = 0.000311 ; Loss = 1.767946\n",
      "2024-12-15 04:19:51.144000: I runner.py:310] Step = 81100 ; steps/s = 1.63, tokens/s = 81104 (37295 source, 43809 target) ; Learning rate = 0.000310 ; Loss = 1.752906\n",
      "2024-12-15 04:20:52.779000: I runner.py:310] Step = 81200 ; steps/s = 1.62, tokens/s = 82035 (37767 source, 44268 target) ; Learning rate = 0.000310 ; Loss = 1.772861\n",
      "2024-12-15 04:21:54.361000: I runner.py:310] Step = 81300 ; steps/s = 1.62, tokens/s = 82043 (37720 source, 44323 target) ; Learning rate = 0.000310 ; Loss = 1.764472\n",
      "2024-12-15 04:22:56.022000: I runner.py:310] Step = 81400 ; steps/s = 1.62, tokens/s = 81936 (37677 source, 44259 target) ; Learning rate = 0.000310 ; Loss = 1.772959\n",
      "2024-12-15 04:23:57.208000: I runner.py:310] Step = 81500 ; steps/s = 1.63, tokens/s = 81222 (37355 source, 43867 target) ; Learning rate = 0.000310 ; Loss = 1.770268\n",
      "2024-12-15 04:24:58.907000: I runner.py:310] Step = 81600 ; steps/s = 1.62, tokens/s = 81895 (37660 source, 44235 target) ; Learning rate = 0.000309 ; Loss = 1.761116\n",
      "2024-12-15 04:26:00.548000: I runner.py:310] Step = 81700 ; steps/s = 1.62, tokens/s = 82001 (37722 source, 44279 target) ; Learning rate = 0.000309 ; Loss = 1.775270\n",
      "2024-12-15 04:27:01.735000: I runner.py:310] Step = 81800 ; steps/s = 1.63, tokens/s = 81244 (37361 source, 43883 target) ; Learning rate = 0.000309 ; Loss = 1.747251\n",
      "2024-12-15 04:28:03.369000: I runner.py:310] Step = 81900 ; steps/s = 1.62, tokens/s = 82001 (37721 source, 44280 target) ; Learning rate = 0.000309 ; Loss = 1.760332\n",
      "2024-12-15 04:29:04.911000: I runner.py:310] Step = 82000 ; steps/s = 1.63, tokens/s = 82121 (37783 source, 44338 target) ; Learning rate = 0.000309 ; Loss = 1.779533\n",
      "2024-12-15 04:30:06.564000: I runner.py:310] Step = 82100 ; steps/s = 1.62, tokens/s = 81957 (37688 source, 44269 target) ; Learning rate = 0.000308 ; Loss = 1.784019\n",
      "2024-12-15 04:31:07.829000: I runner.py:310] Step = 82200 ; steps/s = 1.63, tokens/s = 81143 (37317 source, 43826 target) ; Learning rate = 0.000308 ; Loss = 1.747208\n",
      "2024-12-15 04:32:09.373000: I runner.py:310] Step = 82300 ; steps/s = 1.63, tokens/s = 82089 (37756 source, 44333 target) ; Learning rate = 0.000308 ; Loss = 1.769434\n",
      "2024-12-15 04:33:10.977000: I runner.py:310] Step = 82400 ; steps/s = 1.62, tokens/s = 82012 (37717 source, 44295 target) ; Learning rate = 0.000308 ; Loss = 1.765911\n",
      "2024-12-15 04:34:12.168000: I runner.py:310] Step = 82500 ; steps/s = 1.63, tokens/s = 81226 (37349 source, 43877 target) ; Learning rate = 0.000308 ; Loss = 1.740127\n",
      "2024-12-15 04:35:13.811000: I runner.py:310] Step = 82600 ; steps/s = 1.62, tokens/s = 81996 (37715 source, 44281 target) ; Learning rate = 0.000308 ; Loss = 1.761843\n",
      "2024-12-15 04:36:15.408000: I runner.py:310] Step = 82700 ; steps/s = 1.62, tokens/s = 82042 (37739 source, 44303 target) ; Learning rate = 0.000307 ; Loss = 1.769565\n",
      "2024-12-15 04:37:17.037000: I runner.py:310] Step = 82800 ; steps/s = 1.62, tokens/s = 82001 (37720 source, 44281 target) ; Learning rate = 0.000307 ; Loss = 1.754877\n",
      "2024-12-15 04:38:18.230000: I runner.py:310] Step = 82900 ; steps/s = 1.63, tokens/s = 81265 (37391 source, 43874 target) ; Learning rate = 0.000307 ; Loss = 1.743479\n",
      "2024-12-15 04:39:19.850000: I runner.py:310] Step = 83000 ; steps/s = 1.62, tokens/s = 82002 (37711 source, 44291 target) ; Learning rate = 0.000307 ; Loss = 1.747233\n",
      "2024-12-15 04:40:21.487000: I runner.py:310] Step = 83100 ; steps/s = 1.62, tokens/s = 81968 (37690 source, 44278 target) ; Learning rate = 0.000307 ; Loss = 1.777855\n",
      "2024-12-15 04:41:22.761000: I runner.py:310] Step = 83200 ; steps/s = 1.63, tokens/s = 81113 (37309 source, 43804 target) ; Learning rate = 0.000306 ; Loss = 1.746299\n",
      "2024-12-15 04:42:24.365000: I runner.py:310] Step = 83300 ; steps/s = 1.62, tokens/s = 82025 (37727 source, 44298 target) ; Learning rate = 0.000306 ; Loss = 1.761692\n",
      "2024-12-15 04:43:26.044000: I runner.py:310] Step = 83400 ; steps/s = 1.62, tokens/s = 81928 (37685 source, 44243 target) ; Learning rate = 0.000306 ; Loss = 1.756461\n",
      "2024-12-15 04:44:27.574000: I runner.py:310] Step = 83500 ; steps/s = 1.63, tokens/s = 81835 (37627 source, 44208 target) ; Learning rate = 0.000306 ; Loss = 1.754296\n",
      "2024-12-15 04:45:28.879000: I runner.py:310] Step = 83600 ; steps/s = 1.63, tokens/s = 81365 (37430 source, 43935 target) ; Learning rate = 0.000306 ; Loss = 1.772336\n",
      "2024-12-15 04:46:30.540000: I runner.py:310] Step = 83700 ; steps/s = 1.62, tokens/s = 81970 (37691 source, 44279 target) ; Learning rate = 0.000306 ; Loss = 1.754469\n",
      "2024-12-15 04:47:32.204000: I runner.py:310] Step = 83800 ; steps/s = 1.62, tokens/s = 81938 (37677 source, 44261 target) ; Learning rate = 0.000305 ; Loss = 1.759457\n",
      "2024-12-15 04:48:33.442000: I runner.py:310] Step = 83900 ; steps/s = 1.63, tokens/s = 81197 (37362 source, 43835 target) ; Learning rate = 0.000305 ; Loss = 1.744574\n",
      "2024-12-15 04:49:35.051000: I runner.py:310] Step = 84000 ; steps/s = 1.62, tokens/s = 82039 (37736 source, 44303 target) ; Learning rate = 0.000305 ; Loss = 1.756515\n",
      "2024-12-15 04:50:36.642000: I runner.py:310] Step = 84100 ; steps/s = 1.62, tokens/s = 82020 (37721 source, 44299 target) ; Learning rate = 0.000305 ; Loss = 1.765452\n",
      "2024-12-15 04:51:37.853000: I runner.py:310] Step = 84200 ; steps/s = 1.63, tokens/s = 81194 (37346 source, 43848 target) ; Learning rate = 0.000305 ; Loss = 1.745143\n",
      "2024-12-15 04:52:39.486000: I runner.py:310] Step = 84300 ; steps/s = 1.62, tokens/s = 81986 (37710 source, 44276 target) ; Learning rate = 0.000304 ; Loss = 1.751788\n",
      "2024-12-15 04:53:41.128000: I runner.py:310] Step = 84400 ; steps/s = 1.62, tokens/s = 81972 (37694 source, 44278 target) ; Learning rate = 0.000304 ; Loss = 1.754525\n",
      "2024-12-15 04:54:42.802000: I runner.py:310] Step = 84500 ; steps/s = 1.62, tokens/s = 81940 (37695 source, 44245 target) ; Learning rate = 0.000304 ; Loss = 1.758854\n",
      "2024-12-15 04:55:44.071000: I runner.py:310] Step = 84600 ; steps/s = 1.63, tokens/s = 81129 (37316 source, 43813 target) ; Learning rate = 0.000304 ; Loss = 1.771147\n",
      "2024-12-15 04:56:45.733000: I runner.py:310] Step = 84700 ; steps/s = 1.62, tokens/s = 81969 (37712 source, 44257 target) ; Learning rate = 0.000304 ; Loss = 1.753862\n",
      "2024-12-15 04:57:47.335000: I runner.py:310] Step = 84800 ; steps/s = 1.62, tokens/s = 82031 (37720 source, 44311 target) ; Learning rate = 0.000304 ; Loss = 1.760547\n",
      "2024-12-15 04:58:48.613000: I runner.py:310] Step = 84900 ; steps/s = 1.63, tokens/s = 81095 (37288 source, 43807 target) ; Learning rate = 0.000303 ; Loss = 1.750119\n",
      "2024-12-15 04:59:50.227000: I runner.py:310] Step = 85000 ; steps/s = 1.62, tokens/s = 82029 (37730 source, 44299 target) ; Learning rate = 0.000303 ; Loss = 1.739587\n",
      "2024-12-15 04:59:50.229000: I training.py:192] Running evaluation for step 85000\n",
      "2024-12-15 05:01:53.361000: I training.py:192] Evaluation result for step 85000: loss = 2.617778 ; perplexity = 13.705241\n",
      "2024-12-15 05:02:54.883000: I runner.py:310] Step = 85100 ; steps/s = 1.63, tokens/s = 82157 (37780 source, 44377 target) ; Learning rate = 0.000303 ; Loss = 1.770750\n",
      "2024-12-15 05:03:56.514000: I runner.py:310] Step = 85200 ; steps/s = 1.62, tokens/s = 81994 (37709 source, 44285 target) ; Learning rate = 0.000303 ; Loss = 1.761612\n",
      "2024-12-15 05:04:57.865000: I runner.py:310] Step = 85300 ; steps/s = 1.63, tokens/s = 81037 (37301 source, 43736 target) ; Learning rate = 0.000303 ; Loss = 1.727512\n",
      "2024-12-15 05:05:59.514000: I runner.py:310] Step = 85400 ; steps/s = 1.62, tokens/s = 81990 (37732 source, 44258 target) ; Learning rate = 0.000302 ; Loss = 1.765858\n",
      "2024-12-15 05:07:01.165000: I runner.py:310] Step = 85500 ; steps/s = 1.62, tokens/s = 81949 (37677 source, 44272 target) ; Learning rate = 0.000302 ; Loss = 1.762800\n",
      "2024-12-15 05:08:02.460000: I runner.py:310] Step = 85600 ; steps/s = 1.63, tokens/s = 81064 (37253 source, 43811 target) ; Learning rate = 0.000302 ; Loss = 1.754215\n",
      "2024-12-15 05:09:04.062000: I runner.py:310] Step = 85700 ; steps/s = 1.62, tokens/s = 82033 (37729 source, 44304 target) ; Learning rate = 0.000302 ; Loss = 1.758028\n",
      "2024-12-15 05:10:05.665000: I runner.py:310] Step = 85800 ; steps/s = 1.62, tokens/s = 82022 (37726 source, 44296 target) ; Learning rate = 0.000302 ; Loss = 1.735368\n",
      "2024-12-15 05:11:07.275000: I runner.py:310] Step = 85900 ; steps/s = 1.62, tokens/s = 82019 (37725 source, 44294 target) ; Learning rate = 0.000302 ; Loss = 1.741879\n",
      "2024-12-15 05:12:08.459000: I runner.py:310] Step = 86000 ; steps/s = 1.63, tokens/s = 81251 (37366 source, 43885 target) ; Learning rate = 0.000301 ; Loss = 1.763476\n",
      "2024-12-15 05:13:10.100000: I runner.py:310] Step = 86100 ; steps/s = 1.62, tokens/s = 82005 (37719 source, 44286 target) ; Learning rate = 0.000301 ; Loss = 1.745225\n",
      "2024-12-15 05:14:11.767000: I runner.py:310] Step = 86200 ; steps/s = 1.62, tokens/s = 81941 (37679 source, 44262 target) ; Learning rate = 0.000301 ; Loss = 1.736097\n",
      "2024-12-15 05:15:13.031000: I runner.py:310] Step = 86300 ; steps/s = 1.63, tokens/s = 81113 (37313 source, 43800 target) ; Learning rate = 0.000301 ; Loss = 1.754360\n",
      "2024-12-15 05:16:14.644000: I runner.py:310] Step = 86400 ; steps/s = 1.62, tokens/s = 82034 (37731 source, 44303 target) ; Learning rate = 0.000301 ; Loss = 1.754789\n",
      "2024-12-15 05:17:16.322000: I runner.py:310] Step = 86500 ; steps/s = 1.62, tokens/s = 81929 (37686 source, 44243 target) ; Learning rate = 0.000301 ; Loss = 1.754896\n",
      "2024-12-15 05:18:17.918000: I runner.py:310] Step = 86600 ; steps/s = 1.62, tokens/s = 82053 (37753 source, 44300 target) ; Learning rate = 0.000300 ; Loss = 1.750416\n",
      "2024-12-15 05:19:19.142000: I runner.py:310] Step = 86700 ; steps/s = 1.63, tokens/s = 81162 (37310 source, 43852 target) ; Learning rate = 0.000300 ; Loss = 1.781268\n",
      "2024-12-15 05:20:20.768000: I runner.py:310] Step = 86800 ; steps/s = 1.62, tokens/s = 81977 (37679 source, 44298 target) ; Learning rate = 0.000300 ; Loss = 1.739819\n",
      "2024-12-15 05:21:22.396000: I runner.py:310] Step = 86900 ; steps/s = 1.62, tokens/s = 82001 (37729 source, 44272 target) ; Learning rate = 0.000300 ; Loss = 1.741098\n",
      "2024-12-15 05:22:23.588000: I runner.py:310] Step = 87000 ; steps/s = 1.63, tokens/s = 81246 (37381 source, 43865 target) ; Learning rate = 0.000300 ; Loss = 1.760846\n",
      "2024-12-15 05:23:25.166000: I runner.py:310] Step = 87100 ; steps/s = 1.62, tokens/s = 82061 (37733 source, 44328 target) ; Learning rate = 0.000299 ; Loss = 1.763053\n",
      "2024-12-15 05:24:26.829000: I runner.py:310] Step = 87200 ; steps/s = 1.62, tokens/s = 81964 (37719 source, 44245 target) ; Learning rate = 0.000299 ; Loss = 1.741815\n",
      "2024-12-15 05:25:28.410000: I runner.py:310] Step = 87300 ; steps/s = 1.62, tokens/s = 82070 (37746 source, 44324 target) ; Learning rate = 0.000299 ; Loss = 1.752856\n",
      "2024-12-15 05:26:29.565000: I runner.py:310] Step = 87400 ; steps/s = 1.64, tokens/s = 81294 (37395 source, 43899 target) ; Learning rate = 0.000299 ; Loss = 1.738097\n",
      "2024-12-15 05:27:31.134000: I runner.py:310] Step = 87500 ; steps/s = 1.62, tokens/s = 82081 (37761 source, 44320 target) ; Learning rate = 0.000299 ; Loss = 1.762083\n",
      "2024-12-15 05:28:32.731000: I runner.py:310] Step = 87600 ; steps/s = 1.62, tokens/s = 82015 (37700 source, 44315 target) ; Learning rate = 0.000299 ; Loss = 1.768965\n",
      "2024-12-15 05:29:34.007000: I runner.py:310] Step = 87700 ; steps/s = 1.63, tokens/s = 81110 (37310 source, 43800 target) ; Learning rate = 0.000298 ; Loss = 1.758106\n",
      "2024-12-15 05:30:35.547000: I runner.py:310] Step = 87800 ; steps/s = 1.63, tokens/s = 82093 (37736 source, 44357 target) ; Learning rate = 0.000298 ; Loss = 1.754399\n",
      "2024-12-15 05:31:37.198000: I runner.py:310] Step = 87900 ; steps/s = 1.62, tokens/s = 81974 (37705 source, 44269 target) ; Learning rate = 0.000298 ; Loss = 1.748575\n",
      "2024-12-15 05:32:38.810000: I runner.py:310] Step = 88000 ; steps/s = 1.62, tokens/s = 82027 (37730 source, 44297 target) ; Learning rate = 0.000298 ; Loss = 1.753258\n",
      "2024-12-15 05:33:40.038000: I runner.py:310] Step = 88100 ; steps/s = 1.63, tokens/s = 81199 (37364 source, 43835 target) ; Learning rate = 0.000298 ; Loss = 1.728780\n",
      "2024-12-15 05:34:41.670000: I runner.py:310] Step = 88200 ; steps/s = 1.62, tokens/s = 82008 (37712 source, 44296 target) ; Learning rate = 0.000298 ; Loss = 1.757476\n",
      "2024-12-15 05:35:43.321000: I runner.py:310] Step = 88300 ; steps/s = 1.62, tokens/s = 81967 (37711 source, 44256 target) ; Learning rate = 0.000297 ; Loss = 1.758566\n",
      "2024-12-15 05:36:44.533000: I runner.py:310] Step = 88400 ; steps/s = 1.63, tokens/s = 81185 (37328 source, 43857 target) ; Learning rate = 0.000297 ; Loss = 1.739554\n",
      "2024-12-15 05:37:46.163000: I runner.py:310] Step = 88500 ; steps/s = 1.62, tokens/s = 82005 (37725 source, 44280 target) ; Learning rate = 0.000297 ; Loss = 1.752232\n",
      "2024-12-15 05:38:47.882000: I runner.py:310] Step = 88600 ; steps/s = 1.62, tokens/s = 81878 (37652 source, 44226 target) ; Learning rate = 0.000297 ; Loss = 1.756978\n",
      "2024-12-15 05:39:49.580000: I runner.py:310] Step = 88700 ; steps/s = 1.62, tokens/s = 81903 (37667 source, 44236 target) ; Learning rate = 0.000297 ; Loss = 1.758498\n",
      "2024-12-15 05:40:50.815000: I runner.py:310] Step = 88800 ; steps/s = 1.63, tokens/s = 81132 (37308 source, 43824 target) ; Learning rate = 0.000297 ; Loss = 1.767100\n",
      "2024-12-15 05:41:52.442000: I runner.py:310] Step = 88900 ; steps/s = 1.62, tokens/s = 82046 (37753 source, 44293 target) ; Learning rate = 0.000296 ; Loss = 1.736909\n",
      "2024-12-15 05:42:54.086000: I runner.py:310] Step = 89000 ; steps/s = 1.62, tokens/s = 81964 (37698 source, 44266 target) ; Learning rate = 0.000296 ; Loss = 1.744218\n",
      "2024-12-15 05:43:55.294000: I runner.py:310] Step = 89100 ; steps/s = 1.63, tokens/s = 81210 (37351 source, 43859 target) ; Learning rate = 0.000296 ; Loss = 1.754068\n",
      "2024-12-15 05:44:57.021000: I runner.py:310] Step = 89200 ; steps/s = 1.62, tokens/s = 81868 (37656 source, 44212 target) ; Learning rate = 0.000296 ; Loss = 1.739989\n",
      "2024-12-15 05:45:58.663000: I runner.py:310] Step = 89300 ; steps/s = 1.62, tokens/s = 81996 (37719 source, 44277 target) ; Learning rate = 0.000296 ; Loss = 1.749494\n",
      "2024-12-15 05:47:00.316000: I runner.py:310] Step = 89400 ; steps/s = 1.62, tokens/s = 81945 (37673 source, 44272 target) ; Learning rate = 0.000296 ; Loss = 1.753120\n",
      "2024-12-15 05:48:01.594000: I runner.py:310] Step = 89500 ; steps/s = 1.63, tokens/s = 81140 (37339 source, 43801 target) ; Learning rate = 0.000295 ; Loss = 1.756663\n",
      "2024-12-15 05:49:03.284000: I runner.py:310] Step = 89600 ; steps/s = 1.62, tokens/s = 81902 (37647 source, 44255 target) ; Learning rate = 0.000295 ; Loss = 1.738258\n",
      "2024-12-15 05:50:04.965000: I runner.py:310] Step = 89700 ; steps/s = 1.62, tokens/s = 81936 (37702 source, 44234 target) ; Learning rate = 0.000295 ; Loss = 1.740378\n",
      "2024-12-15 05:51:06.177000: I runner.py:310] Step = 89800 ; steps/s = 1.63, tokens/s = 81190 (37340 source, 43850 target) ; Learning rate = 0.000295 ; Loss = 1.761667\n",
      "2024-12-15 05:52:07.820000: I runner.py:310] Step = 89900 ; steps/s = 1.62, tokens/s = 81978 (37706 source, 44272 target) ; Learning rate = 0.000295 ; Loss = 1.731777\n",
      "2024-12-15 05:53:09.398000: I runner.py:310] Step = 90000 ; steps/s = 1.62, tokens/s = 82064 (37745 source, 44319 target) ; Learning rate = 0.000295 ; Loss = 1.738329\n",
      "2024-12-15 05:53:11.454000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-90000\n",
      "2024-12-15 05:53:11.454000: I training.py:192] Running evaluation for step 90000\n",
      "2024-12-15 05:55:15.725000: I training.py:192] Evaluation result for step 90000: loss = 2.615416 ; perplexity = 13.672896\n",
      "2024-12-15 05:56:17.182000: I runner.py:310] Step = 90100 ; steps/s = 1.63, tokens/s = 82258 (37826 source, 44432 target) ; Learning rate = 0.000294 ; Loss = 1.754496\n",
      "2024-12-15 05:57:18.391000: I runner.py:310] Step = 90200 ; steps/s = 1.63, tokens/s = 81212 (37357 source, 43855 target) ; Learning rate = 0.000294 ; Loss = 1.724964\n",
      "2024-12-15 05:58:19.966000: I runner.py:310] Step = 90300 ; steps/s = 1.62, tokens/s = 82062 (37734 source, 44328 target) ; Learning rate = 0.000294 ; Loss = 1.753848\n",
      "2024-12-15 05:59:21.542000: I runner.py:310] Step = 90400 ; steps/s = 1.62, tokens/s = 82060 (37739 source, 44321 target) ; Learning rate = 0.000294 ; Loss = 1.759062\n",
      "2024-12-15 06:00:22.764000: I runner.py:310] Step = 90500 ; steps/s = 1.63, tokens/s = 81194 (37340 source, 43854 target) ; Learning rate = 0.000294 ; Loss = 1.764435\n",
      "2024-12-15 06:01:24.379000: I runner.py:310] Step = 90600 ; steps/s = 1.62, tokens/s = 82028 (37743 source, 44285 target) ; Learning rate = 0.000294 ; Loss = 1.740566\n",
      "2024-12-15 06:02:25.996000: I runner.py:310] Step = 90700 ; steps/s = 1.62, tokens/s = 82029 (37744 source, 44285 target) ; Learning rate = 0.000293 ; Loss = 1.734184\n",
      "2024-12-15 06:03:27.647000: I runner.py:310] Step = 90800 ; steps/s = 1.62, tokens/s = 81954 (37671 source, 44283 target) ; Learning rate = 0.000293 ; Loss = 1.752341\n",
      "2024-12-15 06:04:28.866000: I runner.py:310] Step = 90900 ; steps/s = 1.63, tokens/s = 81237 (37385 source, 43852 target) ; Learning rate = 0.000293 ; Loss = 1.734064\n",
      "2024-12-15 06:05:30.537000: I runner.py:310] Step = 91000 ; steps/s = 1.62, tokens/s = 81884 (37643 source, 44241 target) ; Learning rate = 0.000293 ; Loss = 1.754152\n",
      "2024-12-15 06:06:32.165000: I runner.py:310] Step = 91100 ; steps/s = 1.62, tokens/s = 82031 (37737 source, 44294 target) ; Learning rate = 0.000293 ; Loss = 1.758973\n",
      "2024-12-15 06:07:33.357000: I runner.py:310] Step = 91200 ; steps/s = 1.63, tokens/s = 81207 (37351 source, 43856 target) ; Learning rate = 0.000293 ; Loss = 1.713828\n",
      "2024-12-15 06:08:35.023000: I runner.py:310] Step = 91300 ; steps/s = 1.62, tokens/s = 81944 (37663 source, 44281 target) ; Learning rate = 0.000293 ; Loss = 1.756981\n",
      "2024-12-15 06:09:36.617000: I runner.py:310] Step = 91400 ; steps/s = 1.62, tokens/s = 82069 (37777 source, 44292 target) ; Learning rate = 0.000292 ; Loss = 1.757741\n",
      "2024-12-15 06:10:38.292000: I runner.py:310] Step = 91500 ; steps/s = 1.62, tokens/s = 81921 (37658 source, 44263 target) ; Learning rate = 0.000292 ; Loss = 1.760676\n",
      "2024-12-15 06:11:39.514000: I runner.py:310] Step = 91600 ; steps/s = 1.63, tokens/s = 81192 (37361 source, 43831 target) ; Learning rate = 0.000292 ; Loss = 1.764250\n",
      "2024-12-15 06:12:41.173000: I runner.py:310] Step = 91700 ; steps/s = 1.62, tokens/s = 81976 (37718 source, 44258 target) ; Learning rate = 0.000292 ; Loss = 1.732867\n",
      "2024-12-15 06:13:42.802000: I runner.py:310] Step = 91800 ; steps/s = 1.62, tokens/s = 81984 (37679 source, 44305 target) ; Learning rate = 0.000292 ; Loss = 1.737396\n",
      "2024-12-15 06:14:44.043000: I runner.py:310] Step = 91900 ; steps/s = 1.63, tokens/s = 81165 (37335 source, 43830 target) ; Learning rate = 0.000292 ; Loss = 1.761527\n",
      "2024-12-15 06:15:45.698000: I runner.py:310] Step = 92000 ; steps/s = 1.62, tokens/s = 81958 (37686 source, 44272 target) ; Learning rate = 0.000291 ; Loss = 1.737637\n",
      "2024-12-15 06:16:47.333000: I runner.py:310] Step = 92100 ; steps/s = 1.62, tokens/s = 81978 (37705 source, 44273 target) ; Learning rate = 0.000291 ; Loss = 1.741947\n",
      "2024-12-15 06:17:48.709000: I runner.py:310] Step = 92200 ; steps/s = 1.63, tokens/s = 81385 (37436 source, 43949 target) ; Learning rate = 0.000291 ; Loss = 1.777481\n",
      "2024-12-15 06:18:50.288000: I runner.py:310] Step = 92300 ; steps/s = 1.62, tokens/s = 81676 (37567 source, 44109 target) ; Learning rate = 0.000291 ; Loss = 1.757573\n",
      "2024-12-15 06:19:51.941000: I runner.py:310] Step = 92400 ; steps/s = 1.62, tokens/s = 81973 (37720 source, 44253 target) ; Learning rate = 0.000291 ; Loss = 1.743843\n",
      "2024-12-15 06:20:53.566000: I runner.py:310] Step = 92500 ; steps/s = 1.62, tokens/s = 82009 (37723 source, 44286 target) ; Learning rate = 0.000291 ; Loss = 1.734408\n",
      "2024-12-15 06:21:54.801000: I runner.py:310] Step = 92600 ; steps/s = 1.63, tokens/s = 81120 (37279 source, 43841 target) ; Learning rate = 0.000290 ; Loss = 1.757422\n",
      "2024-12-15 06:22:56.383000: I runner.py:310] Step = 92700 ; steps/s = 1.62, tokens/s = 82083 (37755 source, 44328 target) ; Learning rate = 0.000290 ; Loss = 1.739935\n",
      "2024-12-15 06:23:58.023000: I runner.py:310] Step = 92800 ; steps/s = 1.62, tokens/s = 81963 (37697 source, 44266 target) ; Learning rate = 0.000290 ; Loss = 1.736184\n",
      "2024-12-15 06:24:59.296000: I runner.py:310] Step = 92900 ; steps/s = 1.63, tokens/s = 81144 (37327 source, 43817 target) ; Learning rate = 0.000290 ; Loss = 1.738228\n",
      "2024-12-15 06:26:00.895000: I runner.py:310] Step = 93000 ; steps/s = 1.62, tokens/s = 82049 (37739 source, 44310 target) ; Learning rate = 0.000290 ; Loss = 1.750523\n",
      "2024-12-15 06:27:02.534000: I runner.py:310] Step = 93100 ; steps/s = 1.62, tokens/s = 81962 (37677 source, 44285 target) ; Learning rate = 0.000290 ; Loss = 1.755107\n",
      "2024-12-15 06:28:04.161000: I runner.py:310] Step = 93200 ; steps/s = 1.62, tokens/s = 82018 (37737 source, 44281 target) ; Learning rate = 0.000290 ; Loss = 1.763305\n",
      "2024-12-15 06:29:05.340000: I runner.py:310] Step = 93300 ; steps/s = 1.63, tokens/s = 81225 (37362 source, 43863 target) ; Learning rate = 0.000289 ; Loss = 1.719653\n",
      "2024-12-15 06:30:06.935000: I runner.py:310] Step = 93400 ; steps/s = 1.62, tokens/s = 82051 (37737 source, 44314 target) ; Learning rate = 0.000289 ; Loss = 1.751937\n",
      "2024-12-15 06:31:08.629000: I runner.py:310] Step = 93500 ; steps/s = 1.62, tokens/s = 81919 (37670 source, 44249 target) ; Learning rate = 0.000289 ; Loss = 1.762052\n",
      "2024-12-15 06:32:09.869000: I runner.py:310] Step = 93600 ; steps/s = 1.63, tokens/s = 81150 (37338 source, 43812 target) ; Learning rate = 0.000289 ; Loss = 1.740333\n",
      "2024-12-15 06:33:11.646000: I runner.py:310] Step = 93700 ; steps/s = 1.62, tokens/s = 81817 (37631 source, 44186 target) ; Learning rate = 0.000289 ; Loss = 1.724249\n",
      "2024-12-15 06:34:13.307000: I runner.py:310] Step = 93800 ; steps/s = 1.62, tokens/s = 81957 (37693 source, 44264 target) ; Learning rate = 0.000289 ; Loss = 1.749882\n",
      "2024-12-15 06:35:14.992000: I runner.py:310] Step = 93900 ; steps/s = 1.62, tokens/s = 81931 (37688 source, 44243 target) ; Learning rate = 0.000288 ; Loss = 1.763526\n",
      "2024-12-15 06:36:16.255000: I runner.py:310] Step = 94000 ; steps/s = 1.63, tokens/s = 81113 (37309 source, 43804 target) ; Learning rate = 0.000288 ; Loss = 1.759044\n",
      "2024-12-15 06:37:17.915000: I runner.py:310] Step = 94100 ; steps/s = 1.62, tokens/s = 81955 (37689 source, 44266 target) ; Learning rate = 0.000288 ; Loss = 1.737081\n",
      "2024-12-15 06:38:19.550000: I runner.py:310] Step = 94200 ; steps/s = 1.62, tokens/s = 81997 (37714 source, 44283 target) ; Learning rate = 0.000288 ; Loss = 1.738342\n",
      "2024-12-15 06:39:20.776000: I runner.py:310] Step = 94300 ; steps/s = 1.63, tokens/s = 81184 (37341 source, 43843 target) ; Learning rate = 0.000288 ; Loss = 1.738870\n",
      "2024-12-15 06:40:22.471000: I runner.py:310] Step = 94400 ; steps/s = 1.62, tokens/s = 81904 (37658 source, 44246 target) ; Learning rate = 0.000288 ; Loss = 1.749520\n",
      "2024-12-15 06:41:24.182000: I runner.py:310] Step = 94500 ; steps/s = 1.62, tokens/s = 81873 (37655 source, 44218 target) ; Learning rate = 0.000288 ; Loss = 1.734718\n",
      "2024-12-15 06:42:25.848000: I runner.py:310] Step = 94600 ; steps/s = 1.62, tokens/s = 81963 (37705 source, 44258 target) ; Learning rate = 0.000287 ; Loss = 1.740512\n",
      "2024-12-15 06:43:27.039000: I runner.py:310] Step = 94700 ; steps/s = 1.63, tokens/s = 81246 (37380 source, 43866 target) ; Learning rate = 0.000287 ; Loss = 1.714308\n",
      "2024-12-15 06:44:28.609000: I runner.py:310] Step = 94800 ; steps/s = 1.62, tokens/s = 82086 (37751 source, 44335 target) ; Learning rate = 0.000287 ; Loss = 1.727912\n",
      "2024-12-15 06:45:30.241000: I runner.py:310] Step = 94900 ; steps/s = 1.62, tokens/s = 81983 (37701 source, 44282 target) ; Learning rate = 0.000287 ; Loss = 1.754121\n",
      "2024-12-15 06:46:31.486000: I runner.py:310] Step = 95000 ; steps/s = 1.63, tokens/s = 81138 (37311 source, 43827 target) ; Learning rate = 0.000287 ; Loss = 1.728584\n",
      "2024-12-15 06:46:31.487000: I training.py:192] Running evaluation for step 95000\n",
      "2024-12-15 06:48:37.003000: I training.py:192] Evaluation result for step 95000: loss = 2.620126 ; perplexity = 13.737452\n",
      "2024-12-15 06:49:38.478000: I runner.py:310] Step = 95100 ; steps/s = 1.63, tokens/s = 82209 (37795 source, 44414 target) ; Learning rate = 0.000287 ; Loss = 1.733863\n",
      "2024-12-15 06:50:40.088000: I runner.py:310] Step = 95200 ; steps/s = 1.62, tokens/s = 82050 (37759 source, 44291 target) ; Learning rate = 0.000286 ; Loss = 1.755529\n",
      "2024-12-15 06:51:41.706000: I runner.py:310] Step = 95300 ; steps/s = 1.62, tokens/s = 82019 (37733 source, 44286 target) ; Learning rate = 0.000286 ; Loss = 1.758062\n",
      "2024-12-15 06:52:42.973000: I runner.py:310] Step = 95400 ; steps/s = 1.63, tokens/s = 81117 (37310 source, 43807 target) ; Learning rate = 0.000286 ; Loss = 1.756798\n",
      "2024-12-15 06:53:44.717000: I runner.py:310] Step = 95500 ; steps/s = 1.62, tokens/s = 81869 (37663 source, 44206 target) ; Learning rate = 0.000286 ; Loss = 1.734257\n",
      "2024-12-15 06:54:46.381000: I runner.py:310] Step = 95600 ; steps/s = 1.62, tokens/s = 81930 (37652 source, 44278 target) ; Learning rate = 0.000286 ; Loss = 1.746836\n",
      "2024-12-15 06:55:47.617000: I runner.py:310] Step = 95700 ; steps/s = 1.63, tokens/s = 81167 (37348 source, 43819 target) ; Learning rate = 0.000286 ; Loss = 1.741325\n",
      "2024-12-15 06:56:49.315000: I runner.py:310] Step = 95800 ; steps/s = 1.62, tokens/s = 81896 (37657 source, 44239 target) ; Learning rate = 0.000286 ; Loss = 1.729116\n",
      "2024-12-15 06:57:50.992000: I runner.py:310] Step = 95900 ; steps/s = 1.62, tokens/s = 81943 (37692 source, 44251 target) ; Learning rate = 0.000285 ; Loss = 1.749874\n",
      "2024-12-15 06:58:52.733000: I runner.py:310] Step = 96000 ; steps/s = 1.62, tokens/s = 81850 (37652 source, 44198 target) ; Learning rate = 0.000285 ; Loss = 1.761990\n",
      "2024-12-15 06:59:54.041000: I runner.py:310] Step = 96100 ; steps/s = 1.63, tokens/s = 81086 (37307 source, 43779 target) ; Learning rate = 0.000285 ; Loss = 1.724559\n",
      "2024-12-15 07:00:55.723000: I runner.py:310] Step = 96200 ; steps/s = 1.62, tokens/s = 81920 (37676 source, 44244 target) ; Learning rate = 0.000285 ; Loss = 1.738523\n",
      "2024-12-15 07:01:57.370000: I runner.py:310] Step = 96300 ; steps/s = 1.62, tokens/s = 81969 (37687 source, 44282 target) ; Learning rate = 0.000285 ; Loss = 1.752301\n",
      "2024-12-15 07:02:58.677000: I runner.py:310] Step = 96400 ; steps/s = 1.63, tokens/s = 81050 (37271 source, 43779 target) ; Learning rate = 0.000285 ; Loss = 1.722130\n",
      "2024-12-15 07:04:00.279000: I runner.py:310] Step = 96500 ; steps/s = 1.62, tokens/s = 82051 (37765 source, 44286 target) ; Learning rate = 0.000285 ; Loss = 1.736798\n",
      "2024-12-15 07:05:01.888000: I runner.py:310] Step = 96600 ; steps/s = 1.62, tokens/s = 82041 (37725 source, 44316 target) ; Learning rate = 0.000284 ; Loss = 1.748487\n",
      "2024-12-15 07:06:03.475000: I runner.py:310] Step = 96700 ; steps/s = 1.62, tokens/s = 82047 (37724 source, 44323 target) ; Learning rate = 0.000284 ; Loss = 1.755938\n",
      "2024-12-15 07:07:04.666000: I runner.py:310] Step = 96800 ; steps/s = 1.63, tokens/s = 81222 (37357 source, 43865 target) ; Learning rate = 0.000284 ; Loss = 1.723124\n",
      "2024-12-15 07:08:06.360000: I runner.py:310] Step = 96900 ; steps/s = 1.62, tokens/s = 81940 (37703 source, 44237 target) ; Learning rate = 0.000284 ; Loss = 1.745199\n",
      "2024-12-15 07:09:08.014000: I runner.py:310] Step = 97000 ; steps/s = 1.62, tokens/s = 81959 (37686 source, 44273 target) ; Learning rate = 0.000284 ; Loss = 1.745696\n",
      "2024-12-15 07:10:09.297000: I runner.py:310] Step = 97100 ; steps/s = 1.63, tokens/s = 81061 (37269 source, 43792 target) ; Learning rate = 0.000284 ; Loss = 1.750269\n",
      "2024-12-15 07:11:11.000000: I runner.py:310] Step = 97200 ; steps/s = 1.62, tokens/s = 81869 (37648 source, 44221 target) ; Learning rate = 0.000284 ; Loss = 1.733810\n",
      "2024-12-15 07:12:12.590000: I runner.py:310] Step = 97300 ; steps/s = 1.62, tokens/s = 82060 (37745 source, 44315 target) ; Learning rate = 0.000283 ; Loss = 1.727525\n",
      "2024-12-15 07:13:14.222000: I runner.py:310] Step = 97400 ; steps/s = 1.62, tokens/s = 82030 (37748 source, 44282 target) ; Learning rate = 0.000283 ; Loss = 1.736942\n",
      "2024-12-15 07:14:15.323000: I runner.py:310] Step = 97500 ; steps/s = 1.64, tokens/s = 81342 (37390 source, 43952 target) ; Learning rate = 0.000283 ; Loss = 1.752124\n",
      "2024-12-15 07:15:16.973000: I runner.py:310] Step = 97600 ; steps/s = 1.62, tokens/s = 81969 (37691 source, 44278 target) ; Learning rate = 0.000283 ; Loss = 1.733589\n",
      "2024-12-15 07:16:18.572000: I runner.py:310] Step = 97700 ; steps/s = 1.62, tokens/s = 82042 (37740 source, 44302 target) ; Learning rate = 0.000283 ; Loss = 1.729115\n",
      "2024-12-15 07:17:19.794000: I runner.py:310] Step = 97800 ; steps/s = 1.63, tokens/s = 81182 (37361 source, 43821 target) ; Learning rate = 0.000283 ; Loss = 1.747204\n",
      "2024-12-15 07:18:21.485000: I runner.py:310] Step = 97900 ; steps/s = 1.62, tokens/s = 81910 (37664 source, 44246 target) ; Learning rate = 0.000282 ; Loss = 1.734770\n",
      "2024-12-15 07:19:23.120000: I runner.py:310] Step = 98000 ; steps/s = 1.62, tokens/s = 81986 (37693 source, 44293 target) ; Learning rate = 0.000282 ; Loss = 1.729616\n",
      "2024-12-15 07:20:24.751000: I runner.py:310] Step = 98100 ; steps/s = 1.62, tokens/s = 82026 (37743 source, 44283 target) ; Learning rate = 0.000282 ; Loss = 1.734434\n",
      "2024-12-15 07:21:25.946000: I runner.py:310] Step = 98200 ; steps/s = 1.63, tokens/s = 81218 (37350 source, 43868 target) ; Learning rate = 0.000282 ; Loss = 1.746396\n",
      "2024-12-15 07:22:27.628000: I runner.py:310] Step = 98300 ; steps/s = 1.62, tokens/s = 81933 (37703 source, 44230 target) ; Learning rate = 0.000282 ; Loss = 1.730458\n",
      "2024-12-15 07:23:29.252000: I runner.py:310] Step = 98400 ; steps/s = 1.62, tokens/s = 81987 (37701 source, 44286 target) ; Learning rate = 0.000282 ; Loss = 1.733138\n",
      "2024-12-15 07:24:30.461000: I runner.py:310] Step = 98500 ; steps/s = 1.63, tokens/s = 81210 (37352 source, 43858 target) ; Learning rate = 0.000282 ; Loss = 1.711428\n",
      "2024-12-15 07:25:32.152000: I runner.py:310] Step = 98600 ; steps/s = 1.62, tokens/s = 81930 (37692 source, 44238 target) ; Learning rate = 0.000281 ; Loss = 1.742743\n",
      "2024-12-15 07:26:33.815000: I runner.py:310] Step = 98700 ; steps/s = 1.62, tokens/s = 81938 (37672 source, 44266 target) ; Learning rate = 0.000281 ; Loss = 1.746504\n",
      "2024-12-15 07:27:35.520000: I runner.py:310] Step = 98800 ; steps/s = 1.62, tokens/s = 81911 (37676 source, 44235 target) ; Learning rate = 0.000281 ; Loss = 1.753888\n",
      "2024-12-15 07:28:36.773000: I runner.py:310] Step = 98900 ; steps/s = 1.63, tokens/s = 81130 (37313 source, 43817 target) ; Learning rate = 0.000281 ; Loss = 1.757962\n",
      "2024-12-15 07:29:38.403000: I runner.py:310] Step = 99000 ; steps/s = 1.62, tokens/s = 82022 (37742 source, 44280 target) ; Learning rate = 0.000281 ; Loss = 1.744750\n",
      "2024-12-15 07:30:40.077000: I runner.py:310] Step = 99100 ; steps/s = 1.62, tokens/s = 81914 (37655 source, 44259 target) ; Learning rate = 0.000281 ; Loss = 1.739204\n",
      "2024-12-15 07:31:41.263000: I runner.py:310] Step = 99200 ; steps/s = 1.63, tokens/s = 81255 (37378 source, 43877 target) ; Learning rate = 0.000281 ; Loss = 1.715811\n",
      "2024-12-15 07:32:42.890000: I runner.py:310] Step = 99300 ; steps/s = 1.62, tokens/s = 82015 (37733 source, 44282 target) ; Learning rate = 0.000280 ; Loss = 1.735377\n",
      "2024-12-15 07:33:44.520000: I runner.py:310] Step = 99400 ; steps/s = 1.62, tokens/s = 81975 (37684 source, 44291 target) ; Learning rate = 0.000280 ; Loss = 1.748705\n",
      "2024-12-15 07:34:46.166000: I runner.py:310] Step = 99500 ; steps/s = 1.62, tokens/s = 81980 (37709 source, 44271 target) ; Learning rate = 0.000280 ; Loss = 1.739929\n",
      "2024-12-15 07:35:47.404000: I runner.py:310] Step = 99600 ; steps/s = 1.63, tokens/s = 81177 (37347 source, 43830 target) ; Learning rate = 0.000280 ; Loss = 1.720210\n",
      "2024-12-15 07:36:49.017000: I runner.py:310] Step = 99700 ; steps/s = 1.62, tokens/s = 81993 (37701 source, 44292 target) ; Learning rate = 0.000280 ; Loss = 1.755820\n",
      "2024-12-15 07:37:50.647000: I runner.py:310] Step = 99800 ; steps/s = 1.62, tokens/s = 82015 (37728 source, 44287 target) ; Learning rate = 0.000280 ; Loss = 1.749115\n",
      "2024-12-15 07:38:51.894000: I runner.py:310] Step = 99900 ; steps/s = 1.63, tokens/s = 81156 (37329 source, 43827 target) ; Learning rate = 0.000280 ; Loss = 1.725431\n",
      "2024-12-15 07:39:53.521000: I runner.py:310] Step = 100000 ; steps/s = 1.62, tokens/s = 82011 (37723 source, 44288 target) ; Learning rate = 0.000280 ; Loss = 1.738667\n",
      "2024-12-15 07:39:55.224000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-100000\n",
      "2024-12-15 07:39:55.224000: I training.py:192] Running evaluation for step 100000\n",
      "2024-12-15 07:41:59.679000: I training.py:192] Evaluation result for step 100000: loss = 2.631348 ; perplexity = 13.892487\n",
      "2024-12-15 07:43:01.174000: I runner.py:310] Step = 100100 ; steps/s = 1.63, tokens/s = 82175 (37779 source, 44396 target) ; Learning rate = 0.000279 ; Loss = 1.773502\n",
      "2024-12-15 07:44:02.777000: I runner.py:310] Step = 100200 ; steps/s = 1.62, tokens/s = 82024 (37728 source, 44296 target) ; Learning rate = 0.000279 ; Loss = 1.753750\n",
      "2024-12-15 07:45:03.973000: I runner.py:310] Step = 100300 ; steps/s = 1.63, tokens/s = 81212 (37337 source, 43875 target) ; Learning rate = 0.000279 ; Loss = 1.741039\n",
      "2024-12-15 07:46:05.631000: I runner.py:310] Step = 100400 ; steps/s = 1.62, tokens/s = 81948 (37688 source, 44260 target) ; Learning rate = 0.000279 ; Loss = 1.720689\n",
      "2024-12-15 07:47:07.346000: I runner.py:310] Step = 100500 ; steps/s = 1.62, tokens/s = 81888 (37676 source, 44212 target) ; Learning rate = 0.000279 ; Loss = 1.728373\n",
      "2024-12-15 07:48:08.513000: I runner.py:310] Step = 100600 ; steps/s = 1.64, tokens/s = 81249 (37366 source, 43883 target) ; Learning rate = 0.000279 ; Loss = 1.747023\n",
      "2024-12-15 07:49:10.208000: I runner.py:310] Step = 100700 ; steps/s = 1.62, tokens/s = 81902 (37666 source, 44236 target) ; Learning rate = 0.000279 ; Loss = 1.722975\n",
      "2024-12-15 07:50:11.883000: I runner.py:310] Step = 100800 ; steps/s = 1.62, tokens/s = 81947 (37701 source, 44246 target) ; Learning rate = 0.000278 ; Loss = 1.732914\n",
      "2024-12-15 07:51:13.165000: I runner.py:310] Step = 100900 ; steps/s = 1.63, tokens/s = 81135 (37320 source, 43815 target) ; Learning rate = 0.000278 ; Loss = 1.753013\n",
      "2024-12-15 07:52:14.754000: I runner.py:310] Step = 101000 ; steps/s = 1.62, tokens/s = 82040 (37729 source, 44311 target) ; Learning rate = 0.000278 ; Loss = 1.732828\n",
      "2024-12-15 07:53:16.380000: I runner.py:310] Step = 101100 ; steps/s = 1.62, tokens/s = 82006 (37715 source, 44291 target) ; Learning rate = 0.000278 ; Loss = 1.744958\n",
      "2024-12-15 07:54:18.095000: I runner.py:310] Step = 101200 ; steps/s = 1.62, tokens/s = 81883 (37658 source, 44225 target) ; Learning rate = 0.000278 ; Loss = 1.760111\n",
      "2024-12-15 07:55:19.392000: I runner.py:310] Step = 101300 ; steps/s = 1.63, tokens/s = 81067 (37295 source, 43772 target) ; Learning rate = 0.000278 ; Loss = 1.750393\n",
      "2024-12-15 07:56:21.079000: I runner.py:310] Step = 101400 ; steps/s = 1.62, tokens/s = 81940 (37686 source, 44254 target) ; Learning rate = 0.000278 ; Loss = 1.723886\n",
      "2024-12-15 07:57:22.720000: I runner.py:310] Step = 101500 ; steps/s = 1.62, tokens/s = 81989 (37711 source, 44278 target) ; Learning rate = 0.000277 ; Loss = 1.729400\n",
      "2024-12-15 07:58:23.952000: I runner.py:310] Step = 101600 ; steps/s = 1.63, tokens/s = 81167 (37336 source, 43831 target) ; Learning rate = 0.000277 ; Loss = 1.738121\n",
      "2024-12-15 07:59:25.556000: I runner.py:310] Step = 101700 ; steps/s = 1.62, tokens/s = 82023 (37734 source, 44289 target) ; Learning rate = 0.000277 ; Loss = 1.740522\n",
      "2024-12-15 08:00:27.186000: I runner.py:310] Step = 101800 ; steps/s = 1.62, tokens/s = 82010 (37719 source, 44291 target) ; Learning rate = 0.000277 ; Loss = 1.716268\n",
      "2024-12-15 08:01:28.822000: I runner.py:310] Step = 101900 ; steps/s = 1.62, tokens/s = 81976 (37689 source, 44287 target) ; Learning rate = 0.000277 ; Loss = 1.718310\n",
      "2024-12-15 08:02:30.080000: I runner.py:310] Step = 102000 ; steps/s = 1.63, tokens/s = 81142 (37314 source, 43828 target) ; Learning rate = 0.000277 ; Loss = 1.717874\n",
      "2024-12-15 08:03:31.749000: I runner.py:310] Step = 102100 ; steps/s = 1.62, tokens/s = 81947 (37685 source, 44262 target) ; Learning rate = 0.000277 ; Loss = 1.742592\n",
      "2024-12-15 08:04:33.435000: I runner.py:310] Step = 102200 ; steps/s = 1.62, tokens/s = 81917 (37680 source, 44237 target) ; Learning rate = 0.000276 ; Loss = 1.745596\n",
      "2024-12-15 08:05:34.721000: I runner.py:310] Step = 102300 ; steps/s = 1.63, tokens/s = 81105 (37313 source, 43792 target) ; Learning rate = 0.000276 ; Loss = 1.718918\n",
      "2024-12-15 08:06:36.307000: I runner.py:310] Step = 102400 ; steps/s = 1.62, tokens/s = 82080 (37751 source, 44329 target) ; Learning rate = 0.000276 ; Loss = 1.738179\n",
      "2024-12-15 08:07:37.972000: I runner.py:310] Step = 102500 ; steps/s = 1.62, tokens/s = 81949 (37678 source, 44271 target) ; Learning rate = 0.000276 ; Loss = 1.743896\n",
      "2024-12-15 08:08:39.593000: I runner.py:310] Step = 102600 ; steps/s = 1.62, tokens/s = 81989 (37718 source, 44271 target) ; Learning rate = 0.000276 ; Loss = 1.744233\n",
      "2024-12-15 08:09:40.893000: I runner.py:310] Step = 102700 ; steps/s = 1.63, tokens/s = 81062 (37285 source, 43777 target) ; Learning rate = 0.000276 ; Loss = 1.744958\n",
      "2024-12-15 08:10:42.535000: I runner.py:310] Step = 102800 ; steps/s = 1.62, tokens/s = 81981 (37701 source, 44280 target) ; Learning rate = 0.000276 ; Loss = 1.732032\n",
      "2024-12-15 08:11:44.246000: I runner.py:310] Step = 102900 ; steps/s = 1.62, tokens/s = 81917 (37691 source, 44226 target) ; Learning rate = 0.000276 ; Loss = 1.733597\n",
      "2024-12-15 08:12:45.592000: I runner.py:310] Step = 103000 ; steps/s = 1.63, tokens/s = 81010 (37250 source, 43760 target) ; Learning rate = 0.000275 ; Loss = 1.733498\n",
      "2024-12-15 08:13:47.245000: I runner.py:310] Step = 103100 ; steps/s = 1.62, tokens/s = 81946 (37685 source, 44261 target) ; Learning rate = 0.000275 ; Loss = 1.741729\n",
      "2024-12-15 08:14:48.942000: I runner.py:310] Step = 103200 ; steps/s = 1.62, tokens/s = 81916 (37673 source, 44243 target) ; Learning rate = 0.000275 ; Loss = 1.735487\n",
      "2024-12-15 08:15:50.598000: I runner.py:310] Step = 103300 ; steps/s = 1.62, tokens/s = 81971 (37705 source, 44266 target) ; Learning rate = 0.000275 ; Loss = 1.729554\n",
      "2024-12-15 08:16:51.831000: I runner.py:310] Step = 103400 ; steps/s = 1.63, tokens/s = 81201 (37350 source, 43851 target) ; Learning rate = 0.000275 ; Loss = 1.704294\n",
      "2024-12-15 08:17:53.531000: I runner.py:310] Step = 103500 ; steps/s = 1.62, tokens/s = 81877 (37655 source, 44222 target) ; Learning rate = 0.000275 ; Loss = 1.735485\n",
      "2024-12-15 08:18:55.097000: I runner.py:310] Step = 103600 ; steps/s = 1.62, tokens/s = 82094 (37758 source, 44336 target) ; Learning rate = 0.000275 ; Loss = 1.749730\n",
      "2024-12-15 08:19:56.378000: I runner.py:310] Step = 103700 ; steps/s = 1.63, tokens/s = 81105 (37305 source, 43800 target) ; Learning rate = 0.000274 ; Loss = 1.727912\n",
      "2024-12-15 08:20:58.066000: I runner.py:310] Step = 103800 ; steps/s = 1.62, tokens/s = 81910 (37673 source, 44237 target) ; Learning rate = 0.000274 ; Loss = 1.733191\n",
      "2024-12-15 08:21:59.714000: I runner.py:310] Step = 103900 ; steps/s = 1.62, tokens/s = 81969 (37706 source, 44263 target) ; Learning rate = 0.000274 ; Loss = 1.724219\n",
      "2024-12-15 08:23:01.373000: I runner.py:310] Step = 104000 ; steps/s = 1.62, tokens/s = 81977 (37709 source, 44268 target) ; Learning rate = 0.000274 ; Loss = 1.724458\n",
      "2024-12-15 08:24:02.660000: I runner.py:310] Step = 104100 ; steps/s = 1.63, tokens/s = 81095 (37283 source, 43812 target) ; Learning rate = 0.000274 ; Loss = 1.714289\n",
      "2024-12-15 08:25:04.356000: I runner.py:310] Step = 104200 ; steps/s = 1.62, tokens/s = 81928 (37691 source, 44237 target) ; Learning rate = 0.000274 ; Loss = 1.730985\n",
      "2024-12-15 08:26:06.019000: I runner.py:310] Step = 104300 ; steps/s = 1.62, tokens/s = 81950 (37683 source, 44267 target) ; Learning rate = 0.000274 ; Loss = 1.745319\n",
      "2024-12-15 08:27:07.251000: I runner.py:310] Step = 104400 ; steps/s = 1.63, tokens/s = 81166 (37335 source, 43831 target) ; Learning rate = 0.000274 ; Loss = 1.738622\n",
      "2024-12-15 08:28:08.919000: I runner.py:310] Step = 104500 ; steps/s = 1.62, tokens/s = 81950 (37694 source, 44256 target) ; Learning rate = 0.000273 ; Loss = 1.720021\n",
      "2024-12-15 08:29:10.567000: I runner.py:310] Step = 104600 ; steps/s = 1.62, tokens/s = 81977 (37710 source, 44267 target) ; Learning rate = 0.000273 ; Loss = 1.721562\n",
      "2024-12-15 08:30:12.237000: I runner.py:310] Step = 104700 ; steps/s = 1.62, tokens/s = 81946 (37700 source, 44246 target) ; Learning rate = 0.000273 ; Loss = 1.731103\n",
      "2024-12-15 08:31:13.478000: I runner.py:310] Step = 104800 ; steps/s = 1.63, tokens/s = 81127 (37296 source, 43831 target) ; Learning rate = 0.000273 ; Loss = 1.755797\n",
      "2024-12-15 08:32:15.134000: I runner.py:310] Step = 104900 ; steps/s = 1.62, tokens/s = 81973 (37706 source, 44267 target) ; Learning rate = 0.000273 ; Loss = 1.727593\n",
      "2024-12-15 08:33:16.784000: I runner.py:310] Step = 105000 ; steps/s = 1.62, tokens/s = 81981 (37703 source, 44278 target) ; Learning rate = 0.000273 ; Loss = 1.732540\n",
      "2024-12-15 08:33:16.785000: I training.py:192] Running evaluation for step 105000\n",
      "2024-12-15 08:35:20.116000: I training.py:192] Evaluation result for step 105000: loss = 2.643388 ; perplexity = 14.060765\n",
      "2024-12-15 08:36:21.065000: I runner.py:310] Step = 105100 ; steps/s = 1.64, tokens/s = 81573 (37527 source, 44046 target) ; Learning rate = 0.000273 ; Loss = 1.751049\n",
      "2024-12-15 08:37:22.733000: I runner.py:310] Step = 105200 ; steps/s = 1.62, tokens/s = 81935 (37670 source, 44265 target) ; Learning rate = 0.000273 ; Loss = 1.728388\n",
      "2024-12-15 08:38:24.367000: I runner.py:310] Step = 105300 ; steps/s = 1.62, tokens/s = 81992 (37717 source, 44275 target) ; Learning rate = 0.000272 ; Loss = 1.725766\n",
      "2024-12-15 08:39:26.024000: I runner.py:310] Step = 105400 ; steps/s = 1.62, tokens/s = 81971 (37697 source, 44274 target) ; Learning rate = 0.000272 ; Loss = 1.727018\n",
      "2024-12-15 08:40:27.293000: I runner.py:310] Step = 105500 ; steps/s = 1.63, tokens/s = 81145 (37340 source, 43805 target) ; Learning rate = 0.000272 ; Loss = 1.716362\n",
      "2024-12-15 08:41:28.922000: I runner.py:310] Step = 105600 ; steps/s = 1.62, tokens/s = 82027 (37725 source, 44302 target) ; Learning rate = 0.000272 ; Loss = 1.746257\n",
      "2024-12-15 08:42:30.621000: I runner.py:310] Step = 105700 ; steps/s = 1.62, tokens/s = 81877 (37653 source, 44224 target) ; Learning rate = 0.000272 ; Loss = 1.738852\n",
      "2024-12-15 08:43:31.872000: I runner.py:310] Step = 105800 ; steps/s = 1.63, tokens/s = 81149 (37330 source, 43819 target) ; Learning rate = 0.000272 ; Loss = 1.723379\n",
      "2024-12-15 08:44:33.475000: I runner.py:310] Step = 105900 ; steps/s = 1.62, tokens/s = 82045 (37729 source, 44316 target) ; Learning rate = 0.000272 ; Loss = 1.732404\n",
      "2024-12-15 08:45:35.176000: I runner.py:310] Step = 106000 ; steps/s = 1.62, tokens/s = 81918 (37695 source, 44223 target) ; Learning rate = 0.000271 ; Loss = 1.733964\n",
      "2024-12-15 08:46:36.867000: I runner.py:310] Step = 106100 ; steps/s = 1.62, tokens/s = 81883 (37646 source, 44237 target) ; Learning rate = 0.000271 ; Loss = 1.747310\n",
      "2024-12-15 08:47:38.143000: I runner.py:310] Step = 106200 ; steps/s = 1.63, tokens/s = 81105 (37299 source, 43806 target) ; Learning rate = 0.000271 ; Loss = 1.747236\n",
      "2024-12-15 08:48:39.866000: I runner.py:310] Step = 106300 ; steps/s = 1.62, tokens/s = 81864 (37642 source, 44222 target) ; Learning rate = 0.000271 ; Loss = 1.726529\n",
      "2024-12-15 08:49:41.497000: I runner.py:310] Step = 106400 ; steps/s = 1.62, tokens/s = 82007 (37713 source, 44294 target) ; Learning rate = 0.000271 ; Loss = 1.717306\n",
      "2024-12-15 08:50:42.806000: I runner.py:310] Step = 106500 ; steps/s = 1.63, tokens/s = 81083 (37305 source, 43778 target) ; Learning rate = 0.000271 ; Loss = 1.738343\n",
      "2024-12-15 08:51:44.464000: I runner.py:310] Step = 106600 ; steps/s = 1.62, tokens/s = 81956 (37691 source, 44265 target) ; Learning rate = 0.000271 ; Loss = 1.725272\n",
      "2024-12-15 08:52:46.127000: I runner.py:310] Step = 106700 ; steps/s = 1.62, tokens/s = 81937 (37668 source, 44269 target) ; Learning rate = 0.000271 ; Loss = 1.738056\n",
      "2024-12-15 08:53:47.821000: I runner.py:310] Step = 106800 ; steps/s = 1.62, tokens/s = 81930 (37708 source, 44222 target) ; Learning rate = 0.000270 ; Loss = 1.718550\n",
      "2024-12-15 08:54:49.075000: I runner.py:310] Step = 106900 ; steps/s = 1.63, tokens/s = 81145 (37324 source, 43821 target) ; Learning rate = 0.000270 ; Loss = 1.756366\n",
      "2024-12-15 08:55:50.729000: I runner.py:310] Step = 107000 ; steps/s = 1.62, tokens/s = 81961 (37695 source, 44266 target) ; Learning rate = 0.000270 ; Loss = 1.720960\n",
      "2024-12-15 08:56:52.385000: I runner.py:310] Step = 107100 ; steps/s = 1.62, tokens/s = 81983 (37719 source, 44264 target) ; Learning rate = 0.000270 ; Loss = 1.712754\n",
      "2024-12-15 08:57:53.605000: I runner.py:310] Step = 107200 ; steps/s = 1.63, tokens/s = 81141 (37297 source, 43844 target) ; Learning rate = 0.000270 ; Loss = 1.735867\n",
      "2024-12-15 08:58:55.266000: I runner.py:310] Step = 107300 ; steps/s = 1.62, tokens/s = 81953 (37702 source, 44251 target) ; Learning rate = 0.000270 ; Loss = 1.735069\n",
      "2024-12-15 08:59:56.893000: I runner.py:310] Step = 107400 ; steps/s = 1.62, tokens/s = 82000 (37708 source, 44292 target) ; Learning rate = 0.000270 ; Loss = 1.737799\n",
      "2024-12-15 09:00:58.617000: I runner.py:310] Step = 107500 ; steps/s = 1.62, tokens/s = 81898 (37670 source, 44228 target) ; Learning rate = 0.000270 ; Loss = 1.727296\n",
      "2024-12-15 09:01:59.846000: I runner.py:310] Step = 107600 ; steps/s = 1.63, tokens/s = 81203 (37366 source, 43837 target) ; Learning rate = 0.000269 ; Loss = 1.714313\n",
      "2024-12-15 09:03:01.529000: I runner.py:310] Step = 107700 ; steps/s = 1.62, tokens/s = 81936 (37676 source, 44260 target) ; Learning rate = 0.000269 ; Loss = 1.736531\n",
      "2024-12-15 09:04:03.251000: I runner.py:310] Step = 107800 ; steps/s = 1.62, tokens/s = 81858 (37651 source, 44207 target) ; Learning rate = 0.000269 ; Loss = 1.746143\n",
      "2024-12-15 09:05:04.491000: I runner.py:310] Step = 107900 ; steps/s = 1.63, tokens/s = 81148 (37325 source, 43823 target) ; Learning rate = 0.000269 ; Loss = 1.726580\n",
      "2024-12-15 09:06:06.196000: I runner.py:310] Step = 108000 ; steps/s = 1.62, tokens/s = 81934 (37698 source, 44236 target) ; Learning rate = 0.000269 ; Loss = 1.728145\n",
      "2024-12-15 09:07:07.860000: I runner.py:310] Step = 108100 ; steps/s = 1.62, tokens/s = 81931 (37671 source, 44260 target) ; Learning rate = 0.000269 ; Loss = 1.732023\n",
      "2024-12-15 09:08:09.518000: I runner.py:310] Step = 108200 ; steps/s = 1.62, tokens/s = 81945 (37683 source, 44262 target) ; Learning rate = 0.000269 ; Loss = 1.743605\n",
      "2024-12-15 09:09:10.802000: I runner.py:310] Step = 108300 ; steps/s = 1.63, tokens/s = 81096 (37290 source, 43806 target) ; Learning rate = 0.000269 ; Loss = 1.716420\n",
      "2024-12-15 09:10:12.433000: I runner.py:310] Step = 108400 ; steps/s = 1.62, tokens/s = 82017 (37742 source, 44275 target) ; Learning rate = 0.000268 ; Loss = 1.733113\n",
      "2024-12-15 09:11:14.025000: I runner.py:310] Step = 108500 ; steps/s = 1.62, tokens/s = 82027 (37706 source, 44321 target) ; Learning rate = 0.000268 ; Loss = 1.751429\n",
      "2024-12-15 09:12:15.318000: I runner.py:310] Step = 108600 ; steps/s = 1.63, tokens/s = 81079 (37287 source, 43792 target) ; Learning rate = 0.000268 ; Loss = 1.731892\n",
      "2024-12-15 09:13:16.991000: I runner.py:310] Step = 108700 ; steps/s = 1.62, tokens/s = 81935 (37670 source, 44265 target) ; Learning rate = 0.000268 ; Loss = 1.721934\n",
      "2024-12-15 09:14:18.652000: I runner.py:310] Step = 108800 ; steps/s = 1.62, tokens/s = 81990 (37730 source, 44260 target) ; Learning rate = 0.000268 ; Loss = 1.726330\n",
      "2024-12-15 09:15:20.076000: I runner.py:310] Step = 108900 ; steps/s = 1.63, tokens/s = 81475 (37481 source, 43994 target) ; Learning rate = 0.000268 ; Loss = 1.746550\n",
      "2024-12-15 09:16:21.586000: I runner.py:310] Step = 109000 ; steps/s = 1.63, tokens/s = 81598 (37532 source, 44066 target) ; Learning rate = 0.000268 ; Loss = 1.720401\n",
      "2024-12-15 09:17:23.240000: I runner.py:310] Step = 109100 ; steps/s = 1.62, tokens/s = 81974 (37697 source, 44277 target) ; Learning rate = 0.000268 ; Loss = 1.740905\n",
      "2024-12-15 09:18:24.914000: I runner.py:310] Step = 109200 ; steps/s = 1.62, tokens/s = 81934 (37686 source, 44248 target) ; Learning rate = 0.000267 ; Loss = 1.750164\n",
      "2024-12-15 09:19:26.173000: I runner.py:310] Step = 109300 ; steps/s = 1.63, tokens/s = 81146 (37338 source, 43808 target) ; Learning rate = 0.000267 ; Loss = 1.729837\n",
      "2024-12-15 09:20:27.916000: I runner.py:310] Step = 109400 ; steps/s = 1.62, tokens/s = 81847 (37642 source, 44205 target) ; Learning rate = 0.000267 ; Loss = 1.731797\n",
      "2024-12-15 09:21:29.522000: I runner.py:310] Step = 109500 ; steps/s = 1.62, tokens/s = 82012 (37710 source, 44302 target) ; Learning rate = 0.000267 ; Loss = 1.706355\n",
      "2024-12-15 09:22:30.837000: I runner.py:310] Step = 109600 ; steps/s = 1.63, tokens/s = 81068 (37286 source, 43782 target) ; Learning rate = 0.000267 ; Loss = 1.735028\n",
      "2024-12-15 09:23:32.536000: I runner.py:310] Step = 109700 ; steps/s = 1.62, tokens/s = 81916 (37674 source, 44242 target) ; Learning rate = 0.000267 ; Loss = 1.729327\n",
      "2024-12-15 09:24:34.142000: I runner.py:310] Step = 109800 ; steps/s = 1.62, tokens/s = 82033 (37724 source, 44309 target) ; Learning rate = 0.000267 ; Loss = 1.716815\n",
      "2024-12-15 09:25:35.842000: I runner.py:310] Step = 109900 ; steps/s = 1.62, tokens/s = 81902 (37678 source, 44224 target) ; Learning rate = 0.000267 ; Loss = 1.730578\n",
      "2024-12-15 09:26:37.050000: I runner.py:310] Step = 110000 ; steps/s = 1.63, tokens/s = 81181 (37328 source, 43853 target) ; Learning rate = 0.000266 ; Loss = 1.747284\n",
      "2024-12-15 09:26:38.904000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-110000\n",
      "2024-12-15 09:26:38.904000: I training.py:192] Running evaluation for step 110000\n",
      "2024-12-15 09:28:39.820000: I training.py:192] Evaluation result for step 110000: loss = 2.644912 ; perplexity = 14.082199\n",
      "2024-12-15 09:29:41.292000: I runner.py:310] Step = 110100 ; steps/s = 1.63, tokens/s = 82229 (37819 source, 44410 target) ; Learning rate = 0.000266 ; Loss = 1.717229\n",
      "2024-12-15 09:30:42.972000: I runner.py:310] Step = 110200 ; steps/s = 1.62, tokens/s = 81944 (37701 source, 44243 target) ; Learning rate = 0.000266 ; Loss = 1.713761\n",
      "2024-12-15 09:31:44.246000: I runner.py:310] Step = 110300 ; steps/s = 1.63, tokens/s = 81093 (37292 source, 43801 target) ; Learning rate = 0.000266 ; Loss = 1.713883\n",
      "2024-12-15 09:32:45.893000: I runner.py:310] Step = 110400 ; steps/s = 1.62, tokens/s = 81998 (37729 source, 44269 target) ; Learning rate = 0.000266 ; Loss = 1.725583\n",
      "2024-12-15 09:33:47.545000: I runner.py:310] Step = 110500 ; steps/s = 1.62, tokens/s = 81955 (37672 source, 44283 target) ; Learning rate = 0.000266 ; Loss = 1.738291\n",
      "2024-12-15 09:34:49.231000: I runner.py:310] Step = 110600 ; steps/s = 1.62, tokens/s = 81939 (37707 source, 44232 target) ; Learning rate = 0.000266 ; Loss = 1.745924\n",
      "2024-12-15 09:35:50.417000: I runner.py:310] Step = 110700 ; steps/s = 1.63, tokens/s = 81202 (37333 source, 43869 target) ; Learning rate = 0.000266 ; Loss = 1.749056\n",
      "2024-12-15 09:36:52.058000: I runner.py:310] Step = 110800 ; steps/s = 1.62, tokens/s = 81961 (37689 source, 44272 target) ; Learning rate = 0.000266 ; Loss = 1.718710\n",
      "2024-12-15 09:37:53.717000: I runner.py:310] Step = 110900 ; steps/s = 1.62, tokens/s = 81969 (37699 source, 44270 target) ; Learning rate = 0.000265 ; Loss = 1.731421\n",
      "2024-12-15 09:38:54.997000: I runner.py:310] Step = 111000 ; steps/s = 1.63, tokens/s = 81133 (37324 source, 43809 target) ; Learning rate = 0.000265 ; Loss = 1.730210\n",
      "2024-12-15 09:39:56.625000: I runner.py:310] Step = 111100 ; steps/s = 1.62, tokens/s = 81977 (37693 source, 44284 target) ; Learning rate = 0.000265 ; Loss = 1.734561\n",
      "2024-12-15 09:40:58.251000: I runner.py:310] Step = 111200 ; steps/s = 1.62, tokens/s = 82004 (37719 source, 44285 target) ; Learning rate = 0.000265 ; Loss = 1.729859\n",
      "2024-12-15 09:41:59.943000: I runner.py:310] Step = 111300 ; steps/s = 1.62, tokens/s = 81927 (37689 source, 44238 target) ; Learning rate = 0.000265 ; Loss = 1.728728\n",
      "2024-12-15 09:43:01.197000: I runner.py:310] Step = 111400 ; steps/s = 1.63, tokens/s = 81161 (37336 source, 43825 target) ; Learning rate = 0.000265 ; Loss = 1.708798\n",
      "2024-12-15 09:44:02.916000: I runner.py:310] Step = 111500 ; steps/s = 1.62, tokens/s = 81874 (37660 source, 44214 target) ; Learning rate = 0.000265 ; Loss = 1.725246\n",
      "2024-12-15 09:45:04.567000: I runner.py:310] Step = 111600 ; steps/s = 1.62, tokens/s = 81992 (37717 source, 44275 target) ; Learning rate = 0.000265 ; Loss = 1.740786\n",
      "2024-12-15 09:46:05.814000: I runner.py:310] Step = 111700 ; steps/s = 1.63, tokens/s = 81121 (37297 source, 43824 target) ; Learning rate = 0.000264 ; Loss = 1.734308\n",
      "2024-12-15 09:47:07.514000: I runner.py:310] Step = 111800 ; steps/s = 1.62, tokens/s = 81882 (37660 source, 44222 target) ; Learning rate = 0.000264 ; Loss = 1.732339\n",
      "2024-12-15 09:48:09.149000: I runner.py:310] Step = 111900 ; steps/s = 1.62, tokens/s = 82003 (37719 source, 44284 target) ; Learning rate = 0.000264 ; Loss = 1.723578\n",
      "2024-12-15 09:49:10.790000: I runner.py:310] Step = 112000 ; steps/s = 1.62, tokens/s = 82000 (37722 source, 44278 target) ; Learning rate = 0.000264 ; Loss = 1.713566\n",
      "2024-12-15 09:50:12.017000: I runner.py:310] Step = 112100 ; steps/s = 1.63, tokens/s = 81160 (37322 source, 43838 target) ; Learning rate = 0.000264 ; Loss = 1.738456\n",
      "2024-12-15 09:51:13.705000: I runner.py:310] Step = 112200 ; steps/s = 1.62, tokens/s = 81942 (37698 source, 44244 target) ; Learning rate = 0.000264 ; Loss = 1.722041\n",
      "2024-12-15 09:52:15.334000: I runner.py:310] Step = 112300 ; steps/s = 1.62, tokens/s = 82001 (37715 source, 44286 target) ; Learning rate = 0.000264 ; Loss = 1.721270\n",
      "2024-12-15 09:53:16.555000: I runner.py:310] Step = 112400 ; steps/s = 1.63, tokens/s = 81165 (37316 source, 43849 target) ; Learning rate = 0.000264 ; Loss = 1.738390\n",
      "2024-12-15 09:54:18.207000: I runner.py:310] Step = 112500 ; steps/s = 1.62, tokens/s = 81977 (37712 source, 44265 target) ; Learning rate = 0.000264 ; Loss = 1.725313\n",
      "2024-12-15 09:55:19.860000: I runner.py:310] Step = 112600 ; steps/s = 1.62, tokens/s = 81953 (37679 source, 44274 target) ; Learning rate = 0.000263 ; Loss = 1.717560\n",
      "2024-12-15 09:56:21.463000: I runner.py:310] Step = 112700 ; steps/s = 1.62, tokens/s = 82058 (37759 source, 44299 target) ; Learning rate = 0.000263 ; Loss = 1.716850\n",
      "2024-12-15 09:57:22.713000: I runner.py:310] Step = 112800 ; steps/s = 1.63, tokens/s = 81146 (37319 source, 43827 target) ; Learning rate = 0.000263 ; Loss = 1.701206\n",
      "2024-12-15 09:58:24.384000: I runner.py:310] Step = 112900 ; steps/s = 1.62, tokens/s = 81956 (37684 source, 44272 target) ; Learning rate = 0.000263 ; Loss = 1.738741\n",
      "2024-12-15 09:59:26.127000: I runner.py:310] Step = 113000 ; steps/s = 1.62, tokens/s = 81812 (37621 source, 44191 target) ; Learning rate = 0.000263 ; Loss = 1.737908\n",
      "2024-12-15 10:00:27.365000: I runner.py:310] Step = 113100 ; steps/s = 1.63, tokens/s = 81180 (37345 source, 43835 target) ; Learning rate = 0.000263 ; Loss = 1.720060\n",
      "2024-12-15 10:01:29.043000: I runner.py:310] Step = 113200 ; steps/s = 1.62, tokens/s = 81951 (37687 source, 44264 target) ; Learning rate = 0.000263 ; Loss = 1.715086\n",
      "2024-12-15 10:02:30.706000: I runner.py:310] Step = 113300 ; steps/s = 1.62, tokens/s = 81950 (37709 source, 44241 target) ; Learning rate = 0.000263 ; Loss = 1.739458\n",
      "2024-12-15 10:03:32.403000: I runner.py:310] Step = 113400 ; steps/s = 1.62, tokens/s = 81903 (37665 source, 44238 target) ; Learning rate = 0.000262 ; Loss = 1.739984\n",
      "2024-12-15 10:04:33.718000: I runner.py:310] Step = 113500 ; steps/s = 1.63, tokens/s = 81087 (37314 source, 43773 target) ; Learning rate = 0.000262 ; Loss = 1.704176\n",
      "2024-12-15 10:05:35.402000: I runner.py:310] Step = 113600 ; steps/s = 1.62, tokens/s = 81922 (37671 source, 44251 target) ; Learning rate = 0.000262 ; Loss = 1.732183\n",
      "2024-12-15 10:06:37.039000: I runner.py:310] Step = 113700 ; steps/s = 1.62, tokens/s = 81972 (37697 source, 44275 target) ; Learning rate = 0.000262 ; Loss = 1.726264\n",
      "2024-12-15 10:07:38.240000: I runner.py:310] Step = 113800 ; steps/s = 1.63, tokens/s = 81200 (37330 source, 43870 target) ; Learning rate = 0.000262 ; Loss = 1.719532\n",
      "2024-12-15 10:08:39.877000: I runner.py:310] Step = 113900 ; steps/s = 1.62, tokens/s = 82016 (37733 source, 44283 target) ; Learning rate = 0.000262 ; Loss = 1.723007\n",
      "2024-12-15 10:09:41.547000: I runner.py:310] Step = 114000 ; steps/s = 1.62, tokens/s = 81930 (37683 source, 44247 target) ; Learning rate = 0.000262 ; Loss = 1.727525\n",
      "2024-12-15 10:10:43.187000: I runner.py:310] Step = 114100 ; steps/s = 1.62, tokens/s = 81975 (37698 source, 44277 target) ; Learning rate = 0.000262 ; Loss = 1.734308\n",
      "2024-12-15 10:11:44.396000: I runner.py:310] Step = 114200 ; steps/s = 1.63, tokens/s = 81199 (37353 source, 43846 target) ; Learning rate = 0.000262 ; Loss = 1.697939\n",
      "2024-12-15 10:12:46.083000: I runner.py:310] Step = 114300 ; steps/s = 1.62, tokens/s = 81957 (37724 source, 44233 target) ; Learning rate = 0.000261 ; Loss = 1.729411\n",
      "2024-12-15 10:13:47.793000: I runner.py:310] Step = 114400 ; steps/s = 1.62, tokens/s = 81892 (37652 source, 44240 target) ; Learning rate = 0.000261 ; Loss = 1.736414\n",
      "2024-12-15 10:14:49.018000: I runner.py:310] Step = 114500 ; steps/s = 1.63, tokens/s = 81162 (37317 source, 43845 target) ; Learning rate = 0.000261 ; Loss = 1.707897\n",
      "2024-12-15 10:15:50.676000: I runner.py:310] Step = 114600 ; steps/s = 1.62, tokens/s = 81968 (37701 source, 44267 target) ; Learning rate = 0.000261 ; Loss = 1.728909\n",
      "2024-12-15 10:16:52.353000: I runner.py:310] Step = 114700 ; steps/s = 1.62, tokens/s = 81951 (37695 source, 44256 target) ; Learning rate = 0.000261 ; Loss = 1.734691\n",
      "2024-12-15 10:17:54.026000: I runner.py:310] Step = 114800 ; steps/s = 1.62, tokens/s = 81924 (37673 source, 44251 target) ; Learning rate = 0.000261 ; Loss = 1.736784\n",
      "2024-12-15 10:18:55.233000: I runner.py:310] Step = 114900 ; steps/s = 1.63, tokens/s = 81204 (37342 source, 43862 target) ; Learning rate = 0.000261 ; Loss = 1.698607\n",
      "2024-12-15 10:19:56.861000: I runner.py:310] Step = 115000 ; steps/s = 1.62, tokens/s = 82033 (37749 source, 44284 target) ; Learning rate = 0.000261 ; Loss = 1.731713\n",
      "2024-12-15 10:19:56.862000: I training.py:192] Running evaluation for step 115000\n",
      "2024-12-15 10:21:58.487000: I training.py:192] Evaluation result for step 115000: loss = 2.656532 ; perplexity = 14.246793\n",
      "2024-12-15 10:22:59.913000: I runner.py:310] Step = 115100 ; steps/s = 1.63, tokens/s = 82267 (37827 source, 44440 target) ; Learning rate = 0.000261 ; Loss = 1.737706\n",
      "2024-12-15 10:24:01.096000: I runner.py:310] Step = 115200 ; steps/s = 1.63, tokens/s = 81215 (37353 source, 43862 target) ; Learning rate = 0.000260 ; Loss = 1.709206\n",
      "2024-12-15 10:25:02.730000: I runner.py:310] Step = 115300 ; steps/s = 1.62, tokens/s = 81993 (37717 source, 44276 target) ; Learning rate = 0.000260 ; Loss = 1.721663\n",
      "2024-12-15 10:26:04.411000: I runner.py:310] Step = 115400 ; steps/s = 1.62, tokens/s = 81945 (37691 source, 44254 target) ; Learning rate = 0.000260 ; Loss = 1.723629\n",
      "2024-12-15 10:27:06.058000: I runner.py:310] Step = 115500 ; steps/s = 1.62, tokens/s = 81960 (37684 source, 44276 target) ; Learning rate = 0.000260 ; Loss = 1.718889\n",
      "2024-12-15 10:28:07.261000: I runner.py:310] Step = 115600 ; steps/s = 1.63, tokens/s = 81206 (37339 source, 43867 target) ; Learning rate = 0.000260 ; Loss = 1.711734\n",
      "2024-12-15 10:29:08.968000: I runner.py:310] Step = 115700 ; steps/s = 1.62, tokens/s = 81877 (37652 source, 44225 target) ; Learning rate = 0.000260 ; Loss = 1.730264\n",
      "2024-12-15 10:30:10.650000: I runner.py:310] Step = 115800 ; steps/s = 1.62, tokens/s = 81947 (37712 source, 44235 target) ; Learning rate = 0.000260 ; Loss = 1.720469\n",
      "2024-12-15 10:31:11.905000: I runner.py:310] Step = 115900 ; steps/s = 1.63, tokens/s = 81139 (37318 source, 43821 target) ; Learning rate = 0.000260 ; Loss = 1.726814\n",
      "2024-12-15 10:32:13.613000: I runner.py:310] Step = 116000 ; steps/s = 1.62, tokens/s = 81872 (37649 source, 44223 target) ; Learning rate = 0.000260 ; Loss = 1.707927\n",
      "2024-12-15 10:33:15.362000: I runner.py:310] Step = 116100 ; steps/s = 1.62, tokens/s = 81847 (37648 source, 44199 target) ; Learning rate = 0.000259 ; Loss = 1.709572\n",
      "2024-12-15 10:34:16.960000: I runner.py:310] Step = 116200 ; steps/s = 1.62, tokens/s = 82062 (37742 source, 44320 target) ; Learning rate = 0.000259 ; Loss = 1.717192\n",
      "2024-12-15 10:35:18.208000: I runner.py:310] Step = 116300 ; steps/s = 1.63, tokens/s = 81168 (37340 source, 43828 target) ; Learning rate = 0.000259 ; Loss = 1.712309\n",
      "2024-12-15 10:36:19.899000: I runner.py:310] Step = 116400 ; steps/s = 1.62, tokens/s = 81930 (37691 source, 44239 target) ; Learning rate = 0.000259 ; Loss = 1.724279\n",
      "2024-12-15 10:37:21.556000: I runner.py:310] Step = 116500 ; steps/s = 1.62, tokens/s = 81914 (37648 source, 44266 target) ; Learning rate = 0.000259 ; Loss = 1.729013\n",
      "2024-12-15 10:38:22.828000: I runner.py:310] Step = 116600 ; steps/s = 1.63, tokens/s = 81122 (37324 source, 43798 target) ; Learning rate = 0.000259 ; Loss = 1.731465\n",
      "2024-12-15 10:39:24.398000: I runner.py:310] Step = 116700 ; steps/s = 1.62, tokens/s = 82083 (37743 source, 44340 target) ; Learning rate = 0.000259 ; Loss = 1.709011\n",
      "2024-12-15 10:40:26.018000: I runner.py:310] Step = 116800 ; steps/s = 1.62, tokens/s = 82014 (37728 source, 44286 target) ; Learning rate = 0.000259 ; Loss = 1.723086\n",
      "2024-12-15 10:41:27.647000: I runner.py:310] Step = 116900 ; steps/s = 1.62, tokens/s = 82003 (37718 source, 44285 target) ; Learning rate = 0.000259 ; Loss = 1.714018\n",
      "2024-12-15 10:42:28.866000: I runner.py:310] Step = 117000 ; steps/s = 1.63, tokens/s = 81220 (37365 source, 43855 target) ; Learning rate = 0.000258 ; Loss = 1.707701\n",
      "2024-12-15 10:43:30.528000: I runner.py:310] Step = 117100 ; steps/s = 1.62, tokens/s = 81938 (37684 source, 44254 target) ; Learning rate = 0.000258 ; Loss = 1.738915\n",
      "2024-12-15 10:44:32.209000: I runner.py:310] Step = 117200 ; steps/s = 1.62, tokens/s = 81902 (37657 source, 44245 target) ; Learning rate = 0.000258 ; Loss = 1.727693\n",
      "2024-12-15 10:45:33.443000: I runner.py:310] Step = 117300 ; steps/s = 1.63, tokens/s = 81183 (37342 source, 43841 target) ; Learning rate = 0.000258 ; Loss = 1.697540\n",
      "2024-12-15 10:46:35.098000: I runner.py:310] Step = 117400 ; steps/s = 1.62, tokens/s = 81966 (37702 source, 44264 target) ; Learning rate = 0.000258 ; Loss = 1.711175\n",
      "2024-12-15 10:47:36.731000: I runner.py:310] Step = 117500 ; steps/s = 1.62, tokens/s = 81991 (37721 source, 44270 target) ; Learning rate = 0.000258 ; Loss = 1.726584\n",
      "2024-12-15 10:48:37.976000: I runner.py:310] Step = 117600 ; steps/s = 1.63, tokens/s = 81157 (37318 source, 43839 target) ; Learning rate = 0.000258 ; Loss = 1.764957\n",
      "2024-12-15 10:49:39.624000: I runner.py:310] Step = 117700 ; steps/s = 1.62, tokens/s = 81972 (37691 source, 44281 target) ; Learning rate = 0.000258 ; Loss = 1.730237\n",
      "2024-12-15 10:50:41.323000: I runner.py:310] Step = 117800 ; steps/s = 1.62, tokens/s = 81899 (37671 source, 44228 target) ; Learning rate = 0.000258 ; Loss = 1.716938\n",
      "2024-12-15 10:51:43.014000: I runner.py:310] Step = 117900 ; steps/s = 1.62, tokens/s = 81927 (37692 source, 44235 target) ; Learning rate = 0.000257 ; Loss = 1.713370\n",
      "2024-12-15 10:52:44.227000: I runner.py:310] Step = 118000 ; steps/s = 1.63, tokens/s = 81188 (37332 source, 43856 target) ; Learning rate = 0.000257 ; Loss = 1.747719\n",
      "2024-12-15 10:53:45.897000: I runner.py:310] Step = 118100 ; steps/s = 1.62, tokens/s = 81932 (37677 source, 44255 target) ; Learning rate = 0.000257 ; Loss = 1.705346\n",
      "2024-12-15 10:54:47.621000: I runner.py:310] Step = 118200 ; steps/s = 1.62, tokens/s = 81865 (37649 source, 44216 target) ; Learning rate = 0.000257 ; Loss = 1.703431\n",
      "2024-12-15 10:55:48.800000: I runner.py:310] Step = 118300 ; steps/s = 1.63, tokens/s = 81263 (37393 source, 43870 target) ; Learning rate = 0.000257 ; Loss = 1.720821\n",
      "2024-12-15 10:56:50.487000: I runner.py:310] Step = 118400 ; steps/s = 1.62, tokens/s = 81890 (37625 source, 44265 target) ; Learning rate = 0.000257 ; Loss = 1.729153\n",
      "2024-12-15 10:57:52.192000: I runner.py:310] Step = 118500 ; steps/s = 1.62, tokens/s = 81925 (37700 source, 44225 target) ; Learning rate = 0.000257 ; Loss = 1.718496\n",
      "2024-12-15 10:58:53.904000: I runner.py:310] Step = 118600 ; steps/s = 1.62, tokens/s = 81893 (37679 source, 44214 target) ; Learning rate = 0.000257 ; Loss = 1.714023\n",
      "2024-12-15 10:59:55.192000: I runner.py:310] Step = 118700 ; steps/s = 1.63, tokens/s = 81087 (37287 source, 43800 target) ; Learning rate = 0.000257 ; Loss = 1.729664\n",
      "2024-12-15 11:00:56.826000: I runner.py:310] Step = 118800 ; steps/s = 1.62, tokens/s = 81998 (37716 source, 44282 target) ; Learning rate = 0.000256 ; Loss = 1.706012\n",
      "2024-12-15 11:01:58.540000: I runner.py:310] Step = 118900 ; steps/s = 1.62, tokens/s = 81904 (37677 source, 44227 target) ; Learning rate = 0.000256 ; Loss = 1.715306\n",
      "2024-12-15 11:02:59.807000: I runner.py:310] Step = 119000 ; steps/s = 1.63, tokens/s = 81108 (37303 source, 43805 target) ; Learning rate = 0.000256 ; Loss = 1.727927\n",
      "2024-12-15 11:04:01.440000: I runner.py:310] Step = 119100 ; steps/s = 1.62, tokens/s = 81990 (37707 source, 44283 target) ; Learning rate = 0.000256 ; Loss = 1.724768\n",
      "2024-12-15 11:05:03.053000: I runner.py:310] Step = 119200 ; steps/s = 1.62, tokens/s = 82029 (37728 source, 44301 target) ; Learning rate = 0.000256 ; Loss = 1.703708\n",
      "2024-12-15 11:06:04.726000: I runner.py:310] Step = 119300 ; steps/s = 1.62, tokens/s = 81934 (37687 source, 44247 target) ; Learning rate = 0.000256 ; Loss = 1.732319\n",
      "2024-12-15 11:07:05.953000: I runner.py:310] Step = 119400 ; steps/s = 1.63, tokens/s = 81169 (37327 source, 43842 target) ; Learning rate = 0.000256 ; Loss = 1.733251\n",
      "2024-12-15 11:08:07.634000: I runner.py:310] Step = 119500 ; steps/s = 1.62, tokens/s = 81960 (37704 source, 44256 target) ; Learning rate = 0.000256 ; Loss = 1.711319\n",
      "2024-12-15 11:09:09.319000: I runner.py:310] Step = 119600 ; steps/s = 1.62, tokens/s = 81916 (37668 source, 44248 target) ; Learning rate = 0.000256 ; Loss = 1.712019\n",
      "2024-12-15 11:10:10.535000: I runner.py:310] Step = 119700 ; steps/s = 1.63, tokens/s = 81182 (37350 source, 43832 target) ; Learning rate = 0.000255 ; Loss = 1.707103\n",
      "2024-12-15 11:11:12.087000: I runner.py:310] Step = 119800 ; steps/s = 1.62, tokens/s = 82117 (37776 source, 44341 target) ; Learning rate = 0.000255 ; Loss = 1.714409\n",
      "2024-12-15 11:12:13.720000: I runner.py:310] Step = 119900 ; steps/s = 1.62, tokens/s = 82002 (37729 source, 44273 target) ; Learning rate = 0.000255 ; Loss = 1.724837\n",
      "2024-12-15 11:13:15.370000: I runner.py:310] Step = 120000 ; steps/s = 1.62, tokens/s = 81976 (37691 source, 44285 target) ; Learning rate = 0.000255 ; Loss = 1.725841\n",
      "2024-12-15 11:13:17.299000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-120000\n",
      "2024-12-15 11:13:17.299000: I training.py:192] Running evaluation for step 120000\n",
      "2024-12-15 11:15:20.029000: I training.py:192] Evaluation result for step 120000: loss = 2.650386 ; perplexity = 14.159508\n",
      "2024-12-15 11:16:21.031000: I runner.py:310] Step = 120100 ; steps/s = 1.64, tokens/s = 81514 (37485 source, 44029 target) ; Learning rate = 0.000255 ; Loss = 1.735431\n",
      "2024-12-15 11:17:22.606000: I runner.py:310] Step = 120200 ; steps/s = 1.62, tokens/s = 82051 (37717 source, 44334 target) ; Learning rate = 0.000255 ; Loss = 1.713943\n",
      "2024-12-15 11:18:24.268000: I runner.py:310] Step = 120300 ; steps/s = 1.62, tokens/s = 81988 (37717 source, 44271 target) ; Learning rate = 0.000255 ; Loss = 1.717590\n",
      "2024-12-15 11:19:25.501000: I runner.py:310] Step = 120400 ; steps/s = 1.63, tokens/s = 81186 (37352 source, 43834 target) ; Learning rate = 0.000255 ; Loss = 1.708296\n",
      "2024-12-15 11:20:27.115000: I runner.py:310] Step = 120500 ; steps/s = 1.62, tokens/s = 82035 (37730 source, 44305 target) ; Learning rate = 0.000255 ; Loss = 1.706569\n",
      "2024-12-15 11:21:28.739000: I runner.py:310] Step = 120600 ; steps/s = 1.62, tokens/s = 81998 (37722 source, 44276 target) ; Learning rate = 0.000255 ; Loss = 1.714179\n",
      "2024-12-15 11:22:30.357000: I runner.py:310] Step = 120700 ; steps/s = 1.62, tokens/s = 81990 (37695 source, 44295 target) ; Learning rate = 0.000254 ; Loss = 1.723574\n",
      "2024-12-15 11:23:31.609000: I runner.py:310] Step = 120800 ; steps/s = 1.63, tokens/s = 81180 (37347 source, 43833 target) ; Learning rate = 0.000254 ; Loss = 1.695400\n",
      "2024-12-15 11:24:33.251000: I runner.py:310] Step = 120900 ; steps/s = 1.62, tokens/s = 81989 (37727 source, 44262 target) ; Learning rate = 0.000254 ; Loss = 1.729471\n",
      "2024-12-15 11:25:34.829000: I runner.py:310] Step = 121000 ; steps/s = 1.62, tokens/s = 82060 (37742 source, 44318 target) ; Learning rate = 0.000254 ; Loss = 1.720891\n",
      "2024-12-15 11:26:36.055000: I runner.py:310] Step = 121100 ; steps/s = 1.63, tokens/s = 81168 (37310 source, 43858 target) ; Learning rate = 0.000254 ; Loss = 1.705412\n",
      "2024-12-15 11:27:37.630000: I runner.py:310] Step = 121200 ; steps/s = 1.62, tokens/s = 82050 (37727 source, 44323 target) ; Learning rate = 0.000254 ; Loss = 1.709870\n",
      "2024-12-15 11:28:39.240000: I runner.py:310] Step = 121300 ; steps/s = 1.62, tokens/s = 82040 (37754 source, 44286 target) ; Learning rate = 0.000254 ; Loss = 1.723104\n",
      "2024-12-15 11:29:40.855000: I runner.py:310] Step = 121400 ; steps/s = 1.62, tokens/s = 82010 (37712 source, 44298 target) ; Learning rate = 0.000254 ; Loss = 1.719761\n",
      "2024-12-15 11:30:42.091000: I runner.py:310] Step = 121500 ; steps/s = 1.63, tokens/s = 81145 (37303 source, 43842 target) ; Learning rate = 0.000254 ; Loss = 1.738848\n",
      "2024-12-15 11:31:43.766000: I runner.py:310] Step = 121600 ; steps/s = 1.62, tokens/s = 81930 (37672 source, 44258 target) ; Learning rate = 0.000253 ; Loss = 1.721234\n",
      "2024-12-15 11:32:45.357000: I runner.py:310] Step = 121700 ; steps/s = 1.62, tokens/s = 82048 (37748 source, 44300 target) ; Learning rate = 0.000253 ; Loss = 1.717924\n",
      "2024-12-15 11:33:46.611000: I runner.py:310] Step = 121800 ; steps/s = 1.63, tokens/s = 81167 (37348 source, 43819 target) ; Learning rate = 0.000253 ; Loss = 1.714043\n",
      "2024-12-15 11:34:48.195000: I runner.py:310] Step = 121900 ; steps/s = 1.62, tokens/s = 82064 (37735 source, 44329 target) ; Learning rate = 0.000253 ; Loss = 1.705380\n",
      "2024-12-15 11:35:49.783000: I runner.py:310] Step = 122000 ; steps/s = 1.62, tokens/s = 82058 (37732 source, 44326 target) ; Learning rate = 0.000253 ; Loss = 1.729396\n",
      "2024-12-15 11:36:51.438000: I runner.py:310] Step = 122100 ; steps/s = 1.62, tokens/s = 81955 (37714 source, 44241 target) ; Learning rate = 0.000253 ; Loss = 1.724180\n",
      "2024-12-15 11:37:52.620000: I runner.py:310] Step = 122200 ; steps/s = 1.63, tokens/s = 81256 (37376 source, 43880 target) ; Learning rate = 0.000253 ; Loss = 1.692459\n",
      "2024-12-15 11:38:54.229000: I runner.py:310] Step = 122300 ; steps/s = 1.62, tokens/s = 82037 (37738 source, 44299 target) ; Learning rate = 0.000253 ; Loss = 1.722683\n",
      "2024-12-15 11:39:55.839000: I runner.py:310] Step = 122400 ; steps/s = 1.62, tokens/s = 82010 (37706 source, 44304 target) ; Learning rate = 0.000253 ; Loss = 1.721771\n",
      "2024-12-15 11:40:57.099000: I runner.py:310] Step = 122500 ; steps/s = 1.63, tokens/s = 81122 (37319 source, 43803 target) ; Learning rate = 0.000253 ; Loss = 1.725558\n",
      "2024-12-15 11:41:58.702000: I runner.py:310] Step = 122600 ; steps/s = 1.62, tokens/s = 82039 (37728 source, 44311 target) ; Learning rate = 0.000252 ; Loss = 1.711285\n",
      "2024-12-15 11:43:00.259000: I runner.py:310] Step = 122700 ; steps/s = 1.62, tokens/s = 82111 (37770 source, 44341 target) ; Learning rate = 0.000252 ; Loss = 1.722096\n",
      "2024-12-15 11:44:01.860000: I runner.py:310] Step = 122800 ; steps/s = 1.62, tokens/s = 82020 (37720 source, 44300 target) ; Learning rate = 0.000252 ; Loss = 1.720747\n",
      "2024-12-15 11:45:03.095000: I runner.py:310] Step = 122900 ; steps/s = 1.63, tokens/s = 81162 (37332 source, 43830 target) ; Learning rate = 0.000252 ; Loss = 1.720313\n",
      "2024-12-15 11:46:04.667000: I runner.py:310] Step = 123000 ; steps/s = 1.62, tokens/s = 82070 (37740 source, 44330 target) ; Learning rate = 0.000252 ; Loss = 1.712697\n",
      "2024-12-15 11:47:06.310000: I runner.py:310] Step = 123100 ; steps/s = 1.62, tokens/s = 81998 (37740 source, 44258 target) ; Learning rate = 0.000252 ; Loss = 1.713685\n",
      "2024-12-15 11:48:07.538000: I runner.py:310] Step = 123200 ; steps/s = 1.63, tokens/s = 81184 (37322 source, 43862 target) ; Learning rate = 0.000252 ; Loss = 1.707350\n",
      "2024-12-15 11:49:09.188000: I runner.py:310] Step = 123300 ; steps/s = 1.62, tokens/s = 81976 (37705 source, 44271 target) ; Learning rate = 0.000252 ; Loss = 1.708148\n",
      "2024-12-15 11:50:10.823000: I runner.py:310] Step = 123400 ; steps/s = 1.62, tokens/s = 81980 (37695 source, 44285 target) ; Learning rate = 0.000252 ; Loss = 1.723134\n",
      "2024-12-15 11:51:12.410000: I runner.py:310] Step = 123500 ; steps/s = 1.62, tokens/s = 82049 (37740 source, 44309 target) ; Learning rate = 0.000252 ; Loss = 1.726720\n",
      "2024-12-15 11:52:13.592000: I runner.py:310] Step = 123600 ; steps/s = 1.63, tokens/s = 81236 (37368 source, 43868 target) ; Learning rate = 0.000251 ; Loss = 1.731318\n",
      "2024-12-15 11:53:15.191000: I runner.py:310] Step = 123700 ; steps/s = 1.62, tokens/s = 82023 (37725 source, 44298 target) ; Learning rate = 0.000251 ; Loss = 1.704032\n",
      "2024-12-15 11:54:16.805000: I runner.py:310] Step = 123800 ; steps/s = 1.62, tokens/s = 82040 (37731 source, 44309 target) ; Learning rate = 0.000251 ; Loss = 1.715736\n",
      "2024-12-15 11:55:17.942000: I runner.py:310] Step = 123900 ; steps/s = 1.64, tokens/s = 81298 (37393 source, 43905 target) ; Learning rate = 0.000251 ; Loss = 1.710931\n",
      "2024-12-15 11:56:19.593000: I runner.py:310] Step = 124000 ; steps/s = 1.62, tokens/s = 81963 (37705 source, 44258 target) ; Learning rate = 0.000251 ; Loss = 1.726701\n",
      "2024-12-15 11:57:21.164000: I runner.py:310] Step = 124100 ; steps/s = 1.62, tokens/s = 82093 (37749 source, 44344 target) ; Learning rate = 0.000251 ; Loss = 1.729260\n",
      "2024-12-15 11:58:22.728000: I runner.py:310] Step = 124200 ; steps/s = 1.62, tokens/s = 82081 (37756 source, 44325 target) ; Learning rate = 0.000251 ; Loss = 1.719705\n",
      "2024-12-15 11:59:23.966000: I runner.py:310] Step = 124300 ; steps/s = 1.63, tokens/s = 81168 (37332 source, 43836 target) ; Learning rate = 0.000251 ; Loss = 1.729789\n",
      "2024-12-15 12:00:25.586000: I runner.py:310] Step = 124400 ; steps/s = 1.62, tokens/s = 82008 (37733 source, 44275 target) ; Learning rate = 0.000251 ; Loss = 1.707768\n",
      "2024-12-15 12:01:27.242000: I runner.py:310] Step = 124500 ; steps/s = 1.62, tokens/s = 81962 (37689 source, 44273 target) ; Learning rate = 0.000251 ; Loss = 1.706946\n",
      "2024-12-15 12:02:28.486000: I runner.py:310] Step = 124600 ; steps/s = 1.63, tokens/s = 81136 (37304 source, 43832 target) ; Learning rate = 0.000250 ; Loss = 1.695462\n",
      "2024-12-15 12:03:30.148000: I runner.py:310] Step = 124700 ; steps/s = 1.62, tokens/s = 81948 (37693 source, 44255 target) ; Learning rate = 0.000250 ; Loss = 1.719787\n",
      "2024-12-15 12:04:31.720000: I runner.py:310] Step = 124800 ; steps/s = 1.62, tokens/s = 82099 (37773 source, 44326 target) ; Learning rate = 0.000250 ; Loss = 1.730494\n",
      "2024-12-15 12:05:33.386000: I runner.py:310] Step = 124900 ; steps/s = 1.62, tokens/s = 81947 (37680 source, 44267 target) ; Learning rate = 0.000250 ; Loss = 1.730577\n",
      "2024-12-15 12:06:34.619000: I runner.py:310] Step = 125000 ; steps/s = 1.63, tokens/s = 81189 (37347 source, 43842 target) ; Learning rate = 0.000250 ; Loss = 1.700330\n",
      "2024-12-15 12:06:34.621000: I training.py:192] Running evaluation for step 125000\n",
      "2024-12-15 12:08:38.624000: I training.py:192] Evaluation result for step 125000: loss = 2.664947 ; perplexity = 14.367192\n",
      "2024-12-15 12:09:40.105000: I runner.py:310] Step = 125100 ; steps/s = 1.63, tokens/s = 82214 (37809 source, 44405 target) ; Learning rate = 0.000250 ; Loss = 1.733053\n",
      "2024-12-15 12:10:41.655000: I runner.py:310] Step = 125200 ; steps/s = 1.62, tokens/s = 82078 (37749 source, 44329 target) ; Learning rate = 0.000250 ; Loss = 1.739854\n",
      "2024-12-15 12:11:42.909000: I runner.py:310] Step = 125300 ; steps/s = 1.63, tokens/s = 81126 (37307 source, 43819 target) ; Learning rate = 0.000250 ; Loss = 1.725889\n",
      "2024-12-15 12:12:44.497000: I runner.py:310] Step = 125400 ; steps/s = 1.62, tokens/s = 82055 (37735 source, 44320 target) ; Learning rate = 0.000250 ; Loss = 1.708526\n",
      "2024-12-15 12:13:46.149000: I runner.py:310] Step = 125500 ; steps/s = 1.62, tokens/s = 81966 (37696 source, 44270 target) ; Learning rate = 0.000250 ; Loss = 1.717702\n",
      "2024-12-15 12:14:47.692000: I runner.py:310] Step = 125600 ; steps/s = 1.63, tokens/s = 81617 (37549 source, 44068 target) ; Learning rate = 0.000249 ; Loss = 1.709842\n",
      "2024-12-15 12:15:49.015000: I runner.py:310] Step = 125700 ; steps/s = 1.63, tokens/s = 81560 (37500 source, 44060 target) ; Learning rate = 0.000249 ; Loss = 1.718432\n",
      "2024-12-15 12:16:50.686000: I runner.py:310] Step = 125800 ; steps/s = 1.62, tokens/s = 81961 (37712 source, 44249 target) ; Learning rate = 0.000249 ; Loss = 1.710548\n",
      "2024-12-15 12:17:52.295000: I runner.py:310] Step = 125900 ; steps/s = 1.62, tokens/s = 82054 (37761 source, 44293 target) ; Learning rate = 0.000249 ; Loss = 1.723748\n",
      "2024-12-15 12:18:53.483000: I runner.py:310] Step = 126000 ; steps/s = 1.63, tokens/s = 81204 (37331 source, 43873 target) ; Learning rate = 0.000249 ; Loss = 1.698572\n",
      "2024-12-15 12:19:55.070000: I runner.py:310] Step = 126100 ; steps/s = 1.62, tokens/s = 82058 (37746 source, 44312 target) ; Learning rate = 0.000249 ; Loss = 1.708577\n",
      "2024-12-15 12:20:56.732000: I runner.py:310] Step = 126200 ; steps/s = 1.62, tokens/s = 81962 (37687 source, 44275 target) ; Learning rate = 0.000249 ; Loss = 1.724278\n",
      "2024-12-15 12:21:57.963000: I runner.py:310] Step = 126300 ; steps/s = 1.63, tokens/s = 81154 (37329 source, 43825 target) ; Learning rate = 0.000249 ; Loss = 1.709238\n",
      "2024-12-15 12:22:59.562000: I runner.py:310] Step = 126400 ; steps/s = 1.62, tokens/s = 82011 (37704 source, 44307 target) ; Learning rate = 0.000249 ; Loss = 1.717728\n",
      "2024-12-15 12:24:01.144000: I runner.py:310] Step = 126500 ; steps/s = 1.62, tokens/s = 82063 (37751 source, 44312 target) ; Learning rate = 0.000249 ; Loss = 1.701049\n",
      "2024-12-15 12:25:02.765000: I runner.py:310] Step = 126600 ; steps/s = 1.62, tokens/s = 82019 (37733 source, 44286 target) ; Learning rate = 0.000248 ; Loss = 1.709012\n",
      "2024-12-15 12:26:03.994000: I runner.py:310] Step = 126700 ; steps/s = 1.63, tokens/s = 81175 (37332 source, 43843 target) ; Learning rate = 0.000248 ; Loss = 1.689219\n",
      "2024-12-15 12:27:05.562000: I runner.py:310] Step = 126800 ; steps/s = 1.62, tokens/s = 82098 (37757 source, 44341 target) ; Learning rate = 0.000248 ; Loss = 1.726000\n",
      "2024-12-15 12:28:07.199000: I runner.py:310] Step = 126900 ; steps/s = 1.62, tokens/s = 81993 (37706 source, 44287 target) ; Learning rate = 0.000248 ; Loss = 1.725564\n",
      "2024-12-15 12:29:08.426000: I runner.py:310] Step = 127000 ; steps/s = 1.63, tokens/s = 81158 (37335 source, 43823 target) ; Learning rate = 0.000248 ; Loss = 1.712872\n",
      "2024-12-15 12:30:10.005000: I runner.py:310] Step = 127100 ; steps/s = 1.62, tokens/s = 82082 (37767 source, 44315 target) ; Learning rate = 0.000248 ; Loss = 1.694678\n",
      "2024-12-15 12:31:11.643000: I runner.py:310] Step = 127200 ; steps/s = 1.62, tokens/s = 82013 (37733 source, 44280 target) ; Learning rate = 0.000248 ; Loss = 1.718303\n",
      "2024-12-15 12:32:13.316000: I runner.py:310] Step = 127300 ; steps/s = 1.62, tokens/s = 81921 (37663 source, 44258 target) ; Learning rate = 0.000248 ; Loss = 1.718662\n",
      "2024-12-15 12:33:14.590000: I runner.py:310] Step = 127400 ; steps/s = 1.63, tokens/s = 81124 (37308 source, 43816 target) ; Learning rate = 0.000248 ; Loss = 1.702340\n",
      "2024-12-15 12:34:16.208000: I runner.py:310] Step = 127500 ; steps/s = 1.62, tokens/s = 82001 (37721 source, 44280 target) ; Learning rate = 0.000248 ; Loss = 1.728254\n",
      "2024-12-15 12:35:17.817000: I runner.py:310] Step = 127600 ; steps/s = 1.62, tokens/s = 82032 (37733 source, 44299 target) ; Learning rate = 0.000247 ; Loss = 1.714931\n",
      "2024-12-15 12:36:19.012000: I runner.py:310] Step = 127700 ; steps/s = 1.63, tokens/s = 81205 (37335 source, 43870 target) ; Learning rate = 0.000247 ; Loss = 1.721380\n",
      "2024-12-15 12:37:20.672000: I runner.py:310] Step = 127800 ; steps/s = 1.62, tokens/s = 81968 (37708 source, 44260 target) ; Learning rate = 0.000247 ; Loss = 1.705350\n",
      "2024-12-15 12:38:22.277000: I runner.py:310] Step = 127900 ; steps/s = 1.62, tokens/s = 82036 (37733 source, 44303 target) ; Learning rate = 0.000247 ; Loss = 1.722461\n",
      "2024-12-15 12:39:23.916000: I runner.py:310] Step = 128000 ; steps/s = 1.62, tokens/s = 81983 (37703 source, 44280 target) ; Learning rate = 0.000247 ; Loss = 1.720687\n",
      "2024-12-15 12:40:25.108000: I runner.py:310] Step = 128100 ; steps/s = 1.63, tokens/s = 81226 (37355 source, 43871 target) ; Learning rate = 0.000247 ; Loss = 1.692392\n",
      "2024-12-15 12:41:26.728000: I runner.py:310] Step = 128200 ; steps/s = 1.62, tokens/s = 82046 (37756 source, 44290 target) ; Learning rate = 0.000247 ; Loss = 1.712777\n",
      "2024-12-15 12:42:28.419000: I runner.py:310] Step = 128300 ; steps/s = 1.62, tokens/s = 81885 (37645 source, 44240 target) ; Learning rate = 0.000247 ; Loss = 1.717450\n",
      "2024-12-15 12:43:29.606000: I runner.py:310] Step = 128400 ; steps/s = 1.63, tokens/s = 81205 (37335 source, 43870 target) ; Learning rate = 0.000247 ; Loss = 1.700020\n",
      "2024-12-15 12:44:31.253000: I runner.py:310] Step = 128500 ; steps/s = 1.62, tokens/s = 81987 (37710 source, 44277 target) ; Learning rate = 0.000247 ; Loss = 1.715628\n",
      "2024-12-15 12:45:32.916000: I runner.py:310] Step = 128600 ; steps/s = 1.62, tokens/s = 81955 (37700 source, 44255 target) ; Learning rate = 0.000246 ; Loss = 1.717579\n",
      "2024-12-15 12:46:34.527000: I runner.py:310] Step = 128700 ; steps/s = 1.62, tokens/s = 82034 (37728 source, 44306 target) ; Learning rate = 0.000246 ; Loss = 1.725184\n",
      "2024-12-15 12:47:35.788000: I runner.py:310] Step = 128800 ; steps/s = 1.63, tokens/s = 81125 (37320 source, 43805 target) ; Learning rate = 0.000246 ; Loss = 1.698684\n",
      "2024-12-15 12:48:37.351000: I runner.py:310] Step = 128900 ; steps/s = 1.62, tokens/s = 82093 (37753 source, 44340 target) ; Learning rate = 0.000246 ; Loss = 1.722492\n",
      "2024-12-15 12:49:38.968000: I runner.py:310] Step = 129000 ; steps/s = 1.62, tokens/s = 81980 (37693 source, 44287 target) ; Learning rate = 0.000246 ; Loss = 1.718216\n",
      "2024-12-15 12:50:40.154000: I runner.py:310] Step = 129100 ; steps/s = 1.63, tokens/s = 81248 (37380 source, 43868 target) ; Learning rate = 0.000246 ; Loss = 1.713309\n",
      "2024-12-15 12:51:41.758000: I runner.py:310] Step = 129200 ; steps/s = 1.62, tokens/s = 82035 (37725 source, 44310 target) ; Learning rate = 0.000246 ; Loss = 1.699281\n",
      "2024-12-15 12:52:43.387000: I runner.py:310] Step = 129300 ; steps/s = 1.62, tokens/s = 81986 (37709 source, 44277 target) ; Learning rate = 0.000246 ; Loss = 1.705094\n",
      "2024-12-15 12:53:45.001000: I runner.py:310] Step = 129400 ; steps/s = 1.62, tokens/s = 82028 (37731 source, 44297 target) ; Learning rate = 0.000246 ; Loss = 1.733004\n",
      "2024-12-15 12:54:46.185000: I runner.py:310] Step = 129500 ; steps/s = 1.63, tokens/s = 81259 (37388 source, 43871 target) ; Learning rate = 0.000246 ; Loss = 1.693959\n",
      "2024-12-15 12:55:47.832000: I runner.py:310] Step = 129600 ; steps/s = 1.62, tokens/s = 81971 (37698 source, 44273 target) ; Learning rate = 0.000246 ; Loss = 1.708243\n",
      "2024-12-15 12:56:49.447000: I runner.py:310] Step = 129700 ; steps/s = 1.62, tokens/s = 82002 (37704 source, 44298 target) ; Learning rate = 0.000245 ; Loss = 1.714667\n",
      "2024-12-15 12:57:50.654000: I runner.py:310] Step = 129800 ; steps/s = 1.63, tokens/s = 81201 (37351 source, 43850 target) ; Learning rate = 0.000245 ; Loss = 1.716877\n",
      "2024-12-15 12:58:52.248000: I runner.py:310] Step = 129900 ; steps/s = 1.62, tokens/s = 82067 (37748 source, 44319 target) ; Learning rate = 0.000245 ; Loss = 1.709805\n",
      "2024-12-15 12:59:53.850000: I runner.py:310] Step = 130000 ; steps/s = 1.62, tokens/s = 82044 (37747 source, 44297 target) ; Learning rate = 0.000245 ; Loss = 1.703108\n",
      "2024-12-15 12:59:55.796000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-130000\n",
      "2024-12-15 12:59:55.796000: I training.py:192] Running evaluation for step 130000\n",
      "2024-12-15 13:01:59.498000: I training.py:192] Evaluation result for step 130000: loss = 2.671277 ; perplexity = 14.458425\n",
      "2024-12-15 13:03:00.966000: I runner.py:310] Step = 130100 ; steps/s = 1.63, tokens/s = 82210 (37802 source, 44408 target) ; Learning rate = 0.000245 ; Loss = 1.712213\n",
      "2024-12-15 13:04:02.198000: I runner.py:310] Step = 130200 ; steps/s = 1.63, tokens/s = 81182 (37337 source, 43845 target) ; Learning rate = 0.000245 ; Loss = 1.691715\n",
      "2024-12-15 13:05:03.802000: I runner.py:310] Step = 130300 ; steps/s = 1.62, tokens/s = 82016 (37710 source, 44306 target) ; Learning rate = 0.000245 ; Loss = 1.712002\n",
      "2024-12-15 13:06:05.349000: I runner.py:310] Step = 130400 ; steps/s = 1.62, tokens/s = 82119 (37778 source, 44341 target) ; Learning rate = 0.000245 ; Loss = 1.725538\n",
      "2024-12-15 13:07:06.560000: I runner.py:310] Step = 130500 ; steps/s = 1.63, tokens/s = 81204 (37349 source, 43855 target) ; Learning rate = 0.000245 ; Loss = 1.718290\n",
      "2024-12-15 13:08:08.213000: I runner.py:310] Step = 130600 ; steps/s = 1.62, tokens/s = 81974 (37691 source, 44283 target) ; Learning rate = 0.000245 ; Loss = 1.704076\n",
      "2024-12-15 13:09:09.868000: I runner.py:310] Step = 130700 ; steps/s = 1.62, tokens/s = 81958 (37696 source, 44262 target) ; Learning rate = 0.000244 ; Loss = 1.708645\n",
      "2024-12-15 13:10:11.516000: I runner.py:310] Step = 130800 ; steps/s = 1.62, tokens/s = 81988 (37722 source, 44266 target) ; Learning rate = 0.000244 ; Loss = 1.701542\n",
      "2024-12-15 13:11:12.749000: I runner.py:310] Step = 130900 ; steps/s = 1.63, tokens/s = 81175 (37328 source, 43847 target) ; Learning rate = 0.000244 ; Loss = 1.696930\n",
      "2024-12-15 13:12:14.384000: I runner.py:310] Step = 131000 ; steps/s = 1.62, tokens/s = 81991 (37706 source, 44285 target) ; Learning rate = 0.000244 ; Loss = 1.719871\n",
      "2024-12-15 13:13:16.052000: I runner.py:310] Step = 131100 ; steps/s = 1.62, tokens/s = 81951 (37694 source, 44257 target) ; Learning rate = 0.000244 ; Loss = 1.723529\n",
      "2024-12-15 13:14:17.223000: I runner.py:310] Step = 131200 ; steps/s = 1.64, tokens/s = 81261 (37385 source, 43876 target) ; Learning rate = 0.000244 ; Loss = 1.726552\n",
      "2024-12-15 13:15:18.874000: I runner.py:310] Step = 131300 ; steps/s = 1.62, tokens/s = 81962 (37692 source, 44270 target) ; Learning rate = 0.000244 ; Loss = 1.718419\n",
      "2024-12-15 13:16:20.495000: I runner.py:310] Step = 131400 ; steps/s = 1.62, tokens/s = 82002 (37710 source, 44292 target) ; Learning rate = 0.000244 ; Loss = 1.702214\n",
      "2024-12-15 13:17:22.109000: I runner.py:310] Step = 131500 ; steps/s = 1.62, tokens/s = 82018 (37724 source, 44294 target) ; Learning rate = 0.000244 ; Loss = 1.693534\n",
      "2024-12-15 13:18:23.333000: I runner.py:310] Step = 131600 ; steps/s = 1.63, tokens/s = 81203 (37338 source, 43865 target) ; Learning rate = 0.000244 ; Loss = 1.697106\n",
      "2024-12-15 13:19:24.929000: I runner.py:310] Step = 131700 ; steps/s = 1.62, tokens/s = 82035 (37719 source, 44316 target) ; Learning rate = 0.000244 ; Loss = 1.713513\n",
      "2024-12-15 13:20:26.562000: I runner.py:310] Step = 131800 ; steps/s = 1.62, tokens/s = 81975 (37721 source, 44254 target) ; Learning rate = 0.000243 ; Loss = 1.712914\n",
      "2024-12-15 13:21:27.764000: I runner.py:310] Step = 131900 ; steps/s = 1.63, tokens/s = 81243 (37386 source, 43857 target) ; Learning rate = 0.000243 ; Loss = 1.696631\n",
      "2024-12-15 13:22:29.391000: I runner.py:310] Step = 132000 ; steps/s = 1.62, tokens/s = 81976 (37681 source, 44295 target) ; Learning rate = 0.000243 ; Loss = 1.713398\n",
      "2024-12-15 13:23:30.984000: I runner.py:310] Step = 132100 ; steps/s = 1.62, tokens/s = 82043 (37731 source, 44312 target) ; Learning rate = 0.000243 ; Loss = 1.719163\n",
      "2024-12-15 13:24:32.708000: I runner.py:310] Step = 132200 ; steps/s = 1.62, tokens/s = 81889 (37681 source, 44208 target) ; Learning rate = 0.000243 ; Loss = 1.719406\n",
      "2024-12-15 13:25:33.968000: I runner.py:310] Step = 132300 ; steps/s = 1.63, tokens/s = 81127 (37319 source, 43808 target) ; Learning rate = 0.000243 ; Loss = 1.691976\n",
      "2024-12-15 13:26:35.520000: I runner.py:310] Step = 132400 ; steps/s = 1.62, tokens/s = 82097 (37758 source, 44339 target) ; Learning rate = 0.000243 ; Loss = 1.711233\n",
      "2024-12-15 13:27:37.156000: I runner.py:310] Step = 132500 ; steps/s = 1.62, tokens/s = 82005 (37710 source, 44295 target) ; Learning rate = 0.000243 ; Loss = 1.711996\n",
      "2024-12-15 13:28:38.359000: I runner.py:310] Step = 132600 ; steps/s = 1.63, tokens/s = 81196 (37342 source, 43854 target) ; Learning rate = 0.000243 ; Loss = 1.725216\n",
      "2024-12-15 13:29:40.016000: I runner.py:310] Step = 132700 ; steps/s = 1.62, tokens/s = 81958 (37697 source, 44261 target) ; Learning rate = 0.000243 ; Loss = 1.696589\n",
      "2024-12-15 13:30:41.668000: I runner.py:310] Step = 132800 ; steps/s = 1.62, tokens/s = 81979 (37710 source, 44269 target) ; Learning rate = 0.000243 ; Loss = 1.704500\n",
      "2024-12-15 13:31:43.313000: I runner.py:310] Step = 132900 ; steps/s = 1.62, tokens/s = 81983 (37705 source, 44278 target) ; Learning rate = 0.000242 ; Loss = 1.714530\n",
      "2024-12-15 13:32:44.515000: I runner.py:310] Step = 133000 ; steps/s = 1.63, tokens/s = 81212 (37346 source, 43866 target) ; Learning rate = 0.000242 ; Loss = 1.700559\n",
      "2024-12-15 13:33:46.253000: I runner.py:310] Step = 133100 ; steps/s = 1.62, tokens/s = 81859 (37655 source, 44204 target) ; Learning rate = 0.000242 ; Loss = 1.711126\n",
      "2024-12-15 13:34:47.911000: I runner.py:310] Step = 133200 ; steps/s = 1.62, tokens/s = 81965 (37697 source, 44268 target) ; Learning rate = 0.000242 ; Loss = 1.728546\n",
      "2024-12-15 13:35:49.175000: I runner.py:310] Step = 133300 ; steps/s = 1.63, tokens/s = 81132 (37322 source, 43810 target) ; Learning rate = 0.000242 ; Loss = 1.689789\n",
      "2024-12-15 13:36:50.751000: I runner.py:310] Step = 133400 ; steps/s = 1.62, tokens/s = 82080 (37766 source, 44314 target) ; Learning rate = 0.000242 ; Loss = 1.712782\n",
      "2024-12-15 13:37:52.393000: I runner.py:310] Step = 133500 ; steps/s = 1.62, tokens/s = 81964 (37686 source, 44278 target) ; Learning rate = 0.000242 ; Loss = 1.718656\n",
      "2024-12-15 13:38:54.064000: I runner.py:310] Step = 133600 ; steps/s = 1.62, tokens/s = 81938 (37680 source, 44258 target) ; Learning rate = 0.000242 ; Loss = 1.722368\n",
      "2024-12-15 13:39:55.259000: I runner.py:310] Step = 133700 ; steps/s = 1.63, tokens/s = 81230 (37356 source, 43874 target) ; Learning rate = 0.000242 ; Loss = 1.704271\n",
      "2024-12-15 13:40:56.907000: I runner.py:310] Step = 133800 ; steps/s = 1.62, tokens/s = 81974 (37703 source, 44271 target) ; Learning rate = 0.000242 ; Loss = 1.722082\n",
      "2024-12-15 13:41:58.541000: I runner.py:310] Step = 133900 ; steps/s = 1.62, tokens/s = 81983 (37713 source, 44270 target) ; Learning rate = 0.000242 ; Loss = 1.711434\n",
      "2024-12-15 13:42:59.695000: I runner.py:310] Step = 134000 ; steps/s = 1.64, tokens/s = 81274 (37368 source, 43906 target) ; Learning rate = 0.000241 ; Loss = 1.728233\n",
      "2024-12-15 13:44:01.301000: I runner.py:310] Step = 134100 ; steps/s = 1.62, tokens/s = 82023 (37726 source, 44297 target) ; Learning rate = 0.000241 ; Loss = 1.693837\n",
      "2024-12-15 13:45:02.933000: I runner.py:310] Step = 134200 ; steps/s = 1.62, tokens/s = 82008 (37723 source, 44285 target) ; Learning rate = 0.000241 ; Loss = 1.708309\n",
      "2024-12-15 13:46:04.206000: I runner.py:310] Step = 134300 ; steps/s = 1.63, tokens/s = 81259 (37381 source, 43878 target) ; Learning rate = 0.000241 ; Loss = 1.757059\n",
      "2024-12-15 13:47:05.745000: I runner.py:310] Step = 134400 ; steps/s = 1.63, tokens/s = 82013 (37754 source, 44259 target) ; Learning rate = 0.000241 ; Loss = 1.699685\n",
      "2024-12-15 13:48:07.391000: I runner.py:310] Step = 134500 ; steps/s = 1.62, tokens/s = 81970 (37694 source, 44276 target) ; Learning rate = 0.000241 ; Loss = 1.705371\n",
      "2024-12-15 13:49:09.070000: I runner.py:310] Step = 134600 ; steps/s = 1.62, tokens/s = 81928 (37663 source, 44265 target) ; Learning rate = 0.000241 ; Loss = 1.712534\n",
      "2024-12-15 13:50:10.317000: I runner.py:310] Step = 134700 ; steps/s = 1.63, tokens/s = 81125 (37299 source, 43826 target) ; Learning rate = 0.000241 ; Loss = 1.682750\n",
      "2024-12-15 13:51:11.929000: I runner.py:310] Step = 134800 ; steps/s = 1.62, tokens/s = 82034 (37725 source, 44309 target) ; Learning rate = 0.000241 ; Loss = 1.708085\n",
      "2024-12-15 13:52:13.516000: I runner.py:310] Step = 134900 ; steps/s = 1.62, tokens/s = 82059 (37750 source, 44309 target) ; Learning rate = 0.000241 ; Loss = 1.716945\n",
      "2024-12-15 13:53:14.693000: I runner.py:310] Step = 135000 ; steps/s = 1.63, tokens/s = 81234 (37364 source, 43870 target) ; Learning rate = 0.000241 ; Loss = 1.698382\n",
      "2024-12-15 13:53:14.694000: I training.py:192] Running evaluation for step 135000\n",
      "2024-12-15 13:55:15.890000: I training.py:192] Evaluation result for step 135000: loss = 2.672063 ; perplexity = 14.469794\n",
      "2024-12-15 13:56:17.351000: I runner.py:310] Step = 135100 ; steps/s = 1.63, tokens/s = 82244 (37824 source, 44420 target) ; Learning rate = 0.000240 ; Loss = 1.705319\n",
      "2024-12-15 13:57:18.954000: I runner.py:310] Step = 135200 ; steps/s = 1.62, tokens/s = 82033 (37727 source, 44306 target) ; Learning rate = 0.000240 ; Loss = 1.708320\n",
      "2024-12-15 13:58:20.596000: I runner.py:310] Step = 135300 ; steps/s = 1.62, tokens/s = 81986 (37720 source, 44266 target) ; Learning rate = 0.000240 ; Loss = 1.715730\n",
      "2024-12-15 13:59:21.901000: I runner.py:310] Step = 135400 ; steps/s = 1.63, tokens/s = 81061 (37282 source, 43779 target) ; Learning rate = 0.000240 ; Loss = 1.720518\n",
      "2024-12-15 14:00:23.539000: I runner.py:310] Step = 135500 ; steps/s = 1.62, tokens/s = 81978 (37682 source, 44296 target) ; Learning rate = 0.000240 ; Loss = 1.699156\n",
      "2024-12-15 14:01:25.219000: I runner.py:310] Step = 135600 ; steps/s = 1.62, tokens/s = 81938 (37709 source, 44229 target) ; Learning rate = 0.000240 ; Loss = 1.698719\n",
      "2024-12-15 14:02:26.425000: I runner.py:310] Step = 135700 ; steps/s = 1.63, tokens/s = 81214 (37347 source, 43867 target) ; Learning rate = 0.000240 ; Loss = 1.719417\n",
      "2024-12-15 14:03:28.069000: I runner.py:310] Step = 135800 ; steps/s = 1.62, tokens/s = 81996 (37722 source, 44274 target) ; Learning rate = 0.000240 ; Loss = 1.705173\n",
      "2024-12-15 14:04:29.705000: I runner.py:310] Step = 135900 ; steps/s = 1.62, tokens/s = 81987 (37698 source, 44289 target) ; Learning rate = 0.000240 ; Loss = 1.687911\n",
      "2024-12-15 14:05:31.320000: I runner.py:310] Step = 136000 ; steps/s = 1.62, tokens/s = 82012 (37712 source, 44300 target) ; Learning rate = 0.000240 ; Loss = 1.708586\n",
      "2024-12-15 14:06:32.557000: I runner.py:310] Step = 136100 ; steps/s = 1.63, tokens/s = 81169 (37343 source, 43826 target) ; Learning rate = 0.000240 ; Loss = 1.684376\n",
      "2024-12-15 14:07:34.184000: I runner.py:310] Step = 136200 ; steps/s = 1.62, tokens/s = 82020 (37738 source, 44282 target) ; Learning rate = 0.000239 ; Loss = 1.715582\n",
      "2024-12-15 14:08:35.868000: I runner.py:310] Step = 136300 ; steps/s = 1.62, tokens/s = 81918 (37676 source, 44242 target) ; Learning rate = 0.000239 ; Loss = 1.717710\n",
      "2024-12-15 14:09:37.079000: I runner.py:310] Step = 136400 ; steps/s = 1.63, tokens/s = 81182 (37327 source, 43855 target) ; Learning rate = 0.000239 ; Loss = 1.701689\n",
      "2024-12-15 14:10:38.727000: I runner.py:310] Step = 136500 ; steps/s = 1.62, tokens/s = 81922 (37655 source, 44267 target) ; Learning rate = 0.000239 ; Loss = 1.703850\n",
      "2024-12-15 14:11:40.405000: I runner.py:310] Step = 136600 ; steps/s = 1.62, tokens/s = 81966 (37708 source, 44258 target) ; Learning rate = 0.000239 ; Loss = 1.703720\n",
      "2024-12-15 14:12:42.077000: I runner.py:310] Step = 136700 ; steps/s = 1.62, tokens/s = 81958 (37707 source, 44251 target) ; Learning rate = 0.000239 ; Loss = 1.704286\n",
      "2024-12-15 14:13:43.306000: I runner.py:310] Step = 136800 ; steps/s = 1.63, tokens/s = 81177 (37334 source, 43843 target) ; Learning rate = 0.000239 ; Loss = 1.710807\n",
      "2024-12-15 14:14:44.933000: I runner.py:310] Step = 136900 ; steps/s = 1.62, tokens/s = 82016 (37717 source, 44299 target) ; Learning rate = 0.000239 ; Loss = 1.707591\n",
      "2024-12-15 14:15:46.561000: I runner.py:310] Step = 137000 ; steps/s = 1.62, tokens/s = 81996 (37714 source, 44282 target) ; Learning rate = 0.000239 ; Loss = 1.711652\n",
      "2024-12-15 14:16:47.758000: I runner.py:310] Step = 137100 ; steps/s = 1.63, tokens/s = 81225 (37370 source, 43855 target) ; Learning rate = 0.000239 ; Loss = 1.717138\n",
      "2024-12-15 14:17:49.317000: I runner.py:310] Step = 137200 ; steps/s = 1.62, tokens/s = 82085 (37745 source, 44340 target) ; Learning rate = 0.000239 ; Loss = 1.706259\n",
      "2024-12-15 14:18:50.956000: I runner.py:310] Step = 137300 ; steps/s = 1.62, tokens/s = 81997 (37723 source, 44274 target) ; Learning rate = 0.000239 ; Loss = 1.698208\n",
      "2024-12-15 14:19:52.657000: I runner.py:310] Step = 137400 ; steps/s = 1.62, tokens/s = 81898 (37662 source, 44236 target) ; Learning rate = 0.000238 ; Loss = 1.704341\n",
      "2024-12-15 14:20:53.942000: I runner.py:310] Step = 137500 ; steps/s = 1.63, tokens/s = 81086 (37295 source, 43791 target) ; Learning rate = 0.000238 ; Loss = 1.708875\n",
      "2024-12-15 14:21:55.582000: I runner.py:310] Step = 137600 ; steps/s = 1.62, tokens/s = 81996 (37713 source, 44283 target) ; Learning rate = 0.000238 ; Loss = 1.703184\n",
      "2024-12-15 14:22:57.147000: I runner.py:310] Step = 137700 ; steps/s = 1.62, tokens/s = 82080 (37745 source, 44335 target) ; Learning rate = 0.000238 ; Loss = 1.710142\n",
      "2024-12-15 14:23:58.353000: I runner.py:310] Step = 137800 ; steps/s = 1.63, tokens/s = 81218 (37368 source, 43850 target) ; Learning rate = 0.000238 ; Loss = 1.699789\n",
      "2024-12-15 14:24:59.877000: I runner.py:310] Step = 137900 ; steps/s = 1.63, tokens/s = 82161 (37782 source, 44379 target) ; Learning rate = 0.000238 ; Loss = 1.702294\n",
      "2024-12-15 14:26:01.526000: I runner.py:310] Step = 138000 ; steps/s = 1.62, tokens/s = 81946 (37683 source, 44263 target) ; Learning rate = 0.000238 ; Loss = 1.699507\n",
      "2024-12-15 14:27:03.210000: I runner.py:310] Step = 138100 ; steps/s = 1.62, tokens/s = 81920 (37667 source, 44253 target) ; Learning rate = 0.000238 ; Loss = 1.713183\n",
      "2024-12-15 14:28:04.394000: I runner.py:310] Step = 138200 ; steps/s = 1.63, tokens/s = 81227 (37368 source, 43859 target) ; Learning rate = 0.000238 ; Loss = 1.725245\n",
      "2024-12-15 14:29:06.003000: I runner.py:310] Step = 138300 ; steps/s = 1.62, tokens/s = 82034 (37730 source, 44304 target) ; Learning rate = 0.000238 ; Loss = 1.708325\n",
      "2024-12-15 14:30:07.691000: I runner.py:310] Step = 138400 ; steps/s = 1.62, tokens/s = 81914 (37680 source, 44234 target) ; Learning rate = 0.000238 ; Loss = 1.697189\n",
      "2024-12-15 14:31:08.929000: I runner.py:310] Step = 138500 ; steps/s = 1.63, tokens/s = 81182 (37335 source, 43847 target) ; Learning rate = 0.000238 ; Loss = 1.697282\n",
      "2024-12-15 14:32:10.645000: I runner.py:310] Step = 138600 ; steps/s = 1.62, tokens/s = 81885 (37666 source, 44219 target) ; Learning rate = 0.000237 ; Loss = 1.709967\n",
      "2024-12-15 14:33:12.277000: I runner.py:310] Step = 138700 ; steps/s = 1.62, tokens/s = 81999 (37717 source, 44282 target) ; Learning rate = 0.000237 ; Loss = 1.706378\n",
      "2024-12-15 14:34:13.846000: I runner.py:310] Step = 138800 ; steps/s = 1.62, tokens/s = 82083 (37761 source, 44322 target) ; Learning rate = 0.000237 ; Loss = 1.715865\n",
      "2024-12-15 14:35:15.082000: I runner.py:310] Step = 138900 ; steps/s = 1.63, tokens/s = 81171 (37329 source, 43842 target) ; Learning rate = 0.000237 ; Loss = 1.687633\n",
      "2024-12-15 14:36:16.727000: I runner.py:310] Step = 139000 ; steps/s = 1.62, tokens/s = 81996 (37712 source, 44284 target) ; Learning rate = 0.000237 ; Loss = 1.715596\n",
      "2024-12-15 14:37:18.310000: I runner.py:310] Step = 139100 ; steps/s = 1.62, tokens/s = 82052 (37738 source, 44314 target) ; Learning rate = 0.000237 ; Loss = 1.730328\n",
      "2024-12-15 14:38:19.486000: I runner.py:310] Step = 139200 ; steps/s = 1.63, tokens/s = 81230 (37363 source, 43867 target) ; Learning rate = 0.000237 ; Loss = 1.700050\n",
      "2024-12-15 14:39:21.101000: I runner.py:310] Step = 139300 ; steps/s = 1.62, tokens/s = 82026 (37738 source, 44288 target) ; Learning rate = 0.000237 ; Loss = 1.695319\n",
      "2024-12-15 14:40:22.722000: I runner.py:310] Step = 139400 ; steps/s = 1.62, tokens/s = 82018 (37720 source, 44298 target) ; Learning rate = 0.000237 ; Loss = 1.712322\n",
      "2024-12-15 14:41:24.345000: I runner.py:310] Step = 139500 ; steps/s = 1.62, tokens/s = 81998 (37702 source, 44296 target) ; Learning rate = 0.000237 ; Loss = 1.721297\n",
      "2024-12-15 14:42:25.564000: I runner.py:310] Step = 139600 ; steps/s = 1.63, tokens/s = 81180 (37342 source, 43838 target) ; Learning rate = 0.000237 ; Loss = 1.692480\n",
      "2024-12-15 14:43:27.128000: I runner.py:310] Step = 139700 ; steps/s = 1.62, tokens/s = 82089 (37764 source, 44325 target) ; Learning rate = 0.000236 ; Loss = 1.707563\n",
      "2024-12-15 14:44:28.775000: I runner.py:310] Step = 139800 ; steps/s = 1.62, tokens/s = 81982 (37702 source, 44280 target) ; Learning rate = 0.000236 ; Loss = 1.703011\n",
      "2024-12-15 14:45:29.998000: I runner.py:310] Step = 139900 ; steps/s = 1.63, tokens/s = 81178 (37331 source, 43847 target) ; Learning rate = 0.000236 ; Loss = 1.699671\n",
      "2024-12-15 14:46:31.595000: I runner.py:310] Step = 140000 ; steps/s = 1.62, tokens/s = 82069 (37753 source, 44316 target) ; Learning rate = 0.000236 ; Loss = 1.698640\n",
      "2024-12-15 14:46:33.638000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-140000\n",
      "2024-12-15 14:46:33.638000: I training.py:192] Running evaluation for step 140000\n",
      "2024-12-15 14:48:34.123000: I training.py:192] Evaluation result for step 140000: loss = 2.687541 ; perplexity = 14.695488\n",
      "2024-12-15 14:49:35.517000: I runner.py:310] Step = 140100 ; steps/s = 1.63, tokens/s = 82282 (37812 source, 44470 target) ; Learning rate = 0.000236 ; Loss = 1.706806\n",
      "2024-12-15 14:50:37.199000: I runner.py:310] Step = 140200 ; steps/s = 1.62, tokens/s = 81944 (37709 source, 44235 target) ; Learning rate = 0.000236 ; Loss = 1.708226\n",
      "2024-12-15 14:51:38.440000: I runner.py:310] Step = 140300 ; steps/s = 1.63, tokens/s = 81150 (37334 source, 43816 target) ; Learning rate = 0.000236 ; Loss = 1.722397\n",
      "2024-12-15 14:52:40.041000: I runner.py:310] Step = 140400 ; steps/s = 1.62, tokens/s = 82046 (37731 source, 44315 target) ; Learning rate = 0.000236 ; Loss = 1.701760\n",
      "2024-12-15 14:53:41.696000: I runner.py:310] Step = 140500 ; steps/s = 1.62, tokens/s = 81965 (37704 source, 44261 target) ; Learning rate = 0.000236 ; Loss = 1.693567\n",
      "2024-12-15 14:54:42.955000: I runner.py:310] Step = 140600 ; steps/s = 1.63, tokens/s = 81141 (37320 source, 43821 target) ; Learning rate = 0.000236 ; Loss = 1.715625\n",
      "2024-12-15 14:55:44.570000: I runner.py:310] Step = 140700 ; steps/s = 1.62, tokens/s = 82006 (37723 source, 44283 target) ; Learning rate = 0.000236 ; Loss = 1.691502\n",
      "2024-12-15 14:56:46.221000: I runner.py:310] Step = 140800 ; steps/s = 1.62, tokens/s = 81966 (37685 source, 44281 target) ; Learning rate = 0.000236 ; Loss = 1.703066\n",
      "2024-12-15 14:57:47.786000: I runner.py:310] Step = 140900 ; steps/s = 1.62, tokens/s = 82086 (37754 source, 44332 target) ; Learning rate = 0.000235 ; Loss = 1.702892\n",
      "2024-12-15 14:58:49.041000: I runner.py:310] Step = 141000 ; steps/s = 1.63, tokens/s = 81146 (37323 source, 43823 target) ; Learning rate = 0.000235 ; Loss = 1.723886\n",
      "2024-12-15 14:59:50.691000: I runner.py:310] Step = 141100 ; steps/s = 1.62, tokens/s = 81957 (37699 source, 44258 target) ; Learning rate = 0.000235 ; Loss = 1.687782\n",
      "2024-12-15 15:00:52.334000: I runner.py:310] Step = 141200 ; steps/s = 1.62, tokens/s = 81987 (37701 source, 44286 target) ; Learning rate = 0.000235 ; Loss = 1.703718\n",
      "2024-12-15 15:01:53.492000: I runner.py:310] Step = 141300 ; steps/s = 1.64, tokens/s = 81278 (37384 source, 43894 target) ; Learning rate = 0.000235 ; Loss = 1.691103\n",
      "2024-12-15 15:02:55.117000: I runner.py:310] Step = 141400 ; steps/s = 1.62, tokens/s = 82005 (37714 source, 44291 target) ; Learning rate = 0.000235 ; Loss = 1.708024\n",
      "2024-12-15 15:03:56.780000: I runner.py:310] Step = 141500 ; steps/s = 1.62, tokens/s = 81932 (37674 source, 44258 target) ; Learning rate = 0.000235 ; Loss = 1.711784\n",
      "2024-12-15 15:04:58.392000: I runner.py:310] Step = 141600 ; steps/s = 1.62, tokens/s = 82044 (37755 source, 44289 target) ; Learning rate = 0.000235 ; Loss = 1.711240\n",
      "2024-12-15 15:05:59.599000: I runner.py:310] Step = 141700 ; steps/s = 1.63, tokens/s = 81220 (37373 source, 43847 target) ; Learning rate = 0.000235 ; Loss = 1.707691\n",
      "2024-12-15 15:07:01.139000: I runner.py:310] Step = 141800 ; steps/s = 1.63, tokens/s = 82112 (37745 source, 44367 target) ; Learning rate = 0.000235 ; Loss = 1.694370\n",
      "2024-12-15 15:08:02.718000: I runner.py:310] Step = 141900 ; steps/s = 1.62, tokens/s = 82059 (37733 source, 44326 target) ; Learning rate = 0.000235 ; Loss = 1.701982\n",
      "2024-12-15 15:09:03.921000: I runner.py:310] Step = 142000 ; steps/s = 1.63, tokens/s = 81208 (37351 source, 43857 target) ; Learning rate = 0.000235 ; Loss = 1.682806\n",
      "2024-12-15 15:10:05.547000: I runner.py:310] Step = 142100 ; steps/s = 1.62, tokens/s = 82016 (37731 source, 44285 target) ; Learning rate = 0.000234 ; Loss = 1.699938\n",
      "2024-12-15 15:11:07.145000: I runner.py:310] Step = 142200 ; steps/s = 1.62, tokens/s = 82030 (37722 source, 44308 target) ; Learning rate = 0.000234 ; Loss = 1.706453\n",
      "2024-12-15 15:12:08.666000: I runner.py:310] Step = 142300 ; steps/s = 1.63, tokens/s = 81834 (37645 source, 44189 target) ; Learning rate = 0.000234 ; Loss = 1.714653\n",
      "2024-12-15 15:13:09.972000: I runner.py:310] Step = 142400 ; steps/s = 1.63, tokens/s = 81391 (37443 source, 43948 target) ; Learning rate = 0.000234 ; Loss = 1.692674\n",
      "2024-12-15 15:14:11.612000: I runner.py:310] Step = 142500 ; steps/s = 1.62, tokens/s = 82011 (37737 source, 44274 target) ; Learning rate = 0.000234 ; Loss = 1.711658\n",
      "2024-12-15 15:15:13.230000: I runner.py:310] Step = 142600 ; steps/s = 1.62, tokens/s = 81990 (37698 source, 44292 target) ; Learning rate = 0.000234 ; Loss = 1.715346\n",
      "2024-12-15 15:16:14.404000: I runner.py:310] Step = 142700 ; steps/s = 1.63, tokens/s = 81224 (37338 source, 43886 target) ; Learning rate = 0.000234 ; Loss = 1.673405\n",
      "2024-12-15 15:17:16.039000: I runner.py:310] Step = 142800 ; steps/s = 1.62, tokens/s = 82001 (37715 source, 44286 target) ; Learning rate = 0.000234 ; Loss = 1.711430\n",
      "2024-12-15 15:18:17.635000: I runner.py:310] Step = 142900 ; steps/s = 1.62, tokens/s = 82045 (37739 source, 44306 target) ; Learning rate = 0.000234 ; Loss = 1.693223\n",
      "2024-12-15 15:19:18.867000: I runner.py:310] Step = 143000 ; steps/s = 1.63, tokens/s = 81177 (37341 source, 43836 target) ; Learning rate = 0.000234 ; Loss = 1.699114\n",
      "2024-12-15 15:20:20.484000: I runner.py:310] Step = 143100 ; steps/s = 1.62, tokens/s = 82002 (37726 source, 44276 target) ; Learning rate = 0.000234 ; Loss = 1.709304\n",
      "2024-12-15 15:21:22.044000: I runner.py:310] Step = 143200 ; steps/s = 1.62, tokens/s = 82108 (37753 source, 44355 target) ; Learning rate = 0.000234 ; Loss = 1.691308\n",
      "2024-12-15 15:22:23.652000: I runner.py:310] Step = 143300 ; steps/s = 1.62, tokens/s = 82022 (37717 source, 44305 target) ; Learning rate = 0.000233 ; Loss = 1.705825\n",
      "2024-12-15 15:23:24.870000: I runner.py:310] Step = 143400 ; steps/s = 1.63, tokens/s = 81172 (37337 source, 43835 target) ; Learning rate = 0.000233 ; Loss = 1.715625\n",
      "2024-12-15 15:24:26.502000: I runner.py:310] Step = 143500 ; steps/s = 1.62, tokens/s = 81979 (37690 source, 44289 target) ; Learning rate = 0.000233 ; Loss = 1.699143\n",
      "2024-12-15 15:25:28.118000: I runner.py:310] Step = 143600 ; steps/s = 1.62, tokens/s = 82037 (37740 source, 44297 target) ; Learning rate = 0.000233 ; Loss = 1.706246\n",
      "2024-12-15 15:26:29.366000: I runner.py:310] Step = 143700 ; steps/s = 1.63, tokens/s = 81179 (37356 source, 43823 target) ; Learning rate = 0.000233 ; Loss = 1.702891\n",
      "2024-12-15 15:27:31.020000: I runner.py:310] Step = 143800 ; steps/s = 1.62, tokens/s = 81957 (37690 source, 44267 target) ; Learning rate = 0.000233 ; Loss = 1.692148\n",
      "2024-12-15 15:28:32.654000: I runner.py:310] Step = 143900 ; steps/s = 1.62, tokens/s = 81987 (37700 source, 44287 target) ; Learning rate = 0.000233 ; Loss = 1.709307\n",
      "2024-12-15 15:29:34.268000: I runner.py:310] Step = 144000 ; steps/s = 1.62, tokens/s = 82006 (37715 source, 44291 target) ; Learning rate = 0.000233 ; Loss = 1.718112\n",
      "2024-12-15 15:30:35.440000: I runner.py:310] Step = 144100 ; steps/s = 1.64, tokens/s = 81265 (37386 source, 43879 target) ; Learning rate = 0.000233 ; Loss = 1.671833\n",
      "2024-12-15 15:31:37.021000: I runner.py:310] Step = 144200 ; steps/s = 1.62, tokens/s = 82058 (37729 source, 44329 target) ; Learning rate = 0.000233 ; Loss = 1.689375\n",
      "2024-12-15 15:32:38.600000: I runner.py:310] Step = 144300 ; steps/s = 1.62, tokens/s = 82074 (37759 source, 44315 target) ; Learning rate = 0.000233 ; Loss = 1.707365\n",
      "2024-12-15 15:33:39.846000: I runner.py:310] Step = 144400 ; steps/s = 1.63, tokens/s = 81143 (37321 source, 43822 target) ; Learning rate = 0.000233 ; Loss = 1.698569\n",
      "2024-12-15 15:34:41.442000: I runner.py:310] Step = 144500 ; steps/s = 1.62, tokens/s = 82086 (37772 source, 44314 target) ; Learning rate = 0.000233 ; Loss = 1.690696\n",
      "2024-12-15 15:35:43.108000: I runner.py:310] Step = 144600 ; steps/s = 1.62, tokens/s = 81949 (37692 source, 44257 target) ; Learning rate = 0.000232 ; Loss = 1.691779\n",
      "2024-12-15 15:36:44.780000: I runner.py:310] Step = 144700 ; steps/s = 1.62, tokens/s = 81904 (37657 source, 44247 target) ; Learning rate = 0.000232 ; Loss = 1.726191\n",
      "2024-12-15 15:37:45.915000: I runner.py:310] Step = 144800 ; steps/s = 1.64, tokens/s = 81312 (37389 source, 43923 target) ; Learning rate = 0.000232 ; Loss = 1.672308\n",
      "2024-12-15 15:38:47.571000: I runner.py:310] Step = 144900 ; steps/s = 1.62, tokens/s = 81971 (37716 source, 44255 target) ; Learning rate = 0.000232 ; Loss = 1.705926\n",
      "2024-12-15 15:39:49.168000: I runner.py:310] Step = 145000 ; steps/s = 1.62, tokens/s = 82024 (37715 source, 44309 target) ; Learning rate = 0.000232 ; Loss = 1.700645\n",
      "2024-12-15 15:39:49.170000: I training.py:192] Running evaluation for step 145000\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Tr-En (TED2020) -> Kk-En\n",
    "!onmt-main --model kk-tr-en-shared.py --config tr-en-shared-vocab.yml --auto_config train --with_eval --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dce6303-d475-469f-b1c2-08b07101a785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-15 16:15:15.056020: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 16:15:15.850608: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-15 16:15:15.850678: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-15 16:15:15.850686: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-15 16:15:16.832000: I onmt-main:8] Creating model directory TR-KK-EN-Shared-vocab\n",
      "2024-12-15 16:15:17.036000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-15 16:15:17.036000: I main.py:315] Using model:\n",
      "(model): MyCustomTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): WordEmbedder()\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): WordEmbedder()\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-15 16:15:17.039711: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 16:15:18.653592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-15 16:15:18.654274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7750 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-12-15 16:15:18.654768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 6099 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "2024-12-15 16:15:18.658000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file: KK_tokens_valid_shared\n",
      "  eval_labels_file: KK_valid_target_tokens_shared.txt\n",
      "  source_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file: KK_tokens_train_shared\n",
      "  train_labels_file: KK_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: TR-KK-EN-Shared-vocab\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 250000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-15 16:15:18.991000: I inputter.py:316] Initialized source input layer:\n",
      "2024-12-15 16:15:18.992000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-15 16:15:18.992000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-15 16:15:19.064000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-15 16:15:19.064000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-15 16:15:19.064000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-15 16:15:19.087000: I runner.py:269] Restored checkpoint TR-EN-Shared-vocab/ckpt-100000\n",
      "2024-12-15 16:15:19.089000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "2024-12-15 16:15:19.134000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-15 16:15:20.049373: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-15 16:15:20.177000: I main.py:325] Accumulate gradients of 7 iterations to reach effective batch size of 25000\n",
      "2024-12-15 16:15:20.300000: I mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2024-12-15 16:15:20.514000: I dataset_ops.py:2542] Training on 318032 examples\n",
      "2024-12-15 16:16:26.461500: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-15 16:16:27.433340: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-15 16:16:27.750812: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-15 16:16:36.610000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-15 16:16:36.630000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-15 16:16:38.183000: I cross_device_ops.py:897] batch_all_reduce: 260 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-15 16:16:42.564000: I cross_device_ops.py:897] batch_all_reduce: 260 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-15 16:16:49.766000: I runner.py:310] Number of model parameters: 93326081\n",
      "2024-12-15 16:16:49.771000: I runner.py:310] Number of model weights: 260 (trainable = 260, non trainable = 0)\n",
      "2024-12-15 16:16:49.808000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-15 16:16:49.814000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-15 16:16:51.868000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-1\n",
      "2024-12-15 16:16:52.441000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-15 16:16:52.463000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-15 16:16:53.078000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-15 16:16:53.098000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-15 16:16:53.693000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-15 16:16:53.714000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-15 16:17:53.368000: I runner.py:310] Step = 100 ; steps/s = 1.61, tokens/s = 82338 (39646 source, 42692 target) ; Learning rate = 0.000009 ; Loss = 8.244104\n",
      "2024-12-15 16:18:55.258000: I runner.py:310] Step = 200 ; steps/s = 1.62, tokens/s = 82465 (39737 source, 42728 target) ; Learning rate = 0.000018 ; Loss = 6.924087\n",
      "2024-12-15 16:19:56.825000: I runner.py:310] Step = 300 ; steps/s = 1.62, tokens/s = 82905 (39952 source, 42953 target) ; Learning rate = 0.000027 ; Loss = 6.518500\n",
      "2024-12-15 16:20:58.164000: I runner.py:310] Step = 400 ; steps/s = 1.63, tokens/s = 81639 (39345 source, 42294 target) ; Learning rate = 0.000035 ; Loss = 6.212160\n",
      "2024-12-15 16:21:59.792000: I runner.py:310] Step = 500 ; steps/s = 1.62, tokens/s = 82811 (39875 source, 42936 target) ; Learning rate = 0.000044 ; Loss = 5.985974\n",
      "2024-12-15 16:23:01.402000: I runner.py:310] Step = 600 ; steps/s = 1.62, tokens/s = 82845 (39929 source, 42916 target) ; Learning rate = 0.000053 ; Loss = 5.757101\n",
      "2024-12-15 16:24:03.015000: I runner.py:310] Step = 700 ; steps/s = 1.62, tokens/s = 82825 (39905 source, 42920 target) ; Learning rate = 0.000062 ; Loss = 5.638368\n",
      "2024-12-15 16:25:04.126000: I runner.py:310] Step = 800 ; steps/s = 1.64, tokens/s = 81966 (39488 source, 42478 target) ; Learning rate = 0.000071 ; Loss = 5.562014\n",
      "2024-12-15 16:26:05.705000: I runner.py:310] Step = 900 ; steps/s = 1.62, tokens/s = 82904 (39958 source, 42946 target) ; Learning rate = 0.000080 ; Loss = 5.370319\n",
      "2024-12-15 16:27:07.287000: I runner.py:310] Step = 1000 ; steps/s = 1.62, tokens/s = 82875 (39927 source, 42948 target) ; Learning rate = 0.000088 ; Loss = 5.121306\n",
      "2024-12-15 16:28:08.867000: I runner.py:310] Step = 1100 ; steps/s = 1.62, tokens/s = 82879 (39921 source, 42958 target) ; Learning rate = 0.000097 ; Loss = 4.848062\n",
      "2024-12-15 16:29:10.131000: I runner.py:310] Step = 1200 ; steps/s = 1.63, tokens/s = 81728 (39370 source, 42358 target) ; Learning rate = 0.000106 ; Loss = 4.331438\n",
      "2024-12-15 16:30:11.674000: I runner.py:310] Step = 1300 ; steps/s = 1.63, tokens/s = 82919 (39944 source, 42975 target) ; Learning rate = 0.000115 ; Loss = 3.977511\n",
      "2024-12-15 16:31:13.163000: I runner.py:310] Step = 1400 ; steps/s = 1.63, tokens/s = 82997 (39974 source, 43023 target) ; Learning rate = 0.000124 ; Loss = 3.778461\n",
      "2024-12-15 16:32:14.764000: I runner.py:310] Step = 1500 ; steps/s = 1.62, tokens/s = 82845 (39923 source, 42922 target) ; Learning rate = 0.000133 ; Loss = 3.472168\n",
      "2024-12-15 16:33:15.895000: I runner.py:310] Step = 1600 ; steps/s = 1.64, tokens/s = 81948 (39511 source, 42437 target) ; Learning rate = 0.000142 ; Loss = 3.359022\n",
      "2024-12-15 16:34:17.442000: I runner.py:310] Step = 1700 ; steps/s = 1.63, tokens/s = 82936 (39945 source, 42991 target) ; Learning rate = 0.000150 ; Loss = 3.259024\n",
      "2024-12-15 16:35:18.952000: I runner.py:310] Step = 1800 ; steps/s = 1.63, tokens/s = 82982 (39985 source, 42997 target) ; Learning rate = 0.000159 ; Loss = 3.121140\n",
      "2024-12-15 16:36:20.531000: I runner.py:310] Step = 1900 ; steps/s = 1.62, tokens/s = 82881 (39918 source, 42963 target) ; Learning rate = 0.000168 ; Loss = 3.113367\n",
      "2024-12-15 16:37:21.617000: I runner.py:310] Step = 2000 ; steps/s = 1.64, tokens/s = 81978 (39506 source, 42472 target) ; Learning rate = 0.000177 ; Loss = 3.007115\n",
      "2024-12-15 16:38:23.236000: I runner.py:310] Step = 2100 ; steps/s = 1.62, tokens/s = 82834 (39900 source, 42934 target) ; Learning rate = 0.000186 ; Loss = 2.988306\n",
      "2024-12-15 16:39:24.739000: I runner.py:310] Step = 2200 ; steps/s = 1.63, tokens/s = 82982 (39993 source, 42989 target) ; Learning rate = 0.000195 ; Loss = 2.887491\n",
      "2024-12-15 16:40:26.244000: I runner.py:310] Step = 2300 ; steps/s = 1.63, tokens/s = 82968 (39978 source, 42990 target) ; Learning rate = 0.000203 ; Loss = 2.859555\n",
      "2024-12-15 16:41:27.301000: I runner.py:310] Step = 2400 ; steps/s = 1.64, tokens/s = 82034 (39508 source, 42526 target) ; Learning rate = 0.000212 ; Loss = 2.766789\n",
      "2024-12-15 16:42:28.866000: I runner.py:310] Step = 2500 ; steps/s = 1.62, tokens/s = 82900 (39950 source, 42950 target) ; Learning rate = 0.000221 ; Loss = 2.766186\n",
      "2024-12-15 16:43:30.490000: I runner.py:310] Step = 2600 ; steps/s = 1.62, tokens/s = 82812 (39922 source, 42890 target) ; Learning rate = 0.000230 ; Loss = 2.702787\n",
      "2024-12-15 16:44:32.031000: I runner.py:310] Step = 2700 ; steps/s = 1.63, tokens/s = 82963 (39963 source, 43000 target) ; Learning rate = 0.000239 ; Loss = 2.761703\n",
      "2024-12-15 16:45:33.182000: I runner.py:310] Step = 2800 ; steps/s = 1.64, tokens/s = 81895 (39438 source, 42457 target) ; Learning rate = 0.000248 ; Loss = 2.604987\n",
      "2024-12-15 16:46:34.796000: I runner.py:310] Step = 2900 ; steps/s = 1.62, tokens/s = 82865 (39909 source, 42956 target) ; Learning rate = 0.000256 ; Loss = 2.630637\n",
      "2024-12-15 16:47:36.348000: I runner.py:310] Step = 3000 ; steps/s = 1.62, tokens/s = 82927 (39978 source, 42949 target) ; Learning rate = 0.000265 ; Loss = 2.547500\n",
      "2024-12-15 16:48:37.972000: I runner.py:310] Step = 3100 ; steps/s = 1.62, tokens/s = 82814 (39907 source, 42907 target) ; Learning rate = 0.000274 ; Loss = 2.550164\n",
      "2024-12-15 16:49:39.079000: I runner.py:310] Step = 3200 ; steps/s = 1.64, tokens/s = 81917 (39465 source, 42452 target) ; Learning rate = 0.000283 ; Loss = 2.484859\n",
      "2024-12-15 16:50:40.615000: I runner.py:310] Step = 3300 ; steps/s = 1.63, tokens/s = 82940 (39973 source, 42967 target) ; Learning rate = 0.000292 ; Loss = 2.445770\n",
      "2024-12-15 16:51:42.201000: I runner.py:310] Step = 3400 ; steps/s = 1.62, tokens/s = 82851 (39908 source, 42943 target) ; Learning rate = 0.000301 ; Loss = 2.443974\n",
      "2024-12-15 16:52:43.784000: I runner.py:310] Step = 3500 ; steps/s = 1.62, tokens/s = 82913 (39954 source, 42959 target) ; Learning rate = 0.000309 ; Loss = 2.433214\n",
      "2024-12-15 16:53:44.893000: I runner.py:310] Step = 3600 ; steps/s = 1.64, tokens/s = 81938 (39451 source, 42487 target) ; Learning rate = 0.000318 ; Loss = 2.399660\n",
      "2024-12-15 16:54:46.475000: I runner.py:310] Step = 3700 ; steps/s = 1.62, tokens/s = 82897 (39971 source, 42926 target) ; Learning rate = 0.000327 ; Loss = 2.419657\n",
      "2024-12-15 16:55:48.094000: I runner.py:310] Step = 3800 ; steps/s = 1.62, tokens/s = 82826 (39899 source, 42927 target) ; Learning rate = 0.000336 ; Loss = 2.365612\n",
      "2024-12-15 16:56:49.690000: I runner.py:310] Step = 3900 ; steps/s = 1.62, tokens/s = 82841 (39897 source, 42944 target) ; Learning rate = 0.000345 ; Loss = 2.332303\n",
      "2024-12-15 16:57:50.840000: I runner.py:310] Step = 4000 ; steps/s = 1.64, tokens/s = 81903 (39462 source, 42441 target) ; Learning rate = 0.000354 ; Loss = 2.327391\n",
      "2024-12-15 16:58:52.433000: I runner.py:310] Step = 4100 ; steps/s = 1.62, tokens/s = 82883 (39920 source, 42963 target) ; Learning rate = 0.000362 ; Loss = 2.341904\n",
      "2024-12-15 16:59:54.014000: I runner.py:310] Step = 4200 ; steps/s = 1.62, tokens/s = 82852 (39917 source, 42935 target) ; Learning rate = 0.000371 ; Loss = 2.320144\n",
      "2024-12-15 17:00:55.609000: I runner.py:310] Step = 4300 ; steps/s = 1.62, tokens/s = 82848 (39924 source, 42924 target) ; Learning rate = 0.000380 ; Loss = 2.270928\n",
      "2024-12-15 17:01:56.701000: I runner.py:310] Step = 4400 ; steps/s = 1.64, tokens/s = 82006 (39524 source, 42482 target) ; Learning rate = 0.000389 ; Loss = 2.263168\n",
      "2024-12-15 17:02:58.326000: I runner.py:310] Step = 4500 ; steps/s = 1.62, tokens/s = 82834 (39910 source, 42924 target) ; Learning rate = 0.000398 ; Loss = 2.246062\n",
      "2024-12-15 17:03:59.902000: I runner.py:310] Step = 4600 ; steps/s = 1.62, tokens/s = 82890 (39945 source, 42945 target) ; Learning rate = 0.000407 ; Loss = 2.223160\n",
      "2024-12-15 17:05:01.455000: I runner.py:310] Step = 4700 ; steps/s = 1.62, tokens/s = 82913 (39927 source, 42986 target) ; Learning rate = 0.000416 ; Loss = 2.206104\n",
      "2024-12-15 17:06:02.588000: I runner.py:310] Step = 4800 ; steps/s = 1.64, tokens/s = 81917 (39483 source, 42434 target) ; Learning rate = 0.000424 ; Loss = 2.181315\n",
      "2024-12-15 17:07:04.154000: I runner.py:310] Step = 4900 ; steps/s = 1.62, tokens/s = 82920 (39918 source, 43002 target) ; Learning rate = 0.000433 ; Loss = 2.148735\n",
      "2024-12-15 17:08:05.787000: I runner.py:310] Step = 5000 ; steps/s = 1.62, tokens/s = 82803 (39914 source, 42889 target) ; Learning rate = 0.000442 ; Loss = 2.166453\n",
      "2024-12-15 17:08:05.788000: I training.py:192] Running evaluation for step 5000\n",
      "2024-12-15 17:13:30.388000: I training.py:192] Evaluation result for step 5000: loss = 1.045230 ; perplexity = 2.844052\n",
      "2024-12-15 17:14:31.859000: I runner.py:310] Step = 5100 ; steps/s = 1.63, tokens/s = 83041 (40028 source, 43013 target) ; Learning rate = 0.000451 ; Loss = 2.174761\n",
      "2024-12-15 17:15:33.081000: I runner.py:310] Step = 5200 ; steps/s = 1.63, tokens/s = 81802 (39411 source, 42391 target) ; Learning rate = 0.000460 ; Loss = 2.183176\n",
      "2024-12-15 17:16:34.813000: I runner.py:310] Step = 5300 ; steps/s = 1.62, tokens/s = 82679 (39806 source, 42873 target) ; Learning rate = 0.000469 ; Loss = 2.142195\n",
      "2024-12-15 17:17:36.586000: I runner.py:310] Step = 5400 ; steps/s = 1.62, tokens/s = 82635 (39829 source, 42806 target) ; Learning rate = 0.000477 ; Loss = 2.115576\n",
      "2024-12-15 17:18:38.309000: I runner.py:310] Step = 5500 ; steps/s = 1.62, tokens/s = 82674 (39836 source, 42838 target) ; Learning rate = 0.000486 ; Loss = 2.123215\n",
      "2024-12-15 17:19:39.592000: I runner.py:310] Step = 5600 ; steps/s = 1.63, tokens/s = 81702 (39351 source, 42351 target) ; Learning rate = 0.000495 ; Loss = 2.098339\n",
      "2024-12-15 17:20:41.338000: I runner.py:310] Step = 5700 ; steps/s = 1.62, tokens/s = 82659 (39837 source, 42822 target) ; Learning rate = 0.000504 ; Loss = 2.132705\n",
      "2024-12-15 17:21:42.999000: I runner.py:310] Step = 5800 ; steps/s = 1.62, tokens/s = 82777 (39902 source, 42875 target) ; Learning rate = 0.000513 ; Loss = 2.116444\n",
      "2024-12-15 17:22:44.728000: I runner.py:310] Step = 5900 ; steps/s = 1.62, tokens/s = 82692 (39817 source, 42875 target) ; Learning rate = 0.000522 ; Loss = 2.109919\n",
      "2024-12-15 17:23:46.029000: I runner.py:310] Step = 6000 ; steps/s = 1.63, tokens/s = 81706 (39353 source, 42353 target) ; Learning rate = 0.000530 ; Loss = 2.055372\n",
      "2024-12-15 17:24:47.680000: I runner.py:310] Step = 6100 ; steps/s = 1.62, tokens/s = 82798 (39906 source, 42892 target) ; Learning rate = 0.000539 ; Loss = 2.053011\n",
      "2024-12-15 17:25:49.421000: I runner.py:310] Step = 6200 ; steps/s = 1.62, tokens/s = 82639 (39822 source, 42817 target) ; Learning rate = 0.000548 ; Loss = 2.041850\n",
      "2024-12-15 17:26:51.116000: I runner.py:310] Step = 6300 ; steps/s = 1.62, tokens/s = 82723 (39860 source, 42863 target) ; Learning rate = 0.000557 ; Loss = 2.023110\n",
      "2024-12-15 17:27:52.386000: I runner.py:310] Step = 6400 ; steps/s = 1.63, tokens/s = 81744 (39381 source, 42363 target) ; Learning rate = 0.000566 ; Loss = 2.064697\n",
      "2024-12-15 17:28:54.145000: I runner.py:310] Step = 6500 ; steps/s = 1.62, tokens/s = 82628 (39807 source, 42821 target) ; Learning rate = 0.000575 ; Loss = 1.995890\n",
      "2024-12-15 17:29:55.823000: I runner.py:310] Step = 6600 ; steps/s = 1.62, tokens/s = 82764 (39872 source, 42892 target) ; Learning rate = 0.000583 ; Loss = 2.029595\n",
      "2024-12-15 17:30:57.438000: I runner.py:310] Step = 6700 ; steps/s = 1.62, tokens/s = 82866 (39916 source, 42950 target) ; Learning rate = 0.000592 ; Loss = 1.992999\n",
      "2024-12-15 17:31:58.633000: I runner.py:310] Step = 6800 ; steps/s = 1.63, tokens/s = 81837 (39452 source, 42385 target) ; Learning rate = 0.000601 ; Loss = 2.012890\n",
      "2024-12-15 17:33:00.282000: I runner.py:310] Step = 6900 ; steps/s = 1.62, tokens/s = 82789 (39890 source, 42899 target) ; Learning rate = 0.000610 ; Loss = 1.963195\n",
      "2024-12-15 17:34:01.860000: I runner.py:310] Step = 7000 ; steps/s = 1.62, tokens/s = 82915 (39929 source, 42986 target) ; Learning rate = 0.000619 ; Loss = 1.979460\n",
      "2024-12-15 17:35:03.428000: I runner.py:310] Step = 7100 ; steps/s = 1.62, tokens/s = 82864 (39923 source, 42941 target) ; Learning rate = 0.000628 ; Loss = 2.002335\n",
      "2024-12-15 17:36:04.544000: I runner.py:310] Step = 7200 ; steps/s = 1.64, tokens/s = 81932 (39473 source, 42459 target) ; Learning rate = 0.000636 ; Loss = 1.934614\n",
      "2024-12-15 17:37:06.208000: I runner.py:310] Step = 7300 ; steps/s = 1.62, tokens/s = 82771 (39878 source, 42893 target) ; Learning rate = 0.000645 ; Loss = 1.979108\n",
      "2024-12-15 17:38:07.849000: I runner.py:310] Step = 7400 ; steps/s = 1.62, tokens/s = 82813 (39914 source, 42899 target) ; Learning rate = 0.000654 ; Loss = 1.980520\n",
      "2024-12-15 17:39:09.473000: I runner.py:310] Step = 7500 ; steps/s = 1.62, tokens/s = 82824 (39887 source, 42937 target) ; Learning rate = 0.000663 ; Loss = 1.958633\n",
      "2024-12-15 17:40:10.669000: I runner.py:310] Step = 7600 ; steps/s = 1.63, tokens/s = 81817 (39442 source, 42375 target) ; Learning rate = 0.000672 ; Loss = 1.885988\n",
      "2024-12-15 17:41:12.299000: I runner.py:310] Step = 7700 ; steps/s = 1.62, tokens/s = 82811 (39868 source, 42943 target) ; Learning rate = 0.000681 ; Loss = 1.924925\n",
      "2024-12-15 17:42:13.932000: I runner.py:310] Step = 7800 ; steps/s = 1.62, tokens/s = 82817 (39904 source, 42913 target) ; Learning rate = 0.000690 ; Loss = 1.930707\n",
      "2024-12-15 17:43:15.535000: I runner.py:310] Step = 7900 ; steps/s = 1.62, tokens/s = 82847 (39928 source, 42919 target) ; Learning rate = 0.000698 ; Loss = 1.935194\n",
      "2024-12-15 17:44:16.703000: I runner.py:310] Step = 8000 ; steps/s = 1.64, tokens/s = 81862 (39464 source, 42398 target) ; Learning rate = 0.000707 ; Loss = 1.907924\n",
      "2024-12-15 17:45:18.279000: I runner.py:310] Step = 8100 ; steps/s = 1.62, tokens/s = 82908 (39939 source, 42969 target) ; Learning rate = 0.000716 ; Loss = 1.908150\n",
      "2024-12-15 17:46:19.927000: I runner.py:310] Step = 8200 ; steps/s = 1.62, tokens/s = 82791 (39883 source, 42908 target) ; Learning rate = 0.000725 ; Loss = 1.916306\n",
      "2024-12-15 17:47:21.652000: I runner.py:310] Step = 8300 ; steps/s = 1.62, tokens/s = 82680 (39831 source, 42849 target) ; Learning rate = 0.000734 ; Loss = 1.953447\n",
      "2024-12-15 17:48:22.869000: I runner.py:310] Step = 8400 ; steps/s = 1.63, tokens/s = 81803 (39416 source, 42387 target) ; Learning rate = 0.000743 ; Loss = 1.841759\n",
      "2024-12-15 17:49:24.473000: I runner.py:310] Step = 8500 ; steps/s = 1.62, tokens/s = 82840 (39865 source, 42975 target) ; Learning rate = 0.000751 ; Loss = 1.892636\n",
      "2024-12-15 17:50:26.138000: I runner.py:310] Step = 8600 ; steps/s = 1.62, tokens/s = 82758 (39895 source, 42863 target) ; Learning rate = 0.000760 ; Loss = 1.921555\n",
      "2024-12-15 17:51:27.755000: I runner.py:310] Step = 8700 ; steps/s = 1.62, tokens/s = 82860 (39952 source, 42908 target) ; Learning rate = 0.000769 ; Loss = 1.909136\n",
      "2024-12-15 17:52:28.847000: I runner.py:310] Step = 8800 ; steps/s = 1.64, tokens/s = 81983 (39470 source, 42513 target) ; Learning rate = 0.000778 ; Loss = 1.860801\n",
      "2024-12-15 17:53:30.404000: I runner.py:310] Step = 8900 ; steps/s = 1.62, tokens/s = 82900 (39917 source, 42983 target) ; Learning rate = 0.000787 ; Loss = 1.864191\n",
      "2024-12-15 17:54:31.983000: I runner.py:310] Step = 9000 ; steps/s = 1.62, tokens/s = 82897 (39936 source, 42961 target) ; Learning rate = 0.000796 ; Loss = 1.893044\n",
      "2024-12-15 17:55:33.596000: I runner.py:310] Step = 9100 ; steps/s = 1.62, tokens/s = 82848 (39953 source, 42895 target) ; Learning rate = 0.000804 ; Loss = 1.887665\n",
      "2024-12-15 17:56:34.745000: I runner.py:310] Step = 9200 ; steps/s = 1.64, tokens/s = 81906 (39468 source, 42438 target) ; Learning rate = 0.000813 ; Loss = 1.846849\n",
      "2024-12-15 17:57:36.368000: I runner.py:310] Step = 9300 ; steps/s = 1.62, tokens/s = 82821 (39908 source, 42913 target) ; Learning rate = 0.000822 ; Loss = 1.843896\n",
      "2024-12-15 17:58:37.952000: I runner.py:310] Step = 9400 ; steps/s = 1.62, tokens/s = 82868 (39936 source, 42932 target) ; Learning rate = 0.000831 ; Loss = 1.861841\n",
      "2024-12-15 17:59:39.598000: I runner.py:310] Step = 9500 ; steps/s = 1.62, tokens/s = 82776 (39869 source, 42907 target) ; Learning rate = 0.000840 ; Loss = 1.864247\n",
      "2024-12-15 18:00:40.780000: I runner.py:310] Step = 9600 ; steps/s = 1.63, tokens/s = 81882 (39434 source, 42448 target) ; Learning rate = 0.000849 ; Loss = 1.810445\n",
      "2024-12-15 18:01:42.463000: I runner.py:310] Step = 9700 ; steps/s = 1.62, tokens/s = 82759 (39885 source, 42874 target) ; Learning rate = 0.000857 ; Loss = 1.850889\n",
      "2024-12-15 18:02:44.152000: I runner.py:310] Step = 9800 ; steps/s = 1.62, tokens/s = 82748 (39846 source, 42902 target) ; Learning rate = 0.000866 ; Loss = 1.872836\n",
      "2024-12-15 18:03:45.718000: I runner.py:310] Step = 9900 ; steps/s = 1.62, tokens/s = 82857 (39954 source, 42903 target) ; Learning rate = 0.000875 ; Loss = 1.871123\n",
      "2024-12-15 18:04:46.843000: I runner.py:310] Step = 10000 ; steps/s = 1.64, tokens/s = 81935 (39449 source, 42486 target) ; Learning rate = 0.000884 ; Loss = 1.810117\n",
      "2024-12-15 18:04:48.491000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-10000\n",
      "2024-12-15 18:04:48.491000: I training.py:192] Running evaluation for step 10000\n",
      "2024-12-15 18:09:30.604000: I training.py:192] Evaluation result for step 10000: loss = 0.995043 ; perplexity = 2.704841\n",
      "2024-12-15 18:10:32.092000: I runner.py:310] Step = 10100 ; steps/s = 1.63, tokens/s = 83019 (39980 source, 43039 target) ; Learning rate = 0.000879 ; Loss = 1.833073\n",
      "2024-12-15 18:11:33.786000: I runner.py:310] Step = 10200 ; steps/s = 1.62, tokens/s = 82737 (39895 source, 42842 target) ; Learning rate = 0.000875 ; Loss = 1.848888\n",
      "2024-12-15 18:12:35.087000: I runner.py:310] Step = 10300 ; steps/s = 1.63, tokens/s = 82206 (39605 source, 42601 target) ; Learning rate = 0.000871 ; Loss = 1.838099\n",
      "2024-12-15 18:13:36.527000: I runner.py:310] Step = 10400 ; steps/s = 1.63, tokens/s = 82562 (39806 source, 42756 target) ; Learning rate = 0.000867 ; Loss = 1.783996\n",
      "2024-12-15 18:14:38.145000: I runner.py:310] Step = 10500 ; steps/s = 1.62, tokens/s = 82827 (39889 source, 42938 target) ; Learning rate = 0.000863 ; Loss = 1.821350\n",
      "2024-12-15 18:15:39.810000: I runner.py:310] Step = 10600 ; steps/s = 1.62, tokens/s = 82736 (39852 source, 42884 target) ; Learning rate = 0.000858 ; Loss = 1.801823\n",
      "2024-12-15 18:16:40.963000: I runner.py:310] Step = 10700 ; steps/s = 1.64, tokens/s = 81911 (39486 source, 42425 target) ; Learning rate = 0.000854 ; Loss = 1.781254\n",
      "2024-12-15 18:17:42.576000: I runner.py:310] Step = 10800 ; steps/s = 1.62, tokens/s = 82853 (39914 source, 42939 target) ; Learning rate = 0.000850 ; Loss = 1.793158\n",
      "2024-12-15 18:18:44.136000: I runner.py:310] Step = 10900 ; steps/s = 1.62, tokens/s = 82899 (39914 source, 42985 target) ; Learning rate = 0.000847 ; Loss = 1.770653\n",
      "2024-12-15 18:19:45.762000: I runner.py:310] Step = 11000 ; steps/s = 1.62, tokens/s = 82819 (39935 source, 42884 target) ; Learning rate = 0.000843 ; Loss = 1.786597\n",
      "2024-12-15 18:20:46.886000: I runner.py:310] Step = 11100 ; steps/s = 1.64, tokens/s = 81926 (39454 source, 42472 target) ; Learning rate = 0.000839 ; Loss = 1.773508\n",
      "2024-12-15 18:21:48.448000: I runner.py:310] Step = 11200 ; steps/s = 1.62, tokens/s = 82926 (39937 source, 42989 target) ; Learning rate = 0.000835 ; Loss = 1.767353\n",
      "2024-12-15 18:22:50.041000: I runner.py:310] Step = 11300 ; steps/s = 1.62, tokens/s = 82830 (39910 source, 42920 target) ; Learning rate = 0.000831 ; Loss = 1.764296\n",
      "2024-12-15 18:23:51.697000: I runner.py:310] Step = 11400 ; steps/s = 1.62, tokens/s = 82781 (39889 source, 42892 target) ; Learning rate = 0.000828 ; Loss = 1.773925\n",
      "2024-12-15 18:24:52.870000: I runner.py:310] Step = 11500 ; steps/s = 1.63, tokens/s = 81870 (39465 source, 42405 target) ; Learning rate = 0.000824 ; Loss = 1.767492\n",
      "2024-12-15 18:25:54.511000: I runner.py:310] Step = 11600 ; steps/s = 1.62, tokens/s = 82847 (39906 source, 42941 target) ; Learning rate = 0.000821 ; Loss = 1.758200\n",
      "2024-12-15 18:26:56.113000: I runner.py:310] Step = 11700 ; steps/s = 1.62, tokens/s = 82852 (39933 source, 42919 target) ; Learning rate = 0.000817 ; Loss = 1.753186\n",
      "2024-12-15 18:27:57.717000: I runner.py:310] Step = 11800 ; steps/s = 1.62, tokens/s = 82821 (39908 source, 42913 target) ; Learning rate = 0.000814 ; Loss = 1.769516\n",
      "2024-12-15 18:28:58.868000: I runner.py:310] Step = 11900 ; steps/s = 1.64, tokens/s = 81871 (39426 source, 42445 target) ; Learning rate = 0.000810 ; Loss = 1.722573\n",
      "2024-12-15 18:30:00.500000: I runner.py:310] Step = 12000 ; steps/s = 1.62, tokens/s = 82815 (39907 source, 42908 target) ; Learning rate = 0.000807 ; Loss = 1.714922\n",
      "2024-12-15 18:31:02.078000: I runner.py:310] Step = 12100 ; steps/s = 1.62, tokens/s = 82864 (39902 source, 42962 target) ; Learning rate = 0.000803 ; Loss = 1.753014\n",
      "2024-12-15 18:32:03.700000: I runner.py:310] Step = 12200 ; steps/s = 1.62, tokens/s = 82844 (39937 source, 42907 target) ; Learning rate = 0.000800 ; Loss = 1.773304\n",
      "2024-12-15 18:33:04.801000: I runner.py:310] Step = 12300 ; steps/s = 1.64, tokens/s = 81964 (39486 source, 42478 target) ; Learning rate = 0.000797 ; Loss = 1.736374\n",
      "2024-12-15 18:34:06.421000: I runner.py:310] Step = 12400 ; steps/s = 1.62, tokens/s = 82828 (39905 source, 42923 target) ; Learning rate = 0.000794 ; Loss = 1.723245\n",
      "2024-12-15 18:35:08.027000: I runner.py:310] Step = 12500 ; steps/s = 1.62, tokens/s = 82831 (39936 source, 42895 target) ; Learning rate = 0.000791 ; Loss = 1.729247\n",
      "2024-12-15 18:36:09.638000: I runner.py:310] Step = 12600 ; steps/s = 1.62, tokens/s = 82866 (39902 source, 42964 target) ; Learning rate = 0.000787 ; Loss = 1.735561\n",
      "2024-12-15 18:37:10.792000: I runner.py:310] Step = 12700 ; steps/s = 1.64, tokens/s = 81883 (39440 source, 42443 target) ; Learning rate = 0.000784 ; Loss = 1.713021\n",
      "2024-12-15 18:38:12.420000: I runner.py:310] Step = 12800 ; steps/s = 1.62, tokens/s = 82822 (39904 source, 42918 target) ; Learning rate = 0.000781 ; Loss = 1.732378\n",
      "2024-12-15 18:39:14.008000: I runner.py:310] Step = 12900 ; steps/s = 1.62, tokens/s = 82871 (39933 source, 42938 target) ; Learning rate = 0.000778 ; Loss = 1.737759\n",
      "2024-12-15 18:40:15.623000: I runner.py:310] Step = 13000 ; steps/s = 1.62, tokens/s = 82849 (39918 source, 42931 target) ; Learning rate = 0.000775 ; Loss = 1.715294\n",
      "2024-12-15 18:41:16.811000: I runner.py:310] Step = 13100 ; steps/s = 1.63, tokens/s = 81846 (39412 source, 42434 target) ; Learning rate = 0.000772 ; Loss = 1.690802\n",
      "2024-12-15 18:42:18.406000: I runner.py:310] Step = 13200 ; steps/s = 1.62, tokens/s = 82834 (39913 source, 42921 target) ; Learning rate = 0.000769 ; Loss = 1.693265\n",
      "2024-12-15 18:43:19.992000: I runner.py:310] Step = 13300 ; steps/s = 1.62, tokens/s = 82871 (39932 source, 42939 target) ; Learning rate = 0.000766 ; Loss = 1.733996\n",
      "2024-12-15 18:44:21.605000: I runner.py:310] Step = 13400 ; steps/s = 1.62, tokens/s = 82849 (39923 source, 42926 target) ; Learning rate = 0.000764 ; Loss = 1.730814\n",
      "2024-12-15 18:45:22.702000: I runner.py:310] Step = 13500 ; steps/s = 1.64, tokens/s = 81975 (39506 source, 42469 target) ; Learning rate = 0.000761 ; Loss = 1.707186\n",
      "2024-12-15 18:46:24.313000: I runner.py:310] Step = 13600 ; steps/s = 1.62, tokens/s = 82858 (39921 source, 42937 target) ; Learning rate = 0.000758 ; Loss = 1.697425\n",
      "2024-12-15 18:47:25.936000: I runner.py:310] Step = 13700 ; steps/s = 1.62, tokens/s = 82812 (39883 source, 42929 target) ; Learning rate = 0.000755 ; Loss = 1.710054\n",
      "2024-12-15 18:48:27.532000: I runner.py:310] Step = 13800 ; steps/s = 1.62, tokens/s = 82842 (39936 source, 42906 target) ; Learning rate = 0.000752 ; Loss = 1.692374\n",
      "2024-12-15 18:49:28.701000: I runner.py:310] Step = 13900 ; steps/s = 1.63, tokens/s = 81898 (39442 source, 42456 target) ; Learning rate = 0.000750 ; Loss = 1.688689\n",
      "2024-12-15 18:50:30.324000: I runner.py:310] Step = 14000 ; steps/s = 1.62, tokens/s = 82795 (39920 source, 42875 target) ; Learning rate = 0.000747 ; Loss = 1.709657\n",
      "2024-12-15 18:51:31.904000: I runner.py:310] Step = 14100 ; steps/s = 1.62, tokens/s = 82887 (39912 source, 42975 target) ; Learning rate = 0.000744 ; Loss = 1.684765\n",
      "2024-12-15 18:52:33.535000: I runner.py:310] Step = 14200 ; steps/s = 1.62, tokens/s = 82819 (39916 source, 42903 target) ; Learning rate = 0.000742 ; Loss = 1.700469\n",
      "2024-12-15 18:53:34.639000: I runner.py:310] Step = 14300 ; steps/s = 1.64, tokens/s = 81992 (39499 source, 42493 target) ; Learning rate = 0.000739 ; Loss = 1.692621\n",
      "2024-12-15 18:54:36.219000: I runner.py:310] Step = 14400 ; steps/s = 1.62, tokens/s = 82864 (39927 source, 42937 target) ; Learning rate = 0.000737 ; Loss = 1.702540\n",
      "2024-12-15 18:55:37.889000: I runner.py:310] Step = 14500 ; steps/s = 1.62, tokens/s = 82768 (39848 source, 42920 target) ; Learning rate = 0.000734 ; Loss = 1.688443\n",
      "2024-12-15 18:56:39.531000: I runner.py:310] Step = 14600 ; steps/s = 1.62, tokens/s = 82760 (39886 source, 42874 target) ; Learning rate = 0.000731 ; Loss = 1.689175\n",
      "2024-12-15 18:57:40.643000: I runner.py:310] Step = 14700 ; steps/s = 1.64, tokens/s = 81957 (39500 source, 42457 target) ; Learning rate = 0.000729 ; Loss = 1.682375\n",
      "2024-12-15 18:58:42.221000: I runner.py:310] Step = 14800 ; steps/s = 1.62, tokens/s = 82875 (39922 source, 42953 target) ; Learning rate = 0.000727 ; Loss = 1.676736\n",
      "2024-12-15 18:59:43.838000: I runner.py:310] Step = 14900 ; steps/s = 1.62, tokens/s = 82834 (39909 source, 42925 target) ; Learning rate = 0.000724 ; Loss = 1.686801\n",
      "2024-12-15 19:00:45.430000: I runner.py:310] Step = 15000 ; steps/s = 1.62, tokens/s = 82890 (39943 source, 42947 target) ; Learning rate = 0.000722 ; Loss = 1.692709\n",
      "2024-12-15 19:00:45.431000: I training.py:192] Running evaluation for step 15000\n",
      "2024-12-15 19:05:15.158000: I training.py:192] Evaluation result for step 15000: loss = 1.022266 ; perplexity = 2.779486\n",
      "2024-12-15 19:06:16.193000: I runner.py:310] Step = 15100 ; steps/s = 1.64, tokens/s = 82081 (39539 source, 42542 target) ; Learning rate = 0.000719 ; Loss = 1.641779\n",
      "2024-12-15 19:07:17.747000: I runner.py:310] Step = 15200 ; steps/s = 1.62, tokens/s = 82918 (39942 source, 42976 target) ; Learning rate = 0.000717 ; Loss = 1.649236\n",
      "2024-12-15 19:08:19.383000: I runner.py:310] Step = 15300 ; steps/s = 1.62, tokens/s = 82828 (39917 source, 42911 target) ; Learning rate = 0.000715 ; Loss = 1.679702\n",
      "2024-12-15 19:09:20.942000: I runner.py:310] Step = 15400 ; steps/s = 1.62, tokens/s = 82874 (39930 source, 42944 target) ; Learning rate = 0.000712 ; Loss = 1.689071\n",
      "2024-12-15 19:10:22.107000: I runner.py:310] Step = 15500 ; steps/s = 1.64, tokens/s = 81886 (39444 source, 42442 target) ; Learning rate = 0.000710 ; Loss = 1.662238\n",
      "2024-12-15 19:11:23.695000: I runner.py:310] Step = 15600 ; steps/s = 1.62, tokens/s = 82857 (39945 source, 42912 target) ; Learning rate = 0.000708 ; Loss = 1.655405\n",
      "2024-12-15 19:12:25.258000: I runner.py:310] Step = 15700 ; steps/s = 1.62, tokens/s = 82918 (39940 source, 42978 target) ; Learning rate = 0.000705 ; Loss = 1.684261\n",
      "2024-12-15 19:13:26.799000: I runner.py:310] Step = 15800 ; steps/s = 1.63, tokens/s = 82898 (39952 source, 42946 target) ; Learning rate = 0.000703 ; Loss = 1.691279\n",
      "2024-12-15 19:14:27.901000: I runner.py:310] Step = 15900 ; steps/s = 1.64, tokens/s = 81985 (39474 source, 42511 target) ; Learning rate = 0.000701 ; Loss = 1.664521\n",
      "2024-12-15 19:15:29.530000: I runner.py:310] Step = 16000 ; steps/s = 1.62, tokens/s = 82823 (39899 source, 42924 target) ; Learning rate = 0.000699 ; Loss = 1.640177\n",
      "2024-12-15 19:16:31.159000: I runner.py:310] Step = 16100 ; steps/s = 1.62, tokens/s = 82797 (39875 source, 42922 target) ; Learning rate = 0.000697 ; Loss = 1.662920\n",
      "2024-12-15 19:17:32.743000: I runner.py:310] Step = 16200 ; steps/s = 1.62, tokens/s = 82875 (39965 source, 42910 target) ; Learning rate = 0.000694 ; Loss = 1.666929\n",
      "2024-12-15 19:18:33.895000: I runner.py:310] Step = 16300 ; steps/s = 1.64, tokens/s = 81910 (39452 source, 42458 target) ; Learning rate = 0.000692 ; Loss = 1.667286\n",
      "2024-12-15 19:19:35.501000: I runner.py:310] Step = 16400 ; steps/s = 1.62, tokens/s = 82844 (39918 source, 42926 target) ; Learning rate = 0.000690 ; Loss = 1.651293\n",
      "2024-12-15 19:20:37.029000: I runner.py:310] Step = 16500 ; steps/s = 1.63, tokens/s = 82936 (39961 source, 42975 target) ; Learning rate = 0.000688 ; Loss = 1.658570\n",
      "2024-12-15 19:21:38.662000: I runner.py:310] Step = 16600 ; steps/s = 1.62, tokens/s = 82825 (39901 source, 42924 target) ; Learning rate = 0.000686 ; Loss = 1.649654\n",
      "2024-12-15 19:22:39.807000: I runner.py:310] Step = 16700 ; steps/s = 1.64, tokens/s = 81875 (39432 source, 42443 target) ; Learning rate = 0.000684 ; Loss = 1.657505\n",
      "2024-12-15 19:23:41.432000: I runner.py:310] Step = 16800 ; steps/s = 1.62, tokens/s = 82841 (39898 source, 42943 target) ; Learning rate = 0.000682 ; Loss = 1.650070\n",
      "2024-12-15 19:24:43.011000: I runner.py:310] Step = 16900 ; steps/s = 1.62, tokens/s = 82909 (39920 source, 42989 target) ; Learning rate = 0.000680 ; Loss = 1.646468\n",
      "2024-12-15 19:25:44.624000: I runner.py:310] Step = 17000 ; steps/s = 1.62, tokens/s = 82856 (39966 source, 42890 target) ; Learning rate = 0.000678 ; Loss = 1.660479\n",
      "2024-12-15 19:26:45.792000: I runner.py:310] Step = 17100 ; steps/s = 1.63, tokens/s = 81834 (39432 source, 42402 target) ; Learning rate = 0.000676 ; Loss = 1.647868\n",
      "2024-12-15 19:27:47.394000: I runner.py:310] Step = 17200 ; steps/s = 1.62, tokens/s = 82860 (39914 source, 42946 target) ; Learning rate = 0.000674 ; Loss = 1.634613\n",
      "2024-12-15 19:28:49.009000: I runner.py:310] Step = 17300 ; steps/s = 1.62, tokens/s = 82847 (39920 source, 42927 target) ; Learning rate = 0.000672 ; Loss = 1.637605\n",
      "2024-12-15 19:29:50.578000: I runner.py:310] Step = 17400 ; steps/s = 1.62, tokens/s = 82899 (39954 source, 42945 target) ; Learning rate = 0.000670 ; Loss = 1.637693\n",
      "2024-12-15 19:30:51.750000: I runner.py:310] Step = 17500 ; steps/s = 1.63, tokens/s = 81860 (39433 source, 42427 target) ; Learning rate = 0.000668 ; Loss = 1.662712\n",
      "2024-12-15 19:31:53.326000: I runner.py:310] Step = 17600 ; steps/s = 1.62, tokens/s = 82877 (39926 source, 42951 target) ; Learning rate = 0.000666 ; Loss = 1.624461\n",
      "2024-12-15 19:32:54.930000: I runner.py:310] Step = 17700 ; steps/s = 1.62, tokens/s = 82865 (39955 source, 42910 target) ; Learning rate = 0.000664 ; Loss = 1.630726\n",
      "2024-12-15 19:33:56.555000: I runner.py:310] Step = 17800 ; steps/s = 1.62, tokens/s = 82803 (39867 source, 42936 target) ; Learning rate = 0.000662 ; Loss = 1.636854\n",
      "2024-12-15 19:34:57.664000: I runner.py:310] Step = 17900 ; steps/s = 1.64, tokens/s = 81963 (39495 source, 42468 target) ; Learning rate = 0.000661 ; Loss = 1.635128\n",
      "2024-12-15 19:35:59.229000: I runner.py:310] Step = 18000 ; steps/s = 1.62, tokens/s = 82921 (39945 source, 42976 target) ; Learning rate = 0.000659 ; Loss = 1.627691\n",
      "2024-12-15 19:37:00.851000: I runner.py:310] Step = 18100 ; steps/s = 1.62, tokens/s = 82793 (39875 source, 42918 target) ; Learning rate = 0.000657 ; Loss = 1.628486\n",
      "2024-12-15 19:38:02.348000: I runner.py:310] Step = 18200 ; steps/s = 1.63, tokens/s = 83000 (40008 source, 42992 target) ; Learning rate = 0.000655 ; Loss = 1.628580\n",
      "2024-12-15 19:39:03.463000: I runner.py:310] Step = 18300 ; steps/s = 1.64, tokens/s = 81968 (39515 source, 42453 target) ; Learning rate = 0.000653 ; Loss = 1.608818\n",
      "2024-12-15 19:40:05.160000: I runner.py:310] Step = 18400 ; steps/s = 1.62, tokens/s = 82698 (39840 source, 42858 target) ; Learning rate = 0.000652 ; Loss = 1.627262\n",
      "2024-12-15 19:41:06.798000: I runner.py:310] Step = 18500 ; steps/s = 1.62, tokens/s = 82811 (39862 source, 42949 target) ; Learning rate = 0.000650 ; Loss = 1.635914\n",
      "2024-12-15 19:42:08.450000: I runner.py:310] Step = 18600 ; steps/s = 1.62, tokens/s = 82780 (39907 source, 42873 target) ; Learning rate = 0.000648 ; Loss = 1.651090\n",
      "2024-12-15 19:43:09.577000: I runner.py:310] Step = 18700 ; steps/s = 1.64, tokens/s = 81937 (39465 source, 42472 target) ; Learning rate = 0.000646 ; Loss = 1.642730\n",
      "2024-12-15 19:44:11.251000: I runner.py:310] Step = 18800 ; steps/s = 1.62, tokens/s = 82784 (39923 source, 42861 target) ; Learning rate = 0.000645 ; Loss = 1.619210\n",
      "2024-12-15 19:45:12.851000: I runner.py:310] Step = 18900 ; steps/s = 1.62, tokens/s = 82854 (39901 source, 42953 target) ; Learning rate = 0.000643 ; Loss = 1.630163\n",
      "2024-12-15 19:46:14.503000: I runner.py:310] Step = 19000 ; steps/s = 1.62, tokens/s = 82757 (39865 source, 42892 target) ; Learning rate = 0.000641 ; Loss = 1.635182\n",
      "2024-12-15 19:47:15.701000: I runner.py:310] Step = 19100 ; steps/s = 1.63, tokens/s = 81810 (39423 source, 42387 target) ; Learning rate = 0.000640 ; Loss = 1.636666\n",
      "2024-12-15 19:48:17.263000: I runner.py:310] Step = 19200 ; steps/s = 1.62, tokens/s = 82934 (39949 source, 42985 target) ; Learning rate = 0.000638 ; Loss = 1.606324\n",
      "2024-12-15 19:49:18.808000: I runner.py:310] Step = 19300 ; steps/s = 1.63, tokens/s = 82913 (39956 source, 42957 target) ; Learning rate = 0.000636 ; Loss = 1.618068\n",
      "2024-12-15 19:50:20.448000: I runner.py:310] Step = 19400 ; steps/s = 1.62, tokens/s = 82801 (39890 source, 42911 target) ; Learning rate = 0.000635 ; Loss = 1.624864\n",
      "2024-12-15 19:51:21.628000: I runner.py:310] Step = 19500 ; steps/s = 1.63, tokens/s = 81888 (39439 source, 42449 target) ; Learning rate = 0.000633 ; Loss = 1.627957\n",
      "2024-12-15 19:52:23.260000: I runner.py:310] Step = 19600 ; steps/s = 1.62, tokens/s = 82800 (39913 source, 42887 target) ; Learning rate = 0.000631 ; Loss = 1.609096\n",
      "2024-12-15 19:53:24.918000: I runner.py:310] Step = 19700 ; steps/s = 1.62, tokens/s = 82770 (39881 source, 42889 target) ; Learning rate = 0.000630 ; Loss = 1.616663\n",
      "2024-12-15 19:54:26.539000: I runner.py:310] Step = 19800 ; steps/s = 1.62, tokens/s = 82823 (39893 source, 42930 target) ; Learning rate = 0.000628 ; Loss = 1.620990\n",
      "2024-12-15 19:55:27.681000: I runner.py:310] Step = 19900 ; steps/s = 1.64, tokens/s = 81913 (39449 source, 42464 target) ; Learning rate = 0.000627 ; Loss = 1.636693\n",
      "2024-12-15 19:56:29.335000: I runner.py:310] Step = 20000 ; steps/s = 1.62, tokens/s = 82776 (39901 source, 42875 target) ; Learning rate = 0.000625 ; Loss = 1.604845\n",
      "2024-12-15 19:56:30.934000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-20000\n",
      "2024-12-15 19:56:30.934000: I training.py:192] Running evaluation for step 20000\n",
      "2024-12-15 20:00:47.333000: I training.py:192] Evaluation result for step 20000: loss = 1.063348 ; perplexity = 2.896050\n",
      "2024-12-15 20:01:48.977000: I runner.py:310] Step = 20100 ; steps/s = 1.62, tokens/s = 82826 (39900 source, 42926 target) ; Learning rate = 0.000623 ; Loss = 1.620892\n",
      "2024-12-15 20:02:50.605000: I runner.py:310] Step = 20200 ; steps/s = 1.62, tokens/s = 82809 (39898 source, 42911 target) ; Learning rate = 0.000622 ; Loss = 1.622639\n",
      "2024-12-15 20:03:51.795000: I runner.py:310] Step = 20300 ; steps/s = 1.63, tokens/s = 81841 (39427 source, 42414 target) ; Learning rate = 0.000620 ; Loss = 1.625001\n",
      "2024-12-15 20:04:53.422000: I runner.py:310] Step = 20400 ; steps/s = 1.62, tokens/s = 82826 (39888 source, 42938 target) ; Learning rate = 0.000619 ; Loss = 1.602413\n",
      "2024-12-15 20:05:55.021000: I runner.py:310] Step = 20500 ; steps/s = 1.62, tokens/s = 82855 (39913 source, 42942 target) ; Learning rate = 0.000617 ; Loss = 1.600173\n",
      "2024-12-15 20:06:56.563000: I runner.py:310] Step = 20600 ; steps/s = 1.63, tokens/s = 82679 (39854 source, 42825 target) ; Learning rate = 0.000616 ; Loss = 1.627109\n",
      "2024-12-15 20:07:57.737000: I runner.py:310] Step = 20700 ; steps/s = 1.63, tokens/s = 82121 (39541 source, 42580 target) ; Learning rate = 0.000614 ; Loss = 1.590962\n",
      "2024-12-15 20:08:59.334000: I runner.py:310] Step = 20800 ; steps/s = 1.62, tokens/s = 82849 (39956 source, 42893 target) ; Learning rate = 0.000613 ; Loss = 1.609185\n",
      "2024-12-15 20:10:01.030000: I runner.py:310] Step = 20900 ; steps/s = 1.62, tokens/s = 82747 (39860 source, 42887 target) ; Learning rate = 0.000611 ; Loss = 1.616409\n",
      "2024-12-15 20:11:02.214000: I runner.py:310] Step = 21000 ; steps/s = 1.63, tokens/s = 81835 (39435 source, 42400 target) ; Learning rate = 0.000610 ; Loss = 1.611703\n",
      "2024-12-15 20:12:03.856000: I runner.py:310] Step = 21100 ; steps/s = 1.62, tokens/s = 82791 (39872 source, 42919 target) ; Learning rate = 0.000608 ; Loss = 1.591971\n",
      "2024-12-15 20:13:05.481000: I runner.py:310] Step = 21200 ; steps/s = 1.62, tokens/s = 82861 (39938 source, 42923 target) ; Learning rate = 0.000607 ; Loss = 1.606541\n",
      "2024-12-15 20:14:07.082000: I runner.py:310] Step = 21300 ; steps/s = 1.62, tokens/s = 82851 (39910 source, 42941 target) ; Learning rate = 0.000606 ; Loss = 1.614969\n",
      "2024-12-15 20:15:08.268000: I runner.py:310] Step = 21400 ; steps/s = 1.63, tokens/s = 81814 (39422 source, 42392 target) ; Learning rate = 0.000604 ; Loss = 1.607741\n",
      "2024-12-15 20:16:09.855000: I runner.py:310] Step = 21500 ; steps/s = 1.62, tokens/s = 82906 (39939 source, 42967 target) ; Learning rate = 0.000603 ; Loss = 1.591479\n",
      "2024-12-15 20:17:11.548000: I runner.py:310] Step = 21600 ; steps/s = 1.62, tokens/s = 82717 (39852 source, 42865 target) ; Learning rate = 0.000601 ; Loss = 1.612651\n",
      "2024-12-15 20:18:13.163000: I runner.py:310] Step = 21700 ; steps/s = 1.62, tokens/s = 82811 (39896 source, 42915 target) ; Learning rate = 0.000600 ; Loss = 1.612457\n",
      "2024-12-15 20:19:14.366000: I runner.py:310] Step = 21800 ; steps/s = 1.63, tokens/s = 81835 (39432 source, 42403 target) ; Learning rate = 0.000599 ; Loss = 1.601799\n",
      "2024-12-15 20:20:15.919000: I runner.py:310] Step = 21900 ; steps/s = 1.62, tokens/s = 82922 (39921 source, 43001 target) ; Learning rate = 0.000597 ; Loss = 1.596337\n",
      "2024-12-15 20:21:17.519000: I runner.py:310] Step = 22000 ; steps/s = 1.62, tokens/s = 82870 (39924 source, 42946 target) ; Learning rate = 0.000596 ; Loss = 1.589312\n",
      "2024-12-15 20:22:19.067000: I runner.py:310] Step = 22100 ; steps/s = 1.63, tokens/s = 82915 (39959 source, 42956 target) ; Learning rate = 0.000595 ; Loss = 1.603581\n",
      "2024-12-15 20:23:20.287000: I runner.py:310] Step = 22200 ; steps/s = 1.63, tokens/s = 81824 (39442 source, 42382 target) ; Learning rate = 0.000593 ; Loss = 1.593546\n",
      "2024-12-15 20:24:21.943000: I runner.py:310] Step = 22300 ; steps/s = 1.62, tokens/s = 82750 (39885 source, 42865 target) ; Learning rate = 0.000592 ; Loss = 1.584063\n",
      "2024-12-15 20:25:23.581000: I runner.py:310] Step = 22400 ; steps/s = 1.62, tokens/s = 82792 (39896 source, 42896 target) ; Learning rate = 0.000591 ; Loss = 1.593698\n",
      "2024-12-15 20:26:25.169000: I runner.py:310] Step = 22500 ; steps/s = 1.62, tokens/s = 82874 (39931 source, 42943 target) ; Learning rate = 0.000589 ; Loss = 1.602850\n",
      "2024-12-15 20:27:26.335000: I runner.py:310] Step = 22600 ; steps/s = 1.64, tokens/s = 81903 (39436 source, 42467 target) ; Learning rate = 0.000588 ; Loss = 1.591938\n",
      "2024-12-15 20:28:27.882000: I runner.py:310] Step = 22700 ; steps/s = 1.62, tokens/s = 82923 (39941 source, 42982 target) ; Learning rate = 0.000587 ; Loss = 1.578050\n",
      "2024-12-15 20:29:29.541000: I runner.py:310] Step = 22800 ; steps/s = 1.62, tokens/s = 82741 (39877 source, 42864 target) ; Learning rate = 0.000585 ; Loss = 1.594423\n",
      "2024-12-15 20:30:31.138000: I runner.py:310] Step = 22900 ; steps/s = 1.62, tokens/s = 82906 (39945 source, 42961 target) ; Learning rate = 0.000584 ; Loss = 1.591412\n",
      "2024-12-15 20:31:32.335000: I runner.py:310] Step = 23000 ; steps/s = 1.63, tokens/s = 81845 (39418 source, 42427 target) ; Learning rate = 0.000583 ; Loss = 1.591499\n",
      "2024-12-15 20:32:33.953000: I runner.py:310] Step = 23100 ; steps/s = 1.62, tokens/s = 82836 (39912 source, 42924 target) ; Learning rate = 0.000582 ; Loss = 1.584653\n",
      "2024-12-15 20:33:35.518000: I runner.py:310] Step = 23200 ; steps/s = 1.62, tokens/s = 82904 (39952 source, 42952 target) ; Learning rate = 0.000580 ; Loss = 1.592840\n",
      "2024-12-15 20:34:37.131000: I runner.py:310] Step = 23300 ; steps/s = 1.62, tokens/s = 82845 (39894 source, 42951 target) ; Learning rate = 0.000579 ; Loss = 1.606071\n",
      "2024-12-15 20:35:38.390000: I runner.py:310] Step = 23400 ; steps/s = 1.63, tokens/s = 81736 (39400 source, 42336 target) ; Learning rate = 0.000578 ; Loss = 1.595058\n",
      "2024-12-15 20:36:40.028000: I runner.py:310] Step = 23500 ; steps/s = 1.62, tokens/s = 82816 (39911 source, 42905 target) ; Learning rate = 0.000577 ; Loss = 1.590803\n",
      "2024-12-15 20:37:41.636000: I runner.py:310] Step = 23600 ; steps/s = 1.62, tokens/s = 82828 (39897 source, 42931 target) ; Learning rate = 0.000575 ; Loss = 1.587081\n",
      "2024-12-15 20:38:43.267000: I runner.py:310] Step = 23700 ; steps/s = 1.62, tokens/s = 82795 (39899 source, 42896 target) ; Learning rate = 0.000574 ; Loss = 1.598949\n",
      "2024-12-15 20:39:44.432000: I runner.py:310] Step = 23800 ; steps/s = 1.64, tokens/s = 81881 (39436 source, 42445 target) ; Learning rate = 0.000573 ; Loss = 1.590300\n",
      "2024-12-15 20:40:45.994000: I runner.py:310] Step = 23900 ; steps/s = 1.62, tokens/s = 82870 (39923 source, 42947 target) ; Learning rate = 0.000572 ; Loss = 1.577166\n",
      "2024-12-15 20:41:47.673000: I runner.py:310] Step = 24000 ; steps/s = 1.62, tokens/s = 82784 (39880 source, 42904 target) ; Learning rate = 0.000571 ; Loss = 1.586334\n",
      "2024-12-15 20:42:49.327000: I runner.py:310] Step = 24100 ; steps/s = 1.62, tokens/s = 82800 (39912 source, 42888 target) ; Learning rate = 0.000569 ; Loss = 1.598642\n",
      "2024-12-15 20:43:50.581000: I runner.py:310] Step = 24200 ; steps/s = 1.63, tokens/s = 81767 (39378 source, 42389 target) ; Learning rate = 0.000568 ; Loss = 1.577732\n",
      "2024-12-15 20:44:52.143000: I runner.py:310] Step = 24300 ; steps/s = 1.62, tokens/s = 82890 (39929 source, 42961 target) ; Learning rate = 0.000567 ; Loss = 1.583243\n",
      "2024-12-15 20:45:53.769000: I runner.py:310] Step = 24400 ; steps/s = 1.62, tokens/s = 82827 (39930 source, 42897 target) ; Learning rate = 0.000566 ; Loss = 1.575373\n",
      "2024-12-15 20:46:55.419000: I runner.py:310] Step = 24500 ; steps/s = 1.62, tokens/s = 82778 (39896 source, 42882 target) ; Learning rate = 0.000565 ; Loss = 1.581876\n",
      "2024-12-15 20:47:56.510000: I runner.py:310] Step = 24600 ; steps/s = 1.64, tokens/s = 82009 (39474 source, 42535 target) ; Learning rate = 0.000564 ; Loss = 1.568811\n",
      "2024-12-15 20:48:58.085000: I runner.py:310] Step = 24700 ; steps/s = 1.62, tokens/s = 82883 (39953 source, 42930 target) ; Learning rate = 0.000562 ; Loss = 1.570206\n",
      "2024-12-15 20:49:59.640000: I runner.py:310] Step = 24800 ; steps/s = 1.62, tokens/s = 82904 (39946 source, 42958 target) ; Learning rate = 0.000561 ; Loss = 1.591809\n",
      "2024-12-15 20:51:01.210000: I runner.py:310] Step = 24900 ; steps/s = 1.62, tokens/s = 82891 (39939 source, 42952 target) ; Learning rate = 0.000560 ; Loss = 1.585280\n",
      "2024-12-15 20:52:02.389000: I runner.py:310] Step = 25000 ; steps/s = 1.63, tokens/s = 81877 (39441 source, 42436 target) ; Learning rate = 0.000559 ; Loss = 1.579089\n",
      "2024-12-15 20:52:02.390000: I training.py:192] Running evaluation for step 25000\n",
      "2024-12-15 20:56:31.407000: I training.py:192] Evaluation result for step 25000: loss = 1.084673 ; perplexity = 2.958471\n",
      "2024-12-15 20:57:32.909000: I runner.py:310] Step = 25100 ; steps/s = 1.63, tokens/s = 82988 (39971 source, 43017 target) ; Learning rate = 0.000558 ; Loss = 1.575730\n",
      "2024-12-15 20:58:34.556000: I runner.py:310] Step = 25200 ; steps/s = 1.62, tokens/s = 82765 (39889 source, 42876 target) ; Learning rate = 0.000557 ; Loss = 1.567171\n",
      "2024-12-15 20:59:36.237000: I runner.py:310] Step = 25300 ; steps/s = 1.62, tokens/s = 82738 (39875 source, 42863 target) ; Learning rate = 0.000556 ; Loss = 1.583306\n",
      "2024-12-15 21:00:37.439000: I runner.py:310] Step = 25400 ; steps/s = 1.63, tokens/s = 81846 (39409 source, 42437 target) ; Learning rate = 0.000555 ; Loss = 1.587542\n",
      "2024-12-15 21:01:39.088000: I runner.py:310] Step = 25500 ; steps/s = 1.62, tokens/s = 82812 (39900 source, 42912 target) ; Learning rate = 0.000553 ; Loss = 1.578838\n",
      "2024-12-15 21:02:40.741000: I runner.py:310] Step = 25600 ; steps/s = 1.62, tokens/s = 82785 (39860 source, 42925 target) ; Learning rate = 0.000552 ; Loss = 1.572634\n",
      "2024-12-15 21:03:42.354000: I runner.py:310] Step = 25700 ; steps/s = 1.62, tokens/s = 82828 (39915 source, 42913 target) ; Learning rate = 0.000551 ; Loss = 1.585160\n",
      "2024-12-15 21:04:43.558000: I runner.py:310] Step = 25800 ; steps/s = 1.63, tokens/s = 81855 (39451 source, 42404 target) ; Learning rate = 0.000550 ; Loss = 1.580044\n",
      "2024-12-15 21:05:45.155000: I runner.py:310] Step = 25900 ; steps/s = 1.62, tokens/s = 82831 (39886 source, 42945 target) ; Learning rate = 0.000549 ; Loss = 1.574121\n",
      "2024-12-15 21:06:46.787000: I runner.py:310] Step = 26000 ; steps/s = 1.62, tokens/s = 82787 (39900 source, 42887 target) ; Learning rate = 0.000548 ; Loss = 1.573077\n",
      "2024-12-15 21:07:48.399000: I runner.py:310] Step = 26100 ; steps/s = 1.62, tokens/s = 82846 (39920 source, 42926 target) ; Learning rate = 0.000547 ; Loss = 1.583239\n",
      "2024-12-15 21:08:49.584000: I runner.py:310] Step = 26200 ; steps/s = 1.63, tokens/s = 81861 (39457 source, 42404 target) ; Learning rate = 0.000546 ; Loss = 1.574379\n",
      "2024-12-15 21:09:51.194000: I runner.py:310] Step = 26300 ; steps/s = 1.62, tokens/s = 82845 (39902 source, 42943 target) ; Learning rate = 0.000545 ; Loss = 1.571062\n",
      "2024-12-15 21:10:52.814000: I runner.py:310] Step = 26400 ; steps/s = 1.62, tokens/s = 82806 (39901 source, 42905 target) ; Learning rate = 0.000544 ; Loss = 1.566315\n",
      "2024-12-15 21:11:54.450000: I runner.py:310] Step = 26500 ; steps/s = 1.62, tokens/s = 82811 (39903 source, 42908 target) ; Learning rate = 0.000543 ; Loss = 1.569469\n",
      "2024-12-15 21:12:55.621000: I runner.py:310] Step = 26600 ; steps/s = 1.63, tokens/s = 81880 (39462 source, 42418 target) ; Learning rate = 0.000542 ; Loss = 1.554784\n",
      "2024-12-15 21:13:57.146000: I runner.py:310] Step = 26700 ; steps/s = 1.63, tokens/s = 82968 (39973 source, 42995 target) ; Learning rate = 0.000541 ; Loss = 1.570727\n",
      "2024-12-15 21:14:58.809000: I runner.py:310] Step = 26800 ; steps/s = 1.62, tokens/s = 82756 (39858 source, 42898 target) ; Learning rate = 0.000540 ; Loss = 1.577366\n",
      "2024-12-15 21:16:00.437000: I runner.py:310] Step = 26900 ; steps/s = 1.62, tokens/s = 82843 (39927 source, 42916 target) ; Learning rate = 0.000539 ; Loss = 1.575569\n",
      "2024-12-15 21:17:01.615000: I runner.py:310] Step = 27000 ; steps/s = 1.63, tokens/s = 81816 (39431 source, 42385 target) ; Learning rate = 0.000538 ; Loss = 1.574110\n",
      "2024-12-15 21:18:03.240000: I runner.py:310] Step = 27100 ; steps/s = 1.62, tokens/s = 82803 (39880 source, 42923 target) ; Learning rate = 0.000537 ; Loss = 1.573929\n",
      "2024-12-15 21:19:04.881000: I runner.py:310] Step = 27200 ; steps/s = 1.62, tokens/s = 82813 (39886 source, 42927 target) ; Learning rate = 0.000536 ; Loss = 1.564437\n",
      "2024-12-15 21:20:06.512000: I runner.py:310] Step = 27300 ; steps/s = 1.62, tokens/s = 82850 (39932 source, 42918 target) ; Learning rate = 0.000535 ; Loss = 1.566336\n",
      "2024-12-15 21:21:07.771000: I runner.py:310] Step = 27400 ; steps/s = 1.63, tokens/s = 81737 (39393 source, 42344 target) ; Learning rate = 0.000534 ; Loss = 1.568489\n",
      "2024-12-15 21:22:09.429000: I runner.py:310] Step = 27500 ; steps/s = 1.62, tokens/s = 82771 (39890 source, 42881 target) ; Learning rate = 0.000533 ; Loss = 1.558857\n",
      "2024-12-15 21:23:11.016000: I runner.py:310] Step = 27600 ; steps/s = 1.62, tokens/s = 82879 (39918 source, 42961 target) ; Learning rate = 0.000532 ; Loss = 1.561923\n",
      "2024-12-15 21:24:12.651000: I runner.py:310] Step = 27700 ; steps/s = 1.62, tokens/s = 82820 (39889 source, 42931 target) ; Learning rate = 0.000531 ; Loss = 1.565172\n",
      "2024-12-15 21:25:13.774000: I runner.py:310] Step = 27800 ; steps/s = 1.64, tokens/s = 81916 (39481 source, 42435 target) ; Learning rate = 0.000530 ; Loss = 1.555671\n",
      "2024-12-15 21:26:15.395000: I runner.py:310] Step = 27900 ; steps/s = 1.62, tokens/s = 82821 (39878 source, 42943 target) ; Learning rate = 0.000529 ; Loss = 1.570604\n",
      "2024-12-15 21:27:17.018000: I runner.py:310] Step = 28000 ; steps/s = 1.62, tokens/s = 82843 (39915 source, 42928 target) ; Learning rate = 0.000528 ; Loss = 1.568258\n",
      "2024-12-15 21:28:18.595000: I runner.py:310] Step = 28100 ; steps/s = 1.62, tokens/s = 82881 (39920 source, 42961 target) ; Learning rate = 0.000527 ; Loss = 1.582055\n",
      "2024-12-15 21:29:19.746000: I runner.py:310] Step = 28200 ; steps/s = 1.64, tokens/s = 81879 (39458 source, 42421 target) ; Learning rate = 0.000526 ; Loss = 1.546557\n",
      "2024-12-15 21:30:21.412000: I runner.py:310] Step = 28300 ; steps/s = 1.62, tokens/s = 82754 (39855 source, 42899 target) ; Learning rate = 0.000525 ; Loss = 1.565331\n",
      "2024-12-15 21:31:23.013000: I runner.py:310] Step = 28400 ; steps/s = 1.62, tokens/s = 82853 (39919 source, 42934 target) ; Learning rate = 0.000524 ; Loss = 1.567109\n",
      "2024-12-15 21:32:24.676000: I runner.py:310] Step = 28500 ; steps/s = 1.62, tokens/s = 82797 (39910 source, 42887 target) ; Learning rate = 0.000524 ; Loss = 1.568292\n",
      "2024-12-15 21:33:25.818000: I runner.py:310] Step = 28600 ; steps/s = 1.64, tokens/s = 81926 (39482 source, 42444 target) ; Learning rate = 0.000523 ; Loss = 1.551008\n",
      "2024-12-15 21:34:27.439000: I runner.py:310] Step = 28700 ; steps/s = 1.62, tokens/s = 82822 (39874 source, 42948 target) ; Learning rate = 0.000522 ; Loss = 1.566588\n",
      "2024-12-15 21:35:29.037000: I runner.py:310] Step = 28800 ; steps/s = 1.62, tokens/s = 82847 (39923 source, 42924 target) ; Learning rate = 0.000521 ; Loss = 1.576421\n",
      "2024-12-15 21:36:30.619000: I runner.py:310] Step = 28900 ; steps/s = 1.62, tokens/s = 82885 (39942 source, 42943 target) ; Learning rate = 0.000520 ; Loss = 1.570911\n",
      "2024-12-15 21:37:31.826000: I runner.py:310] Step = 29000 ; steps/s = 1.63, tokens/s = 81834 (39430 source, 42404 target) ; Learning rate = 0.000519 ; Loss = 1.563200\n",
      "2024-12-15 21:38:33.444000: I runner.py:310] Step = 29100 ; steps/s = 1.62, tokens/s = 82829 (39908 source, 42921 target) ; Learning rate = 0.000518 ; Loss = 1.564311\n",
      "2024-12-15 21:39:35.036000: I runner.py:310] Step = 29200 ; steps/s = 1.62, tokens/s = 82866 (39932 source, 42934 target) ; Learning rate = 0.000517 ; Loss = 1.565808\n",
      "2024-12-15 21:40:36.709000: I runner.py:310] Step = 29300 ; steps/s = 1.62, tokens/s = 82727 (39856 source, 42871 target) ; Learning rate = 0.000516 ; Loss = 1.558023\n",
      "2024-12-15 21:41:37.880000: I runner.py:310] Step = 29400 ; steps/s = 1.63, tokens/s = 81890 (39466 source, 42424 target) ; Learning rate = 0.000515 ; Loss = 1.563689\n",
      "2024-12-15 21:42:39.488000: I runner.py:310] Step = 29500 ; steps/s = 1.62, tokens/s = 82835 (39895 source, 42940 target) ; Learning rate = 0.000515 ; Loss = 1.551565\n",
      "2024-12-15 21:43:41.147000: I runner.py:310] Step = 29600 ; steps/s = 1.62, tokens/s = 82787 (39890 source, 42897 target) ; Learning rate = 0.000514 ; Loss = 1.552336\n",
      "2024-12-15 21:44:42.734000: I runner.py:310] Step = 29700 ; steps/s = 1.62, tokens/s = 82860 (39932 source, 42928 target) ; Learning rate = 0.000513 ; Loss = 1.560142\n",
      "2024-12-15 21:45:43.846000: I runner.py:310] Step = 29800 ; steps/s = 1.64, tokens/s = 81979 (39511 source, 42468 target) ; Learning rate = 0.000512 ; Loss = 1.559372\n",
      "2024-12-15 21:46:45.473000: I runner.py:310] Step = 29900 ; steps/s = 1.62, tokens/s = 82805 (39890 source, 42915 target) ; Learning rate = 0.000511 ; Loss = 1.541736\n",
      "2024-12-15 21:47:47.159000: I runner.py:310] Step = 30000 ; steps/s = 1.62, tokens/s = 82740 (39859 source, 42881 target) ; Learning rate = 0.000510 ; Loss = 1.558180\n",
      "2024-12-15 21:47:48.818000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-30000\n",
      "2024-12-15 21:47:48.818000: I training.py:192] Running evaluation for step 30000\n",
      "2024-12-15 21:52:14.566000: I training.py:192] Evaluation result for step 30000: loss = 1.104151 ; perplexity = 3.016663\n",
      "2024-12-15 21:53:16.062000: I runner.py:310] Step = 30100 ; steps/s = 1.63, tokens/s = 82981 (39984 source, 42997 target) ; Learning rate = 0.000509 ; Loss = 1.555559\n",
      "2024-12-15 21:54:17.184000: I runner.py:310] Step = 30200 ; steps/s = 1.64, tokens/s = 81916 (39439 source, 42477 target) ; Learning rate = 0.000509 ; Loss = 1.561870\n",
      "2024-12-15 21:55:18.793000: I runner.py:310] Step = 30300 ; steps/s = 1.62, tokens/s = 82863 (39929 source, 42934 target) ; Learning rate = 0.000508 ; Loss = 1.550021\n",
      "2024-12-15 21:56:20.465000: I runner.py:310] Step = 30400 ; steps/s = 1.62, tokens/s = 82767 (39900 source, 42867 target) ; Learning rate = 0.000507 ; Loss = 1.561393\n",
      "2024-12-15 21:57:22.076000: I runner.py:310] Step = 30500 ; steps/s = 1.62, tokens/s = 82833 (39903 source, 42930 target) ; Learning rate = 0.000506 ; Loss = 1.558411\n",
      "2024-12-15 21:58:23.242000: I runner.py:310] Step = 30600 ; steps/s = 1.64, tokens/s = 81868 (39419 source, 42449 target) ; Learning rate = 0.000505 ; Loss = 1.550674\n",
      "2024-12-15 21:59:24.856000: I runner.py:310] Step = 30700 ; steps/s = 1.62, tokens/s = 82857 (39908 source, 42949 target) ; Learning rate = 0.000504 ; Loss = 1.550616\n",
      "2024-12-15 22:00:26.460000: I runner.py:310] Step = 30800 ; steps/s = 1.62, tokens/s = 82852 (39940 source, 42912 target) ; Learning rate = 0.000504 ; Loss = 1.556157\n",
      "2024-12-15 22:01:28.149000: I runner.py:310] Step = 30900 ; steps/s = 1.62, tokens/s = 82721 (39860 source, 42861 target) ; Learning rate = 0.000503 ; Loss = 1.561453\n",
      "2024-12-15 22:02:29.296000: I runner.py:310] Step = 31000 ; steps/s = 1.64, tokens/s = 81905 (39476 source, 42429 target) ; Learning rate = 0.000502 ; Loss = 1.544799\n",
      "2024-12-15 22:03:30.950000: I runner.py:310] Step = 31100 ; steps/s = 1.62, tokens/s = 82783 (39865 source, 42918 target) ; Learning rate = 0.000501 ; Loss = 1.554819\n",
      "2024-12-15 22:04:32.576000: I runner.py:310] Step = 31200 ; steps/s = 1.62, tokens/s = 82829 (39909 source, 42920 target) ; Learning rate = 0.000500 ; Loss = 1.562223\n",
      "2024-12-15 22:05:33.861000: I runner.py:310] Step = 31300 ; steps/s = 1.63, tokens/s = 82031 (39531 source, 42500 target) ; Learning rate = 0.000500 ; Loss = 1.560125\n",
      "2024-12-15 22:06:35.401000: I runner.py:310] Step = 31400 ; steps/s = 1.63, tokens/s = 82617 (39798 source, 42819 target) ; Learning rate = 0.000499 ; Loss = 1.555309\n",
      "2024-12-15 22:07:37.085000: I runner.py:310] Step = 31500 ; steps/s = 1.62, tokens/s = 82741 (39890 source, 42851 target) ; Learning rate = 0.000498 ; Loss = 1.548303\n",
      "2024-12-15 22:08:38.729000: I runner.py:310] Step = 31600 ; steps/s = 1.62, tokens/s = 82784 (39904 source, 42880 target) ; Learning rate = 0.000497 ; Loss = 1.553964\n",
      "2024-12-15 22:09:39.924000: I runner.py:310] Step = 31700 ; steps/s = 1.63, tokens/s = 81840 (39403 source, 42437 target) ; Learning rate = 0.000496 ; Loss = 1.553804\n",
      "2024-12-15 22:10:41.583000: I runner.py:310] Step = 31800 ; steps/s = 1.62, tokens/s = 82777 (39846 source, 42931 target) ; Learning rate = 0.000496 ; Loss = 1.545877\n",
      "2024-12-15 22:11:43.279000: I runner.py:310] Step = 31900 ; steps/s = 1.62, tokens/s = 82724 (39873 source, 42851 target) ; Learning rate = 0.000495 ; Loss = 1.550615\n",
      "2024-12-15 22:12:44.911000: I runner.py:310] Step = 32000 ; steps/s = 1.62, tokens/s = 82824 (39932 source, 42892 target) ; Learning rate = 0.000494 ; Loss = 1.557422\n",
      "2024-12-15 22:13:46.101000: I runner.py:310] Step = 32100 ; steps/s = 1.63, tokens/s = 81832 (39409 source, 42423 target) ; Learning rate = 0.000493 ; Loss = 1.549585\n",
      "2024-12-15 22:14:47.673000: I runner.py:310] Step = 32200 ; steps/s = 1.62, tokens/s = 82925 (39933 source, 42992 target) ; Learning rate = 0.000493 ; Loss = 1.554077\n",
      "2024-12-15 22:15:49.331000: I runner.py:310] Step = 32300 ; steps/s = 1.62, tokens/s = 82777 (39870 source, 42907 target) ; Learning rate = 0.000492 ; Loss = 1.545750\n",
      "2024-12-15 22:16:50.937000: I runner.py:310] Step = 32400 ; steps/s = 1.62, tokens/s = 82853 (39922 source, 42931 target) ; Learning rate = 0.000491 ; Loss = 1.546182\n",
      "2024-12-15 22:17:52.106000: I runner.py:310] Step = 32500 ; steps/s = 1.64, tokens/s = 81845 (39454 source, 42391 target) ; Learning rate = 0.000490 ; Loss = 1.546420\n",
      "2024-12-15 22:18:53.714000: I runner.py:310] Step = 32600 ; steps/s = 1.62, tokens/s = 82837 (39891 source, 42946 target) ; Learning rate = 0.000490 ; Loss = 1.547368\n",
      "2024-12-15 22:19:55.306000: I runner.py:310] Step = 32700 ; steps/s = 1.62, tokens/s = 82885 (39933 source, 42952 target) ; Learning rate = 0.000489 ; Loss = 1.539168\n",
      "2024-12-15 22:20:56.916000: I runner.py:310] Step = 32800 ; steps/s = 1.62, tokens/s = 82845 (39898 source, 42947 target) ; Learning rate = 0.000488 ; Loss = 1.539614\n",
      "2024-12-15 22:21:58.117000: I runner.py:310] Step = 32900 ; steps/s = 1.63, tokens/s = 81814 (39460 source, 42354 target) ; Learning rate = 0.000487 ; Loss = 1.534792\n",
      "2024-12-15 22:22:59.767000: I runner.py:310] Step = 33000 ; steps/s = 1.62, tokens/s = 82783 (39876 source, 42907 target) ; Learning rate = 0.000487 ; Loss = 1.544583\n",
      "2024-12-15 22:24:01.402000: I runner.py:310] Step = 33100 ; steps/s = 1.62, tokens/s = 82790 (39897 source, 42893 target) ; Learning rate = 0.000486 ; Loss = 1.557329\n",
      "2024-12-15 22:25:03.056000: I runner.py:310] Step = 33200 ; steps/s = 1.62, tokens/s = 82781 (39895 source, 42886 target) ; Learning rate = 0.000485 ; Loss = 1.553755\n",
      "2024-12-15 22:26:04.282000: I runner.py:310] Step = 33300 ; steps/s = 1.63, tokens/s = 81810 (39395 source, 42415 target) ; Learning rate = 0.000484 ; Loss = 1.548176\n",
      "2024-12-15 22:27:05.917000: I runner.py:310] Step = 33400 ; steps/s = 1.62, tokens/s = 82837 (39921 source, 42916 target) ; Learning rate = 0.000484 ; Loss = 1.550377\n",
      "2024-12-15 22:28:07.494000: I runner.py:310] Step = 33500 ; steps/s = 1.62, tokens/s = 82864 (39925 source, 42939 target) ; Learning rate = 0.000483 ; Loss = 1.544086\n",
      "2024-12-15 22:29:09.094000: I runner.py:310] Step = 33600 ; steps/s = 1.62, tokens/s = 82848 (39928 source, 42920 target) ; Learning rate = 0.000482 ; Loss = 1.539547\n",
      "2024-12-15 22:30:10.203000: I runner.py:310] Step = 33700 ; steps/s = 1.64, tokens/s = 81947 (39460 source, 42487 target) ; Learning rate = 0.000481 ; Loss = 1.534545\n",
      "2024-12-15 22:31:11.817000: I runner.py:310] Step = 33800 ; steps/s = 1.62, tokens/s = 82848 (39915 source, 42933 target) ; Learning rate = 0.000481 ; Loss = 1.544842\n",
      "2024-12-15 22:32:13.527000: I runner.py:310] Step = 33900 ; steps/s = 1.62, tokens/s = 82699 (39851 source, 42848 target) ; Learning rate = 0.000480 ; Loss = 1.551384\n",
      "2024-12-15 22:33:15.121000: I runner.py:310] Step = 34000 ; steps/s = 1.62, tokens/s = 82876 (39933 source, 42943 target) ; Learning rate = 0.000479 ; Loss = 1.552253\n",
      "2024-12-15 22:34:16.249000: I runner.py:310] Step = 34100 ; steps/s = 1.64, tokens/s = 81930 (39469 source, 42461 target) ; Learning rate = 0.000479 ; Loss = 1.547837\n",
      "2024-12-15 22:35:17.875000: I runner.py:310] Step = 34200 ; steps/s = 1.62, tokens/s = 82817 (39890 source, 42927 target) ; Learning rate = 0.000478 ; Loss = 1.538574\n",
      "2024-12-15 22:36:19.535000: I runner.py:310] Step = 34300 ; steps/s = 1.62, tokens/s = 82763 (39885 source, 42878 target) ; Learning rate = 0.000477 ; Loss = 1.535876\n",
      "2024-12-15 22:37:21.172000: I runner.py:310] Step = 34400 ; steps/s = 1.62, tokens/s = 82809 (39911 source, 42898 target) ; Learning rate = 0.000477 ; Loss = 1.553951\n",
      "2024-12-15 22:38:22.340000: I runner.py:310] Step = 34500 ; steps/s = 1.64, tokens/s = 81880 (39452 source, 42428 target) ; Learning rate = 0.000476 ; Loss = 1.549883\n",
      "2024-12-15 22:39:23.921000: I runner.py:310] Step = 34600 ; steps/s = 1.62, tokens/s = 82888 (39928 source, 42960 target) ; Learning rate = 0.000475 ; Loss = 1.541839\n",
      "2024-12-15 22:40:25.568000: I runner.py:310] Step = 34700 ; steps/s = 1.62, tokens/s = 82793 (39904 source, 42889 target) ; Learning rate = 0.000474 ; Loss = 1.536938\n",
      "2024-12-15 22:41:27.135000: I runner.py:310] Step = 34800 ; steps/s = 1.62, tokens/s = 82896 (39939 source, 42957 target) ; Learning rate = 0.000474 ; Loss = 1.540023\n",
      "2024-12-15 22:42:28.286000: I runner.py:310] Step = 34900 ; steps/s = 1.64, tokens/s = 81914 (39452 source, 42462 target) ; Learning rate = 0.000473 ; Loss = 1.544550\n",
      "2024-12-15 22:43:29.884000: I runner.py:310] Step = 35000 ; steps/s = 1.62, tokens/s = 82865 (39930 source, 42935 target) ; Learning rate = 0.000472 ; Loss = 1.533935\n",
      "2024-12-15 22:43:29.886000: I training.py:192] Running evaluation for step 35000\n",
      "2024-12-15 22:47:45.334000: I training.py:192] Evaluation result for step 35000: loss = 1.125696 ; perplexity = 3.082361\n",
      "2024-12-15 22:48:46.790000: I runner.py:310] Step = 35100 ; steps/s = 1.63, tokens/s = 83059 (40005 source, 43054 target) ; Learning rate = 0.000472 ; Loss = 1.541049\n",
      "2024-12-15 22:49:48.363000: I runner.py:310] Step = 35200 ; steps/s = 1.62, tokens/s = 82873 (39957 source, 42916 target) ; Learning rate = 0.000471 ; Loss = 1.543823\n",
      "2024-12-15 22:50:49.482000: I runner.py:310] Step = 35300 ; steps/s = 1.64, tokens/s = 81947 (39452 source, 42495 target) ; Learning rate = 0.000470 ; Loss = 1.536870\n",
      "2024-12-15 22:51:50.993000: I runner.py:310] Step = 35400 ; steps/s = 1.63, tokens/s = 82994 (39976 source, 43018 target) ; Learning rate = 0.000470 ; Loss = 1.542860\n",
      "2024-12-15 22:52:52.564000: I runner.py:310] Step = 35500 ; steps/s = 1.62, tokens/s = 82894 (39961 source, 42933 target) ; Learning rate = 0.000469 ; Loss = 1.545270\n",
      "2024-12-15 22:53:54.191000: I runner.py:310] Step = 35600 ; steps/s = 1.62, tokens/s = 82811 (39900 source, 42911 target) ; Learning rate = 0.000468 ; Loss = 1.539661\n",
      "2024-12-15 22:54:55.328000: I runner.py:310] Step = 35700 ; steps/s = 1.64, tokens/s = 81899 (39462 source, 42437 target) ; Learning rate = 0.000468 ; Loss = 1.540835\n",
      "2024-12-15 22:55:56.976000: I runner.py:310] Step = 35800 ; steps/s = 1.62, tokens/s = 82791 (39885 source, 42906 target) ; Learning rate = 0.000467 ; Loss = 1.540608\n",
      "2024-12-15 22:56:58.560000: I runner.py:310] Step = 35900 ; steps/s = 1.62, tokens/s = 82850 (39905 source, 42945 target) ; Learning rate = 0.000466 ; Loss = 1.544242\n",
      "2024-12-15 22:58:00.453000: I runner.py:310] Step = 36000 ; steps/s = 1.62, tokens/s = 82501 (39756 source, 42745 target) ; Learning rate = 0.000466 ; Loss = 1.542665\n",
      "2024-12-15 22:59:01.680000: I runner.py:310] Step = 36100 ; steps/s = 1.63, tokens/s = 81752 (39390 source, 42362 target) ; Learning rate = 0.000465 ; Loss = 1.535571\n",
      "2024-12-15 23:00:03.222000: I runner.py:310] Step = 36200 ; steps/s = 1.63, tokens/s = 82928 (39917 source, 43011 target) ; Learning rate = 0.000465 ; Loss = 1.534247\n",
      "2024-12-15 23:01:04.802000: I runner.py:310] Step = 36300 ; steps/s = 1.62, tokens/s = 82845 (39949 source, 42896 target) ; Learning rate = 0.000464 ; Loss = 1.531987\n",
      "2024-12-15 23:02:06.407000: I runner.py:310] Step = 36400 ; steps/s = 1.62, tokens/s = 82886 (39933 source, 42953 target) ; Learning rate = 0.000463 ; Loss = 1.548400\n",
      "2024-12-15 23:03:07.537000: I runner.py:310] Step = 36500 ; steps/s = 1.64, tokens/s = 81975 (39496 source, 42479 target) ; Learning rate = 0.000463 ; Loss = 1.545306\n",
      "2024-12-15 23:04:09.075000: I runner.py:310] Step = 36600 ; steps/s = 1.63, tokens/s = 82928 (39954 source, 42974 target) ; Learning rate = 0.000462 ; Loss = 1.529481\n",
      "2024-12-15 23:05:10.726000: I runner.py:310] Step = 36700 ; steps/s = 1.62, tokens/s = 82806 (39895 source, 42911 target) ; Learning rate = 0.000461 ; Loss = 1.536727\n",
      "2024-12-15 23:06:12.307000: I runner.py:310] Step = 36800 ; steps/s = 1.62, tokens/s = 82833 (39927 source, 42906 target) ; Learning rate = 0.000461 ; Loss = 1.538166\n",
      "2024-12-15 23:07:13.484000: I runner.py:310] Step = 36900 ; steps/s = 1.63, tokens/s = 81873 (39443 source, 42430 target) ; Learning rate = 0.000460 ; Loss = 1.527953\n",
      "2024-12-15 23:08:15.098000: I runner.py:310] Step = 37000 ; steps/s = 1.62, tokens/s = 82816 (39866 source, 42950 target) ; Learning rate = 0.000460 ; Loss = 1.540104\n",
      "2024-12-15 23:09:16.745000: I runner.py:310] Step = 37100 ; steps/s = 1.62, tokens/s = 82798 (39887 source, 42911 target) ; Learning rate = 0.000459 ; Loss = 1.544921\n",
      "2024-12-15 23:10:18.365000: I runner.py:310] Step = 37200 ; steps/s = 1.62, tokens/s = 82825 (39936 source, 42889 target) ; Learning rate = 0.000458 ; Loss = 1.538786\n",
      "2024-12-15 23:11:19.507000: I runner.py:310] Step = 37300 ; steps/s = 1.64, tokens/s = 81926 (39461 source, 42465 target) ; Learning rate = 0.000458 ; Loss = 1.527489\n",
      "2024-12-15 23:12:21.070000: I runner.py:310] Step = 37400 ; steps/s = 1.62, tokens/s = 82907 (39937 source, 42970 target) ; Learning rate = 0.000457 ; Loss = 1.537150\n",
      "2024-12-15 23:13:22.617000: I runner.py:310] Step = 37500 ; steps/s = 1.62, tokens/s = 82922 (39966 source, 42956 target) ; Learning rate = 0.000456 ; Loss = 1.537940\n",
      "2024-12-15 23:14:24.248000: I runner.py:310] Step = 37600 ; steps/s = 1.62, tokens/s = 82810 (39881 source, 42929 target) ; Learning rate = 0.000456 ; Loss = 1.538103\n",
      "2024-12-15 23:15:25.317000: I runner.py:310] Step = 37700 ; steps/s = 1.64, tokens/s = 82016 (39529 source, 42487 target) ; Learning rate = 0.000455 ; Loss = 1.541579\n",
      "2024-12-15 23:16:26.880000: I runner.py:310] Step = 37800 ; steps/s = 1.62, tokens/s = 82912 (39940 source, 42972 target) ; Learning rate = 0.000455 ; Loss = 1.533168\n",
      "2024-12-15 23:17:28.482000: I runner.py:310] Step = 37900 ; steps/s = 1.62, tokens/s = 82858 (39923 source, 42935 target) ; Learning rate = 0.000454 ; Loss = 1.540510\n",
      "2024-12-15 23:18:30.049000: I runner.py:310] Step = 38000 ; steps/s = 1.62, tokens/s = 82887 (39953 source, 42934 target) ; Learning rate = 0.000453 ; Loss = 1.536420\n",
      "2024-12-15 23:19:31.187000: I runner.py:310] Step = 38100 ; steps/s = 1.64, tokens/s = 81904 (39450 source, 42454 target) ; Learning rate = 0.000453 ; Loss = 1.541597\n",
      "2024-12-15 23:20:32.740000: I runner.py:310] Step = 38200 ; steps/s = 1.62, tokens/s = 82936 (39933 source, 43003 target) ; Learning rate = 0.000452 ; Loss = 1.530512\n",
      "2024-12-15 23:21:34.408000: I runner.py:310] Step = 38300 ; steps/s = 1.62, tokens/s = 82755 (39887 source, 42868 target) ; Learning rate = 0.000452 ; Loss = 1.533930\n",
      "2024-12-15 23:22:35.990000: I runner.py:310] Step = 38400 ; steps/s = 1.62, tokens/s = 82871 (39939 source, 42932 target) ; Learning rate = 0.000451 ; Loss = 1.534176\n",
      "2024-12-15 23:23:37.077000: I runner.py:310] Step = 38500 ; steps/s = 1.64, tokens/s = 81982 (39485 source, 42497 target) ; Learning rate = 0.000450 ; Loss = 1.527849\n",
      "2024-12-15 23:24:38.694000: I runner.py:310] Step = 38600 ; steps/s = 1.62, tokens/s = 82808 (39893 source, 42915 target) ; Learning rate = 0.000450 ; Loss = 1.531375\n",
      "2024-12-15 23:25:40.403000: I runner.py:310] Step = 38700 ; steps/s = 1.62, tokens/s = 82739 (39857 source, 42882 target) ; Learning rate = 0.000449 ; Loss = 1.534455\n",
      "2024-12-15 23:26:42.056000: I runner.py:310] Step = 38800 ; steps/s = 1.62, tokens/s = 82788 (39920 source, 42868 target) ; Learning rate = 0.000449 ; Loss = 1.538490\n",
      "2024-12-15 23:27:43.191000: I runner.py:310] Step = 38900 ; steps/s = 1.64, tokens/s = 81927 (39453 source, 42474 target) ; Learning rate = 0.000448 ; Loss = 1.536104\n",
      "2024-12-15 23:28:44.862000: I runner.py:310] Step = 39000 ; steps/s = 1.62, tokens/s = 82720 (39837 source, 42883 target) ; Learning rate = 0.000448 ; Loss = 1.521678\n",
      "2024-12-15 23:29:46.407000: I runner.py:310] Step = 39100 ; steps/s = 1.62, tokens/s = 82973 (39971 source, 43002 target) ; Learning rate = 0.000447 ; Loss = 1.524364\n",
      "2024-12-15 23:30:47.989000: I runner.py:310] Step = 39200 ; steps/s = 1.62, tokens/s = 82875 (39968 source, 42907 target) ; Learning rate = 0.000446 ; Loss = 1.533425\n",
      "2024-12-15 23:31:49.133000: I runner.py:310] Step = 39300 ; steps/s = 1.64, tokens/s = 81880 (39447 source, 42433 target) ; Learning rate = 0.000446 ; Loss = 1.533024\n",
      "2024-12-15 23:32:50.727000: I runner.py:310] Step = 39400 ; steps/s = 1.62, tokens/s = 82894 (39930 source, 42964 target) ; Learning rate = 0.000445 ; Loss = 1.534045\n",
      "2024-12-15 23:33:52.341000: I runner.py:310] Step = 39500 ; steps/s = 1.62, tokens/s = 82826 (39920 source, 42906 target) ; Learning rate = 0.000445 ; Loss = 1.532029\n",
      "2024-12-15 23:34:53.851000: I runner.py:310] Step = 39600 ; steps/s = 1.63, tokens/s = 82978 (39963 source, 43015 target) ; Learning rate = 0.000444 ; Loss = 1.534773\n",
      "2024-12-15 23:35:54.983000: I runner.py:310] Step = 39700 ; steps/s = 1.64, tokens/s = 81929 (39474 source, 42455 target) ; Learning rate = 0.000444 ; Loss = 1.535110\n",
      "2024-12-15 23:36:56.544000: I runner.py:310] Step = 39800 ; steps/s = 1.62, tokens/s = 82905 (39944 source, 42961 target) ; Learning rate = 0.000443 ; Loss = 1.528131\n",
      "2024-12-15 23:37:58.133000: I runner.py:310] Step = 39900 ; steps/s = 1.62, tokens/s = 82853 (39918 source, 42935 target) ; Learning rate = 0.000442 ; Loss = 1.532495\n",
      "2024-12-15 23:38:59.730000: I runner.py:310] Step = 40000 ; steps/s = 1.62, tokens/s = 82851 (39928 source, 42923 target) ; Learning rate = 0.000442 ; Loss = 1.526277\n",
      "2024-12-15 23:39:01.294000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-40000\n",
      "2024-12-15 23:39:01.294000: I training.py:192] Running evaluation for step 40000\n",
      "2024-12-15 23:43:16.903000: I training.py:192] Evaluation result for step 40000: loss = 1.135921 ; perplexity = 3.114042\n",
      "2024-12-15 23:44:17.908000: I runner.py:310] Step = 40100 ; steps/s = 1.64, tokens/s = 82112 (39549 source, 42563 target) ; Learning rate = 0.000441 ; Loss = 1.531346\n",
      "2024-12-15 23:45:19.504000: I runner.py:310] Step = 40200 ; steps/s = 1.62, tokens/s = 82856 (39915 source, 42941 target) ; Learning rate = 0.000441 ; Loss = 1.524845\n",
      "2024-12-15 23:46:21.137000: I runner.py:310] Step = 40300 ; steps/s = 1.62, tokens/s = 82808 (39907 source, 42901 target) ; Learning rate = 0.000440 ; Loss = 1.527658\n",
      "2024-12-15 23:47:22.689000: I runner.py:310] Step = 40400 ; steps/s = 1.62, tokens/s = 82935 (39966 source, 42969 target) ; Learning rate = 0.000440 ; Loss = 1.526606\n",
      "2024-12-15 23:48:23.784000: I runner.py:310] Step = 40500 ; steps/s = 1.64, tokens/s = 81964 (39479 source, 42485 target) ; Learning rate = 0.000439 ; Loss = 1.512941\n",
      "2024-12-15 23:49:25.351000: I runner.py:310] Step = 40600 ; steps/s = 1.62, tokens/s = 82909 (39967 source, 42942 target) ; Learning rate = 0.000439 ; Loss = 1.529983\n",
      "2024-12-15 23:50:26.992000: I runner.py:310] Step = 40700 ; steps/s = 1.62, tokens/s = 82778 (39882 source, 42896 target) ; Learning rate = 0.000438 ; Loss = 1.529300\n",
      "2024-12-15 23:51:28.607000: I runner.py:310] Step = 40800 ; steps/s = 1.62, tokens/s = 82854 (39898 source, 42956 target) ; Learning rate = 0.000438 ; Loss = 1.532475\n",
      "2024-12-15 23:52:29.817000: I runner.py:310] Step = 40900 ; steps/s = 1.63, tokens/s = 81823 (39420 source, 42403 target) ; Learning rate = 0.000437 ; Loss = 1.532269\n",
      "2024-12-15 23:53:31.433000: I runner.py:310] Step = 41000 ; steps/s = 1.62, tokens/s = 82850 (39941 source, 42909 target) ; Learning rate = 0.000437 ; Loss = 1.524873\n",
      "2024-12-15 23:54:33.054000: I runner.py:310] Step = 41100 ; steps/s = 1.62, tokens/s = 82819 (39888 source, 42931 target) ; Learning rate = 0.000436 ; Loss = 1.533479\n",
      "2024-12-15 23:55:34.572000: I runner.py:310] Step = 41200 ; steps/s = 1.63, tokens/s = 82948 (39960 source, 42988 target) ; Learning rate = 0.000435 ; Loss = 1.528296\n",
      "2024-12-15 23:56:35.642000: I runner.py:310] Step = 41300 ; steps/s = 1.64, tokens/s = 81994 (39482 source, 42512 target) ; Learning rate = 0.000435 ; Loss = 1.533010\n",
      "2024-12-15 23:57:37.255000: I runner.py:310] Step = 41400 ; steps/s = 1.62, tokens/s = 82845 (39929 source, 42916 target) ; Learning rate = 0.000434 ; Loss = 1.524122\n",
      "2024-12-15 23:58:38.801000: I runner.py:310] Step = 41500 ; steps/s = 1.63, tokens/s = 82928 (39929 source, 42999 target) ; Learning rate = 0.000434 ; Loss = 1.528801\n",
      "2024-12-15 23:59:40.232000: I runner.py:310] Step = 41600 ; steps/s = 1.63, tokens/s = 82550 (39798 source, 42752 target) ; Learning rate = 0.000433 ; Loss = 1.539347\n",
      "2024-12-16 00:00:41.500000: I runner.py:310] Step = 41700 ; steps/s = 1.63, tokens/s = 82280 (39675 source, 42605 target) ; Learning rate = 0.000433 ; Loss = 1.532717\n",
      "2024-12-16 00:01:43.093000: I runner.py:310] Step = 41800 ; steps/s = 1.62, tokens/s = 82906 (39929 source, 42977 target) ; Learning rate = 0.000432 ; Loss = 1.521152\n",
      "2024-12-16 00:02:44.717000: I runner.py:310] Step = 41900 ; steps/s = 1.62, tokens/s = 82805 (39912 source, 42893 target) ; Learning rate = 0.000432 ; Loss = 1.524725\n",
      "2024-12-16 00:03:45.844000: I runner.py:310] Step = 42000 ; steps/s = 1.64, tokens/s = 81899 (39447 source, 42452 target) ; Learning rate = 0.000431 ; Loss = 1.521598\n",
      "2024-12-16 00:04:47.467000: I runner.py:310] Step = 42100 ; steps/s = 1.62, tokens/s = 82862 (39927 source, 42935 target) ; Learning rate = 0.000431 ; Loss = 1.521691\n",
      "2024-12-16 00:05:49.032000: I runner.py:310] Step = 42200 ; steps/s = 1.62, tokens/s = 82897 (39952 source, 42945 target) ; Learning rate = 0.000430 ; Loss = 1.527307\n",
      "2024-12-16 00:06:50.619000: I runner.py:310] Step = 42300 ; steps/s = 1.62, tokens/s = 82885 (39937 source, 42948 target) ; Learning rate = 0.000430 ; Loss = 1.519618\n",
      "2024-12-16 00:07:51.791000: I runner.py:310] Step = 42400 ; steps/s = 1.63, tokens/s = 81823 (39397 source, 42426 target) ; Learning rate = 0.000429 ; Loss = 1.524061\n",
      "2024-12-16 00:08:53.319000: I runner.py:310] Step = 42500 ; steps/s = 1.63, tokens/s = 82966 (39958 source, 43008 target) ; Learning rate = 0.000429 ; Loss = 1.529977\n",
      "2024-12-16 00:09:54.922000: I runner.py:310] Step = 42600 ; steps/s = 1.62, tokens/s = 82829 (39917 source, 42912 target) ; Learning rate = 0.000428 ; Loss = 1.521777\n",
      "2024-12-16 00:10:56.553000: I runner.py:310] Step = 42700 ; steps/s = 1.62, tokens/s = 82830 (39909 source, 42921 target) ; Learning rate = 0.000428 ; Loss = 1.529461\n",
      "2024-12-16 00:11:57.684000: I runner.py:310] Step = 42800 ; steps/s = 1.64, tokens/s = 81920 (39472 source, 42448 target) ; Learning rate = 0.000427 ; Loss = 1.527315\n",
      "2024-12-16 00:12:59.229000: I runner.py:310] Step = 42900 ; steps/s = 1.62, tokens/s = 82926 (39974 source, 42952 target) ; Learning rate = 0.000427 ; Loss = 1.520077\n",
      "2024-12-16 00:14:00.802000: I runner.py:310] Step = 43000 ; steps/s = 1.62, tokens/s = 82904 (39935 source, 42969 target) ; Learning rate = 0.000426 ; Loss = 1.522047\n",
      "2024-12-16 00:15:02.366000: I runner.py:310] Step = 43100 ; steps/s = 1.62, tokens/s = 82900 (39931 source, 42969 target) ; Learning rate = 0.000426 ; Loss = 1.522028\n",
      "2024-12-16 00:16:03.496000: I runner.py:310] Step = 43200 ; steps/s = 1.64, tokens/s = 81927 (39456 source, 42471 target) ; Learning rate = 0.000425 ; Loss = 1.523756\n",
      "2024-12-16 00:17:05.065000: I runner.py:310] Step = 43300 ; steps/s = 1.62, tokens/s = 82905 (39927 source, 42978 target) ; Learning rate = 0.000425 ; Loss = 1.521395\n",
      "2024-12-16 00:18:06.620000: I runner.py:310] Step = 43400 ; steps/s = 1.62, tokens/s = 82894 (39964 source, 42930 target) ; Learning rate = 0.000424 ; Loss = 1.526239\n",
      "2024-12-16 00:19:08.163000: I runner.py:310] Step = 43500 ; steps/s = 1.63, tokens/s = 82926 (39963 source, 42963 target) ; Learning rate = 0.000424 ; Loss = 1.528695\n",
      "2024-12-16 00:20:09.281000: I runner.py:310] Step = 43600 ; steps/s = 1.64, tokens/s = 81957 (39489 source, 42468 target) ; Learning rate = 0.000423 ; Loss = 1.522678\n",
      "2024-12-16 00:21:10.868000: I runner.py:310] Step = 43700 ; steps/s = 1.62, tokens/s = 82889 (39915 source, 42974 target) ; Learning rate = 0.000423 ; Loss = 1.515881\n",
      "2024-12-16 00:22:12.440000: I runner.py:310] Step = 43800 ; steps/s = 1.62, tokens/s = 82875 (39922 source, 42953 target) ; Learning rate = 0.000422 ; Loss = 1.527212\n",
      "2024-12-16 00:23:14.084000: I runner.py:310] Step = 43900 ; steps/s = 1.62, tokens/s = 82782 (39894 source, 42888 target) ; Learning rate = 0.000422 ; Loss = 1.530968\n",
      "2024-12-16 00:24:15.206000: I runner.py:310] Step = 44000 ; steps/s = 1.64, tokens/s = 81929 (39482 source, 42447 target) ; Learning rate = 0.000421 ; Loss = 1.522702\n",
      "2024-12-16 00:25:16.785000: I runner.py:310] Step = 44100 ; steps/s = 1.62, tokens/s = 82879 (39954 source, 42925 target) ; Learning rate = 0.000421 ; Loss = 1.523008\n",
      "2024-12-16 00:26:18.357000: I runner.py:310] Step = 44200 ; steps/s = 1.62, tokens/s = 82912 (39941 source, 42971 target) ; Learning rate = 0.000420 ; Loss = 1.520099\n",
      "2024-12-16 00:27:20.015000: I runner.py:310] Step = 44300 ; steps/s = 1.62, tokens/s = 82755 (39887 source, 42868 target) ; Learning rate = 0.000420 ; Loss = 1.517653\n",
      "2024-12-16 00:28:21.098000: I runner.py:310] Step = 44400 ; steps/s = 1.64, tokens/s = 81994 (39481 source, 42513 target) ; Learning rate = 0.000419 ; Loss = 1.517629\n",
      "2024-12-16 00:29:22.671000: I runner.py:310] Step = 44500 ; steps/s = 1.62, tokens/s = 82916 (39924 source, 42992 target) ; Learning rate = 0.000419 ; Loss = 1.530772\n",
      "2024-12-16 00:30:24.235000: I runner.py:310] Step = 44600 ; steps/s = 1.62, tokens/s = 82863 (39915 source, 42948 target) ; Learning rate = 0.000419 ; Loss = 1.516083\n",
      "2024-12-16 00:31:25.808000: I runner.py:310] Step = 44700 ; steps/s = 1.62, tokens/s = 82901 (39950 source, 42951 target) ; Learning rate = 0.000418 ; Loss = 1.526352\n",
      "2024-12-16 00:32:27.039000: I runner.py:310] Step = 44800 ; steps/s = 1.63, tokens/s = 81785 (39398 source, 42387 target) ; Learning rate = 0.000418 ; Loss = 1.513115\n",
      "2024-12-16 00:33:28.632000: I runner.py:310] Step = 44900 ; steps/s = 1.62, tokens/s = 82883 (39960 source, 42923 target) ; Learning rate = 0.000417 ; Loss = 1.524687\n",
      "2024-12-16 00:34:30.253000: I runner.py:310] Step = 45000 ; steps/s = 1.62, tokens/s = 82822 (39921 source, 42901 target) ; Learning rate = 0.000417 ; Loss = 1.515266\n",
      "2024-12-16 00:34:30.254000: I training.py:192] Running evaluation for step 45000\n",
      "2024-12-16 00:38:48.274000: I training.py:192] Evaluation result for step 45000: loss = 1.144946 ; perplexity = 3.142272\n",
      "2024-12-16 00:39:49.813000: I runner.py:310] Step = 45100 ; steps/s = 1.63, tokens/s = 82948 (39948 source, 43000 target) ; Learning rate = 0.000416 ; Loss = 1.519926\n",
      "2024-12-16 00:40:51.050000: I runner.py:310] Step = 45200 ; steps/s = 1.63, tokens/s = 81782 (39391 source, 42391 target) ; Learning rate = 0.000416 ; Loss = 1.523423\n",
      "2024-12-16 00:41:52.674000: I runner.py:310] Step = 45300 ; steps/s = 1.62, tokens/s = 82871 (39892 source, 42979 target) ; Learning rate = 0.000415 ; Loss = 1.520014\n",
      "2024-12-16 00:42:54.228000: I runner.py:310] Step = 45400 ; steps/s = 1.62, tokens/s = 82900 (39959 source, 42941 target) ; Learning rate = 0.000415 ; Loss = 1.519271\n",
      "2024-12-16 00:43:55.851000: I runner.py:310] Step = 45500 ; steps/s = 1.62, tokens/s = 82783 (39916 source, 42867 target) ; Learning rate = 0.000414 ; Loss = 1.521457\n",
      "2024-12-16 00:44:56.953000: I runner.py:310] Step = 45600 ; steps/s = 1.64, tokens/s = 81970 (39478 source, 42492 target) ; Learning rate = 0.000414 ; Loss = 1.516632\n",
      "2024-12-16 00:45:58.572000: I runner.py:310] Step = 45700 ; steps/s = 1.62, tokens/s = 82844 (39913 source, 42931 target) ; Learning rate = 0.000413 ; Loss = 1.515471\n",
      "2024-12-16 00:47:00.221000: I runner.py:310] Step = 45800 ; steps/s = 1.62, tokens/s = 82769 (39898 source, 42871 target) ; Learning rate = 0.000413 ; Loss = 1.510275\n",
      "2024-12-16 00:48:01.785000: I runner.py:310] Step = 45900 ; steps/s = 1.62, tokens/s = 82926 (39946 source, 42980 target) ; Learning rate = 0.000413 ; Loss = 1.519266\n",
      "2024-12-16 00:49:02.949000: I runner.py:310] Step = 46000 ; steps/s = 1.64, tokens/s = 81857 (39435 source, 42422 target) ; Learning rate = 0.000412 ; Loss = 1.514929\n",
      "2024-12-16 00:50:04.556000: I runner.py:310] Step = 46100 ; steps/s = 1.62, tokens/s = 82856 (39899 source, 42957 target) ; Learning rate = 0.000412 ; Loss = 1.521126\n",
      "2024-12-16 00:51:06.131000: I runner.py:310] Step = 46200 ; steps/s = 1.62, tokens/s = 82877 (39924 source, 42953 target) ; Learning rate = 0.000411 ; Loss = 1.514915\n",
      "2024-12-16 00:52:07.738000: I runner.py:310] Step = 46300 ; steps/s = 1.62, tokens/s = 82853 (39932 source, 42921 target) ; Learning rate = 0.000411 ; Loss = 1.523993\n",
      "2024-12-16 00:53:08.877000: I runner.py:310] Step = 46400 ; steps/s = 1.64, tokens/s = 81912 (39464 source, 42448 target) ; Learning rate = 0.000410 ; Loss = 1.513680\n",
      "2024-12-16 00:54:10.497000: I runner.py:310] Step = 46500 ; steps/s = 1.62, tokens/s = 82807 (39914 source, 42893 target) ; Learning rate = 0.000410 ; Loss = 1.520461\n",
      "2024-12-16 00:55:12.077000: I runner.py:310] Step = 46600 ; steps/s = 1.62, tokens/s = 82895 (39938 source, 42957 target) ; Learning rate = 0.000409 ; Loss = 1.517139\n",
      "2024-12-16 00:56:13.671000: I runner.py:310] Step = 46700 ; steps/s = 1.62, tokens/s = 82857 (39920 source, 42937 target) ; Learning rate = 0.000409 ; Loss = 1.516754\n",
      "2024-12-16 00:57:14.848000: I runner.py:310] Step = 46800 ; steps/s = 1.63, tokens/s = 81869 (39438 source, 42431 target) ; Learning rate = 0.000409 ; Loss = 1.510378\n",
      "2024-12-16 00:58:16.497000: I runner.py:310] Step = 46900 ; steps/s = 1.62, tokens/s = 82817 (39883 source, 42934 target) ; Learning rate = 0.000408 ; Loss = 1.522094\n",
      "2024-12-16 00:59:18.076000: I runner.py:310] Step = 47000 ; steps/s = 1.62, tokens/s = 82890 (39959 source, 42931 target) ; Learning rate = 0.000408 ; Loss = 1.524390\n",
      "2024-12-16 01:00:19.698000: I runner.py:310] Step = 47100 ; steps/s = 1.62, tokens/s = 82800 (39888 source, 42912 target) ; Learning rate = 0.000407 ; Loss = 1.524440\n",
      "2024-12-16 01:01:20.784000: I runner.py:310] Step = 47200 ; steps/s = 1.64, tokens/s = 81980 (39526 source, 42454 target) ; Learning rate = 0.000407 ; Loss = 1.535126\n",
      "2024-12-16 01:02:22.353000: I runner.py:310] Step = 47300 ; steps/s = 1.62, tokens/s = 82907 (39932 source, 42975 target) ; Learning rate = 0.000406 ; Loss = 1.519316\n",
      "2024-12-16 01:03:23.905000: I runner.py:310] Step = 47400 ; steps/s = 1.62, tokens/s = 82932 (39953 source, 42979 target) ; Learning rate = 0.000406 ; Loss = 1.525147\n",
      "2024-12-16 01:04:25.441000: I runner.py:310] Step = 47500 ; steps/s = 1.63, tokens/s = 82922 (39938 source, 42984 target) ; Learning rate = 0.000406 ; Loss = 1.519212\n",
      "2024-12-16 01:05:26.549000: I runner.py:310] Step = 47600 ; steps/s = 1.64, tokens/s = 81949 (39489 source, 42460 target) ; Learning rate = 0.000405 ; Loss = 1.510090\n",
      "2024-12-16 01:06:28.193000: I runner.py:310] Step = 47700 ; steps/s = 1.62, tokens/s = 82807 (39879 source, 42928 target) ; Learning rate = 0.000405 ; Loss = 1.515115\n",
      "2024-12-16 01:07:29.747000: I runner.py:310] Step = 47800 ; steps/s = 1.62, tokens/s = 82905 (39947 source, 42958 target) ; Learning rate = 0.000404 ; Loss = 1.517784\n",
      "2024-12-16 01:08:31.366000: I runner.py:310] Step = 47900 ; steps/s = 1.62, tokens/s = 82818 (39910 source, 42908 target) ; Learning rate = 0.000404 ; Loss = 1.523549\n",
      "2024-12-16 01:09:32.517000: I runner.py:310] Step = 48000 ; steps/s = 1.64, tokens/s = 81886 (39465 source, 42421 target) ; Learning rate = 0.000403 ; Loss = 1.517817\n",
      "2024-12-16 01:10:34.094000: I runner.py:310] Step = 48100 ; steps/s = 1.62, tokens/s = 82865 (39919 source, 42946 target) ; Learning rate = 0.000403 ; Loss = 1.520840\n",
      "2024-12-16 01:11:35.647000: I runner.py:310] Step = 48200 ; steps/s = 1.62, tokens/s = 82927 (39976 source, 42951 target) ; Learning rate = 0.000403 ; Loss = 1.517302\n",
      "2024-12-16 01:12:37.168000: I runner.py:310] Step = 48300 ; steps/s = 1.63, tokens/s = 82980 (39963 source, 43017 target) ; Learning rate = 0.000402 ; Loss = 1.525018\n",
      "2024-12-16 01:13:38.260000: I runner.py:310] Step = 48400 ; steps/s = 1.64, tokens/s = 81988 (39486 source, 42502 target) ; Learning rate = 0.000402 ; Loss = 1.503447\n",
      "2024-12-16 01:14:39.893000: I runner.py:310] Step = 48500 ; steps/s = 1.62, tokens/s = 82806 (39880 source, 42926 target) ; Learning rate = 0.000401 ; Loss = 1.519624\n",
      "2024-12-16 01:15:41.487000: I runner.py:310] Step = 48600 ; steps/s = 1.62, tokens/s = 82849 (39930 source, 42919 target) ; Learning rate = 0.000401 ; Loss = 1.516441\n",
      "2024-12-16 01:16:43.060000: I runner.py:310] Step = 48700 ; steps/s = 1.62, tokens/s = 82894 (39959 source, 42935 target) ; Learning rate = 0.000401 ; Loss = 1.515638\n",
      "2024-12-16 01:17:44.181000: I runner.py:310] Step = 48800 ; steps/s = 1.64, tokens/s = 81950 (39450 source, 42500 target) ; Learning rate = 0.000400 ; Loss = 1.514064\n",
      "2024-12-16 01:18:45.751000: I runner.py:310] Step = 48900 ; steps/s = 1.62, tokens/s = 82920 (39969 source, 42951 target) ; Learning rate = 0.000400 ; Loss = 1.520327\n",
      "2024-12-16 01:19:47.359000: I runner.py:310] Step = 49000 ; steps/s = 1.62, tokens/s = 82837 (39901 source, 42936 target) ; Learning rate = 0.000399 ; Loss = 1.513035\n",
      "2024-12-16 01:20:48.891000: I runner.py:310] Step = 49100 ; steps/s = 1.63, tokens/s = 82935 (39955 source, 42980 target) ; Learning rate = 0.000399 ; Loss = 1.513957\n",
      "2024-12-16 01:21:50.054000: I runner.py:310] Step = 49200 ; steps/s = 1.64, tokens/s = 81881 (39472 source, 42409 target) ; Learning rate = 0.000398 ; Loss = 1.510082\n",
      "2024-12-16 01:22:51.661000: I runner.py:310] Step = 49300 ; steps/s = 1.62, tokens/s = 82836 (39912 source, 42924 target) ; Learning rate = 0.000398 ; Loss = 1.511804\n",
      "2024-12-16 01:23:53.247000: I runner.py:310] Step = 49400 ; steps/s = 1.62, tokens/s = 82893 (39942 source, 42951 target) ; Learning rate = 0.000398 ; Loss = 1.512551\n",
      "2024-12-16 01:24:54.823000: I runner.py:310] Step = 49500 ; steps/s = 1.62, tokens/s = 82875 (39923 source, 42952 target) ; Learning rate = 0.000397 ; Loss = 1.521399\n",
      "2024-12-16 01:25:55.906000: I runner.py:310] Step = 49600 ; steps/s = 1.64, tokens/s = 81973 (39490 source, 42483 target) ; Learning rate = 0.000397 ; Loss = 1.507740\n",
      "2024-12-16 01:26:57.506000: I runner.py:310] Step = 49700 ; steps/s = 1.62, tokens/s = 82854 (39897 source, 42957 target) ; Learning rate = 0.000396 ; Loss = 1.511207\n",
      "2024-12-16 01:27:59.125000: I runner.py:310] Step = 49800 ; steps/s = 1.62, tokens/s = 82856 (39919 source, 42937 target) ; Learning rate = 0.000396 ; Loss = 1.515346\n",
      "2024-12-16 01:29:00.726000: I runner.py:310] Step = 49900 ; steps/s = 1.62, tokens/s = 82829 (39941 source, 42888 target) ; Learning rate = 0.000396 ; Loss = 1.518494\n",
      "2024-12-16 01:30:01.870000: I runner.py:310] Step = 50000 ; steps/s = 1.64, tokens/s = 81939 (39446 source, 42493 target) ; Learning rate = 0.000395 ; Loss = 1.521714\n",
      "2024-12-16 01:30:03.460000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-50000\n",
      "2024-12-16 01:30:03.460000: I training.py:192] Running evaluation for step 50000\n",
      "2024-12-16 01:34:23.178000: I training.py:192] Evaluation result for step 50000: loss = 1.149342 ; perplexity = 3.156116\n",
      "2024-12-16 01:35:24.710000: I runner.py:310] Step = 50100 ; steps/s = 1.63, tokens/s = 82973 (39981 source, 42992 target) ; Learning rate = 0.000395 ; Loss = 1.511661\n",
      "2024-12-16 01:36:26.293000: I runner.py:310] Step = 50200 ; steps/s = 1.62, tokens/s = 82893 (39935 source, 42958 target) ; Learning rate = 0.000394 ; Loss = 1.514191\n",
      "2024-12-16 01:37:27.901000: I runner.py:310] Step = 50300 ; steps/s = 1.62, tokens/s = 82794 (39903 source, 42891 target) ; Learning rate = 0.000394 ; Loss = 1.519326\n",
      "2024-12-16 01:38:29.028000: I runner.py:310] Step = 50400 ; steps/s = 1.64, tokens/s = 81907 (39471 source, 42436 target) ; Learning rate = 0.000394 ; Loss = 1.501255\n",
      "2024-12-16 01:39:30.609000: I runner.py:310] Step = 50500 ; steps/s = 1.62, tokens/s = 82901 (39920 source, 42981 target) ; Learning rate = 0.000393 ; Loss = 1.512486\n",
      "2024-12-16 01:40:32.210000: I runner.py:310] Step = 50600 ; steps/s = 1.62, tokens/s = 82824 (39905 source, 42919 target) ; Learning rate = 0.000393 ; Loss = 1.519594\n",
      "2024-12-16 01:41:33.831000: I runner.py:310] Step = 50700 ; steps/s = 1.62, tokens/s = 82836 (39934 source, 42902 target) ; Learning rate = 0.000393 ; Loss = 1.522067\n",
      "2024-12-16 01:42:34.887000: I runner.py:310] Step = 50800 ; steps/s = 1.64, tokens/s = 82056 (39512 source, 42544 target) ; Learning rate = 0.000392 ; Loss = 1.507533\n",
      "2024-12-16 01:43:36.432000: I runner.py:310] Step = 50900 ; steps/s = 1.63, tokens/s = 82906 (39932 source, 42974 target) ; Learning rate = 0.000392 ; Loss = 1.514270\n",
      "2024-12-16 01:44:38.031000: I runner.py:310] Step = 51000 ; steps/s = 1.62, tokens/s = 82868 (39914 source, 42954 target) ; Learning rate = 0.000391 ; Loss = 1.511518\n",
      "2024-12-16 01:45:39.512000: I runner.py:310] Step = 51100 ; steps/s = 1.63, tokens/s = 83000 (40033 source, 42967 target) ; Learning rate = 0.000391 ; Loss = 1.516652\n",
      "2024-12-16 01:46:40.634000: I runner.py:310] Step = 51200 ; steps/s = 1.64, tokens/s = 81961 (39481 source, 42480 target) ; Learning rate = 0.000391 ; Loss = 1.511780\n",
      "2024-12-16 01:47:42.200000: I runner.py:310] Step = 51300 ; steps/s = 1.62, tokens/s = 82906 (39940 source, 42966 target) ; Learning rate = 0.000390 ; Loss = 1.502209\n",
      "2024-12-16 01:48:43.750000: I runner.py:310] Step = 51400 ; steps/s = 1.62, tokens/s = 82909 (39956 source, 42953 target) ; Learning rate = 0.000390 ; Loss = 1.514503\n",
      "2024-12-16 01:49:45.344000: I runner.py:310] Step = 51500 ; steps/s = 1.62, tokens/s = 82857 (39921 source, 42936 target) ; Learning rate = 0.000389 ; Loss = 1.513141\n",
      "2024-12-16 01:50:46.482000: I runner.py:310] Step = 51600 ; steps/s = 1.64, tokens/s = 81943 (39484 source, 42459 target) ; Learning rate = 0.000389 ; Loss = 1.503323\n",
      "2024-12-16 01:51:48.042000: I runner.py:310] Step = 51700 ; steps/s = 1.62, tokens/s = 82863 (39920 source, 42943 target) ; Learning rate = 0.000389 ; Loss = 1.505972\n",
      "2024-12-16 01:52:49.612000: I runner.py:310] Step = 51800 ; steps/s = 1.62, tokens/s = 82895 (39944 source, 42951 target) ; Learning rate = 0.000388 ; Loss = 1.517956\n",
      "2024-12-16 01:53:51.223000: I runner.py:310] Step = 51900 ; steps/s = 1.62, tokens/s = 82855 (39910 source, 42945 target) ; Learning rate = 0.000388 ; Loss = 1.519859\n",
      "2024-12-16 01:54:52.395000: I runner.py:310] Step = 52000 ; steps/s = 1.63, tokens/s = 81916 (39438 source, 42478 target) ; Learning rate = 0.000388 ; Loss = 1.510610\n",
      "2024-12-16 01:55:53.998000: I runner.py:310] Step = 52100 ; steps/s = 1.62, tokens/s = 82840 (39943 source, 42897 target) ; Learning rate = 0.000387 ; Loss = 1.511935\n",
      "2024-12-16 01:56:55.596000: I runner.py:310] Step = 52200 ; steps/s = 1.62, tokens/s = 82832 (39908 source, 42924 target) ; Learning rate = 0.000387 ; Loss = 1.515557\n",
      "2024-12-16 01:57:56.759000: I runner.py:310] Step = 52300 ; steps/s = 1.64, tokens/s = 82012 (39528 source, 42484 target) ; Learning rate = 0.000386 ; Loss = 1.515059\n",
      "2024-12-16 01:58:58.302000: I runner.py:310] Step = 52400 ; steps/s = 1.63, tokens/s = 82763 (39835 source, 42928 target) ; Learning rate = 0.000386 ; Loss = 1.504854\n",
      "2024-12-16 01:59:59.849000: I runner.py:310] Step = 52500 ; steps/s = 1.62, tokens/s = 82964 (40013 source, 42951 target) ; Learning rate = 0.000386 ; Loss = 1.511461\n",
      "2024-12-16 02:01:01.372000: I runner.py:310] Step = 52600 ; steps/s = 1.63, tokens/s = 82943 (39951 source, 42992 target) ; Learning rate = 0.000385 ; Loss = 1.515527\n",
      "2024-12-16 02:02:02.563000: I runner.py:310] Step = 52700 ; steps/s = 1.63, tokens/s = 81844 (39441 source, 42403 target) ; Learning rate = 0.000385 ; Loss = 1.510491\n",
      "2024-12-16 02:03:04.114000: I runner.py:310] Step = 52800 ; steps/s = 1.62, tokens/s = 82923 (39959 source, 42964 target) ; Learning rate = 0.000385 ; Loss = 1.503496\n",
      "2024-12-16 02:04:05.679000: I runner.py:310] Step = 52900 ; steps/s = 1.62, tokens/s = 82899 (39940 source, 42959 target) ; Learning rate = 0.000384 ; Loss = 1.508621\n",
      "2024-12-16 02:05:07.216000: I runner.py:310] Step = 53000 ; steps/s = 1.63, tokens/s = 82946 (39960 source, 42986 target) ; Learning rate = 0.000384 ; Loss = 1.514654\n",
      "2024-12-16 02:06:08.284000: I runner.py:310] Step = 53100 ; steps/s = 1.64, tokens/s = 82010 (39501 source, 42509 target) ; Learning rate = 0.000384 ; Loss = 1.510367\n",
      "2024-12-16 02:07:09.884000: I runner.py:310] Step = 53200 ; steps/s = 1.62, tokens/s = 82879 (39902 source, 42977 target) ; Learning rate = 0.000383 ; Loss = 1.510851\n",
      "2024-12-16 02:08:11.513000: I runner.py:310] Step = 53300 ; steps/s = 1.62, tokens/s = 82821 (39912 source, 42909 target) ; Learning rate = 0.000383 ; Loss = 1.500225\n",
      "2024-12-16 02:09:13.121000: I runner.py:310] Step = 53400 ; steps/s = 1.62, tokens/s = 82841 (39927 source, 42914 target) ; Learning rate = 0.000382 ; Loss = 1.520039\n",
      "2024-12-16 02:10:14.201000: I runner.py:310] Step = 53500 ; steps/s = 1.64, tokens/s = 81970 (39501 source, 42469 target) ; Learning rate = 0.000382 ; Loss = 1.506023\n",
      "2024-12-16 02:11:15.785000: I runner.py:310] Step = 53600 ; steps/s = 1.62, tokens/s = 82868 (39913 source, 42955 target) ; Learning rate = 0.000382 ; Loss = 1.506925\n",
      "2024-12-16 02:12:17.351000: I runner.py:310] Step = 53700 ; steps/s = 1.62, tokens/s = 82869 (39960 source, 42909 target) ; Learning rate = 0.000381 ; Loss = 1.507592\n",
      "2024-12-16 02:13:18.960000: I runner.py:310] Step = 53800 ; steps/s = 1.62, tokens/s = 82890 (39916 source, 42974 target) ; Learning rate = 0.000381 ; Loss = 1.509281\n",
      "2024-12-16 02:14:20.103000: I runner.py:310] Step = 53900 ; steps/s = 1.64, tokens/s = 81881 (39448 source, 42433 target) ; Learning rate = 0.000381 ; Loss = 1.507572\n",
      "2024-12-16 02:15:21.672000: I runner.py:310] Step = 54000 ; steps/s = 1.62, tokens/s = 82888 (39944 source, 42944 target) ; Learning rate = 0.000380 ; Loss = 1.500155\n",
      "2024-12-16 02:16:23.211000: I runner.py:310] Step = 54100 ; steps/s = 1.63, tokens/s = 82922 (39982 source, 42940 target) ; Learning rate = 0.000380 ; Loss = 1.510782\n",
      "2024-12-16 02:17:24.824000: I runner.py:310] Step = 54200 ; steps/s = 1.62, tokens/s = 82840 (39886 source, 42954 target) ; Learning rate = 0.000380 ; Loss = 1.512356\n",
      "2024-12-16 02:18:26.001000: I runner.py:310] Step = 54300 ; steps/s = 1.63, tokens/s = 81906 (39449 source, 42457 target) ; Learning rate = 0.000379 ; Loss = 1.508256\n",
      "2024-12-16 02:19:27.579000: I runner.py:310] Step = 54400 ; steps/s = 1.62, tokens/s = 82898 (39940 source, 42958 target) ; Learning rate = 0.000379 ; Loss = 1.504787\n",
      "2024-12-16 02:20:29.191000: I runner.py:310] Step = 54500 ; steps/s = 1.62, tokens/s = 82832 (39912 source, 42920 target) ; Learning rate = 0.000379 ; Loss = 1.509758\n",
      "2024-12-16 02:21:30.719000: I runner.py:310] Step = 54600 ; steps/s = 1.63, tokens/s = 82932 (39957 source, 42975 target) ; Learning rate = 0.000378 ; Loss = 1.513575\n",
      "2024-12-16 02:22:31.866000: I runner.py:310] Step = 54700 ; steps/s = 1.64, tokens/s = 81899 (39462 source, 42437 target) ; Learning rate = 0.000378 ; Loss = 1.503975\n",
      "2024-12-16 02:23:33.413000: I runner.py:310] Step = 54800 ; steps/s = 1.62, tokens/s = 82930 (39938 source, 42992 target) ; Learning rate = 0.000378 ; Loss = 1.505023\n",
      "2024-12-16 02:24:34.986000: I runner.py:310] Step = 54900 ; steps/s = 1.62, tokens/s = 82878 (39910 source, 42968 target) ; Learning rate = 0.000377 ; Loss = 1.509752\n",
      "2024-12-16 02:25:36.568000: I runner.py:310] Step = 55000 ; steps/s = 1.62, tokens/s = 82870 (39947 source, 42923 target) ; Learning rate = 0.000377 ; Loss = 1.515802\n",
      "2024-12-16 02:25:36.570000: I training.py:192] Running evaluation for step 55000\n",
      "2024-12-16 02:29:52.868000: I training.py:192] Evaluation result for step 55000: loss = 1.163002 ; perplexity = 3.199523\n",
      "2024-12-16 02:30:53.840000: I runner.py:310] Step = 55100 ; steps/s = 1.64, tokens/s = 82191 (39604 source, 42587 target) ; Learning rate = 0.000377 ; Loss = 1.508067\n",
      "2024-12-16 02:31:55.457000: I runner.py:310] Step = 55200 ; steps/s = 1.62, tokens/s = 82820 (39884 source, 42936 target) ; Learning rate = 0.000376 ; Loss = 1.508820\n",
      "2024-12-16 02:32:57.101000: I runner.py:310] Step = 55300 ; steps/s = 1.62, tokens/s = 82780 (39908 source, 42872 target) ; Learning rate = 0.000376 ; Loss = 1.504539\n",
      "2024-12-16 02:33:58.701000: I runner.py:310] Step = 55400 ; steps/s = 1.62, tokens/s = 82859 (39935 source, 42924 target) ; Learning rate = 0.000376 ; Loss = 1.513822\n",
      "2024-12-16 02:34:59.822000: I runner.py:310] Step = 55500 ; steps/s = 1.64, tokens/s = 81957 (39476 source, 42481 target) ; Learning rate = 0.000375 ; Loss = 1.500077\n",
      "2024-12-16 02:36:01.440000: I runner.py:310] Step = 55600 ; steps/s = 1.62, tokens/s = 82843 (39899 source, 42944 target) ; Learning rate = 0.000375 ; Loss = 1.505480\n",
      "2024-12-16 02:37:03.040000: I runner.py:310] Step = 55700 ; steps/s = 1.62, tokens/s = 82835 (39918 source, 42917 target) ; Learning rate = 0.000375 ; Loss = 1.503214\n",
      "2024-12-16 02:38:04.621000: I runner.py:310] Step = 55800 ; steps/s = 1.62, tokens/s = 82891 (39934 source, 42957 target) ; Learning rate = 0.000374 ; Loss = 1.510183\n",
      "2024-12-16 02:39:05.777000: I runner.py:310] Step = 55900 ; steps/s = 1.64, tokens/s = 81887 (39465 source, 42422 target) ; Learning rate = 0.000374 ; Loss = 1.499142\n",
      "2024-12-16 02:40:07.373000: I runner.py:310] Step = 56000 ; steps/s = 1.62, tokens/s = 82842 (39928 source, 42914 target) ; Learning rate = 0.000374 ; Loss = 1.505793\n",
      "2024-12-16 02:41:08.934000: I runner.py:310] Step = 56100 ; steps/s = 1.62, tokens/s = 82891 (39916 source, 42975 target) ; Learning rate = 0.000373 ; Loss = 1.503199\n",
      "2024-12-16 02:42:10.556000: I runner.py:310] Step = 56200 ; steps/s = 1.62, tokens/s = 82844 (39925 source, 42919 target) ; Learning rate = 0.000373 ; Loss = 1.518431\n",
      "2024-12-16 02:43:11.682000: I runner.py:310] Step = 56300 ; steps/s = 1.64, tokens/s = 81955 (39464 source, 42491 target) ; Learning rate = 0.000373 ; Loss = 1.509161\n",
      "2024-12-16 02:44:13.354000: I runner.py:310] Step = 56400 ; steps/s = 1.62, tokens/s = 82777 (39882 source, 42895 target) ; Learning rate = 0.000372 ; Loss = 1.506091\n",
      "2024-12-16 02:45:14.901000: I runner.py:310] Step = 56500 ; steps/s = 1.62, tokens/s = 82900 (39954 source, 42946 target) ; Learning rate = 0.000372 ; Loss = 1.507345\n",
      "2024-12-16 02:46:16.495000: I runner.py:310] Step = 56600 ; steps/s = 1.62, tokens/s = 82890 (39948 source, 42942 target) ; Learning rate = 0.000372 ; Loss = 1.505262\n",
      "2024-12-16 02:47:17.606000: I runner.py:310] Step = 56700 ; steps/s = 1.64, tokens/s = 81918 (39454 source, 42464 target) ; Learning rate = 0.000371 ; Loss = 1.517764\n",
      "2024-12-16 02:48:19.182000: I runner.py:310] Step = 56800 ; steps/s = 1.62, tokens/s = 82883 (39908 source, 42975 target) ; Learning rate = 0.000371 ; Loss = 1.503753\n",
      "2024-12-16 02:49:20.827000: I runner.py:310] Step = 56900 ; steps/s = 1.62, tokens/s = 82799 (39908 source, 42891 target) ; Learning rate = 0.000371 ; Loss = 1.508677\n",
      "2024-12-16 02:50:22.352000: I runner.py:310] Step = 57000 ; steps/s = 1.63, tokens/s = 82935 (39975 source, 42960 target) ; Learning rate = 0.000370 ; Loss = 1.505919\n",
      "2024-12-16 02:51:23.477000: I runner.py:310] Step = 57100 ; steps/s = 1.64, tokens/s = 81971 (39487 source, 42484 target) ; Learning rate = 0.000370 ; Loss = 1.504874\n",
      "2024-12-16 02:52:25.051000: I runner.py:310] Step = 57200 ; steps/s = 1.62, tokens/s = 82894 (39942 source, 42952 target) ; Learning rate = 0.000370 ; Loss = 1.503030\n",
      "2024-12-16 02:53:26.623000: I runner.py:310] Step = 57300 ; steps/s = 1.62, tokens/s = 82874 (39935 source, 42939 target) ; Learning rate = 0.000369 ; Loss = 1.504572\n",
      "2024-12-16 02:54:28.288000: I runner.py:310] Step = 57400 ; steps/s = 1.62, tokens/s = 82749 (39879 source, 42870 target) ; Learning rate = 0.000369 ; Loss = 1.499445\n",
      "2024-12-16 02:55:29.466000: I runner.py:310] Step = 57500 ; steps/s = 1.63, tokens/s = 81862 (39437 source, 42425 target) ; Learning rate = 0.000369 ; Loss = 1.516482\n",
      "2024-12-16 02:56:31.064000: I runner.py:310] Step = 57600 ; steps/s = 1.62, tokens/s = 82872 (39942 source, 42930 target) ; Learning rate = 0.000368 ; Loss = 1.501011\n",
      "2024-12-16 02:57:32.626000: I runner.py:310] Step = 57700 ; steps/s = 1.62, tokens/s = 82906 (39917 source, 42989 target) ; Learning rate = 0.000368 ; Loss = 1.496558\n",
      "2024-12-16 02:58:34.241000: I runner.py:310] Step = 57800 ; steps/s = 1.62, tokens/s = 82818 (39912 source, 42906 target) ; Learning rate = 0.000368 ; Loss = 1.502415\n",
      "2024-12-16 02:59:35.293000: I runner.py:310] Step = 57900 ; steps/s = 1.64, tokens/s = 82053 (39524 source, 42529 target) ; Learning rate = 0.000367 ; Loss = 1.507225\n",
      "2024-12-16 03:00:36.938000: I runner.py:310] Step = 58000 ; steps/s = 1.62, tokens/s = 82807 (39884 source, 42923 target) ; Learning rate = 0.000367 ; Loss = 1.506004\n",
      "2024-12-16 03:01:38.561000: I runner.py:310] Step = 58100 ; steps/s = 1.62, tokens/s = 82808 (39919 source, 42889 target) ; Learning rate = 0.000367 ; Loss = 1.506000\n",
      "2024-12-16 03:02:40.150000: I runner.py:310] Step = 58200 ; steps/s = 1.62, tokens/s = 82869 (39925 source, 42944 target) ; Learning rate = 0.000366 ; Loss = 1.508269\n",
      "2024-12-16 03:03:41.261000: I runner.py:310] Step = 58300 ; steps/s = 1.64, tokens/s = 81957 (39472 source, 42485 target) ; Learning rate = 0.000366 ; Loss = 1.500296\n",
      "2024-12-16 03:04:42.865000: I runner.py:310] Step = 58400 ; steps/s = 1.62, tokens/s = 82838 (39912 source, 42926 target) ; Learning rate = 0.000366 ; Loss = 1.502279\n",
      "2024-12-16 03:05:44.437000: I runner.py:310] Step = 58500 ; steps/s = 1.62, tokens/s = 82899 (39945 source, 42954 target) ; Learning rate = 0.000365 ; Loss = 1.507473\n",
      "2024-12-16 03:06:46.033000: I runner.py:310] Step = 58600 ; steps/s = 1.62, tokens/s = 82851 (39932 source, 42919 target) ; Learning rate = 0.000365 ; Loss = 1.514414\n",
      "2024-12-16 03:07:47.206000: I runner.py:310] Step = 58700 ; steps/s = 1.63, tokens/s = 81857 (39429 source, 42428 target) ; Learning rate = 0.000365 ; Loss = 1.500777\n",
      "2024-12-16 03:08:48.781000: I runner.py:310] Step = 58800 ; steps/s = 1.62, tokens/s = 82893 (39960 source, 42933 target) ; Learning rate = 0.000365 ; Loss = 1.492542\n",
      "2024-12-16 03:09:50.378000: I runner.py:310] Step = 58900 ; steps/s = 1.62, tokens/s = 82849 (39906 source, 42943 target) ; Learning rate = 0.000364 ; Loss = 1.499003\n",
      "2024-12-16 03:10:51.978000: I runner.py:310] Step = 59000 ; steps/s = 1.62, tokens/s = 82870 (39910 source, 42960 target) ; Learning rate = 0.000364 ; Loss = 1.506851\n",
      "2024-12-16 03:11:53.100000: I runner.py:310] Step = 59100 ; steps/s = 1.64, tokens/s = 81920 (39464 source, 42456 target) ; Learning rate = 0.000364 ; Loss = 1.503668\n",
      "2024-12-16 03:12:54.648000: I runner.py:310] Step = 59200 ; steps/s = 1.62, tokens/s = 82930 (39953 source, 42977 target) ; Learning rate = 0.000363 ; Loss = 1.500158\n",
      "2024-12-16 03:13:56.205000: I runner.py:310] Step = 59300 ; steps/s = 1.62, tokens/s = 82936 (39981 source, 42955 target) ; Learning rate = 0.000363 ; Loss = 1.499025\n",
      "2024-12-16 03:14:57.783000: I runner.py:310] Step = 59400 ; steps/s = 1.62, tokens/s = 82884 (39926 source, 42958 target) ; Learning rate = 0.000363 ; Loss = 1.498406\n",
      "2024-12-16 03:15:58.905000: I runner.py:310] Step = 59500 ; steps/s = 1.64, tokens/s = 81912 (39438 source, 42474 target) ; Learning rate = 0.000362 ; Loss = 1.496061\n",
      "2024-12-16 03:17:00.441000: I runner.py:310] Step = 59600 ; steps/s = 1.63, tokens/s = 82943 (39948 source, 42995 target) ; Learning rate = 0.000362 ; Loss = 1.503230\n",
      "2024-12-16 03:18:01.978000: I runner.py:310] Step = 59700 ; steps/s = 1.63, tokens/s = 82957 (39974 source, 42983 target) ; Learning rate = 0.000362 ; Loss = 1.498861\n",
      "2024-12-16 03:19:03.604000: I runner.py:310] Step = 59800 ; steps/s = 1.62, tokens/s = 82817 (39910 source, 42907 target) ; Learning rate = 0.000361 ; Loss = 1.505701\n",
      "2024-12-16 03:20:04.807000: I runner.py:310] Step = 59900 ; steps/s = 1.63, tokens/s = 81796 (39446 source, 42350 target) ; Learning rate = 0.000361 ; Loss = 1.504811\n",
      "2024-12-16 03:21:06.362000: I runner.py:310] Step = 60000 ; steps/s = 1.62, tokens/s = 82959 (39936 source, 43023 target) ; Learning rate = 0.000361 ; Loss = 1.499137\n",
      "2024-12-16 03:21:07.926000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-60000\n",
      "2024-12-16 03:21:07.926000: I training.py:192] Running evaluation for step 60000\n",
      "2024-12-16 03:25:23.607000: I training.py:192] Evaluation result for step 60000: loss = 1.172363 ; perplexity = 3.229614\n",
      "2024-12-16 03:26:25.124000: I runner.py:310] Step = 60100 ; steps/s = 1.63, tokens/s = 83001 (39993 source, 43008 target) ; Learning rate = 0.000361 ; Loss = 1.505079\n",
      "2024-12-16 03:27:26.702000: I runner.py:310] Step = 60200 ; steps/s = 1.62, tokens/s = 82888 (39944 source, 42944 target) ; Learning rate = 0.000360 ; Loss = 1.504358\n",
      "2024-12-16 03:28:27.880000: I runner.py:310] Step = 60300 ; steps/s = 1.63, tokens/s = 81860 (39421 source, 42439 target) ; Learning rate = 0.000360 ; Loss = 1.504338\n",
      "2024-12-16 03:29:29.488000: I runner.py:310] Step = 60400 ; steps/s = 1.62, tokens/s = 82832 (39916 source, 42916 target) ; Learning rate = 0.000360 ; Loss = 1.499129\n",
      "2024-12-16 03:30:31.085000: I runner.py:310] Step = 60500 ; steps/s = 1.62, tokens/s = 82839 (39897 source, 42942 target) ; Learning rate = 0.000359 ; Loss = 1.501289\n",
      "2024-12-16 03:31:32.746000: I runner.py:310] Step = 60600 ; steps/s = 1.62, tokens/s = 82806 (39914 source, 42892 target) ; Learning rate = 0.000359 ; Loss = 1.506405\n",
      "2024-12-16 03:32:33.834000: I runner.py:310] Step = 60700 ; steps/s = 1.64, tokens/s = 81955 (39492 source, 42463 target) ; Learning rate = 0.000359 ; Loss = 1.508919\n",
      "2024-12-16 03:33:35.420000: I runner.py:310] Step = 60800 ; steps/s = 1.62, tokens/s = 82882 (39937 source, 42945 target) ; Learning rate = 0.000358 ; Loss = 1.493283\n",
      "2024-12-16 03:34:37.093000: I runner.py:310] Step = 60900 ; steps/s = 1.62, tokens/s = 82764 (39890 source, 42874 target) ; Learning rate = 0.000358 ; Loss = 1.495665\n",
      "2024-12-16 03:35:38.702000: I runner.py:310] Step = 61000 ; steps/s = 1.62, tokens/s = 82837 (39907 source, 42930 target) ; Learning rate = 0.000358 ; Loss = 1.505167\n",
      "2024-12-16 03:36:39.858000: I runner.py:310] Step = 61100 ; steps/s = 1.64, tokens/s = 81873 (39458 source, 42415 target) ; Learning rate = 0.000358 ; Loss = 1.503901\n",
      "2024-12-16 03:37:41.494000: I runner.py:310] Step = 61200 ; steps/s = 1.62, tokens/s = 82816 (39886 source, 42930 target) ; Learning rate = 0.000357 ; Loss = 1.494615\n",
      "2024-12-16 03:38:43.056000: I runner.py:310] Step = 61300 ; steps/s = 1.62, tokens/s = 82882 (39949 source, 42933 target) ; Learning rate = 0.000357 ; Loss = 1.498388\n",
      "2024-12-16 03:39:44.624000: I runner.py:310] Step = 61400 ; steps/s = 1.62, tokens/s = 82923 (39943 source, 42980 target) ; Learning rate = 0.000357 ; Loss = 1.497435\n",
      "2024-12-16 03:40:45.772000: I runner.py:310] Step = 61500 ; steps/s = 1.64, tokens/s = 81916 (39430 source, 42486 target) ; Learning rate = 0.000356 ; Loss = 1.491734\n",
      "2024-12-16 03:41:47.318000: I runner.py:310] Step = 61600 ; steps/s = 1.63, tokens/s = 82914 (39935 source, 42979 target) ; Learning rate = 0.000356 ; Loss = 1.501337\n",
      "2024-12-16 03:42:48.866000: I runner.py:310] Step = 61700 ; steps/s = 1.62, tokens/s = 82937 (39989 source, 42948 target) ; Learning rate = 0.000356 ; Loss = 1.497519\n",
      "2024-12-16 03:43:50.456000: I runner.py:310] Step = 61800 ; steps/s = 1.62, tokens/s = 82864 (39937 source, 42927 target) ; Learning rate = 0.000356 ; Loss = 1.507175\n",
      "2024-12-16 03:44:51.603000: I runner.py:310] Step = 61900 ; steps/s = 1.64, tokens/s = 81896 (39444 source, 42452 target) ; Learning rate = 0.000355 ; Loss = 1.486852\n",
      "2024-12-16 03:45:53.169000: I runner.py:310] Step = 62000 ; steps/s = 1.62, tokens/s = 82897 (39932 source, 42965 target) ; Learning rate = 0.000355 ; Loss = 1.502019\n",
      "2024-12-16 03:46:54.729000: I runner.py:310] Step = 62100 ; steps/s = 1.62, tokens/s = 82902 (39949 source, 42953 target) ; Learning rate = 0.000355 ; Loss = 1.499299\n",
      "2024-12-16 03:47:56.316000: I runner.py:310] Step = 62200 ; steps/s = 1.62, tokens/s = 82880 (39944 source, 42936 target) ; Learning rate = 0.000354 ; Loss = 1.507492\n",
      "2024-12-16 03:48:57.446000: I runner.py:310] Step = 62300 ; steps/s = 1.64, tokens/s = 81964 (39481 source, 42483 target) ; Learning rate = 0.000354 ; Loss = 1.488604\n",
      "2024-12-16 03:49:59.071000: I runner.py:310] Step = 62400 ; steps/s = 1.62, tokens/s = 82823 (39919 source, 42904 target) ; Learning rate = 0.000354 ; Loss = 1.497131\n",
      "2024-12-16 03:51:00.639000: I runner.py:310] Step = 62500 ; steps/s = 1.62, tokens/s = 82900 (39912 source, 42988 target) ; Learning rate = 0.000354 ; Loss = 1.507585\n",
      "2024-12-16 03:52:01.969000: I runner.py:310] Step = 62600 ; steps/s = 1.63, tokens/s = 82295 (39662 source, 42633 target) ; Learning rate = 0.000353 ; Loss = 1.499339\n",
      "2024-12-16 03:53:03.394000: I runner.py:310] Step = 62700 ; steps/s = 1.63, tokens/s = 82408 (39724 source, 42684 target) ; Learning rate = 0.000353 ; Loss = 1.501285\n",
      "2024-12-16 03:54:05.041000: I runner.py:310] Step = 62800 ; steps/s = 1.62, tokens/s = 82805 (39895 source, 42910 target) ; Learning rate = 0.000353 ; Loss = 1.501953\n",
      "2024-12-16 03:55:06.615000: I runner.py:310] Step = 62900 ; steps/s = 1.62, tokens/s = 82889 (39949 source, 42940 target) ; Learning rate = 0.000352 ; Loss = 1.506176\n",
      "2024-12-16 03:56:07.789000: I runner.py:310] Step = 63000 ; steps/s = 1.63, tokens/s = 81860 (39424 source, 42436 target) ; Learning rate = 0.000352 ; Loss = 1.500133\n",
      "2024-12-16 03:57:09.330000: I runner.py:310] Step = 63100 ; steps/s = 1.63, tokens/s = 82963 (39966 source, 42997 target) ; Learning rate = 0.000352 ; Loss = 1.490476\n",
      "2024-12-16 03:58:10.993000: I runner.py:310] Step = 63200 ; steps/s = 1.62, tokens/s = 82748 (39859 source, 42889 target) ; Learning rate = 0.000352 ; Loss = 1.502349\n",
      "2024-12-16 03:59:12.613000: I runner.py:310] Step = 63300 ; steps/s = 1.62, tokens/s = 82842 (39933 source, 42909 target) ; Learning rate = 0.000351 ; Loss = 1.502416\n",
      "2024-12-16 04:00:13.711000: I runner.py:310] Step = 63400 ; steps/s = 1.64, tokens/s = 81956 (39473 source, 42483 target) ; Learning rate = 0.000351 ; Loss = 1.499537\n",
      "2024-12-16 04:01:15.299000: I runner.py:310] Step = 63500 ; steps/s = 1.62, tokens/s = 82863 (39915 source, 42948 target) ; Learning rate = 0.000351 ; Loss = 1.499161\n",
      "2024-12-16 04:02:16.927000: I runner.py:310] Step = 63600 ; steps/s = 1.62, tokens/s = 82819 (39902 source, 42917 target) ; Learning rate = 0.000350 ; Loss = 1.498434\n",
      "2024-12-16 04:03:18.561000: I runner.py:310] Step = 63700 ; steps/s = 1.62, tokens/s = 82818 (39907 source, 42911 target) ; Learning rate = 0.000350 ; Loss = 1.499053\n",
      "2024-12-16 04:04:19.726000: I runner.py:310] Step = 63800 ; steps/s = 1.64, tokens/s = 81869 (39447 source, 42422 target) ; Learning rate = 0.000350 ; Loss = 1.508314\n",
      "2024-12-16 04:05:21.281000: I runner.py:310] Step = 63900 ; steps/s = 1.62, tokens/s = 82930 (39960 source, 42970 target) ; Learning rate = 0.000350 ; Loss = 1.492991\n",
      "2024-12-16 04:06:22.914000: I runner.py:310] Step = 64000 ; steps/s = 1.62, tokens/s = 82815 (39878 source, 42937 target) ; Learning rate = 0.000349 ; Loss = 1.500061\n",
      "2024-12-16 04:07:24.511000: I runner.py:310] Step = 64100 ; steps/s = 1.62, tokens/s = 82863 (39926 source, 42937 target) ; Learning rate = 0.000349 ; Loss = 1.500790\n",
      "2024-12-16 04:08:25.660000: I runner.py:310] Step = 64200 ; steps/s = 1.64, tokens/s = 81889 (39472 source, 42417 target) ; Learning rate = 0.000349 ; Loss = 1.501125\n",
      "2024-12-16 04:09:27.334000: I runner.py:310] Step = 64300 ; steps/s = 1.62, tokens/s = 82765 (39877 source, 42888 target) ; Learning rate = 0.000349 ; Loss = 1.497973\n",
      "2024-12-16 04:10:28.856000: I runner.py:310] Step = 64400 ; steps/s = 1.63, tokens/s = 82932 (39917 source, 43015 target) ; Learning rate = 0.000348 ; Loss = 1.496431\n",
      "2024-12-16 04:11:30.486000: I runner.py:310] Step = 64500 ; steps/s = 1.62, tokens/s = 82854 (39941 source, 42913 target) ; Learning rate = 0.000348 ; Loss = 1.498830\n",
      "2024-12-16 04:12:31.600000: I runner.py:310] Step = 64600 ; steps/s = 1.64, tokens/s = 81937 (39484 source, 42453 target) ; Learning rate = 0.000348 ; Loss = 1.491536\n",
      "2024-12-16 04:13:33.164000: I runner.py:310] Step = 64700 ; steps/s = 1.62, tokens/s = 82899 (39951 source, 42948 target) ; Learning rate = 0.000347 ; Loss = 1.490743\n",
      "2024-12-16 04:14:34.693000: I runner.py:310] Step = 64800 ; steps/s = 1.63, tokens/s = 82995 (39983 source, 43012 target) ; Learning rate = 0.000347 ; Loss = 1.499239\n",
      "2024-12-16 04:15:36.245000: I runner.py:310] Step = 64900 ; steps/s = 1.62, tokens/s = 82904 (39937 source, 42967 target) ; Learning rate = 0.000347 ; Loss = 1.497501\n",
      "2024-12-16 04:16:37.410000: I runner.py:310] Step = 65000 ; steps/s = 1.64, tokens/s = 81865 (39449 source, 42416 target) ; Learning rate = 0.000347 ; Loss = 1.502634\n",
      "2024-12-16 04:16:37.412000: I training.py:192] Running evaluation for step 65000\n",
      "2024-12-16 04:20:47.854000: I training.py:192] Evaluation result for step 65000: loss = 1.175501 ; perplexity = 3.239767\n",
      "2024-12-16 04:21:49.319000: I runner.py:310] Step = 65100 ; steps/s = 1.63, tokens/s = 83060 (40014 source, 43046 target) ; Learning rate = 0.000346 ; Loss = 1.495057\n",
      "2024-12-16 04:22:50.875000: I runner.py:310] Step = 65200 ; steps/s = 1.62, tokens/s = 82920 (39942 source, 42978 target) ; Learning rate = 0.000346 ; Loss = 1.492887\n",
      "2024-12-16 04:23:52.495000: I runner.py:310] Step = 65300 ; steps/s = 1.62, tokens/s = 82813 (39918 source, 42895 target) ; Learning rate = 0.000346 ; Loss = 1.503219\n",
      "2024-12-16 04:24:53.701000: I runner.py:310] Step = 65400 ; steps/s = 1.63, tokens/s = 81815 (39410 source, 42405 target) ; Learning rate = 0.000346 ; Loss = 1.496123\n",
      "2024-12-16 04:25:55.330000: I runner.py:310] Step = 65500 ; steps/s = 1.62, tokens/s = 82804 (39881 source, 42923 target) ; Learning rate = 0.000345 ; Loss = 1.490965\n",
      "2024-12-16 04:26:56.971000: I runner.py:310] Step = 65600 ; steps/s = 1.62, tokens/s = 82805 (39940 source, 42865 target) ; Learning rate = 0.000345 ; Loss = 1.501362\n",
      "2024-12-16 04:27:58.633000: I runner.py:310] Step = 65700 ; steps/s = 1.62, tokens/s = 82779 (39878 source, 42901 target) ; Learning rate = 0.000345 ; Loss = 1.497931\n",
      "2024-12-16 04:28:59.745000: I runner.py:310] Step = 65800 ; steps/s = 1.64, tokens/s = 81973 (39482 source, 42491 target) ; Learning rate = 0.000345 ; Loss = 1.493433\n",
      "2024-12-16 04:30:01.331000: I runner.py:310] Step = 65900 ; steps/s = 1.62, tokens/s = 82885 (39927 source, 42958 target) ; Learning rate = 0.000344 ; Loss = 1.495626\n",
      "2024-12-16 04:31:02.960000: I runner.py:310] Step = 66000 ; steps/s = 1.62, tokens/s = 82825 (39897 source, 42928 target) ; Learning rate = 0.000344 ; Loss = 1.495424\n",
      "2024-12-16 04:32:04.567000: I runner.py:310] Step = 66100 ; steps/s = 1.62, tokens/s = 82831 (39900 source, 42931 target) ; Learning rate = 0.000344 ; Loss = 1.501543\n",
      "2024-12-16 04:33:05.722000: I runner.py:310] Step = 66200 ; steps/s = 1.64, tokens/s = 81868 (39476 source, 42392 target) ; Learning rate = 0.000344 ; Loss = 1.491021\n",
      "2024-12-16 04:34:07.295000: I runner.py:310] Step = 66300 ; steps/s = 1.62, tokens/s = 82897 (39921 source, 42976 target) ; Learning rate = 0.000343 ; Loss = 1.492802\n",
      "2024-12-16 04:35:08.936000: I runner.py:310] Step = 66400 ; steps/s = 1.62, tokens/s = 82796 (39859 source, 42937 target) ; Learning rate = 0.000343 ; Loss = 1.499461\n",
      "2024-12-16 04:36:10.587000: I runner.py:310] Step = 66500 ; steps/s = 1.62, tokens/s = 82789 (39899 source, 42890 target) ; Learning rate = 0.000343 ; Loss = 1.499667\n",
      "2024-12-16 04:37:11.731000: I runner.py:310] Step = 66600 ; steps/s = 1.64, tokens/s = 81911 (39496 source, 42415 target) ; Learning rate = 0.000342 ; Loss = 1.492598\n",
      "2024-12-16 04:38:13.310000: I runner.py:310] Step = 66700 ; steps/s = 1.62, tokens/s = 82866 (39943 source, 42923 target) ; Learning rate = 0.000342 ; Loss = 1.493280\n",
      "2024-12-16 04:39:14.872000: I runner.py:310] Step = 66800 ; steps/s = 1.62, tokens/s = 82932 (39934 source, 42998 target) ; Learning rate = 0.000342 ; Loss = 1.501745\n",
      "2024-12-16 04:40:16.474000: I runner.py:310] Step = 66900 ; steps/s = 1.62, tokens/s = 82835 (39909 source, 42926 target) ; Learning rate = 0.000342 ; Loss = 1.494481\n",
      "2024-12-16 04:41:17.679000: I runner.py:310] Step = 67000 ; steps/s = 1.63, tokens/s = 81841 (39409 source, 42432 target) ; Learning rate = 0.000341 ; Loss = 1.487919\n",
      "2024-12-16 04:42:19.332000: I runner.py:310] Step = 67100 ; steps/s = 1.62, tokens/s = 82772 (39863 source, 42909 target) ; Learning rate = 0.000341 ; Loss = 1.497253\n",
      "2024-12-16 04:43:20.872000: I runner.py:310] Step = 67200 ; steps/s = 1.63, tokens/s = 82964 (39997 source, 42967 target) ; Learning rate = 0.000341 ; Loss = 1.498002\n",
      "2024-12-16 04:44:22.545000: I runner.py:310] Step = 67300 ; steps/s = 1.62, tokens/s = 82757 (39872 source, 42885 target) ; Learning rate = 0.000341 ; Loss = 1.498012\n",
      "2024-12-16 04:45:23.684000: I runner.py:310] Step = 67400 ; steps/s = 1.64, tokens/s = 81902 (39457 source, 42445 target) ; Learning rate = 0.000340 ; Loss = 1.501067\n",
      "2024-12-16 04:46:25.261000: I runner.py:310] Step = 67500 ; steps/s = 1.62, tokens/s = 82882 (39937 source, 42945 target) ; Learning rate = 0.000340 ; Loss = 1.495754\n",
      "2024-12-16 04:47:26.867000: I runner.py:310] Step = 67600 ; steps/s = 1.62, tokens/s = 82828 (39905 source, 42923 target) ; Learning rate = 0.000340 ; Loss = 1.492296\n",
      "2024-12-16 04:48:28.520000: I runner.py:310] Step = 67700 ; steps/s = 1.62, tokens/s = 82777 (39888 source, 42889 target) ; Learning rate = 0.000340 ; Loss = 1.509766\n",
      "2024-12-16 04:49:29.637000: I runner.py:310] Step = 67800 ; steps/s = 1.64, tokens/s = 81949 (39487 source, 42462 target) ; Learning rate = 0.000339 ; Loss = 1.489606\n",
      "2024-12-16 04:50:31.259000: I runner.py:310] Step = 67900 ; steps/s = 1.62, tokens/s = 82830 (39902 source, 42928 target) ; Learning rate = 0.000339 ; Loss = 1.487975\n",
      "2024-12-16 04:51:32.854000: I runner.py:310] Step = 68000 ; steps/s = 1.62, tokens/s = 82874 (39924 source, 42950 target) ; Learning rate = 0.000339 ; Loss = 1.495325\n",
      "2024-12-16 04:52:34.452000: I runner.py:310] Step = 68100 ; steps/s = 1.62, tokens/s = 82852 (39940 source, 42912 target) ; Learning rate = 0.000339 ; Loss = 1.495328\n",
      "2024-12-16 04:53:35.552000: I runner.py:310] Step = 68200 ; steps/s = 1.64, tokens/s = 81949 (39476 source, 42473 target) ; Learning rate = 0.000338 ; Loss = 1.494552\n",
      "2024-12-16 04:54:37.177000: I runner.py:310] Step = 68300 ; steps/s = 1.62, tokens/s = 82834 (39886 source, 42948 target) ; Learning rate = 0.000338 ; Loss = 1.485852\n",
      "2024-12-16 04:55:38.768000: I runner.py:310] Step = 68400 ; steps/s = 1.62, tokens/s = 82861 (39938 source, 42923 target) ; Learning rate = 0.000338 ; Loss = 1.495607\n",
      "2024-12-16 04:56:40.408000: I runner.py:310] Step = 68500 ; steps/s = 1.62, tokens/s = 82795 (39897 source, 42898 target) ; Learning rate = 0.000338 ; Loss = 1.493006\n",
      "2024-12-16 04:57:41.498000: I runner.py:310] Step = 68600 ; steps/s = 1.64, tokens/s = 81958 (39478 source, 42480 target) ; Learning rate = 0.000337 ; Loss = 1.485437\n",
      "2024-12-16 04:58:43.097000: I runner.py:310] Step = 68700 ; steps/s = 1.62, tokens/s = 82897 (39945 source, 42952 target) ; Learning rate = 0.000337 ; Loss = 1.500118\n",
      "2024-12-16 04:59:44.725000: I runner.py:310] Step = 68800 ; steps/s = 1.62, tokens/s = 82803 (39895 source, 42908 target) ; Learning rate = 0.000337 ; Loss = 1.494451\n",
      "2024-12-16 05:00:46.337000: I runner.py:310] Step = 68900 ; steps/s = 1.62, tokens/s = 82843 (39926 source, 42917 target) ; Learning rate = 0.000337 ; Loss = 1.494788\n",
      "2024-12-16 05:01:47.531000: I runner.py:310] Step = 69000 ; steps/s = 1.63, tokens/s = 81834 (39408 source, 42426 target) ; Learning rate = 0.000336 ; Loss = 1.495817\n",
      "2024-12-16 05:02:49.128000: I runner.py:310] Step = 69100 ; steps/s = 1.62, tokens/s = 82866 (39939 source, 42927 target) ; Learning rate = 0.000336 ; Loss = 1.486150\n",
      "2024-12-16 05:03:50.730000: I runner.py:310] Step = 69200 ; steps/s = 1.62, tokens/s = 82829 (39910 source, 42919 target) ; Learning rate = 0.000336 ; Loss = 1.495138\n",
      "2024-12-16 05:04:52.341000: I runner.py:310] Step = 69300 ; steps/s = 1.62, tokens/s = 82844 (39898 source, 42946 target) ; Learning rate = 0.000336 ; Loss = 1.491732\n",
      "2024-12-16 05:05:53.485000: I runner.py:310] Step = 69400 ; steps/s = 1.64, tokens/s = 81905 (39465 source, 42440 target) ; Learning rate = 0.000336 ; Loss = 1.485307\n",
      "2024-12-16 05:06:55.068000: I runner.py:310] Step = 69500 ; steps/s = 1.62, tokens/s = 82841 (39921 source, 42920 target) ; Learning rate = 0.000335 ; Loss = 1.495106\n",
      "2024-12-16 05:07:56.702000: I runner.py:310] Step = 69600 ; steps/s = 1.62, tokens/s = 82826 (39895 source, 42931 target) ; Learning rate = 0.000335 ; Loss = 1.499353\n",
      "2024-12-16 05:08:58.313000: I runner.py:310] Step = 69700 ; steps/s = 1.62, tokens/s = 82849 (39907 source, 42942 target) ; Learning rate = 0.000335 ; Loss = 1.498182\n",
      "2024-12-16 05:09:59.385000: I runner.py:310] Step = 69800 ; steps/s = 1.64, tokens/s = 82026 (39541 source, 42485 target) ; Learning rate = 0.000335 ; Loss = 1.496936\n",
      "2024-12-16 05:11:01.057000: I runner.py:310] Step = 69900 ; steps/s = 1.62, tokens/s = 82770 (39909 source, 42861 target) ; Learning rate = 0.000334 ; Loss = 1.492959\n",
      "2024-12-16 05:12:02.707000: I runner.py:310] Step = 70000 ; steps/s = 1.62, tokens/s = 82796 (39888 source, 42908 target) ; Learning rate = 0.000334 ; Loss = 1.495883\n",
      "2024-12-16 05:12:04.400000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-70000\n",
      "2024-12-16 05:12:04.400000: I training.py:192] Running evaluation for step 70000\n",
      "2024-12-16 05:16:16.750000: I training.py:192] Evaluation result for step 70000: loss = 1.184756 ; perplexity = 3.269887\n",
      "2024-12-16 05:17:18.205000: I runner.py:310] Step = 70100 ; steps/s = 1.63, tokens/s = 83039 (39970 source, 43069 target) ; Learning rate = 0.000334 ; Loss = 1.498978\n",
      "2024-12-16 05:18:19.390000: I runner.py:310] Step = 70200 ; steps/s = 1.63, tokens/s = 81891 (39446 source, 42445 target) ; Learning rate = 0.000334 ; Loss = 1.488834\n",
      "2024-12-16 05:19:21.062000: I runner.py:310] Step = 70300 ; steps/s = 1.62, tokens/s = 82760 (39858 source, 42902 target) ; Learning rate = 0.000333 ; Loss = 1.491162\n",
      "2024-12-16 05:20:22.799000: I runner.py:310] Step = 70400 ; steps/s = 1.62, tokens/s = 82648 (39803 source, 42845 target) ; Learning rate = 0.000333 ; Loss = 1.495998\n",
      "2024-12-16 05:21:24.460000: I runner.py:310] Step = 70500 ; steps/s = 1.62, tokens/s = 82759 (39910 source, 42849 target) ; Learning rate = 0.000333 ; Loss = 1.498775\n",
      "2024-12-16 05:22:25.637000: I runner.py:310] Step = 70600 ; steps/s = 1.63, tokens/s = 81877 (39452 source, 42425 target) ; Learning rate = 0.000333 ; Loss = 1.488745\n",
      "2024-12-16 05:23:27.279000: I runner.py:310] Step = 70700 ; steps/s = 1.62, tokens/s = 82811 (39902 source, 42909 target) ; Learning rate = 0.000332 ; Loss = 1.498761\n",
      "2024-12-16 05:24:28.991000: I runner.py:310] Step = 70800 ; steps/s = 1.62, tokens/s = 82717 (39860 source, 42857 target) ; Learning rate = 0.000332 ; Loss = 1.495799\n",
      "2024-12-16 05:25:30.647000: I runner.py:310] Step = 70900 ; steps/s = 1.62, tokens/s = 82769 (39875 source, 42894 target) ; Learning rate = 0.000332 ; Loss = 1.499433\n",
      "2024-12-16 05:26:31.868000: I runner.py:310] Step = 71000 ; steps/s = 1.63, tokens/s = 81805 (39413 source, 42392 target) ; Learning rate = 0.000332 ; Loss = 1.487356\n",
      "2024-12-16 05:27:33.510000: I runner.py:310] Step = 71100 ; steps/s = 1.62, tokens/s = 82745 (39859 source, 42886 target) ; Learning rate = 0.000331 ; Loss = 1.493498\n",
      "2024-12-16 05:28:35.129000: I runner.py:310] Step = 71200 ; steps/s = 1.62, tokens/s = 82864 (39916 source, 42948 target) ; Learning rate = 0.000331 ; Loss = 1.493622\n",
      "2024-12-16 05:29:36.773000: I runner.py:310] Step = 71300 ; steps/s = 1.62, tokens/s = 82781 (39892 source, 42889 target) ; Learning rate = 0.000331 ; Loss = 1.493466\n",
      "2024-12-16 05:30:37.930000: I runner.py:310] Step = 71400 ; steps/s = 1.64, tokens/s = 81914 (39472 source, 42442 target) ; Learning rate = 0.000331 ; Loss = 1.486342\n",
      "2024-12-16 05:31:39.533000: I runner.py:310] Step = 71500 ; steps/s = 1.62, tokens/s = 82833 (39890 source, 42943 target) ; Learning rate = 0.000331 ; Loss = 1.492295\n",
      "2024-12-16 05:32:41.207000: I runner.py:310] Step = 71600 ; steps/s = 1.62, tokens/s = 82746 (39875 source, 42871 target) ; Learning rate = 0.000330 ; Loss = 1.497949\n",
      "2024-12-16 05:33:42.868000: I runner.py:310] Step = 71700 ; steps/s = 1.62, tokens/s = 82784 (39900 source, 42884 target) ; Learning rate = 0.000330 ; Loss = 1.491059\n",
      "2024-12-16 05:34:44.054000: I runner.py:310] Step = 71800 ; steps/s = 1.63, tokens/s = 81858 (39462 source, 42396 target) ; Learning rate = 0.000330 ; Loss = 1.503707\n",
      "2024-12-16 05:35:45.752000: I runner.py:310] Step = 71900 ; steps/s = 1.62, tokens/s = 82691 (39831 source, 42860 target) ; Learning rate = 0.000330 ; Loss = 1.491919\n",
      "2024-12-16 05:36:47.473000: I runner.py:310] Step = 72000 ; steps/s = 1.62, tokens/s = 82709 (39849 source, 42860 target) ; Learning rate = 0.000329 ; Loss = 1.494970\n",
      "2024-12-16 05:37:49.112000: I runner.py:310] Step = 72100 ; steps/s = 1.62, tokens/s = 82809 (39897 source, 42912 target) ; Learning rate = 0.000329 ; Loss = 1.490996\n",
      "2024-12-16 05:38:50.262000: I runner.py:310] Step = 72200 ; steps/s = 1.64, tokens/s = 81898 (39423 source, 42475 target) ; Learning rate = 0.000329 ; Loss = 1.479180\n",
      "2024-12-16 05:39:51.976000: I runner.py:310] Step = 72300 ; steps/s = 1.62, tokens/s = 82709 (39866 source, 42843 target) ; Learning rate = 0.000329 ; Loss = 1.490584\n",
      "2024-12-16 05:40:53.617000: I runner.py:310] Step = 72400 ; steps/s = 1.62, tokens/s = 82825 (39922 source, 42903 target) ; Learning rate = 0.000328 ; Loss = 1.492082\n",
      "2024-12-16 05:41:55.276000: I runner.py:310] Step = 72500 ; steps/s = 1.62, tokens/s = 82752 (39856 source, 42896 target) ; Learning rate = 0.000328 ; Loss = 1.495423\n",
      "2024-12-16 05:42:56.376000: I runner.py:310] Step = 72600 ; steps/s = 1.64, tokens/s = 81983 (39462 source, 42521 target) ; Learning rate = 0.000328 ; Loss = 1.486144\n",
      "2024-12-16 05:43:58.020000: I runner.py:310] Step = 72700 ; steps/s = 1.62, tokens/s = 82779 (39888 source, 42891 target) ; Learning rate = 0.000328 ; Loss = 1.492246\n",
      "2024-12-16 05:44:59.609000: I runner.py:310] Step = 72800 ; steps/s = 1.62, tokens/s = 82883 (39947 source, 42936 target) ; Learning rate = 0.000328 ; Loss = 1.499236\n",
      "2024-12-16 05:46:01.184000: I runner.py:310] Step = 72900 ; steps/s = 1.62, tokens/s = 82819 (39913 source, 42906 target) ; Learning rate = 0.000327 ; Loss = 1.506160\n",
      "2024-12-16 05:47:02.321000: I runner.py:310] Step = 73000 ; steps/s = 1.64, tokens/s = 81975 (39514 source, 42461 target) ; Learning rate = 0.000327 ; Loss = 1.492404\n",
      "2024-12-16 05:48:03.951000: I runner.py:310] Step = 73100 ; steps/s = 1.62, tokens/s = 82794 (39868 source, 42926 target) ; Learning rate = 0.000327 ; Loss = 1.489455\n",
      "2024-12-16 05:49:05.580000: I runner.py:310] Step = 73200 ; steps/s = 1.62, tokens/s = 82811 (39898 source, 42913 target) ; Learning rate = 0.000327 ; Loss = 1.490853\n",
      "2024-12-16 05:50:06.765000: I runner.py:310] Step = 73300 ; steps/s = 1.63, tokens/s = 81863 (39463 source, 42400 target) ; Learning rate = 0.000326 ; Loss = 1.496318\n",
      "2024-12-16 05:51:08.403000: I runner.py:310] Step = 73400 ; steps/s = 1.62, tokens/s = 82826 (39902 source, 42924 target) ; Learning rate = 0.000326 ; Loss = 1.498113\n",
      "2024-12-16 05:52:10.063000: I runner.py:310] Step = 73500 ; steps/s = 1.62, tokens/s = 82763 (39891 source, 42872 target) ; Learning rate = 0.000326 ; Loss = 1.485771\n",
      "2024-12-16 05:53:11.699000: I runner.py:310] Step = 73600 ; steps/s = 1.62, tokens/s = 82807 (39897 source, 42910 target) ; Learning rate = 0.000326 ; Loss = 1.489693\n",
      "2024-12-16 05:54:12.904000: I runner.py:310] Step = 73700 ; steps/s = 1.63, tokens/s = 81816 (39399 source, 42417 target) ; Learning rate = 0.000326 ; Loss = 1.495323\n",
      "2024-12-16 05:55:14.574000: I runner.py:310] Step = 73800 ; steps/s = 1.62, tokens/s = 82776 (39876 source, 42900 target) ; Learning rate = 0.000325 ; Loss = 1.493014\n",
      "2024-12-16 05:56:16.238000: I runner.py:310] Step = 73900 ; steps/s = 1.62, tokens/s = 82773 (39867 source, 42906 target) ; Learning rate = 0.000325 ; Loss = 1.493494\n",
      "2024-12-16 05:57:17.891000: I runner.py:310] Step = 74000 ; steps/s = 1.62, tokens/s = 82786 (39872 source, 42914 target) ; Learning rate = 0.000325 ; Loss = 1.489219\n",
      "2024-12-16 05:58:19.127000: I runner.py:310] Step = 74100 ; steps/s = 1.63, tokens/s = 81770 (39428 source, 42342 target) ; Learning rate = 0.000325 ; Loss = 1.486625\n",
      "2024-12-16 05:59:20.778000: I runner.py:310] Step = 74200 ; steps/s = 1.62, tokens/s = 82792 (39878 source, 42914 target) ; Learning rate = 0.000324 ; Loss = 1.489466\n",
      "2024-12-16 06:00:22.417000: I runner.py:310] Step = 74300 ; steps/s = 1.62, tokens/s = 82796 (39889 source, 42907 target) ; Learning rate = 0.000324 ; Loss = 1.492665\n",
      "2024-12-16 06:01:24.068000: I runner.py:310] Step = 74400 ; steps/s = 1.62, tokens/s = 82808 (39904 source, 42904 target) ; Learning rate = 0.000324 ; Loss = 1.495467\n",
      "2024-12-16 06:02:25.253000: I runner.py:310] Step = 74500 ; steps/s = 1.63, tokens/s = 81840 (39434 source, 42406 target) ; Learning rate = 0.000324 ; Loss = 1.494461\n",
      "2024-12-16 06:03:26.846000: I runner.py:310] Step = 74600 ; steps/s = 1.62, tokens/s = 82883 (39937 source, 42946 target) ; Learning rate = 0.000324 ; Loss = 1.490338\n",
      "2024-12-16 06:04:28.491000: I runner.py:310] Step = 74700 ; steps/s = 1.62, tokens/s = 82769 (39878 source, 42891 target) ; Learning rate = 0.000323 ; Loss = 1.490499\n",
      "2024-12-16 06:05:30.184000: I runner.py:310] Step = 74800 ; steps/s = 1.62, tokens/s = 82714 (39853 source, 42861 target) ; Learning rate = 0.000323 ; Loss = 1.489271\n",
      "2024-12-16 06:06:31.417000: I runner.py:310] Step = 74900 ; steps/s = 1.63, tokens/s = 81792 (39407 source, 42385 target) ; Learning rate = 0.000323 ; Loss = 1.490095\n",
      "2024-12-16 06:07:33.069000: I runner.py:310] Step = 75000 ; steps/s = 1.62, tokens/s = 82752 (39876 source, 42876 target) ; Learning rate = 0.000323 ; Loss = 1.489439\n",
      "2024-12-16 06:07:33.070000: I training.py:192] Running evaluation for step 75000\n",
      "2024-12-16 06:11:52.890000: I training.py:192] Evaluation result for step 75000: loss = 1.193802 ; perplexity = 3.299603\n",
      "2024-12-16 06:12:54.382000: I runner.py:310] Step = 75100 ; steps/s = 1.63, tokens/s = 83013 (40003 source, 43010 target) ; Learning rate = 0.000323 ; Loss = 1.494388\n",
      "2024-12-16 06:13:56.046000: I runner.py:310] Step = 75200 ; steps/s = 1.62, tokens/s = 82810 (39907 source, 42903 target) ; Learning rate = 0.000322 ; Loss = 1.492279\n",
      "2024-12-16 06:14:57.245000: I runner.py:310] Step = 75300 ; steps/s = 1.63, tokens/s = 81840 (39396 source, 42444 target) ; Learning rate = 0.000322 ; Loss = 1.491522\n",
      "2024-12-16 06:15:58.987000: I runner.py:310] Step = 75400 ; steps/s = 1.62, tokens/s = 82659 (39827 source, 42832 target) ; Learning rate = 0.000322 ; Loss = 1.486528\n",
      "2024-12-16 06:17:00.678000: I runner.py:310] Step = 75500 ; steps/s = 1.62, tokens/s = 82717 (39842 source, 42875 target) ; Learning rate = 0.000322 ; Loss = 1.492449\n",
      "2024-12-16 06:18:02.431000: I runner.py:310] Step = 75600 ; steps/s = 1.62, tokens/s = 82649 (39836 source, 42813 target) ; Learning rate = 0.000321 ; Loss = 1.489542\n",
      "2024-12-16 06:19:03.691000: I runner.py:310] Step = 75700 ; steps/s = 1.63, tokens/s = 81783 (39412 source, 42371 target) ; Learning rate = 0.000321 ; Loss = 1.490963\n",
      "2024-12-16 06:20:05.278000: I runner.py:310] Step = 75800 ; steps/s = 1.62, tokens/s = 82908 (39909 source, 42999 target) ; Learning rate = 0.000321 ; Loss = 1.488140\n",
      "2024-12-16 06:21:06.915000: I runner.py:310] Step = 75900 ; steps/s = 1.62, tokens/s = 82796 (39907 source, 42889 target) ; Learning rate = 0.000321 ; Loss = 1.483162\n",
      "2024-12-16 06:22:08.614000: I runner.py:310] Step = 76000 ; steps/s = 1.62, tokens/s = 82691 (39846 source, 42845 target) ; Learning rate = 0.000321 ; Loss = 1.486770\n",
      "2024-12-16 06:23:09.784000: I runner.py:310] Step = 76100 ; steps/s = 1.63, tokens/s = 81853 (39431 source, 42422 target) ; Learning rate = 0.000320 ; Loss = 1.488277\n",
      "2024-12-16 06:24:11.519000: I runner.py:310] Step = 76200 ; steps/s = 1.62, tokens/s = 82678 (39838 source, 42840 target) ; Learning rate = 0.000320 ; Loss = 1.483383\n",
      "2024-12-16 06:25:13.202000: I runner.py:310] Step = 76300 ; steps/s = 1.62, tokens/s = 82753 (39859 source, 42894 target) ; Learning rate = 0.000320 ; Loss = 1.492622\n",
      "2024-12-16 06:26:14.855000: I runner.py:310] Step = 76400 ; steps/s = 1.62, tokens/s = 82750 (39891 source, 42859 target) ; Learning rate = 0.000320 ; Loss = 1.490010\n",
      "2024-12-16 06:27:16.011000: I runner.py:310] Step = 76500 ; steps/s = 1.64, tokens/s = 81938 (39468 source, 42470 target) ; Learning rate = 0.000320 ; Loss = 1.490690\n",
      "2024-12-16 06:28:17.743000: I runner.py:310] Step = 76600 ; steps/s = 1.62, tokens/s = 82675 (39858 source, 42817 target) ; Learning rate = 0.000319 ; Loss = 1.490178\n",
      "2024-12-16 06:29:19.420000: I runner.py:310] Step = 76700 ; steps/s = 1.62, tokens/s = 82775 (39877 source, 42898 target) ; Learning rate = 0.000319 ; Loss = 1.487431\n",
      "2024-12-16 06:30:21.017000: I runner.py:310] Step = 76800 ; steps/s = 1.62, tokens/s = 82833 (39903 source, 42930 target) ; Learning rate = 0.000319 ; Loss = 1.490441\n",
      "2024-12-16 06:31:22.207000: I runner.py:310] Step = 76900 ; steps/s = 1.63, tokens/s = 81845 (39421 source, 42424 target) ; Learning rate = 0.000319 ; Loss = 1.485417\n",
      "2024-12-16 06:32:23.877000: I runner.py:310] Step = 77000 ; steps/s = 1.62, tokens/s = 82736 (39882 source, 42854 target) ; Learning rate = 0.000319 ; Loss = 1.486520\n",
      "2024-12-16 06:33:25.548000: I runner.py:310] Step = 77100 ; steps/s = 1.62, tokens/s = 82775 (39889 source, 42886 target) ; Learning rate = 0.000318 ; Loss = 1.492119\n",
      "2024-12-16 06:34:27.211000: I runner.py:310] Step = 77200 ; steps/s = 1.62, tokens/s = 82753 (39854 source, 42899 target) ; Learning rate = 0.000318 ; Loss = 1.485134\n",
      "2024-12-16 06:35:28.465000: I runner.py:310] Step = 77300 ; steps/s = 1.63, tokens/s = 81781 (39398 source, 42383 target) ; Learning rate = 0.000318 ; Loss = 1.490111\n",
      "2024-12-16 06:36:30.124000: I runner.py:310] Step = 77400 ; steps/s = 1.62, tokens/s = 82753 (39869 source, 42884 target) ; Learning rate = 0.000318 ; Loss = 1.488044\n",
      "2024-12-16 06:37:31.768000: I runner.py:310] Step = 77500 ; steps/s = 1.62, tokens/s = 82782 (39883 source, 42899 target) ; Learning rate = 0.000317 ; Loss = 1.490508\n",
      "2024-12-16 06:38:33.406000: I runner.py:310] Step = 77600 ; steps/s = 1.62, tokens/s = 82823 (39915 source, 42908 target) ; Learning rate = 0.000317 ; Loss = 1.484642\n",
      "2024-12-16 06:39:34.495000: I runner.py:310] Step = 77700 ; steps/s = 1.64, tokens/s = 81984 (39488 source, 42496 target) ; Learning rate = 0.000317 ; Loss = 1.491579\n",
      "2024-12-16 06:40:36.121000: I runner.py:310] Step = 77800 ; steps/s = 1.62, tokens/s = 82842 (39876 source, 42966 target) ; Learning rate = 0.000317 ; Loss = 1.488921\n",
      "2024-12-16 06:41:37.848000: I runner.py:310] Step = 77900 ; steps/s = 1.62, tokens/s = 82688 (39860 source, 42828 target) ; Learning rate = 0.000317 ; Loss = 1.489583\n",
      "2024-12-16 06:42:39.541000: I runner.py:310] Step = 78000 ; steps/s = 1.62, tokens/s = 82701 (39840 source, 42861 target) ; Learning rate = 0.000316 ; Loss = 1.493288\n",
      "2024-12-16 06:43:40.762000: I runner.py:310] Step = 78100 ; steps/s = 1.63, tokens/s = 81819 (39453 source, 42366 target) ; Learning rate = 0.000316 ; Loss = 1.483893\n",
      "2024-12-16 06:44:42.415000: I runner.py:310] Step = 78200 ; steps/s = 1.62, tokens/s = 82793 (39906 source, 42887 target) ; Learning rate = 0.000316 ; Loss = 1.488466\n",
      "2024-12-16 06:45:44.037000: I runner.py:310] Step = 78300 ; steps/s = 1.62, tokens/s = 82824 (39916 source, 42908 target) ; Learning rate = 0.000316 ; Loss = 1.488951\n",
      "2024-12-16 06:46:45.717000: I runner.py:310] Step = 78400 ; steps/s = 1.62, tokens/s = 82715 (39829 source, 42886 target) ; Learning rate = 0.000316 ; Loss = 1.493129\n",
      "2024-12-16 06:47:46.931000: I runner.py:310] Step = 78500 ; steps/s = 1.63, tokens/s = 81834 (39416 source, 42418 target) ; Learning rate = 0.000315 ; Loss = 1.486824\n",
      "2024-12-16 06:48:48.627000: I runner.py:310] Step = 78600 ; steps/s = 1.62, tokens/s = 82736 (39861 source, 42875 target) ; Learning rate = 0.000315 ; Loss = 1.488012\n",
      "2024-12-16 06:49:50.319000: I runner.py:310] Step = 78700 ; steps/s = 1.62, tokens/s = 82752 (39873 source, 42879 target) ; Learning rate = 0.000315 ; Loss = 1.492802\n",
      "2024-12-16 06:50:52.061000: I runner.py:310] Step = 78800 ; steps/s = 1.62, tokens/s = 82634 (39797 source, 42837 target) ; Learning rate = 0.000315 ; Loss = 1.488982\n",
      "2024-12-16 06:51:53.248000: I runner.py:310] Step = 78900 ; steps/s = 1.63, tokens/s = 81871 (39451 source, 42420 target) ; Learning rate = 0.000315 ; Loss = 1.491278\n",
      "2024-12-16 06:52:54.865000: I runner.py:310] Step = 79000 ; steps/s = 1.62, tokens/s = 82834 (39916 source, 42918 target) ; Learning rate = 0.000314 ; Loss = 1.481440\n",
      "2024-12-16 06:53:56.484000: I runner.py:310] Step = 79100 ; steps/s = 1.62, tokens/s = 82816 (39915 source, 42901 target) ; Learning rate = 0.000314 ; Loss = 1.489253\n",
      "2024-12-16 06:54:58.150000: I runner.py:310] Step = 79200 ; steps/s = 1.62, tokens/s = 82741 (39841 source, 42900 target) ; Learning rate = 0.000314 ; Loss = 1.487055\n",
      "2024-12-16 06:55:59.366000: I runner.py:310] Step = 79300 ; steps/s = 1.63, tokens/s = 81812 (39416 source, 42396 target) ; Learning rate = 0.000314 ; Loss = 1.493780\n",
      "2024-12-16 06:57:01.032000: I runner.py:310] Step = 79400 ; steps/s = 1.62, tokens/s = 82761 (39885 source, 42876 target) ; Learning rate = 0.000314 ; Loss = 1.483980\n",
      "2024-12-16 06:58:02.704000: I runner.py:310] Step = 79500 ; steps/s = 1.62, tokens/s = 82721 (39850 source, 42871 target) ; Learning rate = 0.000313 ; Loss = 1.487951\n",
      "2024-12-16 06:59:04.434000: I runner.py:310] Step = 79600 ; steps/s = 1.62, tokens/s = 82718 (39865 source, 42853 target) ; Learning rate = 0.000313 ; Loss = 1.486976\n",
      "2024-12-16 07:00:05.672000: I runner.py:310] Step = 79700 ; steps/s = 1.63, tokens/s = 81787 (39388 source, 42399 target) ; Learning rate = 0.000313 ; Loss = 1.479408\n",
      "2024-12-16 07:01:07.327000: I runner.py:310] Step = 79800 ; steps/s = 1.62, tokens/s = 82803 (39884 source, 42919 target) ; Learning rate = 0.000313 ; Loss = 1.489068\n",
      "2024-12-16 07:02:09.026000: I runner.py:310] Step = 79900 ; steps/s = 1.62, tokens/s = 82711 (39875 source, 42836 target) ; Learning rate = 0.000313 ; Loss = 1.486219\n",
      "2024-12-16 07:03:10.591000: I runner.py:310] Step = 80000 ; steps/s = 1.62, tokens/s = 82893 (39938 source, 42955 target) ; Learning rate = 0.000312 ; Loss = 1.493409\n",
      "2024-12-16 07:03:12.467000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-80000\n",
      "2024-12-16 07:03:12.467000: I training.py:192] Running evaluation for step 80000\n",
      "2024-12-16 07:07:36.122000: I training.py:192] Evaluation result for step 80000: loss = 1.195617 ; perplexity = 3.305596\n",
      "2024-12-16 07:08:37.112000: I runner.py:310] Step = 80100 ; steps/s = 1.64, tokens/s = 82167 (39574 source, 42593 target) ; Learning rate = 0.000312 ; Loss = 1.491823\n",
      "2024-12-16 07:09:38.703000: I runner.py:310] Step = 80200 ; steps/s = 1.62, tokens/s = 82885 (39917 source, 42968 target) ; Learning rate = 0.000312 ; Loss = 1.490919\n",
      "2024-12-16 07:10:40.367000: I runner.py:310] Step = 80300 ; steps/s = 1.62, tokens/s = 82774 (39903 source, 42871 target) ; Learning rate = 0.000312 ; Loss = 1.482991\n",
      "2024-12-16 07:11:42.022000: I runner.py:310] Step = 80400 ; steps/s = 1.62, tokens/s = 82745 (39871 source, 42874 target) ; Learning rate = 0.000312 ; Loss = 1.485389\n",
      "2024-12-16 07:12:43.182000: I runner.py:310] Step = 80500 ; steps/s = 1.64, tokens/s = 81875 (39437 source, 42438 target) ; Learning rate = 0.000312 ; Loss = 1.491725\n",
      "2024-12-16 07:13:44.810000: I runner.py:310] Step = 80600 ; steps/s = 1.62, tokens/s = 82829 (39892 source, 42937 target) ; Learning rate = 0.000311 ; Loss = 1.479540\n",
      "2024-12-16 07:14:46.422000: I runner.py:310] Step = 80700 ; steps/s = 1.62, tokens/s = 82851 (39914 source, 42937 target) ; Learning rate = 0.000311 ; Loss = 1.479919\n",
      "2024-12-16 07:15:48.070000: I runner.py:310] Step = 80800 ; steps/s = 1.62, tokens/s = 82771 (39908 source, 42863 target) ; Learning rate = 0.000311 ; Loss = 1.481901\n",
      "2024-12-16 07:16:49.218000: I runner.py:310] Step = 80900 ; steps/s = 1.64, tokens/s = 81880 (39453 source, 42427 target) ; Learning rate = 0.000311 ; Loss = 1.494711\n",
      "2024-12-16 07:17:50.838000: I runner.py:310] Step = 81000 ; steps/s = 1.62, tokens/s = 82855 (39905 source, 42950 target) ; Learning rate = 0.000311 ; Loss = 1.480661\n",
      "2024-12-16 07:18:52.417000: I runner.py:310] Step = 81100 ; steps/s = 1.62, tokens/s = 82888 (39962 source, 42926 target) ; Learning rate = 0.000310 ; Loss = 1.488301\n",
      "2024-12-16 07:19:54.070000: I runner.py:310] Step = 81200 ; steps/s = 1.62, tokens/s = 82784 (39871 source, 42913 target) ; Learning rate = 0.000310 ; Loss = 1.483332\n",
      "2024-12-16 07:20:55.249000: I runner.py:310] Step = 81300 ; steps/s = 1.63, tokens/s = 81883 (39460 source, 42423 target) ; Learning rate = 0.000310 ; Loss = 1.481732\n",
      "2024-12-16 07:21:56.893000: I runner.py:310] Step = 81400 ; steps/s = 1.62, tokens/s = 82781 (39860 source, 42921 target) ; Learning rate = 0.000310 ; Loss = 1.493581\n",
      "2024-12-16 07:22:58.525000: I runner.py:310] Step = 81500 ; steps/s = 1.62, tokens/s = 82814 (39911 source, 42903 target) ; Learning rate = 0.000310 ; Loss = 1.487414\n",
      "2024-12-16 07:24:00.127000: I runner.py:310] Step = 81600 ; steps/s = 1.62, tokens/s = 82829 (39898 source, 42931 target) ; Learning rate = 0.000309 ; Loss = 1.488367\n",
      "2024-12-16 07:25:01.276000: I runner.py:310] Step = 81700 ; steps/s = 1.64, tokens/s = 81904 (39455 source, 42449 target) ; Learning rate = 0.000309 ; Loss = 1.490773\n",
      "2024-12-16 07:26:02.901000: I runner.py:310] Step = 81800 ; steps/s = 1.62, tokens/s = 82842 (39906 source, 42936 target) ; Learning rate = 0.000309 ; Loss = 1.482985\n",
      "2024-12-16 07:27:04.528000: I runner.py:310] Step = 81900 ; steps/s = 1.62, tokens/s = 82803 (39906 source, 42897 target) ; Learning rate = 0.000309 ; Loss = 1.479955\n",
      "2024-12-16 07:28:06.158000: I runner.py:310] Step = 82000 ; steps/s = 1.62, tokens/s = 82824 (39916 source, 42908 target) ; Learning rate = 0.000309 ; Loss = 1.484594\n",
      "2024-12-16 07:29:07.301000: I runner.py:310] Step = 82100 ; steps/s = 1.64, tokens/s = 81900 (39440 source, 42460 target) ; Learning rate = 0.000308 ; Loss = 1.482594\n",
      "2024-12-16 07:30:08.897000: I runner.py:310] Step = 82200 ; steps/s = 1.62, tokens/s = 82828 (39895 source, 42933 target) ; Learning rate = 0.000308 ; Loss = 1.486333\n",
      "2024-12-16 07:31:10.570000: I runner.py:310] Step = 82300 ; steps/s = 1.62, tokens/s = 82760 (39878 source, 42882 target) ; Learning rate = 0.000308 ; Loss = 1.490792\n",
      "2024-12-16 07:32:12.140000: I runner.py:310] Step = 82400 ; steps/s = 1.62, tokens/s = 82906 (39969 source, 42937 target) ; Learning rate = 0.000308 ; Loss = 1.485879\n",
      "2024-12-16 07:33:13.197000: I runner.py:310] Step = 82500 ; steps/s = 1.64, tokens/s = 82037 (39507 source, 42530 target) ; Learning rate = 0.000308 ; Loss = 1.492677\n",
      "2024-12-16 07:34:14.837000: I runner.py:310] Step = 82600 ; steps/s = 1.62, tokens/s = 82794 (39886 source, 42908 target) ; Learning rate = 0.000308 ; Loss = 1.482189\n",
      "2024-12-16 07:35:16.482000: I runner.py:310] Step = 82700 ; steps/s = 1.62, tokens/s = 82820 (39897 source, 42923 target) ; Learning rate = 0.000307 ; Loss = 1.480326\n",
      "2024-12-16 07:36:18.086000: I runner.py:310] Step = 82800 ; steps/s = 1.62, tokens/s = 82812 (39922 source, 42890 target) ; Learning rate = 0.000307 ; Loss = 1.483519\n",
      "2024-12-16 07:37:19.179000: I runner.py:310] Step = 82900 ; steps/s = 1.64, tokens/s = 82013 (39501 source, 42512 target) ; Learning rate = 0.000307 ; Loss = 1.477178\n",
      "2024-12-16 07:38:20.857000: I runner.py:310] Step = 83000 ; steps/s = 1.62, tokens/s = 82753 (39896 source, 42857 target) ; Learning rate = 0.000307 ; Loss = 1.489071\n",
      "2024-12-16 07:39:22.529000: I runner.py:310] Step = 83100 ; steps/s = 1.62, tokens/s = 82743 (39864 source, 42879 target) ; Learning rate = 0.000307 ; Loss = 1.484783\n",
      "2024-12-16 07:40:24.123000: I runner.py:310] Step = 83200 ; steps/s = 1.62, tokens/s = 82854 (39912 source, 42942 target) ; Learning rate = 0.000306 ; Loss = 1.483262\n",
      "2024-12-16 07:41:25.229000: I runner.py:310] Step = 83300 ; steps/s = 1.64, tokens/s = 81983 (39474 source, 42509 target) ; Learning rate = 0.000306 ; Loss = 1.473799\n",
      "2024-12-16 07:42:26.906000: I runner.py:310] Step = 83400 ; steps/s = 1.62, tokens/s = 82773 (39915 source, 42858 target) ; Learning rate = 0.000306 ; Loss = 1.485393\n",
      "2024-12-16 07:43:28.513000: I runner.py:310] Step = 83500 ; steps/s = 1.62, tokens/s = 82818 (39906 source, 42912 target) ; Learning rate = 0.000306 ; Loss = 1.488869\n",
      "2024-12-16 07:44:29.836000: I runner.py:310] Step = 83600 ; steps/s = 1.63, tokens/s = 82069 (39518 source, 42551 target) ; Learning rate = 0.000306 ; Loss = 1.498445\n",
      "2024-12-16 07:45:31.379000: I runner.py:310] Step = 83700 ; steps/s = 1.63, tokens/s = 82515 (39747 source, 42768 target) ; Learning rate = 0.000306 ; Loss = 1.480237\n",
      "2024-12-16 07:46:33.006000: I runner.py:310] Step = 83800 ; steps/s = 1.62, tokens/s = 82829 (39937 source, 42892 target) ; Learning rate = 0.000305 ; Loss = 1.484258\n",
      "2024-12-16 07:47:34.581000: I runner.py:310] Step = 83900 ; steps/s = 1.62, tokens/s = 82860 (39927 source, 42933 target) ; Learning rate = 0.000305 ; Loss = 1.487365\n",
      "2024-12-16 07:48:35.751000: I runner.py:310] Step = 84000 ; steps/s = 1.63, tokens/s = 81881 (39434 source, 42447 target) ; Learning rate = 0.000305 ; Loss = 1.486428\n",
      "2024-12-16 07:49:37.384000: I runner.py:310] Step = 84100 ; steps/s = 1.62, tokens/s = 82834 (39890 source, 42944 target) ; Learning rate = 0.000305 ; Loss = 1.485164\n",
      "2024-12-16 07:50:38.933000: I runner.py:310] Step = 84200 ; steps/s = 1.62, tokens/s = 82936 (39953 source, 42983 target) ; Learning rate = 0.000305 ; Loss = 1.485019\n",
      "2024-12-16 07:51:40.508000: I runner.py:310] Step = 84300 ; steps/s = 1.62, tokens/s = 82852 (39933 source, 42919 target) ; Learning rate = 0.000304 ; Loss = 1.482049\n",
      "2024-12-16 07:52:41.706000: I runner.py:310] Step = 84400 ; steps/s = 1.63, tokens/s = 81825 (39422 source, 42403 target) ; Learning rate = 0.000304 ; Loss = 1.477571\n",
      "2024-12-16 07:53:43.218000: I runner.py:310] Step = 84500 ; steps/s = 1.63, tokens/s = 83009 (40022 source, 42987 target) ; Learning rate = 0.000304 ; Loss = 1.480011\n",
      "2024-12-16 07:54:44.790000: I runner.py:310] Step = 84600 ; steps/s = 1.62, tokens/s = 82870 (39932 source, 42938 target) ; Learning rate = 0.000304 ; Loss = 1.486398\n",
      "2024-12-16 07:55:46.359000: I runner.py:310] Step = 84700 ; steps/s = 1.62, tokens/s = 82882 (39911 source, 42971 target) ; Learning rate = 0.000304 ; Loss = 1.487581\n",
      "2024-12-16 07:56:47.489000: I runner.py:310] Step = 84800 ; steps/s = 1.64, tokens/s = 81934 (39472 source, 42462 target) ; Learning rate = 0.000304 ; Loss = 1.489811\n",
      "2024-12-16 07:57:49.052000: I runner.py:310] Step = 84900 ; steps/s = 1.62, tokens/s = 82919 (39939 source, 42980 target) ; Learning rate = 0.000303 ; Loss = 1.489607\n",
      "2024-12-16 07:58:50.681000: I runner.py:310] Step = 85000 ; steps/s = 1.62, tokens/s = 82803 (39891 source, 42912 target) ; Learning rate = 0.000303 ; Loss = 1.480618\n",
      "2024-12-16 07:58:50.682000: I training.py:192] Running evaluation for step 85000\n",
      "2024-12-16 08:03:12.532000: I training.py:192] Evaluation result for step 85000: loss = 1.201206 ; perplexity = 3.324125\n",
      "2024-12-16 08:04:14.096000: I runner.py:310] Step = 85100 ; steps/s = 1.62, tokens/s = 82911 (39958 source, 42953 target) ; Learning rate = 0.000303 ; Loss = 1.480984\n",
      "2024-12-16 08:05:15.241000: I runner.py:310] Step = 85200 ; steps/s = 1.64, tokens/s = 81939 (39472 source, 42467 target) ; Learning rate = 0.000303 ; Loss = 1.482143\n",
      "2024-12-16 08:06:16.854000: I runner.py:310] Step = 85300 ; steps/s = 1.62, tokens/s = 82818 (39886 source, 42932 target) ; Learning rate = 0.000303 ; Loss = 1.485729\n",
      "2024-12-16 08:07:18.514000: I runner.py:310] Step = 85400 ; steps/s = 1.62, tokens/s = 82778 (39865 source, 42913 target) ; Learning rate = 0.000302 ; Loss = 1.486156\n",
      "2024-12-16 08:08:20.122000: I runner.py:310] Step = 85500 ; steps/s = 1.62, tokens/s = 82817 (39910 source, 42907 target) ; Learning rate = 0.000302 ; Loss = 1.489420\n",
      "2024-12-16 08:09:21.234000: I runner.py:310] Step = 85600 ; steps/s = 1.64, tokens/s = 81989 (39541 source, 42448 target) ; Learning rate = 0.000302 ; Loss = 1.483220\n",
      "2024-12-16 08:10:22.891000: I runner.py:310] Step = 85700 ; steps/s = 1.62, tokens/s = 82780 (39861 source, 42919 target) ; Learning rate = 0.000302 ; Loss = 1.478389\n",
      "2024-12-16 08:11:24.522000: I runner.py:310] Step = 85800 ; steps/s = 1.62, tokens/s = 82798 (39888 source, 42910 target) ; Learning rate = 0.000302 ; Loss = 1.485873\n",
      "2024-12-16 08:12:26.141000: I runner.py:310] Step = 85900 ; steps/s = 1.62, tokens/s = 82834 (39932 source, 42902 target) ; Learning rate = 0.000302 ; Loss = 1.481116\n",
      "2024-12-16 08:13:27.342000: I runner.py:310] Step = 86000 ; steps/s = 1.63, tokens/s = 81843 (39421 source, 42422 target) ; Learning rate = 0.000301 ; Loss = 1.491822\n",
      "2024-12-16 08:14:28.924000: I runner.py:310] Step = 86100 ; steps/s = 1.62, tokens/s = 82877 (39908 source, 42969 target) ; Learning rate = 0.000301 ; Loss = 1.483417\n",
      "2024-12-16 08:15:30.502000: I runner.py:310] Step = 86200 ; steps/s = 1.62, tokens/s = 82892 (39941 source, 42951 target) ; Learning rate = 0.000301 ; Loss = 1.480316\n",
      "2024-12-16 08:16:32.133000: I runner.py:310] Step = 86300 ; steps/s = 1.62, tokens/s = 82810 (39901 source, 42909 target) ; Learning rate = 0.000301 ; Loss = 1.487598\n",
      "2024-12-16 08:17:33.265000: I runner.py:310] Step = 86400 ; steps/s = 1.64, tokens/s = 81921 (39484 source, 42437 target) ; Learning rate = 0.000301 ; Loss = 1.482991\n",
      "2024-12-16 08:18:34.898000: I runner.py:310] Step = 86500 ; steps/s = 1.62, tokens/s = 82804 (39891 source, 42913 target) ; Learning rate = 0.000301 ; Loss = 1.480620\n",
      "2024-12-16 08:19:36.495000: I runner.py:310] Step = 86600 ; steps/s = 1.62, tokens/s = 82891 (39933 source, 42958 target) ; Learning rate = 0.000300 ; Loss = 1.484064\n",
      "2024-12-16 08:20:38.104000: I runner.py:310] Step = 86700 ; steps/s = 1.62, tokens/s = 82831 (39923 source, 42908 target) ; Learning rate = 0.000300 ; Loss = 1.486130\n",
      "2024-12-16 08:21:39.256000: I runner.py:310] Step = 86800 ; steps/s = 1.64, tokens/s = 81909 (39474 source, 42435 target) ; Learning rate = 0.000300 ; Loss = 1.483393\n",
      "2024-12-16 08:22:40.879000: I runner.py:310] Step = 86900 ; steps/s = 1.62, tokens/s = 82809 (39892 source, 42917 target) ; Learning rate = 0.000300 ; Loss = 1.486022\n",
      "2024-12-16 08:23:42.413000: I runner.py:310] Step = 87000 ; steps/s = 1.63, tokens/s = 82936 (39959 source, 42977 target) ; Learning rate = 0.000300 ; Loss = 1.479592\n",
      "2024-12-16 08:24:44.063000: I runner.py:310] Step = 87100 ; steps/s = 1.62, tokens/s = 82797 (39908 source, 42889 target) ; Learning rate = 0.000299 ; Loss = 1.484858\n",
      "2024-12-16 08:25:45.202000: I runner.py:310] Step = 87200 ; steps/s = 1.64, tokens/s = 81888 (39429 source, 42459 target) ; Learning rate = 0.000299 ; Loss = 1.481771\n",
      "2024-12-16 08:26:46.832000: I runner.py:310] Step = 87300 ; steps/s = 1.62, tokens/s = 82812 (39907 source, 42905 target) ; Learning rate = 0.000299 ; Loss = 1.480946\n",
      "2024-12-16 08:27:48.477000: I runner.py:310] Step = 87400 ; steps/s = 1.62, tokens/s = 82821 (39903 source, 42918 target) ; Learning rate = 0.000299 ; Loss = 1.483923\n",
      "2024-12-16 08:28:50.132000: I runner.py:310] Step = 87500 ; steps/s = 1.62, tokens/s = 82769 (39886 source, 42883 target) ; Learning rate = 0.000299 ; Loss = 1.495481\n",
      "2024-12-16 08:29:51.305000: I runner.py:310] Step = 87600 ; steps/s = 1.63, tokens/s = 81867 (39414 source, 42453 target) ; Learning rate = 0.000299 ; Loss = 1.483932\n",
      "2024-12-16 08:30:52.897000: I runner.py:310] Step = 87700 ; steps/s = 1.62, tokens/s = 82863 (39935 source, 42928 target) ; Learning rate = 0.000298 ; Loss = 1.484581\n",
      "2024-12-16 08:31:54.505000: I runner.py:310] Step = 87800 ; steps/s = 1.62, tokens/s = 82857 (39932 source, 42925 target) ; Learning rate = 0.000298 ; Loss = 1.484274\n",
      "2024-12-16 08:32:56.139000: I runner.py:310] Step = 87900 ; steps/s = 1.62, tokens/s = 82788 (39890 source, 42898 target) ; Learning rate = 0.000298 ; Loss = 1.489511\n",
      "2024-12-16 08:33:57.244000: I runner.py:310] Step = 88000 ; steps/s = 1.64, tokens/s = 81976 (39495 source, 42481 target) ; Learning rate = 0.000298 ; Loss = 1.487545\n",
      "2024-12-16 08:34:58.814000: I runner.py:310] Step = 88100 ; steps/s = 1.62, tokens/s = 82884 (39926 source, 42958 target) ; Learning rate = 0.000298 ; Loss = 1.480848\n",
      "2024-12-16 08:36:00.423000: I runner.py:310] Step = 88200 ; steps/s = 1.62, tokens/s = 82854 (39923 source, 42931 target) ; Learning rate = 0.000298 ; Loss = 1.483529\n",
      "2024-12-16 08:37:02.083000: I runner.py:310] Step = 88300 ; steps/s = 1.62, tokens/s = 82789 (39875 source, 42914 target) ; Learning rate = 0.000297 ; Loss = 1.488932\n",
      "2024-12-16 08:38:03.204000: I runner.py:310] Step = 88400 ; steps/s = 1.64, tokens/s = 81937 (39478 source, 42459 target) ; Learning rate = 0.000297 ; Loss = 1.493688\n",
      "2024-12-16 08:39:04.803000: I runner.py:310] Step = 88500 ; steps/s = 1.62, tokens/s = 82918 (39924 source, 42994 target) ; Learning rate = 0.000297 ; Loss = 1.483402\n",
      "2024-12-16 08:40:06.440000: I runner.py:310] Step = 88600 ; steps/s = 1.62, tokens/s = 82748 (39881 source, 42867 target) ; Learning rate = 0.000297 ; Loss = 1.483403\n",
      "2024-12-16 08:41:08.076000: I runner.py:310] Step = 88700 ; steps/s = 1.62, tokens/s = 82790 (39900 source, 42890 target) ; Learning rate = 0.000297 ; Loss = 1.489755\n",
      "2024-12-16 08:42:09.305000: I runner.py:310] Step = 88800 ; steps/s = 1.63, tokens/s = 81771 (39364 source, 42407 target) ; Learning rate = 0.000297 ; Loss = 1.487496\n",
      "2024-12-16 08:43:10.923000: I runner.py:310] Step = 88900 ; steps/s = 1.62, tokens/s = 82841 (39938 source, 42903 target) ; Learning rate = 0.000296 ; Loss = 1.476684\n",
      "2024-12-16 08:44:12.550000: I runner.py:310] Step = 89000 ; steps/s = 1.62, tokens/s = 82832 (39933 source, 42899 target) ; Learning rate = 0.000296 ; Loss = 1.484603\n",
      "2024-12-16 08:45:14.183000: I runner.py:310] Step = 89100 ; steps/s = 1.62, tokens/s = 82782 (39893 source, 42889 target) ; Learning rate = 0.000296 ; Loss = 1.480502\n",
      "2024-12-16 08:46:15.317000: I runner.py:310] Step = 89200 ; steps/s = 1.64, tokens/s = 81926 (39449 source, 42477 target) ; Learning rate = 0.000296 ; Loss = 1.489363\n",
      "2024-12-16 08:47:16.905000: I runner.py:310] Step = 89300 ; steps/s = 1.62, tokens/s = 82867 (39935 source, 42932 target) ; Learning rate = 0.000296 ; Loss = 1.477031\n",
      "2024-12-16 08:48:18.454000: I runner.py:310] Step = 89400 ; steps/s = 1.62, tokens/s = 82936 (39958 source, 42978 target) ; Learning rate = 0.000296 ; Loss = 1.479951\n",
      "2024-12-16 08:49:20.026000: I runner.py:310] Step = 89500 ; steps/s = 1.62, tokens/s = 82910 (39940 source, 42970 target) ; Learning rate = 0.000295 ; Loss = 1.483497\n",
      "2024-12-16 08:50:21.164000: I runner.py:310] Step = 89600 ; steps/s = 1.64, tokens/s = 81947 (39475 source, 42472 target) ; Learning rate = 0.000295 ; Loss = 1.479189\n",
      "2024-12-16 08:51:22.781000: I runner.py:310] Step = 89700 ; steps/s = 1.62, tokens/s = 82783 (39918 source, 42865 target) ; Learning rate = 0.000295 ; Loss = 1.485290\n",
      "2024-12-16 08:52:24.434000: I runner.py:310] Step = 89800 ; steps/s = 1.62, tokens/s = 82750 (39864 source, 42886 target) ; Learning rate = 0.000295 ; Loss = 1.477425\n",
      "2024-12-16 08:53:26.069000: I runner.py:310] Step = 89900 ; steps/s = 1.62, tokens/s = 82820 (39892 source, 42928 target) ; Learning rate = 0.000295 ; Loss = 1.476671\n",
      "2024-12-16 08:54:27.230000: I runner.py:310] Step = 90000 ; steps/s = 1.64, tokens/s = 81915 (39430 source, 42485 target) ; Learning rate = 0.000295 ; Loss = 1.485977\n",
      "2024-12-16 08:54:29.054000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-90000\n",
      "2024-12-16 08:54:29.054000: I training.py:192] Running evaluation for step 90000\n",
      "2024-12-16 08:58:45.699000: I training.py:192] Evaluation result for step 90000: loss = 1.207136 ; perplexity = 3.343894\n",
      "2024-12-16 08:59:47.281000: I runner.py:310] Step = 90100 ; steps/s = 1.62, tokens/s = 82896 (39943 source, 42953 target) ; Learning rate = 0.000294 ; Loss = 1.479370\n",
      "2024-12-16 09:00:48.904000: I runner.py:310] Step = 90200 ; steps/s = 1.62, tokens/s = 82824 (39925 source, 42899 target) ; Learning rate = 0.000294 ; Loss = 1.485501\n",
      "2024-12-16 09:01:50.603000: I runner.py:310] Step = 90300 ; steps/s = 1.62, tokens/s = 82680 (39836 source, 42844 target) ; Learning rate = 0.000294 ; Loss = 1.488360\n",
      "2024-12-16 09:02:51.831000: I runner.py:310] Step = 90400 ; steps/s = 1.63, tokens/s = 81862 (39420 source, 42442 target) ; Learning rate = 0.000294 ; Loss = 1.480027\n",
      "2024-12-16 09:03:53.435000: I runner.py:310] Step = 90500 ; steps/s = 1.62, tokens/s = 82837 (39894 source, 42943 target) ; Learning rate = 0.000294 ; Loss = 1.474125\n",
      "2024-12-16 09:04:55.077000: I runner.py:310] Step = 90600 ; steps/s = 1.62, tokens/s = 82791 (39919 source, 42872 target) ; Learning rate = 0.000294 ; Loss = 1.482810\n",
      "2024-12-16 09:05:56.690000: I runner.py:310] Step = 90700 ; steps/s = 1.62, tokens/s = 82833 (39917 source, 42916 target) ; Learning rate = 0.000293 ; Loss = 1.481270\n",
      "2024-12-16 09:06:57.811000: I runner.py:310] Step = 90800 ; steps/s = 1.64, tokens/s = 81956 (39475 source, 42481 target) ; Learning rate = 0.000293 ; Loss = 1.475366\n",
      "2024-12-16 09:07:59.472000: I runner.py:310] Step = 90900 ; steps/s = 1.62, tokens/s = 82763 (39876 source, 42887 target) ; Learning rate = 0.000293 ; Loss = 1.485938\n",
      "2024-12-16 09:09:01.155000: I runner.py:310] Step = 91000 ; steps/s = 1.62, tokens/s = 82756 (39882 source, 42874 target) ; Learning rate = 0.000293 ; Loss = 1.485646\n",
      "2024-12-16 09:10:02.765000: I runner.py:310] Step = 91100 ; steps/s = 1.62, tokens/s = 82823 (39919 source, 42904 target) ; Learning rate = 0.000293 ; Loss = 1.487341\n",
      "2024-12-16 09:11:03.881000: I runner.py:310] Step = 91200 ; steps/s = 1.64, tokens/s = 81965 (39465 source, 42500 target) ; Learning rate = 0.000293 ; Loss = 1.482628\n",
      "2024-12-16 09:12:05.543000: I runner.py:310] Step = 91300 ; steps/s = 1.62, tokens/s = 82781 (39879 source, 42902 target) ; Learning rate = 0.000293 ; Loss = 1.484626\n",
      "2024-12-16 09:13:07.229000: I runner.py:310] Step = 91400 ; steps/s = 1.62, tokens/s = 82740 (39864 source, 42876 target) ; Learning rate = 0.000292 ; Loss = 1.477640\n",
      "2024-12-16 09:14:08.865000: I runner.py:310] Step = 91500 ; steps/s = 1.62, tokens/s = 82774 (39900 source, 42874 target) ; Learning rate = 0.000292 ; Loss = 1.484932\n",
      "2024-12-16 09:15:10.078000: I runner.py:310] Step = 91600 ; steps/s = 1.63, tokens/s = 81844 (39437 source, 42407 target) ; Learning rate = 0.000292 ; Loss = 1.489653\n",
      "2024-12-16 09:16:11.717000: I runner.py:310] Step = 91700 ; steps/s = 1.62, tokens/s = 82803 (39876 source, 42927 target) ; Learning rate = 0.000292 ; Loss = 1.479403\n",
      "2024-12-16 09:17:13.315000: I runner.py:310] Step = 91800 ; steps/s = 1.62, tokens/s = 82852 (39932 source, 42920 target) ; Learning rate = 0.000292 ; Loss = 1.483494\n",
      "2024-12-16 09:18:14.909000: I runner.py:310] Step = 91900 ; steps/s = 1.62, tokens/s = 82861 (39920 source, 42941 target) ; Learning rate = 0.000292 ; Loss = 1.480862\n",
      "2024-12-16 09:19:16.059000: I runner.py:310] Step = 92000 ; steps/s = 1.64, tokens/s = 81905 (39465 source, 42440 target) ; Learning rate = 0.000291 ; Loss = 1.481779\n",
      "2024-12-16 09:20:17.661000: I runner.py:310] Step = 92100 ; steps/s = 1.62, tokens/s = 82825 (39863 source, 42962 target) ; Learning rate = 0.000291 ; Loss = 1.474845\n",
      "2024-12-16 09:21:19.291000: I runner.py:310] Step = 92200 ; steps/s = 1.62, tokens/s = 82786 (39914 source, 42872 target) ; Learning rate = 0.000291 ; Loss = 1.476541\n",
      "2024-12-16 09:22:20.997000: I runner.py:310] Step = 92300 ; steps/s = 1.62, tokens/s = 82733 (39873 source, 42860 target) ; Learning rate = 0.000291 ; Loss = 1.479108\n",
      "2024-12-16 09:23:22.207000: I runner.py:310] Step = 92400 ; steps/s = 1.63, tokens/s = 81813 (39429 source, 42384 target) ; Learning rate = 0.000291 ; Loss = 1.479785\n",
      "2024-12-16 09:24:23.861000: I runner.py:310] Step = 92500 ; steps/s = 1.62, tokens/s = 82778 (39850 source, 42928 target) ; Learning rate = 0.000291 ; Loss = 1.479567\n",
      "2024-12-16 09:25:25.532000: I runner.py:310] Step = 92600 ; steps/s = 1.62, tokens/s = 82764 (39886 source, 42878 target) ; Learning rate = 0.000290 ; Loss = 1.477820\n",
      "2024-12-16 09:26:27.142000: I runner.py:310] Step = 92700 ; steps/s = 1.62, tokens/s = 82842 (39927 source, 42915 target) ; Learning rate = 0.000290 ; Loss = 1.478594\n",
      "2024-12-16 09:27:28.290000: I runner.py:310] Step = 92800 ; steps/s = 1.64, tokens/s = 81909 (39452 source, 42457 target) ; Learning rate = 0.000290 ; Loss = 1.476321\n",
      "2024-12-16 09:28:29.903000: I runner.py:310] Step = 92900 ; steps/s = 1.62, tokens/s = 82835 (39904 source, 42931 target) ; Learning rate = 0.000290 ; Loss = 1.482604\n",
      "2024-12-16 09:29:31.515000: I runner.py:310] Step = 93000 ; steps/s = 1.62, tokens/s = 82829 (39925 source, 42904 target) ; Learning rate = 0.000290 ; Loss = 1.488837\n",
      "2024-12-16 09:30:33.139000: I runner.py:310] Step = 93100 ; steps/s = 1.62, tokens/s = 82832 (39914 source, 42918 target) ; Learning rate = 0.000290 ; Loss = 1.480558\n",
      "2024-12-16 09:31:34.284000: I runner.py:310] Step = 93200 ; steps/s = 1.64, tokens/s = 81897 (39437 source, 42460 target) ; Learning rate = 0.000290 ; Loss = 1.479920\n",
      "2024-12-16 09:32:35.883000: I runner.py:310] Step = 93300 ; steps/s = 1.62, tokens/s = 82865 (39922 source, 42943 target) ; Learning rate = 0.000289 ; Loss = 1.483560\n",
      "2024-12-16 09:33:37.508000: I runner.py:310] Step = 93400 ; steps/s = 1.62, tokens/s = 82821 (39900 source, 42921 target) ; Learning rate = 0.000289 ; Loss = 1.480950\n",
      "2024-12-16 09:34:39.148000: I runner.py:310] Step = 93500 ; steps/s = 1.62, tokens/s = 82799 (39915 source, 42884 target) ; Learning rate = 0.000289 ; Loss = 1.476535\n",
      "2024-12-16 09:35:40.321000: I runner.py:310] Step = 93600 ; steps/s = 1.63, tokens/s = 81859 (39410 source, 42449 target) ; Learning rate = 0.000289 ; Loss = 1.473262\n",
      "2024-12-16 09:36:41.951000: I runner.py:310] Step = 93700 ; steps/s = 1.62, tokens/s = 82834 (39921 source, 42913 target) ; Learning rate = 0.000289 ; Loss = 1.481068\n",
      "2024-12-16 09:37:43.611000: I runner.py:310] Step = 93800 ; steps/s = 1.62, tokens/s = 82750 (39878 source, 42872 target) ; Learning rate = 0.000289 ; Loss = 1.482472\n",
      "2024-12-16 09:38:45.131000: I runner.py:310] Step = 93900 ; steps/s = 1.63, tokens/s = 82567 (39785 source, 42782 target) ; Learning rate = 0.000288 ; Loss = 1.501315\n",
      "2024-12-16 09:39:46.391000: I runner.py:310] Step = 94000 ; steps/s = 1.63, tokens/s = 82173 (39554 source, 42619 target) ; Learning rate = 0.000288 ; Loss = 1.477292\n",
      "2024-12-16 09:40:47.949000: I runner.py:310] Step = 94100 ; steps/s = 1.62, tokens/s = 82909 (39956 source, 42953 target) ; Learning rate = 0.000288 ; Loss = 1.485240\n",
      "2024-12-16 09:41:49.673000: I runner.py:310] Step = 94200 ; steps/s = 1.62, tokens/s = 82675 (39851 source, 42824 target) ; Learning rate = 0.000288 ; Loss = 1.480850\n",
      "2024-12-16 09:42:50.858000: I runner.py:310] Step = 94300 ; steps/s = 1.63, tokens/s = 81860 (39455 source, 42405 target) ; Learning rate = 0.000288 ; Loss = 1.479425\n",
      "2024-12-16 09:43:52.436000: I runner.py:310] Step = 94400 ; steps/s = 1.62, tokens/s = 82892 (39932 source, 42960 target) ; Learning rate = 0.000288 ; Loss = 1.479212\n",
      "2024-12-16 09:44:54.086000: I runner.py:310] Step = 94500 ; steps/s = 1.62, tokens/s = 82780 (39870 source, 42910 target) ; Learning rate = 0.000288 ; Loss = 1.479296\n",
      "2024-12-16 09:45:55.733000: I runner.py:310] Step = 94600 ; steps/s = 1.62, tokens/s = 82797 (39903 source, 42894 target) ; Learning rate = 0.000287 ; Loss = 1.478290\n",
      "2024-12-16 09:46:56.915000: I runner.py:310] Step = 94700 ; steps/s = 1.63, tokens/s = 81858 (39440 source, 42418 target) ; Learning rate = 0.000287 ; Loss = 1.481043\n",
      "2024-12-16 09:47:58.536000: I runner.py:310] Step = 94800 ; steps/s = 1.62, tokens/s = 82867 (39934 source, 42933 target) ; Learning rate = 0.000287 ; Loss = 1.476579\n",
      "2024-12-16 09:49:00.073000: I runner.py:310] Step = 94900 ; steps/s = 1.63, tokens/s = 82892 (39952 source, 42940 target) ; Learning rate = 0.000287 ; Loss = 1.477560\n",
      "2024-12-16 09:50:01.689000: I runner.py:310] Step = 95000 ; steps/s = 1.62, tokens/s = 82818 (39882 source, 42936 target) ; Learning rate = 0.000287 ; Loss = 1.483308\n",
      "2024-12-16 09:50:01.690000: I training.py:192] Running evaluation for step 95000\n",
      "2024-12-16 09:54:17.309000: I training.py:192] Evaluation result for step 95000: loss = 1.211538 ; perplexity = 3.358646\n",
      "2024-12-16 09:55:18.366000: I runner.py:310] Step = 95100 ; steps/s = 1.64, tokens/s = 82060 (39533 source, 42527 target) ; Learning rate = 0.000287 ; Loss = 1.484175\n",
      "2024-12-16 09:56:20.096000: I runner.py:310] Step = 95200 ; steps/s = 1.62, tokens/s = 82673 (39818 source, 42855 target) ; Learning rate = 0.000286 ; Loss = 1.475223\n",
      "2024-12-16 09:57:21.789000: I runner.py:310] Step = 95300 ; steps/s = 1.62, tokens/s = 82733 (39865 source, 42868 target) ; Learning rate = 0.000286 ; Loss = 1.482243\n",
      "2024-12-16 09:58:23.404000: I runner.py:310] Step = 95400 ; steps/s = 1.62, tokens/s = 82822 (39907 source, 42915 target) ; Learning rate = 0.000286 ; Loss = 1.478212\n",
      "2024-12-16 09:59:24.625000: I runner.py:310] Step = 95500 ; steps/s = 1.63, tokens/s = 81830 (39422 source, 42408 target) ; Learning rate = 0.000286 ; Loss = 1.476263\n",
      "2024-12-16 10:00:26.336000: I runner.py:310] Step = 95600 ; steps/s = 1.62, tokens/s = 82681 (39831 source, 42850 target) ; Learning rate = 0.000286 ; Loss = 1.473888\n",
      "2024-12-16 10:01:27.967000: I runner.py:310] Step = 95700 ; steps/s = 1.62, tokens/s = 82834 (39938 source, 42896 target) ; Learning rate = 0.000286 ; Loss = 1.481490\n",
      "2024-12-16 10:02:29.536000: I runner.py:310] Step = 95800 ; steps/s = 1.62, tokens/s = 82891 (39934 source, 42957 target) ; Learning rate = 0.000286 ; Loss = 1.483092\n",
      "2024-12-16 10:03:30.698000: I runner.py:310] Step = 95900 ; steps/s = 1.64, tokens/s = 81873 (39433 source, 42440 target) ; Learning rate = 0.000285 ; Loss = 1.482354\n",
      "2024-12-16 10:04:32.214000: I runner.py:310] Step = 96000 ; steps/s = 1.63, tokens/s = 82978 (39983 source, 42995 target) ; Learning rate = 0.000285 ; Loss = 1.476932\n",
      "2024-12-16 10:05:33.919000: I runner.py:310] Step = 96100 ; steps/s = 1.62, tokens/s = 82699 (39832 source, 42867 target) ; Learning rate = 0.000285 ; Loss = 1.483137\n",
      "2024-12-16 10:06:35.585000: I runner.py:310] Step = 96200 ; steps/s = 1.62, tokens/s = 82792 (39891 source, 42901 target) ; Learning rate = 0.000285 ; Loss = 1.481073\n",
      "2024-12-16 10:07:36.745000: I runner.py:310] Step = 96300 ; steps/s = 1.64, tokens/s = 81874 (39448 source, 42426 target) ; Learning rate = 0.000285 ; Loss = 1.483166\n",
      "2024-12-16 10:08:38.375000: I runner.py:310] Step = 96400 ; steps/s = 1.62, tokens/s = 82814 (39892 source, 42922 target) ; Learning rate = 0.000285 ; Loss = 1.481160\n",
      "2024-12-16 10:09:39.993000: I runner.py:310] Step = 96500 ; steps/s = 1.62, tokens/s = 82823 (39899 source, 42924 target) ; Learning rate = 0.000285 ; Loss = 1.478093\n",
      "2024-12-16 10:10:41.656000: I runner.py:310] Step = 96600 ; steps/s = 1.62, tokens/s = 82765 (39906 source, 42859 target) ; Learning rate = 0.000284 ; Loss = 1.479738\n",
      "2024-12-16 10:11:42.759000: I runner.py:310] Step = 96700 ; steps/s = 1.64, tokens/s = 81976 (39497 source, 42479 target) ; Learning rate = 0.000284 ; Loss = 1.480334\n",
      "2024-12-16 10:12:44.356000: I runner.py:310] Step = 96800 ; steps/s = 1.62, tokens/s = 82857 (39919 source, 42938 target) ; Learning rate = 0.000284 ; Loss = 1.476348\n",
      "2024-12-16 10:13:45.962000: I runner.py:310] Step = 96900 ; steps/s = 1.62, tokens/s = 82841 (39907 source, 42934 target) ; Learning rate = 0.000284 ; Loss = 1.475246\n",
      "2024-12-16 10:14:47.632000: I runner.py:310] Step = 97000 ; steps/s = 1.62, tokens/s = 82784 (39889 source, 42895 target) ; Learning rate = 0.000284 ; Loss = 1.478741\n",
      "2024-12-16 10:15:48.846000: I runner.py:310] Step = 97100 ; steps/s = 1.63, tokens/s = 81800 (39398 source, 42402 target) ; Learning rate = 0.000284 ; Loss = 1.485332\n",
      "2024-12-16 10:16:50.426000: I runner.py:310] Step = 97200 ; steps/s = 1.62, tokens/s = 82890 (39950 source, 42940 target) ; Learning rate = 0.000284 ; Loss = 1.479855\n",
      "2024-12-16 10:17:52.104000: I runner.py:310] Step = 97300 ; steps/s = 1.62, tokens/s = 82757 (39844 source, 42913 target) ; Learning rate = 0.000283 ; Loss = 1.475936\n",
      "2024-12-16 10:18:53.820000: I runner.py:310] Step = 97400 ; steps/s = 1.62, tokens/s = 82665 (39829 source, 42836 target) ; Learning rate = 0.000283 ; Loss = 1.474875\n",
      "2024-12-16 10:19:54.942000: I runner.py:310] Step = 97500 ; steps/s = 1.64, tokens/s = 81951 (39490 source, 42461 target) ; Learning rate = 0.000283 ; Loss = 1.484137\n",
      "2024-12-16 10:20:56.475000: I runner.py:310] Step = 97600 ; steps/s = 1.63, tokens/s = 82987 (39993 source, 42994 target) ; Learning rate = 0.000283 ; Loss = 1.471382\n",
      "2024-12-16 10:21:58.056000: I runner.py:310] Step = 97700 ; steps/s = 1.62, tokens/s = 82893 (39939 source, 42954 target) ; Learning rate = 0.000283 ; Loss = 1.476802\n",
      "2024-12-16 10:22:59.701000: I runner.py:310] Step = 97800 ; steps/s = 1.62, tokens/s = 82735 (39850 source, 42885 target) ; Learning rate = 0.000283 ; Loss = 1.479120\n",
      "2024-12-16 10:24:00.973000: I runner.py:310] Step = 97900 ; steps/s = 1.63, tokens/s = 81765 (39397 source, 42368 target) ; Learning rate = 0.000282 ; Loss = 1.473402\n",
      "2024-12-16 10:25:02.959000: I runner.py:310] Step = 98000 ; steps/s = 1.61, tokens/s = 82301 (39643 source, 42658 target) ; Learning rate = 0.000282 ; Loss = 1.473950\n",
      "2024-12-16 10:26:04.650000: I runner.py:310] Step = 98100 ; steps/s = 1.62, tokens/s = 82719 (39851 source, 42868 target) ; Learning rate = 0.000282 ; Loss = 1.484015\n",
      "2024-12-16 10:27:06.307000: I runner.py:310] Step = 98200 ; steps/s = 1.62, tokens/s = 82784 (39904 source, 42880 target) ; Learning rate = 0.000282 ; Loss = 1.478693\n",
      "2024-12-16 10:28:07.561000: I runner.py:310] Step = 98300 ; steps/s = 1.63, tokens/s = 81799 (39407 source, 42392 target) ; Learning rate = 0.000282 ; Loss = 1.470564\n",
      "2024-12-16 10:29:09.206000: I runner.py:310] Step = 98400 ; steps/s = 1.62, tokens/s = 82780 (39859 source, 42921 target) ; Learning rate = 0.000282 ; Loss = 1.478578\n",
      "2024-12-16 10:30:10.938000: I runner.py:310] Step = 98500 ; steps/s = 1.62, tokens/s = 82698 (39854 source, 42844 target) ; Learning rate = 0.000282 ; Loss = 1.483391\n",
      "2024-12-16 10:31:12.570000: I runner.py:310] Step = 98600 ; steps/s = 1.62, tokens/s = 82780 (39895 source, 42885 target) ; Learning rate = 0.000281 ; Loss = 1.482483\n",
      "2024-12-16 10:32:13.823000: I runner.py:310] Step = 98700 ; steps/s = 1.63, tokens/s = 81784 (39413 source, 42371 target) ; Learning rate = 0.000281 ; Loss = 1.478121\n",
      "2024-12-16 10:33:15.536000: I runner.py:310] Step = 98800 ; steps/s = 1.62, tokens/s = 82704 (39815 source, 42889 target) ; Learning rate = 0.000281 ; Loss = 1.477187\n",
      "2024-12-16 10:34:17.219000: I runner.py:310] Step = 98900 ; steps/s = 1.62, tokens/s = 82743 (39890 source, 42853 target) ; Learning rate = 0.000281 ; Loss = 1.478662\n",
      "2024-12-16 10:35:18.870000: I runner.py:310] Step = 99000 ; steps/s = 1.62, tokens/s = 82762 (39890 source, 42872 target) ; Learning rate = 0.000281 ; Loss = 1.480296\n",
      "2024-12-16 10:36:20.135000: I runner.py:310] Step = 99100 ; steps/s = 1.63, tokens/s = 81742 (39363 source, 42379 target) ; Learning rate = 0.000281 ; Loss = 1.480467\n",
      "2024-12-16 10:37:21.842000: I runner.py:310] Step = 99200 ; steps/s = 1.62, tokens/s = 82680 (39842 source, 42838 target) ; Learning rate = 0.000281 ; Loss = 1.479058\n",
      "2024-12-16 10:38:23.493000: I runner.py:310] Step = 99300 ; steps/s = 1.62, tokens/s = 82779 (39874 source, 42905 target) ; Learning rate = 0.000280 ; Loss = 1.477286\n",
      "2024-12-16 10:39:25.172000: I runner.py:310] Step = 99400 ; steps/s = 1.62, tokens/s = 82759 (39890 source, 42869 target) ; Learning rate = 0.000280 ; Loss = 1.476744\n",
      "2024-12-16 10:40:26.390000: I runner.py:310] Step = 99500 ; steps/s = 1.63, tokens/s = 81824 (39425 source, 42399 target) ; Learning rate = 0.000280 ; Loss = 1.475199\n",
      "2024-12-16 10:41:28.084000: I runner.py:310] Step = 99600 ; steps/s = 1.62, tokens/s = 82745 (39853 source, 42892 target) ; Learning rate = 0.000280 ; Loss = 1.480278\n",
      "2024-12-16 10:42:29.774000: I runner.py:310] Step = 99700 ; steps/s = 1.62, tokens/s = 82753 (39893 source, 42860 target) ; Learning rate = 0.000280 ; Loss = 1.479695\n",
      "2024-12-16 10:43:31.438000: I runner.py:310] Step = 99800 ; steps/s = 1.62, tokens/s = 82745 (39859 source, 42886 target) ; Learning rate = 0.000280 ; Loss = 1.482995\n",
      "2024-12-16 10:44:32.760000: I runner.py:310] Step = 99900 ; steps/s = 1.63, tokens/s = 81645 (39319 source, 42326 target) ; Learning rate = 0.000280 ; Loss = 1.471366\n",
      "2024-12-16 10:45:34.427000: I runner.py:310] Step = 100000 ; steps/s = 1.62, tokens/s = 82765 (39889 source, 42876 target) ; Learning rate = 0.000280 ; Loss = 1.477453\n",
      "2024-12-16 10:45:36.274000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-100000\n",
      "2024-12-16 10:45:36.274000: I training.py:192] Running evaluation for step 100000\n",
      "2024-12-16 10:49:51.679000: I training.py:192] Evaluation result for step 100000: loss = 1.214468 ; perplexity = 3.368501\n",
      "2024-12-16 10:50:53.249000: I runner.py:310] Step = 100100 ; steps/s = 1.62, tokens/s = 82915 (39950 source, 42965 target) ; Learning rate = 0.000279 ; Loss = 1.475984\n",
      "2024-12-16 10:51:55.247000: I runner.py:310] Step = 100200 ; steps/s = 1.61, tokens/s = 82326 (39687 source, 42639 target) ; Learning rate = 0.000279 ; Loss = 1.481356\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/omuceng/deneme/bin/onmt-main\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/opennmt/bin/main.py\", line 325, in main\n",
      "    runner.train(\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/opennmt/runner.py\", line 310, in train\n",
      "    summary = trainer(\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/opennmt/training.py\", line 111, in __call__\n",
      "    for i, loss in enumerate(self._steps(dataset, accum_steps=accum_steps)):\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/opennmt/training.py\", line 229, in _steps\n",
      "    accumulate_gradients(batch)\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 880, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 912, in _call\n",
      "    return self._no_variable_creation_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 134, in __call__\n",
      "    return concrete_function._call_flat(\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\", line 1745, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\", line 378, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/eager/execute.py\", line 52, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Tr-En (TED2020) -> Kk-En\n",
    "!onmt-main --model kk-tr-en-shared.py --config Tr-Kk-En_shared_vocab.yml --auto_config --checkpoint_path TR-EN-Shared-vocab/ckpt-100000 train --with_eval --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75754458-7b11-479c-a054-521684ccb045",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-16 10:52:53.847923: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-16 10:52:54.646980: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-16 10:52:54.647050: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-16 10:52:54.647058: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-16 10:52:55.580000: I main.py:308] Loading model description from TR-KK-EN-Shared-vocab/model_description.py\n",
      "2024-12-16 10:52:55.780000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-16 10:52:55.780000: I main.py:315] Using model:\n",
      "(model): MyCustomTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): WordEmbedder()\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): WordEmbedder()\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-16 10:52:55.785000: I main.py:340] Using parameters:\n",
      "data:\n",
      "  eval_features_file: KK_tokens_valid_shared\n",
      "  eval_labels_file: KK_valid_target_tokens_shared.txt\n",
      "  source_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file: KK_tokens_train_shared\n",
      "  train_labels_file: KK_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: TR-KK-EN-Shared-vocab\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 250000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-16 10:52:55.968755: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-16 10:52:56.577862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-16 10:52:56.748000: I inputter.py:316] Initialized source input layer:\n",
      "2024-12-16 10:52:56.748000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-16 10:52:56.748000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-16 10:52:56.820000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-16 10:52:56.821000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-16 10:52:56.821000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-16 10:52:56.839000: I runner.py:462] Restored checkpoint TR-KK-EN-Shared-vocab/ckpt-100000\n",
      "2024-12-16 10:52:56.876000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-16 10:52:57.382868: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-16 10:52:57.494000: I runner.py:471] Tracing and optimizing the inference graph...\n",
      "2024-12-16 10:53:10.455690: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-16 10:53:11.336787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-16 10:53:22.408000: I runner.py:471] 959 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-16 10:53:32.472000: I runner.py:471] 1983 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-16 10:53:42.504000: I runner.py:471] 2815 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-16 10:53:52.588000: I runner.py:471] 3583 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-16 10:54:02.764000: I runner.py:471] 4543 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-16 10:54:13.191000: I runner.py:471] 5407 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-16 10:54:23.535000: I runner.py:471] 6303 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-16 10:54:33.694000: I runner.py:471] 7167 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-16 10:54:43.736000: I runner.py:471] 8127 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-16 10:54:54.154000: I runner.py:471] 9151 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-16 10:55:04.196000: I runner.py:471] 9951 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-16 10:55:14.587000: I runner.py:471] 10847 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-16 10:55:33.451000: I runner.py:471] 12270 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-16 10:55:43.526000: I runner.py:471] 13102 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-16 10:55:53.639000: I runner.py:471] 14062 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-16 10:56:04.073000: I runner.py:471] 14926 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-16 10:56:14.081000: I runner.py:471] 15758 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-16 10:56:24.124000: I runner.py:471] 16622 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-16 10:56:34.558000: I runner.py:471] 17134 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-16 10:56:44.635000: I runner.py:471] 17833 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-16 10:57:07.670000: I runner.py:471] 17803 predictions are buffered, but waiting for the prediction of queued line 281 to advance the output...\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 onmt-main --config Tr-Kk-En_shared_vocab.yml --auto_config --checkpoint_path TR-KK-EN-Shared-vocab/ckpt-100000 infer --features_file KK_tokens_test_shared --predictions_file output_tr_kk_en_shared_vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1489a64b-ead7-4450-af70-98498e1dc87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 MT-Preparation/subwording/3-desubword.py en_shared_vocab.model output_tr_kk_en_shared_vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b2f1805-8965-40b6-a22c-6a7349d0e166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference first sentence: In the developed world, this figure is 35 25%\n",
      "Translated first sentence: In developed countries of the world , this figure is 35% 25%\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "BLEU2:  BLEU = 52.06 72.4/56.3/46.3/38.9 (BP = 1.000 ratio = 1.118 hyp_len = 463010 ref_len = 414303)\n",
      "CHRF:  chrF2 = 76.62\n"
     ]
    }
   ],
   "source": [
    "# BLEU and chrF scores\n",
    "!python3 compute-bleu.py en_test_shuffled.txt-filtered.en output_tr_kk_en_shared_vocab.txt.desubword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3924e665-2243-477c-bb99-015a1d1707a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama METEOR Puanı: 0.7636741748442291\n"
     ]
    }
   ],
   "source": [
    "# Average METEOR score (Ortalama METEOR Puanı)\n",
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "def read_and_tokenize_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return [nltk.word_tokenize(line.strip()) for line in lines]\n",
    "\n",
    "def calculate_meteor(reference_file, hypothesis_file):\n",
    "    references = read_and_tokenize_file(reference_file)\n",
    "    hypotheses = read_and_tokenize_file(hypothesis_file)\n",
    "    \n",
    "    if len(references) != len(hypotheses):\n",
    "        raise ValueError(\"Dosyaların satır sayıları eşleşmiyor\")\n",
    "\n",
    "    total_meteor_score = 0.0\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        total_meteor_score += meteor_score([ref], hyp)\n",
    "\n",
    "    average_meteor_score = total_meteor_score / len(references)\n",
    "    return average_meteor_score\n",
    "\n",
    "reference_file = 'en_test_shuffled.txt-filtered.en'\n",
    "hypothesis_file = 'output_tr_kk_en_shared_vocab.txt.desubword'\n",
    "\n",
    "score = calculate_meteor(reference_file, hypothesis_file)\n",
    "print(f\"Ortalama METEOR Puanı: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8c141-eb00-4b73-98ad-7becdf52d0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
