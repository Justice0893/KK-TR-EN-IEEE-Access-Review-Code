{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2c7cb96-8477-40ad-a314-0e9931805f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tukish POS Tagging\n",
    "import re\n",
    "from zemberek.morphology import TurkishMorphology\n",
    "from zemberek.tokenization import TurkishTokenizer\n",
    "\n",
    "def pos_tagger(input, output):\n",
    "    tokenizer = TurkishTokenizer.DEFAULT\n",
    "    morphology = TurkishMorphology.create_with_defaults()\n",
    "    \n",
    "    input_file_path = input\n",
    "    output_file_path = output\n",
    "    \n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        with open(input_file_path, \"r\") as input_file:\n",
    "            line_count = 0\n",
    "            error_count = 0\n",
    "            \n",
    "            for line in input_file:\n",
    "                line_count += 1\n",
    "                tags = []\n",
    "                data = line.strip()\n",
    "                tokens = tokenizer.tokenize(data)\n",
    "                \n",
    "                for token in tokens:\n",
    "                    analysis = morphology.analyze(token.normalized)\n",
    "                    if analysis.analysis_results:\n",
    "                        match = re.search(r\"\\[(.*?):(.*?)(?=[\\]\\.,;!?]|$)\", str(analysis.analysis_results[0]))\n",
    "                        if match:\n",
    "                            tag = match.group(2)\n",
    "                            tag = re.sub(r'[^\\w\\s]', '', str(tag))\n",
    "                            tags.append(str(tag) + \" \")\n",
    "                    else:\n",
    "                        tags.append(\"X \")\n",
    "                \n",
    "                if len(tokens) != len(tags):\n",
    "                    error_count += 1\n",
    "                    print(f\"Error in line {line_count}: Token count ({len(tokens)}) does not match tag count ({len(tags)})\")\n",
    "    \n",
    "                output_file.write(\"\".join(tags) + \"\\n\")\n",
    "    \n",
    "            print(f\"Total lines processed: {line_count}\")\n",
    "            print(f\"Total errors found: {error_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a78bad4-7b3d-477b-ba43-d7bfda973847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-06 15:17:08,720 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 12.091636180877686\n",
      "\n",
      "Total lines processed: 337547\n",
      "Total errors found: 0\n",
      "2024-12-06 16:40:14,241 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 11.944702386856079\n",
      "\n",
      "Total lines processed: 10000\n",
      "Total errors found: 0\n",
      "2024-12-06 16:42:57,374 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 13.04245138168335\n",
      "\n",
      "Total lines processed: 10000\n",
      "Total errors found: 0\n"
     ]
    }
   ],
   "source": [
    "pos_tagger(\"TED2020.en-tr.tr-filtered.tr.train\",\"TED2020_pos_tags_train\")\n",
    "pos_tagger(\"TED2020.en-tr.tr-filtered.tr.dev\",\"TED2020_pos_tags_dev\")\n",
    "pos_tagger(\"TED2020.en-tr.tr-filtered.tr.test\",\"TED2020_pos_tags_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1aacb1-da1e-4311-b069-5f1545fee11b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TED2020 tr vocab\n",
    "!onmt-build-vocab --sentencepiece model_type=bpe --size 32000 --save_vocab tr_vocab TED2020.en-tr.tr-filtered.tr.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017418b3-992f-4229-886b-245a975e0838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TED2020 en vocab\n",
    "!onmt-build-vocab --sentencepiece model_type=bpe --size 32000 --save_vocab en_vocab TED2020.en-tr.en-filtered.en.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55fcebbf-54aa-4522-bf41-7a9068a6650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turkish side subwording and subword units POS Tags\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "\n",
    "def preprocess_text(text, pos_tags):\n",
    "\n",
    "    text = re.sub(r\"([.,!?;:()\\\"'])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    new_pos_tags = []\n",
    "    i = 0  # pos_tags index'i\n",
    "    for word in words:\n",
    "        #if re.match(r\"[.,!?;:()\\\"']\", word):\n",
    "            #new_pos_tags.append('Punc') \n",
    "            #pass\n",
    "        #else:\n",
    "            if i < len(pos_tags):\n",
    "                new_pos_tags.append(pos_tags[i])\n",
    "                i += 1\n",
    "\n",
    "    return words, new_pos_tags\n",
    "\n",
    "def tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path):\n",
    "\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(model_path)\n",
    "    \n",
    "    with open(input_text_path, 'r', encoding='utf-8') as text_file, \\\n",
    "         open(input_pos_path, 'r', encoding='utf-8') as pos_file, \\\n",
    "         open(output_token_path, 'w', encoding='utf-8') as token_file, \\\n",
    "         open(output_pos_path, 'w', encoding='utf-8') as pos_file_out:\n",
    "        \n",
    "        for text_line, pos_line in zip(text_file, pos_file):\n",
    "            pos_tags = pos_line.strip().split()\n",
    "            preprocessed_words, adjusted_pos_tags = preprocess_text(text_line, pos_tags)\n",
    "            \n",
    "            tokenized_text = []\n",
    "            tokenized_tags = []\n",
    "            \n",
    "            for word, tag in zip(preprocessed_words, adjusted_pos_tags):\n",
    "                tokens = sp.EncodeAsPieces(word)\n",
    "                tokenized_text.extend(tokens)\n",
    "                tokenized_tags.extend([tag] * len(tokens))\n",
    "            \n",
    "            token_file.write(\" \".join(tokenized_text) + \"\\n\")\n",
    "            pos_file_out.write(\" \".join(tokenized_tags) + \"\\n\")\n",
    "\n",
    "\n",
    "input_text_path = \"TED2020.en-tr.tr-filtered.tr.train\"  \n",
    "input_pos_path = \"TED2020_pos_tags_train\"  \n",
    "output_token_path = \"TED2020_tokens_train\"  \n",
    "output_pos_path = \"TED2020_pos_tags_train.txt\"  \n",
    "model_path = \"tr_vocab.model\" \n",
    "\n",
    "tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3b6d19c-17d1-4d0a-b876-43288e872e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_path = \"TED2020.en-tr.tr-filtered.tr.dev\"  \n",
    "input_pos_path = \"TED2020_pos_tags_dev\"  \n",
    "output_token_path = \"TED2020_tokens_dev\"  \n",
    "output_pos_path = \"TED2020_pos_tags_dev.txt\"  \n",
    "model_path = \"tr_vocab.model\"  \n",
    "\n",
    "tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path)\n",
    "\n",
    "input_text_path = \"TED2020.en-tr.tr-filtered.tr.test\"  \n",
    "input_pos_path = \"TED2020_pos_tags_test\"  \n",
    "output_token_path = \"TED2020_tokens_test\"  \n",
    "output_pos_path = \"TED2020_pos_tags_test.txt\"  \n",
    "model_path = \"tr_vocab.model\"  \n",
    "\n",
    "tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ecfee-2417-459c-991f-fc311c23981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turkish side shared vocab subwording and subword units POS Tags\n",
    "input_text_path = \"TED2020.en-tr.tr-filtered.tr.train\"  \n",
    "input_pos_path = \"TED2020_pos_tags_train\"  \n",
    "output_token_path = \"TED2020_tokens_train_shared\"  \n",
    "output_pos_path = \"TED2020_pos_tags_train_shared.txt\"  \n",
    "model_path = \"kk_tr_shared_vocab.model\"  \n",
    "\n",
    "tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path)\n",
    "\n",
    "input_text_path = \"TED2020.en-tr.tr-filtered.tr.dev\" \n",
    "input_pos_path = \"TED2020_pos_tags_dev\"  \n",
    "output_token_path = \"TED2020_tokens_dev_shared\"  \n",
    "output_pos_path = \"TED2020_pos_tags_dev-shared.txt\"  \n",
    "model_path = \"kk_tr_shared_vocab.model\" \n",
    "\n",
    "tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path)\n",
    "\n",
    "input_text_path = \"TED2020.en-tr.tr-filtered.tr.test\"  \n",
    "input_pos_path = \"TED2020_pos_tags_test\"  \n",
    "output_token_path = \"TED2020_tokens_test_shared\"  \n",
    "output_pos_path = \"TED2020_pos_tags_test_shared.txt\"  \n",
    "model_path = \"kk_tr_shared_vocab.model\"  \n",
    "\n",
    "tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54900b24-9ce0-438f-b1c4-4c25ceaa1865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kazakh shared vocab subwording and subword units POS Tags\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "\n",
    "def preprocess_text(text, pos_tags):\n",
    "\n",
    "    text = re.sub(r\"([.,!?;:()\\\"'])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    new_pos_tags = []\n",
    "    i = 0  # pos_tags index'i\n",
    "    for word in words:\n",
    "        #if re.match(r\"[.,!?;:()\\\"']\", word):\n",
    "            #new_pos_tags.append('Punc')  \n",
    "            #pass\n",
    "        #else:\n",
    "            if i < len(pos_tags):\n",
    "                new_pos_tags.append(pos_tags[i])  \n",
    "                i += 1\n",
    "\n",
    "    return words, new_pos_tags\n",
    "\n",
    "def tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path):\n",
    "\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(model_path)\n",
    "    \n",
    "    with open(input_text_path, 'r', encoding='utf-8') as text_file, \\\n",
    "         open(input_pos_path, 'r', encoding='utf-8') as pos_file, \\\n",
    "         open(output_token_path, 'w', encoding='utf-8') as token_file, \\\n",
    "         open(output_pos_path, 'w', encoding='utf-8') as pos_file_out:\n",
    "        \n",
    "        for text_line, pos_line in zip(text_file, pos_file):\n",
    "            pos_tags = pos_line.strip().split()\n",
    "            preprocessed_words, adjusted_pos_tags = preprocess_text(text_line, pos_tags)\n",
    "            \n",
    "            tokenized_text = []\n",
    "            tokenized_tags = []\n",
    "            \n",
    "            for word, tag in zip(preprocessed_words, adjusted_pos_tags):\n",
    "                tokens = sp.EncodeAsPieces(word)\n",
    "                tokenized_text.extend(tokens)\n",
    "                tokenized_tags.extend([tag] * len(tokens))\n",
    "            \n",
    "            token_file.write(\" \".join(tokenized_text) + \"\\n\")\n",
    "            pos_file_out.write(\" \".join(tokenized_tags) + \"\\n\")\n",
    "\n",
    "\n",
    "# Parametreler\n",
    "input_text_path = \"kk_train_shuffled.txt-filtered.kk\" \n",
    "input_pos_path = \"RoBERTa_KK_POS_train\"  \n",
    "output_token_path = \"KK_tokens_train_shared\" \n",
    "output_pos_path = \"KK_pos_tags_train_shared.txt\"  \n",
    "model_path = \"kk_tr_shared_vocab.model\" \n",
    "\n",
    "tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53963021-c3c3-42cc-ad36-b90101cda41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_path = \"kk_valid_shuffled.txt-filtered.kk\" \n",
    "input_pos_path = \"RoBERTa_KK_POS_valid\" \n",
    "output_token_path = \"KK_tokens_valid_shared\"  \n",
    "output_pos_path = \"KK_pos_tags_valid_shared.txt\"  \n",
    "model_path = \"kk_tr_shared_vocab.model\"  \n",
    "\n",
    "tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path)\n",
    "\n",
    "input_text_path = \"kk_test_shuffled.txt-filtered.kk\"  \n",
    "input_pos_path = \"RoBERTa_KK_POS_test\"  \n",
    "output_token_path = \"KK_tokens_test_shared\"  \n",
    "output_pos_path = \"KK_pos_tags_test_shared.txt\" \n",
    "model_path = \"kk_tr_shared_vocab.model\" \n",
    "\n",
    "tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0e5c68d-b515-43b1-939d-32503258786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TED2020 English side subwording\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "\n",
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.Load(\"en_vocab.model\")\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    text = re.sub(r'([.,!?;:()])', r' \\1 ', text)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def tokenize_and_save(input_file, model, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f, open(output_file, 'w', encoding='utf-8') as fo:\n",
    "        for line in f:\n",
    "\n",
    "            preprocessed_line = preprocess(line.strip())\n",
    "            tokens = model.encode_as_pieces(preprocessed_line)\n",
    "            fo.write(\" \".join(tokens) + \"\\n\")\n",
    "\n",
    "tokenize_and_save(\"TED2020.en-tr.en-filtered.en.train\", sp_model, \"TED2020_train_target_tokens.txt\")\n",
    "tokenize_and_save(\"TED2020.en-tr.en-filtered.en.dev\", sp_model, \"TED2020_dev_target_tokens.txt\")\n",
    "tokenize_and_save(\"TED2020.en-tr.en-filtered.en.test\", sp_model, \"TED2020_test_target_tokens.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87d36226-54c1-4fd1-9d9e-71c28eba5872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TED2020 english side shared vocab subwording\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "\n",
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.Load(\"en_shared_vocab.model\")\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    text = re.sub(r'([.,!?;:()])', r' \\1 ', text)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def tokenize_and_save(input_file, model, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f, open(output_file, 'w', encoding='utf-8') as fo:\n",
    "        for line in f:\n",
    "\n",
    "            preprocessed_line = preprocess(line.strip())\n",
    "            tokens = model.encode_as_pieces(preprocessed_line)\n",
    "            fo.write(\" \".join(tokens) + \"\\n\")\n",
    "\n",
    "tokenize_and_save(\"TED2020.en-tr.en-filtered.en.train\", sp_model, \"TED2020_train_target_tokens_shared.txt\")\n",
    "tokenize_and_save(\"TED2020.en-tr.en-filtered.en.dev\", sp_model, \"TED2020_dev_target_tokens_shared.txt\")\n",
    "tokenize_and_save(\"TED2020.en-tr.en-filtered.en.test\", sp_model, \"TED2020_test_target_tokens_shared.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de390a3b-7d13-40f9-8ebe-71cfecb07f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kazakh corpus English side shared vocab subwording\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "\n",
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.Load(\"en_shared_vocab.model\")\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    text = re.sub(r'([.,!?;:()])', r' \\1 ', text)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def tokenize_and_save(input_file, model, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f, open(output_file, 'w', encoding='utf-8') as fo:\n",
    "        for line in f:\n",
    "\n",
    "            preprocessed_line = preprocess(line.strip())\n",
    "            tokens = model.encode_as_pieces(preprocessed_line)\n",
    "            fo.write(\" \".join(tokens) + \"\\n\")\n",
    "\n",
    "tokenize_and_save(\"en_train_shuffled.txt-filtered.en\", sp_model, \"KK_train_target_tokens_shared.txt\")\n",
    "tokenize_and_save(\"en_valid_shuffled.txt-filtered.en\", sp_model, \"KK_valid_target_tokens_shared.txt\")\n",
    "tokenize_and_save(\"en_test_shuffled.txt-filtered.en\", sp_model, \"KK_test_target_tokens_shared.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3765a643-c0e4-4ced-ac42-9bddd3b07fe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-09 11:33:09.892036: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-09 11:33:10.708480: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-09 11:33:10.708546: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-09 11:33:10.708554: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-09 11:33:11.698000: I onmt-main:8] Creating model directory POS_TR_KK_EN\n",
      "2024-12-09 11:33:11.953000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-09 11:33:11.953000: I main.py:315] Using model:\n",
      "(model): MultiFeaturesTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): ParallelInputter(\n",
      "      (inputters): ListWrapper(\n",
      "        (0): WordEmbedder()\n",
      "        (1): WordEmbedder()\n",
      "      )\n",
      "      (reducer): ConcatReducer()\n",
      "    )\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): ParallelInputter(\n",
      "        (inputters): ListWrapper(\n",
      "          (0): WordEmbedder()\n",
      "          (1): WordEmbedder()\n",
      "        )\n",
      "        (reducer): ConcatReducer()\n",
      "      )\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-09 11:33:11.956578: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-09 11:33:13.535700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-09 11:33:13.536368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7754 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-12-09 11:33:13.536879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 6099 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "2024-12-09 11:33:13.540000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file:\n",
      "  - TED2020_tokens_dev_shared\n",
      "  - TED2020_pos_tags_dev_shared.txt\n",
      "  eval_labels_file: TED2020_dev_target_tokens_shared.txt\n",
      "  source_1_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  source_2_vocabulary: Zemberek_TR_unique_pos\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file:\n",
      "  - TED2020_tokens_train_shared\n",
      "  - TED2020_pos_tags_train_shared.txt\n",
      "  train_labels_file: TED2020_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: POS_TR_KK_EN\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 205000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-09 11:33:13.876000: I inputter.py:316] Initialized source_1 input layer:\n",
      "2024-12-09 11:33:13.876000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-09 11:33:13.876000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-09 11:33:13.879000: I inputter.py:316] Initialized source_2 input layer:\n",
      "2024-12-09 11:33:13.879000: I inputter.py:316]  - vocabulary size: 21\n",
      "2024-12-09 11:33:13.879000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-09 11:33:13.954000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-09 11:33:13.954000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-09 11:33:13.954000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-09 11:33:13.959000: W runner.py:269] No checkpoint to restore in POS_TR_KK_EN\n",
      "2024-12-09 11:33:13.961000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "2024-12-09 11:33:14.012000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-09 11:33:15.051040: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-09 11:33:15.175000: I main.py:325] Accumulate gradients of 7 iterations to reach effective batch size of 25000\n",
      "2024-12-09 11:33:15.299000: I mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2024-12-09 11:33:15.488000: I dataset_ops.py:2542] Training on 337547 examples\n",
      "2024-12-09 11:34:23.306883: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-09 11:34:24.456551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-09 11:34:24.671429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-09 11:34:34.940000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-09 11:34:34.966000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-09 11:34:36.531000: I cross_device_ops.py:897] batch_all_reduce: 261 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-09 11:34:41.542000: I cross_device_ops.py:897] batch_all_reduce: 261 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-09 11:34:48.843000: I runner.py:310] Number of model parameters: 93357261\n",
      "2024-12-09 11:34:48.847000: I runner.py:310] Number of model weights: 261 (trainable = 261, non trainable = 0)\n",
      "2024-12-09 11:34:48.881000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-09 11:34:50.973000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-1\n",
      "2024-12-09 11:34:51.580000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-09 11:34:51.603000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-09 11:34:52.222000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-09 11:34:52.242000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-09 11:34:52.846000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-09 11:34:52.871000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-09 11:34:53.513000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-09 11:35:54.139000: I runner.py:310] Step = 100 ; steps/s = 1.57, tokens/s = 42870 (42870 target) ; Learning rate = 0.000009 ; Loss = 9.807713\n",
      "2024-12-09 11:36:56.430000: I runner.py:310] Step = 200 ; steps/s = 1.61, tokens/s = 43818 (43818 target) ; Learning rate = 0.000018 ; Loss = 8.915275\n",
      "2024-12-09 11:37:59.727000: I runner.py:310] Step = 300 ; steps/s = 1.58, tokens/s = 43110 (43110 target) ; Learning rate = 0.000027 ; Loss = 7.809954\n",
      "2024-12-09 11:39:03.450000: I runner.py:310] Step = 400 ; steps/s = 1.57, tokens/s = 42134 (42134 target) ; Learning rate = 0.000035 ; Loss = 7.224036\n",
      "2024-12-09 11:40:05.332000: I runner.py:310] Step = 500 ; steps/s = 1.62, tokens/s = 44106 (44106 target) ; Learning rate = 0.000044 ; Loss = 6.858863\n",
      "2024-12-09 11:41:07.154000: I runner.py:310] Step = 600 ; steps/s = 1.62, tokens/s = 44141 (44141 target) ; Learning rate = 0.000053 ; Loss = 6.387053\n",
      "2024-12-09 11:42:09.206000: I runner.py:310] Step = 700 ; steps/s = 1.61, tokens/s = 43251 (43251 target) ; Learning rate = 0.000062 ; Loss = 6.205907\n",
      "2024-12-09 11:43:11.097000: I runner.py:310] Step = 800 ; steps/s = 1.62, tokens/s = 44101 (44101 target) ; Learning rate = 0.000071 ; Loss = 6.045672\n",
      "2024-12-09 11:44:12.949000: I runner.py:310] Step = 900 ; steps/s = 1.62, tokens/s = 44131 (44131 target) ; Learning rate = 0.000080 ; Loss = 5.926955\n",
      "2024-12-09 11:45:14.795000: I runner.py:310] Step = 1000 ; steps/s = 1.62, tokens/s = 44128 (44128 target) ; Learning rate = 0.000088 ; Loss = 5.732455\n",
      "2024-12-09 11:46:16.176000: I runner.py:310] Step = 1100 ; steps/s = 1.63, tokens/s = 43718 (43718 target) ; Learning rate = 0.000097 ; Loss = 5.728528\n",
      "2024-12-09 11:47:17.975000: I runner.py:310] Step = 1200 ; steps/s = 1.62, tokens/s = 44164 (44164 target) ; Learning rate = 0.000106 ; Loss = 5.572056\n",
      "2024-12-09 11:48:19.732000: I runner.py:310] Step = 1300 ; steps/s = 1.62, tokens/s = 44198 (44198 target) ; Learning rate = 0.000115 ; Loss = 5.579928\n",
      "2024-12-09 11:49:21.143000: I runner.py:310] Step = 1400 ; steps/s = 1.63, tokens/s = 43705 (43705 target) ; Learning rate = 0.000124 ; Loss = 5.441969\n",
      "2024-12-09 11:50:22.999000: I runner.py:310] Step = 1500 ; steps/s = 1.62, tokens/s = 44106 (44106 target) ; Learning rate = 0.000133 ; Loss = 5.137526\n",
      "2024-12-09 11:51:24.816000: I runner.py:310] Step = 1600 ; steps/s = 1.62, tokens/s = 44141 (44141 target) ; Learning rate = 0.000142 ; Loss = 5.389106\n",
      "2024-12-09 11:52:26.617000: I runner.py:310] Step = 1700 ; steps/s = 1.62, tokens/s = 44170 (44170 target) ; Learning rate = 0.000150 ; Loss = 5.323173\n",
      "2024-12-09 11:53:27.963000: I runner.py:310] Step = 1800 ; steps/s = 1.63, tokens/s = 43782 (43782 target) ; Learning rate = 0.000159 ; Loss = 5.086191\n",
      "2024-12-09 11:54:29.714000: I runner.py:310] Step = 1900 ; steps/s = 1.62, tokens/s = 44190 (44190 target) ; Learning rate = 0.000168 ; Loss = 4.926764\n",
      "2024-12-09 11:55:31.466000: I runner.py:310] Step = 2000 ; steps/s = 1.62, tokens/s = 44207 (44207 target) ; Learning rate = 0.000177 ; Loss = 4.828796\n",
      "2024-12-09 11:56:32.945000: I runner.py:310] Step = 2100 ; steps/s = 1.63, tokens/s = 43643 (43643 target) ; Learning rate = 0.000186 ; Loss = 4.550288\n",
      "2024-12-09 11:57:34.725000: I runner.py:310] Step = 2200 ; steps/s = 1.62, tokens/s = 44183 (44183 target) ; Learning rate = 0.000195 ; Loss = 4.527370\n",
      "2024-12-09 11:58:36.571000: I runner.py:310] Step = 2300 ; steps/s = 1.62, tokens/s = 44124 (44124 target) ; Learning rate = 0.000203 ; Loss = 4.577092\n",
      "2024-12-09 11:59:38.405000: I runner.py:310] Step = 2400 ; steps/s = 1.62, tokens/s = 44137 (44137 target) ; Learning rate = 0.000212 ; Loss = 4.383667\n",
      "2024-12-09 12:00:39.858000: I runner.py:310] Step = 2500 ; steps/s = 1.63, tokens/s = 43677 (43677 target) ; Learning rate = 0.000221 ; Loss = 4.359657\n",
      "2024-12-09 12:01:41.624000: I runner.py:310] Step = 2600 ; steps/s = 1.62, tokens/s = 44171 (44171 target) ; Learning rate = 0.000230 ; Loss = 4.246735\n",
      "2024-12-09 12:02:43.469000: I runner.py:310] Step = 2700 ; steps/s = 1.62, tokens/s = 44123 (44123 target) ; Learning rate = 0.000239 ; Loss = 4.195299\n",
      "2024-12-09 12:03:44.830000: I runner.py:310] Step = 2800 ; steps/s = 1.63, tokens/s = 43759 (43759 target) ; Learning rate = 0.000248 ; Loss = 4.045034\n",
      "2024-12-09 12:04:46.614000: I runner.py:310] Step = 2900 ; steps/s = 1.62, tokens/s = 44168 (44168 target) ; Learning rate = 0.000256 ; Loss = 3.934724\n",
      "2024-12-09 12:05:48.387000: I runner.py:310] Step = 3000 ; steps/s = 1.62, tokens/s = 44183 (44183 target) ; Learning rate = 0.000265 ; Loss = 4.002577\n",
      "2024-12-09 12:06:50.206000: I runner.py:310] Step = 3100 ; steps/s = 1.62, tokens/s = 44155 (44155 target) ; Learning rate = 0.000274 ; Loss = 4.093733\n",
      "2024-12-09 12:07:51.678000: I runner.py:310] Step = 3200 ; steps/s = 1.63, tokens/s = 43659 (43659 target) ; Learning rate = 0.000283 ; Loss = 4.012116\n",
      "2024-12-09 12:08:53.481000: I runner.py:310] Step = 3300 ; steps/s = 1.62, tokens/s = 44160 (44160 target) ; Learning rate = 0.000292 ; Loss = 3.696151\n",
      "2024-12-09 12:09:55.328000: I runner.py:310] Step = 3400 ; steps/s = 1.62, tokens/s = 44118 (44118 target) ; Learning rate = 0.000301 ; Loss = 3.722743\n",
      "2024-12-09 12:10:56.667000: I runner.py:310] Step = 3500 ; steps/s = 1.63, tokens/s = 43766 (43766 target) ; Learning rate = 0.000309 ; Loss = 3.718790\n",
      "2024-12-09 12:11:58.458000: I runner.py:310] Step = 3600 ; steps/s = 1.62, tokens/s = 44188 (44188 target) ; Learning rate = 0.000318 ; Loss = 3.593822\n",
      "2024-12-09 12:13:00.200000: I runner.py:310] Step = 3700 ; steps/s = 1.62, tokens/s = 44199 (44199 target) ; Learning rate = 0.000327 ; Loss = 3.664240\n",
      "2024-12-09 12:14:01.991000: I runner.py:310] Step = 3800 ; steps/s = 1.62, tokens/s = 44153 (44153 target) ; Learning rate = 0.000336 ; Loss = 3.637957\n",
      "2024-12-09 12:15:03.339000: I runner.py:310] Step = 3900 ; steps/s = 1.63, tokens/s = 43760 (43760 target) ; Learning rate = 0.000345 ; Loss = 3.488984\n",
      "2024-12-09 12:16:05.131000: I runner.py:310] Step = 4000 ; steps/s = 1.62, tokens/s = 44179 (44179 target) ; Learning rate = 0.000354 ; Loss = 3.481750\n",
      "2024-12-09 12:17:06.962000: I runner.py:310] Step = 4100 ; steps/s = 1.62, tokens/s = 44135 (44135 target) ; Learning rate = 0.000362 ; Loss = 3.450576\n",
      "2024-12-09 12:18:08.386000: I runner.py:310] Step = 4200 ; steps/s = 1.63, tokens/s = 43693 (43693 target) ; Learning rate = 0.000371 ; Loss = 3.360767\n",
      "2024-12-09 12:19:10.198000: I runner.py:310] Step = 4300 ; steps/s = 1.62, tokens/s = 44147 (44147 target) ; Learning rate = 0.000380 ; Loss = 3.374210\n",
      "2024-12-09 12:20:12.044000: I runner.py:310] Step = 4400 ; steps/s = 1.62, tokens/s = 44127 (44127 target) ; Learning rate = 0.000389 ; Loss = 3.378187\n",
      "2024-12-09 12:21:13.829000: I runner.py:310] Step = 4500 ; steps/s = 1.62, tokens/s = 44170 (44170 target) ; Learning rate = 0.000398 ; Loss = 3.363148\n",
      "2024-12-09 12:22:15.258000: I runner.py:310] Step = 4600 ; steps/s = 1.63, tokens/s = 43714 (43714 target) ; Learning rate = 0.000407 ; Loss = 3.221396\n",
      "2024-12-09 12:23:17.110000: I runner.py:310] Step = 4700 ; steps/s = 1.62, tokens/s = 44118 (44118 target) ; Learning rate = 0.000416 ; Loss = 3.288132\n",
      "2024-12-09 12:24:18.982000: I runner.py:310] Step = 4800 ; steps/s = 1.62, tokens/s = 44105 (44105 target) ; Learning rate = 0.000424 ; Loss = 3.340054\n",
      "2024-12-09 12:25:20.357000: I runner.py:310] Step = 4900 ; steps/s = 1.63, tokens/s = 43717 (43717 target) ; Learning rate = 0.000433 ; Loss = 3.196960\n",
      "2024-12-09 12:26:22.141000: I runner.py:310] Step = 5000 ; steps/s = 1.62, tokens/s = 44168 (44168 target) ; Learning rate = 0.000442 ; Loss = 3.182109\n",
      "2024-12-09 12:26:22.142000: I training.py:192] Running evaluation for step 5000\n",
      "2024-12-09 12:41:38.820000: I training.py:192] Evaluation result for step 5000: loss = 2.256140 ; perplexity = 9.546174\n",
      "2024-12-09 12:42:40.640000: I runner.py:310] Step = 5100 ; steps/s = 1.62, tokens/s = 44160 (44160 target) ; Learning rate = 0.000451 ; Loss = 3.174718\n",
      "2024-12-09 12:43:42.563000: I runner.py:310] Step = 5200 ; steps/s = 1.62, tokens/s = 44082 (44082 target) ; Learning rate = 0.000460 ; Loss = 3.227278\n",
      "2024-12-09 12:44:44.029000: I runner.py:310] Step = 5300 ; steps/s = 1.63, tokens/s = 43665 (43665 target) ; Learning rate = 0.000469 ; Loss = 3.182686\n",
      "2024-12-09 12:45:45.960000: I runner.py:310] Step = 5400 ; steps/s = 1.61, tokens/s = 44067 (44067 target) ; Learning rate = 0.000477 ; Loss = 3.113719\n",
      "2024-12-09 12:46:47.872000: I runner.py:310] Step = 5500 ; steps/s = 1.62, tokens/s = 44091 (44091 target) ; Learning rate = 0.000486 ; Loss = 3.138904\n",
      "2024-12-09 12:47:49.386000: I runner.py:310] Step = 5600 ; steps/s = 1.63, tokens/s = 43630 (43630 target) ; Learning rate = 0.000495 ; Loss = 2.987791\n",
      "2024-12-09 12:48:51.242000: I runner.py:310] Step = 5700 ; steps/s = 1.62, tokens/s = 44125 (44125 target) ; Learning rate = 0.000504 ; Loss = 2.980919\n",
      "2024-12-09 12:49:53.105000: I runner.py:310] Step = 5800 ; steps/s = 1.62, tokens/s = 44126 (44126 target) ; Learning rate = 0.000513 ; Loss = 3.172151\n",
      "2024-12-09 12:50:55.010000: I runner.py:310] Step = 5900 ; steps/s = 1.62, tokens/s = 44080 (44080 target) ; Learning rate = 0.000522 ; Loss = 3.107012\n",
      "2024-12-09 12:51:56.469000: I runner.py:310] Step = 6000 ; steps/s = 1.63, tokens/s = 43686 (43686 target) ; Learning rate = 0.000530 ; Loss = 3.024561\n",
      "2024-12-09 12:52:58.398000: I runner.py:310] Step = 6100 ; steps/s = 1.61, tokens/s = 44072 (44072 target) ; Learning rate = 0.000539 ; Loss = 3.007500\n",
      "2024-12-09 12:54:00.266000: I runner.py:310] Step = 6200 ; steps/s = 1.62, tokens/s = 44089 (44089 target) ; Learning rate = 0.000548 ; Loss = 3.069292\n",
      "2024-12-09 12:55:01.766000: I runner.py:310] Step = 6300 ; steps/s = 1.63, tokens/s = 43646 (43646 target) ; Learning rate = 0.000557 ; Loss = 3.031541\n",
      "2024-12-09 12:56:03.703000: I runner.py:310] Step = 6400 ; steps/s = 1.61, tokens/s = 44066 (44066 target) ; Learning rate = 0.000566 ; Loss = 2.899116\n",
      "2024-12-09 12:57:05.632000: I runner.py:310] Step = 6500 ; steps/s = 1.61, tokens/s = 44066 (44066 target) ; Learning rate = 0.000575 ; Loss = 2.907730\n",
      "2024-12-09 12:58:07.452000: I runner.py:310] Step = 6600 ; steps/s = 1.62, tokens/s = 44151 (44151 target) ; Learning rate = 0.000583 ; Loss = 2.925266\n",
      "2024-12-09 12:59:08.905000: I runner.py:310] Step = 6700 ; steps/s = 1.63, tokens/s = 43687 (43687 target) ; Learning rate = 0.000592 ; Loss = 2.771714\n",
      "2024-12-09 13:00:10.772000: I runner.py:310] Step = 6800 ; steps/s = 1.62, tokens/s = 44110 (44110 target) ; Learning rate = 0.000601 ; Loss = 2.866999\n",
      "2024-12-09 13:01:12.606000: I runner.py:310] Step = 6900 ; steps/s = 1.62, tokens/s = 44133 (44133 target) ; Learning rate = 0.000610 ; Loss = 2.987805\n",
      "2024-12-09 13:02:14.075000: I runner.py:310] Step = 7000 ; steps/s = 1.63, tokens/s = 43659 (43659 target) ; Learning rate = 0.000619 ; Loss = 2.811748\n",
      "2024-12-09 13:03:15.860000: I runner.py:310] Step = 7100 ; steps/s = 1.62, tokens/s = 44162 (44162 target) ; Learning rate = 0.000628 ; Loss = 2.842433\n",
      "2024-12-09 13:04:17.669000: I runner.py:310] Step = 7200 ; steps/s = 1.62, tokens/s = 44165 (44165 target) ; Learning rate = 0.000636 ; Loss = 2.899688\n",
      "2024-12-09 13:05:19.471000: I runner.py:310] Step = 7300 ; steps/s = 1.62, tokens/s = 44165 (44165 target) ; Learning rate = 0.000645 ; Loss = 2.848140\n",
      "2024-12-09 13:06:20.836000: I runner.py:310] Step = 7400 ; steps/s = 1.63, tokens/s = 43741 (43741 target) ; Learning rate = 0.000654 ; Loss = 2.811044\n",
      "2024-12-09 13:07:22.632000: I runner.py:310] Step = 7500 ; steps/s = 1.62, tokens/s = 44164 (44164 target) ; Learning rate = 0.000663 ; Loss = 2.772520\n",
      "2024-12-09 13:08:24.464000: I runner.py:310] Step = 7600 ; steps/s = 1.62, tokens/s = 44144 (44144 target) ; Learning rate = 0.000672 ; Loss = 2.822914\n",
      "2024-12-09 13:09:25.878000: I runner.py:310] Step = 7700 ; steps/s = 1.63, tokens/s = 43703 (43703 target) ; Learning rate = 0.000681 ; Loss = 2.753089\n",
      "2024-12-09 13:10:27.691000: I runner.py:310] Step = 7800 ; steps/s = 1.62, tokens/s = 44143 (44143 target) ; Learning rate = 0.000690 ; Loss = 2.815480\n",
      "2024-12-09 13:11:29.541000: I runner.py:310] Step = 7900 ; steps/s = 1.62, tokens/s = 44126 (44126 target) ; Learning rate = 0.000698 ; Loss = 2.807205\n",
      "2024-12-09 13:12:31.200000: I runner.py:310] Step = 8000 ; steps/s = 1.62, tokens/s = 44026 (44026 target) ; Learning rate = 0.000707 ; Loss = 2.726571\n",
      "2024-12-09 13:13:32.772000: I runner.py:310] Step = 8100 ; steps/s = 1.62, tokens/s = 43844 (43844 target) ; Learning rate = 0.000716 ; Loss = 2.763057\n",
      "2024-12-09 13:14:34.574000: I runner.py:310] Step = 8200 ; steps/s = 1.62, tokens/s = 44156 (44156 target) ; Learning rate = 0.000725 ; Loss = 2.778349\n",
      "2024-12-09 13:15:36.359000: I runner.py:310] Step = 8300 ; steps/s = 1.62, tokens/s = 44177 (44177 target) ; Learning rate = 0.000734 ; Loss = 2.746102\n",
      "2024-12-09 13:16:37.852000: I runner.py:310] Step = 8400 ; steps/s = 1.63, tokens/s = 43647 (43647 target) ; Learning rate = 0.000743 ; Loss = 2.717077\n",
      "2024-12-09 13:17:39.686000: I runner.py:310] Step = 8500 ; steps/s = 1.62, tokens/s = 44129 (44129 target) ; Learning rate = 0.000751 ; Loss = 2.714539\n",
      "2024-12-09 13:18:41.462000: I runner.py:310] Step = 8600 ; steps/s = 1.62, tokens/s = 44174 (44174 target) ; Learning rate = 0.000760 ; Loss = 2.774159\n",
      "2024-12-09 13:19:42.888000: I runner.py:310] Step = 8700 ; steps/s = 1.63, tokens/s = 43703 (43703 target) ; Learning rate = 0.000769 ; Loss = 2.606918\n",
      "2024-12-09 13:20:44.724000: I runner.py:310] Step = 8800 ; steps/s = 1.62, tokens/s = 44123 (44123 target) ; Learning rate = 0.000778 ; Loss = 2.650423\n",
      "2024-12-09 13:21:46.522000: I runner.py:310] Step = 8900 ; steps/s = 1.62, tokens/s = 44166 (44166 target) ; Learning rate = 0.000787 ; Loss = 2.701759\n",
      "2024-12-09 13:22:48.287000: I runner.py:310] Step = 9000 ; steps/s = 1.62, tokens/s = 44188 (44188 target) ; Learning rate = 0.000796 ; Loss = 2.713646\n",
      "2024-12-09 13:23:49.702000: I runner.py:310] Step = 9100 ; steps/s = 1.63, tokens/s = 43699 (43699 target) ; Learning rate = 0.000804 ; Loss = 2.696230\n",
      "2024-12-09 13:24:51.556000: I runner.py:310] Step = 9200 ; steps/s = 1.62, tokens/s = 44142 (44142 target) ; Learning rate = 0.000813 ; Loss = 2.667937\n",
      "2024-12-09 13:25:53.370000: I runner.py:310] Step = 9300 ; steps/s = 1.62, tokens/s = 44151 (44151 target) ; Learning rate = 0.000822 ; Loss = 2.660298\n",
      "2024-12-09 13:26:54.810000: I runner.py:310] Step = 9400 ; steps/s = 1.63, tokens/s = 43682 (43682 target) ; Learning rate = 0.000831 ; Loss = 2.649694\n",
      "2024-12-09 13:27:56.627000: I runner.py:310] Step = 9500 ; steps/s = 1.62, tokens/s = 44160 (44160 target) ; Learning rate = 0.000840 ; Loss = 2.552242\n",
      "2024-12-09 13:28:58.416000: I runner.py:310] Step = 9600 ; steps/s = 1.62, tokens/s = 44165 (44165 target) ; Learning rate = 0.000849 ; Loss = 2.654581\n",
      "2024-12-09 13:30:00.214000: I runner.py:310] Step = 9700 ; steps/s = 1.62, tokens/s = 44168 (44168 target) ; Learning rate = 0.000857 ; Loss = 2.705767\n",
      "2024-12-09 13:31:01.624000: I runner.py:310] Step = 9800 ; steps/s = 1.63, tokens/s = 43700 (43700 target) ; Learning rate = 0.000866 ; Loss = 2.670513\n",
      "2024-12-09 13:32:03.486000: I runner.py:310] Step = 9900 ; steps/s = 1.62, tokens/s = 44124 (44124 target) ; Learning rate = 0.000875 ; Loss = 2.642520\n",
      "2024-12-09 13:33:05.227000: I runner.py:310] Step = 10000 ; steps/s = 1.62, tokens/s = 44189 (44189 target) ; Learning rate = 0.000884 ; Loss = 2.634522\n",
      "2024-12-09 13:33:07.058000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-10000\n",
      "2024-12-09 13:33:07.059000: I training.py:192] Running evaluation for step 10000\n",
      "2024-12-09 13:39:19.190000: I training.py:192] Evaluation result for step 10000: loss = 2.078563 ; perplexity = 7.992975\n",
      "2024-12-09 13:40:20.509000: I runner.py:310] Step = 10100 ; steps/s = 1.63, tokens/s = 43791 (43791 target) ; Learning rate = 0.000879 ; Loss = 2.496194\n",
      "2024-12-09 13:41:22.265000: I runner.py:310] Step = 10200 ; steps/s = 1.62, tokens/s = 44192 (44192 target) ; Learning rate = 0.000875 ; Loss = 2.626415\n",
      "2024-12-09 13:42:24.013000: I runner.py:310] Step = 10300 ; steps/s = 1.62, tokens/s = 44190 (44190 target) ; Learning rate = 0.000871 ; Loss = 2.603986\n",
      "2024-12-09 13:43:25.795000: I runner.py:310] Step = 10400 ; steps/s = 1.62, tokens/s = 44183 (44183 target) ; Learning rate = 0.000867 ; Loss = 2.585873\n",
      "2024-12-09 13:44:27.113000: I runner.py:310] Step = 10500 ; steps/s = 1.63, tokens/s = 43785 (43785 target) ; Learning rate = 0.000863 ; Loss = 2.607111\n",
      "2024-12-09 13:45:28.886000: I runner.py:310] Step = 10600 ; steps/s = 1.62, tokens/s = 44183 (44183 target) ; Learning rate = 0.000858 ; Loss = 2.525384\n",
      "2024-12-09 13:46:30.664000: I runner.py:310] Step = 10700 ; steps/s = 1.62, tokens/s = 44176 (44176 target) ; Learning rate = 0.000854 ; Loss = 2.539510\n",
      "2024-12-09 13:47:32.086000: I runner.py:310] Step = 10800 ; steps/s = 1.63, tokens/s = 43687 (43687 target) ; Learning rate = 0.000850 ; Loss = 2.561581\n",
      "2024-12-09 13:48:33.793000: I runner.py:310] Step = 10900 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000847 ; Loss = 2.526081\n",
      "2024-12-09 13:49:35.521000: I runner.py:310] Step = 11000 ; steps/s = 1.62, tokens/s = 44226 (44226 target) ; Learning rate = 0.000843 ; Loss = 2.581737\n",
      "2024-12-09 13:50:37.314000: I runner.py:310] Step = 11100 ; steps/s = 1.62, tokens/s = 44150 (44150 target) ; Learning rate = 0.000839 ; Loss = 2.554718\n",
      "2024-12-09 13:51:38.619000: I runner.py:310] Step = 11200 ; steps/s = 1.63, tokens/s = 43785 (43785 target) ; Learning rate = 0.000835 ; Loss = 2.391460\n",
      "2024-12-09 13:52:40.389000: I runner.py:310] Step = 11300 ; steps/s = 1.62, tokens/s = 44165 (44165 target) ; Learning rate = 0.000831 ; Loss = 2.503410\n",
      "2024-12-09 13:53:42.131000: I runner.py:310] Step = 11400 ; steps/s = 1.62, tokens/s = 44214 (44214 target) ; Learning rate = 0.000828 ; Loss = 2.567124\n",
      "2024-12-09 13:54:43.480000: I runner.py:310] Step = 11500 ; steps/s = 1.63, tokens/s = 43753 (43753 target) ; Learning rate = 0.000824 ; Loss = 2.426002\n",
      "2024-12-09 13:55:45.265000: I runner.py:310] Step = 11600 ; steps/s = 1.62, tokens/s = 44178 (44178 target) ; Learning rate = 0.000821 ; Loss = 2.431759\n",
      "2024-12-09 13:56:47.051000: I runner.py:310] Step = 11700 ; steps/s = 1.62, tokens/s = 44173 (44173 target) ; Learning rate = 0.000817 ; Loss = 2.499302\n",
      "2024-12-09 13:57:48.812000: I runner.py:310] Step = 11800 ; steps/s = 1.62, tokens/s = 44189 (44189 target) ; Learning rate = 0.000814 ; Loss = 2.504930\n",
      "2024-12-09 13:58:50.269000: I runner.py:310] Step = 11900 ; steps/s = 1.63, tokens/s = 43663 (43663 target) ; Learning rate = 0.000810 ; Loss = 2.449954\n",
      "2024-12-09 13:59:52.019000: I runner.py:310] Step = 12000 ; steps/s = 1.62, tokens/s = 44195 (44195 target) ; Learning rate = 0.000807 ; Loss = 2.466184\n",
      "2024-12-09 14:00:53.805000: I runner.py:310] Step = 12100 ; steps/s = 1.62, tokens/s = 44181 (44181 target) ; Learning rate = 0.000803 ; Loss = 2.494639\n",
      "2024-12-09 14:01:55.161000: I runner.py:310] Step = 12200 ; steps/s = 1.63, tokens/s = 43755 (43755 target) ; Learning rate = 0.000800 ; Loss = 2.396766\n",
      "2024-12-09 14:02:56.940000: I runner.py:310] Step = 12300 ; steps/s = 1.62, tokens/s = 44182 (44182 target) ; Learning rate = 0.000797 ; Loss = 2.408058\n",
      "2024-12-09 14:03:58.703000: I runner.py:310] Step = 12400 ; steps/s = 1.62, tokens/s = 44187 (44187 target) ; Learning rate = 0.000794 ; Loss = 2.438552\n",
      "2024-12-09 14:05:00.428000: I runner.py:310] Step = 12500 ; steps/s = 1.62, tokens/s = 44221 (44221 target) ; Learning rate = 0.000791 ; Loss = 2.432332\n",
      "2024-12-09 14:06:01.856000: I runner.py:310] Step = 12600 ; steps/s = 1.63, tokens/s = 43679 (43679 target) ; Learning rate = 0.000787 ; Loss = 2.461765\n",
      "2024-12-09 14:07:03.600000: I runner.py:310] Step = 12700 ; steps/s = 1.62, tokens/s = 44203 (44203 target) ; Learning rate = 0.000784 ; Loss = 2.415810\n",
      "2024-12-09 14:08:05.335000: I runner.py:310] Step = 12800 ; steps/s = 1.62, tokens/s = 44214 (44214 target) ; Learning rate = 0.000781 ; Loss = 2.465659\n",
      "2024-12-09 14:09:06.736000: I runner.py:310] Step = 12900 ; steps/s = 1.63, tokens/s = 43731 (43731 target) ; Learning rate = 0.000778 ; Loss = 2.405328\n",
      "2024-12-09 14:10:08.506000: I runner.py:310] Step = 13000 ; steps/s = 1.62, tokens/s = 44173 (44173 target) ; Learning rate = 0.000775 ; Loss = 2.398575\n",
      "2024-12-09 14:11:10.270000: I runner.py:310] Step = 13100 ; steps/s = 1.62, tokens/s = 44174 (44174 target) ; Learning rate = 0.000772 ; Loss = 2.375627\n",
      "2024-12-09 14:12:12.030000: I runner.py:310] Step = 13200 ; steps/s = 1.62, tokens/s = 44201 (44201 target) ; Learning rate = 0.000769 ; Loss = 2.372827\n",
      "2024-12-09 14:13:13.381000: I runner.py:310] Step = 13300 ; steps/s = 1.63, tokens/s = 43748 (43748 target) ; Learning rate = 0.000766 ; Loss = 2.401616\n",
      "2024-12-09 14:14:15.165000: I runner.py:310] Step = 13400 ; steps/s = 1.62, tokens/s = 44171 (44171 target) ; Learning rate = 0.000764 ; Loss = 2.356120\n",
      "2024-12-09 14:15:16.972000: I runner.py:310] Step = 13500 ; steps/s = 1.62, tokens/s = 44165 (44165 target) ; Learning rate = 0.000761 ; Loss = 2.359995\n",
      "2024-12-09 14:16:18.365000: I runner.py:310] Step = 13600 ; steps/s = 1.63, tokens/s = 43717 (43717 target) ; Learning rate = 0.000758 ; Loss = 2.306913\n",
      "2024-12-09 14:17:20.117000: I runner.py:310] Step = 13700 ; steps/s = 1.62, tokens/s = 44195 (44195 target) ; Learning rate = 0.000755 ; Loss = 2.384619\n",
      "2024-12-09 14:18:21.875000: I runner.py:310] Step = 13800 ; steps/s = 1.62, tokens/s = 44180 (44180 target) ; Learning rate = 0.000752 ; Loss = 2.383643\n",
      "2024-12-09 14:19:23.706000: I runner.py:310] Step = 13900 ; steps/s = 1.62, tokens/s = 44148 (44148 target) ; Learning rate = 0.000750 ; Loss = 2.412788\n",
      "2024-12-09 14:20:25.016000: I runner.py:310] Step = 14000 ; steps/s = 1.63, tokens/s = 43774 (43774 target) ; Learning rate = 0.000747 ; Loss = 2.365398\n",
      "2024-12-09 14:21:26.727000: I runner.py:310] Step = 14100 ; steps/s = 1.62, tokens/s = 44237 (44237 target) ; Learning rate = 0.000744 ; Loss = 2.357064\n",
      "2024-12-09 14:22:28.521000: I runner.py:310] Step = 14200 ; steps/s = 1.62, tokens/s = 44149 (44149 target) ; Learning rate = 0.000742 ; Loss = 2.380390\n",
      "2024-12-09 14:23:29.932000: I runner.py:310] Step = 14300 ; steps/s = 1.63, tokens/s = 43717 (43717 target) ; Learning rate = 0.000739 ; Loss = 2.259927\n",
      "2024-12-09 14:24:31.685000: I runner.py:310] Step = 14400 ; steps/s = 1.62, tokens/s = 44193 (44193 target) ; Learning rate = 0.000737 ; Loss = 2.348605\n",
      "2024-12-09 14:25:33.484000: I runner.py:310] Step = 14500 ; steps/s = 1.62, tokens/s = 44171 (44171 target) ; Learning rate = 0.000734 ; Loss = 2.329656\n",
      "2024-12-09 14:26:35.246000: I runner.py:310] Step = 14600 ; steps/s = 1.62, tokens/s = 44180 (44180 target) ; Learning rate = 0.000731 ; Loss = 2.376764\n",
      "2024-12-09 14:27:36.572000: I runner.py:310] Step = 14700 ; steps/s = 1.63, tokens/s = 43778 (43778 target) ; Learning rate = 0.000729 ; Loss = 2.295537\n",
      "2024-12-09 14:28:38.347000: I runner.py:310] Step = 14800 ; steps/s = 1.62, tokens/s = 44170 (44170 target) ; Learning rate = 0.000727 ; Loss = 2.363048\n",
      "2024-12-09 14:29:40.058000: I runner.py:310] Step = 14900 ; steps/s = 1.62, tokens/s = 44226 (44226 target) ; Learning rate = 0.000724 ; Loss = 2.392173\n",
      "2024-12-09 14:30:41.406000: I runner.py:310] Step = 15000 ; steps/s = 1.63, tokens/s = 43752 (43752 target) ; Learning rate = 0.000722 ; Loss = 2.308096\n",
      "2024-12-09 14:30:41.408000: I training.py:192] Running evaluation for step 15000\n",
      "2024-12-09 14:35:10.854000: I training.py:192] Evaluation result for step 15000: loss = 2.115721 ; perplexity = 8.295561\n",
      "2024-12-09 14:36:12.531000: I runner.py:310] Step = 15100 ; steps/s = 1.62, tokens/s = 44253 (44253 target) ; Learning rate = 0.000719 ; Loss = 2.251471\n",
      "2024-12-09 14:37:14.270000: I runner.py:310] Step = 15200 ; steps/s = 1.62, tokens/s = 44218 (44218 target) ; Learning rate = 0.000717 ; Loss = 2.303902\n",
      "2024-12-09 14:38:16.107000: I runner.py:310] Step = 15300 ; steps/s = 1.62, tokens/s = 44123 (44123 target) ; Learning rate = 0.000715 ; Loss = 2.302400\n",
      "2024-12-09 14:39:17.461000: I runner.py:310] Step = 15400 ; steps/s = 1.63, tokens/s = 43746 (43746 target) ; Learning rate = 0.000712 ; Loss = 2.291439\n",
      "2024-12-09 14:40:19.232000: I runner.py:310] Step = 15500 ; steps/s = 1.62, tokens/s = 44190 (44190 target) ; Learning rate = 0.000710 ; Loss = 2.309084\n",
      "2024-12-09 14:41:21.009000: I runner.py:310] Step = 15600 ; steps/s = 1.62, tokens/s = 44178 (44178 target) ; Learning rate = 0.000708 ; Loss = 2.278989\n",
      "2024-12-09 14:42:22.421000: I runner.py:310] Step = 15700 ; steps/s = 1.63, tokens/s = 43718 (43718 target) ; Learning rate = 0.000705 ; Loss = 2.235819\n",
      "2024-12-09 14:43:24.193000: I runner.py:310] Step = 15800 ; steps/s = 1.62, tokens/s = 44183 (44183 target) ; Learning rate = 0.000703 ; Loss = 2.288072\n",
      "2024-12-09 14:44:25.926000: I runner.py:310] Step = 15900 ; steps/s = 1.62, tokens/s = 44214 (44214 target) ; Learning rate = 0.000701 ; Loss = 2.313583\n",
      "2024-12-09 14:45:27.667000: I runner.py:310] Step = 16000 ; steps/s = 1.62, tokens/s = 44187 (44187 target) ; Learning rate = 0.000699 ; Loss = 2.330814\n",
      "2024-12-09 14:46:29.065000: I runner.py:310] Step = 16100 ; steps/s = 1.63, tokens/s = 43729 (43729 target) ; Learning rate = 0.000697 ; Loss = 2.210621\n",
      "2024-12-09 14:47:30.808000: I runner.py:310] Step = 16200 ; steps/s = 1.62, tokens/s = 44195 (44195 target) ; Learning rate = 0.000694 ; Loss = 2.303855\n",
      "2024-12-09 14:48:32.594000: I runner.py:310] Step = 16300 ; steps/s = 1.62, tokens/s = 44176 (44176 target) ; Learning rate = 0.000692 ; Loss = 2.295508\n",
      "2024-12-09 14:49:34.003000: I runner.py:310] Step = 16400 ; steps/s = 1.63, tokens/s = 43706 (43706 target) ; Learning rate = 0.000690 ; Loss = 2.273834\n",
      "2024-12-09 14:50:35.731000: I runner.py:310] Step = 16500 ; steps/s = 1.62, tokens/s = 44218 (44218 target) ; Learning rate = 0.000688 ; Loss = 2.215700\n",
      "2024-12-09 14:51:37.512000: I runner.py:310] Step = 16600 ; steps/s = 1.62, tokens/s = 44161 (44161 target) ; Learning rate = 0.000686 ; Loss = 2.261579\n",
      "2024-12-09 14:52:39.032000: I runner.py:310] Step = 16700 ; steps/s = 1.63, tokens/s = 43757 (43757 target) ; Learning rate = 0.000684 ; Loss = 2.399684\n",
      "2024-12-09 14:53:40.701000: I runner.py:310] Step = 16800 ; steps/s = 1.62, tokens/s = 44113 (44113 target) ; Learning rate = 0.000682 ; Loss = 2.288342\n",
      "2024-12-09 14:54:42.477000: I runner.py:310] Step = 16900 ; steps/s = 1.62, tokens/s = 44178 (44178 target) ; Learning rate = 0.000680 ; Loss = 2.221930\n",
      "2024-12-09 14:55:44.242000: I runner.py:310] Step = 17000 ; steps/s = 1.62, tokens/s = 44199 (44199 target) ; Learning rate = 0.000678 ; Loss = 2.257073\n",
      "2024-12-09 14:56:45.668000: I runner.py:310] Step = 17100 ; steps/s = 1.63, tokens/s = 43708 (43708 target) ; Learning rate = 0.000676 ; Loss = 2.219548\n",
      "2024-12-09 14:57:47.423000: I runner.py:310] Step = 17200 ; steps/s = 1.62, tokens/s = 44186 (44186 target) ; Learning rate = 0.000674 ; Loss = 2.213607\n",
      "2024-12-09 14:58:49.301000: I runner.py:310] Step = 17300 ; steps/s = 1.62, tokens/s = 44105 (44105 target) ; Learning rate = 0.000672 ; Loss = 2.233356\n",
      "2024-12-09 14:59:50.741000: I runner.py:310] Step = 17400 ; steps/s = 1.63, tokens/s = 43689 (43689 target) ; Learning rate = 0.000670 ; Loss = 2.185614\n",
      "2024-12-09 15:00:52.420000: I runner.py:310] Step = 17500 ; steps/s = 1.62, tokens/s = 44245 (44245 target) ; Learning rate = 0.000668 ; Loss = 2.240517\n",
      "2024-12-09 15:01:54.140000: I runner.py:310] Step = 17600 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000666 ; Loss = 2.210277\n",
      "2024-12-09 15:02:55.999000: I runner.py:310] Step = 17700 ; steps/s = 1.62, tokens/s = 44136 (44136 target) ; Learning rate = 0.000664 ; Loss = 2.249487\n",
      "2024-12-09 15:03:57.419000: I runner.py:310] Step = 17800 ; steps/s = 1.63, tokens/s = 43691 (43691 target) ; Learning rate = 0.000662 ; Loss = 2.144859\n",
      "2024-12-09 15:04:59.189000: I runner.py:310] Step = 17900 ; steps/s = 1.62, tokens/s = 44184 (44184 target) ; Learning rate = 0.000661 ; Loss = 2.233632\n",
      "2024-12-09 15:06:00.958000: I runner.py:310] Step = 18000 ; steps/s = 1.62, tokens/s = 44191 (44191 target) ; Learning rate = 0.000659 ; Loss = 2.229280\n",
      "2024-12-09 15:07:02.366000: I runner.py:310] Step = 18100 ; steps/s = 1.63, tokens/s = 43708 (43708 target) ; Learning rate = 0.000657 ; Loss = 2.203732\n",
      "2024-12-09 15:08:04.133000: I runner.py:310] Step = 18200 ; steps/s = 1.62, tokens/s = 44189 (44189 target) ; Learning rate = 0.000655 ; Loss = 2.173922\n",
      "2024-12-09 15:09:05.986000: I runner.py:310] Step = 18300 ; steps/s = 1.62, tokens/s = 44124 (44124 target) ; Learning rate = 0.000653 ; Loss = 2.223617\n",
      "2024-12-09 15:10:07.788000: I runner.py:310] Step = 18400 ; steps/s = 1.62, tokens/s = 44155 (44155 target) ; Learning rate = 0.000652 ; Loss = 2.248141\n",
      "2024-12-09 15:11:09.115000: I runner.py:310] Step = 18500 ; steps/s = 1.63, tokens/s = 43774 (43774 target) ; Learning rate = 0.000650 ; Loss = 2.122304\n",
      "2024-12-09 15:12:10.908000: I runner.py:310] Step = 18600 ; steps/s = 1.62, tokens/s = 44162 (44162 target) ; Learning rate = 0.000648 ; Loss = 2.211153\n",
      "2024-12-09 15:13:12.657000: I runner.py:310] Step = 18700 ; steps/s = 1.62, tokens/s = 44206 (44206 target) ; Learning rate = 0.000646 ; Loss = 2.259882\n",
      "2024-12-09 15:14:14.034000: I runner.py:310] Step = 18800 ; steps/s = 1.63, tokens/s = 43724 (43724 target) ; Learning rate = 0.000645 ; Loss = 2.178186\n",
      "2024-12-09 15:15:15.791000: I runner.py:310] Step = 18900 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000643 ; Loss = 2.189092\n",
      "2024-12-09 15:16:17.505000: I runner.py:310] Step = 19000 ; steps/s = 1.62, tokens/s = 44205 (44205 target) ; Learning rate = 0.000641 ; Loss = 2.199421\n",
      "2024-12-09 15:17:19.280000: I runner.py:310] Step = 19100 ; steps/s = 1.62, tokens/s = 44178 (44178 target) ; Learning rate = 0.000640 ; Loss = 2.213174\n",
      "2024-12-09 15:18:20.652000: I runner.py:310] Step = 19200 ; steps/s = 1.63, tokens/s = 43742 (43742 target) ; Learning rate = 0.000638 ; Loss = 2.212308\n",
      "2024-12-09 15:19:22.459000: I runner.py:310] Step = 19300 ; steps/s = 1.62, tokens/s = 44152 (44152 target) ; Learning rate = 0.000636 ; Loss = 2.150292\n",
      "2024-12-09 15:20:24.246000: I runner.py:310] Step = 19400 ; steps/s = 1.62, tokens/s = 44181 (44181 target) ; Learning rate = 0.000635 ; Loss = 2.179238\n",
      "2024-12-09 15:21:25.577000: I runner.py:310] Step = 19500 ; steps/s = 1.63, tokens/s = 43755 (43755 target) ; Learning rate = 0.000633 ; Loss = 2.186762\n",
      "2024-12-09 15:22:27.318000: I runner.py:310] Step = 19600 ; steps/s = 1.62, tokens/s = 44200 (44200 target) ; Learning rate = 0.000631 ; Loss = 2.176580\n",
      "2024-12-09 15:23:29.113000: I runner.py:310] Step = 19700 ; steps/s = 1.62, tokens/s = 44164 (44164 target) ; Learning rate = 0.000630 ; Loss = 2.186194\n",
      "2024-12-09 15:24:30.913000: I runner.py:310] Step = 19800 ; steps/s = 1.62, tokens/s = 44172 (44172 target) ; Learning rate = 0.000628 ; Loss = 2.206284\n",
      "2024-12-09 15:25:32.275000: I runner.py:310] Step = 19900 ; steps/s = 1.63, tokens/s = 43750 (43750 target) ; Learning rate = 0.000627 ; Loss = 2.223609\n",
      "2024-12-09 15:26:34.020000: I runner.py:310] Step = 20000 ; steps/s = 1.62, tokens/s = 44203 (44203 target) ; Learning rate = 0.000625 ; Loss = 2.140116\n",
      "2024-12-09 15:26:35.869000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-20000\n",
      "2024-12-09 15:26:35.869000: I training.py:192] Running evaluation for step 20000\n",
      "2024-12-09 15:30:36.549000: I training.py:192] Evaluation result for step 20000: loss = 2.185926 ; perplexity = 8.898883\n",
      "2024-12-09 15:31:38.261000: I runner.py:310] Step = 20100 ; steps/s = 1.62, tokens/s = 44224 (44224 target) ; Learning rate = 0.000623 ; Loss = 2.189724\n",
      "2024-12-09 15:32:39.656000: I runner.py:310] Step = 20200 ; steps/s = 1.63, tokens/s = 43721 (43721 target) ; Learning rate = 0.000622 ; Loss = 2.149142\n",
      "2024-12-09 15:33:41.417000: I runner.py:310] Step = 20300 ; steps/s = 1.62, tokens/s = 44192 (44192 target) ; Learning rate = 0.000620 ; Loss = 2.167178\n",
      "2024-12-09 15:34:43.277000: I runner.py:310] Step = 20400 ; steps/s = 1.62, tokens/s = 44105 (44105 target) ; Learning rate = 0.000619 ; Loss = 2.175061\n",
      "2024-12-09 15:35:45.047000: I runner.py:310] Step = 20500 ; steps/s = 1.62, tokens/s = 44193 (44193 target) ; Learning rate = 0.000617 ; Loss = 2.195221\n",
      "2024-12-09 15:36:46.454000: I runner.py:310] Step = 20600 ; steps/s = 1.63, tokens/s = 43709 (43709 target) ; Learning rate = 0.000616 ; Loss = 2.080782\n",
      "2024-12-09 15:37:48.217000: I runner.py:310] Step = 20700 ; steps/s = 1.62, tokens/s = 44184 (44184 target) ; Learning rate = 0.000614 ; Loss = 2.136157\n",
      "2024-12-09 15:38:50.017000: I runner.py:310] Step = 20800 ; steps/s = 1.62, tokens/s = 44166 (44166 target) ; Learning rate = 0.000613 ; Loss = 2.158671\n",
      "2024-12-09 15:39:51.432000: I runner.py:310] Step = 20900 ; steps/s = 1.63, tokens/s = 43707 (43707 target) ; Learning rate = 0.000611 ; Loss = 2.149573\n",
      "2024-12-09 15:40:53.288000: I runner.py:310] Step = 21000 ; steps/s = 1.62, tokens/s = 44117 (44117 target) ; Learning rate = 0.000610 ; Loss = 2.132340\n",
      "2024-12-09 15:41:55.036000: I runner.py:310] Step = 21100 ; steps/s = 1.62, tokens/s = 44199 (44199 target) ; Learning rate = 0.000608 ; Loss = 2.139948\n",
      "2024-12-09 15:42:56.816000: I runner.py:310] Step = 21200 ; steps/s = 1.62, tokens/s = 44181 (44181 target) ; Learning rate = 0.000607 ; Loss = 2.172197\n",
      "2024-12-09 15:43:58.206000: I runner.py:310] Step = 21300 ; steps/s = 1.63, tokens/s = 43715 (43715 target) ; Learning rate = 0.000606 ; Loss = 2.147665\n",
      "2024-12-09 15:45:00.000000: I runner.py:310] Step = 21400 ; steps/s = 1.62, tokens/s = 44161 (44161 target) ; Learning rate = 0.000604 ; Loss = 2.127727\n",
      "2024-12-09 15:46:01.822000: I runner.py:310] Step = 21500 ; steps/s = 1.62, tokens/s = 44148 (44148 target) ; Learning rate = 0.000603 ; Loss = 2.157439\n",
      "2024-12-09 15:47:03.249000: I runner.py:310] Step = 21600 ; steps/s = 1.63, tokens/s = 43703 (43703 target) ; Learning rate = 0.000601 ; Loss = 2.142114\n",
      "2024-12-09 15:48:05.000000: I runner.py:310] Step = 21700 ; steps/s = 1.62, tokens/s = 44185 (44185 target) ; Learning rate = 0.000600 ; Loss = 2.114852\n",
      "2024-12-09 15:49:06.788000: I runner.py:310] Step = 21800 ; steps/s = 1.62, tokens/s = 44178 (44178 target) ; Learning rate = 0.000599 ; Loss = 2.128776\n",
      "2024-12-09 15:50:08.509000: I runner.py:310] Step = 21900 ; steps/s = 1.62, tokens/s = 44218 (44218 target) ; Learning rate = 0.000597 ; Loss = 2.139471\n",
      "2024-12-09 15:51:09.907000: I runner.py:310] Step = 22000 ; steps/s = 1.63, tokens/s = 43714 (43714 target) ; Learning rate = 0.000596 ; Loss = 2.150788\n",
      "2024-12-09 15:52:11.706000: I runner.py:310] Step = 22100 ; steps/s = 1.62, tokens/s = 44174 (44174 target) ; Learning rate = 0.000595 ; Loss = 2.092978\n",
      "2024-12-09 15:53:13.471000: I runner.py:310] Step = 22200 ; steps/s = 1.62, tokens/s = 44180 (44180 target) ; Learning rate = 0.000593 ; Loss = 2.128583\n",
      "2024-12-09 15:54:14.904000: I runner.py:310] Step = 22300 ; steps/s = 1.63, tokens/s = 43694 (43694 target) ; Learning rate = 0.000592 ; Loss = 2.105229\n",
      "2024-12-09 15:55:16.649000: I runner.py:310] Step = 22400 ; steps/s = 1.62, tokens/s = 44189 (44189 target) ; Learning rate = 0.000591 ; Loss = 2.133499\n",
      "2024-12-09 15:56:18.469000: I runner.py:310] Step = 22500 ; steps/s = 1.62, tokens/s = 44158 (44158 target) ; Learning rate = 0.000589 ; Loss = 2.167656\n",
      "2024-12-09 15:57:20.590000: I runner.py:310] Step = 22600 ; steps/s = 1.61, tokens/s = 43937 (43937 target) ; Learning rate = 0.000588 ; Loss = 2.153436\n",
      "2024-12-09 15:58:22.948000: I runner.py:310] Step = 22700 ; steps/s = 1.60, tokens/s = 43044 (43044 target) ; Learning rate = 0.000587 ; Loss = 2.118283\n",
      "2024-12-09 15:59:25.024000: I runner.py:310] Step = 22800 ; steps/s = 1.61, tokens/s = 43955 (43955 target) ; Learning rate = 0.000585 ; Loss = 2.112864\n",
      "2024-12-09 16:00:27.469000: I runner.py:310] Step = 22900 ; steps/s = 1.60, tokens/s = 43714 (43714 target) ; Learning rate = 0.000584 ; Loss = 2.129057\n",
      "2024-12-09 16:01:29.037000: I runner.py:310] Step = 23000 ; steps/s = 1.62, tokens/s = 43589 (43589 target) ; Learning rate = 0.000583 ; Loss = 2.123636\n",
      "2024-12-09 16:02:30.769000: I runner.py:310] Step = 23100 ; steps/s = 1.62, tokens/s = 44221 (44221 target) ; Learning rate = 0.000582 ; Loss = 2.093157\n",
      "2024-12-09 16:03:32.627000: I runner.py:310] Step = 23200 ; steps/s = 1.62, tokens/s = 44120 (44120 target) ; Learning rate = 0.000580 ; Loss = 2.136780\n",
      "2024-12-09 16:04:34.537000: I runner.py:310] Step = 23300 ; steps/s = 1.62, tokens/s = 44082 (44082 target) ; Learning rate = 0.000579 ; Loss = 2.108180\n",
      "2024-12-09 16:05:35.972000: I runner.py:310] Step = 23400 ; steps/s = 1.63, tokens/s = 43679 (43679 target) ; Learning rate = 0.000578 ; Loss = 2.140768\n",
      "2024-12-09 16:06:38.686000: I runner.py:310] Step = 23500 ; steps/s = 1.59, tokens/s = 43512 (43512 target) ; Learning rate = 0.000577 ; Loss = 2.080124\n",
      "2024-12-09 16:07:40.413000: I runner.py:310] Step = 23600 ; steps/s = 1.62, tokens/s = 44215 (44215 target) ; Learning rate = 0.000575 ; Loss = 2.108002\n",
      "2024-12-09 16:08:41.766000: I runner.py:310] Step = 23700 ; steps/s = 1.63, tokens/s = 43753 (43753 target) ; Learning rate = 0.000574 ; Loss = 2.112965\n",
      "2024-12-09 16:09:43.575000: I runner.py:310] Step = 23800 ; steps/s = 1.62, tokens/s = 44172 (44172 target) ; Learning rate = 0.000573 ; Loss = 2.089213\n",
      "2024-12-09 16:10:45.348000: I runner.py:310] Step = 23900 ; steps/s = 1.62, tokens/s = 44176 (44176 target) ; Learning rate = 0.000572 ; Loss = 2.108552\n",
      "2024-12-09 16:11:47.058000: I runner.py:310] Step = 24000 ; steps/s = 1.62, tokens/s = 44215 (44215 target) ; Learning rate = 0.000571 ; Loss = 2.098856\n",
      "2024-12-09 16:12:48.452000: I runner.py:310] Step = 24100 ; steps/s = 1.63, tokens/s = 43738 (43738 target) ; Learning rate = 0.000569 ; Loss = 2.062245\n",
      "2024-12-09 16:13:50.181000: I runner.py:310] Step = 24200 ; steps/s = 1.62, tokens/s = 44213 (44213 target) ; Learning rate = 0.000568 ; Loss = 2.097521\n",
      "2024-12-09 16:14:52.005000: I runner.py:310] Step = 24300 ; steps/s = 1.62, tokens/s = 44129 (44129 target) ; Learning rate = 0.000567 ; Loss = 2.098934\n",
      "2024-12-09 16:15:53.248000: I runner.py:310] Step = 24400 ; steps/s = 1.63, tokens/s = 43842 (43842 target) ; Learning rate = 0.000566 ; Loss = 2.090184\n",
      "2024-12-09 16:16:54.994000: I runner.py:310] Step = 24500 ; steps/s = 1.62, tokens/s = 44199 (44199 target) ; Learning rate = 0.000565 ; Loss = 2.075203\n",
      "2024-12-09 16:17:56.691000: I runner.py:310] Step = 24600 ; steps/s = 1.62, tokens/s = 44240 (44240 target) ; Learning rate = 0.000564 ; Loss = 2.080894\n",
      "2024-12-09 16:18:58.362000: I runner.py:310] Step = 24700 ; steps/s = 1.62, tokens/s = 44094 (44094 target) ; Learning rate = 0.000562 ; Loss = 2.055032\n",
      "2024-12-09 16:19:59.795000: I runner.py:310] Step = 24800 ; steps/s = 1.63, tokens/s = 43846 (43846 target) ; Learning rate = 0.000561 ; Loss = 2.065483\n",
      "2024-12-09 16:21:01.516000: I runner.py:310] Step = 24900 ; steps/s = 1.62, tokens/s = 44218 (44218 target) ; Learning rate = 0.000560 ; Loss = 2.112843\n",
      "2024-12-09 16:22:03.357000: I runner.py:310] Step = 25000 ; steps/s = 1.62, tokens/s = 44136 (44136 target) ; Learning rate = 0.000559 ; Loss = 2.123093\n",
      "2024-12-09 16:22:03.359000: I training.py:192] Running evaluation for step 25000\n",
      "2024-12-09 16:25:48.025000: I training.py:192] Evaluation result for step 25000: loss = 2.230490 ; perplexity = 9.304423\n",
      "2024-12-09 16:26:49.152000: I runner.py:310] Step = 25100 ; steps/s = 1.64, tokens/s = 43903 (43903 target) ; Learning rate = 0.000558 ; Loss = 2.097393\n",
      "2024-12-09 16:27:50.881000: I runner.py:310] Step = 25200 ; steps/s = 1.62, tokens/s = 44219 (44219 target) ; Learning rate = 0.000557 ; Loss = 2.032408\n",
      "2024-12-09 16:28:52.655000: I runner.py:310] Step = 25300 ; steps/s = 1.62, tokens/s = 44174 (44174 target) ; Learning rate = 0.000556 ; Loss = 2.125736\n",
      "2024-12-09 16:29:54.045000: I runner.py:310] Step = 25400 ; steps/s = 1.63, tokens/s = 43731 (43731 target) ; Learning rate = 0.000555 ; Loss = 2.066728\n",
      "2024-12-09 16:30:55.757000: I runner.py:310] Step = 25500 ; steps/s = 1.62, tokens/s = 44217 (44217 target) ; Learning rate = 0.000553 ; Loss = 2.041629\n",
      "2024-12-09 16:31:57.463000: I runner.py:310] Step = 25600 ; steps/s = 1.62, tokens/s = 44234 (44234 target) ; Learning rate = 0.000552 ; Loss = 2.109168\n",
      "2024-12-09 16:32:59.186000: I runner.py:310] Step = 25700 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000551 ; Loss = 2.112428\n",
      "2024-12-09 16:34:00.574000: I runner.py:310] Step = 25800 ; steps/s = 1.63, tokens/s = 43746 (43746 target) ; Learning rate = 0.000550 ; Loss = 2.103184\n",
      "2024-12-09 16:35:02.331000: I runner.py:310] Step = 25900 ; steps/s = 1.62, tokens/s = 44190 (44190 target) ; Learning rate = 0.000549 ; Loss = 2.084287\n",
      "2024-12-09 16:36:03.985000: I runner.py:310] Step = 26000 ; steps/s = 1.62, tokens/s = 44262 (44262 target) ; Learning rate = 0.000548 ; Loss = 2.062814\n",
      "2024-12-09 16:37:05.355000: I runner.py:310] Step = 26100 ; steps/s = 1.63, tokens/s = 43739 (43739 target) ; Learning rate = 0.000547 ; Loss = 2.082162\n",
      "2024-12-09 16:38:07.065000: I runner.py:310] Step = 26200 ; steps/s = 1.62, tokens/s = 44222 (44222 target) ; Learning rate = 0.000546 ; Loss = 2.056083\n",
      "2024-12-09 16:39:08.829000: I runner.py:310] Step = 26300 ; steps/s = 1.62, tokens/s = 44174 (44174 target) ; Learning rate = 0.000545 ; Loss = 2.065425\n",
      "2024-12-09 16:40:10.550000: I runner.py:310] Step = 26400 ; steps/s = 1.62, tokens/s = 44227 (44227 target) ; Learning rate = 0.000544 ; Loss = 2.060709\n",
      "2024-12-09 16:41:11.899000: I runner.py:310] Step = 26500 ; steps/s = 1.63, tokens/s = 43755 (43755 target) ; Learning rate = 0.000543 ; Loss = 2.087605\n",
      "2024-12-09 16:42:13.597000: I runner.py:310] Step = 26600 ; steps/s = 1.62, tokens/s = 44221 (44221 target) ; Learning rate = 0.000542 ; Loss = 2.060261\n",
      "2024-12-09 16:43:15.345000: I runner.py:310] Step = 26700 ; steps/s = 1.62, tokens/s = 44200 (44200 target) ; Learning rate = 0.000541 ; Loss = 2.077364\n",
      "2024-12-09 16:44:16.701000: I runner.py:310] Step = 26800 ; steps/s = 1.63, tokens/s = 43765 (43765 target) ; Learning rate = 0.000540 ; Loss = 2.062755\n",
      "2024-12-09 16:45:18.473000: I runner.py:310] Step = 26900 ; steps/s = 1.62, tokens/s = 44182 (44182 target) ; Learning rate = 0.000539 ; Loss = 2.069788\n",
      "2024-12-09 16:46:20.198000: I runner.py:310] Step = 27000 ; steps/s = 1.62, tokens/s = 44206 (44206 target) ; Learning rate = 0.000538 ; Loss = 2.061062\n",
      "2024-12-09 16:47:21.887000: I runner.py:310] Step = 27100 ; steps/s = 1.62, tokens/s = 44238 (44238 target) ; Learning rate = 0.000537 ; Loss = 2.076404\n",
      "2024-12-09 16:48:23.180000: I runner.py:310] Step = 27200 ; steps/s = 1.63, tokens/s = 43795 (43795 target) ; Learning rate = 0.000536 ; Loss = 2.089137\n",
      "2024-12-09 16:49:24.829000: I runner.py:310] Step = 27300 ; steps/s = 1.62, tokens/s = 44276 (44276 target) ; Learning rate = 0.000535 ; Loss = 2.034941\n",
      "2024-12-09 16:50:26.534000: I runner.py:310] Step = 27400 ; steps/s = 1.62, tokens/s = 44219 (44219 target) ; Learning rate = 0.000534 ; Loss = 2.047395\n",
      "2024-12-09 16:51:27.856000: I runner.py:310] Step = 27500 ; steps/s = 1.63, tokens/s = 43771 (43771 target) ; Learning rate = 0.000533 ; Loss = 2.042264\n",
      "2024-12-09 16:52:29.590000: I runner.py:310] Step = 27600 ; steps/s = 1.62, tokens/s = 44206 (44206 target) ; Learning rate = 0.000532 ; Loss = 2.028501\n",
      "2024-12-09 16:53:31.263000: I runner.py:310] Step = 27700 ; steps/s = 1.62, tokens/s = 44251 (44251 target) ; Learning rate = 0.000531 ; Loss = 2.056629\n",
      "2024-12-09 16:54:32.964000: I runner.py:310] Step = 27800 ; steps/s = 1.62, tokens/s = 44240 (44240 target) ; Learning rate = 0.000530 ; Loss = 2.083322\n",
      "2024-12-09 16:55:34.151000: I runner.py:310] Step = 27900 ; steps/s = 1.63, tokens/s = 43870 (43870 target) ; Learning rate = 0.000529 ; Loss = 1.994755\n",
      "2024-12-09 16:56:35.811000: I runner.py:310] Step = 28000 ; steps/s = 1.62, tokens/s = 44257 (44257 target) ; Learning rate = 0.000528 ; Loss = 2.054743\n",
      "2024-12-09 16:57:37.517000: I runner.py:310] Step = 28100 ; steps/s = 1.62, tokens/s = 44222 (44222 target) ; Learning rate = 0.000527 ; Loss = 2.065634\n",
      "2024-12-09 16:58:38.813000: I runner.py:310] Step = 28200 ; steps/s = 1.63, tokens/s = 43798 (43798 target) ; Learning rate = 0.000526 ; Loss = 2.025451\n",
      "2024-12-09 16:59:40.526000: I runner.py:310] Step = 28300 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000525 ; Loss = 2.048701\n",
      "2024-12-09 17:00:42.241000: I runner.py:310] Step = 28400 ; steps/s = 1.62, tokens/s = 44224 (44224 target) ; Learning rate = 0.000524 ; Loss = 2.082566\n",
      "2024-12-09 17:01:43.963000: I runner.py:310] Step = 28500 ; steps/s = 1.62, tokens/s = 44221 (44221 target) ; Learning rate = 0.000524 ; Loss = 2.081450\n",
      "2024-12-09 17:02:45.284000: I runner.py:310] Step = 28600 ; steps/s = 1.63, tokens/s = 43767 (43767 target) ; Learning rate = 0.000523 ; Loss = 1.997852\n",
      "2024-12-09 17:03:47.031000: I runner.py:310] Step = 28700 ; steps/s = 1.62, tokens/s = 44208 (44208 target) ; Learning rate = 0.000522 ; Loss = 2.056222\n",
      "2024-12-09 17:04:48.767000: I runner.py:310] Step = 28800 ; steps/s = 1.62, tokens/s = 44197 (44197 target) ; Learning rate = 0.000521 ; Loss = 2.052857\n",
      "2024-12-09 17:05:50.056000: I runner.py:310] Step = 28900 ; steps/s = 1.63, tokens/s = 43800 (43800 target) ; Learning rate = 0.000520 ; Loss = 2.014970\n",
      "2024-12-09 17:06:51.752000: I runner.py:310] Step = 29000 ; steps/s = 1.62, tokens/s = 44233 (44233 target) ; Learning rate = 0.000519 ; Loss = 2.025863\n",
      "2024-12-09 17:07:53.529000: I runner.py:310] Step = 29100 ; steps/s = 1.62, tokens/s = 44194 (44194 target) ; Learning rate = 0.000518 ; Loss = 2.066048\n",
      "2024-12-09 17:08:55.300000: I runner.py:310] Step = 29200 ; steps/s = 1.62, tokens/s = 44173 (44173 target) ; Learning rate = 0.000517 ; Loss = 2.067933\n",
      "2024-12-09 17:09:56.583000: I runner.py:310] Step = 29300 ; steps/s = 1.63, tokens/s = 43790 (43790 target) ; Learning rate = 0.000516 ; Loss = 1.988041\n",
      "2024-12-09 17:10:58.360000: I runner.py:310] Step = 29400 ; steps/s = 1.62, tokens/s = 44166 (44166 target) ; Learning rate = 0.000515 ; Loss = 2.045625\n",
      "2024-12-09 17:12:00.093000: I runner.py:310] Step = 29500 ; steps/s = 1.62, tokens/s = 44216 (44216 target) ; Learning rate = 0.000515 ; Loss = 2.053021\n",
      "2024-12-09 17:13:01.426000: I runner.py:310] Step = 29600 ; steps/s = 1.63, tokens/s = 43776 (43776 target) ; Learning rate = 0.000514 ; Loss = 2.011175\n",
      "2024-12-09 17:14:03.142000: I runner.py:310] Step = 29700 ; steps/s = 1.62, tokens/s = 44227 (44227 target) ; Learning rate = 0.000513 ; Loss = 2.012751\n",
      "2024-12-09 17:15:04.893000: I runner.py:310] Step = 29800 ; steps/s = 1.62, tokens/s = 44194 (44194 target) ; Learning rate = 0.000512 ; Loss = 2.053662\n",
      "2024-12-09 17:16:06.612000: I runner.py:310] Step = 29900 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000511 ; Loss = 2.067641\n",
      "2024-12-09 17:17:08.016000: I runner.py:310] Step = 30000 ; steps/s = 1.63, tokens/s = 43711 (43711 target) ; Learning rate = 0.000510 ; Loss = 1.994637\n",
      "2024-12-09 17:17:09.884000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-30000\n",
      "2024-12-09 17:17:09.884000: I training.py:192] Running evaluation for step 30000\n",
      "2024-12-09 17:20:15.462000: I training.py:192] Evaluation result for step 30000: loss = 2.297775 ; perplexity = 9.952012\n",
      "2024-12-09 17:21:16.975000: I runner.py:310] Step = 30100 ; steps/s = 1.63, tokens/s = 44376 (44376 target) ; Learning rate = 0.000509 ; Loss = 2.013557\n",
      "2024-12-09 17:22:18.687000: I runner.py:310] Step = 30200 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000509 ; Loss = 2.038397\n",
      "2024-12-09 17:23:20.058000: I runner.py:310] Step = 30300 ; steps/s = 1.63, tokens/s = 43734 (43734 target) ; Learning rate = 0.000508 ; Loss = 2.038301\n",
      "2024-12-09 17:24:21.777000: I runner.py:310] Step = 30400 ; steps/s = 1.62, tokens/s = 44238 (44238 target) ; Learning rate = 0.000507 ; Loss = 2.020862\n",
      "2024-12-09 17:25:23.474000: I runner.py:310] Step = 30500 ; steps/s = 1.62, tokens/s = 44230 (44230 target) ; Learning rate = 0.000506 ; Loss = 2.026767\n",
      "2024-12-09 17:26:25.213000: I runner.py:310] Step = 30600 ; steps/s = 1.62, tokens/s = 44189 (44189 target) ; Learning rate = 0.000505 ; Loss = 2.027137\n",
      "2024-12-09 17:27:26.541000: I runner.py:310] Step = 30700 ; steps/s = 1.63, tokens/s = 43756 (43756 target) ; Learning rate = 0.000504 ; Loss = 1.974481\n",
      "2024-12-09 17:28:28.243000: I runner.py:310] Step = 30800 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000504 ; Loss = 2.045842\n",
      "2024-12-09 17:29:29.999000: I runner.py:310] Step = 30900 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000503 ; Loss = 2.051395\n",
      "2024-12-09 17:30:31.259000: I runner.py:310] Step = 31000 ; steps/s = 1.63, tokens/s = 43807 (43807 target) ; Learning rate = 0.000502 ; Loss = 1.987843\n",
      "2024-12-09 17:31:32.989000: I runner.py:310] Step = 31100 ; steps/s = 1.62, tokens/s = 44213 (44213 target) ; Learning rate = 0.000501 ; Loss = 2.004051\n",
      "2024-12-09 17:32:34.763000: I runner.py:310] Step = 31200 ; steps/s = 1.62, tokens/s = 44183 (44183 target) ; Learning rate = 0.000500 ; Loss = 2.031803\n",
      "2024-12-09 17:33:36.487000: I runner.py:310] Step = 31300 ; steps/s = 1.62, tokens/s = 44219 (44219 target) ; Learning rate = 0.000500 ; Loss = 2.071765\n",
      "2024-12-09 17:34:37.787000: I runner.py:310] Step = 31400 ; steps/s = 1.63, tokens/s = 43789 (43789 target) ; Learning rate = 0.000499 ; Loss = 2.049248\n",
      "2024-12-09 17:35:39.520000: I runner.py:310] Step = 31500 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000498 ; Loss = 2.021980\n",
      "2024-12-09 17:36:41.205000: I runner.py:310] Step = 31600 ; steps/s = 1.62, tokens/s = 44240 (44240 target) ; Learning rate = 0.000497 ; Loss = 2.003858\n",
      "2024-12-09 17:37:42.453000: I runner.py:310] Step = 31700 ; steps/s = 1.63, tokens/s = 43826 (43826 target) ; Learning rate = 0.000496 ; Loss = 2.036983\n",
      "2024-12-09 17:38:44.176000: I runner.py:310] Step = 31800 ; steps/s = 1.62, tokens/s = 44230 (44230 target) ; Learning rate = 0.000496 ; Loss = 2.006950\n",
      "2024-12-09 17:39:45.831000: I runner.py:310] Step = 31900 ; steps/s = 1.62, tokens/s = 44253 (44253 target) ; Learning rate = 0.000495 ; Loss = 1.994542\n",
      "2024-12-09 17:40:47.582000: I runner.py:310] Step = 32000 ; steps/s = 1.62, tokens/s = 44192 (44192 target) ; Learning rate = 0.000494 ; Loss = 2.013829\n",
      "2024-12-09 17:41:48.801000: I runner.py:310] Step = 32100 ; steps/s = 1.63, tokens/s = 43856 (43856 target) ; Learning rate = 0.000493 ; Loss = 2.039808\n",
      "2024-12-09 17:42:50.466000: I runner.py:310] Step = 32200 ; steps/s = 1.62, tokens/s = 44250 (44250 target) ; Learning rate = 0.000493 ; Loss = 1.997979\n",
      "2024-12-09 17:43:52.178000: I runner.py:310] Step = 32300 ; steps/s = 1.62, tokens/s = 44216 (44216 target) ; Learning rate = 0.000492 ; Loss = 2.009473\n",
      "2024-12-09 17:44:53.568000: I runner.py:310] Step = 32400 ; steps/s = 1.63, tokens/s = 43724 (43724 target) ; Learning rate = 0.000491 ; Loss = 2.013582\n",
      "2024-12-09 17:45:55.285000: I runner.py:310] Step = 32500 ; steps/s = 1.62, tokens/s = 44216 (44216 target) ; Learning rate = 0.000490 ; Loss = 1.996944\n",
      "2024-12-09 17:46:56.970000: I runner.py:310] Step = 32600 ; steps/s = 1.62, tokens/s = 44235 (44235 target) ; Learning rate = 0.000490 ; Loss = 2.020214\n",
      "2024-12-09 17:47:58.682000: I runner.py:310] Step = 32700 ; steps/s = 1.62, tokens/s = 44237 (44237 target) ; Learning rate = 0.000489 ; Loss = 2.013009\n",
      "2024-12-09 17:49:00.000000: I runner.py:310] Step = 32800 ; steps/s = 1.63, tokens/s = 43771 (43771 target) ; Learning rate = 0.000488 ; Loss = 2.019727\n",
      "2024-12-09 17:50:01.692000: I runner.py:310] Step = 32900 ; steps/s = 1.62, tokens/s = 44247 (44247 target) ; Learning rate = 0.000487 ; Loss = 2.003223\n",
      "2024-12-09 17:51:03.365000: I runner.py:310] Step = 33000 ; steps/s = 1.62, tokens/s = 44256 (44256 target) ; Learning rate = 0.000487 ; Loss = 1.993896\n",
      "2024-12-09 17:52:04.723000: I runner.py:310] Step = 33100 ; steps/s = 1.63, tokens/s = 43735 (43735 target) ; Learning rate = 0.000486 ; Loss = 2.017429\n",
      "2024-12-09 17:53:06.472000: I runner.py:310] Step = 33200 ; steps/s = 1.62, tokens/s = 44197 (44197 target) ; Learning rate = 0.000485 ; Loss = 1.964412\n",
      "2024-12-09 17:54:08.196000: I runner.py:310] Step = 33300 ; steps/s = 1.62, tokens/s = 44222 (44222 target) ; Learning rate = 0.000484 ; Loss = 1.995137\n",
      "2024-12-09 17:55:09.584000: I runner.py:310] Step = 33400 ; steps/s = 1.63, tokens/s = 43942 (43942 target) ; Learning rate = 0.000484 ; Loss = 2.039671\n",
      "2024-12-09 17:56:11.229000: I runner.py:310] Step = 33500 ; steps/s = 1.62, tokens/s = 44062 (44062 target) ; Learning rate = 0.000483 ; Loss = 1.977175\n",
      "2024-12-09 17:57:12.892000: I runner.py:310] Step = 33600 ; steps/s = 1.62, tokens/s = 44252 (44252 target) ; Learning rate = 0.000482 ; Loss = 1.994171\n",
      "2024-12-09 17:58:14.637000: I runner.py:310] Step = 33700 ; steps/s = 1.62, tokens/s = 44200 (44200 target) ; Learning rate = 0.000481 ; Loss = 2.019110\n",
      "2024-12-09 17:59:15.911000: I runner.py:310] Step = 33800 ; steps/s = 1.63, tokens/s = 43808 (43808 target) ; Learning rate = 0.000481 ; Loss = 1.956073\n",
      "2024-12-09 18:00:17.648000: I runner.py:310] Step = 33900 ; steps/s = 1.62, tokens/s = 44206 (44206 target) ; Learning rate = 0.000480 ; Loss = 2.004823\n",
      "2024-12-09 18:01:19.387000: I runner.py:310] Step = 34000 ; steps/s = 1.62, tokens/s = 44205 (44205 target) ; Learning rate = 0.000479 ; Loss = 2.016597\n",
      "2024-12-09 18:02:20.673000: I runner.py:310] Step = 34100 ; steps/s = 1.63, tokens/s = 43794 (43794 target) ; Learning rate = 0.000479 ; Loss = 1.965556\n",
      "2024-12-09 18:03:22.391000: I runner.py:310] Step = 34200 ; steps/s = 1.62, tokens/s = 44215 (44215 target) ; Learning rate = 0.000478 ; Loss = 2.006414\n",
      "2024-12-09 18:04:24.126000: I runner.py:310] Step = 34300 ; steps/s = 1.62, tokens/s = 44220 (44220 target) ; Learning rate = 0.000477 ; Loss = 1.977215\n",
      "2024-12-09 18:05:25.936000: I runner.py:310] Step = 34400 ; steps/s = 1.62, tokens/s = 44135 (44135 target) ; Learning rate = 0.000477 ; Loss = 1.984104\n",
      "2024-12-09 18:06:27.220000: I runner.py:310] Step = 34500 ; steps/s = 1.63, tokens/s = 43823 (43823 target) ; Learning rate = 0.000476 ; Loss = 1.941939\n",
      "2024-12-09 18:07:28.937000: I runner.py:310] Step = 34600 ; steps/s = 1.62, tokens/s = 44213 (44213 target) ; Learning rate = 0.000475 ; Loss = 1.989880\n",
      "2024-12-09 18:08:30.610000: I runner.py:310] Step = 34700 ; steps/s = 1.62, tokens/s = 44263 (44263 target) ; Learning rate = 0.000474 ; Loss = 2.020292\n",
      "2024-12-09 18:09:31.935000: I runner.py:310] Step = 34800 ; steps/s = 1.63, tokens/s = 43757 (43757 target) ; Learning rate = 0.000474 ; Loss = 1.979793\n",
      "2024-12-09 18:10:33.588000: I runner.py:310] Step = 34900 ; steps/s = 1.62, tokens/s = 44271 (44271 target) ; Learning rate = 0.000473 ; Loss = 1.982840\n",
      "2024-12-09 18:11:35.312000: I runner.py:310] Step = 35000 ; steps/s = 1.62, tokens/s = 44201 (44201 target) ; Learning rate = 0.000472 ; Loss = 1.982117\n",
      "2024-12-09 18:11:35.314000: I training.py:192] Running evaluation for step 35000\n",
      "2024-12-09 18:14:56.479000: I training.py:192] Evaluation result for step 35000: loss = 2.314773 ; perplexity = 10.122628\n",
      "2024-12-09 18:15:58.046000: I runner.py:310] Step = 35100 ; steps/s = 1.62, tokens/s = 44347 (44347 target) ; Learning rate = 0.000472 ; Loss = 1.979464\n",
      "2024-12-09 18:16:59.421000: I runner.py:310] Step = 35200 ; steps/s = 1.63, tokens/s = 43731 (43731 target) ; Learning rate = 0.000471 ; Loss = 2.034084\n",
      "2024-12-09 18:18:01.095000: I runner.py:310] Step = 35300 ; steps/s = 1.62, tokens/s = 44233 (44233 target) ; Learning rate = 0.000470 ; Loss = 1.953967\n",
      "2024-12-09 18:19:02.865000: I runner.py:310] Step = 35400 ; steps/s = 1.62, tokens/s = 44194 (44194 target) ; Learning rate = 0.000470 ; Loss = 1.973318\n",
      "2024-12-09 18:20:04.193000: I runner.py:310] Step = 35500 ; steps/s = 1.63, tokens/s = 43785 (43785 target) ; Learning rate = 0.000469 ; Loss = 1.959310\n",
      "2024-12-09 18:21:05.900000: I runner.py:310] Step = 35600 ; steps/s = 1.62, tokens/s = 44230 (44230 target) ; Learning rate = 0.000468 ; Loss = 1.959330\n",
      "2024-12-09 18:22:07.558000: I runner.py:310] Step = 35700 ; steps/s = 1.62, tokens/s = 44258 (44258 target) ; Learning rate = 0.000468 ; Loss = 1.986405\n",
      "2024-12-09 18:23:09.299000: I runner.py:310] Step = 35800 ; steps/s = 1.62, tokens/s = 44192 (44192 target) ; Learning rate = 0.000467 ; Loss = 2.008202\n",
      "2024-12-09 18:24:10.637000: I runner.py:310] Step = 35900 ; steps/s = 1.63, tokens/s = 43770 (43770 target) ; Learning rate = 0.000466 ; Loss = 1.993612\n",
      "2024-12-09 18:25:12.365000: I runner.py:310] Step = 36000 ; steps/s = 1.62, tokens/s = 44224 (44224 target) ; Learning rate = 0.000466 ; Loss = 1.983738\n",
      "2024-12-09 18:26:14.103000: I runner.py:310] Step = 36100 ; steps/s = 1.62, tokens/s = 44198 (44198 target) ; Learning rate = 0.000465 ; Loss = 1.975012\n",
      "2024-12-09 18:27:15.437000: I runner.py:310] Step = 36200 ; steps/s = 1.63, tokens/s = 43758 (43758 target) ; Learning rate = 0.000465 ; Loss = 1.960603\n",
      "2024-12-09 18:28:17.208000: I runner.py:310] Step = 36300 ; steps/s = 1.62, tokens/s = 44181 (44181 target) ; Learning rate = 0.000464 ; Loss = 1.967794\n",
      "2024-12-09 18:29:18.925000: I runner.py:310] Step = 36400 ; steps/s = 1.62, tokens/s = 44221 (44221 target) ; Learning rate = 0.000463 ; Loss = 1.982055\n",
      "2024-12-09 18:30:20.609000: I runner.py:310] Step = 36500 ; steps/s = 1.62, tokens/s = 44254 (44254 target) ; Learning rate = 0.000463 ; Loss = 1.970877\n",
      "2024-12-09 18:31:21.903000: I runner.py:310] Step = 36600 ; steps/s = 1.63, tokens/s = 43799 (43799 target) ; Learning rate = 0.000462 ; Loss = 1.927672\n",
      "2024-12-09 18:32:23.639000: I runner.py:310] Step = 36700 ; steps/s = 1.62, tokens/s = 44191 (44191 target) ; Learning rate = 0.000461 ; Loss = 1.980133\n",
      "2024-12-09 18:33:25.318000: I runner.py:310] Step = 36800 ; steps/s = 1.62, tokens/s = 44254 (44254 target) ; Learning rate = 0.000461 ; Loss = 2.005803\n",
      "2024-12-09 18:34:26.623000: I runner.py:310] Step = 36900 ; steps/s = 1.63, tokens/s = 43781 (43781 target) ; Learning rate = 0.000460 ; Loss = 1.969373\n",
      "2024-12-09 18:35:28.319000: I runner.py:310] Step = 37000 ; steps/s = 1.62, tokens/s = 44239 (44239 target) ; Learning rate = 0.000460 ; Loss = 1.972633\n",
      "2024-12-09 18:36:30.035000: I runner.py:310] Step = 37100 ; steps/s = 1.62, tokens/s = 44206 (44206 target) ; Learning rate = 0.000459 ; Loss = 1.955356\n",
      "2024-12-09 18:37:31.790000: I runner.py:310] Step = 37200 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000458 ; Loss = 1.972622\n",
      "2024-12-09 18:38:33.177000: I runner.py:310] Step = 37300 ; steps/s = 1.63, tokens/s = 43742 (43742 target) ; Learning rate = 0.000458 ; Loss = 1.929520\n",
      "2024-12-09 18:39:34.905000: I runner.py:310] Step = 37400 ; steps/s = 1.62, tokens/s = 44208 (44208 target) ; Learning rate = 0.000457 ; Loss = 1.970185\n",
      "2024-12-09 18:40:36.586000: I runner.py:310] Step = 37500 ; steps/s = 1.62, tokens/s = 44243 (44243 target) ; Learning rate = 0.000456 ; Loss = 1.995596\n",
      "2024-12-09 18:41:37.804000: I runner.py:310] Step = 37600 ; steps/s = 1.63, tokens/s = 43845 (43845 target) ; Learning rate = 0.000456 ; Loss = 1.968756\n",
      "2024-12-09 18:42:39.566000: I runner.py:310] Step = 37700 ; steps/s = 1.62, tokens/s = 44187 (44187 target) ; Learning rate = 0.000455 ; Loss = 1.969349\n",
      "2024-12-09 18:43:41.335000: I runner.py:310] Step = 37800 ; steps/s = 1.62, tokens/s = 44194 (44194 target) ; Learning rate = 0.000455 ; Loss = 1.969226\n",
      "2024-12-09 18:44:43.058000: I runner.py:310] Step = 37900 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000454 ; Loss = 1.952075\n",
      "2024-12-09 18:45:44.348000: I runner.py:310] Step = 38000 ; steps/s = 1.63, tokens/s = 43800 (43800 target) ; Learning rate = 0.000453 ; Loss = 1.982616\n",
      "2024-12-09 18:46:45.978000: I runner.py:310] Step = 38100 ; steps/s = 1.62, tokens/s = 44295 (44295 target) ; Learning rate = 0.000453 ; Loss = 1.943055\n",
      "2024-12-09 18:47:47.641000: I runner.py:310] Step = 38200 ; steps/s = 1.62, tokens/s = 44250 (44250 target) ; Learning rate = 0.000452 ; Loss = 1.963891\n",
      "2024-12-09 18:48:48.995000: I runner.py:310] Step = 38300 ; steps/s = 1.63, tokens/s = 43746 (43746 target) ; Learning rate = 0.000452 ; Loss = 1.974773\n",
      "2024-12-09 18:49:50.700000: I runner.py:310] Step = 38400 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000451 ; Loss = 1.947359\n",
      "2024-12-09 18:50:52.380000: I runner.py:310] Step = 38500 ; steps/s = 1.62, tokens/s = 44255 (44255 target) ; Learning rate = 0.000450 ; Loss = 1.955510\n",
      "2024-12-09 18:51:54.080000: I runner.py:310] Step = 38600 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000450 ; Loss = 1.985069\n",
      "2024-12-09 18:52:55.474000: I runner.py:310] Step = 38700 ; steps/s = 1.63, tokens/s = 43726 (43726 target) ; Learning rate = 0.000449 ; Loss = 1.994579\n",
      "2024-12-09 18:53:57.040000: I runner.py:310] Step = 38800 ; steps/s = 1.62, tokens/s = 44327 (44327 target) ; Learning rate = 0.000449 ; Loss = 1.958245\n",
      "2024-12-09 18:54:58.747000: I runner.py:310] Step = 38900 ; steps/s = 1.62, tokens/s = 44218 (44218 target) ; Learning rate = 0.000448 ; Loss = 1.954640\n",
      "2024-12-09 18:56:00.026000: I runner.py:310] Step = 39000 ; steps/s = 1.63, tokens/s = 43807 (43807 target) ; Learning rate = 0.000448 ; Loss = 1.999096\n",
      "2024-12-09 18:57:01.735000: I runner.py:310] Step = 39100 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000447 ; Loss = 1.947419\n",
      "2024-12-09 18:58:03.407000: I runner.py:310] Step = 39200 ; steps/s = 1.62, tokens/s = 44250 (44250 target) ; Learning rate = 0.000446 ; Loss = 1.937238\n",
      "2024-12-09 18:59:05.065000: I runner.py:310] Step = 39300 ; steps/s = 1.62, tokens/s = 44281 (44281 target) ; Learning rate = 0.000446 ; Loss = 1.959969\n",
      "2024-12-09 19:00:06.350000: I runner.py:310] Step = 39400 ; steps/s = 1.63, tokens/s = 43800 (43800 target) ; Learning rate = 0.000445 ; Loss = 1.920814\n",
      "2024-12-09 19:01:07.998000: I runner.py:310] Step = 39500 ; steps/s = 1.62, tokens/s = 44274 (44274 target) ; Learning rate = 0.000445 ; Loss = 1.967220\n",
      "2024-12-09 19:02:09.688000: I runner.py:310] Step = 39600 ; steps/s = 1.62, tokens/s = 44236 (44236 target) ; Learning rate = 0.000444 ; Loss = 1.976356\n",
      "2024-12-09 19:03:10.967000: I runner.py:310] Step = 39700 ; steps/s = 1.63, tokens/s = 43806 (43806 target) ; Learning rate = 0.000444 ; Loss = 1.961663\n",
      "2024-12-09 19:04:12.743000: I runner.py:310] Step = 39800 ; steps/s = 1.62, tokens/s = 44177 (44177 target) ; Learning rate = 0.000443 ; Loss = 1.929437\n",
      "2024-12-09 19:05:14.463000: I runner.py:310] Step = 39900 ; steps/s = 1.62, tokens/s = 44217 (44217 target) ; Learning rate = 0.000442 ; Loss = 1.953802\n",
      "2024-12-09 19:06:16.292000: I runner.py:310] Step = 40000 ; steps/s = 1.62, tokens/s = 44145 (44145 target) ; Learning rate = 0.000442 ; Loss = 1.952508\n",
      "2024-12-09 19:06:18.214000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-40000\n",
      "2024-12-09 19:06:18.214000: I training.py:192] Running evaluation for step 40000\n",
      "2024-12-09 19:09:16.273000: I training.py:192] Evaluation result for step 40000: loss = 2.350044 ; perplexity = 10.486034\n",
      "2024-12-09 19:10:17.557000: I runner.py:310] Step = 40100 ; steps/s = 1.63, tokens/s = 43786 (43786 target) ; Learning rate = 0.000441 ; Loss = 1.968217\n",
      "2024-12-09 19:11:19.277000: I runner.py:310] Step = 40200 ; steps/s = 1.62, tokens/s = 44222 (44222 target) ; Learning rate = 0.000441 ; Loss = 1.961422\n",
      "2024-12-09 19:12:21.057000: I runner.py:310] Step = 40300 ; steps/s = 1.62, tokens/s = 44187 (44187 target) ; Learning rate = 0.000440 ; Loss = 1.942069\n",
      "2024-12-09 19:13:22.418000: I runner.py:310] Step = 40400 ; steps/s = 1.63, tokens/s = 43741 (43741 target) ; Learning rate = 0.000440 ; Loss = 1.967941\n",
      "2024-12-09 19:14:24.170000: I runner.py:310] Step = 40500 ; steps/s = 1.62, tokens/s = 44196 (44196 target) ; Learning rate = 0.000439 ; Loss = 1.932229\n",
      "2024-12-09 19:15:25.978000: I runner.py:310] Step = 40600 ; steps/s = 1.62, tokens/s = 44165 (44165 target) ; Learning rate = 0.000439 ; Loss = 1.944232\n",
      "2024-12-09 19:16:27.757000: I runner.py:310] Step = 40700 ; steps/s = 1.62, tokens/s = 44175 (44175 target) ; Learning rate = 0.000438 ; Loss = 1.940895\n",
      "2024-12-09 19:17:29.139000: I runner.py:310] Step = 40800 ; steps/s = 1.63, tokens/s = 43729 (43729 target) ; Learning rate = 0.000438 ; Loss = 1.969403\n",
      "2024-12-09 19:18:30.895000: I runner.py:310] Step = 40900 ; steps/s = 1.62, tokens/s = 44187 (44187 target) ; Learning rate = 0.000437 ; Loss = 1.944871\n",
      "2024-12-09 19:19:32.577000: I runner.py:310] Step = 41000 ; steps/s = 1.62, tokens/s = 44244 (44244 target) ; Learning rate = 0.000437 ; Loss = 1.943070\n",
      "2024-12-09 19:20:33.949000: I runner.py:310] Step = 41100 ; steps/s = 1.63, tokens/s = 43746 (43746 target) ; Learning rate = 0.000436 ; Loss = 1.952775\n",
      "2024-12-09 19:21:35.670000: I runner.py:310] Step = 41200 ; steps/s = 1.62, tokens/s = 44215 (44215 target) ; Learning rate = 0.000435 ; Loss = 1.914016\n",
      "2024-12-09 19:22:37.402000: I runner.py:310] Step = 41300 ; steps/s = 1.62, tokens/s = 44211 (44211 target) ; Learning rate = 0.000435 ; Loss = 1.946734\n",
      "2024-12-09 19:23:39.097000: I runner.py:310] Step = 41400 ; steps/s = 1.62, tokens/s = 44235 (44235 target) ; Learning rate = 0.000434 ; Loss = 1.945609\n",
      "2024-12-09 19:24:40.387000: I runner.py:310] Step = 41500 ; steps/s = 1.63, tokens/s = 43808 (43808 target) ; Learning rate = 0.000434 ; Loss = 1.914809\n",
      "2024-12-09 19:25:42.144000: I runner.py:310] Step = 41600 ; steps/s = 1.62, tokens/s = 44181 (44181 target) ; Learning rate = 0.000433 ; Loss = 1.954381\n",
      "2024-12-09 19:26:43.914000: I runner.py:310] Step = 41700 ; steps/s = 1.62, tokens/s = 44193 (44193 target) ; Learning rate = 0.000433 ; Loss = 1.959827\n",
      "2024-12-09 19:27:45.300000: I runner.py:310] Step = 41800 ; steps/s = 1.63, tokens/s = 43698 (43698 target) ; Learning rate = 0.000432 ; Loss = 1.967996\n",
      "2024-12-09 19:28:46.986000: I runner.py:310] Step = 41900 ; steps/s = 1.62, tokens/s = 44252 (44252 target) ; Learning rate = 0.000432 ; Loss = 1.916915\n",
      "2024-12-09 19:29:48.715000: I runner.py:310] Step = 42000 ; steps/s = 1.62, tokens/s = 44215 (44215 target) ; Learning rate = 0.000431 ; Loss = 1.932679\n",
      "2024-12-09 19:30:50.059000: I runner.py:310] Step = 42100 ; steps/s = 1.63, tokens/s = 43762 (43762 target) ; Learning rate = 0.000431 ; Loss = 1.945938\n",
      "2024-12-09 19:31:51.784000: I runner.py:310] Step = 42200 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000430 ; Loss = 1.947802\n",
      "2024-12-09 19:32:53.476000: I runner.py:310] Step = 42300 ; steps/s = 1.62, tokens/s = 44239 (44239 target) ; Learning rate = 0.000430 ; Loss = 1.921092\n",
      "2024-12-09 19:33:55.228000: I runner.py:310] Step = 42400 ; steps/s = 1.62, tokens/s = 44188 (44188 target) ; Learning rate = 0.000429 ; Loss = 1.926994\n",
      "2024-12-09 19:34:56.528000: I runner.py:310] Step = 42500 ; steps/s = 1.63, tokens/s = 43784 (43784 target) ; Learning rate = 0.000429 ; Loss = 1.897465\n",
      "2024-12-09 19:35:58.290000: I runner.py:310] Step = 42600 ; steps/s = 1.62, tokens/s = 44191 (44191 target) ; Learning rate = 0.000428 ; Loss = 1.944804\n",
      "2024-12-09 19:37:00.044000: I runner.py:310] Step = 42700 ; steps/s = 1.62, tokens/s = 44203 (44203 target) ; Learning rate = 0.000428 ; Loss = 1.970213\n",
      "2024-12-09 19:38:01.342000: I runner.py:310] Step = 42800 ; steps/s = 1.63, tokens/s = 43775 (43775 target) ; Learning rate = 0.000427 ; Loss = 1.940877\n",
      "2024-12-09 19:39:03.073000: I runner.py:310] Step = 42900 ; steps/s = 1.62, tokens/s = 44213 (44213 target) ; Learning rate = 0.000427 ; Loss = 1.917918\n",
      "2024-12-09 19:40:04.780000: I runner.py:310] Step = 43000 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000426 ; Loss = 1.940845\n",
      "2024-12-09 19:41:06.492000: I runner.py:310] Step = 43100 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000426 ; Loss = 1.965702\n",
      "2024-12-09 19:42:07.847000: I runner.py:310] Step = 43200 ; steps/s = 1.63, tokens/s = 43748 (43748 target) ; Learning rate = 0.000425 ; Loss = 1.903603\n",
      "2024-12-09 19:43:09.537000: I runner.py:310] Step = 43300 ; steps/s = 1.62, tokens/s = 44237 (44237 target) ; Learning rate = 0.000425 ; Loss = 1.928507\n",
      "2024-12-09 19:44:11.227000: I runner.py:310] Step = 43400 ; steps/s = 1.62, tokens/s = 44239 (44239 target) ; Learning rate = 0.000424 ; Loss = 1.945261\n",
      "2024-12-09 19:45:12.531000: I runner.py:310] Step = 43500 ; steps/s = 1.63, tokens/s = 43789 (43789 target) ; Learning rate = 0.000424 ; Loss = 1.920271\n",
      "2024-12-09 19:46:14.273000: I runner.py:310] Step = 43600 ; steps/s = 1.62, tokens/s = 44194 (44194 target) ; Learning rate = 0.000423 ; Loss = 1.914823\n",
      "2024-12-09 19:47:16.041000: I runner.py:310] Step = 43700 ; steps/s = 1.62, tokens/s = 44178 (44178 target) ; Learning rate = 0.000423 ; Loss = 1.936004\n",
      "2024-12-09 19:48:17.797000: I runner.py:310] Step = 43800 ; steps/s = 1.62, tokens/s = 44209 (44209 target) ; Learning rate = 0.000422 ; Loss = 1.968543\n",
      "2024-12-09 19:49:19.105000: I runner.py:310] Step = 43900 ; steps/s = 1.63, tokens/s = 43778 (43778 target) ; Learning rate = 0.000422 ; Loss = 1.948966\n",
      "2024-12-09 19:50:20.873000: I runner.py:310] Step = 44000 ; steps/s = 1.62, tokens/s = 44183 (44183 target) ; Learning rate = 0.000421 ; Loss = 1.909080\n",
      "2024-12-09 19:51:22.565000: I runner.py:310] Step = 44100 ; steps/s = 1.62, tokens/s = 44244 (44244 target) ; Learning rate = 0.000421 ; Loss = 1.935169\n",
      "2024-12-09 19:52:23.932000: I runner.py:310] Step = 44200 ; steps/s = 1.63, tokens/s = 43737 (43737 target) ; Learning rate = 0.000420 ; Loss = 1.924949\n",
      "2024-12-09 19:53:25.583000: I runner.py:310] Step = 44300 ; steps/s = 1.62, tokens/s = 44280 (44280 target) ; Learning rate = 0.000420 ; Loss = 1.934808\n",
      "2024-12-09 19:54:27.324000: I runner.py:310] Step = 44400 ; steps/s = 1.62, tokens/s = 44203 (44203 target) ; Learning rate = 0.000419 ; Loss = 1.914747\n",
      "2024-12-09 19:55:29.096000: I runner.py:310] Step = 44500 ; steps/s = 1.62, tokens/s = 44176 (44176 target) ; Learning rate = 0.000419 ; Loss = 1.918298\n",
      "2024-12-09 19:56:30.397000: I runner.py:310] Step = 44600 ; steps/s = 1.63, tokens/s = 43779 (43779 target) ; Learning rate = 0.000419 ; Loss = 1.874418\n",
      "2024-12-09 19:57:32.234000: I runner.py:310] Step = 44700 ; steps/s = 1.62, tokens/s = 44143 (44143 target) ; Learning rate = 0.000418 ; Loss = 1.922663\n",
      "2024-12-09 19:58:33.970000: I runner.py:310] Step = 44800 ; steps/s = 1.62, tokens/s = 44193 (44193 target) ; Learning rate = 0.000418 ; Loss = 1.937819\n",
      "2024-12-09 19:59:35.314000: I runner.py:310] Step = 44900 ; steps/s = 1.63, tokens/s = 43756 (43756 target) ; Learning rate = 0.000417 ; Loss = 1.909699\n",
      "2024-12-09 20:00:37.024000: I runner.py:310] Step = 45000 ; steps/s = 1.62, tokens/s = 44231 (44231 target) ; Learning rate = 0.000417 ; Loss = 1.918796\n",
      "2024-12-09 20:00:37.025000: I training.py:192] Running evaluation for step 45000\n",
      "2024-12-09 20:03:29.775000: I training.py:192] Evaluation result for step 45000: loss = 2.379646 ; perplexity = 10.801074\n",
      "2024-12-09 20:04:31.314000: I runner.py:310] Step = 45100 ; steps/s = 1.63, tokens/s = 44361 (44361 target) ; Learning rate = 0.000416 ; Loss = 1.947440\n",
      "2024-12-09 20:05:33.057000: I runner.py:310] Step = 45200 ; steps/s = 1.62, tokens/s = 44201 (44201 target) ; Learning rate = 0.000416 ; Loss = 1.939971\n",
      "2024-12-09 20:06:34.433000: I runner.py:310] Step = 45300 ; steps/s = 1.63, tokens/s = 43742 (43742 target) ; Learning rate = 0.000415 ; Loss = 1.881596\n",
      "2024-12-09 20:07:36.130000: I runner.py:310] Step = 45400 ; steps/s = 1.62, tokens/s = 44233 (44233 target) ; Learning rate = 0.000415 ; Loss = 1.933448\n",
      "2024-12-09 20:08:37.906000: I runner.py:310] Step = 45500 ; steps/s = 1.62, tokens/s = 44177 (44177 target) ; Learning rate = 0.000414 ; Loss = 1.955012\n",
      "2024-12-09 20:09:39.232000: I runner.py:310] Step = 45600 ; steps/s = 1.63, tokens/s = 43760 (43760 target) ; Learning rate = 0.000414 ; Loss = 1.900291\n",
      "2024-12-09 20:10:40.963000: I runner.py:310] Step = 45700 ; steps/s = 1.62, tokens/s = 44213 (44213 target) ; Learning rate = 0.000413 ; Loss = 1.918936\n",
      "2024-12-09 20:11:42.725000: I runner.py:310] Step = 45800 ; steps/s = 1.62, tokens/s = 44189 (44189 target) ; Learning rate = 0.000413 ; Loss = 1.926550\n",
      "2024-12-09 20:12:44.457000: I runner.py:310] Step = 45900 ; steps/s = 1.62, tokens/s = 44199 (44199 target) ; Learning rate = 0.000413 ; Loss = 1.941741\n",
      "2024-12-09 20:13:45.821000: I runner.py:310] Step = 46000 ; steps/s = 1.63, tokens/s = 43758 (43758 target) ; Learning rate = 0.000412 ; Loss = 1.947499\n",
      "2024-12-09 20:14:47.467000: I runner.py:310] Step = 46100 ; steps/s = 1.62, tokens/s = 44268 (44268 target) ; Learning rate = 0.000412 ; Loss = 1.902356\n",
      "2024-12-09 20:15:49.225000: I runner.py:310] Step = 46200 ; steps/s = 1.62, tokens/s = 44178 (44178 target) ; Learning rate = 0.000411 ; Loss = 1.897247\n",
      "2024-12-09 20:16:50.595000: I runner.py:310] Step = 46300 ; steps/s = 1.63, tokens/s = 43753 (43753 target) ; Learning rate = 0.000411 ; Loss = 1.932818\n",
      "2024-12-09 20:17:52.266000: I runner.py:310] Step = 46400 ; steps/s = 1.62, tokens/s = 44253 (44253 target) ; Learning rate = 0.000410 ; Loss = 1.909456\n",
      "2024-12-09 20:18:54.029000: I runner.py:310] Step = 46500 ; steps/s = 1.62, tokens/s = 44182 (44182 target) ; Learning rate = 0.000410 ; Loss = 1.919829\n",
      "2024-12-09 20:19:55.744000: I runner.py:310] Step = 46600 ; steps/s = 1.62, tokens/s = 44231 (44231 target) ; Learning rate = 0.000409 ; Loss = 1.915203\n",
      "2024-12-09 20:20:57.008000: I runner.py:310] Step = 46700 ; steps/s = 1.63, tokens/s = 43813 (43813 target) ; Learning rate = 0.000409 ; Loss = 1.951088\n",
      "2024-12-09 20:21:58.750000: I runner.py:310] Step = 46800 ; steps/s = 1.62, tokens/s = 44213 (44213 target) ; Learning rate = 0.000409 ; Loss = 1.903499\n",
      "2024-12-09 20:23:00.534000: I runner.py:310] Step = 46900 ; steps/s = 1.62, tokens/s = 44168 (44168 target) ; Learning rate = 0.000408 ; Loss = 1.910174\n",
      "2024-12-09 20:24:01.875000: I runner.py:310] Step = 47000 ; steps/s = 1.63, tokens/s = 43748 (43748 target) ; Learning rate = 0.000408 ; Loss = 1.893154\n",
      "2024-12-09 20:25:03.597000: I runner.py:310] Step = 47100 ; steps/s = 1.62, tokens/s = 44236 (44236 target) ; Learning rate = 0.000407 ; Loss = 1.908888\n",
      "2024-12-09 20:26:05.290000: I runner.py:310] Step = 47200 ; steps/s = 1.62, tokens/s = 44218 (44218 target) ; Learning rate = 0.000407 ; Loss = 1.943372\n",
      "2024-12-09 20:27:07.036000: I runner.py:310] Step = 47300 ; steps/s = 1.62, tokens/s = 44202 (44202 target) ; Learning rate = 0.000406 ; Loss = 1.939779\n",
      "2024-12-09 20:28:08.366000: I runner.py:310] Step = 47400 ; steps/s = 1.63, tokens/s = 43762 (43762 target) ; Learning rate = 0.000406 ; Loss = 1.939770\n",
      "2024-12-09 20:29:10.215000: I runner.py:310] Step = 47500 ; steps/s = 1.62, tokens/s = 44133 (44133 target) ; Learning rate = 0.000406 ; Loss = 1.895143\n",
      "2024-12-09 20:30:12.004000: I runner.py:310] Step = 47600 ; steps/s = 1.62, tokens/s = 44166 (44166 target) ; Learning rate = 0.000405 ; Loss = 1.930011\n",
      "2024-12-09 20:31:13.383000: I runner.py:310] Step = 47700 ; steps/s = 1.63, tokens/s = 43725 (43725 target) ; Learning rate = 0.000405 ; Loss = 1.925028\n",
      "2024-12-09 20:32:15.122000: I runner.py:310] Step = 47800 ; steps/s = 1.62, tokens/s = 44211 (44211 target) ; Learning rate = 0.000404 ; Loss = 1.911458\n",
      "2024-12-09 20:33:16.852000: I runner.py:310] Step = 47900 ; steps/s = 1.62, tokens/s = 44216 (44216 target) ; Learning rate = 0.000404 ; Loss = 1.928223\n",
      "2024-12-09 20:34:18.556000: I runner.py:310] Step = 48000 ; steps/s = 1.62, tokens/s = 44230 (44230 target) ; Learning rate = 0.000403 ; Loss = 1.916301\n",
      "2024-12-09 20:35:19.909000: I runner.py:310] Step = 48100 ; steps/s = 1.63, tokens/s = 43757 (43757 target) ; Learning rate = 0.000403 ; Loss = 1.945740\n",
      "2024-12-09 20:36:21.641000: I runner.py:310] Step = 48200 ; steps/s = 1.62, tokens/s = 44189 (44189 target) ; Learning rate = 0.000403 ; Loss = 1.892851\n",
      "2024-12-09 20:37:23.422000: I runner.py:310] Step = 48300 ; steps/s = 1.62, tokens/s = 44172 (44172 target) ; Learning rate = 0.000402 ; Loss = 1.905253\n",
      "2024-12-09 20:38:24.723000: I runner.py:310] Step = 48400 ; steps/s = 1.63, tokens/s = 43803 (43803 target) ; Learning rate = 0.000402 ; Loss = 1.916347\n",
      "2024-12-09 20:39:26.410000: I runner.py:310] Step = 48500 ; steps/s = 1.62, tokens/s = 44239 (44239 target) ; Learning rate = 0.000401 ; Loss = 1.893020\n",
      "2024-12-09 20:40:28.095000: I runner.py:310] Step = 48600 ; steps/s = 1.62, tokens/s = 44242 (44242 target) ; Learning rate = 0.000401 ; Loss = 1.905146\n",
      "2024-12-09 20:41:29.835000: I runner.py:310] Step = 48700 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000401 ; Loss = 1.907022\n",
      "2024-12-09 20:42:31.155000: I runner.py:310] Step = 48800 ; steps/s = 1.63, tokens/s = 43772 (43772 target) ; Learning rate = 0.000400 ; Loss = 1.918842\n",
      "2024-12-09 20:43:32.822000: I runner.py:310] Step = 48900 ; steps/s = 1.62, tokens/s = 44250 (44250 target) ; Learning rate = 0.000400 ; Loss = 1.884484\n",
      "2024-12-09 20:44:34.561000: I runner.py:310] Step = 49000 ; steps/s = 1.62, tokens/s = 44214 (44214 target) ; Learning rate = 0.000399 ; Loss = 1.902197\n",
      "2024-12-09 20:45:35.841000: I runner.py:310] Step = 49100 ; steps/s = 1.63, tokens/s = 43797 (43797 target) ; Learning rate = 0.000399 ; Loss = 1.879282\n",
      "2024-12-09 20:46:37.571000: I runner.py:310] Step = 49200 ; steps/s = 1.62, tokens/s = 44213 (44213 target) ; Learning rate = 0.000398 ; Loss = 1.921047\n",
      "2024-12-09 20:47:39.242000: I runner.py:310] Step = 49300 ; steps/s = 1.62, tokens/s = 44247 (44247 target) ; Learning rate = 0.000398 ; Loss = 1.919945\n",
      "2024-12-09 20:48:40.971000: I runner.py:310] Step = 49400 ; steps/s = 1.62, tokens/s = 44219 (44219 target) ; Learning rate = 0.000398 ; Loss = 1.945501\n",
      "2024-12-09 20:49:42.304000: I runner.py:310] Step = 49500 ; steps/s = 1.63, tokens/s = 43746 (43746 target) ; Learning rate = 0.000397 ; Loss = 1.929150\n",
      "2024-12-09 20:50:43.978000: I runner.py:310] Step = 49600 ; steps/s = 1.62, tokens/s = 44271 (44271 target) ; Learning rate = 0.000397 ; Loss = 1.893848\n",
      "2024-12-09 20:51:45.676000: I runner.py:310] Step = 49700 ; steps/s = 1.62, tokens/s = 44240 (44240 target) ; Learning rate = 0.000396 ; Loss = 1.892997\n",
      "2024-12-09 20:52:46.989000: I runner.py:310] Step = 49800 ; steps/s = 1.63, tokens/s = 43778 (43778 target) ; Learning rate = 0.000396 ; Loss = 1.922767\n",
      "2024-12-09 20:53:48.776000: I runner.py:310] Step = 49900 ; steps/s = 1.62, tokens/s = 44167 (44167 target) ; Learning rate = 0.000396 ; Loss = 1.888319\n",
      "2024-12-09 20:54:50.553000: I runner.py:310] Step = 50000 ; steps/s = 1.62, tokens/s = 44171 (44171 target) ; Learning rate = 0.000395 ; Loss = 1.904772\n",
      "2024-12-09 20:54:52.555000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-50000\n",
      "2024-12-09 20:54:52.555000: I training.py:192] Running evaluation for step 50000\n",
      "2024-12-09 20:57:53.362000: I training.py:192] Evaluation result for step 50000: loss = 2.400255 ; perplexity = 11.025985\n",
      "2024-12-09 20:58:54.668000: I runner.py:310] Step = 50100 ; steps/s = 1.63, tokens/s = 44121 (44121 target) ; Learning rate = 0.000395 ; Loss = 1.933314\n",
      "2024-12-09 20:59:56.296000: I runner.py:310] Step = 50200 ; steps/s = 1.62, tokens/s = 43977 (43977 target) ; Learning rate = 0.000394 ; Loss = 1.874476\n",
      "2024-12-09 21:00:58.034000: I runner.py:310] Step = 50300 ; steps/s = 1.62, tokens/s = 44206 (44206 target) ; Learning rate = 0.000394 ; Loss = 1.920626\n",
      "2024-12-09 21:01:59.823000: I runner.py:310] Step = 50400 ; steps/s = 1.62, tokens/s = 44152 (44152 target) ; Learning rate = 0.000394 ; Loss = 1.923710\n",
      "2024-12-09 21:03:01.154000: I runner.py:310] Step = 50500 ; steps/s = 1.63, tokens/s = 43769 (43769 target) ; Learning rate = 0.000393 ; Loss = 1.863059\n",
      "2024-12-09 21:04:02.836000: I runner.py:310] Step = 50600 ; steps/s = 1.62, tokens/s = 44240 (44240 target) ; Learning rate = 0.000393 ; Loss = 1.909444\n",
      "2024-12-09 21:05:04.586000: I runner.py:310] Step = 50700 ; steps/s = 1.62, tokens/s = 44200 (44200 target) ; Learning rate = 0.000393 ; Loss = 1.932166\n",
      "2024-12-09 21:06:05.948000: I runner.py:310] Step = 50800 ; steps/s = 1.63, tokens/s = 43754 (43754 target) ; Learning rate = 0.000392 ; Loss = 1.884014\n",
      "2024-12-09 21:07:07.665000: I runner.py:310] Step = 50900 ; steps/s = 1.62, tokens/s = 44230 (44230 target) ; Learning rate = 0.000392 ; Loss = 1.877014\n",
      "2024-12-09 21:08:09.407000: I runner.py:310] Step = 51000 ; steps/s = 1.62, tokens/s = 44208 (44208 target) ; Learning rate = 0.000391 ; Loss = 1.919059\n",
      "2024-12-09 21:09:11.146000: I runner.py:310] Step = 51100 ; steps/s = 1.62, tokens/s = 44188 (44188 target) ; Learning rate = 0.000391 ; Loss = 1.928786\n",
      "2024-12-09 21:10:12.525000: I runner.py:310] Step = 51200 ; steps/s = 1.63, tokens/s = 43734 (43734 target) ; Learning rate = 0.000391 ; Loss = 1.875937\n",
      "2024-12-09 21:11:14.253000: I runner.py:310] Step = 51300 ; steps/s = 1.62, tokens/s = 44218 (44218 target) ; Learning rate = 0.000390 ; Loss = 1.916273\n",
      "2024-12-09 21:12:15.924000: I runner.py:310] Step = 51400 ; steps/s = 1.62, tokens/s = 44227 (44227 target) ; Learning rate = 0.000390 ; Loss = 1.912236\n",
      "2024-12-09 21:13:17.306000: I runner.py:310] Step = 51500 ; steps/s = 1.63, tokens/s = 43750 (43750 target) ; Learning rate = 0.000389 ; Loss = 1.901122\n",
      "2024-12-09 21:14:19.071000: I runner.py:310] Step = 51600 ; steps/s = 1.62, tokens/s = 44198 (44198 target) ; Learning rate = 0.000389 ; Loss = 1.902936\n",
      "2024-12-09 21:15:20.702000: I runner.py:310] Step = 51700 ; steps/s = 1.62, tokens/s = 44296 (44296 target) ; Learning rate = 0.000389 ; Loss = 1.910498\n",
      "2024-12-09 21:16:22.380000: I runner.py:310] Step = 51800 ; steps/s = 1.62, tokens/s = 44230 (44230 target) ; Learning rate = 0.000388 ; Loss = 1.909002\n",
      "2024-12-09 21:17:23.710000: I runner.py:310] Step = 51900 ; steps/s = 1.63, tokens/s = 43751 (43751 target) ; Learning rate = 0.000388 ; Loss = 1.921950\n",
      "2024-12-09 21:18:25.480000: I runner.py:310] Step = 52000 ; steps/s = 1.62, tokens/s = 44185 (44185 target) ; Learning rate = 0.000388 ; Loss = 1.877369\n",
      "2024-12-09 21:19:27.217000: I runner.py:310] Step = 52100 ; steps/s = 1.62, tokens/s = 44208 (44208 target) ; Learning rate = 0.000387 ; Loss = 1.911891\n",
      "2024-12-09 21:20:28.586000: I runner.py:310] Step = 52200 ; steps/s = 1.63, tokens/s = 43742 (43742 target) ; Learning rate = 0.000387 ; Loss = 1.885325\n",
      "2024-12-09 21:21:30.303000: I runner.py:310] Step = 52300 ; steps/s = 1.62, tokens/s = 44227 (44227 target) ; Learning rate = 0.000386 ; Loss = 1.886384\n",
      "2024-12-09 21:22:32.001000: I runner.py:310] Step = 52400 ; steps/s = 1.62, tokens/s = 44216 (44216 target) ; Learning rate = 0.000386 ; Loss = 1.911845\n",
      "2024-12-09 21:23:33.682000: I runner.py:310] Step = 52500 ; steps/s = 1.62, tokens/s = 44259 (44259 target) ; Learning rate = 0.000386 ; Loss = 1.916887\n",
      "2024-12-09 21:24:35.079000: I runner.py:310] Step = 52600 ; steps/s = 1.63, tokens/s = 43718 (43718 target) ; Learning rate = 0.000385 ; Loss = 1.861544\n",
      "2024-12-09 21:25:36.803000: I runner.py:310] Step = 52700 ; steps/s = 1.62, tokens/s = 44224 (44224 target) ; Learning rate = 0.000385 ; Loss = 1.906806\n",
      "2024-12-09 21:26:38.523000: I runner.py:310] Step = 52800 ; steps/s = 1.62, tokens/s = 44219 (44219 target) ; Learning rate = 0.000385 ; Loss = 1.920271\n",
      "2024-12-09 21:27:39.830000: I runner.py:310] Step = 52900 ; steps/s = 1.63, tokens/s = 43768 (43768 target) ; Learning rate = 0.000384 ; Loss = 1.902104\n",
      "2024-12-09 21:28:41.531000: I runner.py:310] Step = 53000 ; steps/s = 1.62, tokens/s = 44233 (44233 target) ; Learning rate = 0.000384 ; Loss = 1.897351\n",
      "2024-12-09 21:29:43.272000: I runner.py:310] Step = 53100 ; steps/s = 1.62, tokens/s = 44203 (44203 target) ; Learning rate = 0.000384 ; Loss = 1.878152\n",
      "2024-12-09 21:30:45.067000: I runner.py:310] Step = 53200 ; steps/s = 1.62, tokens/s = 44182 (44182 target) ; Learning rate = 0.000383 ; Loss = 1.893481\n",
      "2024-12-09 21:31:46.457000: I runner.py:310] Step = 53300 ; steps/s = 1.63, tokens/s = 43719 (43719 target) ; Learning rate = 0.000383 ; Loss = 1.910593\n",
      "2024-12-09 21:32:48.230000: I runner.py:310] Step = 53400 ; steps/s = 1.62, tokens/s = 44177 (44177 target) ; Learning rate = 0.000382 ; Loss = 1.874126\n",
      "2024-12-09 21:33:50.001000: I runner.py:310] Step = 53500 ; steps/s = 1.62, tokens/s = 44182 (44182 target) ; Learning rate = 0.000382 ; Loss = 1.891499\n",
      "2024-12-09 21:34:51.300000: I runner.py:310] Step = 53600 ; steps/s = 1.63, tokens/s = 43779 (43779 target) ; Learning rate = 0.000382 ; Loss = 1.906550\n",
      "2024-12-09 21:35:52.987000: I runner.py:310] Step = 53700 ; steps/s = 1.62, tokens/s = 44240 (44240 target) ; Learning rate = 0.000381 ; Loss = 1.881475\n",
      "2024-12-09 21:36:54.750000: I runner.py:310] Step = 53800 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000381 ; Loss = 1.892184\n",
      "2024-12-09 21:37:56.425000: I runner.py:310] Step = 53900 ; steps/s = 1.62, tokens/s = 44250 (44250 target) ; Learning rate = 0.000381 ; Loss = 1.882831\n",
      "2024-12-09 21:38:57.768000: I runner.py:310] Step = 54000 ; steps/s = 1.63, tokens/s = 43757 (43757 target) ; Learning rate = 0.000380 ; Loss = 1.909440\n",
      "2024-12-09 21:39:59.491000: I runner.py:310] Step = 54100 ; steps/s = 1.62, tokens/s = 44223 (44223 target) ; Learning rate = 0.000380 ; Loss = 1.879454\n",
      "2024-12-09 21:41:01.232000: I runner.py:310] Step = 54200 ; steps/s = 1.62, tokens/s = 44191 (44191 target) ; Learning rate = 0.000380 ; Loss = 1.883366\n",
      "2024-12-09 21:42:02.597000: I runner.py:310] Step = 54300 ; steps/s = 1.63, tokens/s = 43750 (43750 target) ; Learning rate = 0.000379 ; Loss = 1.895467\n",
      "2024-12-09 21:43:04.291000: I runner.py:310] Step = 54400 ; steps/s = 1.62, tokens/s = 44224 (44224 target) ; Learning rate = 0.000379 ; Loss = 1.888194\n",
      "2024-12-09 21:44:06.008000: I runner.py:310] Step = 54500 ; steps/s = 1.62, tokens/s = 44223 (44223 target) ; Learning rate = 0.000379 ; Loss = 1.872303\n",
      "2024-12-09 21:45:07.747000: I runner.py:310] Step = 54600 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000378 ; Loss = 1.894495\n",
      "2024-12-09 21:46:09.031000: I runner.py:310] Step = 54700 ; steps/s = 1.63, tokens/s = 43801 (43801 target) ; Learning rate = 0.000378 ; Loss = 1.858509\n",
      "2024-12-09 21:47:10.773000: I runner.py:310] Step = 54800 ; steps/s = 1.62, tokens/s = 44199 (44199 target) ; Learning rate = 0.000378 ; Loss = 1.890099\n",
      "2024-12-09 21:48:12.524000: I runner.py:310] Step = 54900 ; steps/s = 1.62, tokens/s = 44194 (44194 target) ; Learning rate = 0.000377 ; Loss = 1.905012\n",
      "2024-12-09 21:49:13.858000: I runner.py:310] Step = 55000 ; steps/s = 1.63, tokens/s = 43771 (43771 target) ; Learning rate = 0.000377 ; Loss = 1.886316\n",
      "2024-12-09 21:49:13.860000: I training.py:192] Running evaluation for step 55000\n",
      "2024-12-09 21:52:03.523000: I training.py:192] Evaluation result for step 55000: loss = 2.412028 ; perplexity = 11.156559\n",
      "2024-12-09 21:53:05.168000: I runner.py:310] Step = 55100 ; steps/s = 1.62, tokens/s = 44292 (44292 target) ; Learning rate = 0.000377 ; Loss = 1.901863\n",
      "2024-12-09 21:54:06.927000: I runner.py:310] Step = 55200 ; steps/s = 1.62, tokens/s = 44191 (44191 target) ; Learning rate = 0.000376 ; Loss = 1.902451\n",
      "2024-12-09 21:55:08.638000: I runner.py:310] Step = 55300 ; steps/s = 1.62, tokens/s = 44213 (44213 target) ; Learning rate = 0.000376 ; Loss = 1.913572\n",
      "2024-12-09 21:56:09.985000: I runner.py:310] Step = 55400 ; steps/s = 1.63, tokens/s = 43753 (43753 target) ; Learning rate = 0.000376 ; Loss = 1.910001\n",
      "2024-12-09 21:57:11.719000: I runner.py:310] Step = 55500 ; steps/s = 1.62, tokens/s = 44196 (44196 target) ; Learning rate = 0.000375 ; Loss = 1.877300\n",
      "2024-12-09 21:58:13.460000: I runner.py:310] Step = 55600 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000375 ; Loss = 1.872135\n",
      "2024-12-09 21:59:14.872000: I runner.py:310] Step = 55700 ; steps/s = 1.63, tokens/s = 43710 (43710 target) ; Learning rate = 0.000375 ; Loss = 1.870045\n",
      "2024-12-09 22:00:16.636000: I runner.py:310] Step = 55800 ; steps/s = 1.62, tokens/s = 44183 (44183 target) ; Learning rate = 0.000374 ; Loss = 1.874292\n",
      "2024-12-09 22:01:18.395000: I runner.py:310] Step = 55900 ; steps/s = 1.62, tokens/s = 44184 (44184 target) ; Learning rate = 0.000374 ; Loss = 1.888174\n",
      "2024-12-09 22:02:20.187000: I runner.py:310] Step = 56000 ; steps/s = 1.62, tokens/s = 44173 (44173 target) ; Learning rate = 0.000374 ; Loss = 1.906954\n",
      "2024-12-09 22:03:21.584000: I runner.py:310] Step = 56100 ; steps/s = 1.63, tokens/s = 43707 (43707 target) ; Learning rate = 0.000373 ; Loss = 1.908641\n",
      "2024-12-09 22:04:23.306000: I runner.py:310] Step = 56200 ; steps/s = 1.62, tokens/s = 44221 (44221 target) ; Learning rate = 0.000373 ; Loss = 1.881262\n",
      "2024-12-09 22:05:25.055000: I runner.py:310] Step = 56300 ; steps/s = 1.62, tokens/s = 44198 (44198 target) ; Learning rate = 0.000373 ; Loss = 1.880038\n",
      "2024-12-09 22:06:26.422000: I runner.py:310] Step = 56400 ; steps/s = 1.63, tokens/s = 43747 (43747 target) ; Learning rate = 0.000372 ; Loss = 1.895554\n",
      "2024-12-09 22:07:28.144000: I runner.py:310] Step = 56500 ; steps/s = 1.62, tokens/s = 44215 (44215 target) ; Learning rate = 0.000372 ; Loss = 1.896615\n",
      "2024-12-09 22:08:29.928000: I runner.py:310] Step = 56600 ; steps/s = 1.62, tokens/s = 44175 (44175 target) ; Learning rate = 0.000372 ; Loss = 1.880099\n",
      "2024-12-09 22:09:31.734000: I runner.py:310] Step = 56700 ; steps/s = 1.62, tokens/s = 44158 (44158 target) ; Learning rate = 0.000371 ; Loss = 1.898235\n",
      "2024-12-09 22:10:33.132000: I runner.py:310] Step = 56800 ; steps/s = 1.63, tokens/s = 43705 (43705 target) ; Learning rate = 0.000371 ; Loss = 1.850724\n",
      "2024-12-09 22:11:34.903000: I runner.py:310] Step = 56900 ; steps/s = 1.62, tokens/s = 44178 (44178 target) ; Learning rate = 0.000371 ; Loss = 1.885930\n",
      "2024-12-09 22:12:36.616000: I runner.py:310] Step = 57000 ; steps/s = 1.62, tokens/s = 44222 (44222 target) ; Learning rate = 0.000370 ; Loss = 1.887343\n",
      "2024-12-09 22:13:37.955000: I runner.py:310] Step = 57100 ; steps/s = 1.63, tokens/s = 43763 (43763 target) ; Learning rate = 0.000370 ; Loss = 1.862237\n",
      "2024-12-09 22:14:39.650000: I runner.py:310] Step = 57200 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000370 ; Loss = 1.882494\n",
      "2024-12-09 22:15:41.414000: I runner.py:310] Step = 57300 ; steps/s = 1.62, tokens/s = 44202 (44202 target) ; Learning rate = 0.000369 ; Loss = 1.897581\n",
      "2024-12-09 22:16:43.181000: I runner.py:310] Step = 57400 ; steps/s = 1.62, tokens/s = 44189 (44189 target) ; Learning rate = 0.000369 ; Loss = 1.896777\n",
      "2024-12-09 22:17:44.610000: I runner.py:310] Step = 57500 ; steps/s = 1.63, tokens/s = 43693 (43693 target) ; Learning rate = 0.000369 ; Loss = 1.865132\n",
      "2024-12-09 22:18:46.338000: I runner.py:310] Step = 57600 ; steps/s = 1.62, tokens/s = 44216 (44216 target) ; Learning rate = 0.000368 ; Loss = 1.903152\n",
      "2024-12-09 22:19:48.150000: I runner.py:310] Step = 57700 ; steps/s = 1.62, tokens/s = 44144 (44144 target) ; Learning rate = 0.000368 ; Loss = 1.901327\n",
      "2024-12-09 22:20:49.478000: I runner.py:310] Step = 57800 ; steps/s = 1.63, tokens/s = 43768 (43768 target) ; Learning rate = 0.000368 ; Loss = 1.850955\n",
      "2024-12-09 22:21:51.247000: I runner.py:310] Step = 57900 ; steps/s = 1.62, tokens/s = 44184 (44184 target) ; Learning rate = 0.000367 ; Loss = 1.879192\n",
      "2024-12-09 22:22:53.001000: I runner.py:310] Step = 58000 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000367 ; Loss = 1.891427\n",
      "2024-12-09 22:23:54.703000: I runner.py:310] Step = 58100 ; steps/s = 1.62, tokens/s = 44224 (44224 target) ; Learning rate = 0.000367 ; Loss = 1.908946\n",
      "2024-12-09 22:24:56.085000: I runner.py:310] Step = 58200 ; steps/s = 1.63, tokens/s = 43727 (43727 target) ; Learning rate = 0.000366 ; Loss = 1.886110\n",
      "2024-12-09 22:25:57.774000: I runner.py:310] Step = 58300 ; steps/s = 1.62, tokens/s = 44247 (44247 target) ; Learning rate = 0.000366 ; Loss = 1.854441\n",
      "2024-12-09 22:26:59.557000: I runner.py:310] Step = 58400 ; steps/s = 1.62, tokens/s = 44173 (44173 target) ; Learning rate = 0.000366 ; Loss = 1.863338\n",
      "2024-12-09 22:28:00.840000: I runner.py:310] Step = 58500 ; steps/s = 1.63, tokens/s = 43802 (43802 target) ; Learning rate = 0.000365 ; Loss = 1.834413\n",
      "2024-12-09 22:29:02.604000: I runner.py:310] Step = 58600 ; steps/s = 1.62, tokens/s = 44177 (44177 target) ; Learning rate = 0.000365 ; Loss = 1.891306\n",
      "2024-12-09 22:30:04.352000: I runner.py:310] Step = 58700 ; steps/s = 1.62, tokens/s = 44191 (44191 target) ; Learning rate = 0.000365 ; Loss = 1.893813\n",
      "2024-12-09 22:31:05.745000: I runner.py:310] Step = 58800 ; steps/s = 1.63, tokens/s = 43740 (43740 target) ; Learning rate = 0.000365 ; Loss = 1.934169\n",
      "2024-12-09 22:32:07.465000: I runner.py:310] Step = 58900 ; steps/s = 1.62, tokens/s = 44218 (44218 target) ; Learning rate = 0.000364 ; Loss = 1.866310\n",
      "2024-12-09 22:33:09.187000: I runner.py:310] Step = 59000 ; steps/s = 1.62, tokens/s = 44219 (44219 target) ; Learning rate = 0.000364 ; Loss = 1.884144\n",
      "2024-12-09 22:34:10.911000: I runner.py:310] Step = 59100 ; steps/s = 1.62, tokens/s = 44226 (44226 target) ; Learning rate = 0.000364 ; Loss = 1.900014\n",
      "2024-12-09 22:35:12.313000: I runner.py:310] Step = 59200 ; steps/s = 1.63, tokens/s = 43701 (43701 target) ; Learning rate = 0.000363 ; Loss = 1.893985\n",
      "2024-12-09 22:36:14.065000: I runner.py:310] Step = 59300 ; steps/s = 1.62, tokens/s = 44207 (44207 target) ; Learning rate = 0.000363 ; Loss = 1.867517\n",
      "2024-12-09 22:37:15.868000: I runner.py:310] Step = 59400 ; steps/s = 1.62, tokens/s = 44154 (44154 target) ; Learning rate = 0.000363 ; Loss = 1.861075\n",
      "2024-12-09 22:38:17.232000: I runner.py:310] Step = 59500 ; steps/s = 1.63, tokens/s = 43735 (43735 target) ; Learning rate = 0.000362 ; Loss = 1.876062\n",
      "2024-12-09 22:39:18.993000: I runner.py:310] Step = 59600 ; steps/s = 1.62, tokens/s = 44182 (44182 target) ; Learning rate = 0.000362 ; Loss = 1.881502\n",
      "2024-12-09 22:40:20.763000: I runner.py:310] Step = 59700 ; steps/s = 1.62, tokens/s = 44186 (44186 target) ; Learning rate = 0.000362 ; Loss = 1.854932\n",
      "2024-12-09 22:41:22.527000: I runner.py:310] Step = 59800 ; steps/s = 1.62, tokens/s = 44191 (44191 target) ; Learning rate = 0.000361 ; Loss = 1.874346\n",
      "2024-12-09 22:42:23.873000: I runner.py:310] Step = 59900 ; steps/s = 1.63, tokens/s = 43751 (43751 target) ; Learning rate = 0.000361 ; Loss = 1.891452\n",
      "2024-12-09 22:43:25.604000: I runner.py:310] Step = 60000 ; steps/s = 1.62, tokens/s = 44213 (44213 target) ; Learning rate = 0.000361 ; Loss = 1.845306\n",
      "2024-12-09 22:43:27.681000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-60000\n",
      "2024-12-09 22:43:27.681000: I training.py:192] Running evaluation for step 60000\n",
      "2024-12-09 22:46:23.683000: I training.py:192] Evaluation result for step 60000: loss = 2.437896 ; perplexity = 11.448922\n",
      "2024-12-09 22:47:25.263000: I runner.py:310] Step = 60100 ; steps/s = 1.62, tokens/s = 44330 (44330 target) ; Learning rate = 0.000361 ; Loss = 1.876852\n",
      "2024-12-09 22:48:26.593000: I runner.py:310] Step = 60200 ; steps/s = 1.63, tokens/s = 43770 (43770 target) ; Learning rate = 0.000360 ; Loss = 1.865375\n",
      "2024-12-09 22:49:28.329000: I runner.py:310] Step = 60300 ; steps/s = 1.62, tokens/s = 44201 (44201 target) ; Learning rate = 0.000360 ; Loss = 1.877916\n",
      "2024-12-09 22:50:30.058000: I runner.py:310] Step = 60400 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000360 ; Loss = 1.856913\n",
      "2024-12-09 22:51:31.866000: I runner.py:310] Step = 60500 ; steps/s = 1.62, tokens/s = 44154 (44154 target) ; Learning rate = 0.000359 ; Loss = 1.862147\n",
      "2024-12-09 22:52:33.259000: I runner.py:310] Step = 60600 ; steps/s = 1.63, tokens/s = 43736 (43736 target) ; Learning rate = 0.000359 ; Loss = 1.896015\n",
      "2024-12-09 22:53:35.011000: I runner.py:310] Step = 60700 ; steps/s = 1.62, tokens/s = 44198 (44198 target) ; Learning rate = 0.000359 ; Loss = 1.853668\n",
      "2024-12-09 22:54:36.690000: I runner.py:310] Step = 60800 ; steps/s = 1.62, tokens/s = 44233 (44233 target) ; Learning rate = 0.000358 ; Loss = 1.866883\n",
      "2024-12-09 22:55:38.050000: I runner.py:310] Step = 60900 ; steps/s = 1.63, tokens/s = 43750 (43750 target) ; Learning rate = 0.000358 ; Loss = 1.863760\n",
      "2024-12-09 22:56:39.785000: I runner.py:310] Step = 61000 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000358 ; Loss = 1.876117\n",
      "2024-12-09 22:57:41.515000: I runner.py:310] Step = 61100 ; steps/s = 1.62, tokens/s = 44223 (44223 target) ; Learning rate = 0.000358 ; Loss = 1.857173\n",
      "2024-12-09 22:58:43.259000: I runner.py:310] Step = 61200 ; steps/s = 1.62, tokens/s = 44190 (44190 target) ; Learning rate = 0.000357 ; Loss = 1.879775\n",
      "2024-12-09 22:59:44.523000: I runner.py:310] Step = 61300 ; steps/s = 1.63, tokens/s = 43823 (43823 target) ; Learning rate = 0.000357 ; Loss = 1.886331\n",
      "2024-12-09 23:00:46.264000: I runner.py:310] Step = 61400 ; steps/s = 1.62, tokens/s = 44200 (44200 target) ; Learning rate = 0.000357 ; Loss = 1.851837\n",
      "2024-12-09 23:01:47.957000: I runner.py:310] Step = 61500 ; steps/s = 1.62, tokens/s = 44236 (44236 target) ; Learning rate = 0.000356 ; Loss = 1.871934\n",
      "2024-12-09 23:02:49.250000: I runner.py:310] Step = 61600 ; steps/s = 1.63, tokens/s = 43784 (43784 target) ; Learning rate = 0.000356 ; Loss = 1.853765\n",
      "2024-12-09 23:03:50.923000: I runner.py:310] Step = 61700 ; steps/s = 1.62, tokens/s = 44248 (44248 target) ; Learning rate = 0.000356 ; Loss = 1.867641\n",
      "2024-12-09 23:04:52.663000: I runner.py:310] Step = 61800 ; steps/s = 1.62, tokens/s = 44209 (44209 target) ; Learning rate = 0.000356 ; Loss = 1.872173\n",
      "2024-12-09 23:05:54.361000: I runner.py:310] Step = 61900 ; steps/s = 1.62, tokens/s = 44233 (44233 target) ; Learning rate = 0.000355 ; Loss = 1.889611\n",
      "2024-12-09 23:06:55.661000: I runner.py:310] Step = 62000 ; steps/s = 1.63, tokens/s = 43791 (43791 target) ; Learning rate = 0.000355 ; Loss = 1.824781\n",
      "2024-12-09 23:07:57.355000: I runner.py:310] Step = 62100 ; steps/s = 1.62, tokens/s = 44232 (44232 target) ; Learning rate = 0.000355 ; Loss = 1.867696\n",
      "2024-12-09 23:08:59.027000: I runner.py:310] Step = 62200 ; steps/s = 1.62, tokens/s = 44256 (44256 target) ; Learning rate = 0.000354 ; Loss = 1.869210\n",
      "2024-12-09 23:10:00.451000: I runner.py:310] Step = 62300 ; steps/s = 1.63, tokens/s = 43692 (43692 target) ; Learning rate = 0.000354 ; Loss = 1.874553\n",
      "2024-12-09 23:11:02.183000: I runner.py:310] Step = 62400 ; steps/s = 1.62, tokens/s = 44203 (44203 target) ; Learning rate = 0.000354 ; Loss = 1.856639\n",
      "2024-12-09 23:12:03.904000: I runner.py:310] Step = 62500 ; steps/s = 1.62, tokens/s = 44237 (44237 target) ; Learning rate = 0.000354 ; Loss = 1.859259\n",
      "2024-12-09 23:13:05.632000: I runner.py:310] Step = 62600 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000353 ; Loss = 1.869164\n",
      "2024-12-09 23:14:06.941000: I runner.py:310] Step = 62700 ; steps/s = 1.63, tokens/s = 43775 (43775 target) ; Learning rate = 0.000353 ; Loss = 1.896043\n",
      "2024-12-09 23:15:08.654000: I runner.py:310] Step = 62800 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000353 ; Loss = 1.856916\n",
      "2024-12-09 23:16:10.359000: I runner.py:310] Step = 62900 ; steps/s = 1.62, tokens/s = 44236 (44236 target) ; Learning rate = 0.000352 ; Loss = 1.854687\n",
      "2024-12-09 23:17:11.614000: I runner.py:310] Step = 63000 ; steps/s = 1.63, tokens/s = 43809 (43809 target) ; Learning rate = 0.000352 ; Loss = 1.866997\n",
      "2024-12-09 23:18:13.399000: I runner.py:310] Step = 63100 ; steps/s = 1.62, tokens/s = 44179 (44179 target) ; Learning rate = 0.000352 ; Loss = 1.853697\n",
      "2024-12-09 23:19:15.158000: I runner.py:310] Step = 63200 ; steps/s = 1.62, tokens/s = 44197 (44197 target) ; Learning rate = 0.000352 ; Loss = 1.860725\n",
      "2024-12-09 23:20:16.887000: I runner.py:310] Step = 63300 ; steps/s = 1.62, tokens/s = 44209 (44209 target) ; Learning rate = 0.000351 ; Loss = 1.862949\n",
      "2024-12-09 23:21:18.214000: I runner.py:310] Step = 63400 ; steps/s = 1.63, tokens/s = 43775 (43775 target) ; Learning rate = 0.000351 ; Loss = 1.834267\n",
      "2024-12-09 23:22:19.881000: I runner.py:310] Step = 63500 ; steps/s = 1.62, tokens/s = 44259 (44259 target) ; Learning rate = 0.000351 ; Loss = 1.849552\n",
      "2024-12-09 23:23:21.659000: I runner.py:310] Step = 63600 ; steps/s = 1.62, tokens/s = 44168 (44168 target) ; Learning rate = 0.000350 ; Loss = 1.882959\n",
      "2024-12-09 23:24:23.018000: I runner.py:310] Step = 63700 ; steps/s = 1.63, tokens/s = 43742 (43742 target) ; Learning rate = 0.000350 ; Loss = 1.875848\n",
      "2024-12-09 23:25:24.686000: I runner.py:310] Step = 63800 ; steps/s = 1.62, tokens/s = 44254 (44254 target) ; Learning rate = 0.000350 ; Loss = 1.853860\n",
      "2024-12-09 23:26:26.343000: I runner.py:310] Step = 63900 ; steps/s = 1.62, tokens/s = 44273 (44273 target) ; Learning rate = 0.000350 ; Loss = 1.854648\n",
      "2024-12-09 23:27:28.039000: I runner.py:310] Step = 64000 ; steps/s = 1.62, tokens/s = 44235 (44235 target) ; Learning rate = 0.000349 ; Loss = 1.859611\n",
      "2024-12-09 23:28:29.362000: I runner.py:310] Step = 64100 ; steps/s = 1.63, tokens/s = 43774 (43774 target) ; Learning rate = 0.000349 ; Loss = 1.883251\n",
      "2024-12-09 23:29:31.035000: I runner.py:310] Step = 64200 ; steps/s = 1.62, tokens/s = 44256 (44256 target) ; Learning rate = 0.000349 ; Loss = 1.843579\n",
      "2024-12-09 23:30:32.784000: I runner.py:310] Step = 64300 ; steps/s = 1.62, tokens/s = 44203 (44203 target) ; Learning rate = 0.000349 ; Loss = 1.844896\n",
      "2024-12-09 23:31:34.133000: I runner.py:310] Step = 64400 ; steps/s = 1.63, tokens/s = 43733 (43733 target) ; Learning rate = 0.000348 ; Loss = 1.863577\n",
      "2024-12-09 23:32:35.845000: I runner.py:310] Step = 64500 ; steps/s = 1.62, tokens/s = 44235 (44235 target) ; Learning rate = 0.000348 ; Loss = 1.846379\n",
      "2024-12-09 23:33:37.530000: I runner.py:310] Step = 64600 ; steps/s = 1.62, tokens/s = 44233 (44233 target) ; Learning rate = 0.000348 ; Loss = 1.835467\n",
      "2024-12-09 23:34:39.225000: I runner.py:310] Step = 64700 ; steps/s = 1.62, tokens/s = 44239 (44239 target) ; Learning rate = 0.000347 ; Loss = 1.858186\n",
      "2024-12-09 23:35:40.528000: I runner.py:310] Step = 64800 ; steps/s = 1.63, tokens/s = 43786 (43786 target) ; Learning rate = 0.000347 ; Loss = 1.833059\n",
      "2024-12-09 23:36:42.254000: I runner.py:310] Step = 64900 ; steps/s = 1.62, tokens/s = 44211 (44211 target) ; Learning rate = 0.000347 ; Loss = 1.867241\n",
      "2024-12-09 23:37:43.909000: I runner.py:310] Step = 65000 ; steps/s = 1.62, tokens/s = 44266 (44266 target) ; Learning rate = 0.000347 ; Loss = 1.880120\n",
      "2024-12-09 23:37:43.910000: I training.py:192] Running evaluation for step 65000\n",
      "2024-12-09 23:40:42.260000: I training.py:192] Evaluation result for step 65000: loss = 2.442243 ; perplexity = 11.498802\n",
      "2024-12-09 23:41:43.466000: I runner.py:310] Step = 65100 ; steps/s = 1.63, tokens/s = 43871 (43871 target) ; Learning rate = 0.000346 ; Loss = 1.885386\n",
      "2024-12-09 23:42:45.128000: I runner.py:310] Step = 65200 ; steps/s = 1.62, tokens/s = 44257 (44257 target) ; Learning rate = 0.000346 ; Loss = 1.853552\n",
      "2024-12-09 23:43:46.814000: I runner.py:310] Step = 65300 ; steps/s = 1.62, tokens/s = 44240 (44240 target) ; Learning rate = 0.000346 ; Loss = 1.855304\n",
      "2024-12-09 23:44:48.623000: I runner.py:310] Step = 65400 ; steps/s = 1.62, tokens/s = 44154 (44154 target) ; Learning rate = 0.000346 ; Loss = 1.862369\n",
      "2024-12-09 23:45:49.886000: I runner.py:310] Step = 65500 ; steps/s = 1.63, tokens/s = 43823 (43823 target) ; Learning rate = 0.000345 ; Loss = 1.859387\n",
      "2024-12-09 23:46:51.610000: I runner.py:310] Step = 65600 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000345 ; Loss = 1.848286\n",
      "2024-12-09 23:47:53.313000: I runner.py:310] Step = 65700 ; steps/s = 1.62, tokens/s = 44214 (44214 target) ; Learning rate = 0.000345 ; Loss = 1.851023\n",
      "2024-12-09 23:48:54.623000: I runner.py:310] Step = 65800 ; steps/s = 1.63, tokens/s = 43799 (43799 target) ; Learning rate = 0.000345 ; Loss = 1.870111\n",
      "2024-12-09 23:49:56.282000: I runner.py:310] Step = 65900 ; steps/s = 1.62, tokens/s = 44269 (44269 target) ; Learning rate = 0.000344 ; Loss = 1.835355\n",
      "2024-12-09 23:50:57.996000: I runner.py:310] Step = 66000 ; steps/s = 1.62, tokens/s = 44224 (44224 target) ; Learning rate = 0.000344 ; Loss = 1.845912\n",
      "2024-12-09 23:51:59.714000: I runner.py:310] Step = 66100 ; steps/s = 1.62, tokens/s = 44211 (44211 target) ; Learning rate = 0.000344 ; Loss = 1.857892\n",
      "2024-12-09 23:53:01.041000: I runner.py:310] Step = 66200 ; steps/s = 1.63, tokens/s = 43781 (43781 target) ; Learning rate = 0.000344 ; Loss = 1.838563\n",
      "2024-12-09 23:54:02.680000: I runner.py:310] Step = 66300 ; steps/s = 1.62, tokens/s = 44249 (44249 target) ; Learning rate = 0.000343 ; Loss = 1.862370\n",
      "2024-12-09 23:55:04.406000: I runner.py:310] Step = 66400 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000343 ; Loss = 1.878529\n",
      "2024-12-09 23:56:05.681000: I runner.py:310] Step = 66500 ; steps/s = 1.63, tokens/s = 43806 (43806 target) ; Learning rate = 0.000343 ; Loss = 1.839571\n",
      "2024-12-09 23:57:07.379000: I runner.py:310] Step = 66600 ; steps/s = 1.62, tokens/s = 44241 (44241 target) ; Learning rate = 0.000342 ; Loss = 1.853375\n",
      "2024-12-09 23:58:09.034000: I runner.py:310] Step = 66700 ; steps/s = 1.62, tokens/s = 44276 (44276 target) ; Learning rate = 0.000342 ; Loss = 1.857297\n",
      "2024-12-09 23:59:10.614000: I runner.py:310] Step = 66800 ; steps/s = 1.62, tokens/s = 44045 (44045 target) ; Learning rate = 0.000342 ; Loss = 1.860695\n",
      "2024-12-10 00:00:12.049000: I runner.py:310] Step = 66900 ; steps/s = 1.63, tokens/s = 43960 (43960 target) ; Learning rate = 0.000342 ; Loss = 1.866985\n",
      "2024-12-10 00:01:13.778000: I runner.py:310] Step = 67000 ; steps/s = 1.62, tokens/s = 44215 (44215 target) ; Learning rate = 0.000341 ; Loss = 1.854835\n",
      "2024-12-10 00:02:15.518000: I runner.py:310] Step = 67100 ; steps/s = 1.62, tokens/s = 44202 (44202 target) ; Learning rate = 0.000341 ; Loss = 1.852691\n",
      "2024-12-10 00:03:16.897000: I runner.py:310] Step = 67200 ; steps/s = 1.63, tokens/s = 43722 (43722 target) ; Learning rate = 0.000341 ; Loss = 1.876210\n",
      "2024-12-10 00:04:18.644000: I runner.py:310] Step = 67300 ; steps/s = 1.62, tokens/s = 44214 (44214 target) ; Learning rate = 0.000341 ; Loss = 1.844701\n",
      "2024-12-10 00:05:20.346000: I runner.py:310] Step = 67400 ; steps/s = 1.62, tokens/s = 44217 (44217 target) ; Learning rate = 0.000340 ; Loss = 1.859527\n",
      "2024-12-10 00:06:21.645000: I runner.py:310] Step = 67500 ; steps/s = 1.63, tokens/s = 43790 (43790 target) ; Learning rate = 0.000340 ; Loss = 1.842738\n",
      "2024-12-10 00:07:23.374000: I runner.py:310] Step = 67600 ; steps/s = 1.62, tokens/s = 44217 (44217 target) ; Learning rate = 0.000340 ; Loss = 1.831148\n",
      "2024-12-10 00:08:25.024000: I runner.py:310] Step = 67700 ; steps/s = 1.62, tokens/s = 44258 (44258 target) ; Learning rate = 0.000340 ; Loss = 1.868366\n",
      "2024-12-10 00:09:26.650000: I runner.py:310] Step = 67800 ; steps/s = 1.62, tokens/s = 44293 (44293 target) ; Learning rate = 0.000339 ; Loss = 1.866307\n",
      "2024-12-10 00:10:27.931000: I runner.py:310] Step = 67900 ; steps/s = 1.63, tokens/s = 43803 (43803 target) ; Learning rate = 0.000339 ; Loss = 1.876087\n",
      "2024-12-10 00:11:29.584000: I runner.py:310] Step = 68000 ; steps/s = 1.62, tokens/s = 44263 (44263 target) ; Learning rate = 0.000339 ; Loss = 1.846882\n",
      "2024-12-10 00:12:31.324000: I runner.py:310] Step = 68100 ; steps/s = 1.62, tokens/s = 44203 (44203 target) ; Learning rate = 0.000339 ; Loss = 1.853478\n",
      "2024-12-10 00:13:32.638000: I runner.py:310] Step = 68200 ; steps/s = 1.63, tokens/s = 43779 (43779 target) ; Learning rate = 0.000338 ; Loss = 1.844025\n",
      "2024-12-10 00:14:34.385000: I runner.py:310] Step = 68300 ; steps/s = 1.62, tokens/s = 44196 (44196 target) ; Learning rate = 0.000338 ; Loss = 1.851438\n",
      "2024-12-10 00:15:36.098000: I runner.py:310] Step = 68400 ; steps/s = 1.62, tokens/s = 44231 (44231 target) ; Learning rate = 0.000338 ; Loss = 1.867110\n",
      "2024-12-10 00:16:37.781000: I runner.py:310] Step = 68500 ; steps/s = 1.62, tokens/s = 44242 (44242 target) ; Learning rate = 0.000338 ; Loss = 1.868327\n",
      "2024-12-10 00:17:39.037000: I runner.py:310] Step = 68600 ; steps/s = 1.63, tokens/s = 43810 (43810 target) ; Learning rate = 0.000337 ; Loss = 1.829100\n",
      "2024-12-10 00:18:40.678000: I runner.py:310] Step = 68700 ; steps/s = 1.62, tokens/s = 44289 (44289 target) ; Learning rate = 0.000337 ; Loss = 1.852088\n",
      "2024-12-10 00:19:42.369000: I runner.py:310] Step = 68800 ; steps/s = 1.62, tokens/s = 44235 (44235 target) ; Learning rate = 0.000337 ; Loss = 1.856843\n",
      "2024-12-10 00:20:43.663000: I runner.py:310] Step = 68900 ; steps/s = 1.63, tokens/s = 43791 (43791 target) ; Learning rate = 0.000337 ; Loss = 1.849455\n",
      "2024-12-10 00:21:45.325000: I runner.py:310] Step = 69000 ; steps/s = 1.62, tokens/s = 44256 (44256 target) ; Learning rate = 0.000336 ; Loss = 1.849216\n",
      "2024-12-10 00:22:47.059000: I runner.py:310] Step = 69100 ; steps/s = 1.62, tokens/s = 44201 (44201 target) ; Learning rate = 0.000336 ; Loss = 1.840459\n",
      "2024-12-10 00:23:48.781000: I runner.py:310] Step = 69200 ; steps/s = 1.62, tokens/s = 44223 (44223 target) ; Learning rate = 0.000336 ; Loss = 1.840852\n",
      "2024-12-10 00:24:50.100000: I runner.py:310] Step = 69300 ; steps/s = 1.63, tokens/s = 43784 (43784 target) ; Learning rate = 0.000336 ; Loss = 1.870989\n",
      "2024-12-10 00:25:51.827000: I runner.py:310] Step = 69400 ; steps/s = 1.62, tokens/s = 44206 (44206 target) ; Learning rate = 0.000336 ; Loss = 1.843069\n",
      "2024-12-10 00:26:53.538000: I runner.py:310] Step = 69500 ; steps/s = 1.62, tokens/s = 44250 (44250 target) ; Learning rate = 0.000335 ; Loss = 1.842533\n",
      "2024-12-10 00:27:54.901000: I runner.py:310] Step = 69600 ; steps/s = 1.63, tokens/s = 43726 (43726 target) ; Learning rate = 0.000335 ; Loss = 1.844102\n",
      "2024-12-10 00:28:56.561000: I runner.py:310] Step = 69700 ; steps/s = 1.62, tokens/s = 44257 (44257 target) ; Learning rate = 0.000335 ; Loss = 1.850995\n",
      "2024-12-10 00:29:58.264000: I runner.py:310] Step = 69800 ; steps/s = 1.62, tokens/s = 44231 (44231 target) ; Learning rate = 0.000335 ; Loss = 1.830552\n",
      "2024-12-10 00:30:59.951000: I runner.py:310] Step = 69900 ; steps/s = 1.62, tokens/s = 44257 (44257 target) ; Learning rate = 0.000334 ; Loss = 1.848984\n",
      "2024-12-10 00:32:01.279000: I runner.py:310] Step = 70000 ; steps/s = 1.63, tokens/s = 43747 (43747 target) ; Learning rate = 0.000334 ; Loss = 1.863842\n",
      "2024-12-10 00:32:03.832000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-70000\n",
      "2024-12-10 00:32:03.832000: I training.py:192] Running evaluation for step 70000\n",
      "2024-12-10 00:35:05.309000: I training.py:192] Evaluation result for step 70000: loss = 2.460636 ; perplexity = 11.712256\n",
      "2024-12-10 00:36:06.970000: I runner.py:310] Step = 70100 ; steps/s = 1.62, tokens/s = 44277 (44277 target) ; Learning rate = 0.000334 ; Loss = 1.844457\n",
      "2024-12-10 00:37:08.678000: I runner.py:310] Step = 70200 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000334 ; Loss = 1.850272\n",
      "2024-12-10 00:38:09.998000: I runner.py:310] Step = 70300 ; steps/s = 1.63, tokens/s = 43766 (43766 target) ; Learning rate = 0.000333 ; Loss = 1.840307\n",
      "2024-12-10 00:39:11.656000: I runner.py:310] Step = 70400 ; steps/s = 1.62, tokens/s = 44269 (44269 target) ; Learning rate = 0.000333 ; Loss = 1.827643\n",
      "2024-12-10 00:40:13.355000: I runner.py:310] Step = 70500 ; steps/s = 1.62, tokens/s = 44246 (44246 target) ; Learning rate = 0.000333 ; Loss = 1.856942\n",
      "2024-12-10 00:41:15.163000: I runner.py:310] Step = 70600 ; steps/s = 1.62, tokens/s = 44150 (44150 target) ; Learning rate = 0.000333 ; Loss = 1.868301\n",
      "2024-12-10 00:42:16.442000: I runner.py:310] Step = 70700 ; steps/s = 1.63, tokens/s = 43796 (43796 target) ; Learning rate = 0.000332 ; Loss = 1.821258\n",
      "2024-12-10 00:43:18.184000: I runner.py:310] Step = 70800 ; steps/s = 1.62, tokens/s = 44206 (44206 target) ; Learning rate = 0.000332 ; Loss = 1.847645\n",
      "2024-12-10 00:44:19.930000: I runner.py:310] Step = 70900 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000332 ; Loss = 1.870820\n",
      "2024-12-10 00:45:21.178000: I runner.py:310] Step = 71000 ; steps/s = 1.63, tokens/s = 43809 (43809 target) ; Learning rate = 0.000332 ; Loss = 1.829206\n",
      "2024-12-10 00:46:22.927000: I runner.py:310] Step = 71100 ; steps/s = 1.62, tokens/s = 44208 (44208 target) ; Learning rate = 0.000331 ; Loss = 1.835843\n",
      "2024-12-10 00:47:24.634000: I runner.py:310] Step = 71200 ; steps/s = 1.62, tokens/s = 44235 (44235 target) ; Learning rate = 0.000331 ; Loss = 1.860309\n",
      "2024-12-10 00:48:26.304000: I runner.py:310] Step = 71300 ; steps/s = 1.62, tokens/s = 44244 (44244 target) ; Learning rate = 0.000331 ; Loss = 1.871194\n",
      "2024-12-10 00:49:27.724000: I runner.py:310] Step = 71400 ; steps/s = 1.63, tokens/s = 43697 (43697 target) ; Learning rate = 0.000331 ; Loss = 1.811125\n",
      "2024-12-10 00:50:29.491000: I runner.py:310] Step = 71500 ; steps/s = 1.62, tokens/s = 44194 (44194 target) ; Learning rate = 0.000331 ; Loss = 1.849002\n",
      "2024-12-10 00:51:31.233000: I runner.py:310] Step = 71600 ; steps/s = 1.62, tokens/s = 44197 (44197 target) ; Learning rate = 0.000330 ; Loss = 1.858963\n",
      "2024-12-10 00:52:32.548000: I runner.py:310] Step = 71700 ; steps/s = 1.63, tokens/s = 43785 (43785 target) ; Learning rate = 0.000330 ; Loss = 1.837759\n",
      "2024-12-10 00:53:34.218000: I runner.py:310] Step = 71800 ; steps/s = 1.62, tokens/s = 44244 (44244 target) ; Learning rate = 0.000330 ; Loss = 1.839362\n",
      "2024-12-10 00:54:35.925000: I runner.py:310] Step = 71900 ; steps/s = 1.62, tokens/s = 44236 (44236 target) ; Learning rate = 0.000330 ; Loss = 1.846963\n",
      "2024-12-10 00:55:37.612000: I runner.py:310] Step = 72000 ; steps/s = 1.62, tokens/s = 44242 (44242 target) ; Learning rate = 0.000329 ; Loss = 1.861268\n",
      "2024-12-10 00:56:38.964000: I runner.py:310] Step = 72100 ; steps/s = 1.63, tokens/s = 43762 (43762 target) ; Learning rate = 0.000329 ; Loss = 1.869897\n",
      "2024-12-10 00:57:40.678000: I runner.py:310] Step = 72200 ; steps/s = 1.62, tokens/s = 44220 (44220 target) ; Learning rate = 0.000329 ; Loss = 1.846892\n",
      "2024-12-10 00:58:42.370000: I runner.py:310] Step = 72300 ; steps/s = 1.62, tokens/s = 44240 (44240 target) ; Learning rate = 0.000329 ; Loss = 1.841921\n",
      "2024-12-10 00:59:43.656000: I runner.py:310] Step = 72400 ; steps/s = 1.63, tokens/s = 43783 (43783 target) ; Learning rate = 0.000328 ; Loss = 1.833895\n",
      "2024-12-10 01:00:45.445000: I runner.py:310] Step = 72500 ; steps/s = 1.62, tokens/s = 44178 (44178 target) ; Learning rate = 0.000328 ; Loss = 1.843107\n",
      "2024-12-10 01:01:47.124000: I runner.py:310] Step = 72600 ; steps/s = 1.62, tokens/s = 44242 (44242 target) ; Learning rate = 0.000328 ; Loss = 1.863705\n",
      "2024-12-10 01:02:48.841000: I runner.py:310] Step = 72700 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000328 ; Loss = 1.858572\n",
      "2024-12-10 01:03:50.154000: I runner.py:310] Step = 72800 ; steps/s = 1.63, tokens/s = 43765 (43765 target) ; Learning rate = 0.000328 ; Loss = 1.866442\n",
      "2024-12-10 01:04:51.852000: I runner.py:310] Step = 72900 ; steps/s = 1.62, tokens/s = 44249 (44249 target) ; Learning rate = 0.000327 ; Loss = 1.827453\n",
      "2024-12-10 01:05:53.539000: I runner.py:310] Step = 73000 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000327 ; Loss = 1.836343\n",
      "2024-12-10 01:06:54.745000: I runner.py:310] Step = 73100 ; steps/s = 1.63, tokens/s = 43860 (43860 target) ; Learning rate = 0.000327 ; Loss = 1.843956\n",
      "2024-12-10 01:07:56.398000: I runner.py:310] Step = 73200 ; steps/s = 1.62, tokens/s = 44276 (44276 target) ; Learning rate = 0.000327 ; Loss = 1.836142\n",
      "2024-12-10 01:08:58.076000: I runner.py:310] Step = 73300 ; steps/s = 1.62, tokens/s = 44237 (44237 target) ; Learning rate = 0.000326 ; Loss = 1.835549\n",
      "2024-12-10 01:09:59.784000: I runner.py:310] Step = 73400 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000326 ; Loss = 1.839262\n",
      "2024-12-10 01:11:01.037000: I runner.py:310] Step = 73500 ; steps/s = 1.63, tokens/s = 43830 (43830 target) ; Learning rate = 0.000326 ; Loss = 1.851435\n",
      "2024-12-10 01:12:02.696000: I runner.py:310] Step = 73600 ; steps/s = 1.62, tokens/s = 44264 (44264 target) ; Learning rate = 0.000326 ; Loss = 1.822913\n",
      "2024-12-10 01:13:04.377000: I runner.py:310] Step = 73700 ; steps/s = 1.62, tokens/s = 44247 (44247 target) ; Learning rate = 0.000326 ; Loss = 1.823206\n",
      "2024-12-10 01:14:05.696000: I runner.py:310] Step = 73800 ; steps/s = 1.63, tokens/s = 43786 (43786 target) ; Learning rate = 0.000325 ; Loss = 1.817261\n",
      "2024-12-10 01:15:07.418000: I runner.py:310] Step = 73900 ; steps/s = 1.62, tokens/s = 44224 (44224 target) ; Learning rate = 0.000325 ; Loss = 1.837527\n",
      "2024-12-10 01:16:09.103000: I runner.py:310] Step = 74000 ; steps/s = 1.62, tokens/s = 44250 (44250 target) ; Learning rate = 0.000325 ; Loss = 1.860182\n",
      "2024-12-10 01:17:10.787000: I runner.py:310] Step = 74100 ; steps/s = 1.62, tokens/s = 44218 (44218 target) ; Learning rate = 0.000325 ; Loss = 1.864812\n",
      "2024-12-10 01:18:12.063000: I runner.py:310] Step = 74200 ; steps/s = 1.63, tokens/s = 43799 (43799 target) ; Learning rate = 0.000324 ; Loss = 1.851085\n",
      "2024-12-10 01:19:13.785000: I runner.py:310] Step = 74300 ; steps/s = 1.62, tokens/s = 44214 (44214 target) ; Learning rate = 0.000324 ; Loss = 1.822724\n",
      "2024-12-10 01:20:15.440000: I runner.py:310] Step = 74400 ; steps/s = 1.62, tokens/s = 44276 (44276 target) ; Learning rate = 0.000324 ; Loss = 1.837248\n",
      "2024-12-10 01:21:16.757000: I runner.py:310] Step = 74500 ; steps/s = 1.63, tokens/s = 43775 (43775 target) ; Learning rate = 0.000324 ; Loss = 1.805459\n",
      "2024-12-10 01:22:18.437000: I runner.py:310] Step = 74600 ; steps/s = 1.62, tokens/s = 44249 (44249 target) ; Learning rate = 0.000324 ; Loss = 1.841276\n",
      "2024-12-10 01:23:20.169000: I runner.py:310] Step = 74700 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000323 ; Loss = 1.850775\n",
      "2024-12-10 01:24:21.906000: I runner.py:310] Step = 74800 ; steps/s = 1.62, tokens/s = 44202 (44202 target) ; Learning rate = 0.000323 ; Loss = 1.849365\n",
      "2024-12-10 01:25:23.273000: I runner.py:310] Step = 74900 ; steps/s = 1.63, tokens/s = 43738 (43738 target) ; Learning rate = 0.000323 ; Loss = 1.858921\n",
      "2024-12-10 01:26:25.014000: I runner.py:310] Step = 75000 ; steps/s = 1.62, tokens/s = 44202 (44202 target) ; Learning rate = 0.000323 ; Loss = 1.827221\n",
      "2024-12-10 01:26:25.015000: I training.py:192] Running evaluation for step 75000\n",
      "2024-12-10 01:29:18.150000: I training.py:192] Evaluation result for step 75000: loss = 2.476734 ; perplexity = 11.902328\n",
      "2024-12-10 01:30:19.698000: I runner.py:310] Step = 75100 ; steps/s = 1.63, tokens/s = 44340 (44340 target) ; Learning rate = 0.000323 ; Loss = 1.831679\n",
      "2024-12-10 01:31:21.004000: I runner.py:310] Step = 75200 ; steps/s = 1.63, tokens/s = 43785 (43785 target) ; Learning rate = 0.000322 ; Loss = 1.846747\n",
      "2024-12-10 01:32:22.717000: I runner.py:310] Step = 75300 ; steps/s = 1.62, tokens/s = 44222 (44222 target) ; Learning rate = 0.000322 ; Loss = 1.809067\n",
      "2024-12-10 01:33:24.486000: I runner.py:310] Step = 75400 ; steps/s = 1.62, tokens/s = 44198 (44198 target) ; Learning rate = 0.000322 ; Loss = 1.833033\n",
      "2024-12-10 01:34:25.924000: I runner.py:310] Step = 75500 ; steps/s = 1.63, tokens/s = 43778 (43778 target) ; Learning rate = 0.000322 ; Loss = 1.885895\n",
      "2024-12-10 01:35:27.602000: I runner.py:310] Step = 75600 ; steps/s = 1.62, tokens/s = 44150 (44150 target) ; Learning rate = 0.000321 ; Loss = 1.816226\n",
      "2024-12-10 01:36:29.240000: I runner.py:310] Step = 75700 ; steps/s = 1.62, tokens/s = 44269 (44269 target) ; Learning rate = 0.000321 ; Loss = 1.863414\n",
      "2024-12-10 01:37:31.020000: I runner.py:310] Step = 75800 ; steps/s = 1.62, tokens/s = 44179 (44179 target) ; Learning rate = 0.000321 ; Loss = 1.845326\n",
      "2024-12-10 01:38:32.349000: I runner.py:310] Step = 75900 ; steps/s = 1.63, tokens/s = 43773 (43773 target) ; Learning rate = 0.000321 ; Loss = 1.855986\n",
      "2024-12-10 01:39:34.076000: I runner.py:310] Step = 76000 ; steps/s = 1.62, tokens/s = 44227 (44227 target) ; Learning rate = 0.000321 ; Loss = 1.815883\n",
      "2024-12-10 01:40:35.850000: I runner.py:310] Step = 76100 ; steps/s = 1.62, tokens/s = 44178 (44178 target) ; Learning rate = 0.000320 ; Loss = 1.830608\n",
      "2024-12-10 01:41:37.202000: I runner.py:310] Step = 76200 ; steps/s = 1.63, tokens/s = 43737 (43737 target) ; Learning rate = 0.000320 ; Loss = 1.819048\n",
      "2024-12-10 01:42:38.886000: I runner.py:310] Step = 76300 ; steps/s = 1.62, tokens/s = 44236 (44236 target) ; Learning rate = 0.000320 ; Loss = 1.816496\n",
      "2024-12-10 01:43:40.569000: I runner.py:310] Step = 76400 ; steps/s = 1.62, tokens/s = 44254 (44254 target) ; Learning rate = 0.000320 ; Loss = 1.847059\n",
      "2024-12-10 01:44:42.320000: I runner.py:310] Step = 76500 ; steps/s = 1.62, tokens/s = 44196 (44196 target) ; Learning rate = 0.000320 ; Loss = 1.846286\n",
      "2024-12-10 01:45:43.587000: I runner.py:310] Step = 76600 ; steps/s = 1.63, tokens/s = 43811 (43811 target) ; Learning rate = 0.000319 ; Loss = 1.850737\n",
      "2024-12-10 01:46:45.243000: I runner.py:310] Step = 76700 ; steps/s = 1.62, tokens/s = 44272 (44272 target) ; Learning rate = 0.000319 ; Loss = 1.823586\n",
      "2024-12-10 01:47:46.970000: I runner.py:310] Step = 76800 ; steps/s = 1.62, tokens/s = 44201 (44201 target) ; Learning rate = 0.000319 ; Loss = 1.831510\n",
      "2024-12-10 01:48:48.252000: I runner.py:310] Step = 76900 ; steps/s = 1.63, tokens/s = 43813 (43813 target) ; Learning rate = 0.000319 ; Loss = 1.835569\n",
      "2024-12-10 01:49:49.954000: I runner.py:310] Step = 77000 ; steps/s = 1.62, tokens/s = 44218 (44218 target) ; Learning rate = 0.000319 ; Loss = 1.838209\n",
      "2024-12-10 01:50:51.628000: I runner.py:310] Step = 77100 ; steps/s = 1.62, tokens/s = 44255 (44255 target) ; Learning rate = 0.000318 ; Loss = 1.821460\n",
      "2024-12-10 01:51:53.331000: I runner.py:310] Step = 77200 ; steps/s = 1.62, tokens/s = 44242 (44242 target) ; Learning rate = 0.000318 ; Loss = 1.830311\n",
      "2024-12-10 01:52:54.659000: I runner.py:310] Step = 77300 ; steps/s = 1.63, tokens/s = 43755 (43755 target) ; Learning rate = 0.000318 ; Loss = 1.862597\n",
      "2024-12-10 01:53:56.411000: I runner.py:310] Step = 77400 ; steps/s = 1.62, tokens/s = 44198 (44198 target) ; Learning rate = 0.000318 ; Loss = 1.832854\n",
      "2024-12-10 01:54:58.164000: I runner.py:310] Step = 77500 ; steps/s = 1.62, tokens/s = 44193 (44193 target) ; Learning rate = 0.000317 ; Loss = 1.814005\n",
      "2024-12-10 01:55:59.487000: I runner.py:310] Step = 77600 ; steps/s = 1.63, tokens/s = 43782 (43782 target) ; Learning rate = 0.000317 ; Loss = 1.839527\n",
      "2024-12-10 01:57:01.159000: I runner.py:310] Step = 77700 ; steps/s = 1.62, tokens/s = 44250 (44250 target) ; Learning rate = 0.000317 ; Loss = 1.825584\n",
      "2024-12-10 01:58:02.826000: I runner.py:310] Step = 77800 ; steps/s = 1.62, tokens/s = 44260 (44260 target) ; Learning rate = 0.000317 ; Loss = 1.850120\n",
      "2024-12-10 01:59:04.556000: I runner.py:310] Step = 77900 ; steps/s = 1.62, tokens/s = 44205 (44205 target) ; Learning rate = 0.000317 ; Loss = 1.833264\n",
      "2024-12-10 02:00:05.783000: I runner.py:310] Step = 78000 ; steps/s = 1.63, tokens/s = 43840 (43840 target) ; Learning rate = 0.000316 ; Loss = 1.843005\n",
      "2024-12-10 02:01:07.497000: I runner.py:310] Step = 78100 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000316 ; Loss = 1.831354\n",
      "2024-12-10 02:02:09.194000: I runner.py:310] Step = 78200 ; steps/s = 1.62, tokens/s = 44230 (44230 target) ; Learning rate = 0.000316 ; Loss = 1.828878\n",
      "2024-12-10 02:03:10.385000: I runner.py:310] Step = 78300 ; steps/s = 1.63, tokens/s = 43871 (43871 target) ; Learning rate = 0.000316 ; Loss = 1.821085\n",
      "2024-12-10 02:04:12.078000: I runner.py:310] Step = 78400 ; steps/s = 1.62, tokens/s = 44249 (44249 target) ; Learning rate = 0.000316 ; Loss = 1.809812\n",
      "2024-12-10 02:05:13.773000: I runner.py:310] Step = 78500 ; steps/s = 1.62, tokens/s = 44236 (44236 target) ; Learning rate = 0.000315 ; Loss = 1.831219\n",
      "2024-12-10 02:06:15.527000: I runner.py:310] Step = 78600 ; steps/s = 1.62, tokens/s = 44172 (44172 target) ; Learning rate = 0.000315 ; Loss = 1.844090\n",
      "2024-12-10 02:07:16.815000: I runner.py:310] Step = 78700 ; steps/s = 1.63, tokens/s = 43807 (43807 target) ; Learning rate = 0.000315 ; Loss = 1.795148\n",
      "2024-12-10 02:08:18.511000: I runner.py:310] Step = 78800 ; steps/s = 1.62, tokens/s = 44240 (44240 target) ; Learning rate = 0.000315 ; Loss = 1.815499\n",
      "2024-12-10 02:09:20.242000: I runner.py:310] Step = 78900 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000315 ; Loss = 1.837159\n",
      "2024-12-10 02:10:21.597000: I runner.py:310] Step = 79000 ; steps/s = 1.63, tokens/s = 43744 (43744 target) ; Learning rate = 0.000314 ; Loss = 1.836077\n",
      "2024-12-10 02:11:23.241000: I runner.py:310] Step = 79100 ; steps/s = 1.62, tokens/s = 44275 (44275 target) ; Learning rate = 0.000314 ; Loss = 1.817715\n",
      "2024-12-10 02:12:24.961000: I runner.py:310] Step = 79200 ; steps/s = 1.62, tokens/s = 44224 (44224 target) ; Learning rate = 0.000314 ; Loss = 1.825090\n",
      "2024-12-10 02:13:26.738000: I runner.py:310] Step = 79300 ; steps/s = 1.62, tokens/s = 44178 (44178 target) ; Learning rate = 0.000314 ; Loss = 1.820623\n",
      "2024-12-10 02:14:28.083000: I runner.py:310] Step = 79400 ; steps/s = 1.63, tokens/s = 43741 (43741 target) ; Learning rate = 0.000314 ; Loss = 1.789837\n",
      "2024-12-10 02:15:29.811000: I runner.py:310] Step = 79500 ; steps/s = 1.62, tokens/s = 44215 (44215 target) ; Learning rate = 0.000313 ; Loss = 1.835203\n",
      "2024-12-10 02:16:31.554000: I runner.py:310] Step = 79600 ; steps/s = 1.62, tokens/s = 44180 (44180 target) ; Learning rate = 0.000313 ; Loss = 1.837560\n",
      "2024-12-10 02:17:32.847000: I runner.py:310] Step = 79700 ; steps/s = 1.63, tokens/s = 43836 (43836 target) ; Learning rate = 0.000313 ; Loss = 1.820543\n",
      "2024-12-10 02:18:34.602000: I runner.py:310] Step = 79800 ; steps/s = 1.62, tokens/s = 44184 (44184 target) ; Learning rate = 0.000313 ; Loss = 1.827184\n",
      "2024-12-10 02:19:36.295000: I runner.py:310] Step = 79900 ; steps/s = 1.62, tokens/s = 44231 (44231 target) ; Learning rate = 0.000313 ; Loss = 1.846196\n",
      "2024-12-10 02:20:38.066000: I runner.py:310] Step = 80000 ; steps/s = 1.62, tokens/s = 44192 (44192 target) ; Learning rate = 0.000312 ; Loss = 1.856370\n",
      "2024-12-10 02:20:40.195000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-80000\n",
      "2024-12-10 02:20:40.195000: I training.py:192] Running evaluation for step 80000\n",
      "2024-12-10 02:23:36.985000: I training.py:192] Evaluation result for step 80000: loss = 2.481124 ; perplexity = 11.954691\n",
      "2024-12-10 02:24:38.163000: I runner.py:310] Step = 80100 ; steps/s = 1.64, tokens/s = 43889 (43889 target) ; Learning rate = 0.000312 ; Loss = 1.796038\n",
      "2024-12-10 02:25:39.820000: I runner.py:310] Step = 80200 ; steps/s = 1.62, tokens/s = 44257 (44257 target) ; Learning rate = 0.000312 ; Loss = 1.822453\n",
      "2024-12-10 02:26:41.565000: I runner.py:310] Step = 80300 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000312 ; Loss = 1.835593\n",
      "2024-12-10 02:27:42.871000: I runner.py:310] Step = 80400 ; steps/s = 1.63, tokens/s = 43774 (43774 target) ; Learning rate = 0.000312 ; Loss = 1.833436\n",
      "2024-12-10 02:28:44.626000: I runner.py:310] Step = 80500 ; steps/s = 1.62, tokens/s = 44194 (44194 target) ; Learning rate = 0.000312 ; Loss = 1.828529\n",
      "2024-12-10 02:29:46.384000: I runner.py:310] Step = 80600 ; steps/s = 1.62, tokens/s = 44192 (44192 target) ; Learning rate = 0.000311 ; Loss = 1.823565\n",
      "2024-12-10 02:30:48.151000: I runner.py:310] Step = 80700 ; steps/s = 1.62, tokens/s = 44177 (44177 target) ; Learning rate = 0.000311 ; Loss = 1.822837\n",
      "2024-12-10 02:31:49.448000: I runner.py:310] Step = 80800 ; steps/s = 1.63, tokens/s = 43787 (43787 target) ; Learning rate = 0.000311 ; Loss = 1.838171\n",
      "2024-12-10 02:32:51.218000: I runner.py:310] Step = 80900 ; steps/s = 1.62, tokens/s = 44188 (44188 target) ; Learning rate = 0.000311 ; Loss = 1.818753\n",
      "2024-12-10 02:33:52.875000: I runner.py:310] Step = 81000 ; steps/s = 1.62, tokens/s = 44259 (44259 target) ; Learning rate = 0.000311 ; Loss = 1.819236\n",
      "2024-12-10 02:34:54.180000: I runner.py:310] Step = 81100 ; steps/s = 1.63, tokens/s = 43798 (43798 target) ; Learning rate = 0.000310 ; Loss = 1.846628\n",
      "2024-12-10 02:35:55.849000: I runner.py:310] Step = 81200 ; steps/s = 1.62, tokens/s = 44245 (44245 target) ; Learning rate = 0.000310 ; Loss = 1.826831\n",
      "2024-12-10 02:36:57.565000: I runner.py:310] Step = 81300 ; steps/s = 1.62, tokens/s = 44226 (44226 target) ; Learning rate = 0.000310 ; Loss = 1.824105\n",
      "2024-12-10 02:37:59.283000: I runner.py:310] Step = 81400 ; steps/s = 1.62, tokens/s = 44221 (44221 target) ; Learning rate = 0.000310 ; Loss = 1.827367\n",
      "2024-12-10 02:39:00.478000: I runner.py:310] Step = 81500 ; steps/s = 1.63, tokens/s = 43856 (43856 target) ; Learning rate = 0.000310 ; Loss = 1.850277\n",
      "2024-12-10 02:40:02.141000: I runner.py:310] Step = 81600 ; steps/s = 1.62, tokens/s = 44279 (44279 target) ; Learning rate = 0.000309 ; Loss = 1.819433\n",
      "2024-12-10 02:41:03.857000: I runner.py:310] Step = 81700 ; steps/s = 1.62, tokens/s = 44219 (44219 target) ; Learning rate = 0.000309 ; Loss = 1.819013\n",
      "2024-12-10 02:42:05.105000: I runner.py:310] Step = 81800 ; steps/s = 1.63, tokens/s = 43812 (43812 target) ; Learning rate = 0.000309 ; Loss = 1.799077\n",
      "2024-12-10 02:43:06.835000: I runner.py:310] Step = 81900 ; steps/s = 1.62, tokens/s = 44214 (44214 target) ; Learning rate = 0.000309 ; Loss = 1.825361\n",
      "2024-12-10 02:44:08.578000: I runner.py:310] Step = 82000 ; steps/s = 1.62, tokens/s = 44194 (44194 target) ; Learning rate = 0.000309 ; Loss = 1.834540\n",
      "2024-12-10 02:45:10.341000: I runner.py:310] Step = 82100 ; steps/s = 1.62, tokens/s = 44189 (44189 target) ; Learning rate = 0.000308 ; Loss = 1.844357\n",
      "2024-12-10 02:46:11.636000: I runner.py:310] Step = 82200 ; steps/s = 1.63, tokens/s = 43790 (43790 target) ; Learning rate = 0.000308 ; Loss = 1.841675\n",
      "2024-12-10 02:47:13.325000: I runner.py:310] Step = 82300 ; steps/s = 1.62, tokens/s = 44240 (44240 target) ; Learning rate = 0.000308 ; Loss = 1.810071\n",
      "2024-12-10 02:48:15.049000: I runner.py:310] Step = 82400 ; steps/s = 1.62, tokens/s = 44223 (44223 target) ; Learning rate = 0.000308 ; Loss = 1.818123\n",
      "2024-12-10 02:49:16.347000: I runner.py:310] Step = 82500 ; steps/s = 1.63, tokens/s = 43783 (43783 target) ; Learning rate = 0.000308 ; Loss = 1.833545\n",
      "2024-12-10 02:50:18.148000: I runner.py:310] Step = 82600 ; steps/s = 1.62, tokens/s = 44159 (44159 target) ; Learning rate = 0.000308 ; Loss = 1.815921\n",
      "2024-12-10 02:51:19.813000: I runner.py:310] Step = 82700 ; steps/s = 1.62, tokens/s = 44249 (44249 target) ; Learning rate = 0.000307 ; Loss = 1.810627\n",
      "2024-12-10 02:52:21.501000: I runner.py:310] Step = 82800 ; steps/s = 1.62, tokens/s = 44261 (44261 target) ; Learning rate = 0.000307 ; Loss = 1.812926\n",
      "2024-12-10 02:53:22.799000: I runner.py:310] Step = 82900 ; steps/s = 1.63, tokens/s = 43782 (43782 target) ; Learning rate = 0.000307 ; Loss = 1.801947\n",
      "2024-12-10 02:54:24.545000: I runner.py:310] Step = 83000 ; steps/s = 1.62, tokens/s = 44190 (44190 target) ; Learning rate = 0.000307 ; Loss = 1.827956\n",
      "2024-12-10 02:55:26.297000: I runner.py:310] Step = 83100 ; steps/s = 1.62, tokens/s = 44208 (44208 target) ; Learning rate = 0.000307 ; Loss = 1.830003\n",
      "2024-12-10 02:56:27.525000: I runner.py:310] Step = 83200 ; steps/s = 1.63, tokens/s = 43841 (43841 target) ; Learning rate = 0.000306 ; Loss = 1.847820\n",
      "2024-12-10 02:57:29.152000: I runner.py:310] Step = 83300 ; steps/s = 1.62, tokens/s = 44282 (44282 target) ; Learning rate = 0.000306 ; Loss = 1.811225\n",
      "2024-12-10 02:58:30.896000: I runner.py:310] Step = 83400 ; steps/s = 1.62, tokens/s = 44209 (44209 target) ; Learning rate = 0.000306 ; Loss = 1.804768\n",
      "2024-12-10 02:59:32.464000: I runner.py:310] Step = 83500 ; steps/s = 1.62, tokens/s = 44163 (44163 target) ; Learning rate = 0.000306 ; Loss = 1.816055\n",
      "2024-12-10 03:00:33.895000: I runner.py:310] Step = 83600 ; steps/s = 1.63, tokens/s = 43853 (43853 target) ; Learning rate = 0.000306 ; Loss = 1.833562\n",
      "2024-12-10 03:01:35.715000: I runner.py:310] Step = 83700 ; steps/s = 1.62, tokens/s = 44139 (44139 target) ; Learning rate = 0.000306 ; Loss = 1.809012\n",
      "2024-12-10 03:02:37.452000: I runner.py:310] Step = 83800 ; steps/s = 1.62, tokens/s = 44221 (44221 target) ; Learning rate = 0.000305 ; Loss = 1.801608\n",
      "2024-12-10 03:03:38.829000: I runner.py:310] Step = 83900 ; steps/s = 1.63, tokens/s = 43714 (43714 target) ; Learning rate = 0.000305 ; Loss = 1.800403\n",
      "2024-12-10 03:04:40.500000: I runner.py:310] Step = 84000 ; steps/s = 1.62, tokens/s = 44269 (44269 target) ; Learning rate = 0.000305 ; Loss = 1.827250\n",
      "2024-12-10 03:05:42.137000: I runner.py:310] Step = 84100 ; steps/s = 1.62, tokens/s = 44290 (44290 target) ; Learning rate = 0.000305 ; Loss = 1.838611\n",
      "2024-12-10 03:06:43.492000: I runner.py:310] Step = 84200 ; steps/s = 1.63, tokens/s = 43732 (43732 target) ; Learning rate = 0.000305 ; Loss = 1.804094\n",
      "2024-12-10 03:07:45.159000: I runner.py:310] Step = 84300 ; steps/s = 1.62, tokens/s = 44257 (44257 target) ; Learning rate = 0.000304 ; Loss = 1.846201\n",
      "2024-12-10 03:08:46.955000: I runner.py:310] Step = 84400 ; steps/s = 1.62, tokens/s = 44163 (44163 target) ; Learning rate = 0.000304 ; Loss = 1.825560\n",
      "2024-12-10 03:09:48.665000: I runner.py:310] Step = 84500 ; steps/s = 1.62, tokens/s = 44232 (44232 target) ; Learning rate = 0.000304 ; Loss = 1.816910\n",
      "2024-12-10 03:10:50.027000: I runner.py:310] Step = 84600 ; steps/s = 1.63, tokens/s = 43734 (43734 target) ; Learning rate = 0.000304 ; Loss = 1.840249\n",
      "2024-12-10 03:11:51.715000: I runner.py:310] Step = 84700 ; steps/s = 1.62, tokens/s = 44230 (44230 target) ; Learning rate = 0.000304 ; Loss = 1.805524\n",
      "2024-12-10 03:12:53.399000: I runner.py:310] Step = 84800 ; steps/s = 1.62, tokens/s = 44275 (44275 target) ; Learning rate = 0.000304 ; Loss = 1.815341\n",
      "2024-12-10 03:13:54.711000: I runner.py:310] Step = 84900 ; steps/s = 1.63, tokens/s = 43767 (43767 target) ; Learning rate = 0.000303 ; Loss = 1.829520\n",
      "2024-12-10 03:14:56.444000: I runner.py:310] Step = 85000 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000303 ; Loss = 1.805015\n",
      "2024-12-10 03:14:56.446000: I training.py:192] Running evaluation for step 85000\n",
      "2024-12-10 03:17:54.802000: I training.py:192] Evaluation result for step 85000: loss = 2.510785 ; perplexity = 12.314589\n",
      "2024-12-10 03:18:56.372000: I runner.py:310] Step = 85100 ; steps/s = 1.62, tokens/s = 44340 (44340 target) ; Learning rate = 0.000303 ; Loss = 1.821866\n",
      "2024-12-10 03:19:58.100000: I runner.py:310] Step = 85200 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000303 ; Loss = 1.840240\n",
      "2024-12-10 03:20:59.519000: I runner.py:310] Step = 85300 ; steps/s = 1.63, tokens/s = 43700 (43700 target) ; Learning rate = 0.000303 ; Loss = 1.794807\n",
      "2024-12-10 03:22:01.230000: I runner.py:310] Step = 85400 ; steps/s = 1.62, tokens/s = 44220 (44220 target) ; Learning rate = 0.000302 ; Loss = 1.836519\n",
      "2024-12-10 03:23:02.884000: I runner.py:310] Step = 85500 ; steps/s = 1.62, tokens/s = 44274 (44274 target) ; Learning rate = 0.000302 ; Loss = 1.842296\n",
      "2024-12-10 03:24:04.142000: I runner.py:310] Step = 85600 ; steps/s = 1.63, tokens/s = 43815 (43815 target) ; Learning rate = 0.000302 ; Loss = 1.819338\n",
      "2024-12-10 03:25:05.822000: I runner.py:310] Step = 85700 ; steps/s = 1.62, tokens/s = 44255 (44255 target) ; Learning rate = 0.000302 ; Loss = 1.814743\n",
      "2024-12-10 03:26:07.528000: I runner.py:310] Step = 85800 ; steps/s = 1.62, tokens/s = 44238 (44238 target) ; Learning rate = 0.000302 ; Loss = 1.831572\n",
      "2024-12-10 03:27:09.276000: I runner.py:310] Step = 85900 ; steps/s = 1.62, tokens/s = 44180 (44180 target) ; Learning rate = 0.000302 ; Loss = 1.847709\n",
      "2024-12-10 03:28:10.588000: I runner.py:310] Step = 86000 ; steps/s = 1.63, tokens/s = 43781 (43781 target) ; Learning rate = 0.000301 ; Loss = 1.838402\n",
      "2024-12-10 03:29:12.287000: I runner.py:310] Step = 86100 ; steps/s = 1.62, tokens/s = 44241 (44241 target) ; Learning rate = 0.000301 ; Loss = 1.809818\n",
      "2024-12-10 03:30:13.973000: I runner.py:310] Step = 86200 ; steps/s = 1.62, tokens/s = 44236 (44236 target) ; Learning rate = 0.000301 ; Loss = 1.830054\n",
      "2024-12-10 03:31:15.285000: I runner.py:310] Step = 86300 ; steps/s = 1.63, tokens/s = 43780 (43780 target) ; Learning rate = 0.000301 ; Loss = 1.796360\n",
      "2024-12-10 03:32:17.027000: I runner.py:310] Step = 86400 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000301 ; Loss = 1.807573\n",
      "2024-12-10 03:33:18.702000: I runner.py:310] Step = 86500 ; steps/s = 1.62, tokens/s = 44244 (44244 target) ; Learning rate = 0.000301 ; Loss = 1.834057\n",
      "2024-12-10 03:34:20.459000: I runner.py:310] Step = 86600 ; steps/s = 1.62, tokens/s = 44197 (44197 target) ; Learning rate = 0.000300 ; Loss = 1.835536\n",
      "2024-12-10 03:35:21.778000: I runner.py:310] Step = 86700 ; steps/s = 1.63, tokens/s = 43773 (43773 target) ; Learning rate = 0.000300 ; Loss = 1.785877\n",
      "2024-12-10 03:36:23.530000: I runner.py:310] Step = 86800 ; steps/s = 1.62, tokens/s = 44190 (44190 target) ; Learning rate = 0.000300 ; Loss = 1.821596\n",
      "2024-12-10 03:37:25.264000: I runner.py:310] Step = 86900 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000300 ; Loss = 1.830858\n",
      "2024-12-10 03:38:26.523000: I runner.py:310] Step = 87000 ; steps/s = 1.63, tokens/s = 43829 (43829 target) ; Learning rate = 0.000300 ; Loss = 1.814235\n",
      "2024-12-10 03:39:28.214000: I runner.py:310] Step = 87100 ; steps/s = 1.62, tokens/s = 44240 (44240 target) ; Learning rate = 0.000299 ; Loss = 1.806686\n",
      "2024-12-10 03:40:29.851000: I runner.py:310] Step = 87200 ; steps/s = 1.62, tokens/s = 44288 (44288 target) ; Learning rate = 0.000299 ; Loss = 1.806162\n",
      "2024-12-10 03:41:31.497000: I runner.py:310] Step = 87300 ; steps/s = 1.62, tokens/s = 44260 (44260 target) ; Learning rate = 0.000299 ; Loss = 1.820624\n",
      "2024-12-10 03:42:32.783000: I runner.py:310] Step = 87400 ; steps/s = 1.63, tokens/s = 43808 (43808 target) ; Learning rate = 0.000299 ; Loss = 1.785437\n",
      "2024-12-10 03:43:34.446000: I runner.py:310] Step = 87500 ; steps/s = 1.62, tokens/s = 44249 (44249 target) ; Learning rate = 0.000299 ; Loss = 1.817686\n",
      "2024-12-10 03:44:36.165000: I runner.py:310] Step = 87600 ; steps/s = 1.62, tokens/s = 44217 (44217 target) ; Learning rate = 0.000299 ; Loss = 1.826502\n",
      "2024-12-10 03:45:37.424000: I runner.py:310] Step = 87700 ; steps/s = 1.63, tokens/s = 43815 (43815 target) ; Learning rate = 0.000298 ; Loss = 1.802501\n",
      "2024-12-10 03:46:39.140000: I runner.py:310] Step = 87800 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000298 ; Loss = 1.810939\n",
      "2024-12-10 03:47:40.835000: I runner.py:310] Step = 87900 ; steps/s = 1.62, tokens/s = 44237 (44237 target) ; Learning rate = 0.000298 ; Loss = 1.828812\n",
      "2024-12-10 03:48:42.577000: I runner.py:310] Step = 88000 ; steps/s = 1.62, tokens/s = 44205 (44205 target) ; Learning rate = 0.000298 ; Loss = 1.823702\n",
      "2024-12-10 03:49:43.873000: I runner.py:310] Step = 88100 ; steps/s = 1.63, tokens/s = 43782 (43782 target) ; Learning rate = 0.000298 ; Loss = 1.789980\n",
      "2024-12-10 03:50:45.559000: I runner.py:310] Step = 88200 ; steps/s = 1.62, tokens/s = 44246 (44246 target) ; Learning rate = 0.000298 ; Loss = 1.831589\n",
      "2024-12-10 03:51:47.247000: I runner.py:310] Step = 88300 ; steps/s = 1.62, tokens/s = 44241 (44241 target) ; Learning rate = 0.000297 ; Loss = 1.816093\n",
      "2024-12-10 03:52:48.512000: I runner.py:310] Step = 88400 ; steps/s = 1.63, tokens/s = 43815 (43815 target) ; Learning rate = 0.000297 ; Loss = 1.820673\n",
      "2024-12-10 03:53:50.220000: I runner.py:310] Step = 88500 ; steps/s = 1.62, tokens/s = 44232 (44232 target) ; Learning rate = 0.000297 ; Loss = 1.807879\n",
      "2024-12-10 03:54:51.864000: I runner.py:310] Step = 88600 ; steps/s = 1.62, tokens/s = 44265 (44265 target) ; Learning rate = 0.000297 ; Loss = 1.797588\n",
      "2024-12-10 03:55:53.592000: I runner.py:310] Step = 88700 ; steps/s = 1.62, tokens/s = 44219 (44219 target) ; Learning rate = 0.000297 ; Loss = 1.801890\n",
      "2024-12-10 03:56:54.905000: I runner.py:310] Step = 88800 ; steps/s = 1.63, tokens/s = 43765 (43765 target) ; Learning rate = 0.000297 ; Loss = 1.845986\n",
      "2024-12-10 03:57:56.607000: I runner.py:310] Step = 88900 ; steps/s = 1.62, tokens/s = 44237 (44237 target) ; Learning rate = 0.000296 ; Loss = 1.808043\n",
      "2024-12-10 03:58:58.427000: I runner.py:310] Step = 89000 ; steps/s = 1.62, tokens/s = 44145 (44145 target) ; Learning rate = 0.000296 ; Loss = 1.803983\n",
      "2024-12-10 03:59:59.749000: I runner.py:310] Step = 89100 ; steps/s = 1.63, tokens/s = 43770 (43770 target) ; Learning rate = 0.000296 ; Loss = 1.828189\n",
      "2024-12-10 04:01:01.438000: I runner.py:310] Step = 89200 ; steps/s = 1.62, tokens/s = 44244 (44244 target) ; Learning rate = 0.000296 ; Loss = 1.807407\n",
      "2024-12-10 04:02:03.135000: I runner.py:310] Step = 89300 ; steps/s = 1.62, tokens/s = 44244 (44244 target) ; Learning rate = 0.000296 ; Loss = 1.801362\n",
      "2024-12-10 04:03:04.876000: I runner.py:310] Step = 89400 ; steps/s = 1.62, tokens/s = 44193 (44193 target) ; Learning rate = 0.000296 ; Loss = 1.811973\n",
      "2024-12-10 04:04:06.202000: I runner.py:310] Step = 89500 ; steps/s = 1.63, tokens/s = 43773 (43773 target) ; Learning rate = 0.000295 ; Loss = 1.834180\n",
      "2024-12-10 04:05:07.883000: I runner.py:310] Step = 89600 ; steps/s = 1.62, tokens/s = 44247 (44247 target) ; Learning rate = 0.000295 ; Loss = 1.803832\n",
      "2024-12-10 04:06:09.622000: I runner.py:310] Step = 89700 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000295 ; Loss = 1.797136\n",
      "2024-12-10 04:07:10.957000: I runner.py:310] Step = 89800 ; steps/s = 1.63, tokens/s = 43757 (43757 target) ; Learning rate = 0.000295 ; Loss = 1.829546\n",
      "2024-12-10 04:08:12.658000: I runner.py:310] Step = 89900 ; steps/s = 1.62, tokens/s = 44220 (44220 target) ; Learning rate = 0.000295 ; Loss = 1.800570\n",
      "2024-12-10 04:09:14.379000: I runner.py:310] Step = 90000 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000295 ; Loss = 1.804629\n",
      "2024-12-10 04:09:16.535000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-90000\n",
      "2024-12-10 04:09:16.535000: I training.py:192] Running evaluation for step 90000\n",
      "2024-12-10 04:12:10.976000: I training.py:192] Evaluation result for step 90000: loss = 2.515356 ; perplexity = 12.371016\n",
      "2024-12-10 04:13:12.544000: I runner.py:310] Step = 90100 ; steps/s = 1.62, tokens/s = 44346 (44346 target) ; Learning rate = 0.000294 ; Loss = 1.820000\n",
      "2024-12-10 04:14:13.908000: I runner.py:310] Step = 90200 ; steps/s = 1.63, tokens/s = 43730 (43730 target) ; Learning rate = 0.000294 ; Loss = 1.822730\n",
      "2024-12-10 04:15:15.628000: I runner.py:310] Step = 90300 ; steps/s = 1.62, tokens/s = 44229 (44229 target) ; Learning rate = 0.000294 ; Loss = 1.789950\n",
      "2024-12-10 04:16:17.355000: I runner.py:310] Step = 90400 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000294 ; Loss = 1.810816\n",
      "2024-12-10 04:17:18.716000: I runner.py:310] Step = 90500 ; steps/s = 1.63, tokens/s = 43763 (43763 target) ; Learning rate = 0.000294 ; Loss = 1.777862\n",
      "2024-12-10 04:18:20.395000: I runner.py:310] Step = 90600 ; steps/s = 1.62, tokens/s = 44244 (44244 target) ; Learning rate = 0.000294 ; Loss = 1.811344\n",
      "2024-12-10 04:19:22.164000: I runner.py:310] Step = 90700 ; steps/s = 1.62, tokens/s = 44176 (44176 target) ; Learning rate = 0.000293 ; Loss = 1.817320\n",
      "2024-12-10 04:20:23.889000: I runner.py:310] Step = 90800 ; steps/s = 1.62, tokens/s = 44217 (44217 target) ; Learning rate = 0.000293 ; Loss = 1.830676\n",
      "2024-12-10 04:21:25.230000: I runner.py:310] Step = 90900 ; steps/s = 1.63, tokens/s = 43760 (43760 target) ; Learning rate = 0.000293 ; Loss = 1.791065\n",
      "2024-12-10 04:22:26.998000: I runner.py:310] Step = 91000 ; steps/s = 1.62, tokens/s = 44189 (44189 target) ; Learning rate = 0.000293 ; Loss = 1.808391\n",
      "2024-12-10 04:23:28.707000: I runner.py:310] Step = 91100 ; steps/s = 1.62, tokens/s = 44213 (44213 target) ; Learning rate = 0.000293 ; Loss = 1.818570\n",
      "2024-12-10 04:24:30.107000: I runner.py:310] Step = 91200 ; steps/s = 1.63, tokens/s = 43728 (43728 target) ; Learning rate = 0.000293 ; Loss = 1.823377\n",
      "2024-12-10 04:25:31.858000: I runner.py:310] Step = 91300 ; steps/s = 1.62, tokens/s = 44201 (44201 target) ; Learning rate = 0.000293 ; Loss = 1.818354\n",
      "2024-12-10 04:26:33.636000: I runner.py:310] Step = 91400 ; steps/s = 1.62, tokens/s = 44158 (44158 target) ; Learning rate = 0.000292 ; Loss = 1.811203\n",
      "2024-12-10 04:27:35.365000: I runner.py:310] Step = 91500 ; steps/s = 1.62, tokens/s = 44215 (44215 target) ; Learning rate = 0.000292 ; Loss = 1.809076\n",
      "2024-12-10 04:28:36.685000: I runner.py:310] Step = 91600 ; steps/s = 1.63, tokens/s = 43776 (43776 target) ; Learning rate = 0.000292 ; Loss = 1.832127\n",
      "2024-12-10 04:29:38.362000: I runner.py:310] Step = 91700 ; steps/s = 1.62, tokens/s = 44250 (44250 target) ; Learning rate = 0.000292 ; Loss = 1.810664\n",
      "2024-12-10 04:30:40.130000: I runner.py:310] Step = 91800 ; steps/s = 1.62, tokens/s = 44186 (44186 target) ; Learning rate = 0.000292 ; Loss = 1.803051\n",
      "2024-12-10 04:31:41.430000: I runner.py:310] Step = 91900 ; steps/s = 1.63, tokens/s = 43769 (43769 target) ; Learning rate = 0.000292 ; Loss = 1.821524\n",
      "2024-12-10 04:32:43.185000: I runner.py:310] Step = 92000 ; steps/s = 1.62, tokens/s = 44192 (44192 target) ; Learning rate = 0.000291 ; Loss = 1.797740\n",
      "2024-12-10 04:33:44.921000: I runner.py:310] Step = 92100 ; steps/s = 1.62, tokens/s = 44205 (44205 target) ; Learning rate = 0.000291 ; Loss = 1.801762\n",
      "2024-12-10 04:34:46.455000: I runner.py:310] Step = 92200 ; steps/s = 1.63, tokens/s = 43851 (43851 target) ; Learning rate = 0.000291 ; Loss = 1.836144\n",
      "2024-12-10 04:35:48.097000: I runner.py:310] Step = 92300 ; steps/s = 1.62, tokens/s = 44055 (44055 target) ; Learning rate = 0.000291 ; Loss = 1.799755\n",
      "2024-12-10 04:36:49.893000: I runner.py:310] Step = 92400 ; steps/s = 1.62, tokens/s = 44164 (44164 target) ; Learning rate = 0.000291 ; Loss = 1.812259\n",
      "2024-12-10 04:37:51.649000: I runner.py:310] Step = 92500 ; steps/s = 1.62, tokens/s = 44197 (44197 target) ; Learning rate = 0.000291 ; Loss = 1.833482\n",
      "2024-12-10 04:38:52.940000: I runner.py:310] Step = 92600 ; steps/s = 1.63, tokens/s = 43787 (43787 target) ; Learning rate = 0.000290 ; Loss = 1.812991\n",
      "2024-12-10 04:39:54.691000: I runner.py:310] Step = 92700 ; steps/s = 1.62, tokens/s = 44206 (44206 target) ; Learning rate = 0.000290 ; Loss = 1.800167\n",
      "2024-12-10 04:40:56.416000: I runner.py:310] Step = 92800 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000290 ; Loss = 1.796682\n",
      "2024-12-10 04:41:57.726000: I runner.py:310] Step = 92900 ; steps/s = 1.63, tokens/s = 43787 (43787 target) ; Learning rate = 0.000290 ; Loss = 1.805936\n",
      "2024-12-10 04:42:59.402000: I runner.py:310] Step = 93000 ; steps/s = 1.62, tokens/s = 44244 (44244 target) ; Learning rate = 0.000290 ; Loss = 1.815990\n",
      "2024-12-10 04:44:01.158000: I runner.py:310] Step = 93100 ; steps/s = 1.62, tokens/s = 44196 (44196 target) ; Learning rate = 0.000290 ; Loss = 1.817522\n",
      "2024-12-10 04:45:02.943000: I runner.py:310] Step = 93200 ; steps/s = 1.62, tokens/s = 44169 (44169 target) ; Learning rate = 0.000290 ; Loss = 1.807880\n",
      "2024-12-10 04:46:04.237000: I runner.py:310] Step = 93300 ; steps/s = 1.63, tokens/s = 43797 (43797 target) ; Learning rate = 0.000289 ; Loss = 1.787744\n",
      "2024-12-10 04:47:06.037000: I runner.py:310] Step = 93400 ; steps/s = 1.62, tokens/s = 44173 (44173 target) ; Learning rate = 0.000289 ; Loss = 1.804652\n",
      "2024-12-10 04:48:07.761000: I runner.py:310] Step = 93500 ; steps/s = 1.62, tokens/s = 44219 (44219 target) ; Learning rate = 0.000289 ; Loss = 1.814581\n",
      "2024-12-10 04:49:09.120000: I runner.py:310] Step = 93600 ; steps/s = 1.63, tokens/s = 43727 (43727 target) ; Learning rate = 0.000289 ; Loss = 1.798844\n",
      "2024-12-10 04:50:10.830000: I runner.py:310] Step = 93700 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000289 ; Loss = 1.794031\n",
      "2024-12-10 04:51:12.588000: I runner.py:310] Step = 93800 ; steps/s = 1.62, tokens/s = 44200 (44200 target) ; Learning rate = 0.000289 ; Loss = 1.814842\n",
      "2024-12-10 04:52:14.290000: I runner.py:310] Step = 93900 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000288 ; Loss = 1.815776\n",
      "2024-12-10 04:53:15.670000: I runner.py:310] Step = 94000 ; steps/s = 1.63, tokens/s = 43733 (43733 target) ; Learning rate = 0.000288 ; Loss = 1.785282\n",
      "2024-12-10 04:54:17.386000: I runner.py:310] Step = 94100 ; steps/s = 1.62, tokens/s = 44222 (44222 target) ; Learning rate = 0.000288 ; Loss = 1.807967\n",
      "2024-12-10 04:55:19.103000: I runner.py:310] Step = 94200 ; steps/s = 1.62, tokens/s = 44213 (44213 target) ; Learning rate = 0.000288 ; Loss = 1.818817\n",
      "2024-12-10 04:56:20.409000: I runner.py:310] Step = 94300 ; steps/s = 1.63, tokens/s = 43783 (43783 target) ; Learning rate = 0.000288 ; Loss = 1.808281\n",
      "2024-12-10 04:57:22.121000: I runner.py:310] Step = 94400 ; steps/s = 1.62, tokens/s = 44217 (44217 target) ; Learning rate = 0.000288 ; Loss = 1.812254\n",
      "2024-12-10 04:58:23.826000: I runner.py:310] Step = 94500 ; steps/s = 1.62, tokens/s = 44230 (44230 target) ; Learning rate = 0.000288 ; Loss = 1.805475\n",
      "2024-12-10 04:59:25.550000: I runner.py:310] Step = 94600 ; steps/s = 1.62, tokens/s = 44215 (44215 target) ; Learning rate = 0.000287 ; Loss = 1.802738\n",
      "2024-12-10 05:00:26.863000: I runner.py:310] Step = 94700 ; steps/s = 1.63, tokens/s = 43789 (43789 target) ; Learning rate = 0.000287 ; Loss = 1.771670\n",
      "2024-12-10 05:01:28.618000: I runner.py:310] Step = 94800 ; steps/s = 1.62, tokens/s = 44199 (44199 target) ; Learning rate = 0.000287 ; Loss = 1.813452\n",
      "2024-12-10 05:02:30.348000: I runner.py:310] Step = 94900 ; steps/s = 1.62, tokens/s = 44229 (44229 target) ; Learning rate = 0.000287 ; Loss = 1.813618\n",
      "2024-12-10 05:03:31.706000: I runner.py:310] Step = 95000 ; steps/s = 1.63, tokens/s = 43728 (43728 target) ; Learning rate = 0.000287 ; Loss = 1.800848\n",
      "2024-12-10 05:03:31.707000: I training.py:192] Running evaluation for step 95000\n",
      "2024-12-10 05:06:26.119000: I training.py:192] Evaluation result for step 95000: loss = 2.514131 ; perplexity = 12.355871\n",
      "2024-12-10 05:07:27.726000: I runner.py:310] Step = 95100 ; steps/s = 1.62, tokens/s = 44307 (44307 target) ; Learning rate = 0.000287 ; Loss = 1.813831\n",
      "2024-12-10 05:08:29.457000: I runner.py:310] Step = 95200 ; steps/s = 1.62, tokens/s = 44207 (44207 target) ; Learning rate = 0.000286 ; Loss = 1.781998\n",
      "2024-12-10 05:09:31.243000: I runner.py:310] Step = 95300 ; steps/s = 1.62, tokens/s = 44167 (44167 target) ; Learning rate = 0.000286 ; Loss = 1.810219\n",
      "2024-12-10 05:10:32.524000: I runner.py:310] Step = 95400 ; steps/s = 1.63, tokens/s = 43799 (43799 target) ; Learning rate = 0.000286 ; Loss = 1.816464\n",
      "2024-12-10 05:11:34.227000: I runner.py:310] Step = 95500 ; steps/s = 1.62, tokens/s = 44229 (44229 target) ; Learning rate = 0.000286 ; Loss = 1.781081\n",
      "2024-12-10 05:12:36.029000: I runner.py:310] Step = 95600 ; steps/s = 1.62, tokens/s = 44160 (44160 target) ; Learning rate = 0.000286 ; Loss = 1.803386\n",
      "2024-12-10 05:13:37.341000: I runner.py:310] Step = 95700 ; steps/s = 1.63, tokens/s = 43780 (43780 target) ; Learning rate = 0.000286 ; Loss = 1.811911\n",
      "2024-12-10 05:14:39.016000: I runner.py:310] Step = 95800 ; steps/s = 1.62, tokens/s = 44254 (44254 target) ; Learning rate = 0.000286 ; Loss = 1.802844\n",
      "2024-12-10 05:15:40.742000: I runner.py:310] Step = 95900 ; steps/s = 1.62, tokens/s = 44219 (44219 target) ; Learning rate = 0.000285 ; Loss = 1.786785\n",
      "2024-12-10 05:16:42.497000: I runner.py:310] Step = 96000 ; steps/s = 1.62, tokens/s = 44193 (44193 target) ; Learning rate = 0.000285 ; Loss = 1.805008\n",
      "2024-12-10 05:17:43.825000: I runner.py:310] Step = 96100 ; steps/s = 1.63, tokens/s = 43761 (43761 target) ; Learning rate = 0.000285 ; Loss = 1.817021\n",
      "2024-12-10 05:18:45.559000: I runner.py:310] Step = 96200 ; steps/s = 1.62, tokens/s = 44222 (44222 target) ; Learning rate = 0.000285 ; Loss = 1.798867\n",
      "2024-12-10 05:19:47.282000: I runner.py:310] Step = 96300 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000285 ; Loss = 1.815515\n",
      "2024-12-10 05:20:48.626000: I runner.py:310] Step = 96400 ; steps/s = 1.63, tokens/s = 43749 (43749 target) ; Learning rate = 0.000285 ; Loss = 1.775781\n",
      "2024-12-10 05:21:50.337000: I runner.py:310] Step = 96500 ; steps/s = 1.62, tokens/s = 44230 (44230 target) ; Learning rate = 0.000285 ; Loss = 1.793024\n",
      "2024-12-10 05:22:52.057000: I runner.py:310] Step = 96600 ; steps/s = 1.62, tokens/s = 44207 (44207 target) ; Learning rate = 0.000284 ; Loss = 1.798886\n",
      "2024-12-10 05:23:53.765000: I runner.py:310] Step = 96700 ; steps/s = 1.62, tokens/s = 44237 (44237 target) ; Learning rate = 0.000284 ; Loss = 1.824589\n",
      "2024-12-10 05:24:55.187000: I runner.py:310] Step = 96800 ; steps/s = 1.63, tokens/s = 43701 (43701 target) ; Learning rate = 0.000284 ; Loss = 1.783679\n",
      "2024-12-10 05:25:56.905000: I runner.py:310] Step = 96900 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000284 ; Loss = 1.802575\n",
      "2024-12-10 05:26:58.684000: I runner.py:310] Step = 97000 ; steps/s = 1.62, tokens/s = 44178 (44178 target) ; Learning rate = 0.000284 ; Loss = 1.816139\n",
      "2024-12-10 05:28:00.006000: I runner.py:310] Step = 97100 ; steps/s = 1.63, tokens/s = 43777 (43777 target) ; Learning rate = 0.000284 ; Loss = 1.820393\n",
      "2024-12-10 05:29:01.716000: I runner.py:310] Step = 97200 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000284 ; Loss = 1.797024\n",
      "2024-12-10 05:30:03.426000: I runner.py:310] Step = 97300 ; steps/s = 1.62, tokens/s = 44230 (44230 target) ; Learning rate = 0.000283 ; Loss = 1.801698\n",
      "2024-12-10 05:31:05.211000: I runner.py:310] Step = 97400 ; steps/s = 1.62, tokens/s = 44171 (44171 target) ; Learning rate = 0.000283 ; Loss = 1.778795\n",
      "2024-12-10 05:32:06.528000: I runner.py:310] Step = 97500 ; steps/s = 1.63, tokens/s = 43787 (43787 target) ; Learning rate = 0.000283 ; Loss = 1.769388\n",
      "2024-12-10 05:33:08.294000: I runner.py:310] Step = 97600 ; steps/s = 1.62, tokens/s = 44167 (44167 target) ; Learning rate = 0.000283 ; Loss = 1.811583\n",
      "2024-12-10 05:34:10.059000: I runner.py:310] Step = 97700 ; steps/s = 1.62, tokens/s = 44196 (44196 target) ; Learning rate = 0.000283 ; Loss = 1.815643\n",
      "2024-12-10 05:35:11.341000: I runner.py:310] Step = 97800 ; steps/s = 1.63, tokens/s = 43791 (43791 target) ; Learning rate = 0.000283 ; Loss = 1.816430\n",
      "2024-12-10 05:36:13.096000: I runner.py:310] Step = 97900 ; steps/s = 1.62, tokens/s = 44202 (44202 target) ; Learning rate = 0.000282 ; Loss = 1.795149\n",
      "2024-12-10 05:37:14.840000: I runner.py:310] Step = 98000 ; steps/s = 1.62, tokens/s = 44199 (44199 target) ; Learning rate = 0.000282 ; Loss = 1.798519\n",
      "2024-12-10 05:38:16.547000: I runner.py:310] Step = 98100 ; steps/s = 1.62, tokens/s = 44227 (44227 target) ; Learning rate = 0.000282 ; Loss = 1.796006\n",
      "2024-12-10 05:39:17.859000: I runner.py:310] Step = 98200 ; steps/s = 1.63, tokens/s = 43767 (43767 target) ; Learning rate = 0.000282 ; Loss = 1.776693\n",
      "2024-12-10 05:40:19.631000: I runner.py:310] Step = 98300 ; steps/s = 1.62, tokens/s = 44169 (44169 target) ; Learning rate = 0.000282 ; Loss = 1.811186\n",
      "2024-12-10 05:41:21.326000: I runner.py:310] Step = 98400 ; steps/s = 1.62, tokens/s = 44244 (44244 target) ; Learning rate = 0.000282 ; Loss = 1.826565\n",
      "2024-12-10 05:42:22.672000: I runner.py:310] Step = 98500 ; steps/s = 1.63, tokens/s = 43777 (43777 target) ; Learning rate = 0.000282 ; Loss = 1.782908\n",
      "2024-12-10 05:43:24.383000: I runner.py:310] Step = 98600 ; steps/s = 1.62, tokens/s = 44216 (44216 target) ; Learning rate = 0.000281 ; Loss = 1.796212\n",
      "2024-12-10 05:44:26.136000: I runner.py:310] Step = 98700 ; steps/s = 1.62, tokens/s = 44195 (44195 target) ; Learning rate = 0.000281 ; Loss = 1.821285\n",
      "2024-12-10 05:45:27.856000: I runner.py:310] Step = 98800 ; steps/s = 1.62, tokens/s = 44217 (44217 target) ; Learning rate = 0.000281 ; Loss = 1.822181\n",
      "2024-12-10 05:46:29.183000: I runner.py:310] Step = 98900 ; steps/s = 1.63, tokens/s = 43754 (43754 target) ; Learning rate = 0.000281 ; Loss = 1.829324\n",
      "2024-12-10 05:47:30.958000: I runner.py:310] Step = 99000 ; steps/s = 1.62, tokens/s = 44196 (44196 target) ; Learning rate = 0.000281 ; Loss = 1.803643\n",
      "2024-12-10 05:48:32.656000: I runner.py:310] Step = 99100 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000281 ; Loss = 1.794812\n",
      "2024-12-10 05:49:34.021000: I runner.py:310] Step = 99200 ; steps/s = 1.63, tokens/s = 43747 (43747 target) ; Learning rate = 0.000281 ; Loss = 1.812603\n",
      "2024-12-10 05:50:35.763000: I runner.py:310] Step = 99300 ; steps/s = 1.62, tokens/s = 44194 (44194 target) ; Learning rate = 0.000280 ; Loss = 1.788379\n",
      "2024-12-10 05:51:37.456000: I runner.py:310] Step = 99400 ; steps/s = 1.62, tokens/s = 44246 (44246 target) ; Learning rate = 0.000280 ; Loss = 1.794470\n",
      "2024-12-10 05:52:39.296000: I runner.py:310] Step = 99500 ; steps/s = 1.62, tokens/s = 44129 (44129 target) ; Learning rate = 0.000280 ; Loss = 1.792142\n",
      "2024-12-10 05:53:40.682000: I runner.py:310] Step = 99600 ; steps/s = 1.63, tokens/s = 43734 (43734 target) ; Learning rate = 0.000280 ; Loss = 1.782362\n",
      "2024-12-10 05:54:42.438000: I runner.py:310] Step = 99700 ; steps/s = 1.62, tokens/s = 44187 (44187 target) ; Learning rate = 0.000280 ; Loss = 1.808916\n",
      "2024-12-10 05:55:44.120000: I runner.py:310] Step = 99800 ; steps/s = 1.62, tokens/s = 44242 (44242 target) ; Learning rate = 0.000280 ; Loss = 1.809568\n",
      "2024-12-10 05:56:45.407000: I runner.py:310] Step = 99900 ; steps/s = 1.63, tokens/s = 43803 (43803 target) ; Learning rate = 0.000280 ; Loss = 1.781636\n",
      "2024-12-10 05:57:47.149000: I runner.py:310] Step = 100000 ; steps/s = 1.62, tokens/s = 44198 (44198 target) ; Learning rate = 0.000280 ; Loss = 1.801448\n",
      "2024-12-10 05:57:49.277000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-100000\n",
      "2024-12-10 05:57:49.277000: I training.py:192] Running evaluation for step 100000\n",
      "2024-12-10 06:00:45.683000: I training.py:192] Evaluation result for step 100000: loss = 2.523175 ; perplexity = 12.468126\n",
      "2024-12-10 06:01:47.220000: I runner.py:310] Step = 100100 ; steps/s = 1.63, tokens/s = 44361 (44361 target) ; Learning rate = 0.000279 ; Loss = 1.813234\n",
      "2024-12-10 06:02:48.960000: I runner.py:310] Step = 100200 ; steps/s = 1.62, tokens/s = 44207 (44207 target) ; Learning rate = 0.000279 ; Loss = 1.811363\n",
      "2024-12-10 06:03:50.390000: I runner.py:310] Step = 100300 ; steps/s = 1.63, tokens/s = 43700 (43700 target) ; Learning rate = 0.000279 ; Loss = 1.807921\n",
      "2024-12-10 06:04:52.120000: I runner.py:310] Step = 100400 ; steps/s = 1.62, tokens/s = 44213 (44213 target) ; Learning rate = 0.000279 ; Loss = 1.780803\n",
      "2024-12-10 06:05:53.868000: I runner.py:310] Step = 100500 ; steps/s = 1.62, tokens/s = 44183 (44183 target) ; Learning rate = 0.000279 ; Loss = 1.788425\n",
      "2024-12-10 06:06:55.238000: I runner.py:310] Step = 100600 ; steps/s = 1.63, tokens/s = 43754 (43754 target) ; Learning rate = 0.000279 ; Loss = 1.817464\n",
      "2024-12-10 06:07:56.992000: I runner.py:310] Step = 100700 ; steps/s = 1.62, tokens/s = 44194 (44194 target) ; Learning rate = 0.000279 ; Loss = 1.789227\n",
      "2024-12-10 06:08:58.707000: I runner.py:310] Step = 100800 ; steps/s = 1.62, tokens/s = 44220 (44220 target) ; Learning rate = 0.000278 ; Loss = 1.780850\n",
      "2024-12-10 06:10:00.112000: I runner.py:310] Step = 100900 ; steps/s = 1.63, tokens/s = 43710 (43710 target) ; Learning rate = 0.000278 ; Loss = 1.804197\n",
      "2024-12-10 06:11:01.871000: I runner.py:310] Step = 101000 ; steps/s = 1.62, tokens/s = 44196 (44196 target) ; Learning rate = 0.000278 ; Loss = 1.794504\n",
      "2024-12-10 06:12:03.580000: I runner.py:310] Step = 101100 ; steps/s = 1.62, tokens/s = 44216 (44216 target) ; Learning rate = 0.000278 ; Loss = 1.806542\n",
      "2024-12-10 06:13:05.381000: I runner.py:310] Step = 101200 ; steps/s = 1.62, tokens/s = 44162 (44162 target) ; Learning rate = 0.000278 ; Loss = 1.823413\n",
      "2024-12-10 06:14:06.685000: I runner.py:310] Step = 101300 ; steps/s = 1.63, tokens/s = 43798 (43798 target) ; Learning rate = 0.000278 ; Loss = 1.806218\n",
      "2024-12-10 06:15:08.489000: I runner.py:310] Step = 101400 ; steps/s = 1.62, tokens/s = 44160 (44160 target) ; Learning rate = 0.000278 ; Loss = 1.786386\n",
      "2024-12-10 06:16:10.292000: I runner.py:310] Step = 101500 ; steps/s = 1.62, tokens/s = 44159 (44159 target) ; Learning rate = 0.000277 ; Loss = 1.787966\n",
      "2024-12-10 06:17:11.601000: I runner.py:310] Step = 101600 ; steps/s = 1.63, tokens/s = 43763 (43763 target) ; Learning rate = 0.000277 ; Loss = 1.799860\n",
      "2024-12-10 06:18:13.399000: I runner.py:310] Step = 101700 ; steps/s = 1.62, tokens/s = 44170 (44170 target) ; Learning rate = 0.000277 ; Loss = 1.791289\n",
      "2024-12-10 06:19:15.148000: I runner.py:310] Step = 101800 ; steps/s = 1.62, tokens/s = 44190 (44190 target) ; Learning rate = 0.000277 ; Loss = 1.786686\n",
      "2024-12-10 06:20:16.905000: I runner.py:310] Step = 101900 ; steps/s = 1.62, tokens/s = 44202 (44202 target) ; Learning rate = 0.000277 ; Loss = 1.806067\n",
      "2024-12-10 06:21:18.278000: I runner.py:310] Step = 102000 ; steps/s = 1.63, tokens/s = 43736 (43736 target) ; Learning rate = 0.000277 ; Loss = 1.771902\n",
      "2024-12-10 06:22:20.077000: I runner.py:310] Step = 102100 ; steps/s = 1.62, tokens/s = 44166 (44166 target) ; Learning rate = 0.000277 ; Loss = 1.808051\n",
      "2024-12-10 06:23:21.925000: I runner.py:310] Step = 102200 ; steps/s = 1.62, tokens/s = 44133 (44133 target) ; Learning rate = 0.000276 ; Loss = 1.813542\n",
      "2024-12-10 06:24:23.327000: I runner.py:310] Step = 102300 ; steps/s = 1.63, tokens/s = 43703 (43703 target) ; Learning rate = 0.000276 ; Loss = 1.799852\n",
      "2024-12-10 06:25:25.039000: I runner.py:310] Step = 102400 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000276 ; Loss = 1.795603\n",
      "2024-12-10 06:26:26.785000: I runner.py:310] Step = 102500 ; steps/s = 1.62, tokens/s = 44197 (44197 target) ; Learning rate = 0.000276 ; Loss = 1.789536\n",
      "2024-12-10 06:27:28.519000: I runner.py:310] Step = 102600 ; steps/s = 1.62, tokens/s = 44209 (44209 target) ; Learning rate = 0.000276 ; Loss = 1.794067\n",
      "2024-12-10 06:28:29.880000: I runner.py:310] Step = 102700 ; steps/s = 1.63, tokens/s = 43742 (43742 target) ; Learning rate = 0.000276 ; Loss = 1.781737\n",
      "2024-12-10 06:29:31.620000: I runner.py:310] Step = 102800 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000276 ; Loss = 1.803531\n",
      "2024-12-10 06:30:33.364000: I runner.py:310] Step = 102900 ; steps/s = 1.62, tokens/s = 44205 (44205 target) ; Learning rate = 0.000276 ; Loss = 1.804579\n",
      "2024-12-10 06:31:34.719000: I runner.py:310] Step = 103000 ; steps/s = 1.63, tokens/s = 43754 (43754 target) ; Learning rate = 0.000275 ; Loss = 1.798291\n",
      "2024-12-10 06:32:36.450000: I runner.py:310] Step = 103100 ; steps/s = 1.62, tokens/s = 44201 (44201 target) ; Learning rate = 0.000275 ; Loss = 1.801875\n",
      "2024-12-10 06:33:38.176000: I runner.py:310] Step = 103200 ; steps/s = 1.62, tokens/s = 44216 (44216 target) ; Learning rate = 0.000275 ; Loss = 1.794870\n",
      "2024-12-10 06:34:39.868000: I runner.py:310] Step = 103300 ; steps/s = 1.62, tokens/s = 44242 (44242 target) ; Learning rate = 0.000275 ; Loss = 1.785370\n",
      "2024-12-10 06:35:41.279000: I runner.py:310] Step = 103400 ; steps/s = 1.63, tokens/s = 43698 (43698 target) ; Learning rate = 0.000275 ; Loss = 1.762522\n",
      "2024-12-10 06:36:43.064000: I runner.py:310] Step = 103500 ; steps/s = 1.62, tokens/s = 44179 (44179 target) ; Learning rate = 0.000275 ; Loss = 1.808568\n",
      "2024-12-10 06:37:44.772000: I runner.py:310] Step = 103600 ; steps/s = 1.62, tokens/s = 44229 (44229 target) ; Learning rate = 0.000275 ; Loss = 1.799510\n",
      "2024-12-10 06:38:46.144000: I runner.py:310] Step = 103700 ; steps/s = 1.63, tokens/s = 43735 (43735 target) ; Learning rate = 0.000274 ; Loss = 1.795592\n",
      "2024-12-10 06:39:47.909000: I runner.py:310] Step = 103800 ; steps/s = 1.62, tokens/s = 44197 (44197 target) ; Learning rate = 0.000274 ; Loss = 1.785705\n",
      "2024-12-10 06:40:49.716000: I runner.py:310] Step = 103900 ; steps/s = 1.62, tokens/s = 44154 (44154 target) ; Learning rate = 0.000274 ; Loss = 1.781573\n",
      "2024-12-10 06:41:51.491000: I runner.py:310] Step = 104000 ; steps/s = 1.62, tokens/s = 44167 (44167 target) ; Learning rate = 0.000274 ; Loss = 1.787679\n",
      "2024-12-10 06:42:52.900000: I runner.py:310] Step = 104100 ; steps/s = 1.63, tokens/s = 43703 (43703 target) ; Learning rate = 0.000274 ; Loss = 1.812048\n",
      "2024-12-10 06:43:54.694000: I runner.py:310] Step = 104200 ; steps/s = 1.62, tokens/s = 44166 (44166 target) ; Learning rate = 0.000274 ; Loss = 1.779781\n",
      "2024-12-10 06:44:56.414000: I runner.py:310] Step = 104300 ; steps/s = 1.62, tokens/s = 44223 (44223 target) ; Learning rate = 0.000274 ; Loss = 1.783963\n",
      "2024-12-10 06:45:57.778000: I runner.py:310] Step = 104400 ; steps/s = 1.63, tokens/s = 43741 (43741 target) ; Learning rate = 0.000274 ; Loss = 1.794511\n",
      "2024-12-10 06:46:59.547000: I runner.py:310] Step = 104500 ; steps/s = 1.62, tokens/s = 44183 (44183 target) ; Learning rate = 0.000273 ; Loss = 1.792531\n",
      "2024-12-10 06:48:01.286000: I runner.py:310] Step = 104600 ; steps/s = 1.62, tokens/s = 44208 (44208 target) ; Learning rate = 0.000273 ; Loss = 1.791208\n",
      "2024-12-10 06:49:03.139000: I runner.py:310] Step = 104700 ; steps/s = 1.62, tokens/s = 44125 (44125 target) ; Learning rate = 0.000273 ; Loss = 1.780475\n",
      "2024-12-10 06:50:04.484000: I runner.py:310] Step = 104800 ; steps/s = 1.63, tokens/s = 43763 (43763 target) ; Learning rate = 0.000273 ; Loss = 1.825772\n",
      "2024-12-10 06:51:06.289000: I runner.py:310] Step = 104900 ; steps/s = 1.62, tokens/s = 44151 (44151 target) ; Learning rate = 0.000273 ; Loss = 1.782102\n",
      "2024-12-10 06:52:08.044000: I runner.py:310] Step = 105000 ; steps/s = 1.62, tokens/s = 44197 (44197 target) ; Learning rate = 0.000273 ; Loss = 1.782816\n",
      "2024-12-10 06:52:08.046000: I training.py:192] Running evaluation for step 105000\n",
      "2024-12-10 06:55:13.958000: I training.py:192] Evaluation result for step 105000: loss = 2.536474 ; perplexity = 12.635044\n",
      "2024-12-10 06:56:15.174000: I runner.py:310] Step = 105100 ; steps/s = 1.63, tokens/s = 43850 (43850 target) ; Learning rate = 0.000273 ; Loss = 1.780824\n",
      "2024-12-10 06:57:16.985000: I runner.py:310] Step = 105200 ; steps/s = 1.62, tokens/s = 44150 (44150 target) ; Learning rate = 0.000273 ; Loss = 1.784226\n",
      "2024-12-10 06:58:18.649000: I runner.py:310] Step = 105300 ; steps/s = 1.62, tokens/s = 44258 (44258 target) ; Learning rate = 0.000272 ; Loss = 1.807355\n",
      "2024-12-10 06:59:20.373000: I runner.py:310] Step = 105400 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000272 ; Loss = 1.809910\n",
      "2024-12-10 07:00:21.773000: I runner.py:310] Step = 105500 ; steps/s = 1.63, tokens/s = 43709 (43709 target) ; Learning rate = 0.000272 ; Loss = 1.814907\n",
      "2024-12-10 07:01:23.617000: I runner.py:310] Step = 105600 ; steps/s = 1.62, tokens/s = 44129 (44129 target) ; Learning rate = 0.000272 ; Loss = 1.779848\n",
      "2024-12-10 07:02:25.439000: I runner.py:310] Step = 105700 ; steps/s = 1.62, tokens/s = 44158 (44158 target) ; Learning rate = 0.000272 ; Loss = 1.792240\n",
      "2024-12-10 07:03:26.787000: I runner.py:310] Step = 105800 ; steps/s = 1.63, tokens/s = 43741 (43741 target) ; Learning rate = 0.000272 ; Loss = 1.792389\n",
      "2024-12-10 07:04:28.546000: I runner.py:310] Step = 105900 ; steps/s = 1.62, tokens/s = 44199 (44199 target) ; Learning rate = 0.000272 ; Loss = 1.795548\n",
      "2024-12-10 07:05:30.335000: I runner.py:310] Step = 106000 ; steps/s = 1.62, tokens/s = 44171 (44171 target) ; Learning rate = 0.000271 ; Loss = 1.793770\n",
      "2024-12-10 07:06:32.112000: I runner.py:310] Step = 106100 ; steps/s = 1.62, tokens/s = 44179 (44179 target) ; Learning rate = 0.000271 ; Loss = 1.810264\n",
      "2024-12-10 07:07:33.434000: I runner.py:310] Step = 106200 ; steps/s = 1.63, tokens/s = 43773 (43773 target) ; Learning rate = 0.000271 ; Loss = 1.799979\n",
      "2024-12-10 07:08:35.139000: I runner.py:310] Step = 106300 ; steps/s = 1.62, tokens/s = 44227 (44227 target) ; Learning rate = 0.000271 ; Loss = 1.777647\n",
      "2024-12-10 07:09:36.878000: I runner.py:310] Step = 106400 ; steps/s = 1.62, tokens/s = 44205 (44205 target) ; Learning rate = 0.000271 ; Loss = 1.787046\n",
      "2024-12-10 07:10:38.259000: I runner.py:310] Step = 106500 ; steps/s = 1.63, tokens/s = 43737 (43737 target) ; Learning rate = 0.000271 ; Loss = 1.784741\n",
      "2024-12-10 07:11:40.024000: I runner.py:310] Step = 106600 ; steps/s = 1.62, tokens/s = 44171 (44171 target) ; Learning rate = 0.000271 ; Loss = 1.790143\n",
      "2024-12-10 07:12:41.785000: I runner.py:310] Step = 106700 ; steps/s = 1.62, tokens/s = 44187 (44187 target) ; Learning rate = 0.000271 ; Loss = 1.800911\n",
      "2024-12-10 07:13:43.517000: I runner.py:310] Step = 106800 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000270 ; Loss = 1.816325\n",
      "2024-12-10 07:14:44.869000: I runner.py:310] Step = 106900 ; steps/s = 1.63, tokens/s = 43755 (43755 target) ; Learning rate = 0.000270 ; Loss = 1.783406\n",
      "2024-12-10 07:15:46.553000: I runner.py:310] Step = 107000 ; steps/s = 1.62, tokens/s = 44244 (44244 target) ; Learning rate = 0.000270 ; Loss = 1.804801\n",
      "2024-12-10 07:16:48.312000: I runner.py:310] Step = 107100 ; steps/s = 1.62, tokens/s = 44196 (44196 target) ; Learning rate = 0.000270 ; Loss = 1.807730\n",
      "2024-12-10 07:17:49.704000: I runner.py:310] Step = 107200 ; steps/s = 1.63, tokens/s = 43700 (43700 target) ; Learning rate = 0.000270 ; Loss = 1.810073\n",
      "2024-12-10 07:18:51.483000: I runner.py:310] Step = 107300 ; steps/s = 1.62, tokens/s = 44189 (44189 target) ; Learning rate = 0.000270 ; Loss = 1.778996\n",
      "2024-12-10 07:19:53.238000: I runner.py:310] Step = 107400 ; steps/s = 1.62, tokens/s = 44206 (44206 target) ; Learning rate = 0.000270 ; Loss = 1.794910\n",
      "2024-12-10 07:20:55.021000: I runner.py:310] Step = 107500 ; steps/s = 1.62, tokens/s = 44156 (44156 target) ; Learning rate = 0.000270 ; Loss = 1.774440\n",
      "2024-12-10 07:21:56.384000: I runner.py:310] Step = 107600 ; steps/s = 1.63, tokens/s = 43740 (43740 target) ; Learning rate = 0.000269 ; Loss = 1.799865\n",
      "2024-12-10 07:22:58.100000: I runner.py:310] Step = 107700 ; steps/s = 1.62, tokens/s = 44222 (44222 target) ; Learning rate = 0.000269 ; Loss = 1.775443\n",
      "2024-12-10 07:23:59.843000: I runner.py:310] Step = 107800 ; steps/s = 1.62, tokens/s = 44208 (44208 target) ; Learning rate = 0.000269 ; Loss = 1.776058\n",
      "2024-12-10 07:25:01.195000: I runner.py:310] Step = 107900 ; steps/s = 1.63, tokens/s = 43752 (43752 target) ; Learning rate = 0.000269 ; Loss = 1.773300\n",
      "2024-12-10 07:26:02.959000: I runner.py:310] Step = 108000 ; steps/s = 1.62, tokens/s = 44185 (44185 target) ; Learning rate = 0.000269 ; Loss = 1.786933\n",
      "2024-12-10 07:27:04.738000: I runner.py:310] Step = 108100 ; steps/s = 1.62, tokens/s = 44180 (44180 target) ; Learning rate = 0.000269 ; Loss = 1.790968\n",
      "2024-12-10 07:28:06.485000: I runner.py:310] Step = 108200 ; steps/s = 1.62, tokens/s = 44187 (44187 target) ; Learning rate = 0.000269 ; Loss = 1.809745\n",
      "2024-12-10 07:29:07.783000: I runner.py:310] Step = 108300 ; steps/s = 1.63, tokens/s = 43782 (43782 target) ; Learning rate = 0.000269 ; Loss = 1.774339\n",
      "2024-12-10 07:30:09.514000: I runner.py:310] Step = 108400 ; steps/s = 1.62, tokens/s = 44227 (44227 target) ; Learning rate = 0.000268 ; Loss = 1.792407\n",
      "2024-12-10 07:31:11.313000: I runner.py:310] Step = 108500 ; steps/s = 1.62, tokens/s = 44158 (44158 target) ; Learning rate = 0.000268 ; Loss = 1.797900\n",
      "2024-12-10 07:32:12.689000: I runner.py:310] Step = 108600 ; steps/s = 1.63, tokens/s = 43732 (43732 target) ; Learning rate = 0.000268 ; Loss = 1.810428\n",
      "2024-12-10 07:33:14.425000: I runner.py:310] Step = 108700 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000268 ; Loss = 1.777635\n",
      "2024-12-10 07:34:16.060000: I runner.py:310] Step = 108800 ; steps/s = 1.62, tokens/s = 44279 (44279 target) ; Learning rate = 0.000268 ; Loss = 1.783720\n",
      "2024-12-10 07:35:17.573000: I runner.py:310] Step = 108900 ; steps/s = 1.63, tokens/s = 43938 (43938 target) ; Learning rate = 0.000268 ; Loss = 1.797011\n",
      "2024-12-10 07:36:19.166000: I runner.py:310] Step = 109000 ; steps/s = 1.62, tokens/s = 43989 (43989 target) ; Learning rate = 0.000268 ; Loss = 1.775083\n",
      "2024-12-10 07:37:20.947000: I runner.py:310] Step = 109100 ; steps/s = 1.62, tokens/s = 44193 (44193 target) ; Learning rate = 0.000268 ; Loss = 1.803682\n",
      "2024-12-10 07:38:22.753000: I runner.py:310] Step = 109200 ; steps/s = 1.62, tokens/s = 44153 (44153 target) ; Learning rate = 0.000267 ; Loss = 1.808668\n",
      "2024-12-10 07:39:24.180000: I runner.py:310] Step = 109300 ; steps/s = 1.63, tokens/s = 43700 (43700 target) ; Learning rate = 0.000267 ; Loss = 1.803433\n",
      "2024-12-10 07:40:25.911000: I runner.py:310] Step = 109400 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000267 ; Loss = 1.792770\n",
      "2024-12-10 07:41:27.659000: I runner.py:310] Step = 109500 ; steps/s = 1.62, tokens/s = 44186 (44186 target) ; Learning rate = 0.000267 ; Loss = 1.787328\n",
      "2024-12-10 07:42:28.979000: I runner.py:310] Step = 109600 ; steps/s = 1.63, tokens/s = 43784 (43784 target) ; Learning rate = 0.000267 ; Loss = 1.794727\n",
      "2024-12-10 07:43:30.681000: I runner.py:310] Step = 109700 ; steps/s = 1.62, tokens/s = 44233 (44233 target) ; Learning rate = 0.000267 ; Loss = 1.772574\n",
      "2024-12-10 07:44:32.455000: I runner.py:310] Step = 109800 ; steps/s = 1.62, tokens/s = 44180 (44180 target) ; Learning rate = 0.000267 ; Loss = 1.792210\n",
      "2024-12-10 07:45:34.277000: I runner.py:310] Step = 109900 ; steps/s = 1.62, tokens/s = 44143 (44143 target) ; Learning rate = 0.000267 ; Loss = 1.805229\n",
      "2024-12-10 07:46:35.617000: I runner.py:310] Step = 110000 ; steps/s = 1.63, tokens/s = 43766 (43766 target) ; Learning rate = 0.000266 ; Loss = 1.769238\n",
      "2024-12-10 07:46:37.835000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-110000\n",
      "2024-12-10 07:46:37.835000: I training.py:192] Running evaluation for step 110000\n",
      "2024-12-10 07:49:44.421000: I training.py:192] Evaluation result for step 110000: loss = 2.544062 ; perplexity = 12.731283\n",
      "2024-12-10 07:50:46.020000: I runner.py:310] Step = 110100 ; steps/s = 1.62, tokens/s = 44315 (44315 target) ; Learning rate = 0.000266 ; Loss = 1.797386\n",
      "2024-12-10 07:51:47.805000: I runner.py:310] Step = 110200 ; steps/s = 1.62, tokens/s = 44171 (44171 target) ; Learning rate = 0.000266 ; Loss = 1.813261\n",
      "2024-12-10 07:52:49.189000: I runner.py:310] Step = 110300 ; steps/s = 1.63, tokens/s = 43723 (43723 target) ; Learning rate = 0.000266 ; Loss = 1.784750\n",
      "2024-12-10 07:53:50.978000: I runner.py:310] Step = 110400 ; steps/s = 1.62, tokens/s = 44180 (44180 target) ; Learning rate = 0.000266 ; Loss = 1.770228\n",
      "2024-12-10 07:54:52.763000: I runner.py:310] Step = 110500 ; steps/s = 1.62, tokens/s = 44181 (44181 target) ; Learning rate = 0.000266 ; Loss = 1.789473\n",
      "2024-12-10 07:55:54.503000: I runner.py:310] Step = 110600 ; steps/s = 1.62, tokens/s = 44194 (44194 target) ; Learning rate = 0.000266 ; Loss = 1.806673\n",
      "2024-12-10 07:56:55.903000: I runner.py:310] Step = 110700 ; steps/s = 1.63, tokens/s = 43702 (43702 target) ; Learning rate = 0.000266 ; Loss = 1.809798\n",
      "2024-12-10 07:57:57.608000: I runner.py:310] Step = 110800 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000266 ; Loss = 1.785930\n",
      "2024-12-10 07:58:59.389000: I runner.py:310] Step = 110900 ; steps/s = 1.62, tokens/s = 44190 (44190 target) ; Learning rate = 0.000265 ; Loss = 1.780855\n",
      "2024-12-10 08:00:00.695000: I runner.py:310] Step = 111000 ; steps/s = 1.63, tokens/s = 43778 (43778 target) ; Learning rate = 0.000265 ; Loss = 1.798078\n",
      "2024-12-10 08:01:02.373000: I runner.py:310] Step = 111100 ; steps/s = 1.62, tokens/s = 44251 (44251 target) ; Learning rate = 0.000265 ; Loss = 1.787755\n",
      "2024-12-10 08:02:04.128000: I runner.py:310] Step = 111200 ; steps/s = 1.62, tokens/s = 44196 (44196 target) ; Learning rate = 0.000265 ; Loss = 1.772452\n",
      "2024-12-10 08:03:05.926000: I runner.py:310] Step = 111300 ; steps/s = 1.62, tokens/s = 44167 (44167 target) ; Learning rate = 0.000265 ; Loss = 1.781077\n",
      "2024-12-10 08:04:07.306000: I runner.py:310] Step = 111400 ; steps/s = 1.63, tokens/s = 43719 (43719 target) ; Learning rate = 0.000265 ; Loss = 1.759472\n",
      "2024-12-10 08:05:09.128000: I runner.py:310] Step = 111500 ; steps/s = 1.62, tokens/s = 44140 (44140 target) ; Learning rate = 0.000265 ; Loss = 1.788986\n",
      "2024-12-10 08:06:10.903000: I runner.py:310] Step = 111600 ; steps/s = 1.62, tokens/s = 44192 (44192 target) ; Learning rate = 0.000265 ; Loss = 1.793370\n",
      "2024-12-10 08:07:12.262000: I runner.py:310] Step = 111700 ; steps/s = 1.63, tokens/s = 43743 (43743 target) ; Learning rate = 0.000264 ; Loss = 1.794510\n",
      "2024-12-10 08:08:14.078000: I runner.py:310] Step = 111800 ; steps/s = 1.62, tokens/s = 44151 (44151 target) ; Learning rate = 0.000264 ; Loss = 1.800583\n",
      "2024-12-10 08:09:15.857000: I runner.py:310] Step = 111900 ; steps/s = 1.62, tokens/s = 44182 (44182 target) ; Learning rate = 0.000264 ; Loss = 1.779489\n",
      "2024-12-10 08:10:17.560000: I runner.py:310] Step = 112000 ; steps/s = 1.62, tokens/s = 44222 (44222 target) ; Learning rate = 0.000264 ; Loss = 1.796399\n",
      "2024-12-10 08:11:18.870000: I runner.py:310] Step = 112100 ; steps/s = 1.63, tokens/s = 43783 (43783 target) ; Learning rate = 0.000264 ; Loss = 1.814376\n",
      "2024-12-10 08:12:20.658000: I runner.py:310] Step = 112200 ; steps/s = 1.62, tokens/s = 44180 (44180 target) ; Learning rate = 0.000264 ; Loss = 1.774289\n",
      "2024-12-10 08:13:22.380000: I runner.py:310] Step = 112300 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000264 ; Loss = 1.790582\n",
      "2024-12-10 08:14:23.764000: I runner.py:310] Step = 112400 ; steps/s = 1.63, tokens/s = 43723 (43723 target) ; Learning rate = 0.000264 ; Loss = 1.768274\n",
      "2024-12-10 08:15:25.556000: I runner.py:310] Step = 112500 ; steps/s = 1.62, tokens/s = 44173 (44173 target) ; Learning rate = 0.000264 ; Loss = 1.778835\n",
      "2024-12-10 08:16:27.266000: I runner.py:310] Step = 112600 ; steps/s = 1.62, tokens/s = 44227 (44227 target) ; Learning rate = 0.000263 ; Loss = 1.798701\n",
      "2024-12-10 08:17:29.062000: I runner.py:310] Step = 112700 ; steps/s = 1.62, tokens/s = 44167 (44167 target) ; Learning rate = 0.000263 ; Loss = 1.807080\n",
      "2024-12-10 08:18:30.531000: I runner.py:310] Step = 112800 ; steps/s = 1.63, tokens/s = 43667 (43667 target) ; Learning rate = 0.000263 ; Loss = 1.761639\n",
      "2024-12-10 08:19:32.340000: I runner.py:310] Step = 112900 ; steps/s = 1.62, tokens/s = 44136 (44136 target) ; Learning rate = 0.000263 ; Loss = 1.791975\n",
      "2024-12-10 08:20:34.128000: I runner.py:310] Step = 113000 ; steps/s = 1.62, tokens/s = 44174 (44174 target) ; Learning rate = 0.000263 ; Loss = 1.813334\n",
      "2024-12-10 08:21:35.508000: I runner.py:310] Step = 113100 ; steps/s = 1.63, tokens/s = 43727 (43727 target) ; Learning rate = 0.000263 ; Loss = 1.778576\n",
      "2024-12-10 08:22:37.287000: I runner.py:310] Step = 113200 ; steps/s = 1.62, tokens/s = 44194 (44194 target) ; Learning rate = 0.000263 ; Loss = 1.786868\n",
      "2024-12-10 08:23:39.042000: I runner.py:310] Step = 113300 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000263 ; Loss = 1.797052\n",
      "2024-12-10 08:24:40.869000: I runner.py:310] Step = 113400 ; steps/s = 1.62, tokens/s = 44137 (44137 target) ; Learning rate = 0.000262 ; Loss = 1.795078\n",
      "2024-12-10 08:25:42.269000: I runner.py:310] Step = 113500 ; steps/s = 1.63, tokens/s = 43723 (43723 target) ; Learning rate = 0.000262 ; Loss = 1.809484\n",
      "2024-12-10 08:26:44.006000: I runner.py:310] Step = 113600 ; steps/s = 1.62, tokens/s = 44187 (44187 target) ; Learning rate = 0.000262 ; Loss = 1.773613\n",
      "2024-12-10 08:27:45.787000: I runner.py:310] Step = 113700 ; steps/s = 1.62, tokens/s = 44198 (44198 target) ; Learning rate = 0.000262 ; Loss = 1.776145\n",
      "2024-12-10 08:28:47.112000: I runner.py:310] Step = 113800 ; steps/s = 1.63, tokens/s = 43756 (43756 target) ; Learning rate = 0.000262 ; Loss = 1.791091\n",
      "2024-12-10 08:29:48.839000: I runner.py:310] Step = 113900 ; steps/s = 1.62, tokens/s = 44217 (44217 target) ; Learning rate = 0.000262 ; Loss = 1.776858\n",
      "2024-12-10 08:30:50.630000: I runner.py:310] Step = 114000 ; steps/s = 1.62, tokens/s = 44163 (44163 target) ; Learning rate = 0.000262 ; Loss = 1.772698\n",
      "2024-12-10 08:31:52.403000: I runner.py:310] Step = 114100 ; steps/s = 1.62, tokens/s = 44177 (44177 target) ; Learning rate = 0.000262 ; Loss = 1.771482\n",
      "2024-12-10 08:32:53.748000: I runner.py:310] Step = 114200 ; steps/s = 1.63, tokens/s = 43759 (43759 target) ; Learning rate = 0.000262 ; Loss = 1.748595\n",
      "2024-12-10 08:33:55.559000: I runner.py:310] Step = 114300 ; steps/s = 1.62, tokens/s = 44161 (44161 target) ; Learning rate = 0.000261 ; Loss = 1.787432\n",
      "2024-12-10 08:34:57.349000: I runner.py:310] Step = 114400 ; steps/s = 1.62, tokens/s = 44160 (44160 target) ; Learning rate = 0.000261 ; Loss = 1.791078\n",
      "2024-12-10 08:35:58.723000: I runner.py:310] Step = 114500 ; steps/s = 1.63, tokens/s = 43725 (43725 target) ; Learning rate = 0.000261 ; Loss = 1.776180\n",
      "2024-12-10 08:37:00.472000: I runner.py:310] Step = 114600 ; steps/s = 1.62, tokens/s = 44193 (44193 target) ; Learning rate = 0.000261 ; Loss = 1.783806\n",
      "2024-12-10 08:38:02.329000: I runner.py:310] Step = 114700 ; steps/s = 1.62, tokens/s = 44123 (44123 target) ; Learning rate = 0.000261 ; Loss = 1.806480\n",
      "2024-12-10 08:39:04.070000: I runner.py:310] Step = 114800 ; steps/s = 1.62, tokens/s = 44208 (44208 target) ; Learning rate = 0.000261 ; Loss = 1.792311\n",
      "2024-12-10 08:40:05.451000: I runner.py:310] Step = 114900 ; steps/s = 1.63, tokens/s = 43729 (43729 target) ; Learning rate = 0.000261 ; Loss = 1.794028\n",
      "2024-12-10 08:41:07.225000: I runner.py:310] Step = 115000 ; steps/s = 1.62, tokens/s = 44177 (44177 target) ; Learning rate = 0.000261 ; Loss = 1.780916\n",
      "2024-12-10 08:41:07.227000: I training.py:192] Running evaluation for step 115000\n",
      "2024-12-10 08:44:06.277000: I training.py:192] Evaluation result for step 115000: loss = 2.549998 ; perplexity = 12.807073\n",
      "2024-12-10 08:45:07.902000: I runner.py:310] Step = 115100 ; steps/s = 1.62, tokens/s = 44300 (44300 target) ; Learning rate = 0.000261 ; Loss = 1.783442\n",
      "2024-12-10 08:46:09.308000: I runner.py:310] Step = 115200 ; steps/s = 1.63, tokens/s = 43718 (43718 target) ; Learning rate = 0.000260 ; Loss = 1.768004\n",
      "2024-12-10 08:47:11.095000: I runner.py:310] Step = 115300 ; steps/s = 1.62, tokens/s = 44182 (44182 target) ; Learning rate = 0.000260 ; Loss = 1.779387\n",
      "2024-12-10 08:48:12.875000: I runner.py:310] Step = 115400 ; steps/s = 1.62, tokens/s = 44173 (44173 target) ; Learning rate = 0.000260 ; Loss = 1.791560\n",
      "2024-12-10 08:49:14.631000: I runner.py:310] Step = 115500 ; steps/s = 1.62, tokens/s = 44188 (44188 target) ; Learning rate = 0.000260 ; Loss = 1.800746\n",
      "2024-12-10 08:50:15.942000: I runner.py:310] Step = 115600 ; steps/s = 1.63, tokens/s = 43773 (43773 target) ; Learning rate = 0.000260 ; Loss = 1.808578\n",
      "2024-12-10 08:51:17.757000: I runner.py:310] Step = 115700 ; steps/s = 1.62, tokens/s = 44148 (44148 target) ; Learning rate = 0.000260 ; Loss = 1.775746\n",
      "2024-12-10 08:52:19.563000: I runner.py:310] Step = 115800 ; steps/s = 1.62, tokens/s = 44161 (44161 target) ; Learning rate = 0.000260 ; Loss = 1.783989\n",
      "2024-12-10 08:53:20.925000: I runner.py:310] Step = 115900 ; steps/s = 1.63, tokens/s = 43736 (43736 target) ; Learning rate = 0.000260 ; Loss = 1.788391\n",
      "2024-12-10 08:54:22.734000: I runner.py:310] Step = 116000 ; steps/s = 1.62, tokens/s = 44159 (44159 target) ; Learning rate = 0.000260 ; Loss = 1.765354\n",
      "2024-12-10 08:55:24.468000: I runner.py:310] Step = 116100 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000259 ; Loss = 1.781696\n",
      "2024-12-10 08:56:26.216000: I runner.py:310] Step = 116200 ; steps/s = 1.62, tokens/s = 44201 (44201 target) ; Learning rate = 0.000259 ; Loss = 1.778278\n",
      "2024-12-10 08:57:27.602000: I runner.py:310] Step = 116300 ; steps/s = 1.63, tokens/s = 43727 (43727 target) ; Learning rate = 0.000259 ; Loss = 1.806308\n",
      "2024-12-10 08:58:29.362000: I runner.py:310] Step = 116400 ; steps/s = 1.62, tokens/s = 44190 (44190 target) ; Learning rate = 0.000259 ; Loss = 1.775227\n",
      "2024-12-10 08:59:31.141000: I runner.py:310] Step = 116500 ; steps/s = 1.62, tokens/s = 44178 (44178 target) ; Learning rate = 0.000259 ; Loss = 1.776919\n",
      "2024-12-10 09:00:32.543000: I runner.py:310] Step = 116600 ; steps/s = 1.63, tokens/s = 43699 (43699 target) ; Learning rate = 0.000259 ; Loss = 1.790900\n",
      "2024-12-10 09:01:34.328000: I runner.py:310] Step = 116700 ; steps/s = 1.62, tokens/s = 44181 (44181 target) ; Learning rate = 0.000259 ; Loss = 1.767212\n",
      "2024-12-10 09:02:36.101000: I runner.py:310] Step = 116800 ; steps/s = 1.62, tokens/s = 44179 (44179 target) ; Learning rate = 0.000259 ; Loss = 1.773981\n",
      "2024-12-10 09:03:37.888000: I runner.py:310] Step = 116900 ; steps/s = 1.62, tokens/s = 44167 (44167 target) ; Learning rate = 0.000259 ; Loss = 1.783187\n",
      "2024-12-10 09:04:39.208000: I runner.py:310] Step = 117000 ; steps/s = 1.63, tokens/s = 43780 (43780 target) ; Learning rate = 0.000258 ; Loss = 1.798122\n",
      "2024-12-10 09:05:40.976000: I runner.py:310] Step = 117100 ; steps/s = 1.62, tokens/s = 44179 (44179 target) ; Learning rate = 0.000258 ; Loss = 1.772158\n",
      "2024-12-10 09:06:42.729000: I runner.py:310] Step = 117200 ; steps/s = 1.62, tokens/s = 44190 (44190 target) ; Learning rate = 0.000258 ; Loss = 1.770318\n",
      "2024-12-10 09:07:44.057000: I runner.py:310] Step = 117300 ; steps/s = 1.63, tokens/s = 43767 (43767 target) ; Learning rate = 0.000258 ; Loss = 1.750135\n",
      "2024-12-10 09:08:45.906000: I runner.py:310] Step = 117400 ; steps/s = 1.62, tokens/s = 44125 (44125 target) ; Learning rate = 0.000258 ; Loss = 1.782105\n",
      "2024-12-10 09:09:47.626000: I runner.py:310] Step = 117500 ; steps/s = 1.62, tokens/s = 44213 (44213 target) ; Learning rate = 0.000258 ; Loss = 1.796940\n",
      "2024-12-10 09:10:49.034000: I runner.py:310] Step = 117600 ; steps/s = 1.63, tokens/s = 43725 (43725 target) ; Learning rate = 0.000258 ; Loss = 1.815024\n",
      "2024-12-10 09:11:50.868000: I runner.py:310] Step = 117700 ; steps/s = 1.62, tokens/s = 44134 (44134 target) ; Learning rate = 0.000258 ; Loss = 1.773446\n",
      "2024-12-10 09:12:52.672000: I runner.py:310] Step = 117800 ; steps/s = 1.62, tokens/s = 44160 (44160 target) ; Learning rate = 0.000258 ; Loss = 1.786022\n",
      "2024-12-10 09:13:54.402000: I runner.py:310] Step = 117900 ; steps/s = 1.62, tokens/s = 44217 (44217 target) ; Learning rate = 0.000257 ; Loss = 1.789341\n",
      "2024-12-10 09:14:55.866000: I runner.py:310] Step = 118000 ; steps/s = 1.63, tokens/s = 43668 (43668 target) ; Learning rate = 0.000257 ; Loss = 1.753456\n",
      "2024-12-10 09:15:57.620000: I runner.py:310] Step = 118100 ; steps/s = 1.62, tokens/s = 44189 (44189 target) ; Learning rate = 0.000257 ; Loss = 1.782789\n",
      "2024-12-10 09:16:59.472000: I runner.py:310] Step = 118200 ; steps/s = 1.62, tokens/s = 44130 (44130 target) ; Learning rate = 0.000257 ; Loss = 1.793608\n",
      "2024-12-10 09:18:00.796000: I runner.py:310] Step = 118300 ; steps/s = 1.63, tokens/s = 43769 (43769 target) ; Learning rate = 0.000257 ; Loss = 1.780634\n",
      "2024-12-10 09:19:02.532000: I runner.py:310] Step = 118400 ; steps/s = 1.62, tokens/s = 44207 (44207 target) ; Learning rate = 0.000257 ; Loss = 1.784337\n",
      "2024-12-10 09:20:04.270000: I runner.py:310] Step = 118500 ; steps/s = 1.62, tokens/s = 44205 (44205 target) ; Learning rate = 0.000257 ; Loss = 1.774100\n",
      "2024-12-10 09:21:06.109000: I runner.py:310] Step = 118600 ; steps/s = 1.62, tokens/s = 44131 (44131 target) ; Learning rate = 0.000257 ; Loss = 1.771620\n",
      "2024-12-10 09:22:07.529000: I runner.py:310] Step = 118700 ; steps/s = 1.63, tokens/s = 43706 (43706 target) ; Learning rate = 0.000257 ; Loss = 1.785166\n",
      "2024-12-10 09:23:09.285000: I runner.py:310] Step = 118800 ; steps/s = 1.62, tokens/s = 44184 (44184 target) ; Learning rate = 0.000256 ; Loss = 1.769687\n",
      "2024-12-10 09:24:11.076000: I runner.py:310] Step = 118900 ; steps/s = 1.62, tokens/s = 44176 (44176 target) ; Learning rate = 0.000256 ; Loss = 1.770676\n",
      "2024-12-10 09:25:12.506000: I runner.py:310] Step = 119000 ; steps/s = 1.63, tokens/s = 43690 (43690 target) ; Learning rate = 0.000256 ; Loss = 1.778336\n",
      "2024-12-10 09:26:14.213000: I runner.py:310] Step = 119100 ; steps/s = 1.62, tokens/s = 44245 (44245 target) ; Learning rate = 0.000256 ; Loss = 1.772427\n",
      "2024-12-10 09:27:16.019000: I runner.py:310] Step = 119200 ; steps/s = 1.62, tokens/s = 44152 (44152 target) ; Learning rate = 0.000256 ; Loss = 1.765434\n",
      "2024-12-10 09:28:17.766000: I runner.py:310] Step = 119300 ; steps/s = 1.62, tokens/s = 44190 (44190 target) ; Learning rate = 0.000256 ; Loss = 1.775857\n",
      "2024-12-10 09:29:19.184000: I runner.py:310] Step = 119400 ; steps/s = 1.63, tokens/s = 43710 (43710 target) ; Learning rate = 0.000256 ; Loss = 1.748367\n",
      "2024-12-10 09:30:20.927000: I runner.py:310] Step = 119500 ; steps/s = 1.62, tokens/s = 44200 (44200 target) ; Learning rate = 0.000256 ; Loss = 1.782177\n",
      "2024-12-10 09:31:22.732000: I runner.py:310] Step = 119600 ; steps/s = 1.62, tokens/s = 44158 (44158 target) ; Learning rate = 0.000256 ; Loss = 1.784340\n",
      "2024-12-10 09:32:24.124000: I runner.py:310] Step = 119700 ; steps/s = 1.63, tokens/s = 43714 (43714 target) ; Learning rate = 0.000255 ; Loss = 1.767512\n",
      "2024-12-10 09:33:25.855000: I runner.py:310] Step = 119800 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000255 ; Loss = 1.774518\n",
      "2024-12-10 09:34:27.688000: I runner.py:310] Step = 119900 ; steps/s = 1.62, tokens/s = 44143 (44143 target) ; Learning rate = 0.000255 ; Loss = 1.781699\n",
      "2024-12-10 09:35:29.411000: I runner.py:310] Step = 120000 ; steps/s = 1.62, tokens/s = 44209 (44209 target) ; Learning rate = 0.000255 ; Loss = 1.781878\n",
      "2024-12-10 09:35:31.554000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-120000\n",
      "2024-12-10 09:35:31.555000: I training.py:192] Running evaluation for step 120000\n",
      "2024-12-10 09:38:22.791000: I training.py:192] Evaluation result for step 120000: loss = 2.550663 ; perplexity = 12.815598\n",
      "2024-12-10 09:39:24.002000: I runner.py:310] Step = 120100 ; steps/s = 1.63, tokens/s = 43867 (43867 target) ; Learning rate = 0.000255 ; Loss = 1.795747\n",
      "2024-12-10 09:40:25.756000: I runner.py:310] Step = 120200 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000255 ; Loss = 1.754753\n",
      "2024-12-10 09:41:27.521000: I runner.py:310] Step = 120300 ; steps/s = 1.62, tokens/s = 44188 (44188 target) ; Learning rate = 0.000255 ; Loss = 1.770965\n",
      "2024-12-10 09:42:28.817000: I runner.py:310] Step = 120400 ; steps/s = 1.63, tokens/s = 43783 (43783 target) ; Learning rate = 0.000255 ; Loss = 1.770477\n",
      "2024-12-10 09:43:30.520000: I runner.py:310] Step = 120500 ; steps/s = 1.62, tokens/s = 44238 (44238 target) ; Learning rate = 0.000255 ; Loss = 1.762287\n",
      "2024-12-10 09:44:32.260000: I runner.py:310] Step = 120600 ; steps/s = 1.62, tokens/s = 44193 (44193 target) ; Learning rate = 0.000255 ; Loss = 1.790454\n",
      "2024-12-10 09:45:34.014000: I runner.py:310] Step = 120700 ; steps/s = 1.62, tokens/s = 44194 (44194 target) ; Learning rate = 0.000254 ; Loss = 1.796369\n",
      "2024-12-10 09:46:35.356000: I runner.py:310] Step = 120800 ; steps/s = 1.63, tokens/s = 43772 (43772 target) ; Learning rate = 0.000254 ; Loss = 1.800470\n",
      "2024-12-10 09:47:37.142000: I runner.py:310] Step = 120900 ; steps/s = 1.62, tokens/s = 44174 (44174 target) ; Learning rate = 0.000254 ; Loss = 1.772169\n",
      "2024-12-10 09:48:38.845000: I runner.py:310] Step = 121000 ; steps/s = 1.62, tokens/s = 44222 (44222 target) ; Learning rate = 0.000254 ; Loss = 1.774028\n",
      "2024-12-10 09:49:40.171000: I runner.py:310] Step = 121100 ; steps/s = 1.63, tokens/s = 43768 (43768 target) ; Learning rate = 0.000254 ; Loss = 1.778139\n",
      "2024-12-10 09:50:41.878000: I runner.py:310] Step = 121200 ; steps/s = 1.62, tokens/s = 44241 (44241 target) ; Learning rate = 0.000254 ; Loss = 1.772256\n",
      "2024-12-10 09:51:43.612000: I runner.py:310] Step = 121300 ; steps/s = 1.62, tokens/s = 44200 (44200 target) ; Learning rate = 0.000254 ; Loss = 1.788776\n",
      "2024-12-10 09:52:45.414000: I runner.py:310] Step = 121400 ; steps/s = 1.62, tokens/s = 44153 (44153 target) ; Learning rate = 0.000254 ; Loss = 1.799788\n",
      "2024-12-10 09:53:46.827000: I runner.py:310] Step = 121500 ; steps/s = 1.63, tokens/s = 43725 (43725 target) ; Learning rate = 0.000254 ; Loss = 1.796812\n",
      "2024-12-10 09:54:48.600000: I runner.py:310] Step = 121600 ; steps/s = 1.62, tokens/s = 44180 (44180 target) ; Learning rate = 0.000253 ; Loss = 1.773358\n",
      "2024-12-10 09:55:50.376000: I runner.py:310] Step = 121700 ; steps/s = 1.62, tokens/s = 44170 (44170 target) ; Learning rate = 0.000253 ; Loss = 1.776646\n",
      "2024-12-10 09:56:51.825000: I runner.py:310] Step = 121800 ; steps/s = 1.63, tokens/s = 43676 (43676 target) ; Learning rate = 0.000253 ; Loss = 1.768748\n",
      "2024-12-10 09:57:53.554000: I runner.py:310] Step = 121900 ; steps/s = 1.62, tokens/s = 44223 (44223 target) ; Learning rate = 0.000253 ; Loss = 1.780699\n",
      "2024-12-10 09:58:55.343000: I runner.py:310] Step = 122000 ; steps/s = 1.62, tokens/s = 44149 (44149 target) ; Learning rate = 0.000253 ; Loss = 1.795101\n",
      "2024-12-10 09:59:57.128000: I runner.py:310] Step = 122100 ; steps/s = 1.62, tokens/s = 44174 (44174 target) ; Learning rate = 0.000253 ; Loss = 1.783344\n",
      "2024-12-10 10:00:58.458000: I runner.py:310] Step = 122200 ; steps/s = 1.63, tokens/s = 43775 (43775 target) ; Learning rate = 0.000253 ; Loss = 1.767464\n",
      "2024-12-10 10:02:00.260000: I runner.py:310] Step = 122300 ; steps/s = 1.62, tokens/s = 44171 (44171 target) ; Learning rate = 0.000253 ; Loss = 1.771043\n",
      "2024-12-10 10:03:02.039000: I runner.py:310] Step = 122400 ; steps/s = 1.62, tokens/s = 44165 (44165 target) ; Learning rate = 0.000253 ; Loss = 1.774942\n",
      "2024-12-10 10:04:03.413000: I runner.py:310] Step = 122500 ; steps/s = 1.63, tokens/s = 43732 (43732 target) ; Learning rate = 0.000253 ; Loss = 1.768270\n",
      "2024-12-10 10:05:05.203000: I runner.py:310] Step = 122600 ; steps/s = 1.62, tokens/s = 44175 (44175 target) ; Learning rate = 0.000252 ; Loss = 1.773130\n",
      "2024-12-10 10:06:06.993000: I runner.py:310] Step = 122700 ; steps/s = 1.62, tokens/s = 44163 (44163 target) ; Learning rate = 0.000252 ; Loss = 1.770936\n",
      "2024-12-10 10:07:08.761000: I runner.py:310] Step = 122800 ; steps/s = 1.62, tokens/s = 44187 (44187 target) ; Learning rate = 0.000252 ; Loss = 1.788265\n",
      "2024-12-10 10:08:10.141000: I runner.py:310] Step = 122900 ; steps/s = 1.63, tokens/s = 43725 (43725 target) ; Learning rate = 0.000252 ; Loss = 1.791840\n",
      "2024-12-10 10:09:11.865000: I runner.py:310] Step = 123000 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000252 ; Loss = 1.763290\n",
      "2024-12-10 10:10:13.626000: I runner.py:310] Step = 123100 ; steps/s = 1.62, tokens/s = 44201 (44201 target) ; Learning rate = 0.000252 ; Loss = 1.767281\n",
      "2024-12-10 10:11:15.049000: I runner.py:310] Step = 123200 ; steps/s = 1.63, tokens/s = 43687 (43687 target) ; Learning rate = 0.000252 ; Loss = 1.767300\n",
      "2024-12-10 10:12:16.794000: I runner.py:310] Step = 123300 ; steps/s = 1.62, tokens/s = 44200 (44200 target) ; Learning rate = 0.000252 ; Loss = 1.773846\n",
      "2024-12-10 10:13:18.528000: I runner.py:310] Step = 123400 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000252 ; Loss = 1.787260\n",
      "2024-12-10 10:14:20.300000: I runner.py:310] Step = 123500 ; steps/s = 1.62, tokens/s = 44189 (44189 target) ; Learning rate = 0.000252 ; Loss = 1.786025\n",
      "2024-12-10 10:15:21.719000: I runner.py:310] Step = 123600 ; steps/s = 1.63, tokens/s = 43697 (43697 target) ; Learning rate = 0.000251 ; Loss = 1.794185\n",
      "2024-12-10 10:16:23.486000: I runner.py:310] Step = 123700 ; steps/s = 1.62, tokens/s = 44179 (44179 target) ; Learning rate = 0.000251 ; Loss = 1.764349\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Tr-En (TED2020)(POS Tags) -> Kk-En (POS Tags)\n",
    "!onmt-main --model kk-tr-en-modelim.py --config kk-tr-en-pos.yml --auto_config train --with_eval --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18b6183a-35a3-451a-93c8-7c84bf96ccc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-10 10:17:05.140603: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-10 10:17:06.272511: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-10 10:17:06.272698: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-10 10:17:06.272707: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-10 10:17:07.867000: I onmt-main:8] Creating model directory POS_TR_KK_EN-2\n",
      "2024-12-10 10:17:08.088000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-10 10:17:08.089000: I main.py:315] Using model:\n",
      "(model): MultiFeaturesTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): ParallelInputter(\n",
      "      (inputters): ListWrapper(\n",
      "        (0): WordEmbedder()\n",
      "        (1): WordEmbedder()\n",
      "      )\n",
      "      (reducer): ConcatReducer()\n",
      "    )\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): ParallelInputter(\n",
      "        (inputters): ListWrapper(\n",
      "          (0): WordEmbedder()\n",
      "          (1): WordEmbedder()\n",
      "        )\n",
      "        (reducer): ConcatReducer()\n",
      "      )\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-10 10:17:08.094570: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-10 10:17:11.024874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-10 10:17:11.025506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7800 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-12-10 10:17:11.025982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 674 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "2024-12-10 10:17:11.029000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file:\n",
      "  - KK_tokens_valid_shared\n",
      "  - KK_pos_tags_valid_shared.txt\n",
      "  eval_labels_file: KK_valid_target_tokens_shared.txt\n",
      "  source_1_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  source_2_vocabulary: RoBERTa_KK_unique_pos\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file:\n",
      "  - KK_tokens_train_shared\n",
      "  - KK_pos_tags_train_shared.txt\n",
      "  train_labels_file: KK_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: POS_TR_KK_EN-2\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 205000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-10 10:17:11.410000: I inputter.py:316] Initialized source_1 input layer:\n",
      "2024-12-10 10:17:11.410000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-10 10:17:11.410000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-10 10:17:11.416000: I inputter.py:316] Initialized source_2 input layer:\n",
      "2024-12-10 10:17:11.416000: I inputter.py:316]  - vocabulary size: 21\n",
      "2024-12-10 10:17:11.416000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-10 10:17:11.492000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-10 10:17:11.492000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-10 10:17:11.492000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-10 10:17:11.521000: I runner.py:269] Restored checkpoint POS_TR_KK_EN/ckpt-100000\n",
      "2024-12-10 10:17:11.523000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "2024-12-10 10:17:11.580000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-10 10:17:12.733170: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-10 10:17:12.980000: I main.py:325] Accumulate gradients of 7 iterations to reach effective batch size of 25000\n",
      "2024-12-10 10:17:13.108000: I mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2024-12-10 10:17:13.614000: I dataset_ops.py:2542] Training on 318032 examples\n",
      "2024-12-10 10:18:23.156549: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-10 10:18:24.224665: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-10 10:18:24.579765: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-10 10:18:34.069000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-10 10:18:34.091000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-10 10:18:35.669000: I cross_device_ops.py:897] batch_all_reduce: 261 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-10 10:18:40.813000: I cross_device_ops.py:897] batch_all_reduce: 261 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-10 10:18:47.727000: I runner.py:310] Number of model parameters: 93357261\n",
      "2024-12-10 10:18:47.731000: I runner.py:310] Number of model weights: 261 (trainable = 261, non trainable = 0)\n",
      "2024-12-10 10:18:47.766000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-10 10:18:49.847000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-1\n",
      "2024-12-10 10:18:50.513000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-10 10:18:50.531000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-10 10:18:51.165000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-10 10:18:51.190000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-10 10:18:51.800000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-10 10:18:51.821000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-10 10:18:52.447000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-10 10:19:51.307000: I runner.py:310] Step = 100 ; steps/s = 1.62, tokens/s = 42781 (42781 target) ; Learning rate = 0.000009 ; Loss = 8.091535\n",
      "2024-12-10 10:20:53.183000: I runner.py:310] Step = 200 ; steps/s = 1.62, tokens/s = 42764 (42764 target) ; Learning rate = 0.000018 ; Loss = 6.840012\n",
      "2024-12-10 10:21:55.524000: I runner.py:310] Step = 300 ; steps/s = 1.60, tokens/s = 42415 (42415 target) ; Learning rate = 0.000027 ; Loss = 6.466323\n",
      "2024-12-10 10:22:57.775000: I runner.py:310] Step = 400 ; steps/s = 1.61, tokens/s = 41679 (41679 target) ; Learning rate = 0.000035 ; Loss = 6.188881\n",
      "2024-12-10 10:23:59.778000: I runner.py:310] Step = 500 ; steps/s = 1.61, tokens/s = 42666 (42666 target) ; Learning rate = 0.000044 ; Loss = 5.938087\n",
      "2024-12-10 10:25:01.749000: I runner.py:310] Step = 600 ; steps/s = 1.61, tokens/s = 42681 (42681 target) ; Learning rate = 0.000053 ; Loss = 5.754418\n",
      "2024-12-10 10:26:03.702000: I runner.py:310] Step = 700 ; steps/s = 1.61, tokens/s = 42681 (42681 target) ; Learning rate = 0.000062 ; Loss = 5.580061\n",
      "2024-12-10 10:27:05.600000: I runner.py:310] Step = 800 ; steps/s = 1.62, tokens/s = 41933 (41933 target) ; Learning rate = 0.000071 ; Loss = 5.379157\n",
      "2024-12-10 10:28:07.502000: I runner.py:310] Step = 900 ; steps/s = 1.62, tokens/s = 42758 (42758 target) ; Learning rate = 0.000080 ; Loss = 4.833931\n",
      "2024-12-10 10:29:09.429000: I runner.py:310] Step = 1000 ; steps/s = 1.61, tokens/s = 42687 (42687 target) ; Learning rate = 0.000088 ; Loss = 4.243041\n",
      "2024-12-10 10:30:11.310000: I runner.py:310] Step = 1100 ; steps/s = 1.62, tokens/s = 42747 (42747 target) ; Learning rate = 0.000097 ; Loss = 3.915650\n",
      "2024-12-10 10:31:12.742000: I runner.py:310] Step = 1200 ; steps/s = 1.63, tokens/s = 42229 (42229 target) ; Learning rate = 0.000106 ; Loss = 3.567145\n",
      "2024-12-10 10:32:14.677000: I runner.py:310] Step = 1300 ; steps/s = 1.61, tokens/s = 42683 (42683 target) ; Learning rate = 0.000115 ; Loss = 3.446448\n",
      "2024-12-10 10:33:16.639000: I runner.py:310] Step = 1400 ; steps/s = 1.61, tokens/s = 42673 (42673 target) ; Learning rate = 0.000124 ; Loss = 3.398290\n",
      "2024-12-10 10:34:18.532000: I runner.py:310] Step = 1500 ; steps/s = 1.62, tokens/s = 42749 (42749 target) ; Learning rate = 0.000133 ; Loss = 3.252717\n",
      "2024-12-10 10:35:19.978000: I runner.py:310] Step = 1600 ; steps/s = 1.63, tokens/s = 42246 (42246 target) ; Learning rate = 0.000142 ; Loss = 3.275866\n",
      "2024-12-10 10:36:21.775000: I runner.py:310] Step = 1700 ; steps/s = 1.62, tokens/s = 42805 (42805 target) ; Learning rate = 0.000150 ; Loss = 3.058840\n",
      "2024-12-10 10:37:23.687000: I runner.py:310] Step = 1800 ; steps/s = 1.62, tokens/s = 42737 (42737 target) ; Learning rate = 0.000159 ; Loss = 3.038033\n",
      "2024-12-10 10:38:25.547000: I runner.py:310] Step = 1900 ; steps/s = 1.62, tokens/s = 42721 (42721 target) ; Learning rate = 0.000168 ; Loss = 3.045016\n",
      "2024-12-10 10:39:27.003000: I runner.py:310] Step = 2000 ; steps/s = 1.63, tokens/s = 42235 (42235 target) ; Learning rate = 0.000177 ; Loss = 2.895077\n",
      "2024-12-10 10:40:28.920000: I runner.py:310] Step = 2100 ; steps/s = 1.62, tokens/s = 42706 (42706 target) ; Learning rate = 0.000186 ; Loss = 2.962418\n",
      "2024-12-10 10:41:30.855000: I runner.py:310] Step = 2200 ; steps/s = 1.61, tokens/s = 42710 (42710 target) ; Learning rate = 0.000195 ; Loss = 2.768742\n",
      "2024-12-10 10:42:32.729000: I runner.py:310] Step = 2300 ; steps/s = 1.62, tokens/s = 42763 (42763 target) ; Learning rate = 0.000203 ; Loss = 2.802961\n",
      "2024-12-10 10:43:34.121000: I runner.py:310] Step = 2400 ; steps/s = 1.63, tokens/s = 42244 (42244 target) ; Learning rate = 0.000212 ; Loss = 2.761445\n",
      "2024-12-10 10:44:35.972000: I runner.py:310] Step = 2500 ; steps/s = 1.62, tokens/s = 42766 (42766 target) ; Learning rate = 0.000221 ; Loss = 2.778391\n",
      "2024-12-10 10:45:37.861000: I runner.py:310] Step = 2600 ; steps/s = 1.62, tokens/s = 42727 (42727 target) ; Learning rate = 0.000230 ; Loss = 2.665946\n",
      "2024-12-10 10:46:39.857000: I runner.py:310] Step = 2700 ; steps/s = 1.61, tokens/s = 42656 (42656 target) ; Learning rate = 0.000239 ; Loss = 2.692631\n",
      "2024-12-10 10:47:41.323000: I runner.py:310] Step = 2800 ; steps/s = 1.63, tokens/s = 42256 (42256 target) ; Learning rate = 0.000248 ; Loss = 2.547941\n",
      "2024-12-10 10:48:43.221000: I runner.py:310] Step = 2900 ; steps/s = 1.62, tokens/s = 42709 (42709 target) ; Learning rate = 0.000256 ; Loss = 2.597625\n",
      "2024-12-10 10:49:45.107000: I runner.py:310] Step = 3000 ; steps/s = 1.62, tokens/s = 42755 (42755 target) ; Learning rate = 0.000265 ; Loss = 2.528239\n",
      "2024-12-10 10:50:46.915000: I runner.py:310] Step = 3100 ; steps/s = 1.62, tokens/s = 42765 (42765 target) ; Learning rate = 0.000274 ; Loss = 2.568070\n",
      "2024-12-10 10:51:48.339000: I runner.py:310] Step = 3200 ; steps/s = 1.63, tokens/s = 42260 (42260 target) ; Learning rate = 0.000283 ; Loss = 2.511960\n",
      "2024-12-10 10:52:50.152000: I runner.py:310] Step = 3300 ; steps/s = 1.62, tokens/s = 42767 (42767 target) ; Learning rate = 0.000292 ; Loss = 2.441733\n",
      "2024-12-10 10:53:52.069000: I runner.py:310] Step = 3400 ; steps/s = 1.62, tokens/s = 42681 (42681 target) ; Learning rate = 0.000301 ; Loss = 2.469548\n",
      "2024-12-10 10:54:53.933000: I runner.py:310] Step = 3500 ; steps/s = 1.62, tokens/s = 42770 (42770 target) ; Learning rate = 0.000309 ; Loss = 2.455598\n",
      "2024-12-10 10:55:55.371000: I runner.py:310] Step = 3600 ; steps/s = 1.63, tokens/s = 42293 (42293 target) ; Learning rate = 0.000318 ; Loss = 2.354879\n",
      "2024-12-10 10:56:57.248000: I runner.py:310] Step = 3700 ; steps/s = 1.62, tokens/s = 42740 (42740 target) ; Learning rate = 0.000327 ; Loss = 2.351405\n",
      "2024-12-10 10:57:59.129000: I runner.py:310] Step = 3800 ; steps/s = 1.62, tokens/s = 42711 (42711 target) ; Learning rate = 0.000336 ; Loss = 2.345497\n",
      "2024-12-10 10:59:00.970000: I runner.py:310] Step = 3900 ; steps/s = 1.62, tokens/s = 42778 (42778 target) ; Learning rate = 0.000345 ; Loss = 2.364817\n",
      "2024-12-10 11:00:02.422000: I runner.py:310] Step = 4000 ; steps/s = 1.63, tokens/s = 42235 (42235 target) ; Learning rate = 0.000354 ; Loss = 2.325146\n",
      "2024-12-10 11:01:04.361000: I runner.py:310] Step = 4100 ; steps/s = 1.61, tokens/s = 42719 (42719 target) ; Learning rate = 0.000362 ; Loss = 2.328578\n",
      "2024-12-10 11:02:06.249000: I runner.py:310] Step = 4200 ; steps/s = 1.62, tokens/s = 42743 (42743 target) ; Learning rate = 0.000371 ; Loss = 2.336969\n",
      "2024-12-10 11:03:08.056000: I runner.py:310] Step = 4300 ; steps/s = 1.62, tokens/s = 42764 (42764 target) ; Learning rate = 0.000380 ; Loss = 2.283621\n",
      "2024-12-10 11:04:09.470000: I runner.py:310] Step = 4400 ; steps/s = 1.63, tokens/s = 42227 (42227 target) ; Learning rate = 0.000389 ; Loss = 2.265604\n",
      "2024-12-10 11:05:11.304000: I runner.py:310] Step = 4500 ; steps/s = 1.62, tokens/s = 42770 (42770 target) ; Learning rate = 0.000398 ; Loss = 2.257290\n",
      "2024-12-10 11:06:13.140000: I runner.py:310] Step = 4600 ; steps/s = 1.62, tokens/s = 42752 (42752 target) ; Learning rate = 0.000407 ; Loss = 2.252929\n",
      "2024-12-10 11:07:15.014000: I runner.py:310] Step = 4700 ; steps/s = 1.62, tokens/s = 42773 (42773 target) ; Learning rate = 0.000416 ; Loss = 2.270301\n",
      "2024-12-10 11:08:16.464000: I runner.py:310] Step = 4800 ; steps/s = 1.63, tokens/s = 42260 (42260 target) ; Learning rate = 0.000424 ; Loss = 2.244697\n",
      "2024-12-10 11:09:18.366000: I runner.py:310] Step = 4900 ; steps/s = 1.62, tokens/s = 42736 (42736 target) ; Learning rate = 0.000433 ; Loss = 2.181907\n",
      "2024-12-10 11:10:20.202000: I runner.py:310] Step = 5000 ; steps/s = 1.62, tokens/s = 42726 (42726 target) ; Learning rate = 0.000442 ; Loss = 2.184985\n",
      "2024-12-10 11:10:20.203000: I training.py:192] Running evaluation for step 5000\n",
      "2024-12-10 11:17:49.243000: I training.py:192] Evaluation result for step 5000: loss = 1.047382 ; perplexity = 2.850181\n",
      "2024-12-10 11:18:51.096000: I runner.py:310] Step = 5100 ; steps/s = 1.62, tokens/s = 42781 (42781 target) ; Learning rate = 0.000451 ; Loss = 2.189170\n",
      "2024-12-10 11:19:52.646000: I runner.py:310] Step = 5200 ; steps/s = 1.62, tokens/s = 42183 (42183 target) ; Learning rate = 0.000460 ; Loss = 2.179809\n",
      "2024-12-10 11:20:54.719000: I runner.py:310] Step = 5300 ; steps/s = 1.61, tokens/s = 42620 (42620 target) ; Learning rate = 0.000469 ; Loss = 2.145438\n",
      "2024-12-10 11:21:56.710000: I runner.py:310] Step = 5400 ; steps/s = 1.61, tokens/s = 42667 (42667 target) ; Learning rate = 0.000477 ; Loss = 2.104617\n",
      "2024-12-10 11:22:58.749000: I runner.py:310] Step = 5500 ; steps/s = 1.61, tokens/s = 42610 (42610 target) ; Learning rate = 0.000486 ; Loss = 2.131553\n",
      "2024-12-10 11:24:00.246000: I runner.py:310] Step = 5600 ; steps/s = 1.63, tokens/s = 42212 (42212 target) ; Learning rate = 0.000495 ; Loss = 2.076134\n",
      "2024-12-10 11:25:02.171000: I runner.py:310] Step = 5700 ; steps/s = 1.62, tokens/s = 42703 (42703 target) ; Learning rate = 0.000504 ; Loss = 2.133040\n",
      "2024-12-10 11:26:04.224000: I runner.py:310] Step = 5800 ; steps/s = 1.61, tokens/s = 42594 (42594 target) ; Learning rate = 0.000513 ; Loss = 2.087304\n",
      "2024-12-10 11:27:06.258000: I runner.py:310] Step = 5900 ; steps/s = 1.61, tokens/s = 42649 (42649 target) ; Learning rate = 0.000522 ; Loss = 2.178824\n",
      "2024-12-10 11:28:07.789000: I runner.py:310] Step = 6000 ; steps/s = 1.63, tokens/s = 42190 (42190 target) ; Learning rate = 0.000530 ; Loss = 2.082240\n",
      "2024-12-10 11:29:09.771000: I runner.py:310] Step = 6100 ; steps/s = 1.61, tokens/s = 42671 (42671 target) ; Learning rate = 0.000539 ; Loss = 2.105395\n",
      "2024-12-10 11:30:11.735000: I runner.py:310] Step = 6200 ; steps/s = 1.61, tokens/s = 42666 (42666 target) ; Learning rate = 0.000548 ; Loss = 2.112615\n",
      "2024-12-10 11:31:13.655000: I runner.py:310] Step = 6300 ; steps/s = 1.62, tokens/s = 42704 (42704 target) ; Learning rate = 0.000557 ; Loss = 2.059544\n",
      "2024-12-10 11:32:15.214000: I runner.py:310] Step = 6400 ; steps/s = 1.62, tokens/s = 42151 (42151 target) ; Learning rate = 0.000566 ; Loss = 2.062062\n",
      "2024-12-10 11:33:17.202000: I runner.py:310] Step = 6500 ; steps/s = 1.61, tokens/s = 42648 (42648 target) ; Learning rate = 0.000575 ; Loss = 2.050178\n",
      "2024-12-10 11:34:19.152000: I runner.py:310] Step = 6600 ; steps/s = 1.61, tokens/s = 42677 (42677 target) ; Learning rate = 0.000583 ; Loss = 2.036771\n",
      "2024-12-10 11:35:21.119000: I runner.py:310] Step = 6700 ; steps/s = 1.61, tokens/s = 42713 (42713 target) ; Learning rate = 0.000592 ; Loss = 2.046525\n",
      "2024-12-10 11:36:22.668000: I runner.py:310] Step = 6800 ; steps/s = 1.62, tokens/s = 42165 (42165 target) ; Learning rate = 0.000601 ; Loss = 2.018592\n",
      "2024-12-10 11:37:24.693000: I runner.py:310] Step = 6900 ; steps/s = 1.61, tokens/s = 42670 (42670 target) ; Learning rate = 0.000610 ; Loss = 2.038767\n",
      "2024-12-10 11:38:26.675000: I runner.py:310] Step = 7000 ; steps/s = 1.61, tokens/s = 42678 (42678 target) ; Learning rate = 0.000619 ; Loss = 2.034977\n",
      "2024-12-10 11:39:28.639000: I runner.py:310] Step = 7100 ; steps/s = 1.61, tokens/s = 42642 (42642 target) ; Learning rate = 0.000628 ; Loss = 2.070113\n",
      "2024-12-10 11:40:30.102000: I runner.py:310] Step = 7200 ; steps/s = 1.63, tokens/s = 42249 (42249 target) ; Learning rate = 0.000636 ; Loss = 1.939505\n",
      "2024-12-10 11:41:31.979000: I runner.py:310] Step = 7300 ; steps/s = 1.62, tokens/s = 42767 (42767 target) ; Learning rate = 0.000645 ; Loss = 1.970141\n",
      "2024-12-10 11:42:33.922000: I runner.py:310] Step = 7400 ; steps/s = 1.61, tokens/s = 42656 (42656 target) ; Learning rate = 0.000654 ; Loss = 1.990133\n",
      "2024-12-10 11:43:35.982000: I runner.py:310] Step = 7500 ; steps/s = 1.61, tokens/s = 42623 (42623 target) ; Learning rate = 0.000663 ; Loss = 1.989735\n",
      "2024-12-10 11:44:37.482000: I runner.py:310] Step = 7600 ; steps/s = 1.63, tokens/s = 42210 (42210 target) ; Learning rate = 0.000672 ; Loss = 1.927820\n",
      "2024-12-10 11:45:39.378000: I runner.py:310] Step = 7700 ; steps/s = 1.62, tokens/s = 42707 (42707 target) ; Learning rate = 0.000681 ; Loss = 1.971270\n",
      "2024-12-10 11:46:41.255000: I runner.py:310] Step = 7800 ; steps/s = 1.62, tokens/s = 42718 (42718 target) ; Learning rate = 0.000690 ; Loss = 1.939106\n",
      "2024-12-10 11:47:43.193000: I runner.py:310] Step = 7900 ; steps/s = 1.61, tokens/s = 42725 (42725 target) ; Learning rate = 0.000698 ; Loss = 2.007711\n",
      "2024-12-10 11:48:44.640000: I runner.py:310] Step = 8000 ; steps/s = 1.63, tokens/s = 42245 (42245 target) ; Learning rate = 0.000707 ; Loss = 1.874270\n",
      "2024-12-10 11:49:46.572000: I runner.py:310] Step = 8100 ; steps/s = 1.61, tokens/s = 42695 (42695 target) ; Learning rate = 0.000716 ; Loss = 1.925982\n",
      "2024-12-10 11:50:48.477000: I runner.py:310] Step = 8200 ; steps/s = 1.62, tokens/s = 42724 (42724 target) ; Learning rate = 0.000725 ; Loss = 1.951070\n",
      "2024-12-10 11:51:50.408000: I runner.py:310] Step = 8300 ; steps/s = 1.61, tokens/s = 42709 (42709 target) ; Learning rate = 0.000734 ; Loss = 1.969002\n",
      "2024-12-10 11:52:51.897000: I runner.py:310] Step = 8400 ; steps/s = 1.63, tokens/s = 42244 (42244 target) ; Learning rate = 0.000743 ; Loss = 1.933835\n",
      "2024-12-10 11:53:53.874000: I runner.py:310] Step = 8500 ; steps/s = 1.61, tokens/s = 42675 (42675 target) ; Learning rate = 0.000751 ; Loss = 1.898110\n",
      "2024-12-10 11:54:55.779000: I runner.py:310] Step = 8600 ; steps/s = 1.62, tokens/s = 42701 (42701 target) ; Learning rate = 0.000760 ; Loss = 1.922582\n",
      "2024-12-10 11:55:57.693000: I runner.py:310] Step = 8700 ; steps/s = 1.62, tokens/s = 42697 (42697 target) ; Learning rate = 0.000769 ; Loss = 1.935695\n",
      "2024-12-10 11:56:59.214000: I runner.py:310] Step = 8800 ; steps/s = 1.63, tokens/s = 42169 (42169 target) ; Learning rate = 0.000778 ; Loss = 1.907103\n",
      "2024-12-10 11:58:01.135000: I runner.py:310] Step = 8900 ; steps/s = 1.62, tokens/s = 42716 (42716 target) ; Learning rate = 0.000787 ; Loss = 1.890027\n",
      "2024-12-10 11:59:03.081000: I runner.py:310] Step = 9000 ; steps/s = 1.61, tokens/s = 42709 (42709 target) ; Learning rate = 0.000796 ; Loss = 1.894534\n",
      "2024-12-10 12:00:04.994000: I runner.py:310] Step = 9100 ; steps/s = 1.62, tokens/s = 42714 (42714 target) ; Learning rate = 0.000804 ; Loss = 1.929257\n",
      "2024-12-10 12:01:06.523000: I runner.py:310] Step = 9200 ; steps/s = 1.63, tokens/s = 42195 (42195 target) ; Learning rate = 0.000813 ; Loss = 1.854328\n",
      "2024-12-10 12:02:08.454000: I runner.py:310] Step = 9300 ; steps/s = 1.61, tokens/s = 42717 (42717 target) ; Learning rate = 0.000822 ; Loss = 1.897883\n",
      "2024-12-10 12:03:10.392000: I runner.py:310] Step = 9400 ; steps/s = 1.61, tokens/s = 42677 (42677 target) ; Learning rate = 0.000831 ; Loss = 1.900798\n",
      "2024-12-10 12:04:12.337000: I runner.py:310] Step = 9500 ; steps/s = 1.61, tokens/s = 42677 (42677 target) ; Learning rate = 0.000840 ; Loss = 1.903215\n",
      "2024-12-10 12:05:13.792000: I runner.py:310] Step = 9600 ; steps/s = 1.63, tokens/s = 42239 (42239 target) ; Learning rate = 0.000849 ; Loss = 1.837411\n",
      "2024-12-10 12:06:15.743000: I runner.py:310] Step = 9700 ; steps/s = 1.61, tokens/s = 42691 (42691 target) ; Learning rate = 0.000857 ; Loss = 1.877498\n",
      "2024-12-10 12:07:17.699000: I runner.py:310] Step = 9800 ; steps/s = 1.61, tokens/s = 42694 (42694 target) ; Learning rate = 0.000866 ; Loss = 1.879259\n",
      "2024-12-10 12:08:19.647000: I runner.py:310] Step = 9900 ; steps/s = 1.61, tokens/s = 42675 (42675 target) ; Learning rate = 0.000875 ; Loss = 1.883447\n",
      "2024-12-10 12:09:21.177000: I runner.py:310] Step = 10000 ; steps/s = 1.63, tokens/s = 42181 (42181 target) ; Learning rate = 0.000884 ; Loss = 1.871356\n",
      "2024-12-10 12:09:23.114000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-10000\n",
      "2024-12-10 12:09:23.114000: I training.py:192] Running evaluation for step 10000\n",
      "2024-12-10 12:15:17.332000: I training.py:192] Evaluation result for step 10000: loss = 0.974352 ; perplexity = 2.649451\n",
      "2024-12-10 12:16:19.089000: I runner.py:310] Step = 10100 ; steps/s = 1.62, tokens/s = 42835 (42835 target) ; Learning rate = 0.000879 ; Loss = 1.842920\n",
      "2024-12-10 12:17:21.041000: I runner.py:310] Step = 10200 ; steps/s = 1.61, tokens/s = 42708 (42708 target) ; Learning rate = 0.000875 ; Loss = 1.891711\n",
      "2024-12-10 12:18:22.767000: I runner.py:310] Step = 10300 ; steps/s = 1.62, tokens/s = 42301 (42301 target) ; Learning rate = 0.000871 ; Loss = 1.880193\n",
      "2024-12-10 12:19:24.562000: I runner.py:310] Step = 10400 ; steps/s = 1.62, tokens/s = 42513 (42513 target) ; Learning rate = 0.000867 ; Loss = 1.858197\n",
      "2024-12-10 12:20:26.506000: I runner.py:310] Step = 10500 ; steps/s = 1.61, tokens/s = 42703 (42703 target) ; Learning rate = 0.000863 ; Loss = 1.831348\n",
      "2024-12-10 12:21:28.538000: I runner.py:310] Step = 10600 ; steps/s = 1.61, tokens/s = 42625 (42625 target) ; Learning rate = 0.000858 ; Loss = 1.827691\n",
      "2024-12-10 12:22:30.078000: I runner.py:310] Step = 10700 ; steps/s = 1.63, tokens/s = 42176 (42176 target) ; Learning rate = 0.000854 ; Loss = 1.819934\n",
      "2024-12-10 12:23:32.013000: I runner.py:310] Step = 10800 ; steps/s = 1.61, tokens/s = 42740 (42740 target) ; Learning rate = 0.000850 ; Loss = 1.825499\n",
      "2024-12-10 12:24:33.954000: I runner.py:310] Step = 10900 ; steps/s = 1.61, tokens/s = 42669 (42669 target) ; Learning rate = 0.000847 ; Loss = 1.814727\n",
      "2024-12-10 12:25:35.960000: I runner.py:310] Step = 11000 ; steps/s = 1.61, tokens/s = 42659 (42659 target) ; Learning rate = 0.000843 ; Loss = 1.829207\n",
      "2024-12-10 12:26:37.465000: I runner.py:310] Step = 11100 ; steps/s = 1.63, tokens/s = 42194 (42194 target) ; Learning rate = 0.000839 ; Loss = 1.792594\n",
      "2024-12-10 12:27:39.381000: I runner.py:310] Step = 11200 ; steps/s = 1.62, tokens/s = 42720 (42720 target) ; Learning rate = 0.000835 ; Loss = 1.775281\n",
      "2024-12-10 12:28:41.290000: I runner.py:310] Step = 11300 ; steps/s = 1.62, tokens/s = 42726 (42726 target) ; Learning rate = 0.000831 ; Loss = 1.805657\n",
      "2024-12-10 12:29:43.243000: I runner.py:310] Step = 11400 ; steps/s = 1.61, tokens/s = 42669 (42669 target) ; Learning rate = 0.000828 ; Loss = 1.837927\n",
      "2024-12-10 12:30:44.731000: I runner.py:310] Step = 11500 ; steps/s = 1.63, tokens/s = 42216 (42216 target) ; Learning rate = 0.000824 ; Loss = 1.799169\n",
      "2024-12-10 12:31:46.683000: I runner.py:310] Step = 11600 ; steps/s = 1.61, tokens/s = 42726 (42726 target) ; Learning rate = 0.000821 ; Loss = 1.783260\n",
      "2024-12-10 12:32:48.719000: I runner.py:310] Step = 11700 ; steps/s = 1.61, tokens/s = 42622 (42622 target) ; Learning rate = 0.000817 ; Loss = 1.802898\n",
      "2024-12-10 12:33:50.684000: I runner.py:310] Step = 11800 ; steps/s = 1.61, tokens/s = 42671 (42671 target) ; Learning rate = 0.000814 ; Loss = 1.818253\n",
      "2024-12-10 12:34:52.237000: I runner.py:310] Step = 11900 ; steps/s = 1.62, tokens/s = 42137 (42137 target) ; Learning rate = 0.000810 ; Loss = 1.785083\n",
      "2024-12-10 12:35:54.161000: I runner.py:310] Step = 12000 ; steps/s = 1.62, tokens/s = 42744 (42744 target) ; Learning rate = 0.000807 ; Loss = 1.763895\n",
      "2024-12-10 12:36:56.149000: I runner.py:310] Step = 12100 ; steps/s = 1.61, tokens/s = 42671 (42671 target) ; Learning rate = 0.000803 ; Loss = 1.798855\n",
      "2024-12-10 12:37:58.150000: I runner.py:310] Step = 12200 ; steps/s = 1.61, tokens/s = 42628 (42628 target) ; Learning rate = 0.000800 ; Loss = 1.814223\n",
      "2024-12-10 12:38:59.686000: I runner.py:310] Step = 12300 ; steps/s = 1.63, tokens/s = 42170 (42170 target) ; Learning rate = 0.000797 ; Loss = 1.777277\n",
      "2024-12-10 12:40:01.722000: I runner.py:310] Step = 12400 ; steps/s = 1.61, tokens/s = 42651 (42651 target) ; Learning rate = 0.000794 ; Loss = 1.765411\n",
      "2024-12-10 12:41:03.740000: I runner.py:310] Step = 12500 ; steps/s = 1.61, tokens/s = 42619 (42619 target) ; Learning rate = 0.000791 ; Loss = 1.761844\n",
      "2024-12-10 12:42:05.684000: I runner.py:310] Step = 12600 ; steps/s = 1.61, tokens/s = 42709 (42709 target) ; Learning rate = 0.000787 ; Loss = 1.767853\n",
      "2024-12-10 12:43:07.160000: I runner.py:310] Step = 12700 ; steps/s = 1.63, tokens/s = 42203 (42203 target) ; Learning rate = 0.000784 ; Loss = 1.777908\n",
      "2024-12-10 12:44:09.074000: I runner.py:310] Step = 12800 ; steps/s = 1.62, tokens/s = 42717 (42717 target) ; Learning rate = 0.000781 ; Loss = 1.749802\n",
      "2024-12-10 12:45:11.060000: I runner.py:310] Step = 12900 ; steps/s = 1.61, tokens/s = 42676 (42676 target) ; Learning rate = 0.000778 ; Loss = 1.766503\n",
      "2024-12-10 12:46:12.999000: I runner.py:310] Step = 13000 ; steps/s = 1.61, tokens/s = 42709 (42709 target) ; Learning rate = 0.000775 ; Loss = 1.773584\n",
      "2024-12-10 12:47:14.487000: I runner.py:310] Step = 13100 ; steps/s = 1.63, tokens/s = 42198 (42198 target) ; Learning rate = 0.000772 ; Loss = 1.754373\n",
      "2024-12-10 12:48:16.406000: I runner.py:310] Step = 13200 ; steps/s = 1.62, tokens/s = 42711 (42711 target) ; Learning rate = 0.000769 ; Loss = 1.740196\n",
      "2024-12-10 12:49:18.298000: I runner.py:310] Step = 13300 ; steps/s = 1.62, tokens/s = 42707 (42707 target) ; Learning rate = 0.000766 ; Loss = 1.752181\n",
      "2024-12-10 12:50:20.273000: I runner.py:310] Step = 13400 ; steps/s = 1.61, tokens/s = 42684 (42684 target) ; Learning rate = 0.000764 ; Loss = 1.749274\n",
      "2024-12-10 12:51:21.724000: I runner.py:310] Step = 13500 ; steps/s = 1.63, tokens/s = 42235 (42235 target) ; Learning rate = 0.000761 ; Loss = 1.751468\n",
      "2024-12-10 12:52:23.704000: I runner.py:310] Step = 13600 ; steps/s = 1.61, tokens/s = 42649 (42649 target) ; Learning rate = 0.000758 ; Loss = 1.731143\n",
      "2024-12-10 12:53:25.720000: I runner.py:310] Step = 13700 ; steps/s = 1.61, tokens/s = 42658 (42658 target) ; Learning rate = 0.000755 ; Loss = 1.761274\n",
      "2024-12-10 12:54:27.694000: I runner.py:310] Step = 13800 ; steps/s = 1.61, tokens/s = 42696 (42696 target) ; Learning rate = 0.000752 ; Loss = 1.770425\n",
      "2024-12-10 12:55:29.236000: I runner.py:310] Step = 13900 ; steps/s = 1.63, tokens/s = 42168 (42168 target) ; Learning rate = 0.000750 ; Loss = 1.740825\n",
      "2024-12-10 12:56:31.134000: I runner.py:310] Step = 14000 ; steps/s = 1.62, tokens/s = 42730 (42730 target) ; Learning rate = 0.000747 ; Loss = 1.706386\n",
      "2024-12-10 12:57:33.146000: I runner.py:310] Step = 14100 ; steps/s = 1.61, tokens/s = 42639 (42639 target) ; Learning rate = 0.000744 ; Loss = 1.743248\n",
      "2024-12-10 12:58:35.079000: I runner.py:310] Step = 14200 ; steps/s = 1.61, tokens/s = 42733 (42733 target) ; Learning rate = 0.000742 ; Loss = 1.753978\n",
      "2024-12-10 12:59:36.581000: I runner.py:310] Step = 14300 ; steps/s = 1.63, tokens/s = 42191 (42191 target) ; Learning rate = 0.000739 ; Loss = 1.712067\n",
      "2024-12-10 13:00:38.495000: I runner.py:310] Step = 14400 ; steps/s = 1.62, tokens/s = 42721 (42721 target) ; Learning rate = 0.000737 ; Loss = 1.724714\n",
      "2024-12-10 13:01:40.452000: I runner.py:310] Step = 14500 ; steps/s = 1.61, tokens/s = 42689 (42689 target) ; Learning rate = 0.000734 ; Loss = 1.748639\n",
      "2024-12-10 13:02:42.393000: I runner.py:310] Step = 14600 ; steps/s = 1.61, tokens/s = 42701 (42701 target) ; Learning rate = 0.000731 ; Loss = 1.755271\n",
      "2024-12-10 13:03:43.988000: I runner.py:310] Step = 14700 ; steps/s = 1.62, tokens/s = 42108 (42108 target) ; Learning rate = 0.000729 ; Loss = 1.729802\n",
      "2024-12-10 13:04:45.964000: I runner.py:310] Step = 14800 ; steps/s = 1.61, tokens/s = 42678 (42678 target) ; Learning rate = 0.000727 ; Loss = 1.719076\n",
      "2024-12-10 13:05:47.919000: I runner.py:310] Step = 14900 ; steps/s = 1.61, tokens/s = 42702 (42702 target) ; Learning rate = 0.000724 ; Loss = 1.715032\n",
      "2024-12-10 13:06:49.902000: I runner.py:310] Step = 15000 ; steps/s = 1.61, tokens/s = 42663 (42663 target) ; Learning rate = 0.000722 ; Loss = 1.716211\n",
      "2024-12-10 13:06:49.903000: I training.py:192] Running evaluation for step 15000\n",
      "2024-12-10 13:12:10.008000: I training.py:192] Evaluation result for step 15000: loss = 0.992193 ; perplexity = 2.697144\n",
      "2024-12-10 13:13:11.347000: I runner.py:310] Step = 15100 ; steps/s = 1.63, tokens/s = 42338 (42338 target) ; Learning rate = 0.000719 ; Loss = 1.698470\n",
      "2024-12-10 13:14:13.216000: I runner.py:310] Step = 15200 ; steps/s = 1.62, tokens/s = 42717 (42717 target) ; Learning rate = 0.000717 ; Loss = 1.707372\n",
      "2024-12-10 13:15:15.152000: I runner.py:310] Step = 15300 ; steps/s = 1.61, tokens/s = 42720 (42720 target) ; Learning rate = 0.000715 ; Loss = 1.733203\n",
      "2024-12-10 13:16:17.125000: I runner.py:310] Step = 15400 ; steps/s = 1.61, tokens/s = 42680 (42680 target) ; Learning rate = 0.000712 ; Loss = 1.729868\n",
      "2024-12-10 13:17:18.703000: I runner.py:310] Step = 15500 ; steps/s = 1.62, tokens/s = 42123 (42123 target) ; Learning rate = 0.000710 ; Loss = 1.720605\n",
      "2024-12-10 13:18:20.628000: I runner.py:310] Step = 15600 ; steps/s = 1.62, tokens/s = 42725 (42725 target) ; Learning rate = 0.000708 ; Loss = 1.707815\n",
      "2024-12-10 13:19:22.660000: I runner.py:310] Step = 15700 ; steps/s = 1.61, tokens/s = 42627 (42627 target) ; Learning rate = 0.000705 ; Loss = 1.702790\n",
      "2024-12-10 13:20:24.709000: I runner.py:310] Step = 15800 ; steps/s = 1.61, tokens/s = 42619 (42619 target) ; Learning rate = 0.000703 ; Loss = 1.704257\n",
      "2024-12-10 13:21:26.278000: I runner.py:310] Step = 15900 ; steps/s = 1.62, tokens/s = 42159 (42159 target) ; Learning rate = 0.000701 ; Loss = 1.679324\n",
      "2024-12-10 13:22:28.239000: I runner.py:310] Step = 16000 ; steps/s = 1.61, tokens/s = 42709 (42709 target) ; Learning rate = 0.000699 ; Loss = 1.685964\n",
      "2024-12-10 13:23:30.251000: I runner.py:310] Step = 16100 ; steps/s = 1.61, tokens/s = 42611 (42611 target) ; Learning rate = 0.000697 ; Loss = 1.713249\n",
      "2024-12-10 13:24:32.176000: I runner.py:310] Step = 16200 ; steps/s = 1.61, tokens/s = 42711 (42711 target) ; Learning rate = 0.000694 ; Loss = 1.707139\n",
      "2024-12-10 13:25:33.633000: I runner.py:310] Step = 16300 ; steps/s = 1.63, tokens/s = 42244 (42244 target) ; Learning rate = 0.000692 ; Loss = 1.709836\n",
      "2024-12-10 13:26:35.585000: I runner.py:310] Step = 16400 ; steps/s = 1.61, tokens/s = 42717 (42717 target) ; Learning rate = 0.000690 ; Loss = 1.690645\n",
      "2024-12-10 13:27:37.567000: I runner.py:310] Step = 16500 ; steps/s = 1.61, tokens/s = 42653 (42653 target) ; Learning rate = 0.000688 ; Loss = 1.701506\n",
      "2024-12-10 13:28:39.540000: I runner.py:310] Step = 16600 ; steps/s = 1.61, tokens/s = 42667 (42667 target) ; Learning rate = 0.000686 ; Loss = 1.703538\n",
      "2024-12-10 13:29:41.065000: I runner.py:310] Step = 16700 ; steps/s = 1.63, tokens/s = 42171 (42171 target) ; Learning rate = 0.000684 ; Loss = 1.693429\n",
      "2024-12-10 13:30:43.008000: I runner.py:310] Step = 16800 ; steps/s = 1.61, tokens/s = 42692 (42692 target) ; Learning rate = 0.000682 ; Loss = 1.674077\n",
      "2024-12-10 13:31:44.945000: I runner.py:310] Step = 16900 ; steps/s = 1.61, tokens/s = 42703 (42703 target) ; Learning rate = 0.000680 ; Loss = 1.681200\n",
      "2024-12-10 13:32:46.944000: I runner.py:310] Step = 17000 ; steps/s = 1.61, tokens/s = 42642 (42642 target) ; Learning rate = 0.000678 ; Loss = 1.697720\n",
      "2024-12-10 13:33:48.400000: I runner.py:310] Step = 17100 ; steps/s = 1.63, tokens/s = 42260 (42260 target) ; Learning rate = 0.000676 ; Loss = 1.694615\n",
      "2024-12-10 13:34:50.402000: I runner.py:310] Step = 17200 ; steps/s = 1.61, tokens/s = 42649 (42649 target) ; Learning rate = 0.000674 ; Loss = 1.684549\n",
      "2024-12-10 13:35:52.387000: I runner.py:310] Step = 17300 ; steps/s = 1.61, tokens/s = 42668 (42668 target) ; Learning rate = 0.000672 ; Loss = 1.677458\n",
      "2024-12-10 13:36:54.366000: I runner.py:310] Step = 17400 ; steps/s = 1.61, tokens/s = 42669 (42669 target) ; Learning rate = 0.000670 ; Loss = 1.686850\n",
      "2024-12-10 13:37:55.845000: I runner.py:310] Step = 17500 ; steps/s = 1.63, tokens/s = 42191 (42191 target) ; Learning rate = 0.000668 ; Loss = 1.671529\n",
      "2024-12-10 13:38:57.839000: I runner.py:310] Step = 17600 ; steps/s = 1.61, tokens/s = 42660 (42660 target) ; Learning rate = 0.000666 ; Loss = 1.668808\n",
      "2024-12-10 13:39:59.806000: I runner.py:310] Step = 17700 ; steps/s = 1.61, tokens/s = 42692 (42692 target) ; Learning rate = 0.000664 ; Loss = 1.682140\n",
      "2024-12-10 13:41:01.806000: I runner.py:310] Step = 17800 ; steps/s = 1.61, tokens/s = 42671 (42671 target) ; Learning rate = 0.000662 ; Loss = 1.698671\n",
      "2024-12-10 13:42:03.381000: I runner.py:310] Step = 17900 ; steps/s = 1.62, tokens/s = 42172 (42172 target) ; Learning rate = 0.000661 ; Loss = 1.655647\n",
      "2024-12-10 13:43:05.308000: I runner.py:310] Step = 18000 ; steps/s = 1.61, tokens/s = 42685 (42685 target) ; Learning rate = 0.000659 ; Loss = 1.664918\n",
      "2024-12-10 13:44:07.235000: I runner.py:310] Step = 18100 ; steps/s = 1.61, tokens/s = 42704 (42704 target) ; Learning rate = 0.000657 ; Loss = 1.685420\n",
      "2024-12-10 13:45:09.212000: I runner.py:310] Step = 18200 ; steps/s = 1.61, tokens/s = 42682 (42682 target) ; Learning rate = 0.000655 ; Loss = 1.692062\n",
      "2024-12-10 13:46:10.650000: I runner.py:310] Step = 18300 ; steps/s = 1.63, tokens/s = 42207 (42207 target) ; Learning rate = 0.000653 ; Loss = 1.645210\n",
      "2024-12-10 13:47:12.575000: I runner.py:310] Step = 18400 ; steps/s = 1.62, tokens/s = 42704 (42704 target) ; Learning rate = 0.000652 ; Loss = 1.657240\n",
      "2024-12-10 13:48:14.542000: I runner.py:310] Step = 18500 ; steps/s = 1.61, tokens/s = 42698 (42698 target) ; Learning rate = 0.000650 ; Loss = 1.674653\n",
      "2024-12-10 13:49:16.476000: I runner.py:310] Step = 18600 ; steps/s = 1.61, tokens/s = 42690 (42690 target) ; Learning rate = 0.000648 ; Loss = 1.696751\n",
      "2024-12-10 13:50:17.953000: I runner.py:310] Step = 18700 ; steps/s = 1.63, tokens/s = 42232 (42232 target) ; Learning rate = 0.000646 ; Loss = 1.646265\n",
      "2024-12-10 13:51:19.914000: I runner.py:310] Step = 18800 ; steps/s = 1.61, tokens/s = 42681 (42681 target) ; Learning rate = 0.000645 ; Loss = 1.677304\n",
      "2024-12-10 13:52:21.880000: I runner.py:310] Step = 18900 ; steps/s = 1.61, tokens/s = 42695 (42695 target) ; Learning rate = 0.000643 ; Loss = 1.667213\n",
      "2024-12-10 13:53:23.922000: I runner.py:310] Step = 19000 ; steps/s = 1.61, tokens/s = 42614 (42614 target) ; Learning rate = 0.000641 ; Loss = 1.673631\n",
      "2024-12-10 13:54:25.437000: I runner.py:310] Step = 19100 ; steps/s = 1.63, tokens/s = 42217 (42217 target) ; Learning rate = 0.000640 ; Loss = 1.679761\n",
      "2024-12-10 13:55:27.438000: I runner.py:310] Step = 19200 ; steps/s = 1.61, tokens/s = 42643 (42643 target) ; Learning rate = 0.000638 ; Loss = 1.652639\n",
      "2024-12-10 13:56:29.448000: I runner.py:310] Step = 19300 ; steps/s = 1.61, tokens/s = 42640 (42640 target) ; Learning rate = 0.000636 ; Loss = 1.660677\n",
      "2024-12-10 13:57:31.443000: I runner.py:310] Step = 19400 ; steps/s = 1.61, tokens/s = 42669 (42669 target) ; Learning rate = 0.000635 ; Loss = 1.654911\n",
      "2024-12-10 13:58:32.967000: I runner.py:310] Step = 19500 ; steps/s = 1.63, tokens/s = 42168 (42168 target) ; Learning rate = 0.000633 ; Loss = 1.668971\n",
      "2024-12-10 13:59:34.945000: I runner.py:310] Step = 19600 ; steps/s = 1.61, tokens/s = 42665 (42665 target) ; Learning rate = 0.000631 ; Loss = 1.648455\n",
      "2024-12-10 14:00:36.909000: I runner.py:310] Step = 19700 ; steps/s = 1.61, tokens/s = 42682 (42682 target) ; Learning rate = 0.000630 ; Loss = 1.649421\n",
      "2024-12-10 14:01:38.936000: I runner.py:310] Step = 19800 ; steps/s = 1.61, tokens/s = 42644 (42644 target) ; Learning rate = 0.000628 ; Loss = 1.656333\n",
      "2024-12-10 14:02:40.389000: I runner.py:310] Step = 19900 ; steps/s = 1.63, tokens/s = 42248 (42248 target) ; Learning rate = 0.000627 ; Loss = 1.653732\n",
      "2024-12-10 14:03:42.311000: I runner.py:310] Step = 20000 ; steps/s = 1.62, tokens/s = 42699 (42699 target) ; Learning rate = 0.000625 ; Loss = 1.652915\n",
      "2024-12-10 14:03:44.166000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-20000\n",
      "2024-12-10 14:03:44.166000: I training.py:192] Running evaluation for step 20000\n",
      "2024-12-10 14:08:57.378000: I training.py:192] Evaluation result for step 20000: loss = 1.019232 ; perplexity = 2.771065\n",
      "2024-12-10 14:09:59.178000: I runner.py:310] Step = 20100 ; steps/s = 1.62, tokens/s = 42812 (42812 target) ; Learning rate = 0.000623 ; Loss = 1.642039\n",
      "2024-12-10 14:11:01.137000: I runner.py:310] Step = 20200 ; steps/s = 1.61, tokens/s = 42679 (42679 target) ; Learning rate = 0.000622 ; Loss = 1.658532\n",
      "2024-12-10 14:12:02.690000: I runner.py:310] Step = 20300 ; steps/s = 1.62, tokens/s = 42188 (42188 target) ; Learning rate = 0.000620 ; Loss = 1.662366\n",
      "2024-12-10 14:13:04.677000: I runner.py:310] Step = 20400 ; steps/s = 1.61, tokens/s = 42643 (42643 target) ; Learning rate = 0.000619 ; Loss = 1.639738\n",
      "2024-12-10 14:14:06.682000: I runner.py:310] Step = 20500 ; steps/s = 1.61, tokens/s = 42638 (42638 target) ; Learning rate = 0.000617 ; Loss = 1.645779\n",
      "2024-12-10 14:15:08.569000: I runner.py:310] Step = 20600 ; steps/s = 1.62, tokens/s = 42628 (42628 target) ; Learning rate = 0.000616 ; Loss = 1.659366\n",
      "2024-12-10 14:16:10.145000: I runner.py:310] Step = 20700 ; steps/s = 1.62, tokens/s = 42261 (42261 target) ; Learning rate = 0.000614 ; Loss = 1.659344\n",
      "2024-12-10 14:17:12.147000: I runner.py:310] Step = 20800 ; steps/s = 1.61, tokens/s = 42679 (42679 target) ; Learning rate = 0.000613 ; Loss = 1.634260\n",
      "2024-12-10 14:18:14.079000: I runner.py:310] Step = 20900 ; steps/s = 1.61, tokens/s = 42700 (42700 target) ; Learning rate = 0.000611 ; Loss = 1.651462\n",
      "2024-12-10 14:19:15.617000: I runner.py:310] Step = 21000 ; steps/s = 1.63, tokens/s = 42142 (42142 target) ; Learning rate = 0.000610 ; Loss = 1.638584\n",
      "2024-12-10 14:20:17.544000: I runner.py:310] Step = 21100 ; steps/s = 1.61, tokens/s = 42754 (42754 target) ; Learning rate = 0.000608 ; Loss = 1.640116\n",
      "2024-12-10 14:21:19.515000: I runner.py:310] Step = 21200 ; steps/s = 1.61, tokens/s = 42658 (42658 target) ; Learning rate = 0.000607 ; Loss = 1.642152\n",
      "2024-12-10 14:22:21.467000: I runner.py:310] Step = 21300 ; steps/s = 1.61, tokens/s = 42671 (42671 target) ; Learning rate = 0.000606 ; Loss = 1.642065\n",
      "2024-12-10 14:23:23.014000: I runner.py:310] Step = 21400 ; steps/s = 1.62, tokens/s = 42167 (42167 target) ; Learning rate = 0.000604 ; Loss = 1.643580\n",
      "2024-12-10 14:24:25.016000: I runner.py:310] Step = 21500 ; steps/s = 1.61, tokens/s = 42690 (42690 target) ; Learning rate = 0.000603 ; Loss = 1.631610\n",
      "2024-12-10 14:25:26.916000: I runner.py:310] Step = 21600 ; steps/s = 1.62, tokens/s = 42696 (42696 target) ; Learning rate = 0.000601 ; Loss = 1.646069\n",
      "2024-12-10 14:26:28.927000: I runner.py:310] Step = 21700 ; steps/s = 1.61, tokens/s = 42664 (42664 target) ; Learning rate = 0.000600 ; Loss = 1.655374\n",
      "2024-12-10 14:27:30.538000: I runner.py:310] Step = 21800 ; steps/s = 1.62, tokens/s = 42111 (42111 target) ; Learning rate = 0.000599 ; Loss = 1.629230\n",
      "2024-12-10 14:28:32.482000: I runner.py:310] Step = 21900 ; steps/s = 1.61, tokens/s = 42672 (42672 target) ; Learning rate = 0.000597 ; Loss = 1.641859\n",
      "2024-12-10 14:29:34.443000: I runner.py:310] Step = 22000 ; steps/s = 1.61, tokens/s = 42724 (42724 target) ; Learning rate = 0.000596 ; Loss = 1.631129\n",
      "2024-12-10 14:30:36.404000: I runner.py:310] Step = 22100 ; steps/s = 1.61, tokens/s = 42690 (42690 target) ; Learning rate = 0.000595 ; Loss = 1.632347\n",
      "2024-12-10 14:31:37.959000: I runner.py:310] Step = 22200 ; steps/s = 1.62, tokens/s = 42132 (42132 target) ; Learning rate = 0.000593 ; Loss = 1.626949\n",
      "2024-12-10 14:32:39.911000: I runner.py:310] Step = 22300 ; steps/s = 1.61, tokens/s = 42717 (42717 target) ; Learning rate = 0.000592 ; Loss = 1.632986\n",
      "2024-12-10 14:33:41.852000: I runner.py:310] Step = 22400 ; steps/s = 1.61, tokens/s = 42694 (42694 target) ; Learning rate = 0.000591 ; Loss = 1.626955\n",
      "2024-12-10 14:34:43.845000: I runner.py:310] Step = 22500 ; steps/s = 1.61, tokens/s = 42626 (42626 target) ; Learning rate = 0.000589 ; Loss = 1.637569\n",
      "2024-12-10 14:35:45.471000: I runner.py:310] Step = 22600 ; steps/s = 1.62, tokens/s = 42135 (42135 target) ; Learning rate = 0.000588 ; Loss = 1.645543\n",
      "2024-12-10 14:36:47.428000: I runner.py:310] Step = 22700 ; steps/s = 1.61, tokens/s = 42719 (42719 target) ; Learning rate = 0.000587 ; Loss = 1.628787\n",
      "2024-12-10 14:37:49.426000: I runner.py:310] Step = 22800 ; steps/s = 1.61, tokens/s = 42677 (42677 target) ; Learning rate = 0.000585 ; Loss = 1.627881\n",
      "2024-12-10 14:38:51.404000: I runner.py:310] Step = 22900 ; steps/s = 1.61, tokens/s = 42650 (42650 target) ; Learning rate = 0.000584 ; Loss = 1.617326\n",
      "2024-12-10 14:39:52.927000: I runner.py:310] Step = 23000 ; steps/s = 1.63, tokens/s = 42160 (42160 target) ; Learning rate = 0.000583 ; Loss = 1.615770\n",
      "2024-12-10 14:40:54.932000: I runner.py:310] Step = 23100 ; steps/s = 1.61, tokens/s = 42671 (42671 target) ; Learning rate = 0.000582 ; Loss = 1.611657\n",
      "2024-12-10 14:41:56.915000: I runner.py:310] Step = 23200 ; steps/s = 1.61, tokens/s = 42671 (42671 target) ; Learning rate = 0.000580 ; Loss = 1.627582\n",
      "2024-12-10 14:42:58.864000: I runner.py:310] Step = 23300 ; steps/s = 1.61, tokens/s = 42687 (42687 target) ; Learning rate = 0.000579 ; Loss = 1.633749\n",
      "2024-12-10 14:44:00.353000: I runner.py:310] Step = 23400 ; steps/s = 1.63, tokens/s = 42187 (42187 target) ; Learning rate = 0.000578 ; Loss = 1.623530\n",
      "2024-12-10 14:45:02.353000: I runner.py:310] Step = 23500 ; steps/s = 1.61, tokens/s = 42678 (42678 target) ; Learning rate = 0.000577 ; Loss = 1.624044\n",
      "2024-12-10 14:46:04.378000: I runner.py:310] Step = 23600 ; steps/s = 1.61, tokens/s = 42607 (42607 target) ; Learning rate = 0.000575 ; Loss = 1.628414\n",
      "2024-12-10 14:47:06.287000: I runner.py:310] Step = 23700 ; steps/s = 1.62, tokens/s = 42716 (42716 target) ; Learning rate = 0.000574 ; Loss = 1.627987\n",
      "2024-12-10 14:48:07.847000: I runner.py:310] Step = 23800 ; steps/s = 1.62, tokens/s = 42177 (42177 target) ; Learning rate = 0.000573 ; Loss = 1.605497\n",
      "2024-12-10 14:49:09.806000: I runner.py:310] Step = 23900 ; steps/s = 1.61, tokens/s = 42678 (42678 target) ; Learning rate = 0.000572 ; Loss = 1.620117\n",
      "2024-12-10 14:50:11.757000: I runner.py:310] Step = 24000 ; steps/s = 1.61, tokens/s = 42681 (42681 target) ; Learning rate = 0.000571 ; Loss = 1.627799\n",
      "2024-12-10 14:51:13.687000: I runner.py:310] Step = 24100 ; steps/s = 1.61, tokens/s = 42705 (42705 target) ; Learning rate = 0.000569 ; Loss = 1.630409\n",
      "2024-12-10 14:52:15.154000: I runner.py:310] Step = 24200 ; steps/s = 1.63, tokens/s = 42248 (42248 target) ; Learning rate = 0.000568 ; Loss = 1.607647\n",
      "2024-12-10 14:53:17.136000: I runner.py:310] Step = 24300 ; steps/s = 1.61, tokens/s = 42693 (42693 target) ; Learning rate = 0.000567 ; Loss = 1.610352\n",
      "2024-12-10 14:54:19.026000: I runner.py:310] Step = 24400 ; steps/s = 1.62, tokens/s = 42722 (42722 target) ; Learning rate = 0.000566 ; Loss = 1.620356\n",
      "2024-12-10 14:55:20.964000: I runner.py:310] Step = 24500 ; steps/s = 1.61, tokens/s = 42686 (42686 target) ; Learning rate = 0.000565 ; Loss = 1.636688\n",
      "2024-12-10 14:56:22.461000: I runner.py:310] Step = 24600 ; steps/s = 1.63, tokens/s = 42199 (42199 target) ; Learning rate = 0.000564 ; Loss = 1.608910\n",
      "2024-12-10 14:57:24.447000: I runner.py:310] Step = 24700 ; steps/s = 1.61, tokens/s = 42693 (42693 target) ; Learning rate = 0.000562 ; Loss = 1.611038\n",
      "2024-12-10 14:58:26.411000: I runner.py:310] Step = 24800 ; steps/s = 1.61, tokens/s = 42681 (42681 target) ; Learning rate = 0.000561 ; Loss = 1.610509\n",
      "2024-12-10 14:59:28.334000: I runner.py:310] Step = 24900 ; steps/s = 1.62, tokens/s = 42655 (42655 target) ; Learning rate = 0.000560 ; Loss = 1.623082\n",
      "2024-12-10 15:00:29.847000: I runner.py:310] Step = 25000 ; steps/s = 1.63, tokens/s = 42203 (42203 target) ; Learning rate = 0.000559 ; Loss = 1.622823\n",
      "2024-12-10 15:00:29.848000: I training.py:192] Running evaluation for step 25000\n",
      "2024-12-10 15:05:29.443000: I training.py:192] Evaluation result for step 25000: loss = 1.040170 ; perplexity = 2.829697\n",
      "2024-12-10 15:06:31.279000: I runner.py:310] Step = 25100 ; steps/s = 1.62, tokens/s = 42778 (42778 target) ; Learning rate = 0.000558 ; Loss = 1.610689\n",
      "2024-12-10 15:07:33.275000: I runner.py:310] Step = 25200 ; steps/s = 1.61, tokens/s = 42670 (42670 target) ; Learning rate = 0.000557 ; Loss = 1.607880\n",
      "2024-12-10 15:08:35.337000: I runner.py:310] Step = 25300 ; steps/s = 1.61, tokens/s = 42603 (42603 target) ; Learning rate = 0.000556 ; Loss = 1.624014\n",
      "2024-12-10 15:09:36.935000: I runner.py:310] Step = 25400 ; steps/s = 1.62, tokens/s = 42141 (42141 target) ; Learning rate = 0.000555 ; Loss = 1.607439\n",
      "2024-12-10 15:10:38.813000: I runner.py:310] Step = 25500 ; steps/s = 1.62, tokens/s = 42712 (42712 target) ; Learning rate = 0.000553 ; Loss = 1.613492\n",
      "2024-12-10 15:11:40.836000: I runner.py:310] Step = 25600 ; steps/s = 1.61, tokens/s = 42650 (42650 target) ; Learning rate = 0.000552 ; Loss = 1.616518\n",
      "2024-12-10 15:12:42.864000: I runner.py:310] Step = 25700 ; steps/s = 1.61, tokens/s = 42641 (42641 target) ; Learning rate = 0.000551 ; Loss = 1.614297\n",
      "2024-12-10 15:13:44.325000: I runner.py:310] Step = 25800 ; steps/s = 1.63, tokens/s = 42253 (42253 target) ; Learning rate = 0.000550 ; Loss = 1.600406\n",
      "2024-12-10 15:14:46.297000: I runner.py:310] Step = 25900 ; steps/s = 1.61, tokens/s = 42655 (42655 target) ; Learning rate = 0.000549 ; Loss = 1.612839\n",
      "2024-12-10 15:15:48.212000: I runner.py:310] Step = 26000 ; steps/s = 1.62, tokens/s = 42752 (42752 target) ; Learning rate = 0.000548 ; Loss = 1.610304\n",
      "2024-12-10 15:16:50.162000: I runner.py:310] Step = 26100 ; steps/s = 1.61, tokens/s = 42687 (42687 target) ; Learning rate = 0.000547 ; Loss = 1.610315\n",
      "2024-12-10 15:17:51.736000: I runner.py:310] Step = 26200 ; steps/s = 1.62, tokens/s = 42134 (42134 target) ; Learning rate = 0.000546 ; Loss = 1.597446\n",
      "2024-12-10 15:18:53.745000: I runner.py:310] Step = 26300 ; steps/s = 1.61, tokens/s = 42678 (42678 target) ; Learning rate = 0.000545 ; Loss = 1.596509\n",
      "2024-12-10 15:19:55.686000: I runner.py:310] Step = 26400 ; steps/s = 1.61, tokens/s = 42663 (42663 target) ; Learning rate = 0.000544 ; Loss = 1.613197\n",
      "2024-12-10 15:20:57.625000: I runner.py:310] Step = 26500 ; steps/s = 1.61, tokens/s = 42680 (42680 target) ; Learning rate = 0.000543 ; Loss = 1.613897\n",
      "2024-12-10 15:21:59.103000: I runner.py:310] Step = 26600 ; steps/s = 1.63, tokens/s = 42247 (42247 target) ; Learning rate = 0.000542 ; Loss = 1.606653\n",
      "2024-12-10 15:23:01.101000: I runner.py:310] Step = 26700 ; steps/s = 1.61, tokens/s = 42691 (42691 target) ; Learning rate = 0.000541 ; Loss = 1.596885\n",
      "2024-12-10 15:24:03.127000: I runner.py:310] Step = 26800 ; steps/s = 1.61, tokens/s = 42616 (42616 target) ; Learning rate = 0.000540 ; Loss = 1.612081\n",
      "2024-12-10 15:25:05.100000: I runner.py:310] Step = 26900 ; steps/s = 1.61, tokens/s = 42653 (42653 target) ; Learning rate = 0.000539 ; Loss = 1.609161\n",
      "2024-12-10 15:26:06.696000: I runner.py:310] Step = 27000 ; steps/s = 1.62, tokens/s = 42139 (42139 target) ; Learning rate = 0.000538 ; Loss = 1.612514\n",
      "2024-12-10 15:27:08.647000: I runner.py:310] Step = 27100 ; steps/s = 1.61, tokens/s = 42714 (42714 target) ; Learning rate = 0.000537 ; Loss = 1.598909\n",
      "2024-12-10 15:28:10.627000: I runner.py:310] Step = 27200 ; steps/s = 1.61, tokens/s = 42676 (42676 target) ; Learning rate = 0.000536 ; Loss = 1.599713\n",
      "2024-12-10 15:29:12.608000: I runner.py:310] Step = 27300 ; steps/s = 1.61, tokens/s = 42651 (42651 target) ; Learning rate = 0.000535 ; Loss = 1.600466\n",
      "2024-12-10 15:30:14.187000: I runner.py:310] Step = 27400 ; steps/s = 1.62, tokens/s = 42149 (42149 target) ; Learning rate = 0.000534 ; Loss = 1.589234\n",
      "2024-12-10 15:31:16.170000: I runner.py:310] Step = 27500 ; steps/s = 1.61, tokens/s = 42661 (42661 target) ; Learning rate = 0.000533 ; Loss = 1.607623\n",
      "2024-12-10 15:32:18.127000: I runner.py:310] Step = 27600 ; steps/s = 1.61, tokens/s = 42682 (42682 target) ; Learning rate = 0.000532 ; Loss = 1.595385\n",
      "2024-12-10 15:33:20.092000: I runner.py:310] Step = 27700 ; steps/s = 1.61, tokens/s = 42674 (42674 target) ; Learning rate = 0.000531 ; Loss = 1.610424\n",
      "2024-12-10 15:34:21.563000: I runner.py:310] Step = 27800 ; steps/s = 1.63, tokens/s = 42208 (42208 target) ; Learning rate = 0.000530 ; Loss = 1.586104\n",
      "2024-12-10 15:35:23.539000: I runner.py:310] Step = 27900 ; steps/s = 1.61, tokens/s = 42680 (42680 target) ; Learning rate = 0.000529 ; Loss = 1.611659\n",
      "2024-12-10 15:36:26.054000: I runner.py:310] Step = 28000 ; steps/s = 1.60, tokens/s = 42309 (42309 target) ; Learning rate = 0.000528 ; Loss = 1.610628\n",
      "2024-12-10 15:37:28.046000: I runner.py:310] Step = 28100 ; steps/s = 1.61, tokens/s = 42662 (42662 target) ; Learning rate = 0.000527 ; Loss = 1.603422\n",
      "2024-12-10 15:38:29.601000: I runner.py:310] Step = 28200 ; steps/s = 1.62, tokens/s = 42152 (42152 target) ; Learning rate = 0.000526 ; Loss = 1.604011\n",
      "2024-12-10 15:39:31.609000: I runner.py:310] Step = 28300 ; steps/s = 1.61, tokens/s = 42655 (42655 target) ; Learning rate = 0.000525 ; Loss = 1.595753\n",
      "2024-12-10 15:40:33.593000: I runner.py:310] Step = 28400 ; steps/s = 1.61, tokens/s = 42674 (42674 target) ; Learning rate = 0.000524 ; Loss = 1.608511\n",
      "2024-12-10 15:41:35.547000: I runner.py:310] Step = 28500 ; steps/s = 1.61, tokens/s = 42682 (42682 target) ; Learning rate = 0.000524 ; Loss = 1.599276\n",
      "2024-12-10 15:42:37.146000: I runner.py:310] Step = 28600 ; steps/s = 1.62, tokens/s = 42141 (42141 target) ; Learning rate = 0.000523 ; Loss = 1.602757\n",
      "2024-12-10 15:43:39.146000: I runner.py:310] Step = 28700 ; steps/s = 1.61, tokens/s = 42692 (42692 target) ; Learning rate = 0.000522 ; Loss = 1.592266\n",
      "2024-12-10 15:44:41.123000: I runner.py:310] Step = 28800 ; steps/s = 1.61, tokens/s = 42663 (42663 target) ; Learning rate = 0.000521 ; Loss = 1.604073\n",
      "2024-12-10 15:45:43.124000: I runner.py:310] Step = 28900 ; steps/s = 1.61, tokens/s = 42642 (42642 target) ; Learning rate = 0.000520 ; Loss = 1.598993\n",
      "2024-12-10 15:46:44.722000: I runner.py:310] Step = 29000 ; steps/s = 1.62, tokens/s = 42124 (42124 target) ; Learning rate = 0.000519 ; Loss = 1.583988\n",
      "2024-12-10 15:47:46.622000: I runner.py:310] Step = 29100 ; steps/s = 1.62, tokens/s = 42734 (42734 target) ; Learning rate = 0.000518 ; Loss = 1.598925\n",
      "2024-12-10 15:48:48.601000: I runner.py:310] Step = 29200 ; steps/s = 1.61, tokens/s = 42664 (42664 target) ; Learning rate = 0.000517 ; Loss = 1.595441\n",
      "2024-12-10 15:49:50.599000: I runner.py:310] Step = 29300 ; steps/s = 1.61, tokens/s = 42638 (42638 target) ; Learning rate = 0.000516 ; Loss = 1.601514\n",
      "2024-12-10 15:50:52.111000: I runner.py:310] Step = 29400 ; steps/s = 1.63, tokens/s = 42202 (42202 target) ; Learning rate = 0.000515 ; Loss = 1.573142\n",
      "2024-12-10 15:51:54.114000: I runner.py:310] Step = 29500 ; steps/s = 1.61, tokens/s = 42666 (42666 target) ; Learning rate = 0.000515 ; Loss = 1.585075\n",
      "2024-12-10 15:52:56.062000: I runner.py:310] Step = 29600 ; steps/s = 1.61, tokens/s = 42686 (42686 target) ; Learning rate = 0.000514 ; Loss = 1.605094\n",
      "2024-12-10 15:53:58.048000: I runner.py:310] Step = 29700 ; steps/s = 1.61, tokens/s = 42660 (42660 target) ; Learning rate = 0.000513 ; Loss = 1.599304\n",
      "2024-12-10 15:54:59.545000: I runner.py:310] Step = 29800 ; steps/s = 1.63, tokens/s = 42206 (42206 target) ; Learning rate = 0.000512 ; Loss = 1.577052\n",
      "2024-12-10 15:56:01.484000: I runner.py:310] Step = 29900 ; steps/s = 1.61, tokens/s = 42674 (42674 target) ; Learning rate = 0.000511 ; Loss = 1.590400\n",
      "2024-12-10 15:57:03.408000: I runner.py:310] Step = 30000 ; steps/s = 1.62, tokens/s = 42707 (42707 target) ; Learning rate = 0.000510 ; Loss = 1.607349\n",
      "2024-12-10 15:57:05.318000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-30000\n",
      "2024-12-10 15:57:05.318000: I training.py:192] Running evaluation for step 30000\n",
      "2024-12-10 16:02:02.298000: I training.py:192] Evaluation result for step 30000: loss = 1.057450 ; perplexity = 2.879020\n",
      "2024-12-10 16:03:04.081000: I runner.py:310] Step = 30100 ; steps/s = 1.62, tokens/s = 42832 (42832 target) ; Learning rate = 0.000509 ; Loss = 1.603274\n",
      "2024-12-10 16:04:05.505000: I runner.py:310] Step = 30200 ; steps/s = 1.63, tokens/s = 42249 (42249 target) ; Learning rate = 0.000509 ; Loss = 1.605954\n",
      "2024-12-10 16:05:07.423000: I runner.py:310] Step = 30300 ; steps/s = 1.62, tokens/s = 42702 (42702 target) ; Learning rate = 0.000508 ; Loss = 1.588642\n",
      "2024-12-10 16:06:09.475000: I runner.py:310] Step = 30400 ; steps/s = 1.61, tokens/s = 42639 (42639 target) ; Learning rate = 0.000507 ; Loss = 1.597939\n",
      "2024-12-10 16:07:11.524000: I runner.py:310] Step = 30500 ; steps/s = 1.61, tokens/s = 42621 (42621 target) ; Learning rate = 0.000506 ; Loss = 1.590852\n",
      "2024-12-10 16:08:13.029000: I runner.py:310] Step = 30600 ; steps/s = 1.63, tokens/s = 42217 (42217 target) ; Learning rate = 0.000505 ; Loss = 1.570081\n",
      "2024-12-10 16:09:14.977000: I runner.py:310] Step = 30700 ; steps/s = 1.61, tokens/s = 42671 (42671 target) ; Learning rate = 0.000504 ; Loss = 1.586994\n",
      "2024-12-10 16:10:16.939000: I runner.py:310] Step = 30800 ; steps/s = 1.61, tokens/s = 42672 (42672 target) ; Learning rate = 0.000504 ; Loss = 1.583970\n",
      "2024-12-10 16:11:18.909000: I runner.py:310] Step = 30900 ; steps/s = 1.61, tokens/s = 42697 (42697 target) ; Learning rate = 0.000503 ; Loss = 1.607182\n",
      "2024-12-10 16:12:20.464000: I runner.py:310] Step = 31000 ; steps/s = 1.62, tokens/s = 42175 (42175 target) ; Learning rate = 0.000502 ; Loss = 1.582396\n",
      "2024-12-10 16:13:22.449000: I runner.py:310] Step = 31100 ; steps/s = 1.61, tokens/s = 42638 (42638 target) ; Learning rate = 0.000501 ; Loss = 1.588468\n",
      "2024-12-10 16:14:24.428000: I runner.py:310] Step = 31200 ; steps/s = 1.61, tokens/s = 42670 (42670 target) ; Learning rate = 0.000500 ; Loss = 1.589723\n",
      "2024-12-10 16:15:26.054000: I runner.py:310] Step = 31300 ; steps/s = 1.62, tokens/s = 42302 (42302 target) ; Learning rate = 0.000500 ; Loss = 1.586092\n",
      "2024-12-10 16:16:27.925000: I runner.py:310] Step = 31400 ; steps/s = 1.62, tokens/s = 42585 (42585 target) ; Learning rate = 0.000499 ; Loss = 1.575921\n",
      "2024-12-10 16:17:29.930000: I runner.py:310] Step = 31500 ; steps/s = 1.61, tokens/s = 42638 (42638 target) ; Learning rate = 0.000498 ; Loss = 1.582035\n",
      "2024-12-10 16:18:31.900000: I runner.py:310] Step = 31600 ; steps/s = 1.61, tokens/s = 42682 (42682 target) ; Learning rate = 0.000497 ; Loss = 1.585380\n",
      "2024-12-10 16:19:33.512000: I runner.py:310] Step = 31700 ; steps/s = 1.62, tokens/s = 42107 (42107 target) ; Learning rate = 0.000496 ; Loss = 1.581926\n",
      "2024-12-10 16:20:35.423000: I runner.py:310] Step = 31800 ; steps/s = 1.62, tokens/s = 42732 (42732 target) ; Learning rate = 0.000496 ; Loss = 1.583495\n",
      "2024-12-10 16:21:37.366000: I runner.py:310] Step = 31900 ; steps/s = 1.61, tokens/s = 42709 (42709 target) ; Learning rate = 0.000495 ; Loss = 1.582010\n",
      "2024-12-10 16:22:39.360000: I runner.py:310] Step = 32000 ; steps/s = 1.61, tokens/s = 42661 (42661 target) ; Learning rate = 0.000494 ; Loss = 1.586356\n",
      "2024-12-10 16:23:40.868000: I runner.py:310] Step = 32100 ; steps/s = 1.63, tokens/s = 42177 (42177 target) ; Learning rate = 0.000493 ; Loss = 1.572784\n",
      "2024-12-10 16:24:43.616000: I runner.py:310] Step = 32200 ; steps/s = 1.59, tokens/s = 42143 (42143 target) ; Learning rate = 0.000493 ; Loss = 1.572700\n",
      "2024-12-10 16:25:45.865000: I runner.py:310] Step = 32300 ; steps/s = 1.61, tokens/s = 42478 (42478 target) ; Learning rate = 0.000492 ; Loss = 1.587226\n",
      "2024-12-10 16:26:47.953000: I runner.py:310] Step = 32400 ; steps/s = 1.61, tokens/s = 42598 (42598 target) ; Learning rate = 0.000491 ; Loss = 1.581884\n",
      "2024-12-10 16:27:49.541000: I runner.py:310] Step = 32500 ; steps/s = 1.62, tokens/s = 42143 (42143 target) ; Learning rate = 0.000490 ; Loss = 1.578117\n",
      "2024-12-10 16:28:51.541000: I runner.py:310] Step = 32600 ; steps/s = 1.61, tokens/s = 42663 (42663 target) ; Learning rate = 0.000490 ; Loss = 1.571715\n",
      "2024-12-10 16:29:53.579000: I runner.py:310] Step = 32700 ; steps/s = 1.61, tokens/s = 42608 (42608 target) ; Learning rate = 0.000489 ; Loss = 1.580363\n",
      "2024-12-10 16:30:55.587000: I runner.py:310] Step = 32800 ; steps/s = 1.61, tokens/s = 42672 (42672 target) ; Learning rate = 0.000488 ; Loss = 1.582300\n",
      "2024-12-10 16:31:57.191000: I runner.py:310] Step = 32900 ; steps/s = 1.62, tokens/s = 42129 (42129 target) ; Learning rate = 0.000487 ; Loss = 1.574596\n",
      "2024-12-10 16:32:59.217000: I runner.py:310] Step = 33000 ; steps/s = 1.61, tokens/s = 42653 (42653 target) ; Learning rate = 0.000487 ; Loss = 1.564978\n",
      "2024-12-10 16:34:01.265000: I runner.py:310] Step = 33100 ; steps/s = 1.61, tokens/s = 42643 (42643 target) ; Learning rate = 0.000486 ; Loss = 1.578615\n",
      "2024-12-10 16:35:03.323000: I runner.py:310] Step = 33200 ; steps/s = 1.61, tokens/s = 42591 (42591 target) ; Learning rate = 0.000485 ; Loss = 1.585559\n",
      "2024-12-10 16:36:04.920000: I runner.py:310] Step = 33300 ; steps/s = 1.62, tokens/s = 42141 (42141 target) ; Learning rate = 0.000484 ; Loss = 1.567566\n",
      "2024-12-10 16:37:06.929000: I runner.py:310] Step = 33400 ; steps/s = 1.61, tokens/s = 42651 (42651 target) ; Learning rate = 0.000484 ; Loss = 1.571086\n",
      "2024-12-10 16:38:08.981000: I runner.py:310] Step = 33500 ; steps/s = 1.61, tokens/s = 42590 (42590 target) ; Learning rate = 0.000483 ; Loss = 1.585207\n",
      "2024-12-10 16:39:11.012000: I runner.py:310] Step = 33600 ; steps/s = 1.61, tokens/s = 42667 (42667 target) ; Learning rate = 0.000482 ; Loss = 1.577237\n",
      "2024-12-10 16:40:12.593000: I runner.py:310] Step = 33700 ; steps/s = 1.62, tokens/s = 42131 (42131 target) ; Learning rate = 0.000481 ; Loss = 1.578202\n",
      "2024-12-10 16:41:14.622000: I runner.py:310] Step = 33800 ; steps/s = 1.61, tokens/s = 42660 (42660 target) ; Learning rate = 0.000481 ; Loss = 1.565566\n",
      "2024-12-10 16:42:16.699000: I runner.py:310] Step = 33900 ; steps/s = 1.61, tokens/s = 42571 (42571 target) ; Learning rate = 0.000480 ; Loss = 1.568140\n",
      "2024-12-10 16:43:18.789000: I runner.py:310] Step = 34000 ; steps/s = 1.61, tokens/s = 42583 (42583 target) ; Learning rate = 0.000479 ; Loss = 1.577132\n",
      "2024-12-10 16:44:20.451000: I runner.py:310] Step = 34100 ; steps/s = 1.62, tokens/s = 42107 (42107 target) ; Learning rate = 0.000479 ; Loss = 1.576951\n",
      "2024-12-10 16:45:22.528000: I runner.py:310] Step = 34200 ; steps/s = 1.61, tokens/s = 42613 (42613 target) ; Learning rate = 0.000478 ; Loss = 1.569651\n",
      "2024-12-10 16:46:24.569000: I runner.py:310] Step = 34300 ; steps/s = 1.61, tokens/s = 42637 (42637 target) ; Learning rate = 0.000477 ; Loss = 1.581511\n",
      "2024-12-10 16:47:26.621000: I runner.py:310] Step = 34400 ; steps/s = 1.61, tokens/s = 42590 (42590 target) ; Learning rate = 0.000477 ; Loss = 1.586519\n",
      "2024-12-10 16:48:28.183000: I runner.py:310] Step = 34500 ; steps/s = 1.62, tokens/s = 42139 (42139 target) ; Learning rate = 0.000476 ; Loss = 1.587092\n",
      "2024-12-10 16:49:30.253000: I runner.py:310] Step = 34600 ; steps/s = 1.61, tokens/s = 42631 (42631 target) ; Learning rate = 0.000475 ; Loss = 1.573130\n",
      "2024-12-10 16:50:32.306000: I runner.py:310] Step = 34700 ; steps/s = 1.61, tokens/s = 42632 (42632 target) ; Learning rate = 0.000474 ; Loss = 1.571269\n",
      "2024-12-10 16:51:34.366000: I runner.py:310] Step = 34800 ; steps/s = 1.61, tokens/s = 42592 (42592 target) ; Learning rate = 0.000474 ; Loss = 1.566880\n",
      "2024-12-10 16:52:35.912000: I runner.py:310] Step = 34900 ; steps/s = 1.62, tokens/s = 42171 (42171 target) ; Learning rate = 0.000473 ; Loss = 1.576739\n",
      "2024-12-10 16:53:37.937000: I runner.py:310] Step = 35000 ; steps/s = 1.61, tokens/s = 42655 (42655 target) ; Learning rate = 0.000472 ; Loss = 1.577097\n",
      "2024-12-10 16:53:37.939000: I training.py:192] Running evaluation for step 35000\n",
      "2024-12-10 16:58:17.276000: I training.py:192] Evaluation result for step 35000: loss = 1.071224 ; perplexity = 2.918950\n",
      "2024-12-10 16:59:19.137000: I runner.py:310] Step = 35100 ; steps/s = 1.62, tokens/s = 42748 (42748 target) ; Learning rate = 0.000472 ; Loss = 1.567341\n",
      "2024-12-10 17:00:21.133000: I runner.py:310] Step = 35200 ; steps/s = 1.61, tokens/s = 42639 (42639 target) ; Learning rate = 0.000471 ; Loss = 1.571859\n",
      "2024-12-10 17:01:22.724000: I runner.py:310] Step = 35300 ; steps/s = 1.62, tokens/s = 42171 (42171 target) ; Learning rate = 0.000470 ; Loss = 1.579312\n",
      "2024-12-10 17:02:24.756000: I runner.py:310] Step = 35400 ; steps/s = 1.61, tokens/s = 42663 (42663 target) ; Learning rate = 0.000470 ; Loss = 1.565002\n",
      "2024-12-10 17:03:26.797000: I runner.py:310] Step = 35500 ; steps/s = 1.61, tokens/s = 42620 (42620 target) ; Learning rate = 0.000469 ; Loss = 1.585527\n",
      "2024-12-10 17:04:28.867000: I runner.py:310] Step = 35600 ; steps/s = 1.61, tokens/s = 42597 (42597 target) ; Learning rate = 0.000468 ; Loss = 1.568928\n",
      "2024-12-10 17:05:30.430000: I runner.py:310] Step = 35700 ; steps/s = 1.62, tokens/s = 42139 (42139 target) ; Learning rate = 0.000468 ; Loss = 1.576026\n",
      "2024-12-10 17:06:32.456000: I runner.py:310] Step = 35800 ; steps/s = 1.61, tokens/s = 42613 (42613 target) ; Learning rate = 0.000467 ; Loss = 1.564216\n",
      "2024-12-10 17:07:34.551000: I runner.py:310] Step = 35900 ; steps/s = 1.61, tokens/s = 42584 (42584 target) ; Learning rate = 0.000466 ; Loss = 1.564438\n",
      "2024-12-10 17:08:36.559000: I runner.py:310] Step = 36000 ; steps/s = 1.61, tokens/s = 42677 (42677 target) ; Learning rate = 0.000466 ; Loss = 1.573412\n",
      "2024-12-10 17:09:38.174000: I runner.py:310] Step = 36100 ; steps/s = 1.62, tokens/s = 42137 (42137 target) ; Learning rate = 0.000465 ; Loss = 1.576914\n",
      "2024-12-10 17:10:40.243000: I runner.py:310] Step = 36200 ; steps/s = 1.61, tokens/s = 42616 (42616 target) ; Learning rate = 0.000465 ; Loss = 1.558816\n",
      "2024-12-10 17:11:42.281000: I runner.py:310] Step = 36300 ; steps/s = 1.61, tokens/s = 42614 (42614 target) ; Learning rate = 0.000464 ; Loss = 1.564237\n",
      "2024-12-10 17:12:44.312000: I runner.py:310] Step = 36400 ; steps/s = 1.61, tokens/s = 42623 (42623 target) ; Learning rate = 0.000463 ; Loss = 1.569461\n",
      "2024-12-10 17:13:45.864000: I runner.py:310] Step = 36500 ; steps/s = 1.62, tokens/s = 42190 (42190 target) ; Learning rate = 0.000463 ; Loss = 1.562838\n",
      "2024-12-10 17:14:47.880000: I runner.py:310] Step = 36600 ; steps/s = 1.61, tokens/s = 42641 (42641 target) ; Learning rate = 0.000462 ; Loss = 1.560360\n",
      "2024-12-10 17:15:49.932000: I runner.py:310] Step = 36700 ; steps/s = 1.61, tokens/s = 42624 (42624 target) ; Learning rate = 0.000461 ; Loss = 1.571865\n",
      "2024-12-10 17:16:52.073000: I runner.py:310] Step = 36800 ; steps/s = 1.61, tokens/s = 42558 (42558 target) ; Learning rate = 0.000461 ; Loss = 1.574677\n",
      "2024-12-10 17:17:53.639000: I runner.py:310] Step = 36900 ; steps/s = 1.62, tokens/s = 42153 (42153 target) ; Learning rate = 0.000460 ; Loss = 1.571051\n",
      "2024-12-10 17:18:55.622000: I runner.py:310] Step = 37000 ; steps/s = 1.61, tokens/s = 42656 (42656 target) ; Learning rate = 0.000460 ; Loss = 1.561535\n",
      "2024-12-10 17:19:57.628000: I runner.py:310] Step = 37100 ; steps/s = 1.61, tokens/s = 42654 (42654 target) ; Learning rate = 0.000459 ; Loss = 1.557859\n",
      "2024-12-10 17:20:59.604000: I runner.py:310] Step = 37200 ; steps/s = 1.61, tokens/s = 42700 (42700 target) ; Learning rate = 0.000458 ; Loss = 1.563090\n",
      "2024-12-10 17:22:01.231000: I runner.py:310] Step = 37300 ; steps/s = 1.62, tokens/s = 42106 (42106 target) ; Learning rate = 0.000458 ; Loss = 1.571893\n",
      "2024-12-10 17:23:03.264000: I runner.py:310] Step = 37400 ; steps/s = 1.61, tokens/s = 42610 (42610 target) ; Learning rate = 0.000457 ; Loss = 1.560282\n",
      "2024-12-10 17:24:05.287000: I runner.py:310] Step = 37500 ; steps/s = 1.61, tokens/s = 42634 (42634 target) ; Learning rate = 0.000456 ; Loss = 1.561390\n",
      "2024-12-10 17:25:07.335000: I runner.py:310] Step = 37600 ; steps/s = 1.61, tokens/s = 42653 (42653 target) ; Learning rate = 0.000456 ; Loss = 1.562905\n",
      "2024-12-10 17:26:08.916000: I runner.py:310] Step = 37700 ; steps/s = 1.62, tokens/s = 42140 (42140 target) ; Learning rate = 0.000455 ; Loss = 1.568196\n",
      "2024-12-10 17:27:10.903000: I runner.py:310] Step = 37800 ; steps/s = 1.61, tokens/s = 42655 (42655 target) ; Learning rate = 0.000455 ; Loss = 1.563103\n",
      "2024-12-10 17:28:12.939000: I runner.py:310] Step = 37900 ; steps/s = 1.61, tokens/s = 42649 (42649 target) ; Learning rate = 0.000454 ; Loss = 1.559961\n",
      "2024-12-10 17:29:14.985000: I runner.py:310] Step = 38000 ; steps/s = 1.61, tokens/s = 42613 (42613 target) ; Learning rate = 0.000453 ; Loss = 1.564805\n",
      "2024-12-10 17:30:16.593000: I runner.py:310] Step = 38100 ; steps/s = 1.62, tokens/s = 42116 (42116 target) ; Learning rate = 0.000453 ; Loss = 1.574524\n",
      "2024-12-10 17:31:18.667000: I runner.py:310] Step = 38200 ; steps/s = 1.61, tokens/s = 42599 (42599 target) ; Learning rate = 0.000452 ; Loss = 1.558835\n",
      "2024-12-10 17:32:20.747000: I runner.py:310] Step = 38300 ; steps/s = 1.61, tokens/s = 42617 (42617 target) ; Learning rate = 0.000452 ; Loss = 1.561797\n",
      "2024-12-10 17:33:22.769000: I runner.py:310] Step = 38400 ; steps/s = 1.61, tokens/s = 42651 (42651 target) ; Learning rate = 0.000451 ; Loss = 1.557022\n",
      "2024-12-10 17:34:24.368000: I runner.py:310] Step = 38500 ; steps/s = 1.62, tokens/s = 42111 (42111 target) ; Learning rate = 0.000450 ; Loss = 1.548579\n",
      "2024-12-10 17:35:26.413000: I runner.py:310] Step = 38600 ; steps/s = 1.61, tokens/s = 42624 (42624 target) ; Learning rate = 0.000450 ; Loss = 1.566520\n",
      "2024-12-10 17:36:28.400000: I runner.py:310] Step = 38700 ; steps/s = 1.61, tokens/s = 42682 (42682 target) ; Learning rate = 0.000449 ; Loss = 1.565630\n",
      "2024-12-10 17:37:30.433000: I runner.py:310] Step = 38800 ; steps/s = 1.61, tokens/s = 42622 (42622 target) ; Learning rate = 0.000449 ; Loss = 1.569718\n",
      "2024-12-10 17:38:31.957000: I runner.py:310] Step = 38900 ; steps/s = 1.63, tokens/s = 42190 (42190 target) ; Learning rate = 0.000448 ; Loss = 1.549259\n",
      "2024-12-10 17:39:34.007000: I runner.py:310] Step = 39000 ; steps/s = 1.61, tokens/s = 42648 (42648 target) ; Learning rate = 0.000448 ; Loss = 1.551124\n",
      "2024-12-10 17:40:36.080000: I runner.py:310] Step = 39100 ; steps/s = 1.61, tokens/s = 42624 (42624 target) ; Learning rate = 0.000447 ; Loss = 1.568617\n",
      "2024-12-10 17:41:38.201000: I runner.py:310] Step = 39200 ; steps/s = 1.61, tokens/s = 42545 (42545 target) ; Learning rate = 0.000446 ; Loss = 1.567631\n",
      "2024-12-10 17:42:39.787000: I runner.py:310] Step = 39300 ; steps/s = 1.62, tokens/s = 42150 (42150 target) ; Learning rate = 0.000446 ; Loss = 1.554446\n",
      "2024-12-10 17:43:41.791000: I runner.py:310] Step = 39400 ; steps/s = 1.61, tokens/s = 42657 (42657 target) ; Learning rate = 0.000445 ; Loss = 1.566718\n",
      "2024-12-10 17:44:43.872000: I runner.py:310] Step = 39500 ; steps/s = 1.61, tokens/s = 42603 (42603 target) ; Learning rate = 0.000445 ; Loss = 1.564902\n",
      "2024-12-10 17:45:45.993000: I runner.py:310] Step = 39600 ; steps/s = 1.61, tokens/s = 42558 (42558 target) ; Learning rate = 0.000444 ; Loss = 1.564977\n",
      "2024-12-10 17:46:47.649000: I runner.py:310] Step = 39700 ; steps/s = 1.62, tokens/s = 42081 (42081 target) ; Learning rate = 0.000444 ; Loss = 1.557547\n",
      "2024-12-10 17:47:49.666000: I runner.py:310] Step = 39800 ; steps/s = 1.61, tokens/s = 42660 (42660 target) ; Learning rate = 0.000443 ; Loss = 1.559467\n",
      "2024-12-10 17:48:51.645000: I runner.py:310] Step = 39900 ; steps/s = 1.61, tokens/s = 42665 (42665 target) ; Learning rate = 0.000442 ; Loss = 1.555337\n",
      "2024-12-10 17:49:53.653000: I runner.py:310] Step = 40000 ; steps/s = 1.61, tokens/s = 42643 (42643 target) ; Learning rate = 0.000442 ; Loss = 1.558795\n",
      "2024-12-10 17:49:55.556000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-40000\n",
      "2024-12-10 17:49:55.556000: I training.py:192] Running evaluation for step 40000\n",
      "2024-12-10 17:54:48.810000: I training.py:192] Evaluation result for step 40000: loss = 1.081435 ; perplexity = 2.948910\n",
      "2024-12-10 17:55:50.228000: I runner.py:310] Step = 40100 ; steps/s = 1.63, tokens/s = 42282 (42282 target) ; Learning rate = 0.000441 ; Loss = 1.567014\n",
      "2024-12-10 17:56:52.265000: I runner.py:310] Step = 40200 ; steps/s = 1.61, tokens/s = 42665 (42665 target) ; Learning rate = 0.000441 ; Loss = 1.555225\n",
      "2024-12-10 17:57:54.367000: I runner.py:310] Step = 40300 ; steps/s = 1.61, tokens/s = 42556 (42556 target) ; Learning rate = 0.000440 ; Loss = 1.560694\n",
      "2024-12-10 17:58:56.455000: I runner.py:310] Step = 40400 ; steps/s = 1.61, tokens/s = 42578 (42578 target) ; Learning rate = 0.000440 ; Loss = 1.559030\n",
      "2024-12-10 17:59:58.072000: I runner.py:310] Step = 40500 ; steps/s = 1.62, tokens/s = 42116 (42116 target) ; Learning rate = 0.000439 ; Loss = 1.571640\n",
      "2024-12-10 18:01:00.117000: I runner.py:310] Step = 40600 ; steps/s = 1.61, tokens/s = 42631 (42631 target) ; Learning rate = 0.000439 ; Loss = 1.555403\n",
      "2024-12-10 18:02:02.190000: I runner.py:310] Step = 40700 ; steps/s = 1.61, tokens/s = 42577 (42577 target) ; Learning rate = 0.000438 ; Loss = 1.556601\n",
      "2024-12-10 18:03:04.254000: I runner.py:310] Step = 40800 ; steps/s = 1.61, tokens/s = 42625 (42625 target) ; Learning rate = 0.000438 ; Loss = 1.554281\n",
      "2024-12-10 18:04:05.819000: I runner.py:310] Step = 40900 ; steps/s = 1.62, tokens/s = 42173 (42173 target) ; Learning rate = 0.000437 ; Loss = 1.541927\n",
      "2024-12-10 18:05:07.895000: I runner.py:310] Step = 41000 ; steps/s = 1.61, tokens/s = 42602 (42602 target) ; Learning rate = 0.000437 ; Loss = 1.555138\n",
      "2024-12-10 18:06:09.922000: I runner.py:310] Step = 41100 ; steps/s = 1.61, tokens/s = 42609 (42609 target) ; Learning rate = 0.000436 ; Loss = 1.563152\n",
      "2024-12-10 18:07:11.926000: I runner.py:310] Step = 41200 ; steps/s = 1.61, tokens/s = 42669 (42669 target) ; Learning rate = 0.000435 ; Loss = 1.566921\n",
      "2024-12-10 18:08:13.480000: I runner.py:310] Step = 41300 ; steps/s = 1.62, tokens/s = 42157 (42157 target) ; Learning rate = 0.000435 ; Loss = 1.548880\n",
      "2024-12-10 18:09:15.480000: I runner.py:310] Step = 41400 ; steps/s = 1.61, tokens/s = 42621 (42621 target) ; Learning rate = 0.000434 ; Loss = 1.566515\n",
      "2024-12-10 18:10:17.491000: I runner.py:310] Step = 41500 ; steps/s = 1.61, tokens/s = 42669 (42669 target) ; Learning rate = 0.000434 ; Loss = 1.567115\n",
      "2024-12-10 18:11:19.429000: I runner.py:310] Step = 41600 ; steps/s = 1.61, tokens/s = 42463 (42463 target) ; Learning rate = 0.000433 ; Loss = 1.574503\n",
      "2024-12-10 18:12:21.210000: I runner.py:310] Step = 41700 ; steps/s = 1.62, tokens/s = 42313 (42313 target) ; Learning rate = 0.000433 ; Loss = 1.546213\n",
      "2024-12-10 18:13:23.300000: I runner.py:310] Step = 41800 ; steps/s = 1.61, tokens/s = 42567 (42567 target) ; Learning rate = 0.000432 ; Loss = 1.557233\n",
      "2024-12-10 18:14:25.387000: I runner.py:310] Step = 41900 ; steps/s = 1.61, tokens/s = 42593 (42593 target) ; Learning rate = 0.000432 ; Loss = 1.561326\n",
      "2024-12-10 18:15:26.929000: I runner.py:310] Step = 42000 ; steps/s = 1.63, tokens/s = 42148 (42148 target) ; Learning rate = 0.000431 ; Loss = 1.566408\n",
      "2024-12-10 18:16:28.934000: I runner.py:310] Step = 42100 ; steps/s = 1.61, tokens/s = 42691 (42691 target) ; Learning rate = 0.000431 ; Loss = 1.544842\n",
      "2024-12-10 18:17:30.990000: I runner.py:310] Step = 42200 ; steps/s = 1.61, tokens/s = 42609 (42609 target) ; Learning rate = 0.000430 ; Loss = 1.556037\n",
      "2024-12-10 18:18:33.078000: I runner.py:310] Step = 42300 ; steps/s = 1.61, tokens/s = 42583 (42583 target) ; Learning rate = 0.000430 ; Loss = 1.557014\n",
      "2024-12-10 18:19:34.663000: I runner.py:310] Step = 42400 ; steps/s = 1.62, tokens/s = 42127 (42127 target) ; Learning rate = 0.000429 ; Loss = 1.553966\n",
      "2024-12-10 18:20:36.735000: I runner.py:310] Step = 42500 ; steps/s = 1.61, tokens/s = 42599 (42599 target) ; Learning rate = 0.000429 ; Loss = 1.545680\n",
      "2024-12-10 18:21:38.758000: I runner.py:310] Step = 42600 ; steps/s = 1.61, tokens/s = 42636 (42636 target) ; Learning rate = 0.000428 ; Loss = 1.554343\n",
      "2024-12-10 18:22:40.855000: I runner.py:310] Step = 42700 ; steps/s = 1.61, tokens/s = 42604 (42604 target) ; Learning rate = 0.000428 ; Loss = 1.557745\n",
      "2024-12-10 18:23:42.462000: I runner.py:310] Step = 42800 ; steps/s = 1.62, tokens/s = 42134 (42134 target) ; Learning rate = 0.000427 ; Loss = 1.557335\n",
      "2024-12-10 18:24:44.536000: I runner.py:310] Step = 42900 ; steps/s = 1.61, tokens/s = 42630 (42630 target) ; Learning rate = 0.000427 ; Loss = 1.549512\n",
      "2024-12-10 18:25:46.532000: I runner.py:310] Step = 43000 ; steps/s = 1.61, tokens/s = 42663 (42663 target) ; Learning rate = 0.000426 ; Loss = 1.547195\n",
      "2024-12-10 18:26:48.602000: I runner.py:310] Step = 43100 ; steps/s = 1.61, tokens/s = 42585 (42585 target) ; Learning rate = 0.000426 ; Loss = 1.556632\n",
      "2024-12-10 18:27:50.182000: I runner.py:310] Step = 43200 ; steps/s = 1.62, tokens/s = 42151 (42151 target) ; Learning rate = 0.000425 ; Loss = 1.552189\n",
      "2024-12-10 18:28:52.198000: I runner.py:310] Step = 43300 ; steps/s = 1.61, tokens/s = 42651 (42651 target) ; Learning rate = 0.000425 ; Loss = 1.552723\n",
      "2024-12-10 18:29:54.257000: I runner.py:310] Step = 43400 ; steps/s = 1.61, tokens/s = 42571 (42571 target) ; Learning rate = 0.000424 ; Loss = 1.542953\n",
      "2024-12-10 18:30:56.306000: I runner.py:310] Step = 43500 ; steps/s = 1.61, tokens/s = 42637 (42637 target) ; Learning rate = 0.000424 ; Loss = 1.560039\n",
      "2024-12-10 18:31:57.904000: I runner.py:310] Step = 43600 ; steps/s = 1.62, tokens/s = 42149 (42149 target) ; Learning rate = 0.000423 ; Loss = 1.557899\n",
      "2024-12-10 18:32:59.898000: I runner.py:310] Step = 43700 ; steps/s = 1.61, tokens/s = 42701 (42701 target) ; Learning rate = 0.000423 ; Loss = 1.552584\n",
      "2024-12-10 18:34:01.946000: I runner.py:310] Step = 43800 ; steps/s = 1.61, tokens/s = 42622 (42622 target) ; Learning rate = 0.000422 ; Loss = 1.548577\n",
      "2024-12-10 18:35:03.975000: I runner.py:310] Step = 43900 ; steps/s = 1.61, tokens/s = 42635 (42635 target) ; Learning rate = 0.000422 ; Loss = 1.550298\n",
      "2024-12-10 18:36:05.676000: I runner.py:310] Step = 44000 ; steps/s = 1.62, tokens/s = 42034 (42034 target) ; Learning rate = 0.000421 ; Loss = 1.539679\n",
      "2024-12-10 18:37:07.701000: I runner.py:310] Step = 44100 ; steps/s = 1.61, tokens/s = 42652 (42652 target) ; Learning rate = 0.000421 ; Loss = 1.545669\n",
      "2024-12-10 18:38:09.693000: I runner.py:310] Step = 44200 ; steps/s = 1.61, tokens/s = 42658 (42658 target) ; Learning rate = 0.000420 ; Loss = 1.556080\n",
      "2024-12-10 18:39:11.749000: I runner.py:310] Step = 44300 ; steps/s = 1.61, tokens/s = 42617 (42617 target) ; Learning rate = 0.000420 ; Loss = 1.551003\n",
      "2024-12-10 18:40:13.280000: I runner.py:310] Step = 44400 ; steps/s = 1.63, tokens/s = 42193 (42193 target) ; Learning rate = 0.000419 ; Loss = 1.546302\n",
      "2024-12-10 18:41:15.300000: I runner.py:310] Step = 44500 ; steps/s = 1.61, tokens/s = 42623 (42623 target) ; Learning rate = 0.000419 ; Loss = 1.539067\n",
      "2024-12-10 18:42:17.327000: I runner.py:310] Step = 44600 ; steps/s = 1.61, tokens/s = 42649 (42649 target) ; Learning rate = 0.000419 ; Loss = 1.551545\n",
      "2024-12-10 18:43:19.290000: I runner.py:310] Step = 44700 ; steps/s = 1.61, tokens/s = 42649 (42649 target) ; Learning rate = 0.000418 ; Loss = 1.556133\n",
      "2024-12-10 18:44:20.936000: I runner.py:310] Step = 44800 ; steps/s = 1.62, tokens/s = 42129 (42129 target) ; Learning rate = 0.000418 ; Loss = 1.542691\n",
      "2024-12-10 18:45:22.938000: I runner.py:310] Step = 44900 ; steps/s = 1.61, tokens/s = 42636 (42636 target) ; Learning rate = 0.000417 ; Loss = 1.540535\n",
      "2024-12-10 18:46:24.977000: I runner.py:310] Step = 45000 ; steps/s = 1.61, tokens/s = 42652 (42652 target) ; Learning rate = 0.000417 ; Loss = 1.547985\n",
      "2024-12-10 18:46:24.978000: I training.py:192] Running evaluation for step 45000\n",
      "2024-12-10 18:51:19.428000: I training.py:192] Evaluation result for step 45000: loss = 1.094762 ; perplexity = 2.988472\n",
      "2024-12-10 18:52:21.306000: I runner.py:310] Step = 45100 ; steps/s = 1.62, tokens/s = 42732 (42732 target) ; Learning rate = 0.000416 ; Loss = 1.554065\n",
      "2024-12-10 18:53:22.883000: I runner.py:310] Step = 45200 ; steps/s = 1.62, tokens/s = 42164 (42164 target) ; Learning rate = 0.000416 ; Loss = 1.562137\n",
      "2024-12-10 18:54:24.899000: I runner.py:310] Step = 45300 ; steps/s = 1.61, tokens/s = 42634 (42634 target) ; Learning rate = 0.000415 ; Loss = 1.544945\n",
      "2024-12-10 18:55:26.970000: I runner.py:310] Step = 45400 ; steps/s = 1.61, tokens/s = 42617 (42617 target) ; Learning rate = 0.000415 ; Loss = 1.546525\n",
      "2024-12-10 18:56:29.019000: I runner.py:310] Step = 45500 ; steps/s = 1.61, tokens/s = 42616 (42616 target) ; Learning rate = 0.000414 ; Loss = 1.549123\n",
      "2024-12-10 18:57:30.685000: I runner.py:310] Step = 45600 ; steps/s = 1.62, tokens/s = 42105 (42105 target) ; Learning rate = 0.000414 ; Loss = 1.543829\n",
      "2024-12-10 18:58:32.678000: I runner.py:310] Step = 45700 ; steps/s = 1.61, tokens/s = 42669 (42669 target) ; Learning rate = 0.000413 ; Loss = 1.545052\n",
      "2024-12-10 18:59:34.625000: I runner.py:310] Step = 45800 ; steps/s = 1.61, tokens/s = 42707 (42707 target) ; Learning rate = 0.000413 ; Loss = 1.559922\n",
      "2024-12-10 19:00:36.606000: I runner.py:310] Step = 45900 ; steps/s = 1.61, tokens/s = 42661 (42661 target) ; Learning rate = 0.000413 ; Loss = 1.547425\n",
      "2024-12-10 19:01:38.155000: I runner.py:310] Step = 46000 ; steps/s = 1.62, tokens/s = 42148 (42148 target) ; Learning rate = 0.000412 ; Loss = 1.542981\n",
      "2024-12-10 19:02:40.098000: I runner.py:310] Step = 46100 ; steps/s = 1.61, tokens/s = 42686 (42686 target) ; Learning rate = 0.000412 ; Loss = 1.548969\n",
      "2024-12-10 19:03:42.163000: I runner.py:310] Step = 46200 ; steps/s = 1.61, tokens/s = 42594 (42594 target) ; Learning rate = 0.000411 ; Loss = 1.547956\n",
      "2024-12-10 19:04:44.168000: I runner.py:310] Step = 46300 ; steps/s = 1.61, tokens/s = 42661 (42661 target) ; Learning rate = 0.000411 ; Loss = 1.556243\n",
      "2024-12-10 19:05:45.770000: I runner.py:310] Step = 46400 ; steps/s = 1.62, tokens/s = 42140 (42140 target) ; Learning rate = 0.000410 ; Loss = 1.548188\n",
      "2024-12-10 19:06:47.792000: I runner.py:310] Step = 46500 ; steps/s = 1.61, tokens/s = 42649 (42649 target) ; Learning rate = 0.000410 ; Loss = 1.550270\n",
      "2024-12-10 19:07:49.926000: I runner.py:310] Step = 46600 ; steps/s = 1.61, tokens/s = 42562 (42562 target) ; Learning rate = 0.000409 ; Loss = 1.549482\n",
      "2024-12-10 19:08:51.941000: I runner.py:310] Step = 46700 ; steps/s = 1.61, tokens/s = 42676 (42676 target) ; Learning rate = 0.000409 ; Loss = 1.543748\n",
      "2024-12-10 19:09:53.585000: I runner.py:310] Step = 46800 ; steps/s = 1.62, tokens/s = 42076 (42076 target) ; Learning rate = 0.000409 ; Loss = 1.547931\n",
      "2024-12-10 19:10:55.687000: I runner.py:310] Step = 46900 ; steps/s = 1.61, tokens/s = 42567 (42567 target) ; Learning rate = 0.000408 ; Loss = 1.535355\n",
      "2024-12-10 19:11:57.825000: I runner.py:310] Step = 47000 ; steps/s = 1.61, tokens/s = 42573 (42573 target) ; Learning rate = 0.000408 ; Loss = 1.550606\n",
      "2024-12-10 19:12:59.907000: I runner.py:310] Step = 47100 ; steps/s = 1.61, tokens/s = 42602 (42602 target) ; Learning rate = 0.000407 ; Loss = 1.540802\n",
      "2024-12-10 19:14:01.630000: I runner.py:310] Step = 47200 ; steps/s = 1.62, tokens/s = 42056 (42056 target) ; Learning rate = 0.000407 ; Loss = 1.546743\n",
      "2024-12-10 19:15:03.691000: I runner.py:310] Step = 47300 ; steps/s = 1.61, tokens/s = 42618 (42618 target) ; Learning rate = 0.000406 ; Loss = 1.536553\n",
      "2024-12-10 19:16:05.727000: I runner.py:310] Step = 47400 ; steps/s = 1.61, tokens/s = 42658 (42658 target) ; Learning rate = 0.000406 ; Loss = 1.550200\n",
      "2024-12-10 19:17:07.762000: I runner.py:310] Step = 47500 ; steps/s = 1.61, tokens/s = 42601 (42601 target) ; Learning rate = 0.000406 ; Loss = 1.543852\n",
      "2024-12-10 19:18:09.366000: I runner.py:310] Step = 47600 ; steps/s = 1.62, tokens/s = 42127 (42127 target) ; Learning rate = 0.000405 ; Loss = 1.550015\n",
      "2024-12-10 19:19:11.373000: I runner.py:310] Step = 47700 ; steps/s = 1.61, tokens/s = 42668 (42668 target) ; Learning rate = 0.000405 ; Loss = 1.541146\n",
      "2024-12-10 19:20:13.394000: I runner.py:310] Step = 47800 ; steps/s = 1.61, tokens/s = 42642 (42642 target) ; Learning rate = 0.000404 ; Loss = 1.536851\n",
      "2024-12-10 19:21:15.465000: I runner.py:310] Step = 47900 ; steps/s = 1.61, tokens/s = 42601 (42601 target) ; Learning rate = 0.000404 ; Loss = 1.545209\n",
      "2024-12-10 19:22:17.013000: I runner.py:310] Step = 48000 ; steps/s = 1.62, tokens/s = 42149 (42149 target) ; Learning rate = 0.000403 ; Loss = 1.545414\n",
      "2024-12-10 19:23:19.126000: I runner.py:310] Step = 48100 ; steps/s = 1.61, tokens/s = 42578 (42578 target) ; Learning rate = 0.000403 ; Loss = 1.534732\n",
      "2024-12-10 19:24:21.119000: I runner.py:310] Step = 48200 ; steps/s = 1.61, tokens/s = 42655 (42655 target) ; Learning rate = 0.000403 ; Loss = 1.533952\n",
      "2024-12-10 19:25:23.144000: I runner.py:310] Step = 48300 ; steps/s = 1.61, tokens/s = 42637 (42637 target) ; Learning rate = 0.000402 ; Loss = 1.550845\n",
      "2024-12-10 19:26:24.761000: I runner.py:310] Step = 48400 ; steps/s = 1.62, tokens/s = 42147 (42147 target) ; Learning rate = 0.000402 ; Loss = 1.550021\n",
      "2024-12-10 19:27:26.729000: I runner.py:310] Step = 48500 ; steps/s = 1.61, tokens/s = 42684 (42684 target) ; Learning rate = 0.000401 ; Loss = 1.536869\n",
      "2024-12-10 19:28:28.799000: I runner.py:310] Step = 48600 ; steps/s = 1.61, tokens/s = 42617 (42617 target) ; Learning rate = 0.000401 ; Loss = 1.543118\n",
      "2024-12-10 19:29:30.844000: I runner.py:310] Step = 48700 ; steps/s = 1.61, tokens/s = 42600 (42600 target) ; Learning rate = 0.000401 ; Loss = 1.547524\n",
      "2024-12-10 19:30:32.453000: I runner.py:310] Step = 48800 ; steps/s = 1.62, tokens/s = 42145 (42145 target) ; Learning rate = 0.000400 ; Loss = 1.546037\n",
      "2024-12-10 19:31:34.467000: I runner.py:310] Step = 48900 ; steps/s = 1.61, tokens/s = 42632 (42632 target) ; Learning rate = 0.000400 ; Loss = 1.543363\n",
      "2024-12-10 19:32:36.511000: I runner.py:310] Step = 49000 ; steps/s = 1.61, tokens/s = 42610 (42610 target) ; Learning rate = 0.000399 ; Loss = 1.547281\n",
      "2024-12-10 19:33:38.554000: I runner.py:310] Step = 49100 ; steps/s = 1.61, tokens/s = 42639 (42639 target) ; Learning rate = 0.000399 ; Loss = 1.545768\n",
      "2024-12-10 19:34:40.127000: I runner.py:310] Step = 49200 ; steps/s = 1.62, tokens/s = 42165 (42165 target) ; Learning rate = 0.000398 ; Loss = 1.546779\n",
      "2024-12-10 19:35:42.217000: I runner.py:310] Step = 49300 ; steps/s = 1.61, tokens/s = 42620 (42620 target) ; Learning rate = 0.000398 ; Loss = 1.541279\n",
      "2024-12-10 19:36:44.245000: I runner.py:310] Step = 49400 ; steps/s = 1.61, tokens/s = 42620 (42620 target) ; Learning rate = 0.000398 ; Loss = 1.540084\n",
      "2024-12-10 19:37:46.274000: I runner.py:310] Step = 49500 ; steps/s = 1.61, tokens/s = 42624 (42624 target) ; Learning rate = 0.000397 ; Loss = 1.543507\n",
      "2024-12-10 19:38:47.853000: I runner.py:310] Step = 49600 ; steps/s = 1.62, tokens/s = 42138 (42138 target) ; Learning rate = 0.000397 ; Loss = 1.533374\n",
      "2024-12-10 19:39:49.953000: I runner.py:310] Step = 49700 ; steps/s = 1.61, tokens/s = 42568 (42568 target) ; Learning rate = 0.000396 ; Loss = 1.541166\n",
      "2024-12-10 19:40:52.014000: I runner.py:310] Step = 49800 ; steps/s = 1.61, tokens/s = 42622 (42622 target) ; Learning rate = 0.000396 ; Loss = 1.537800\n",
      "2024-12-10 19:41:54.067000: I runner.py:310] Step = 49900 ; steps/s = 1.61, tokens/s = 42639 (42639 target) ; Learning rate = 0.000396 ; Loss = 1.538387\n",
      "2024-12-10 19:42:55.646000: I runner.py:310] Step = 50000 ; steps/s = 1.62, tokens/s = 42129 (42129 target) ; Learning rate = 0.000395 ; Loss = 1.536904\n",
      "2024-12-10 19:42:57.701000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-50000\n",
      "2024-12-10 19:42:57.701000: I training.py:192] Running evaluation for step 50000\n",
      "2024-12-10 19:47:37.436000: I training.py:192] Evaluation result for step 50000: loss = 1.101351 ; perplexity = 3.008227\n",
      "2024-12-10 19:48:39.297000: I runner.py:310] Step = 50100 ; steps/s = 1.62, tokens/s = 42752 (42752 target) ; Learning rate = 0.000395 ; Loss = 1.534391\n",
      "2024-12-10 19:49:41.361000: I runner.py:310] Step = 50200 ; steps/s = 1.61, tokens/s = 42648 (42648 target) ; Learning rate = 0.000394 ; Loss = 1.545045\n",
      "2024-12-10 19:50:43.404000: I runner.py:310] Step = 50300 ; steps/s = 1.61, tokens/s = 42595 (42595 target) ; Learning rate = 0.000394 ; Loss = 1.540327\n",
      "2024-12-10 19:51:45.035000: I runner.py:310] Step = 50400 ; steps/s = 1.62, tokens/s = 42108 (42108 target) ; Learning rate = 0.000394 ; Loss = 1.533126\n",
      "2024-12-10 19:52:47.051000: I runner.py:310] Step = 50500 ; steps/s = 1.61, tokens/s = 42648 (42648 target) ; Learning rate = 0.000393 ; Loss = 1.537583\n",
      "2024-12-10 19:53:49.118000: I runner.py:310] Step = 50600 ; steps/s = 1.61, tokens/s = 42610 (42610 target) ; Learning rate = 0.000393 ; Loss = 1.543237\n",
      "2024-12-10 19:54:51.237000: I runner.py:310] Step = 50700 ; steps/s = 1.61, tokens/s = 42581 (42581 target) ; Learning rate = 0.000393 ; Loss = 1.545925\n",
      "2024-12-10 19:55:52.891000: I runner.py:310] Step = 50800 ; steps/s = 1.62, tokens/s = 42123 (42123 target) ; Learning rate = 0.000392 ; Loss = 1.546116\n",
      "2024-12-10 19:56:54.811000: I runner.py:310] Step = 50900 ; steps/s = 1.62, tokens/s = 42691 (42691 target) ; Learning rate = 0.000392 ; Loss = 1.540385\n",
      "2024-12-10 19:57:56.734000: I runner.py:310] Step = 51000 ; steps/s = 1.62, tokens/s = 42687 (42687 target) ; Learning rate = 0.000391 ; Loss = 1.534093\n",
      "2024-12-10 19:58:58.802000: I runner.py:310] Step = 51100 ; steps/s = 1.61, tokens/s = 42618 (42618 target) ; Learning rate = 0.000391 ; Loss = 1.536644\n",
      "2024-12-10 20:00:00.357000: I runner.py:310] Step = 51200 ; steps/s = 1.62, tokens/s = 42169 (42169 target) ; Learning rate = 0.000391 ; Loss = 1.531325\n",
      "2024-12-10 20:01:02.339000: I runner.py:310] Step = 51300 ; steps/s = 1.61, tokens/s = 42664 (42664 target) ; Learning rate = 0.000390 ; Loss = 1.543660\n",
      "2024-12-10 20:02:04.352000: I runner.py:310] Step = 51400 ; steps/s = 1.61, tokens/s = 42642 (42642 target) ; Learning rate = 0.000390 ; Loss = 1.537752\n",
      "2024-12-10 20:03:06.330000: I runner.py:310] Step = 51500 ; steps/s = 1.61, tokens/s = 42684 (42684 target) ; Learning rate = 0.000389 ; Loss = 1.543032\n",
      "2024-12-10 20:04:07.801000: I runner.py:310] Step = 51600 ; steps/s = 1.63, tokens/s = 42226 (42226 target) ; Learning rate = 0.000389 ; Loss = 1.532971\n",
      "2024-12-10 20:05:09.774000: I runner.py:310] Step = 51700 ; steps/s = 1.61, tokens/s = 42689 (42689 target) ; Learning rate = 0.000389 ; Loss = 1.534288\n",
      "2024-12-10 20:06:11.862000: I runner.py:310] Step = 51800 ; steps/s = 1.61, tokens/s = 42559 (42559 target) ; Learning rate = 0.000388 ; Loss = 1.545286\n",
      "2024-12-10 20:07:13.919000: I runner.py:310] Step = 51900 ; steps/s = 1.61, tokens/s = 42633 (42633 target) ; Learning rate = 0.000388 ; Loss = 1.548999\n",
      "2024-12-10 20:08:15.446000: I runner.py:310] Step = 52000 ; steps/s = 1.63, tokens/s = 42184 (42184 target) ; Learning rate = 0.000388 ; Loss = 1.528727\n",
      "2024-12-10 20:09:17.429000: I runner.py:310] Step = 52100 ; steps/s = 1.61, tokens/s = 42660 (42660 target) ; Learning rate = 0.000387 ; Loss = 1.539256\n",
      "2024-12-10 20:10:19.494000: I runner.py:310] Step = 52200 ; steps/s = 1.61, tokens/s = 42604 (42604 target) ; Learning rate = 0.000387 ; Loss = 1.545668\n",
      "2024-12-10 20:11:21.086000: I runner.py:310] Step = 52300 ; steps/s = 1.62, tokens/s = 42201 (42201 target) ; Learning rate = 0.000386 ; Loss = 1.539226\n",
      "2024-12-10 20:12:23.018000: I runner.py:310] Step = 52400 ; steps/s = 1.61, tokens/s = 42656 (42656 target) ; Learning rate = 0.000386 ; Loss = 1.543627\n",
      "2024-12-10 20:13:25.107000: I runner.py:310] Step = 52500 ; steps/s = 1.61, tokens/s = 42625 (42625 target) ; Learning rate = 0.000386 ; Loss = 1.538825\n",
      "2024-12-10 20:14:27.177000: I runner.py:310] Step = 52600 ; steps/s = 1.61, tokens/s = 42617 (42617 target) ; Learning rate = 0.000385 ; Loss = 1.532218\n",
      "2024-12-10 20:15:28.794000: I runner.py:310] Step = 52700 ; steps/s = 1.62, tokens/s = 42053 (42053 target) ; Learning rate = 0.000385 ; Loss = 1.543057\n",
      "2024-12-10 20:16:30.865000: I runner.py:310] Step = 52800 ; steps/s = 1.61, tokens/s = 42585 (42585 target) ; Learning rate = 0.000385 ; Loss = 1.532250\n",
      "2024-12-10 20:17:32.831000: I runner.py:310] Step = 52900 ; steps/s = 1.61, tokens/s = 42726 (42726 target) ; Learning rate = 0.000384 ; Loss = 1.544794\n",
      "2024-12-10 20:18:34.911000: I runner.py:310] Step = 53000 ; steps/s = 1.61, tokens/s = 42624 (42624 target) ; Learning rate = 0.000384 ; Loss = 1.532898\n",
      "2024-12-10 20:19:36.514000: I runner.py:310] Step = 53100 ; steps/s = 1.62, tokens/s = 42099 (42099 target) ; Learning rate = 0.000384 ; Loss = 1.532113\n",
      "2024-12-10 20:20:38.595000: I runner.py:310] Step = 53200 ; steps/s = 1.61, tokens/s = 42599 (42599 target) ; Learning rate = 0.000383 ; Loss = 1.533854\n",
      "2024-12-10 20:21:40.682000: I runner.py:310] Step = 53300 ; steps/s = 1.61, tokens/s = 42585 (42585 target) ; Learning rate = 0.000383 ; Loss = 1.525483\n",
      "2024-12-10 20:22:42.780000: I runner.py:310] Step = 53400 ; steps/s = 1.61, tokens/s = 42586 (42586 target) ; Learning rate = 0.000382 ; Loss = 1.535852\n",
      "2024-12-10 20:23:44.361000: I runner.py:310] Step = 53500 ; steps/s = 1.62, tokens/s = 42161 (42161 target) ; Learning rate = 0.000382 ; Loss = 1.530693\n",
      "2024-12-10 20:24:46.389000: I runner.py:310] Step = 53600 ; steps/s = 1.61, tokens/s = 42681 (42681 target) ; Learning rate = 0.000382 ; Loss = 1.532423\n",
      "2024-12-10 20:25:48.448000: I runner.py:310] Step = 53700 ; steps/s = 1.61, tokens/s = 42608 (42608 target) ; Learning rate = 0.000381 ; Loss = 1.531291\n",
      "2024-12-10 20:26:50.475000: I runner.py:310] Step = 53800 ; steps/s = 1.61, tokens/s = 42616 (42616 target) ; Learning rate = 0.000381 ; Loss = 1.530644\n",
      "2024-12-10 20:27:52.109000: I runner.py:310] Step = 53900 ; steps/s = 1.62, tokens/s = 42091 (42091 target) ; Learning rate = 0.000381 ; Loss = 1.529824\n",
      "2024-12-10 20:28:54.115000: I runner.py:310] Step = 54000 ; steps/s = 1.61, tokens/s = 42665 (42665 target) ; Learning rate = 0.000380 ; Loss = 1.528156\n",
      "2024-12-10 20:29:56.137000: I runner.py:310] Step = 54100 ; steps/s = 1.61, tokens/s = 42632 (42632 target) ; Learning rate = 0.000380 ; Loss = 1.537800\n",
      "2024-12-10 20:30:58.214000: I runner.py:310] Step = 54200 ; steps/s = 1.61, tokens/s = 42591 (42591 target) ; Learning rate = 0.000380 ; Loss = 1.539689\n",
      "2024-12-10 20:31:59.744000: I runner.py:310] Step = 54300 ; steps/s = 1.63, tokens/s = 42212 (42212 target) ; Learning rate = 0.000379 ; Loss = 1.530759\n",
      "2024-12-10 20:33:01.804000: I runner.py:310] Step = 54400 ; steps/s = 1.61, tokens/s = 42590 (42590 target) ; Learning rate = 0.000379 ; Loss = 1.539301\n",
      "2024-12-10 20:34:03.789000: I runner.py:310] Step = 54500 ; steps/s = 1.61, tokens/s = 42661 (42661 target) ; Learning rate = 0.000379 ; Loss = 1.531787\n",
      "2024-12-10 20:35:05.823000: I runner.py:310] Step = 54600 ; steps/s = 1.61, tokens/s = 42623 (42623 target) ; Learning rate = 0.000378 ; Loss = 1.529062\n",
      "2024-12-10 20:36:07.393000: I runner.py:310] Step = 54700 ; steps/s = 1.62, tokens/s = 42162 (42162 target) ; Learning rate = 0.000378 ; Loss = 1.537433\n",
      "2024-12-10 20:37:09.363000: I runner.py:310] Step = 54800 ; steps/s = 1.61, tokens/s = 42648 (42648 target) ; Learning rate = 0.000378 ; Loss = 1.540144\n",
      "2024-12-10 20:38:11.372000: I runner.py:310] Step = 54900 ; steps/s = 1.61, tokens/s = 42685 (42685 target) ; Learning rate = 0.000377 ; Loss = 1.535455\n",
      "2024-12-10 20:39:13.347000: I runner.py:310] Step = 55000 ; steps/s = 1.61, tokens/s = 42678 (42678 target) ; Learning rate = 0.000377 ; Loss = 1.532425\n",
      "2024-12-10 20:39:13.348000: I training.py:192] Running evaluation for step 55000\n",
      "2024-12-10 20:43:51.832000: I training.py:192] Evaluation result for step 55000: loss = 1.108906 ; perplexity = 3.031039\n",
      "2024-12-10 20:44:53.320000: I runner.py:310] Step = 55100 ; steps/s = 1.63, tokens/s = 42223 (42223 target) ; Learning rate = 0.000377 ; Loss = 1.532835\n",
      "2024-12-10 20:45:55.323000: I runner.py:310] Step = 55200 ; steps/s = 1.61, tokens/s = 42650 (42650 target) ; Learning rate = 0.000376 ; Loss = 1.530889\n",
      "2024-12-10 20:46:57.253000: I runner.py:310] Step = 55300 ; steps/s = 1.61, tokens/s = 42694 (42694 target) ; Learning rate = 0.000376 ; Loss = 1.532993\n",
      "2024-12-10 20:47:59.381000: I runner.py:310] Step = 55400 ; steps/s = 1.61, tokens/s = 42578 (42578 target) ; Learning rate = 0.000376 ; Loss = 1.531774\n",
      "2024-12-10 20:49:00.952000: I runner.py:310] Step = 55500 ; steps/s = 1.62, tokens/s = 42172 (42172 target) ; Learning rate = 0.000375 ; Loss = 1.530985\n",
      "2024-12-10 20:50:02.889000: I runner.py:310] Step = 55600 ; steps/s = 1.61, tokens/s = 42661 (42661 target) ; Learning rate = 0.000375 ; Loss = 1.527767\n",
      "2024-12-10 20:51:04.815000: I runner.py:310] Step = 55700 ; steps/s = 1.61, tokens/s = 42730 (42730 target) ; Learning rate = 0.000375 ; Loss = 1.541432\n",
      "2024-12-10 20:52:06.773000: I runner.py:310] Step = 55800 ; steps/s = 1.61, tokens/s = 42693 (42693 target) ; Learning rate = 0.000374 ; Loss = 1.535152\n",
      "2024-12-10 20:53:08.270000: I runner.py:310] Step = 55900 ; steps/s = 1.63, tokens/s = 42198 (42198 target) ; Learning rate = 0.000374 ; Loss = 1.532185\n",
      "2024-12-10 20:54:10.252000: I runner.py:310] Step = 56000 ; steps/s = 1.61, tokens/s = 42679 (42679 target) ; Learning rate = 0.000374 ; Loss = 1.535624\n",
      "2024-12-10 20:55:12.215000: I runner.py:310] Step = 56100 ; steps/s = 1.61, tokens/s = 42661 (42661 target) ; Learning rate = 0.000373 ; Loss = 1.522792\n",
      "2024-12-10 20:56:14.259000: I runner.py:310] Step = 56200 ; steps/s = 1.61, tokens/s = 42631 (42631 target) ; Learning rate = 0.000373 ; Loss = 1.539319\n",
      "2024-12-10 20:57:15.838000: I runner.py:310] Step = 56300 ; steps/s = 1.62, tokens/s = 42143 (42143 target) ; Learning rate = 0.000373 ; Loss = 1.530991\n",
      "2024-12-10 20:58:17.818000: I runner.py:310] Step = 56400 ; steps/s = 1.61, tokens/s = 42692 (42692 target) ; Learning rate = 0.000372 ; Loss = 1.522819\n",
      "2024-12-10 20:59:19.827000: I runner.py:310] Step = 56500 ; steps/s = 1.61, tokens/s = 42618 (42618 target) ; Learning rate = 0.000372 ; Loss = 1.533919\n",
      "2024-12-10 21:00:21.884000: I runner.py:310] Step = 56600 ; steps/s = 1.61, tokens/s = 42622 (42622 target) ; Learning rate = 0.000372 ; Loss = 1.528859\n",
      "2024-12-10 21:01:23.537000: I runner.py:310] Step = 56700 ; steps/s = 1.62, tokens/s = 42111 (42111 target) ; Learning rate = 0.000371 ; Loss = 1.535067\n",
      "2024-12-10 21:02:25.511000: I runner.py:310] Step = 56800 ; steps/s = 1.61, tokens/s = 42658 (42658 target) ; Learning rate = 0.000371 ; Loss = 1.531569\n",
      "2024-12-10 21:03:27.523000: I runner.py:310] Step = 56900 ; steps/s = 1.61, tokens/s = 42636 (42636 target) ; Learning rate = 0.000371 ; Loss = 1.535152\n",
      "2024-12-10 21:04:29.450000: I runner.py:310] Step = 57000 ; steps/s = 1.61, tokens/s = 42722 (42722 target) ; Learning rate = 0.000370 ; Loss = 1.533950\n",
      "2024-12-10 21:05:30.911000: I runner.py:310] Step = 57100 ; steps/s = 1.63, tokens/s = 42236 (42236 target) ; Learning rate = 0.000370 ; Loss = 1.526496\n",
      "2024-12-10 21:06:32.961000: I runner.py:310] Step = 57200 ; steps/s = 1.61, tokens/s = 42608 (42608 target) ; Learning rate = 0.000370 ; Loss = 1.534663\n",
      "2024-12-10 21:07:35.060000: I runner.py:310] Step = 57300 ; steps/s = 1.61, tokens/s = 42597 (42597 target) ; Learning rate = 0.000369 ; Loss = 1.526286\n",
      "2024-12-10 21:08:37.128000: I runner.py:310] Step = 57400 ; steps/s = 1.61, tokens/s = 42611 (42611 target) ; Learning rate = 0.000369 ; Loss = 1.534181\n",
      "2024-12-10 21:09:38.734000: I runner.py:310] Step = 57500 ; steps/s = 1.62, tokens/s = 42118 (42118 target) ; Learning rate = 0.000369 ; Loss = 1.521394\n",
      "2024-12-10 21:10:40.687000: I runner.py:310] Step = 57600 ; steps/s = 1.61, tokens/s = 42711 (42711 target) ; Learning rate = 0.000368 ; Loss = 1.531972\n",
      "2024-12-10 21:11:42.696000: I runner.py:310] Step = 57700 ; steps/s = 1.61, tokens/s = 42655 (42655 target) ; Learning rate = 0.000368 ; Loss = 1.527913\n",
      "2024-12-10 21:12:44.708000: I runner.py:310] Step = 57800 ; steps/s = 1.61, tokens/s = 42624 (42624 target) ; Learning rate = 0.000368 ; Loss = 1.540402\n",
      "2024-12-10 21:13:46.279000: I runner.py:310] Step = 57900 ; steps/s = 1.62, tokens/s = 42170 (42170 target) ; Learning rate = 0.000367 ; Loss = 1.541481\n",
      "2024-12-10 21:14:48.310000: I runner.py:310] Step = 58000 ; steps/s = 1.61, tokens/s = 42661 (42661 target) ; Learning rate = 0.000367 ; Loss = 1.522902\n",
      "2024-12-10 21:15:50.364000: I runner.py:310] Step = 58100 ; steps/s = 1.61, tokens/s = 42599 (42599 target) ; Learning rate = 0.000367 ; Loss = 1.528270\n",
      "2024-12-10 21:16:52.433000: I runner.py:310] Step = 58200 ; steps/s = 1.61, tokens/s = 42594 (42594 target) ; Learning rate = 0.000366 ; Loss = 1.532186\n",
      "2024-12-10 21:17:54.061000: I runner.py:310] Step = 58300 ; steps/s = 1.62, tokens/s = 42111 (42111 target) ; Learning rate = 0.000366 ; Loss = 1.533441\n",
      "2024-12-10 21:18:56.054000: I runner.py:310] Step = 58400 ; steps/s = 1.61, tokens/s = 42621 (42621 target) ; Learning rate = 0.000366 ; Loss = 1.530899\n",
      "2024-12-10 21:19:58.050000: I runner.py:310] Step = 58500 ; steps/s = 1.61, tokens/s = 42693 (42693 target) ; Learning rate = 0.000365 ; Loss = 1.524140\n",
      "2024-12-10 21:21:00.025000: I runner.py:310] Step = 58600 ; steps/s = 1.61, tokens/s = 42668 (42668 target) ; Learning rate = 0.000365 ; Loss = 1.528374\n",
      "2024-12-10 21:22:01.602000: I runner.py:310] Step = 58700 ; steps/s = 1.62, tokens/s = 42159 (42159 target) ; Learning rate = 0.000365 ; Loss = 1.528029\n",
      "2024-12-10 21:23:03.586000: I runner.py:310] Step = 58800 ; steps/s = 1.61, tokens/s = 42653 (42653 target) ; Learning rate = 0.000365 ; Loss = 1.523758\n",
      "2024-12-10 21:24:05.655000: I runner.py:310] Step = 58900 ; steps/s = 1.61, tokens/s = 42599 (42599 target) ; Learning rate = 0.000364 ; Loss = 1.532294\n",
      "2024-12-10 21:25:07.655000: I runner.py:310] Step = 59000 ; steps/s = 1.61, tokens/s = 42677 (42677 target) ; Learning rate = 0.000364 ; Loss = 1.530735\n",
      "2024-12-10 21:26:09.165000: I runner.py:310] Step = 59100 ; steps/s = 1.63, tokens/s = 42189 (42189 target) ; Learning rate = 0.000364 ; Loss = 1.535655\n",
      "2024-12-10 21:27:11.201000: I runner.py:310] Step = 59200 ; steps/s = 1.61, tokens/s = 42642 (42642 target) ; Learning rate = 0.000363 ; Loss = 1.523136\n",
      "2024-12-10 21:28:13.333000: I runner.py:310] Step = 59300 ; steps/s = 1.61, tokens/s = 42563 (42563 target) ; Learning rate = 0.000363 ; Loss = 1.528777\n",
      "2024-12-10 21:29:15.341000: I runner.py:310] Step = 59400 ; steps/s = 1.61, tokens/s = 42650 (42650 target) ; Learning rate = 0.000363 ; Loss = 1.527049\n",
      "2024-12-10 21:30:16.943000: I runner.py:310] Step = 59500 ; steps/s = 1.62, tokens/s = 42124 (42124 target) ; Learning rate = 0.000362 ; Loss = 1.518358\n",
      "2024-12-10 21:31:18.992000: I runner.py:310] Step = 59600 ; steps/s = 1.61, tokens/s = 42652 (42652 target) ; Learning rate = 0.000362 ; Loss = 1.533845\n",
      "2024-12-10 21:32:21.087000: I runner.py:310] Step = 59700 ; steps/s = 1.61, tokens/s = 42557 (42557 target) ; Learning rate = 0.000362 ; Loss = 1.531484\n",
      "2024-12-10 21:33:23.060000: I runner.py:310] Step = 59800 ; steps/s = 1.61, tokens/s = 42686 (42686 target) ; Learning rate = 0.000361 ; Loss = 1.528624\n",
      "2024-12-10 21:34:24.648000: I runner.py:310] Step = 59900 ; steps/s = 1.62, tokens/s = 42174 (42174 target) ; Learning rate = 0.000361 ; Loss = 1.519966\n",
      "2024-12-10 21:35:26.660000: I runner.py:310] Step = 60000 ; steps/s = 1.61, tokens/s = 42628 (42628 target) ; Learning rate = 0.000361 ; Loss = 1.525353\n",
      "2024-12-10 21:35:28.782000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-60000\n",
      "2024-12-10 21:35:28.782000: I training.py:192] Running evaluation for step 60000\n",
      "2024-12-10 21:40:08.899000: I training.py:192] Evaluation result for step 60000: loss = 1.115748 ; perplexity = 3.051851\n",
      "2024-12-10 21:41:10.788000: I runner.py:310] Step = 60100 ; steps/s = 1.62, tokens/s = 42749 (42749 target) ; Learning rate = 0.000361 ; Loss = 1.523704\n",
      "2024-12-10 21:42:12.746000: I runner.py:310] Step = 60200 ; steps/s = 1.61, tokens/s = 42684 (42684 target) ; Learning rate = 0.000360 ; Loss = 1.531606\n",
      "2024-12-10 21:43:14.336000: I runner.py:310] Step = 60300 ; steps/s = 1.62, tokens/s = 42104 (42104 target) ; Learning rate = 0.000360 ; Loss = 1.542895\n",
      "2024-12-10 21:44:16.376000: I runner.py:310] Step = 60400 ; steps/s = 1.61, tokens/s = 42657 (42657 target) ; Learning rate = 0.000360 ; Loss = 1.526314\n",
      "2024-12-10 21:45:18.450000: I runner.py:310] Step = 60500 ; steps/s = 1.61, tokens/s = 42593 (42593 target) ; Learning rate = 0.000359 ; Loss = 1.526254\n",
      "2024-12-10 21:46:20.480000: I runner.py:310] Step = 60600 ; steps/s = 1.61, tokens/s = 42633 (42633 target) ; Learning rate = 0.000359 ; Loss = 1.524823\n",
      "2024-12-10 21:47:22.099000: I runner.py:310] Step = 60700 ; steps/s = 1.62, tokens/s = 42141 (42141 target) ; Learning rate = 0.000359 ; Loss = 1.531853\n",
      "2024-12-10 21:48:24.217000: I runner.py:310] Step = 60800 ; steps/s = 1.61, tokens/s = 42574 (42574 target) ; Learning rate = 0.000358 ; Loss = 1.523051\n",
      "2024-12-10 21:49:26.292000: I runner.py:310] Step = 60900 ; steps/s = 1.61, tokens/s = 42584 (42584 target) ; Learning rate = 0.000358 ; Loss = 1.522849\n",
      "2024-12-10 21:50:28.303000: I runner.py:310] Step = 61000 ; steps/s = 1.61, tokens/s = 42647 (42647 target) ; Learning rate = 0.000358 ; Loss = 1.528149\n",
      "2024-12-10 21:51:29.878000: I runner.py:310] Step = 61100 ; steps/s = 1.62, tokens/s = 42133 (42133 target) ; Learning rate = 0.000358 ; Loss = 1.536832\n",
      "2024-12-10 21:52:31.971000: I runner.py:310] Step = 61200 ; steps/s = 1.61, tokens/s = 42594 (42594 target) ; Learning rate = 0.000357 ; Loss = 1.527328\n",
      "2024-12-10 21:53:34.081000: I runner.py:310] Step = 61300 ; steps/s = 1.61, tokens/s = 42606 (42606 target) ; Learning rate = 0.000357 ; Loss = 1.527453\n",
      "2024-12-10 21:54:36.130000: I runner.py:310] Step = 61400 ; steps/s = 1.61, tokens/s = 42611 (42611 target) ; Learning rate = 0.000357 ; Loss = 1.523981\n",
      "2024-12-10 21:55:37.720000: I runner.py:310] Step = 61500 ; steps/s = 1.62, tokens/s = 42126 (42126 target) ; Learning rate = 0.000356 ; Loss = 1.532495\n",
      "2024-12-10 21:56:39.689000: I runner.py:310] Step = 61600 ; steps/s = 1.61, tokens/s = 42692 (42692 target) ; Learning rate = 0.000356 ; Loss = 1.520828\n",
      "2024-12-10 21:57:41.707000: I runner.py:310] Step = 61700 ; steps/s = 1.61, tokens/s = 42644 (42644 target) ; Learning rate = 0.000356 ; Loss = 1.519499\n",
      "2024-12-10 21:58:43.795000: I runner.py:310] Step = 61800 ; steps/s = 1.61, tokens/s = 42596 (42596 target) ; Learning rate = 0.000356 ; Loss = 1.526769\n",
      "2024-12-10 21:59:45.398000: I runner.py:310] Step = 61900 ; steps/s = 1.62, tokens/s = 42135 (42135 target) ; Learning rate = 0.000355 ; Loss = 1.517110\n",
      "2024-12-10 22:00:47.493000: I runner.py:310] Step = 62000 ; steps/s = 1.61, tokens/s = 42600 (42600 target) ; Learning rate = 0.000355 ; Loss = 1.523627\n",
      "2024-12-10 22:01:49.559000: I runner.py:310] Step = 62100 ; steps/s = 1.61, tokens/s = 42608 (42608 target) ; Learning rate = 0.000355 ; Loss = 1.530183\n",
      "2024-12-10 22:02:51.597000: I runner.py:310] Step = 62200 ; steps/s = 1.61, tokens/s = 42633 (42633 target) ; Learning rate = 0.000354 ; Loss = 1.529764\n",
      "2024-12-10 22:03:53.199000: I runner.py:310] Step = 62300 ; steps/s = 1.62, tokens/s = 42126 (42126 target) ; Learning rate = 0.000354 ; Loss = 1.526097\n",
      "2024-12-10 22:04:55.251000: I runner.py:310] Step = 62400 ; steps/s = 1.61, tokens/s = 42633 (42633 target) ; Learning rate = 0.000354 ; Loss = 1.521501\n",
      "2024-12-10 22:05:57.321000: I runner.py:310] Step = 62500 ; steps/s = 1.61, tokens/s = 42601 (42601 target) ; Learning rate = 0.000354 ; Loss = 1.523334\n",
      "2024-12-10 22:06:59.015000: I runner.py:310] Step = 62600 ; steps/s = 1.62, tokens/s = 42410 (42410 target) ; Learning rate = 0.000353 ; Loss = 1.521610\n",
      "2024-12-10 22:08:00.903000: I runner.py:310] Step = 62700 ; steps/s = 1.62, tokens/s = 42406 (42406 target) ; Learning rate = 0.000353 ; Loss = 1.523851\n",
      "2024-12-10 22:09:02.929000: I runner.py:310] Step = 62800 ; steps/s = 1.61, tokens/s = 42664 (42664 target) ; Learning rate = 0.000353 ; Loss = 1.523968\n",
      "2024-12-10 22:10:04.968000: I runner.py:310] Step = 62900 ; steps/s = 1.61, tokens/s = 42634 (42634 target) ; Learning rate = 0.000352 ; Loss = 1.530603\n",
      "2024-12-10 22:11:06.463000: I runner.py:310] Step = 63000 ; steps/s = 1.63, tokens/s = 42139 (42139 target) ; Learning rate = 0.000352 ; Loss = 1.521576\n",
      "2024-12-10 22:12:08.509000: I runner.py:310] Step = 63100 ; steps/s = 1.61, tokens/s = 42632 (42632 target) ; Learning rate = 0.000352 ; Loss = 1.520592\n",
      "2024-12-10 22:13:10.460000: I runner.py:310] Step = 63200 ; steps/s = 1.61, tokens/s = 42698 (42698 target) ; Learning rate = 0.000352 ; Loss = 1.519762\n",
      "2024-12-10 22:14:12.485000: I runner.py:310] Step = 63300 ; steps/s = 1.61, tokens/s = 42636 (42636 target) ; Learning rate = 0.000351 ; Loss = 1.527971\n",
      "2024-12-10 22:15:14.040000: I runner.py:310] Step = 63400 ; steps/s = 1.62, tokens/s = 42166 (42166 target) ; Learning rate = 0.000351 ; Loss = 1.522530\n",
      "2024-12-10 22:16:16.133000: I runner.py:310] Step = 63500 ; steps/s = 1.61, tokens/s = 42612 (42612 target) ; Learning rate = 0.000351 ; Loss = 1.521677\n",
      "2024-12-10 22:17:18.164000: I runner.py:310] Step = 63600 ; steps/s = 1.61, tokens/s = 42605 (42605 target) ; Learning rate = 0.000350 ; Loss = 1.526141\n",
      "2024-12-10 22:18:20.153000: I runner.py:310] Step = 63700 ; steps/s = 1.61, tokens/s = 42665 (42665 target) ; Learning rate = 0.000350 ; Loss = 1.523565\n",
      "2024-12-10 22:19:21.768000: I runner.py:310] Step = 63800 ; steps/s = 1.62, tokens/s = 42134 (42134 target) ; Learning rate = 0.000350 ; Loss = 1.522856\n",
      "2024-12-10 22:20:23.808000: I runner.py:310] Step = 63900 ; steps/s = 1.61, tokens/s = 42657 (42657 target) ; Learning rate = 0.000350 ; Loss = 1.523081\n",
      "2024-12-10 22:21:25.853000: I runner.py:310] Step = 64000 ; steps/s = 1.61, tokens/s = 42582 (42582 target) ; Learning rate = 0.000349 ; Loss = 1.528263\n",
      "2024-12-10 22:22:27.822000: I runner.py:310] Step = 64100 ; steps/s = 1.61, tokens/s = 42665 (42665 target) ; Learning rate = 0.000349 ; Loss = 1.525017\n",
      "2024-12-10 22:23:29.444000: I runner.py:310] Step = 64200 ; steps/s = 1.62, tokens/s = 42138 (42138 target) ; Learning rate = 0.000349 ; Loss = 1.516243\n",
      "2024-12-10 22:24:31.404000: I runner.py:310] Step = 64300 ; steps/s = 1.61, tokens/s = 42677 (42677 target) ; Learning rate = 0.000349 ; Loss = 1.513976\n",
      "2024-12-10 22:25:33.440000: I runner.py:310] Step = 64400 ; steps/s = 1.61, tokens/s = 42628 (42628 target) ; Learning rate = 0.000348 ; Loss = 1.527077\n",
      "2024-12-10 22:26:35.416000: I runner.py:310] Step = 64500 ; steps/s = 1.61, tokens/s = 42675 (42675 target) ; Learning rate = 0.000348 ; Loss = 1.527767\n",
      "2024-12-10 22:27:37.063000: I runner.py:310] Step = 64600 ; steps/s = 1.62, tokens/s = 42095 (42095 target) ; Learning rate = 0.000348 ; Loss = 1.528042\n",
      "2024-12-10 22:28:39.111000: I runner.py:310] Step = 64700 ; steps/s = 1.61, tokens/s = 42646 (42646 target) ; Learning rate = 0.000347 ; Loss = 1.523304\n",
      "2024-12-10 22:29:41.134000: I runner.py:310] Step = 64800 ; steps/s = 1.61, tokens/s = 42649 (42649 target) ; Learning rate = 0.000347 ; Loss = 1.520348\n",
      "2024-12-10 22:30:43.101000: I runner.py:310] Step = 64900 ; steps/s = 1.61, tokens/s = 42636 (42636 target) ; Learning rate = 0.000347 ; Loss = 1.522633\n",
      "2024-12-10 22:31:44.679000: I runner.py:310] Step = 65000 ; steps/s = 1.62, tokens/s = 42177 (42177 target) ; Learning rate = 0.000347 ; Loss = 1.526190\n",
      "2024-12-10 22:31:44.681000: I training.py:192] Running evaluation for step 65000\n",
      "2024-12-10 22:36:26.745000: I training.py:192] Evaluation result for step 65000: loss = 1.121873 ; perplexity = 3.070602\n",
      "2024-12-10 22:37:28.554000: I runner.py:310] Step = 65100 ; steps/s = 1.62, tokens/s = 42794 (42794 target) ; Learning rate = 0.000346 ; Loss = 1.521908\n",
      "2024-12-10 22:38:30.576000: I runner.py:310] Step = 65200 ; steps/s = 1.61, tokens/s = 42667 (42667 target) ; Learning rate = 0.000346 ; Loss = 1.522365\n",
      "2024-12-10 22:39:32.713000: I runner.py:310] Step = 65300 ; steps/s = 1.61, tokens/s = 42572 (42572 target) ; Learning rate = 0.000346 ; Loss = 1.520815\n",
      "2024-12-10 22:40:34.286000: I runner.py:310] Step = 65400 ; steps/s = 1.62, tokens/s = 42114 (42114 target) ; Learning rate = 0.000346 ; Loss = 1.520991\n",
      "2024-12-10 22:41:36.348000: I runner.py:310] Step = 65500 ; steps/s = 1.61, tokens/s = 42588 (42588 target) ; Learning rate = 0.000345 ; Loss = 1.515978\n",
      "2024-12-10 22:42:38.359000: I runner.py:310] Step = 65600 ; steps/s = 1.61, tokens/s = 42638 (42638 target) ; Learning rate = 0.000345 ; Loss = 1.524134\n",
      "2024-12-10 22:43:40.472000: I runner.py:310] Step = 65700 ; steps/s = 1.61, tokens/s = 42605 (42605 target) ; Learning rate = 0.000345 ; Loss = 1.513202\n",
      "2024-12-10 22:44:42.084000: I runner.py:310] Step = 65800 ; steps/s = 1.62, tokens/s = 42145 (42145 target) ; Learning rate = 0.000345 ; Loss = 1.515378\n",
      "2024-12-10 22:45:44.027000: I runner.py:310] Step = 65900 ; steps/s = 1.61, tokens/s = 42727 (42727 target) ; Learning rate = 0.000344 ; Loss = 1.523707\n",
      "2024-12-10 22:46:46.102000: I runner.py:310] Step = 66000 ; steps/s = 1.61, tokens/s = 42586 (42586 target) ; Learning rate = 0.000344 ; Loss = 1.522956\n",
      "2024-12-10 22:47:48.167000: I runner.py:310] Step = 66100 ; steps/s = 1.61, tokens/s = 42637 (42637 target) ; Learning rate = 0.000344 ; Loss = 1.520044\n",
      "2024-12-10 22:48:49.804000: I runner.py:310] Step = 66200 ; steps/s = 1.62, tokens/s = 42052 (42052 target) ; Learning rate = 0.000344 ; Loss = 1.533545\n",
      "2024-12-10 22:49:51.887000: I runner.py:310] Step = 66300 ; steps/s = 1.61, tokens/s = 42569 (42569 target) ; Learning rate = 0.000343 ; Loss = 1.518270\n",
      "2024-12-10 22:50:53.944000: I runner.py:310] Step = 66400 ; steps/s = 1.61, tokens/s = 42641 (42641 target) ; Learning rate = 0.000343 ; Loss = 1.518780\n",
      "2024-12-10 22:51:56.009000: I runner.py:310] Step = 66500 ; steps/s = 1.61, tokens/s = 42611 (42611 target) ; Learning rate = 0.000343 ; Loss = 1.521628\n",
      "2024-12-10 22:52:57.575000: I runner.py:310] Step = 66600 ; steps/s = 1.62, tokens/s = 42188 (42188 target) ; Learning rate = 0.000342 ; Loss = 1.519328\n",
      "2024-12-10 22:53:59.662000: I runner.py:310] Step = 66700 ; steps/s = 1.61, tokens/s = 42596 (42596 target) ; Learning rate = 0.000342 ; Loss = 1.519545\n",
      "2024-12-10 22:55:01.672000: I runner.py:310] Step = 66800 ; steps/s = 1.61, tokens/s = 42626 (42626 target) ; Learning rate = 0.000342 ; Loss = 1.525685\n",
      "2024-12-10 22:56:03.717000: I runner.py:310] Step = 66900 ; steps/s = 1.61, tokens/s = 42643 (42643 target) ; Learning rate = 0.000342 ; Loss = 1.517430\n",
      "2024-12-10 22:57:05.246000: I runner.py:310] Step = 67000 ; steps/s = 1.63, tokens/s = 42178 (42178 target) ; Learning rate = 0.000341 ; Loss = 1.534124\n",
      "2024-12-10 22:58:07.342000: I runner.py:310] Step = 67100 ; steps/s = 1.61, tokens/s = 42558 (42558 target) ; Learning rate = 0.000341 ; Loss = 1.515869\n",
      "2024-12-10 22:59:09.356000: I runner.py:310] Step = 67200 ; steps/s = 1.61, tokens/s = 42661 (42661 target) ; Learning rate = 0.000341 ; Loss = 1.521628\n",
      "2024-12-10 23:00:11.378000: I runner.py:310] Step = 67300 ; steps/s = 1.61, tokens/s = 42649 (42649 target) ; Learning rate = 0.000341 ; Loss = 1.517353\n",
      "2024-12-10 23:01:12.980000: I runner.py:310] Step = 67400 ; steps/s = 1.62, tokens/s = 42156 (42156 target) ; Learning rate = 0.000340 ; Loss = 1.510558\n",
      "2024-12-10 23:02:14.977000: I runner.py:310] Step = 67500 ; steps/s = 1.61, tokens/s = 42664 (42664 target) ; Learning rate = 0.000340 ; Loss = 1.516135\n",
      "2024-12-10 23:03:16.974000: I runner.py:310] Step = 67600 ; steps/s = 1.61, tokens/s = 42656 (42656 target) ; Learning rate = 0.000340 ; Loss = 1.522764\n",
      "2024-12-10 23:04:19.026000: I runner.py:310] Step = 67700 ; steps/s = 1.61, tokens/s = 42604 (42604 target) ; Learning rate = 0.000340 ; Loss = 1.530976\n",
      "2024-12-10 23:05:20.634000: I runner.py:310] Step = 67800 ; steps/s = 1.62, tokens/s = 42123 (42123 target) ; Learning rate = 0.000339 ; Loss = 1.521100\n",
      "2024-12-10 23:06:22.669000: I runner.py:310] Step = 67900 ; steps/s = 1.61, tokens/s = 42633 (42633 target) ; Learning rate = 0.000339 ; Loss = 1.521258\n",
      "2024-12-10 23:07:24.700000: I runner.py:310] Step = 68000 ; steps/s = 1.61, tokens/s = 42634 (42634 target) ; Learning rate = 0.000339 ; Loss = 1.520206\n",
      "2024-12-10 23:08:26.804000: I runner.py:310] Step = 68100 ; steps/s = 1.61, tokens/s = 42602 (42602 target) ; Learning rate = 0.000339 ; Loss = 1.527898\n",
      "2024-12-10 23:09:28.405000: I runner.py:310] Step = 68200 ; steps/s = 1.62, tokens/s = 42116 (42116 target) ; Learning rate = 0.000338 ; Loss = 1.521927\n",
      "2024-12-10 23:10:30.377000: I runner.py:310] Step = 68300 ; steps/s = 1.61, tokens/s = 42684 (42684 target) ; Learning rate = 0.000338 ; Loss = 1.522322\n",
      "2024-12-10 23:11:32.403000: I runner.py:310] Step = 68400 ; steps/s = 1.61, tokens/s = 42636 (42636 target) ; Learning rate = 0.000338 ; Loss = 1.523232\n",
      "2024-12-10 23:12:34.447000: I runner.py:310] Step = 68500 ; steps/s = 1.61, tokens/s = 42632 (42632 target) ; Learning rate = 0.000338 ; Loss = 1.521334\n",
      "2024-12-10 23:13:36.007000: I runner.py:310] Step = 68600 ; steps/s = 1.62, tokens/s = 42158 (42158 target) ; Learning rate = 0.000337 ; Loss = 1.519825\n",
      "2024-12-10 23:14:37.980000: I runner.py:310] Step = 68700 ; steps/s = 1.61, tokens/s = 42672 (42672 target) ; Learning rate = 0.000337 ; Loss = 1.522934\n",
      "2024-12-10 23:15:40.046000: I runner.py:310] Step = 68800 ; steps/s = 1.61, tokens/s = 42598 (42598 target) ; Learning rate = 0.000337 ; Loss = 1.523359\n",
      "2024-12-10 23:16:42.039000: I runner.py:310] Step = 68900 ; steps/s = 1.61, tokens/s = 42658 (42658 target) ; Learning rate = 0.000337 ; Loss = 1.521876\n",
      "2024-12-10 23:17:43.658000: I runner.py:310] Step = 69000 ; steps/s = 1.62, tokens/s = 42134 (42134 target) ; Learning rate = 0.000336 ; Loss = 1.519410\n",
      "2024-12-10 23:18:45.733000: I runner.py:310] Step = 69100 ; steps/s = 1.61, tokens/s = 42617 (42617 target) ; Learning rate = 0.000336 ; Loss = 1.525776\n",
      "2024-12-10 23:19:47.862000: I runner.py:310] Step = 69200 ; steps/s = 1.61, tokens/s = 42550 (42550 target) ; Learning rate = 0.000336 ; Loss = 1.521485\n",
      "2024-12-10 23:20:49.906000: I runner.py:310] Step = 69300 ; steps/s = 1.61, tokens/s = 42642 (42642 target) ; Learning rate = 0.000336 ; Loss = 1.525771\n",
      "2024-12-10 23:21:51.572000: I runner.py:310] Step = 69400 ; steps/s = 1.62, tokens/s = 42067 (42067 target) ; Learning rate = 0.000336 ; Loss = 1.526091\n",
      "2024-12-10 23:22:53.556000: I runner.py:310] Step = 69500 ; steps/s = 1.61, tokens/s = 42679 (42679 target) ; Learning rate = 0.000335 ; Loss = 1.514859\n",
      "2024-12-10 23:23:55.637000: I runner.py:310] Step = 69600 ; steps/s = 1.61, tokens/s = 42593 (42593 target) ; Learning rate = 0.000335 ; Loss = 1.510900\n",
      "2024-12-10 23:24:57.642000: I runner.py:310] Step = 69700 ; steps/s = 1.61, tokens/s = 42659 (42659 target) ; Learning rate = 0.000335 ; Loss = 1.519510\n",
      "2024-12-10 23:25:59.197000: I runner.py:310] Step = 69800 ; steps/s = 1.62, tokens/s = 42176 (42176 target) ; Learning rate = 0.000335 ; Loss = 1.511940\n",
      "2024-12-10 23:27:01.169000: I runner.py:310] Step = 69900 ; steps/s = 1.61, tokens/s = 42679 (42679 target) ; Learning rate = 0.000334 ; Loss = 1.525048\n",
      "2024-12-10 23:28:03.235000: I runner.py:310] Step = 70000 ; steps/s = 1.61, tokens/s = 42607 (42607 target) ; Learning rate = 0.000334 ; Loss = 1.521177\n",
      "2024-12-10 23:28:05.910000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-70000\n",
      "2024-12-10 23:28:05.910000: I training.py:192] Running evaluation for step 70000\n",
      "2024-12-10 23:32:44.373000: I training.py:192] Evaluation result for step 70000: loss = 1.127453 ; perplexity = 3.087783\n",
      "2024-12-10 23:33:46.299000: I runner.py:310] Step = 70100 ; steps/s = 1.62, tokens/s = 42712 (42712 target) ; Learning rate = 0.000334 ; Loss = 1.522443\n",
      "2024-12-10 23:34:47.819000: I runner.py:310] Step = 70200 ; steps/s = 1.63, tokens/s = 42167 (42167 target) ; Learning rate = 0.000334 ; Loss = 1.507306\n",
      "2024-12-10 23:35:49.936000: I runner.py:310] Step = 70300 ; steps/s = 1.61, tokens/s = 42583 (42583 target) ; Learning rate = 0.000333 ; Loss = 1.520001\n",
      "2024-12-10 23:36:52.007000: I runner.py:310] Step = 70400 ; steps/s = 1.61, tokens/s = 42588 (42588 target) ; Learning rate = 0.000333 ; Loss = 1.525740\n",
      "2024-12-10 23:37:54.076000: I runner.py:310] Step = 70500 ; steps/s = 1.61, tokens/s = 42647 (42647 target) ; Learning rate = 0.000333 ; Loss = 1.524202\n",
      "2024-12-10 23:38:55.656000: I runner.py:310] Step = 70600 ; steps/s = 1.62, tokens/s = 42155 (42155 target) ; Learning rate = 0.000333 ; Loss = 1.511241\n",
      "2024-12-10 23:39:57.695000: I runner.py:310] Step = 70700 ; steps/s = 1.61, tokens/s = 42630 (42630 target) ; Learning rate = 0.000332 ; Loss = 1.520894\n",
      "2024-12-10 23:40:59.743000: I runner.py:310] Step = 70800 ; steps/s = 1.61, tokens/s = 42595 (42595 target) ; Learning rate = 0.000332 ; Loss = 1.518089\n",
      "2024-12-10 23:42:01.799000: I runner.py:310] Step = 70900 ; steps/s = 1.61, tokens/s = 42628 (42628 target) ; Learning rate = 0.000332 ; Loss = 1.526927\n",
      "2024-12-10 23:43:03.367000: I runner.py:310] Step = 71000 ; steps/s = 1.62, tokens/s = 42165 (42165 target) ; Learning rate = 0.000332 ; Loss = 1.525985\n",
      "2024-12-10 23:44:05.419000: I runner.py:310] Step = 71100 ; steps/s = 1.61, tokens/s = 42621 (42621 target) ; Learning rate = 0.000331 ; Loss = 1.521934\n",
      "2024-12-10 23:45:07.523000: I runner.py:310] Step = 71200 ; steps/s = 1.61, tokens/s = 42575 (42575 target) ; Learning rate = 0.000331 ; Loss = 1.519335\n",
      "2024-12-10 23:46:09.641000: I runner.py:310] Step = 71300 ; steps/s = 1.61, tokens/s = 42562 (42562 target) ; Learning rate = 0.000331 ; Loss = 1.516231\n",
      "2024-12-10 23:47:11.235000: I runner.py:310] Step = 71400 ; steps/s = 1.62, tokens/s = 42173 (42173 target) ; Learning rate = 0.000331 ; Loss = 1.511193\n",
      "2024-12-10 23:48:13.265000: I runner.py:310] Step = 71500 ; steps/s = 1.61, tokens/s = 42604 (42604 target) ; Learning rate = 0.000331 ; Loss = 1.516012\n",
      "2024-12-10 23:49:15.328000: I runner.py:310] Step = 71600 ; steps/s = 1.61, tokens/s = 42634 (42634 target) ; Learning rate = 0.000330 ; Loss = 1.520454\n",
      "2024-12-10 23:50:17.407000: I runner.py:310] Step = 71700 ; steps/s = 1.61, tokens/s = 42573 (42573 target) ; Learning rate = 0.000330 ; Loss = 1.513806\n",
      "2024-12-10 23:51:18.952000: I runner.py:310] Step = 71800 ; steps/s = 1.63, tokens/s = 42177 (42177 target) ; Learning rate = 0.000330 ; Loss = 1.527017\n",
      "2024-12-10 23:52:20.995000: I runner.py:310] Step = 71900 ; steps/s = 1.61, tokens/s = 42626 (42626 target) ; Learning rate = 0.000330 ; Loss = 1.512775\n",
      "2024-12-10 23:53:23.043000: I runner.py:310] Step = 72000 ; steps/s = 1.61, tokens/s = 42625 (42625 target) ; Learning rate = 0.000329 ; Loss = 1.508947\n",
      "2024-12-10 23:54:25.132000: I runner.py:310] Step = 72100 ; steps/s = 1.61, tokens/s = 42592 (42592 target) ; Learning rate = 0.000329 ; Loss = 1.519036\n",
      "2024-12-10 23:55:26.734000: I runner.py:310] Step = 72200 ; steps/s = 1.62, tokens/s = 42163 (42163 target) ; Learning rate = 0.000329 ; Loss = 1.508456\n",
      "2024-12-10 23:56:28.822000: I runner.py:310] Step = 72300 ; steps/s = 1.61, tokens/s = 42594 (42594 target) ; Learning rate = 0.000329 ; Loss = 1.521518\n",
      "2024-12-10 23:57:30.850000: I runner.py:310] Step = 72400 ; steps/s = 1.61, tokens/s = 42634 (42634 target) ; Learning rate = 0.000328 ; Loss = 1.523544\n",
      "2024-12-10 23:58:32.865000: I runner.py:310] Step = 72500 ; steps/s = 1.61, tokens/s = 42622 (42622 target) ; Learning rate = 0.000328 ; Loss = 1.517912\n",
      "2024-12-10 23:59:34.500000: I runner.py:310] Step = 72600 ; steps/s = 1.62, tokens/s = 42124 (42124 target) ; Learning rate = 0.000328 ; Loss = 1.524260\n",
      "2024-12-11 00:00:36.560000: I runner.py:310] Step = 72700 ; steps/s = 1.61, tokens/s = 42620 (42620 target) ; Learning rate = 0.000328 ; Loss = 1.514544\n",
      "2024-12-11 00:01:38.586000: I runner.py:310] Step = 72800 ; steps/s = 1.61, tokens/s = 42603 (42603 target) ; Learning rate = 0.000328 ; Loss = 1.516055\n",
      "2024-12-11 00:02:40.661000: I runner.py:310] Step = 72900 ; steps/s = 1.61, tokens/s = 42591 (42591 target) ; Learning rate = 0.000327 ; Loss = 1.525922\n",
      "2024-12-11 00:03:42.223000: I runner.py:310] Step = 73000 ; steps/s = 1.62, tokens/s = 42194 (42194 target) ; Learning rate = 0.000327 ; Loss = 1.516782\n",
      "2024-12-11 00:04:44.182000: I runner.py:310] Step = 73100 ; steps/s = 1.61, tokens/s = 42695 (42695 target) ; Learning rate = 0.000327 ; Loss = 1.511949\n",
      "2024-12-11 00:05:46.297000: I runner.py:310] Step = 73200 ; steps/s = 1.61, tokens/s = 42560 (42560 target) ; Learning rate = 0.000327 ; Loss = 1.515543\n",
      "2024-12-11 00:06:47.924000: I runner.py:310] Step = 73300 ; steps/s = 1.62, tokens/s = 42094 (42094 target) ; Learning rate = 0.000326 ; Loss = 1.521144\n",
      "2024-12-11 00:07:49.936000: I runner.py:310] Step = 73400 ; steps/s = 1.61, tokens/s = 42678 (42678 target) ; Learning rate = 0.000326 ; Loss = 1.511561\n",
      "2024-12-11 00:08:51.920000: I runner.py:310] Step = 73500 ; steps/s = 1.61, tokens/s = 42684 (42684 target) ; Learning rate = 0.000326 ; Loss = 1.518114\n",
      "2024-12-11 00:09:53.976000: I runner.py:310] Step = 73600 ; steps/s = 1.61, tokens/s = 42604 (42604 target) ; Learning rate = 0.000326 ; Loss = 1.518340\n",
      "2024-12-11 00:10:55.542000: I runner.py:310] Step = 73700 ; steps/s = 1.62, tokens/s = 42136 (42136 target) ; Learning rate = 0.000326 ; Loss = 1.520295\n",
      "2024-12-11 00:11:57.567000: I runner.py:310] Step = 73800 ; steps/s = 1.61, tokens/s = 42673 (42673 target) ; Learning rate = 0.000325 ; Loss = 1.521748\n",
      "2024-12-11 00:12:59.588000: I runner.py:310] Step = 73900 ; steps/s = 1.61, tokens/s = 42641 (42641 target) ; Learning rate = 0.000325 ; Loss = 1.514409\n",
      "2024-12-11 00:14:01.635000: I runner.py:310] Step = 74000 ; steps/s = 1.61, tokens/s = 42636 (42636 target) ; Learning rate = 0.000325 ; Loss = 1.513489\n",
      "2024-12-11 00:15:03.237000: I runner.py:310] Step = 74100 ; steps/s = 1.62, tokens/s = 42085 (42085 target) ; Learning rate = 0.000325 ; Loss = 1.514562\n",
      "2024-12-11 00:16:05.283000: I runner.py:310] Step = 74200 ; steps/s = 1.61, tokens/s = 42644 (42644 target) ; Learning rate = 0.000324 ; Loss = 1.517150\n",
      "2024-12-11 00:17:07.310000: I runner.py:310] Step = 74300 ; steps/s = 1.61, tokens/s = 42662 (42662 target) ; Learning rate = 0.000324 ; Loss = 1.511701\n",
      "2024-12-11 00:18:09.273000: I runner.py:310] Step = 74400 ; steps/s = 1.61, tokens/s = 42650 (42650 target) ; Learning rate = 0.000324 ; Loss = 1.520924\n",
      "2024-12-11 00:19:10.907000: I runner.py:310] Step = 74500 ; steps/s = 1.62, tokens/s = 42108 (42108 target) ; Learning rate = 0.000324 ; Loss = 1.511276\n",
      "2024-12-11 00:20:12.982000: I runner.py:310] Step = 74600 ; steps/s = 1.61, tokens/s = 42601 (42601 target) ; Learning rate = 0.000324 ; Loss = 1.517576\n",
      "2024-12-11 00:21:15.071000: I runner.py:310] Step = 74700 ; steps/s = 1.61, tokens/s = 42577 (42577 target) ; Learning rate = 0.000323 ; Loss = 1.517086\n",
      "2024-12-11 00:22:17.117000: I runner.py:310] Step = 74800 ; steps/s = 1.61, tokens/s = 42619 (42619 target) ; Learning rate = 0.000323 ; Loss = 1.505818\n",
      "2024-12-11 00:23:18.707000: I runner.py:310] Step = 74900 ; steps/s = 1.62, tokens/s = 42168 (42168 target) ; Learning rate = 0.000323 ; Loss = 1.519986\n",
      "2024-12-11 00:24:20.671000: I runner.py:310] Step = 75000 ; steps/s = 1.61, tokens/s = 42694 (42694 target) ; Learning rate = 0.000323 ; Loss = 1.518091\n",
      "2024-12-11 00:24:20.672000: I training.py:192] Running evaluation for step 75000\n",
      "2024-12-11 00:28:52.620000: I training.py:192] Evaluation result for step 75000: loss = 1.133499 ; perplexity = 3.106506\n",
      "2024-12-11 00:29:54.509000: I runner.py:310] Step = 75100 ; steps/s = 1.62, tokens/s = 42737 (42737 target) ; Learning rate = 0.000323 ; Loss = 1.506111\n",
      "2024-12-11 00:30:56.511000: I runner.py:310] Step = 75200 ; steps/s = 1.61, tokens/s = 42664 (42664 target) ; Learning rate = 0.000322 ; Loss = 1.512069\n",
      "2024-12-11 00:31:58.137000: I runner.py:310] Step = 75300 ; steps/s = 1.62, tokens/s = 42086 (42086 target) ; Learning rate = 0.000322 ; Loss = 1.507995\n",
      "2024-12-11 00:33:00.191000: I runner.py:310] Step = 75400 ; steps/s = 1.61, tokens/s = 42619 (42619 target) ; Learning rate = 0.000322 ; Loss = 1.514475\n",
      "2024-12-11 00:34:02.181000: I runner.py:310] Step = 75500 ; steps/s = 1.61, tokens/s = 42667 (42667 target) ; Learning rate = 0.000322 ; Loss = 1.506400\n",
      "2024-12-11 00:35:04.292000: I runner.py:310] Step = 75600 ; steps/s = 1.61, tokens/s = 42591 (42591 target) ; Learning rate = 0.000321 ; Loss = 1.511104\n",
      "2024-12-11 00:36:05.859000: I runner.py:310] Step = 75700 ; steps/s = 1.62, tokens/s = 42156 (42156 target) ; Learning rate = 0.000321 ; Loss = 1.512795\n",
      "2024-12-11 00:37:07.918000: I runner.py:310] Step = 75800 ; steps/s = 1.61, tokens/s = 42624 (42624 target) ; Learning rate = 0.000321 ; Loss = 1.510671\n",
      "2024-12-11 00:38:09.948000: I runner.py:310] Step = 75900 ; steps/s = 1.61, tokens/s = 42612 (42612 target) ; Learning rate = 0.000321 ; Loss = 1.509237\n",
      "2024-12-11 00:39:12.025000: I runner.py:310] Step = 76000 ; steps/s = 1.61, tokens/s = 42612 (42612 target) ; Learning rate = 0.000321 ; Loss = 1.511251\n",
      "2024-12-11 00:40:13.661000: I runner.py:310] Step = 76100 ; steps/s = 1.62, tokens/s = 42112 (42112 target) ; Learning rate = 0.000320 ; Loss = 1.510914\n",
      "2024-12-11 00:41:15.663000: I runner.py:310] Step = 76200 ; steps/s = 1.61, tokens/s = 42668 (42668 target) ; Learning rate = 0.000320 ; Loss = 1.520376\n",
      "2024-12-11 00:42:17.811000: I runner.py:310] Step = 76300 ; steps/s = 1.61, tokens/s = 42550 (42550 target) ; Learning rate = 0.000320 ; Loss = 1.519285\n",
      "2024-12-11 00:43:19.856000: I runner.py:310] Step = 76400 ; steps/s = 1.61, tokens/s = 42612 (42612 target) ; Learning rate = 0.000320 ; Loss = 1.513727\n",
      "2024-12-11 00:44:21.469000: I runner.py:310] Step = 76500 ; steps/s = 1.62, tokens/s = 42148 (42148 target) ; Learning rate = 0.000320 ; Loss = 1.517188\n",
      "2024-12-11 00:45:23.563000: I runner.py:310] Step = 76600 ; steps/s = 1.61, tokens/s = 42595 (42595 target) ; Learning rate = 0.000319 ; Loss = 1.519738\n",
      "2024-12-11 00:46:25.609000: I runner.py:310] Step = 76700 ; steps/s = 1.61, tokens/s = 42618 (42618 target) ; Learning rate = 0.000319 ; Loss = 1.513579\n",
      "2024-12-11 00:47:27.612000: I runner.py:310] Step = 76800 ; steps/s = 1.61, tokens/s = 42633 (42633 target) ; Learning rate = 0.000319 ; Loss = 1.512585\n",
      "2024-12-11 00:48:29.224000: I runner.py:310] Step = 76900 ; steps/s = 1.62, tokens/s = 42119 (42119 target) ; Learning rate = 0.000319 ; Loss = 1.515101\n",
      "2024-12-11 00:49:31.273000: I runner.py:310] Step = 77000 ; steps/s = 1.61, tokens/s = 42652 (42652 target) ; Learning rate = 0.000319 ; Loss = 1.507787\n",
      "2024-12-11 00:50:33.373000: I runner.py:310] Step = 77100 ; steps/s = 1.61, tokens/s = 42581 (42581 target) ; Learning rate = 0.000318 ; Loss = 1.515919\n",
      "2024-12-11 00:51:35.409000: I runner.py:310] Step = 77200 ; steps/s = 1.61, tokens/s = 42605 (42605 target) ; Learning rate = 0.000318 ; Loss = 1.519175\n",
      "2024-12-11 00:52:37.026000: I runner.py:310] Step = 77300 ; steps/s = 1.62, tokens/s = 42112 (42112 target) ; Learning rate = 0.000318 ; Loss = 1.509383\n",
      "2024-12-11 00:53:39.025000: I runner.py:310] Step = 77400 ; steps/s = 1.61, tokens/s = 42664 (42664 target) ; Learning rate = 0.000318 ; Loss = 1.512860\n",
      "2024-12-11 00:54:41.120000: I runner.py:310] Step = 77500 ; steps/s = 1.61, tokens/s = 42601 (42601 target) ; Learning rate = 0.000317 ; Loss = 1.512221\n",
      "2024-12-11 00:55:43.102000: I runner.py:310] Step = 77600 ; steps/s = 1.61, tokens/s = 42693 (42693 target) ; Learning rate = 0.000317 ; Loss = 1.516847\n",
      "2024-12-11 00:56:44.651000: I runner.py:310] Step = 77700 ; steps/s = 1.62, tokens/s = 42132 (42132 target) ; Learning rate = 0.000317 ; Loss = 1.509277\n",
      "2024-12-11 00:57:46.643000: I runner.py:310] Step = 77800 ; steps/s = 1.61, tokens/s = 42670 (42670 target) ; Learning rate = 0.000317 ; Loss = 1.505392\n",
      "2024-12-11 00:58:48.769000: I runner.py:310] Step = 77900 ; steps/s = 1.61, tokens/s = 42575 (42575 target) ; Learning rate = 0.000317 ; Loss = 1.514002\n",
      "2024-12-11 00:59:50.840000: I runner.py:310] Step = 78000 ; steps/s = 1.61, tokens/s = 42606 (42606 target) ; Learning rate = 0.000316 ; Loss = 1.515227\n",
      "2024-12-11 01:00:52.464000: I runner.py:310] Step = 78100 ; steps/s = 1.62, tokens/s = 42125 (42125 target) ; Learning rate = 0.000316 ; Loss = 1.505796\n",
      "2024-12-11 01:01:54.545000: I runner.py:310] Step = 78200 ; steps/s = 1.61, tokens/s = 42574 (42574 target) ; Learning rate = 0.000316 ; Loss = 1.509424\n",
      "2024-12-11 01:02:56.593000: I runner.py:310] Step = 78300 ; steps/s = 1.61, tokens/s = 42670 (42670 target) ; Learning rate = 0.000316 ; Loss = 1.512146\n",
      "2024-12-11 01:03:58.630000: I runner.py:310] Step = 78400 ; steps/s = 1.61, tokens/s = 42636 (42636 target) ; Learning rate = 0.000316 ; Loss = 1.511197\n",
      "2024-12-11 01:05:00.268000: I runner.py:310] Step = 78500 ; steps/s = 1.62, tokens/s = 42104 (42104 target) ; Learning rate = 0.000315 ; Loss = 1.515951\n",
      "2024-12-11 01:06:02.303000: I runner.py:310] Step = 78600 ; steps/s = 1.61, tokens/s = 42608 (42608 target) ; Learning rate = 0.000315 ; Loss = 1.504590\n",
      "2024-12-11 01:07:04.405000: I runner.py:310] Step = 78700 ; steps/s = 1.61, tokens/s = 42586 (42586 target) ; Learning rate = 0.000315 ; Loss = 1.512266\n",
      "2024-12-11 01:08:06.495000: I runner.py:310] Step = 78800 ; steps/s = 1.61, tokens/s = 42580 (42580 target) ; Learning rate = 0.000315 ; Loss = 1.506582\n",
      "2024-12-11 01:09:08.125000: I runner.py:310] Step = 78900 ; steps/s = 1.62, tokens/s = 42120 (42120 target) ; Learning rate = 0.000315 ; Loss = 1.516985\n",
      "2024-12-11 01:10:10.166000: I runner.py:310] Step = 79000 ; steps/s = 1.61, tokens/s = 42637 (42637 target) ; Learning rate = 0.000314 ; Loss = 1.515695\n",
      "2024-12-11 01:11:12.206000: I runner.py:310] Step = 79100 ; steps/s = 1.61, tokens/s = 42613 (42613 target) ; Learning rate = 0.000314 ; Loss = 1.515668\n",
      "2024-12-11 01:12:14.224000: I runner.py:310] Step = 79200 ; steps/s = 1.61, tokens/s = 42641 (42641 target) ; Learning rate = 0.000314 ; Loss = 1.511931\n",
      "2024-12-11 01:13:15.782000: I runner.py:310] Step = 79300 ; steps/s = 1.62, tokens/s = 42166 (42166 target) ; Learning rate = 0.000314 ; Loss = 1.518831\n",
      "2024-12-11 01:14:17.836000: I runner.py:310] Step = 79400 ; steps/s = 1.61, tokens/s = 42643 (42643 target) ; Learning rate = 0.000314 ; Loss = 1.510120\n",
      "2024-12-11 01:15:19.861000: I runner.py:310] Step = 79500 ; steps/s = 1.61, tokens/s = 42655 (42655 target) ; Learning rate = 0.000313 ; Loss = 1.511054\n",
      "2024-12-11 01:16:21.873000: I runner.py:310] Step = 79600 ; steps/s = 1.61, tokens/s = 42618 (42618 target) ; Learning rate = 0.000313 ; Loss = 1.509162\n",
      "2024-12-11 01:17:23.426000: I runner.py:310] Step = 79700 ; steps/s = 1.62, tokens/s = 42145 (42145 target) ; Learning rate = 0.000313 ; Loss = 1.514339\n",
      "2024-12-11 01:18:25.427000: I runner.py:310] Step = 79800 ; steps/s = 1.61, tokens/s = 42657 (42657 target) ; Learning rate = 0.000313 ; Loss = 1.508235\n",
      "2024-12-11 01:19:27.469000: I runner.py:310] Step = 79900 ; steps/s = 1.61, tokens/s = 42666 (42666 target) ; Learning rate = 0.000313 ; Loss = 1.507445\n",
      "2024-12-11 01:20:29.481000: I runner.py:310] Step = 80000 ; steps/s = 1.61, tokens/s = 42655 (42655 target) ; Learning rate = 0.000312 ; Loss = 1.511204\n",
      "2024-12-11 01:20:31.642000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-80000\n",
      "2024-12-11 01:20:31.643000: I training.py:192] Running evaluation for step 80000\n",
      "2024-12-11 01:25:07.408000: I training.py:192] Evaluation result for step 80000: loss = 1.137675 ; perplexity = 3.119506\n",
      "2024-12-11 01:26:08.792000: I runner.py:310] Step = 80100 ; steps/s = 1.63, tokens/s = 42244 (42244 target) ; Learning rate = 0.000312 ; Loss = 1.510444\n",
      "2024-12-11 01:27:10.812000: I runner.py:310] Step = 80200 ; steps/s = 1.61, tokens/s = 42627 (42627 target) ; Learning rate = 0.000312 ; Loss = 1.509865\n",
      "2024-12-11 01:28:12.868000: I runner.py:310] Step = 80300 ; steps/s = 1.61, tokens/s = 42620 (42620 target) ; Learning rate = 0.000312 ; Loss = 1.506281\n",
      "2024-12-11 01:29:14.866000: I runner.py:310] Step = 80400 ; steps/s = 1.61, tokens/s = 42679 (42679 target) ; Learning rate = 0.000312 ; Loss = 1.504710\n",
      "2024-12-11 01:30:16.419000: I runner.py:310] Step = 80500 ; steps/s = 1.62, tokens/s = 42169 (42169 target) ; Learning rate = 0.000312 ; Loss = 1.503767\n",
      "2024-12-11 01:31:18.383000: I runner.py:310] Step = 80600 ; steps/s = 1.61, tokens/s = 42705 (42705 target) ; Learning rate = 0.000311 ; Loss = 1.505865\n",
      "2024-12-11 01:32:20.399000: I runner.py:310] Step = 80700 ; steps/s = 1.61, tokens/s = 42654 (42654 target) ; Learning rate = 0.000311 ; Loss = 1.518592\n",
      "2024-12-11 01:33:22.337000: I runner.py:310] Step = 80800 ; steps/s = 1.61, tokens/s = 42680 (42680 target) ; Learning rate = 0.000311 ; Loss = 1.512041\n",
      "2024-12-11 01:34:23.953000: I runner.py:310] Step = 80900 ; steps/s = 1.62, tokens/s = 42107 (42107 target) ; Learning rate = 0.000311 ; Loss = 1.516957\n",
      "2024-12-11 01:35:25.945000: I runner.py:310] Step = 81000 ; steps/s = 1.61, tokens/s = 42672 (42672 target) ; Learning rate = 0.000311 ; Loss = 1.508890\n",
      "2024-12-11 01:36:27.955000: I runner.py:310] Step = 81100 ; steps/s = 1.61, tokens/s = 42668 (42668 target) ; Learning rate = 0.000310 ; Loss = 1.516009\n",
      "2024-12-11 01:37:29.928000: I runner.py:310] Step = 81200 ; steps/s = 1.61, tokens/s = 42664 (42664 target) ; Learning rate = 0.000310 ; Loss = 1.507089\n",
      "2024-12-11 01:38:31.502000: I runner.py:310] Step = 81300 ; steps/s = 1.62, tokens/s = 42142 (42142 target) ; Learning rate = 0.000310 ; Loss = 1.503610\n",
      "2024-12-11 01:39:33.427000: I runner.py:310] Step = 81400 ; steps/s = 1.62, tokens/s = 42718 (42718 target) ; Learning rate = 0.000310 ; Loss = 1.514415\n",
      "2024-12-11 01:40:35.486000: I runner.py:310] Step = 81500 ; steps/s = 1.61, tokens/s = 42598 (42598 target) ; Learning rate = 0.000310 ; Loss = 1.510017\n",
      "2024-12-11 01:41:37.436000: I runner.py:310] Step = 81600 ; steps/s = 1.61, tokens/s = 42703 (42703 target) ; Learning rate = 0.000309 ; Loss = 1.516951\n",
      "2024-12-11 01:42:39.052000: I runner.py:310] Step = 81700 ; steps/s = 1.62, tokens/s = 42117 (42117 target) ; Learning rate = 0.000309 ; Loss = 1.509919\n",
      "2024-12-11 01:43:41.098000: I runner.py:310] Step = 81800 ; steps/s = 1.61, tokens/s = 42636 (42636 target) ; Learning rate = 0.000309 ; Loss = 1.505982\n",
      "2024-12-11 01:44:43.144000: I runner.py:310] Step = 81900 ; steps/s = 1.61, tokens/s = 42621 (42621 target) ; Learning rate = 0.000309 ; Loss = 1.511407\n",
      "2024-12-11 01:45:45.197000: I runner.py:310] Step = 82000 ; steps/s = 1.61, tokens/s = 42599 (42599 target) ; Learning rate = 0.000309 ; Loss = 1.506434\n",
      "2024-12-11 01:46:46.698000: I runner.py:310] Step = 82100 ; steps/s = 1.63, tokens/s = 42205 (42205 target) ; Learning rate = 0.000308 ; Loss = 1.502948\n",
      "2024-12-11 01:47:48.673000: I runner.py:310] Step = 82200 ; steps/s = 1.61, tokens/s = 42651 (42651 target) ; Learning rate = 0.000308 ; Loss = 1.503867\n",
      "2024-12-11 01:48:50.660000: I runner.py:310] Step = 82300 ; steps/s = 1.61, tokens/s = 42674 (42674 target) ; Learning rate = 0.000308 ; Loss = 1.514489\n",
      "2024-12-11 01:49:52.620000: I runner.py:310] Step = 82400 ; steps/s = 1.61, tokens/s = 42690 (42690 target) ; Learning rate = 0.000308 ; Loss = 1.512443\n",
      "2024-12-11 01:50:54.213000: I runner.py:310] Step = 82500 ; steps/s = 1.62, tokens/s = 42143 (42143 target) ; Learning rate = 0.000308 ; Loss = 1.510094\n",
      "2024-12-11 01:51:56.166000: I runner.py:310] Step = 82600 ; steps/s = 1.61, tokens/s = 42684 (42684 target) ; Learning rate = 0.000308 ; Loss = 1.502559\n",
      "2024-12-11 01:52:58.112000: I runner.py:310] Step = 82700 ; steps/s = 1.61, tokens/s = 42721 (42721 target) ; Learning rate = 0.000307 ; Loss = 1.505695\n",
      "2024-12-11 01:54:00.152000: I runner.py:310] Step = 82800 ; steps/s = 1.61, tokens/s = 42599 (42599 target) ; Learning rate = 0.000307 ; Loss = 1.511509\n",
      "2024-12-11 01:55:01.621000: I runner.py:310] Step = 82900 ; steps/s = 1.63, tokens/s = 42244 (42244 target) ; Learning rate = 0.000307 ; Loss = 1.518197\n",
      "2024-12-11 01:56:03.662000: I runner.py:310] Step = 83000 ; steps/s = 1.61, tokens/s = 42642 (42642 target) ; Learning rate = 0.000307 ; Loss = 1.504614\n",
      "2024-12-11 01:57:05.582000: I runner.py:310] Step = 83100 ; steps/s = 1.62, tokens/s = 42689 (42689 target) ; Learning rate = 0.000307 ; Loss = 1.513685\n",
      "2024-12-11 01:58:07.585000: I runner.py:310] Step = 83200 ; steps/s = 1.61, tokens/s = 42656 (42656 target) ; Learning rate = 0.000306 ; Loss = 1.511765\n",
      "2024-12-11 01:59:09.130000: I runner.py:310] Step = 83300 ; steps/s = 1.63, tokens/s = 42193 (42193 target) ; Learning rate = 0.000306 ; Loss = 1.505505\n",
      "2024-12-11 02:00:11.082000: I runner.py:310] Step = 83400 ; steps/s = 1.61, tokens/s = 42699 (42699 target) ; Learning rate = 0.000306 ; Loss = 1.504659\n",
      "2024-12-11 02:01:13.108000: I runner.py:310] Step = 83500 ; steps/s = 1.61, tokens/s = 42625 (42625 target) ; Learning rate = 0.000306 ; Loss = 1.504455\n",
      "2024-12-11 02:02:14.749000: I runner.py:310] Step = 83600 ; steps/s = 1.62, tokens/s = 42310 (42310 target) ; Learning rate = 0.000306 ; Loss = 1.505812\n",
      "2024-12-11 02:03:16.631000: I runner.py:310] Step = 83700 ; steps/s = 1.62, tokens/s = 42554 (42554 target) ; Learning rate = 0.000306 ; Loss = 1.505586\n",
      "2024-12-11 02:04:18.616000: I runner.py:310] Step = 83800 ; steps/s = 1.61, tokens/s = 42641 (42641 target) ; Learning rate = 0.000305 ; Loss = 1.501690\n",
      "2024-12-11 02:05:20.596000: I runner.py:310] Step = 83900 ; steps/s = 1.61, tokens/s = 42669 (42669 target) ; Learning rate = 0.000305 ; Loss = 1.515836\n",
      "2024-12-11 02:06:22.135000: I runner.py:310] Step = 84000 ; steps/s = 1.63, tokens/s = 42151 (42151 target) ; Learning rate = 0.000305 ; Loss = 1.510923\n",
      "2024-12-11 02:07:24.069000: I runner.py:310] Step = 84100 ; steps/s = 1.61, tokens/s = 42736 (42736 target) ; Learning rate = 0.000305 ; Loss = 1.510696\n",
      "2024-12-11 02:08:26.049000: I runner.py:310] Step = 84200 ; steps/s = 1.61, tokens/s = 42645 (42645 target) ; Learning rate = 0.000305 ; Loss = 1.503611\n",
      "2024-12-11 02:09:28.070000: I runner.py:310] Step = 84300 ; steps/s = 1.61, tokens/s = 42629 (42629 target) ; Learning rate = 0.000304 ; Loss = 1.503614\n",
      "2024-12-11 02:10:29.640000: I runner.py:310] Step = 84400 ; steps/s = 1.62, tokens/s = 42169 (42169 target) ; Learning rate = 0.000304 ; Loss = 1.503762\n",
      "2024-12-11 02:11:31.631000: I runner.py:310] Step = 84500 ; steps/s = 1.61, tokens/s = 42662 (42662 target) ; Learning rate = 0.000304 ; Loss = 1.514847\n",
      "2024-12-11 02:12:33.645000: I runner.py:310] Step = 84600 ; steps/s = 1.61, tokens/s = 42648 (42648 target) ; Learning rate = 0.000304 ; Loss = 1.507271\n",
      "2024-12-11 02:13:35.677000: I runner.py:310] Step = 84700 ; steps/s = 1.61, tokens/s = 42621 (42621 target) ; Learning rate = 0.000304 ; Loss = 1.503406\n",
      "2024-12-11 02:14:37.185000: I runner.py:310] Step = 84800 ; steps/s = 1.63, tokens/s = 42214 (42214 target) ; Learning rate = 0.000304 ; Loss = 1.511764\n",
      "2024-12-11 02:15:39.194000: I runner.py:310] Step = 84900 ; steps/s = 1.61, tokens/s = 42668 (42668 target) ; Learning rate = 0.000303 ; Loss = 1.501144\n",
      "2024-12-11 02:16:41.152000: I runner.py:310] Step = 85000 ; steps/s = 1.61, tokens/s = 42671 (42671 target) ; Learning rate = 0.000303 ; Loss = 1.508007\n",
      "2024-12-11 02:16:41.153000: I training.py:192] Running evaluation for step 85000\n",
      "2024-12-11 02:21:15.110000: I training.py:192] Evaluation result for step 85000: loss = 1.143227 ; perplexity = 3.136875\n",
      "2024-12-11 02:22:16.989000: I runner.py:310] Step = 85100 ; steps/s = 1.62, tokens/s = 42750 (42750 target) ; Learning rate = 0.000303 ; Loss = 1.517453\n",
      "2024-12-11 02:23:18.519000: I runner.py:310] Step = 85200 ; steps/s = 1.63, tokens/s = 42173 (42173 target) ; Learning rate = 0.000303 ; Loss = 1.508105\n",
      "2024-12-11 02:24:20.551000: I runner.py:310] Step = 85300 ; steps/s = 1.61, tokens/s = 42663 (42663 target) ; Learning rate = 0.000303 ; Loss = 1.509507\n",
      "2024-12-11 02:25:22.543000: I runner.py:310] Step = 85400 ; steps/s = 1.61, tokens/s = 42649 (42649 target) ; Learning rate = 0.000302 ; Loss = 1.503406\n",
      "2024-12-11 02:26:24.664000: I runner.py:310] Step = 85500 ; steps/s = 1.61, tokens/s = 42580 (42580 target) ; Learning rate = 0.000302 ; Loss = 1.517250\n",
      "2024-12-11 02:27:26.267000: I runner.py:310] Step = 85600 ; steps/s = 1.62, tokens/s = 42113 (42113 target) ; Learning rate = 0.000302 ; Loss = 1.502687\n",
      "2024-12-11 02:28:28.253000: I runner.py:310] Step = 85700 ; steps/s = 1.61, tokens/s = 42678 (42678 target) ; Learning rate = 0.000302 ; Loss = 1.511228\n",
      "2024-12-11 02:29:30.240000: I runner.py:310] Step = 85800 ; steps/s = 1.61, tokens/s = 42650 (42650 target) ; Learning rate = 0.000302 ; Loss = 1.507924\n",
      "2024-12-11 02:30:32.245000: I runner.py:310] Step = 85900 ; steps/s = 1.61, tokens/s = 42656 (42656 target) ; Learning rate = 0.000302 ; Loss = 1.502779\n",
      "2024-12-11 02:31:33.775000: I runner.py:310] Step = 86000 ; steps/s = 1.63, tokens/s = 42185 (42185 target) ; Learning rate = 0.000301 ; Loss = 1.507843\n",
      "2024-12-11 02:32:35.761000: I runner.py:310] Step = 86100 ; steps/s = 1.61, tokens/s = 42673 (42673 target) ; Learning rate = 0.000301 ; Loss = 1.508293\n",
      "2024-12-11 02:33:37.768000: I runner.py:310] Step = 86200 ; steps/s = 1.61, tokens/s = 42630 (42630 target) ; Learning rate = 0.000301 ; Loss = 1.510158\n",
      "2024-12-11 02:34:39.791000: I runner.py:310] Step = 86300 ; steps/s = 1.61, tokens/s = 42629 (42629 target) ; Learning rate = 0.000301 ; Loss = 1.511516\n",
      "2024-12-11 02:35:41.380000: I runner.py:310] Step = 86400 ; steps/s = 1.62, tokens/s = 42166 (42166 target) ; Learning rate = 0.000301 ; Loss = 1.504043\n",
      "2024-12-11 02:36:43.461000: I runner.py:310] Step = 86500 ; steps/s = 1.61, tokens/s = 42605 (42605 target) ; Learning rate = 0.000301 ; Loss = 1.503549\n",
      "2024-12-11 02:37:45.416000: I runner.py:310] Step = 86600 ; steps/s = 1.61, tokens/s = 42681 (42681 target) ; Learning rate = 0.000300 ; Loss = 1.505356\n",
      "2024-12-11 02:38:47.406000: I runner.py:310] Step = 86700 ; steps/s = 1.61, tokens/s = 42676 (42676 target) ; Learning rate = 0.000300 ; Loss = 1.516501\n",
      "2024-12-11 02:39:49.051000: I runner.py:310] Step = 86800 ; steps/s = 1.62, tokens/s = 42087 (42087 target) ; Learning rate = 0.000300 ; Loss = 1.507864\n",
      "2024-12-11 02:40:51.114000: I runner.py:310] Step = 86900 ; steps/s = 1.61, tokens/s = 42625 (42625 target) ; Learning rate = 0.000300 ; Loss = 1.503293\n",
      "2024-12-11 02:41:53.121000: I runner.py:310] Step = 87000 ; steps/s = 1.61, tokens/s = 42625 (42625 target) ; Learning rate = 0.000300 ; Loss = 1.507539\n",
      "2024-12-11 02:42:55.083000: I runner.py:310] Step = 87100 ; steps/s = 1.61, tokens/s = 42668 (42668 target) ; Learning rate = 0.000299 ; Loss = 1.507168\n",
      "2024-12-11 02:43:56.626000: I runner.py:310] Step = 87200 ; steps/s = 1.63, tokens/s = 42216 (42216 target) ; Learning rate = 0.000299 ; Loss = 1.508648\n",
      "2024-12-11 02:44:58.605000: I runner.py:310] Step = 87300 ; steps/s = 1.61, tokens/s = 42658 (42658 target) ; Learning rate = 0.000299 ; Loss = 1.504597\n",
      "2024-12-11 02:46:00.644000: I runner.py:310] Step = 87400 ; steps/s = 1.61, tokens/s = 42622 (42622 target) ; Learning rate = 0.000299 ; Loss = 1.500955\n",
      "2024-12-11 02:47:02.624000: I runner.py:310] Step = 87500 ; steps/s = 1.61, tokens/s = 42678 (42678 target) ; Learning rate = 0.000299 ; Loss = 1.505233\n",
      "2024-12-11 02:48:04.177000: I runner.py:310] Step = 87600 ; steps/s = 1.62, tokens/s = 42161 (42161 target) ; Learning rate = 0.000299 ; Loss = 1.512503\n",
      "2024-12-11 02:49:06.151000: I runner.py:310] Step = 87700 ; steps/s = 1.61, tokens/s = 42664 (42664 target) ; Learning rate = 0.000298 ; Loss = 1.500155\n",
      "2024-12-11 02:50:08.157000: I runner.py:310] Step = 87800 ; steps/s = 1.61, tokens/s = 42676 (42676 target) ; Learning rate = 0.000298 ; Loss = 1.503267\n",
      "2024-12-11 02:51:10.176000: I runner.py:310] Step = 87900 ; steps/s = 1.61, tokens/s = 42631 (42631 target) ; Learning rate = 0.000298 ; Loss = 1.507528\n",
      "2024-12-11 02:52:11.775000: I runner.py:310] Step = 88000 ; steps/s = 1.62, tokens/s = 42116 (42116 target) ; Learning rate = 0.000298 ; Loss = 1.512120\n",
      "2024-12-11 02:53:13.841000: I runner.py:310] Step = 88100 ; steps/s = 1.61, tokens/s = 42628 (42628 target) ; Learning rate = 0.000298 ; Loss = 1.492464\n",
      "2024-12-11 02:54:15.857000: I runner.py:310] Step = 88200 ; steps/s = 1.61, tokens/s = 42656 (42656 target) ; Learning rate = 0.000298 ; Loss = 1.502793\n",
      "2024-12-11 02:55:17.878000: I runner.py:310] Step = 88300 ; steps/s = 1.61, tokens/s = 42632 (42632 target) ; Learning rate = 0.000297 ; Loss = 1.507925\n",
      "2024-12-11 02:56:19.433000: I runner.py:310] Step = 88400 ; steps/s = 1.62, tokens/s = 42143 (42143 target) ; Learning rate = 0.000297 ; Loss = 1.496913\n",
      "2024-12-11 02:57:21.466000: I runner.py:310] Step = 88500 ; steps/s = 1.61, tokens/s = 42622 (42622 target) ; Learning rate = 0.000297 ; Loss = 1.505698\n",
      "2024-12-11 02:58:23.506000: I runner.py:310] Step = 88600 ; steps/s = 1.61, tokens/s = 42638 (42638 target) ; Learning rate = 0.000297 ; Loss = 1.510287\n",
      "2024-12-11 02:59:25.507000: I runner.py:310] Step = 88700 ; steps/s = 1.61, tokens/s = 42654 (42654 target) ; Learning rate = 0.000297 ; Loss = 1.509621\n",
      "2024-12-11 03:00:27.035000: I runner.py:310] Step = 88800 ; steps/s = 1.63, tokens/s = 42216 (42216 target) ; Learning rate = 0.000297 ; Loss = 1.508135\n",
      "2024-12-11 03:01:29.018000: I runner.py:310] Step = 88900 ; steps/s = 1.61, tokens/s = 42662 (42662 target) ; Learning rate = 0.000296 ; Loss = 1.511151\n",
      "2024-12-11 03:02:31.009000: I runner.py:310] Step = 89000 ; steps/s = 1.61, tokens/s = 42642 (42642 target) ; Learning rate = 0.000296 ; Loss = 1.502400\n",
      "2024-12-11 03:03:33.019000: I runner.py:310] Step = 89100 ; steps/s = 1.61, tokens/s = 42682 (42682 target) ; Learning rate = 0.000296 ; Loss = 1.509529\n",
      "2024-12-11 03:04:34.659000: I runner.py:310] Step = 89200 ; steps/s = 1.62, tokens/s = 42080 (42080 target) ; Learning rate = 0.000296 ; Loss = 1.502386\n",
      "2024-12-11 03:05:36.635000: I runner.py:310] Step = 89300 ; steps/s = 1.61, tokens/s = 42661 (42661 target) ; Learning rate = 0.000296 ; Loss = 1.504186\n",
      "2024-12-11 03:06:38.628000: I runner.py:310] Step = 89400 ; steps/s = 1.61, tokens/s = 42672 (42672 target) ; Learning rate = 0.000296 ; Loss = 1.505197\n",
      "2024-12-11 03:07:40.656000: I runner.py:310] Step = 89500 ; steps/s = 1.61, tokens/s = 42645 (42645 target) ; Learning rate = 0.000295 ; Loss = 1.501159\n",
      "2024-12-11 03:08:42.253000: I runner.py:310] Step = 89600 ; steps/s = 1.62, tokens/s = 42156 (42156 target) ; Learning rate = 0.000295 ; Loss = 1.500492\n",
      "2024-12-11 03:09:44.267000: I runner.py:310] Step = 89700 ; steps/s = 1.61, tokens/s = 42615 (42615 target) ; Learning rate = 0.000295 ; Loss = 1.508288\n",
      "2024-12-11 03:10:46.336000: I runner.py:310] Step = 89800 ; steps/s = 1.61, tokens/s = 42601 (42601 target) ; Learning rate = 0.000295 ; Loss = 1.501519\n",
      "2024-12-11 03:11:48.380000: I runner.py:310] Step = 89900 ; steps/s = 1.61, tokens/s = 42633 (42633 target) ; Learning rate = 0.000295 ; Loss = 1.506436\n",
      "2024-12-11 03:12:50.037000: I runner.py:310] Step = 90000 ; steps/s = 1.62, tokens/s = 42101 (42101 target) ; Learning rate = 0.000295 ; Loss = 1.497600\n",
      "2024-12-11 03:12:52.236000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-90000\n",
      "2024-12-11 03:12:52.236000: I training.py:192] Running evaluation for step 90000\n",
      "2024-12-11 03:17:26.028000: I training.py:192] Evaluation result for step 90000: loss = 1.147184 ; perplexity = 3.149312\n",
      "2024-12-11 03:18:27.887000: I runner.py:310] Step = 90100 ; steps/s = 1.62, tokens/s = 42779 (42779 target) ; Learning rate = 0.000294 ; Loss = 1.507187\n",
      "2024-12-11 03:19:29.874000: I runner.py:310] Step = 90200 ; steps/s = 1.61, tokens/s = 42639 (42639 target) ; Learning rate = 0.000294 ; Loss = 1.506205\n",
      "2024-12-11 03:20:31.842000: I runner.py:310] Step = 90300 ; steps/s = 1.61, tokens/s = 42671 (42671 target) ; Learning rate = 0.000294 ; Loss = 1.507734\n",
      "2024-12-11 03:21:33.417000: I runner.py:310] Step = 90400 ; steps/s = 1.62, tokens/s = 42168 (42168 target) ; Learning rate = 0.000294 ; Loss = 1.492852\n",
      "2024-12-11 03:22:35.411000: I runner.py:310] Step = 90500 ; steps/s = 1.61, tokens/s = 42658 (42658 target) ; Learning rate = 0.000294 ; Loss = 1.502210\n",
      "2024-12-11 03:23:37.413000: I runner.py:310] Step = 90600 ; steps/s = 1.61, tokens/s = 42637 (42637 target) ; Learning rate = 0.000294 ; Loss = 1.501485\n",
      "2024-12-11 03:24:39.406000: I runner.py:310] Step = 90700 ; steps/s = 1.61, tokens/s = 42701 (42701 target) ; Learning rate = 0.000293 ; Loss = 1.509612\n",
      "2024-12-11 03:25:40.969000: I runner.py:310] Step = 90800 ; steps/s = 1.62, tokens/s = 42135 (42135 target) ; Learning rate = 0.000293 ; Loss = 1.497466\n",
      "2024-12-11 03:26:42.959000: I runner.py:310] Step = 90900 ; steps/s = 1.61, tokens/s = 42647 (42647 target) ; Learning rate = 0.000293 ; Loss = 1.503550\n",
      "2024-12-11 03:27:44.940000: I runner.py:310] Step = 91000 ; steps/s = 1.61, tokens/s = 42644 (42644 target) ; Learning rate = 0.000293 ; Loss = 1.511310\n",
      "2024-12-11 03:28:46.922000: I runner.py:310] Step = 91100 ; steps/s = 1.61, tokens/s = 42719 (42719 target) ; Learning rate = 0.000293 ; Loss = 1.505744\n",
      "2024-12-11 03:29:48.484000: I runner.py:310] Step = 91200 ; steps/s = 1.62, tokens/s = 42163 (42163 target) ; Learning rate = 0.000293 ; Loss = 1.506936\n",
      "2024-12-11 03:30:50.503000: I runner.py:310] Step = 91300 ; steps/s = 1.61, tokens/s = 42657 (42657 target) ; Learning rate = 0.000293 ; Loss = 1.497826\n",
      "2024-12-11 03:31:52.491000: I runner.py:310] Step = 91400 ; steps/s = 1.61, tokens/s = 42661 (42661 target) ; Learning rate = 0.000292 ; Loss = 1.505438\n",
      "2024-12-11 03:32:54.443000: I runner.py:310] Step = 91500 ; steps/s = 1.61, tokens/s = 42676 (42676 target) ; Learning rate = 0.000292 ; Loss = 1.507794\n",
      "2024-12-11 03:33:56.084000: I runner.py:310] Step = 91600 ; steps/s = 1.62, tokens/s = 42115 (42115 target) ; Learning rate = 0.000292 ; Loss = 1.495122\n",
      "2024-12-11 03:34:58.063000: I runner.py:310] Step = 91700 ; steps/s = 1.61, tokens/s = 42661 (42661 target) ; Learning rate = 0.000292 ; Loss = 1.504592\n",
      "2024-12-11 03:36:00.092000: I runner.py:310] Step = 91800 ; steps/s = 1.61, tokens/s = 42665 (42665 target) ; Learning rate = 0.000292 ; Loss = 1.500668\n",
      "2024-12-11 03:37:02.118000: I runner.py:310] Step = 91900 ; steps/s = 1.61, tokens/s = 42596 (42596 target) ; Learning rate = 0.000292 ; Loss = 1.513268\n",
      "2024-12-11 03:38:03.664000: I runner.py:310] Step = 92000 ; steps/s = 1.62, tokens/s = 42158 (42158 target) ; Learning rate = 0.000291 ; Loss = 1.508236\n",
      "2024-12-11 03:39:05.609000: I runner.py:310] Step = 92100 ; steps/s = 1.61, tokens/s = 42666 (42666 target) ; Learning rate = 0.000291 ; Loss = 1.499728\n",
      "2024-12-11 03:40:07.621000: I runner.py:310] Step = 92200 ; steps/s = 1.61, tokens/s = 42654 (42654 target) ; Learning rate = 0.000291 ; Loss = 1.497445\n",
      "2024-12-11 03:41:09.635000: I runner.py:310] Step = 92300 ; steps/s = 1.61, tokens/s = 42673 (42673 target) ; Learning rate = 0.000291 ; Loss = 1.504386\n",
      "2024-12-11 03:42:11.185000: I runner.py:310] Step = 92400 ; steps/s = 1.62, tokens/s = 42185 (42185 target) ; Learning rate = 0.000291 ; Loss = 1.505398\n",
      "2024-12-11 03:43:13.161000: I runner.py:310] Step = 92500 ; steps/s = 1.61, tokens/s = 42640 (42640 target) ; Learning rate = 0.000291 ; Loss = 1.497949\n",
      "2024-12-11 03:44:15.204000: I runner.py:310] Step = 92600 ; steps/s = 1.61, tokens/s = 42645 (42645 target) ; Learning rate = 0.000290 ; Loss = 1.507524\n",
      "2024-12-11 03:45:17.193000: I runner.py:310] Step = 92700 ; steps/s = 1.61, tokens/s = 42687 (42687 target) ; Learning rate = 0.000290 ; Loss = 1.502646\n",
      "2024-12-11 03:46:18.776000: I runner.py:310] Step = 92800 ; steps/s = 1.62, tokens/s = 42130 (42130 target) ; Learning rate = 0.000290 ; Loss = 1.491647\n",
      "2024-12-11 03:47:20.764000: I runner.py:310] Step = 92900 ; steps/s = 1.61, tokens/s = 42688 (42688 target) ; Learning rate = 0.000290 ; Loss = 1.504513\n",
      "2024-12-11 03:48:22.756000: I runner.py:310] Step = 93000 ; steps/s = 1.61, tokens/s = 42622 (42622 target) ; Learning rate = 0.000290 ; Loss = 1.506358\n",
      "2024-12-11 03:49:24.775000: I runner.py:310] Step = 93100 ; steps/s = 1.61, tokens/s = 42673 (42673 target) ; Learning rate = 0.000290 ; Loss = 1.510120\n",
      "2024-12-11 03:50:26.320000: I runner.py:310] Step = 93200 ; steps/s = 1.63, tokens/s = 42177 (42177 target) ; Learning rate = 0.000290 ; Loss = 1.502722\n",
      "2024-12-11 03:51:28.350000: I runner.py:310] Step = 93300 ; steps/s = 1.61, tokens/s = 42628 (42628 target) ; Learning rate = 0.000289 ; Loss = 1.495194\n",
      "2024-12-11 03:52:30.365000: I runner.py:310] Step = 93400 ; steps/s = 1.61, tokens/s = 42646 (42646 target) ; Learning rate = 0.000289 ; Loss = 1.503728\n",
      "2024-12-11 03:53:32.408000: I runner.py:310] Step = 93500 ; steps/s = 1.61, tokens/s = 42615 (42615 target) ; Learning rate = 0.000289 ; Loss = 1.501208\n",
      "2024-12-11 03:54:33.977000: I runner.py:310] Step = 93600 ; steps/s = 1.62, tokens/s = 42159 (42159 target) ; Learning rate = 0.000289 ; Loss = 1.499693\n",
      "2024-12-11 03:55:35.975000: I runner.py:310] Step = 93700 ; steps/s = 1.61, tokens/s = 42689 (42689 target) ; Learning rate = 0.000289 ; Loss = 1.498960\n",
      "2024-12-11 03:56:38.007000: I runner.py:310] Step = 93800 ; steps/s = 1.61, tokens/s = 42595 (42595 target) ; Learning rate = 0.000289 ; Loss = 1.500738\n",
      "2024-12-11 03:57:39.918000: I runner.py:310] Step = 93900 ; steps/s = 1.62, tokens/s = 42515 (42515 target) ; Learning rate = 0.000288 ; Loss = 1.512969\n",
      "2024-12-11 03:58:41.607000: I runner.py:310] Step = 94000 ; steps/s = 1.62, tokens/s = 42294 (42294 target) ; Learning rate = 0.000288 ; Loss = 1.508617\n",
      "2024-12-11 03:59:43.613000: I runner.py:310] Step = 94100 ; steps/s = 1.61, tokens/s = 42650 (42650 target) ; Learning rate = 0.000288 ; Loss = 1.502779\n",
      "2024-12-11 04:00:45.626000: I runner.py:310] Step = 94200 ; steps/s = 1.61, tokens/s = 42660 (42660 target) ; Learning rate = 0.000288 ; Loss = 1.498138\n",
      "2024-12-11 04:01:47.210000: I runner.py:310] Step = 94300 ; steps/s = 1.62, tokens/s = 42096 (42096 target) ; Learning rate = 0.000288 ; Loss = 1.498284\n",
      "2024-12-11 04:02:49.161000: I runner.py:310] Step = 94400 ; steps/s = 1.61, tokens/s = 42719 (42719 target) ; Learning rate = 0.000288 ; Loss = 1.503976\n",
      "2024-12-11 04:03:51.163000: I runner.py:310] Step = 94500 ; steps/s = 1.61, tokens/s = 42671 (42671 target) ; Learning rate = 0.000288 ; Loss = 1.494737\n",
      "2024-12-11 04:04:53.184000: I runner.py:310] Step = 94600 ; steps/s = 1.61, tokens/s = 42618 (42618 target) ; Learning rate = 0.000287 ; Loss = 1.498882\n",
      "2024-12-11 04:05:54.768000: I runner.py:310] Step = 94700 ; steps/s = 1.62, tokens/s = 42132 (42132 target) ; Learning rate = 0.000287 ; Loss = 1.511003\n",
      "2024-12-11 04:06:56.749000: I runner.py:310] Step = 94800 ; steps/s = 1.61, tokens/s = 42657 (42657 target) ; Learning rate = 0.000287 ; Loss = 1.502064\n",
      "2024-12-11 04:07:58.766000: I runner.py:310] Step = 94900 ; steps/s = 1.61, tokens/s = 42639 (42639 target) ; Learning rate = 0.000287 ; Loss = 1.503017\n",
      "2024-12-11 04:09:00.802000: I runner.py:310] Step = 95000 ; steps/s = 1.61, tokens/s = 42653 (42653 target) ; Learning rate = 0.000287 ; Loss = 1.501056\n",
      "2024-12-11 04:09:00.805000: I training.py:192] Running evaluation for step 95000\n",
      "2024-12-11 04:13:31.438000: I training.py:192] Evaluation result for step 95000: loss = 1.153620 ; perplexity = 3.169647\n",
      "2024-12-11 04:14:32.802000: I runner.py:310] Step = 95100 ; steps/s = 1.63, tokens/s = 42305 (42305 target) ; Learning rate = 0.000287 ; Loss = 1.507494\n",
      "2024-12-11 04:15:34.822000: I runner.py:310] Step = 95200 ; steps/s = 1.61, tokens/s = 42639 (42639 target) ; Learning rate = 0.000286 ; Loss = 1.506915\n",
      "2024-12-11 04:16:36.833000: I runner.py:310] Step = 95300 ; steps/s = 1.61, tokens/s = 42648 (42648 target) ; Learning rate = 0.000286 ; Loss = 1.499317\n",
      "2024-12-11 04:17:38.889000: I runner.py:310] Step = 95400 ; steps/s = 1.61, tokens/s = 42613 (42613 target) ; Learning rate = 0.000286 ; Loss = 1.492572\n",
      "2024-12-11 04:18:40.487000: I runner.py:310] Step = 95500 ; steps/s = 1.62, tokens/s = 42142 (42142 target) ; Learning rate = 0.000286 ; Loss = 1.499667\n",
      "2024-12-11 04:19:42.444000: I runner.py:310] Step = 95600 ; steps/s = 1.61, tokens/s = 42720 (42720 target) ; Learning rate = 0.000286 ; Loss = 1.498918\n",
      "2024-12-11 04:20:44.477000: I runner.py:310] Step = 95700 ; steps/s = 1.61, tokens/s = 42631 (42631 target) ; Learning rate = 0.000286 ; Loss = 1.503373\n",
      "2024-12-11 04:21:46.551000: I runner.py:310] Step = 95800 ; steps/s = 1.61, tokens/s = 42612 (42612 target) ; Learning rate = 0.000286 ; Loss = 1.502598\n",
      "2024-12-11 04:22:48.140000: I runner.py:310] Step = 95900 ; steps/s = 1.62, tokens/s = 42107 (42107 target) ; Learning rate = 0.000285 ; Loss = 1.497625\n",
      "2024-12-11 04:23:50.197000: I runner.py:310] Step = 96000 ; steps/s = 1.61, tokens/s = 42668 (42668 target) ; Learning rate = 0.000285 ; Loss = 1.495137\n",
      "2024-12-11 04:24:52.267000: I runner.py:310] Step = 96100 ; steps/s = 1.61, tokens/s = 42584 (42584 target) ; Learning rate = 0.000285 ; Loss = 1.495556\n",
      "2024-12-11 04:25:54.279000: I runner.py:310] Step = 96200 ; steps/s = 1.61, tokens/s = 42632 (42632 target) ; Learning rate = 0.000285 ; Loss = 1.501294\n",
      "2024-12-11 04:26:55.837000: I runner.py:310] Step = 96300 ; steps/s = 1.62, tokens/s = 42159 (42159 target) ; Learning rate = 0.000285 ; Loss = 1.495841\n",
      "2024-12-11 04:27:57.889000: I runner.py:310] Step = 96400 ; steps/s = 1.61, tokens/s = 42614 (42614 target) ; Learning rate = 0.000285 ; Loss = 1.495133\n",
      "2024-12-11 04:28:59.906000: I runner.py:310] Step = 96500 ; steps/s = 1.61, tokens/s = 42641 (42641 target) ; Learning rate = 0.000285 ; Loss = 1.502882\n",
      "2024-12-11 04:30:01.882000: I runner.py:310] Step = 96600 ; steps/s = 1.61, tokens/s = 42671 (42671 target) ; Learning rate = 0.000284 ; Loss = 1.504082\n",
      "2024-12-11 04:31:03.410000: I runner.py:310] Step = 96700 ; steps/s = 1.63, tokens/s = 42199 (42199 target) ; Learning rate = 0.000284 ; Loss = 1.497366\n",
      "2024-12-11 04:32:05.471000: I runner.py:310] Step = 96800 ; steps/s = 1.61, tokens/s = 42585 (42585 target) ; Learning rate = 0.000284 ; Loss = 1.494804\n",
      "2024-12-11 04:33:07.538000: I runner.py:310] Step = 96900 ; steps/s = 1.61, tokens/s = 42620 (42620 target) ; Learning rate = 0.000284 ; Loss = 1.506558\n",
      "2024-12-11 04:34:09.543000: I runner.py:310] Step = 97000 ; steps/s = 1.61, tokens/s = 42654 (42654 target) ; Learning rate = 0.000284 ; Loss = 1.507170\n",
      "2024-12-11 04:35:11.119000: I runner.py:310] Step = 97100 ; steps/s = 1.62, tokens/s = 42160 (42160 target) ; Learning rate = 0.000284 ; Loss = 1.500248\n",
      "2024-12-11 04:36:13.100000: I runner.py:310] Step = 97200 ; steps/s = 1.61, tokens/s = 42670 (42670 target) ; Learning rate = 0.000284 ; Loss = 1.504441\n",
      "2024-12-11 04:37:15.079000: I runner.py:310] Step = 97300 ; steps/s = 1.61, tokens/s = 42674 (42674 target) ; Learning rate = 0.000283 ; Loss = 1.498993\n",
      "2024-12-11 04:38:17.127000: I runner.py:310] Step = 97400 ; steps/s = 1.61, tokens/s = 42621 (42621 target) ; Learning rate = 0.000283 ; Loss = 1.502254\n",
      "2024-12-11 04:39:18.702000: I runner.py:310] Step = 97500 ; steps/s = 1.62, tokens/s = 42147 (42147 target) ; Learning rate = 0.000283 ; Loss = 1.500096\n",
      "2024-12-11 04:40:20.727000: I runner.py:310] Step = 97600 ; steps/s = 1.61, tokens/s = 42657 (42657 target) ; Learning rate = 0.000283 ; Loss = 1.495941\n",
      "2024-12-11 04:41:22.778000: I runner.py:310] Step = 97700 ; steps/s = 1.61, tokens/s = 42613 (42613 target) ; Learning rate = 0.000283 ; Loss = 1.502220\n",
      "2024-12-11 04:42:24.796000: I runner.py:310] Step = 97800 ; steps/s = 1.61, tokens/s = 42631 (42631 target) ; Learning rate = 0.000283 ; Loss = 1.508954\n",
      "2024-12-11 04:43:26.356000: I runner.py:310] Step = 97900 ; steps/s = 1.62, tokens/s = 42167 (42167 target) ; Learning rate = 0.000282 ; Loss = 1.502557\n",
      "2024-12-11 04:44:28.437000: I runner.py:310] Step = 98000 ; steps/s = 1.61, tokens/s = 42595 (42595 target) ; Learning rate = 0.000282 ; Loss = 1.500172\n",
      "2024-12-11 04:45:30.422000: I runner.py:310] Step = 98100 ; steps/s = 1.61, tokens/s = 42684 (42684 target) ; Learning rate = 0.000282 ; Loss = 1.501995\n",
      "2024-12-11 04:46:32.423000: I runner.py:310] Step = 98200 ; steps/s = 1.61, tokens/s = 42669 (42669 target) ; Learning rate = 0.000282 ; Loss = 1.494515\n",
      "2024-12-11 04:47:33.994000: I runner.py:310] Step = 98300 ; steps/s = 1.62, tokens/s = 42105 (42105 target) ; Learning rate = 0.000282 ; Loss = 1.503085\n",
      "2024-12-11 04:48:36.045000: I runner.py:310] Step = 98400 ; steps/s = 1.61, tokens/s = 42623 (42623 target) ; Learning rate = 0.000282 ; Loss = 1.495066\n",
      "2024-12-11 04:49:38.045000: I runner.py:310] Step = 98500 ; steps/s = 1.61, tokens/s = 42684 (42684 target) ; Learning rate = 0.000282 ; Loss = 1.506342\n",
      "2024-12-11 04:50:40.009000: I runner.py:310] Step = 98600 ; steps/s = 1.61, tokens/s = 42667 (42667 target) ; Learning rate = 0.000281 ; Loss = 1.501502\n",
      "2024-12-11 04:51:41.573000: I runner.py:310] Step = 98700 ; steps/s = 1.62, tokens/s = 42153 (42153 target) ; Learning rate = 0.000281 ; Loss = 1.496673\n",
      "2024-12-11 04:52:43.573000: I runner.py:310] Step = 98800 ; steps/s = 1.61, tokens/s = 42670 (42670 target) ; Learning rate = 0.000281 ; Loss = 1.498474\n",
      "2024-12-11 04:53:45.542000: I runner.py:310] Step = 98900 ; steps/s = 1.61, tokens/s = 42684 (42684 target) ; Learning rate = 0.000281 ; Loss = 1.498493\n",
      "2024-12-11 04:54:47.509000: I runner.py:310] Step = 99000 ; steps/s = 1.61, tokens/s = 42684 (42684 target) ; Learning rate = 0.000281 ; Loss = 1.505046\n",
      "2024-12-11 04:55:49.118000: I runner.py:310] Step = 99100 ; steps/s = 1.62, tokens/s = 42090 (42090 target) ; Learning rate = 0.000281 ; Loss = 1.503945\n",
      "2024-12-11 04:56:51.177000: I runner.py:310] Step = 99200 ; steps/s = 1.61, tokens/s = 42630 (42630 target) ; Learning rate = 0.000281 ; Loss = 1.495656\n",
      "2024-12-11 04:57:53.231000: I runner.py:310] Step = 99300 ; steps/s = 1.61, tokens/s = 42638 (42638 target) ; Learning rate = 0.000280 ; Loss = 1.497438\n",
      "2024-12-11 04:58:55.315000: I runner.py:310] Step = 99400 ; steps/s = 1.61, tokens/s = 42586 (42586 target) ; Learning rate = 0.000280 ; Loss = 1.497502\n",
      "2024-12-11 04:59:56.893000: I runner.py:310] Step = 99500 ; steps/s = 1.62, tokens/s = 42152 (42152 target) ; Learning rate = 0.000280 ; Loss = 1.496295\n",
      "2024-12-11 05:00:58.922000: I runner.py:310] Step = 99600 ; steps/s = 1.61, tokens/s = 42625 (42625 target) ; Learning rate = 0.000280 ; Loss = 1.497610\n",
      "2024-12-11 05:02:00.985000: I runner.py:310] Step = 99700 ; steps/s = 1.61, tokens/s = 42621 (42621 target) ; Learning rate = 0.000280 ; Loss = 1.498918\n",
      "2024-12-11 05:03:03.020000: I runner.py:310] Step = 99800 ; steps/s = 1.61, tokens/s = 42620 (42620 target) ; Learning rate = 0.000280 ; Loss = 1.497441\n",
      "2024-12-11 05:04:04.623000: I runner.py:310] Step = 99900 ; steps/s = 1.62, tokens/s = 42141 (42141 target) ; Learning rate = 0.000280 ; Loss = 1.495673\n",
      "2024-12-11 05:05:06.695000: I runner.py:310] Step = 100000 ; steps/s = 1.61, tokens/s = 42627 (42627 target) ; Learning rate = 0.000280 ; Loss = 1.492954\n",
      "2024-12-11 05:05:08.951000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-100000\n",
      "2024-12-11 05:05:08.951000: I training.py:192] Running evaluation for step 100000\n",
      "2024-12-11 05:09:50.030000: I training.py:192] Evaluation result for step 100000: loss = 1.153766 ; perplexity = 3.170109\n",
      "2024-12-11 05:10:51.929000: I runner.py:310] Step = 100100 ; steps/s = 1.62, tokens/s = 42742 (42742 target) ; Learning rate = 0.000279 ; Loss = 1.503753\n",
      "2024-12-11 05:11:54.033000: I runner.py:310] Step = 100200 ; steps/s = 1.61, tokens/s = 42566 (42566 target) ; Learning rate = 0.000279 ; Loss = 1.509042\n",
      "2024-12-11 05:12:55.680000: I runner.py:310] Step = 100300 ; steps/s = 1.62, tokens/s = 42076 (42076 target) ; Learning rate = 0.000279 ; Loss = 1.507382\n",
      "2024-12-11 05:13:57.784000: I runner.py:310] Step = 100400 ; steps/s = 1.61, tokens/s = 42591 (42591 target) ; Learning rate = 0.000279 ; Loss = 1.493332\n",
      "2024-12-11 05:14:59.863000: I runner.py:310] Step = 100500 ; steps/s = 1.61, tokens/s = 42624 (42624 target) ; Learning rate = 0.000279 ; Loss = 1.494680\n",
      "2024-12-11 05:16:01.940000: I runner.py:310] Step = 100600 ; steps/s = 1.61, tokens/s = 42597 (42597 target) ; Learning rate = 0.000279 ; Loss = 1.492144\n",
      "2024-12-11 05:17:03.577000: I runner.py:310] Step = 100700 ; steps/s = 1.62, tokens/s = 42128 (42128 target) ; Learning rate = 0.000279 ; Loss = 1.493064\n",
      "2024-12-11 05:18:05.542000: I runner.py:310] Step = 100800 ; steps/s = 1.61, tokens/s = 42690 (42690 target) ; Learning rate = 0.000278 ; Loss = 1.503213\n",
      "2024-12-11 05:19:07.647000: I runner.py:310] Step = 100900 ; steps/s = 1.61, tokens/s = 42554 (42554 target) ; Learning rate = 0.000278 ; Loss = 1.500712\n",
      "2024-12-11 05:20:09.732000: I runner.py:310] Step = 101000 ; steps/s = 1.61, tokens/s = 42584 (42584 target) ; Learning rate = 0.000278 ; Loss = 1.499436\n",
      "2024-12-11 05:21:11.364000: I runner.py:310] Step = 101100 ; steps/s = 1.62, tokens/s = 42121 (42121 target) ; Learning rate = 0.000278 ; Loss = 1.498516\n",
      "2024-12-11 05:22:13.418000: I runner.py:310] Step = 101200 ; steps/s = 1.61, tokens/s = 42615 (42615 target) ; Learning rate = 0.000278 ; Loss = 1.499526\n",
      "2024-12-11 05:23:15.497000: I runner.py:310] Step = 101300 ; steps/s = 1.61, tokens/s = 42605 (42605 target) ; Learning rate = 0.000278 ; Loss = 1.497312\n",
      "2024-12-11 05:24:17.561000: I runner.py:310] Step = 101400 ; steps/s = 1.61, tokens/s = 42602 (42602 target) ; Learning rate = 0.000278 ; Loss = 1.498851\n",
      "2024-12-11 05:25:19.132000: I runner.py:310] Step = 101500 ; steps/s = 1.62, tokens/s = 42159 (42159 target) ; Learning rate = 0.000277 ; Loss = 1.492876\n",
      "2024-12-11 05:26:21.210000: I runner.py:310] Step = 101600 ; steps/s = 1.61, tokens/s = 42612 (42612 target) ; Learning rate = 0.000277 ; Loss = 1.501729\n",
      "2024-12-11 05:27:23.248000: I runner.py:310] Step = 101700 ; steps/s = 1.61, tokens/s = 42601 (42601 target) ; Learning rate = 0.000277 ; Loss = 1.507490\n",
      "2024-12-11 05:28:25.305000: I runner.py:310] Step = 101800 ; steps/s = 1.61, tokens/s = 42659 (42659 target) ; Learning rate = 0.000277 ; Loss = 1.501643\n",
      "2024-12-11 05:29:26.934000: I runner.py:310] Step = 101900 ; steps/s = 1.62, tokens/s = 42089 (42089 target) ; Learning rate = 0.000277 ; Loss = 1.502190\n",
      "2024-12-11 05:30:29.028000: I runner.py:310] Step = 102000 ; steps/s = 1.61, tokens/s = 42600 (42600 target) ; Learning rate = 0.000277 ; Loss = 1.498843\n",
      "2024-12-11 05:31:31.085000: I runner.py:310] Step = 102100 ; steps/s = 1.61, tokens/s = 42613 (42613 target) ; Learning rate = 0.000277 ; Loss = 1.497708\n",
      "2024-12-11 05:32:33.184000: I runner.py:310] Step = 102200 ; steps/s = 1.61, tokens/s = 42592 (42592 target) ; Learning rate = 0.000276 ; Loss = 1.497939\n",
      "2024-12-11 05:33:34.859000: I runner.py:310] Step = 102300 ; steps/s = 1.62, tokens/s = 42106 (42106 target) ; Learning rate = 0.000276 ; Loss = 1.496486\n",
      "2024-12-11 05:34:36.941000: I runner.py:310] Step = 102400 ; steps/s = 1.61, tokens/s = 42575 (42575 target) ; Learning rate = 0.000276 ; Loss = 1.502007\n",
      "2024-12-11 05:35:39.053000: I runner.py:310] Step = 102500 ; steps/s = 1.61, tokens/s = 42539 (42539 target) ; Learning rate = 0.000276 ; Loss = 1.504628\n",
      "2024-12-11 05:36:41.167000: I runner.py:310] Step = 102600 ; steps/s = 1.61, tokens/s = 42594 (42594 target) ; Learning rate = 0.000276 ; Loss = 1.501088\n",
      "2024-12-11 05:37:42.816000: I runner.py:310] Step = 102700 ; steps/s = 1.62, tokens/s = 42123 (42123 target) ; Learning rate = 0.000276 ; Loss = 1.499526\n",
      "2024-12-11 05:38:44.904000: I runner.py:310] Step = 102800 ; steps/s = 1.61, tokens/s = 42596 (42596 target) ; Learning rate = 0.000276 ; Loss = 1.492641\n",
      "2024-12-11 05:39:46.999000: I runner.py:310] Step = 102900 ; steps/s = 1.61, tokens/s = 42578 (42578 target) ; Learning rate = 0.000276 ; Loss = 1.498046\n",
      "2024-12-11 05:40:49.134000: I runner.py:310] Step = 103000 ; steps/s = 1.61, tokens/s = 42565 (42565 target) ; Learning rate = 0.000275 ; Loss = 1.494895\n",
      "2024-12-11 05:41:50.788000: I runner.py:310] Step = 103100 ; steps/s = 1.62, tokens/s = 42106 (42106 target) ; Learning rate = 0.000275 ; Loss = 1.493274\n",
      "2024-12-11 05:42:52.837000: I runner.py:310] Step = 103200 ; steps/s = 1.61, tokens/s = 42636 (42636 target) ; Learning rate = 0.000275 ; Loss = 1.503213\n",
      "2024-12-11 05:43:54.822000: I runner.py:310] Step = 103300 ; steps/s = 1.61, tokens/s = 42654 (42654 target) ; Learning rate = 0.000275 ; Loss = 1.499132\n",
      "2024-12-11 05:44:56.824000: I runner.py:310] Step = 103400 ; steps/s = 1.61, tokens/s = 42639 (42639 target) ; Learning rate = 0.000275 ; Loss = 1.505963\n",
      "2024-12-11 05:45:58.387000: I runner.py:310] Step = 103500 ; steps/s = 1.62, tokens/s = 42139 (42139 target) ; Learning rate = 0.000275 ; Loss = 1.504595\n",
      "2024-12-11 05:47:00.502000: I runner.py:310] Step = 103600 ; steps/s = 1.61, tokens/s = 42586 (42586 target) ; Learning rate = 0.000275 ; Loss = 1.497855\n",
      "2024-12-11 05:48:02.579000: I runner.py:310] Step = 103700 ; steps/s = 1.61, tokens/s = 42622 (42622 target) ; Learning rate = 0.000274 ; Loss = 1.500166\n",
      "2024-12-11 05:49:04.661000: I runner.py:310] Step = 103800 ; steps/s = 1.61, tokens/s = 42596 (42596 target) ; Learning rate = 0.000274 ; Loss = 1.496209\n",
      "2024-12-11 05:50:06.338000: I runner.py:310] Step = 103900 ; steps/s = 1.62, tokens/s = 42070 (42070 target) ; Learning rate = 0.000274 ; Loss = 1.493073\n",
      "2024-12-11 05:51:08.412000: I runner.py:310] Step = 104000 ; steps/s = 1.61, tokens/s = 42610 (42610 target) ; Learning rate = 0.000274 ; Loss = 1.496481\n",
      "2024-12-11 05:52:10.444000: I runner.py:310] Step = 104100 ; steps/s = 1.61, tokens/s = 42662 (42662 target) ; Learning rate = 0.000274 ; Loss = 1.501775\n",
      "2024-12-11 05:53:12.438000: I runner.py:310] Step = 104200 ; steps/s = 1.61, tokens/s = 42636 (42636 target) ; Learning rate = 0.000274 ; Loss = 1.505319\n",
      "2024-12-11 05:54:14.071000: I runner.py:310] Step = 104300 ; steps/s = 1.62, tokens/s = 42134 (42134 target) ; Learning rate = 0.000274 ; Loss = 1.501530\n",
      "2024-12-11 05:55:16.100000: I runner.py:310] Step = 104400 ; steps/s = 1.61, tokens/s = 42641 (42641 target) ; Learning rate = 0.000274 ; Loss = 1.495187\n",
      "2024-12-11 05:56:18.147000: I runner.py:310] Step = 104500 ; steps/s = 1.61, tokens/s = 42614 (42614 target) ; Learning rate = 0.000273 ; Loss = 1.498395\n",
      "2024-12-11 05:57:19.845000: I runner.py:310] Step = 104600 ; steps/s = 1.62, tokens/s = 42143 (42143 target) ; Learning rate = 0.000273 ; Loss = 1.505241\n",
      "2024-12-11 05:58:21.831000: I runner.py:310] Step = 104700 ; steps/s = 1.61, tokens/s = 42577 (42577 target) ; Learning rate = 0.000273 ; Loss = 1.501154\n",
      "2024-12-11 05:59:23.904000: I runner.py:310] Step = 104800 ; steps/s = 1.61, tokens/s = 42612 (42612 target) ; Learning rate = 0.000273 ; Loss = 1.497561\n",
      "2024-12-11 06:00:25.898000: I runner.py:310] Step = 104900 ; steps/s = 1.61, tokens/s = 42651 (42651 target) ; Learning rate = 0.000273 ; Loss = 1.498051\n",
      "2024-12-11 06:01:27.559000: I runner.py:310] Step = 105000 ; steps/s = 1.62, tokens/s = 42077 (42077 target) ; Learning rate = 0.000273 ; Loss = 1.498575\n",
      "2024-12-11 06:01:27.560000: I training.py:192] Running evaluation for step 105000\n",
      "2024-12-11 06:06:13.896000: I training.py:192] Evaluation result for step 105000: loss = 1.159358 ; perplexity = 3.187886\n",
      "2024-12-11 06:07:15.764000: I runner.py:310] Step = 105100 ; steps/s = 1.62, tokens/s = 42790 (42790 target) ; Learning rate = 0.000273 ; Loss = 1.493836\n",
      "2024-12-11 06:08:17.772000: I runner.py:310] Step = 105200 ; steps/s = 1.61, tokens/s = 42641 (42641 target) ; Learning rate = 0.000273 ; Loss = 1.505179\n",
      "2024-12-11 06:09:19.841000: I runner.py:310] Step = 105300 ; steps/s = 1.61, tokens/s = 42595 (42595 target) ; Learning rate = 0.000272 ; Loss = 1.499973\n",
      "2024-12-11 06:10:21.455000: I runner.py:310] Step = 105400 ; steps/s = 1.62, tokens/s = 42107 (42107 target) ; Learning rate = 0.000272 ; Loss = 1.499182\n",
      "2024-12-11 06:11:23.443000: I runner.py:310] Step = 105500 ; steps/s = 1.61, tokens/s = 42720 (42720 target) ; Learning rate = 0.000272 ; Loss = 1.503009\n",
      "2024-12-11 06:12:25.455000: I runner.py:310] Step = 105600 ; steps/s = 1.61, tokens/s = 42663 (42663 target) ; Learning rate = 0.000272 ; Loss = 1.495309\n",
      "2024-12-11 06:13:27.447000: I runner.py:310] Step = 105700 ; steps/s = 1.61, tokens/s = 42593 (42593 target) ; Learning rate = 0.000272 ; Loss = 1.503866\n",
      "2024-12-11 06:14:29.030000: I runner.py:310] Step = 105800 ; steps/s = 1.62, tokens/s = 42164 (42164 target) ; Learning rate = 0.000272 ; Loss = 1.497130\n",
      "2024-12-11 06:15:31.007000: I runner.py:310] Step = 105900 ; steps/s = 1.61, tokens/s = 42705 (42705 target) ; Learning rate = 0.000272 ; Loss = 1.495121\n",
      "2024-12-11 06:16:33.025000: I runner.py:310] Step = 106000 ; steps/s = 1.61, tokens/s = 42622 (42622 target) ; Learning rate = 0.000271 ; Loss = 1.500777\n",
      "2024-12-11 06:17:35.092000: I runner.py:310] Step = 106100 ; steps/s = 1.61, tokens/s = 42583 (42583 target) ; Learning rate = 0.000271 ; Loss = 1.500223\n",
      "2024-12-11 06:18:36.644000: I runner.py:310] Step = 106200 ; steps/s = 1.62, tokens/s = 42181 (42181 target) ; Learning rate = 0.000271 ; Loss = 1.503017\n",
      "2024-12-11 06:19:38.577000: I runner.py:310] Step = 106300 ; steps/s = 1.61, tokens/s = 42675 (42675 target) ; Learning rate = 0.000271 ; Loss = 1.498465\n",
      "2024-12-11 06:20:40.551000: I runner.py:310] Step = 106400 ; steps/s = 1.61, tokens/s = 42672 (42672 target) ; Learning rate = 0.000271 ; Loss = 1.495812\n",
      "2024-12-11 06:21:42.515000: I runner.py:310] Step = 106500 ; steps/s = 1.61, tokens/s = 42699 (42699 target) ; Learning rate = 0.000271 ; Loss = 1.488323\n",
      "2024-12-11 06:22:44.105000: I runner.py:310] Step = 106600 ; steps/s = 1.62, tokens/s = 42131 (42131 target) ; Learning rate = 0.000271 ; Loss = 1.494756\n",
      "2024-12-11 06:23:46.089000: I runner.py:310] Step = 106700 ; steps/s = 1.61, tokens/s = 42681 (42681 target) ; Learning rate = 0.000271 ; Loss = 1.498805\n",
      "2024-12-11 06:24:48.083000: I runner.py:310] Step = 106800 ; steps/s = 1.61, tokens/s = 42649 (42649 target) ; Learning rate = 0.000270 ; Loss = 1.510231\n",
      "2024-12-11 06:25:50.144000: I runner.py:310] Step = 106900 ; steps/s = 1.61, tokens/s = 42609 (42609 target) ; Learning rate = 0.000270 ; Loss = 1.493763\n",
      "2024-12-11 06:26:51.638000: I runner.py:310] Step = 107000 ; steps/s = 1.63, tokens/s = 42237 (42237 target) ; Learning rate = 0.000270 ; Loss = 1.500223\n",
      "2024-12-11 06:27:53.650000: I runner.py:310] Step = 107100 ; steps/s = 1.61, tokens/s = 42646 (42646 target) ; Learning rate = 0.000270 ; Loss = 1.495841\n",
      "2024-12-11 06:28:55.752000: I runner.py:310] Step = 107200 ; steps/s = 1.61, tokens/s = 42593 (42593 target) ; Learning rate = 0.000270 ; Loss = 1.501077\n",
      "2024-12-11 06:29:57.782000: I runner.py:310] Step = 107300 ; steps/s = 1.61, tokens/s = 42619 (42619 target) ; Learning rate = 0.000270 ; Loss = 1.495859\n",
      "2024-12-11 06:30:59.370000: I runner.py:310] Step = 107400 ; steps/s = 1.62, tokens/s = 42139 (42139 target) ; Learning rate = 0.000270 ; Loss = 1.494091\n",
      "2024-12-11 06:32:01.317000: I runner.py:310] Step = 107500 ; steps/s = 1.61, tokens/s = 42685 (42685 target) ; Learning rate = 0.000270 ; Loss = 1.495293\n",
      "2024-12-11 06:33:03.291000: I runner.py:310] Step = 107600 ; steps/s = 1.61, tokens/s = 42640 (42640 target) ; Learning rate = 0.000269 ; Loss = 1.501752\n",
      "2024-12-11 06:34:05.237000: I runner.py:310] Step = 107700 ; steps/s = 1.61, tokens/s = 42689 (42689 target) ; Learning rate = 0.000269 ; Loss = 1.497712\n",
      "2024-12-11 06:35:06.747000: I runner.py:310] Step = 107800 ; steps/s = 1.63, tokens/s = 42226 (42226 target) ; Learning rate = 0.000269 ; Loss = 1.501416\n",
      "2024-12-11 06:36:08.739000: I runner.py:310] Step = 107900 ; steps/s = 1.61, tokens/s = 42657 (42657 target) ; Learning rate = 0.000269 ; Loss = 1.495123\n",
      "2024-12-11 06:37:10.728000: I runner.py:310] Step = 108000 ; steps/s = 1.61, tokens/s = 42655 (42655 target) ; Learning rate = 0.000269 ; Loss = 1.496849\n",
      "2024-12-11 06:38:12.708000: I runner.py:310] Step = 108100 ; steps/s = 1.61, tokens/s = 42654 (42654 target) ; Learning rate = 0.000269 ; Loss = 1.493553\n",
      "2024-12-11 06:39:14.307000: I runner.py:310] Step = 108200 ; steps/s = 1.62, tokens/s = 42160 (42160 target) ; Learning rate = 0.000269 ; Loss = 1.499844\n",
      "2024-12-11 06:40:16.266000: I runner.py:310] Step = 108300 ; steps/s = 1.61, tokens/s = 42685 (42685 target) ; Learning rate = 0.000269 ; Loss = 1.497113\n",
      "2024-12-11 06:41:18.281000: I runner.py:310] Step = 108400 ; steps/s = 1.61, tokens/s = 42648 (42648 target) ; Learning rate = 0.000268 ; Loss = 1.494615\n",
      "2024-12-11 06:42:20.306000: I runner.py:310] Step = 108500 ; steps/s = 1.61, tokens/s = 42665 (42665 target) ; Learning rate = 0.000268 ; Loss = 1.495763\n",
      "2024-12-11 06:43:21.902000: I runner.py:310] Step = 108600 ; steps/s = 1.62, tokens/s = 42101 (42101 target) ; Learning rate = 0.000268 ; Loss = 1.500904\n",
      "2024-12-11 06:44:23.871000: I runner.py:310] Step = 108700 ; steps/s = 1.61, tokens/s = 42657 (42657 target) ; Learning rate = 0.000268 ; Loss = 1.497835\n",
      "2024-12-11 06:45:25.871000: I runner.py:310] Step = 108800 ; steps/s = 1.61, tokens/s = 42710 (42710 target) ; Learning rate = 0.000268 ; Loss = 1.496046\n",
      "2024-12-11 06:46:27.851000: I runner.py:310] Step = 108900 ; steps/s = 1.61, tokens/s = 42661 (42661 target) ; Learning rate = 0.000268 ; Loss = 1.496125\n",
      "2024-12-11 06:47:29.428000: I runner.py:310] Step = 109000 ; steps/s = 1.62, tokens/s = 42127 (42127 target) ; Learning rate = 0.000268 ; Loss = 1.503851\n",
      "2024-12-11 06:48:31.451000: I runner.py:310] Step = 109100 ; steps/s = 1.61, tokens/s = 42643 (42643 target) ; Learning rate = 0.000268 ; Loss = 1.493806\n",
      "2024-12-11 06:49:33.445000: I runner.py:310] Step = 109200 ; steps/s = 1.61, tokens/s = 42674 (42674 target) ; Learning rate = 0.000267 ; Loss = 1.486882\n",
      "2024-12-11 06:50:35.461000: I runner.py:310] Step = 109300 ; steps/s = 1.61, tokens/s = 42630 (42630 target) ; Learning rate = 0.000267 ; Loss = 1.491299\n",
      "2024-12-11 06:51:37.027000: I runner.py:310] Step = 109400 ; steps/s = 1.62, tokens/s = 42160 (42160 target) ; Learning rate = 0.000267 ; Loss = 1.501083\n",
      "2024-12-11 06:52:38.946000: I runner.py:310] Step = 109500 ; steps/s = 1.62, tokens/s = 42743 (42743 target) ; Learning rate = 0.000267 ; Loss = 1.493547\n",
      "2024-12-11 06:53:40.949000: I runner.py:310] Step = 109600 ; steps/s = 1.61, tokens/s = 42645 (42645 target) ; Learning rate = 0.000267 ; Loss = 1.491544\n",
      "2024-12-11 06:54:42.921000: I runner.py:310] Step = 109700 ; steps/s = 1.61, tokens/s = 42653 (42653 target) ; Learning rate = 0.000267 ; Loss = 1.498897\n",
      "2024-12-11 06:55:44.542000: I runner.py:310] Step = 109800 ; steps/s = 1.62, tokens/s = 42100 (42100 target) ; Learning rate = 0.000267 ; Loss = 1.500700\n",
      "2024-12-11 06:56:46.542000: I runner.py:310] Step = 109900 ; steps/s = 1.61, tokens/s = 42689 (42689 target) ; Learning rate = 0.000267 ; Loss = 1.494268\n",
      "2024-12-11 06:57:48.524000: I runner.py:310] Step = 110000 ; steps/s = 1.61, tokens/s = 42636 (42636 target) ; Learning rate = 0.000266 ; Loss = 1.497740\n",
      "2024-12-11 06:57:50.713000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-110000\n",
      "2024-12-11 06:57:50.713000: I training.py:192] Running evaluation for step 110000\n",
      "2024-12-11 07:02:20.683000: I training.py:192] Evaluation result for step 110000: loss = 1.160698 ; perplexity = 3.192160\n",
      "2024-12-11 07:03:22.488000: I runner.py:310] Step = 110100 ; steps/s = 1.62, tokens/s = 42786 (42786 target) ; Learning rate = 0.000266 ; Loss = 1.497344\n",
      "2024-12-11 07:04:24.063000: I runner.py:310] Step = 110200 ; steps/s = 1.62, tokens/s = 42177 (42177 target) ; Learning rate = 0.000266 ; Loss = 1.494253\n",
      "2024-12-11 07:05:26.107000: I runner.py:310] Step = 110300 ; steps/s = 1.61, tokens/s = 42608 (42608 target) ; Learning rate = 0.000266 ; Loss = 1.492740\n",
      "2024-12-11 07:06:28.063000: I runner.py:310] Step = 110400 ; steps/s = 1.61, tokens/s = 42693 (42693 target) ; Learning rate = 0.000266 ; Loss = 1.490935\n",
      "2024-12-11 07:07:30.051000: I runner.py:310] Step = 110500 ; steps/s = 1.61, tokens/s = 42685 (42685 target) ; Learning rate = 0.000266 ; Loss = 1.500982\n",
      "2024-12-11 07:08:31.538000: I runner.py:310] Step = 110600 ; steps/s = 1.63, tokens/s = 42216 (42216 target) ; Learning rate = 0.000266 ; Loss = 1.494363\n",
      "2024-12-11 07:09:33.487000: I runner.py:310] Step = 110700 ; steps/s = 1.61, tokens/s = 42706 (42706 target) ; Learning rate = 0.000266 ; Loss = 1.496438\n",
      "2024-12-11 07:10:35.503000: I runner.py:310] Step = 110800 ; steps/s = 1.61, tokens/s = 42652 (42652 target) ; Learning rate = 0.000266 ; Loss = 1.494497\n",
      "2024-12-11 07:11:37.520000: I runner.py:310] Step = 110900 ; steps/s = 1.61, tokens/s = 42627 (42627 target) ; Learning rate = 0.000265 ; Loss = 1.492165\n",
      "2024-12-11 07:12:39.117000: I runner.py:310] Step = 111000 ; steps/s = 1.62, tokens/s = 42123 (42123 target) ; Learning rate = 0.000265 ; Loss = 1.497128\n",
      "2024-12-11 07:13:41.068000: I runner.py:310] Step = 111100 ; steps/s = 1.61, tokens/s = 42678 (42678 target) ; Learning rate = 0.000265 ; Loss = 1.491292\n",
      "2024-12-11 07:14:43.065000: I runner.py:310] Step = 111200 ; steps/s = 1.61, tokens/s = 42635 (42635 target) ; Learning rate = 0.000265 ; Loss = 1.491007\n",
      "2024-12-11 07:15:45.102000: I runner.py:310] Step = 111300 ; steps/s = 1.61, tokens/s = 42629 (42629 target) ; Learning rate = 0.000265 ; Loss = 1.492129\n",
      "2024-12-11 07:16:46.645000: I runner.py:310] Step = 111400 ; steps/s = 1.63, tokens/s = 42239 (42239 target) ; Learning rate = 0.000265 ; Loss = 1.500181\n",
      "2024-12-11 07:17:48.627000: I runner.py:310] Step = 111500 ; steps/s = 1.61, tokens/s = 42668 (42668 target) ; Learning rate = 0.000265 ; Loss = 1.493181\n",
      "2024-12-11 07:18:50.594000: I runner.py:310] Step = 111600 ; steps/s = 1.61, tokens/s = 42689 (42689 target) ; Learning rate = 0.000265 ; Loss = 1.499835\n",
      "2024-12-11 07:19:52.574000: I runner.py:310] Step = 111700 ; steps/s = 1.61, tokens/s = 42644 (42644 target) ; Learning rate = 0.000264 ; Loss = 1.497899\n",
      "2024-12-11 07:20:54.266000: I runner.py:310] Step = 111800 ; steps/s = 1.62, tokens/s = 42044 (42044 target) ; Learning rate = 0.000264 ; Loss = 1.483644\n",
      "2024-12-11 07:21:56.279000: I runner.py:310] Step = 111900 ; steps/s = 1.61, tokens/s = 42634 (42634 target) ; Learning rate = 0.000264 ; Loss = 1.498366\n",
      "2024-12-11 07:22:58.285000: I runner.py:310] Step = 112000 ; steps/s = 1.61, tokens/s = 42651 (42651 target) ; Learning rate = 0.000264 ; Loss = 1.493502\n",
      "2024-12-11 07:24:00.354000: I runner.py:310] Step = 112100 ; steps/s = 1.61, tokens/s = 42628 (42628 target) ; Learning rate = 0.000264 ; Loss = 1.496132\n",
      "2024-12-11 07:25:01.988000: I runner.py:310] Step = 112200 ; steps/s = 1.62, tokens/s = 42104 (42104 target) ; Learning rate = 0.000264 ; Loss = 1.491438\n",
      "2024-12-11 07:26:03.987000: I runner.py:310] Step = 112300 ; steps/s = 1.61, tokens/s = 42653 (42653 target) ; Learning rate = 0.000264 ; Loss = 1.495596\n",
      "2024-12-11 07:27:05.946000: I runner.py:310] Step = 112400 ; steps/s = 1.61, tokens/s = 42701 (42701 target) ; Learning rate = 0.000264 ; Loss = 1.491895\n",
      "2024-12-11 07:28:07.970000: I runner.py:310] Step = 112500 ; steps/s = 1.61, tokens/s = 42648 (42648 target) ; Learning rate = 0.000264 ; Loss = 1.494564\n",
      "2024-12-11 07:29:09.522000: I runner.py:310] Step = 112600 ; steps/s = 1.62, tokens/s = 42148 (42148 target) ; Learning rate = 0.000263 ; Loss = 1.490928\n",
      "2024-12-11 07:30:11.492000: I runner.py:310] Step = 112700 ; steps/s = 1.61, tokens/s = 42659 (42659 target) ; Learning rate = 0.000263 ; Loss = 1.493175\n",
      "2024-12-11 07:31:13.494000: I runner.py:310] Step = 112800 ; steps/s = 1.61, tokens/s = 42663 (42663 target) ; Learning rate = 0.000263 ; Loss = 1.493947\n",
      "2024-12-11 07:32:15.479000: I runner.py:310] Step = 112900 ; steps/s = 1.61, tokens/s = 42671 (42671 target) ; Learning rate = 0.000263 ; Loss = 1.499641\n",
      "2024-12-11 07:33:17.011000: I runner.py:310] Step = 113000 ; steps/s = 1.63, tokens/s = 42206 (42206 target) ; Learning rate = 0.000263 ; Loss = 1.497917\n",
      "2024-12-11 07:34:19.008000: I runner.py:310] Step = 113100 ; steps/s = 1.61, tokens/s = 42644 (42644 target) ; Learning rate = 0.000263 ; Loss = 1.486338\n",
      "2024-12-11 07:35:21.004000: I runner.py:310] Step = 113200 ; steps/s = 1.61, tokens/s = 42656 (42656 target) ; Learning rate = 0.000263 ; Loss = 1.493077\n",
      "2024-12-11 07:36:22.968000: I runner.py:310] Step = 113300 ; steps/s = 1.61, tokens/s = 42669 (42669 target) ; Learning rate = 0.000263 ; Loss = 1.494042\n",
      "2024-12-11 07:37:24.561000: I runner.py:310] Step = 113400 ; steps/s = 1.62, tokens/s = 42164 (42164 target) ; Learning rate = 0.000262 ; Loss = 1.484983\n",
      "2024-12-11 07:38:26.572000: I runner.py:310] Step = 113500 ; steps/s = 1.61, tokens/s = 42630 (42630 target) ; Learning rate = 0.000262 ; Loss = 1.495136\n",
      "2024-12-11 07:39:28.575000: I runner.py:310] Step = 113600 ; steps/s = 1.61, tokens/s = 42648 (42648 target) ; Learning rate = 0.000262 ; Loss = 1.493667\n",
      "2024-12-11 07:40:30.595000: I runner.py:310] Step = 113700 ; steps/s = 1.61, tokens/s = 42644 (42644 target) ; Learning rate = 0.000262 ; Loss = 1.496885\n",
      "2024-12-11 07:41:32.133000: I runner.py:310] Step = 113800 ; steps/s = 1.63, tokens/s = 42187 (42187 target) ; Learning rate = 0.000262 ; Loss = 1.484668\n",
      "2024-12-11 07:42:34.101000: I runner.py:310] Step = 113900 ; steps/s = 1.61, tokens/s = 42709 (42709 target) ; Learning rate = 0.000262 ; Loss = 1.493493\n",
      "2024-12-11 07:43:36.094000: I runner.py:310] Step = 114000 ; steps/s = 1.61, tokens/s = 42638 (42638 target) ; Learning rate = 0.000262 ; Loss = 1.499144\n",
      "2024-12-11 07:44:38.158000: I runner.py:310] Step = 114100 ; steps/s = 1.61, tokens/s = 42597 (42597 target) ; Learning rate = 0.000262 ; Loss = 1.494727\n",
      "2024-12-11 07:45:39.694000: I runner.py:310] Step = 114200 ; steps/s = 1.63, tokens/s = 42190 (42190 target) ; Learning rate = 0.000262 ; Loss = 1.491075\n",
      "2024-12-11 07:46:41.728000: I runner.py:310] Step = 114300 ; steps/s = 1.61, tokens/s = 42653 (42653 target) ; Learning rate = 0.000261 ; Loss = 1.492224\n",
      "2024-12-11 07:47:43.719000: I runner.py:310] Step = 114400 ; steps/s = 1.61, tokens/s = 42640 (42640 target) ; Learning rate = 0.000261 ; Loss = 1.498655\n",
      "2024-12-11 07:48:45.682000: I runner.py:310] Step = 114500 ; steps/s = 1.61, tokens/s = 42663 (42663 target) ; Learning rate = 0.000261 ; Loss = 1.496626\n",
      "2024-12-11 07:49:47.197000: I runner.py:310] Step = 114600 ; steps/s = 1.63, tokens/s = 42209 (42209 target) ; Learning rate = 0.000261 ; Loss = 1.487879\n",
      "2024-12-11 07:50:49.215000: I runner.py:310] Step = 114700 ; steps/s = 1.61, tokens/s = 42626 (42626 target) ; Learning rate = 0.000261 ; Loss = 1.494044\n",
      "2024-12-11 07:51:51.194000: I runner.py:310] Step = 114800 ; steps/s = 1.61, tokens/s = 42669 (42669 target) ; Learning rate = 0.000261 ; Loss = 1.490224\n",
      "2024-12-11 07:52:52.944000: I runner.py:310] Step = 114900 ; steps/s = 1.62, tokens/s = 42449 (42449 target) ; Learning rate = 0.000261 ; Loss = 1.491980\n",
      "2024-12-11 07:53:54.639000: I runner.py:310] Step = 115000 ; steps/s = 1.62, tokens/s = 42449 (42449 target) ; Learning rate = 0.000261 ; Loss = 1.488662\n",
      "2024-12-11 07:53:54.641000: I training.py:192] Running evaluation for step 115000\n",
      "2024-12-11 07:58:29.923000: I training.py:192] Evaluation result for step 115000: loss = 1.165886 ; perplexity = 3.208764\n",
      "2024-12-11 07:59:31.861000: I runner.py:310] Step = 115100 ; steps/s = 1.62, tokens/s = 42712 (42712 target) ; Learning rate = 0.000261 ; Loss = 1.492675\n",
      "2024-12-11 08:00:33.943000: I runner.py:310] Step = 115200 ; steps/s = 1.61, tokens/s = 42618 (42618 target) ; Learning rate = 0.000260 ; Loss = 1.496538\n",
      "2024-12-11 08:01:35.464000: I runner.py:310] Step = 115300 ; steps/s = 1.63, tokens/s = 42154 (42154 target) ; Learning rate = 0.000260 ; Loss = 1.491051\n",
      "2024-12-11 08:02:37.439000: I runner.py:310] Step = 115400 ; steps/s = 1.61, tokens/s = 42682 (42682 target) ; Learning rate = 0.000260 ; Loss = 1.496691\n",
      "2024-12-11 08:03:39.470000: I runner.py:310] Step = 115500 ; steps/s = 1.61, tokens/s = 42659 (42659 target) ; Learning rate = 0.000260 ; Loss = 1.492290\n",
      "2024-12-11 08:04:41.557000: I runner.py:310] Step = 115600 ; steps/s = 1.61, tokens/s = 42600 (42600 target) ; Learning rate = 0.000260 ; Loss = 1.494723\n",
      "2024-12-11 08:05:43.164000: I runner.py:310] Step = 115700 ; steps/s = 1.62, tokens/s = 42094 (42094 target) ; Learning rate = 0.000260 ; Loss = 1.491962\n",
      "2024-12-11 08:06:45.145000: I runner.py:310] Step = 115800 ; steps/s = 1.61, tokens/s = 42663 (42663 target) ; Learning rate = 0.000260 ; Loss = 1.490723\n",
      "2024-12-11 08:07:47.113000: I runner.py:310] Step = 115900 ; steps/s = 1.61, tokens/s = 42673 (42673 target) ; Learning rate = 0.000260 ; Loss = 1.494732\n",
      "2024-12-11 08:08:49.104000: I runner.py:310] Step = 116000 ; steps/s = 1.61, tokens/s = 42655 (42655 target) ; Learning rate = 0.000260 ; Loss = 1.495624\n",
      "2024-12-11 08:09:50.708000: I runner.py:310] Step = 116100 ; steps/s = 1.62, tokens/s = 42156 (42156 target) ; Learning rate = 0.000259 ; Loss = 1.488443\n",
      "2024-12-11 08:10:52.690000: I runner.py:310] Step = 116200 ; steps/s = 1.61, tokens/s = 42675 (42675 target) ; Learning rate = 0.000259 ; Loss = 1.490339\n",
      "2024-12-11 08:11:54.672000: I runner.py:310] Step = 116300 ; steps/s = 1.61, tokens/s = 42673 (42673 target) ; Learning rate = 0.000259 ; Loss = 1.494618\n",
      "2024-12-11 08:12:56.671000: I runner.py:310] Step = 116400 ; steps/s = 1.61, tokens/s = 42648 (42648 target) ; Learning rate = 0.000259 ; Loss = 1.494811\n",
      "2024-12-11 08:13:58.286000: I runner.py:310] Step = 116500 ; steps/s = 1.62, tokens/s = 42123 (42123 target) ; Learning rate = 0.000259 ; Loss = 1.492369\n",
      "2024-12-11 08:15:00.267000: I runner.py:310] Step = 116600 ; steps/s = 1.61, tokens/s = 42672 (42672 target) ; Learning rate = 0.000259 ; Loss = 1.485383\n",
      "2024-12-11 08:16:02.224000: I runner.py:310] Step = 116700 ; steps/s = 1.61, tokens/s = 42694 (42694 target) ; Learning rate = 0.000259 ; Loss = 1.493773\n",
      "2024-12-11 08:17:04.162000: I runner.py:310] Step = 116800 ; steps/s = 1.61, tokens/s = 42713 (42713 target) ; Learning rate = 0.000259 ; Loss = 1.499643\n",
      "2024-12-11 08:18:05.735000: I runner.py:310] Step = 116900 ; steps/s = 1.62, tokens/s = 42117 (42117 target) ; Learning rate = 0.000259 ; Loss = 1.497509\n",
      "2024-12-11 08:19:07.767000: I runner.py:310] Step = 117000 ; steps/s = 1.61, tokens/s = 42661 (42661 target) ; Learning rate = 0.000258 ; Loss = 1.494463\n",
      "2024-12-11 08:20:09.814000: I runner.py:310] Step = 117100 ; steps/s = 1.61, tokens/s = 42603 (42603 target) ; Learning rate = 0.000258 ; Loss = 1.485551\n",
      "2024-12-11 08:21:11.885000: I runner.py:310] Step = 117200 ; steps/s = 1.61, tokens/s = 42592 (42592 target) ; Learning rate = 0.000258 ; Loss = 1.495209\n",
      "2024-12-11 08:22:13.438000: I runner.py:310] Step = 117300 ; steps/s = 1.62, tokens/s = 42194 (42194 target) ; Learning rate = 0.000258 ; Loss = 1.488893\n",
      "2024-12-11 08:23:15.424000: I runner.py:310] Step = 117400 ; steps/s = 1.61, tokens/s = 42660 (42660 target) ; Learning rate = 0.000258 ; Loss = 1.487315\n",
      "2024-12-11 08:24:17.439000: I runner.py:310] Step = 117500 ; steps/s = 1.61, tokens/s = 42679 (42679 target) ; Learning rate = 0.000258 ; Loss = 1.489331\n",
      "2024-12-11 08:25:19.467000: I runner.py:310] Step = 117600 ; steps/s = 1.61, tokens/s = 42623 (42623 target) ; Learning rate = 0.000258 ; Loss = 1.490188\n",
      "2024-12-11 08:26:21.122000: I runner.py:310] Step = 117700 ; steps/s = 1.62, tokens/s = 42076 (42076 target) ; Learning rate = 0.000258 ; Loss = 1.489514\n",
      "2024-12-11 08:27:23.103000: I runner.py:310] Step = 117800 ; steps/s = 1.61, tokens/s = 42657 (42657 target) ; Learning rate = 0.000258 ; Loss = 1.488652\n",
      "2024-12-11 08:28:25.115000: I runner.py:310] Step = 117900 ; steps/s = 1.61, tokens/s = 42663 (42663 target) ; Learning rate = 0.000257 ; Loss = 1.494114\n",
      "2024-12-11 08:29:27.244000: I runner.py:310] Step = 118000 ; steps/s = 1.61, tokens/s = 42567 (42567 target) ; Learning rate = 0.000257 ; Loss = 1.496832\n",
      "2024-12-11 08:30:28.789000: I runner.py:310] Step = 118100 ; steps/s = 1.62, tokens/s = 42172 (42172 target) ; Learning rate = 0.000257 ; Loss = 1.496181\n",
      "2024-12-11 08:31:30.764000: I runner.py:310] Step = 118200 ; steps/s = 1.61, tokens/s = 42685 (42685 target) ; Learning rate = 0.000257 ; Loss = 1.492762\n",
      "2024-12-11 08:32:32.783000: I runner.py:310] Step = 118300 ; steps/s = 1.61, tokens/s = 42633 (42633 target) ; Learning rate = 0.000257 ; Loss = 1.492460\n",
      "2024-12-11 08:33:34.767000: I runner.py:310] Step = 118400 ; steps/s = 1.61, tokens/s = 42681 (42681 target) ; Learning rate = 0.000257 ; Loss = 1.493005\n",
      "2024-12-11 08:34:36.404000: I runner.py:310] Step = 118500 ; steps/s = 1.62, tokens/s = 42095 (42095 target) ; Learning rate = 0.000257 ; Loss = 1.489645\n",
      "2024-12-11 08:35:38.415000: I runner.py:310] Step = 118600 ; steps/s = 1.61, tokens/s = 42659 (42659 target) ; Learning rate = 0.000257 ; Loss = 1.488877\n",
      "2024-12-11 08:36:40.387000: I runner.py:310] Step = 118700 ; steps/s = 1.61, tokens/s = 42661 (42661 target) ; Learning rate = 0.000257 ; Loss = 1.492631\n",
      "2024-12-11 08:37:42.360000: I runner.py:310] Step = 118800 ; steps/s = 1.61, tokens/s = 42670 (42670 target) ; Learning rate = 0.000256 ; Loss = 1.495254\n",
      "2024-12-11 08:38:43.902000: I runner.py:310] Step = 118900 ; steps/s = 1.63, tokens/s = 42159 (42159 target) ; Learning rate = 0.000256 ; Loss = 1.486121\n",
      "2024-12-11 08:39:45.898000: I runner.py:310] Step = 119000 ; steps/s = 1.61, tokens/s = 42687 (42687 target) ; Learning rate = 0.000256 ; Loss = 1.488273\n",
      "2024-12-11 08:40:47.956000: I runner.py:310] Step = 119100 ; steps/s = 1.61, tokens/s = 42627 (42627 target) ; Learning rate = 0.000256 ; Loss = 1.490864\n",
      "2024-12-11 08:41:49.929000: I runner.py:310] Step = 119200 ; steps/s = 1.61, tokens/s = 42651 (42651 target) ; Learning rate = 0.000256 ; Loss = 1.494686\n",
      "2024-12-11 08:42:51.544000: I runner.py:310] Step = 119300 ; steps/s = 1.62, tokens/s = 42129 (42129 target) ; Learning rate = 0.000256 ; Loss = 1.488407\n",
      "2024-12-11 08:43:53.599000: I runner.py:310] Step = 119400 ; steps/s = 1.61, tokens/s = 42592 (42592 target) ; Learning rate = 0.000256 ; Loss = 1.491166\n",
      "2024-12-11 08:44:55.655000: I runner.py:310] Step = 119500 ; steps/s = 1.61, tokens/s = 42623 (42623 target) ; Learning rate = 0.000256 ; Loss = 1.492080\n",
      "2024-12-11 08:45:57.684000: I runner.py:310] Step = 119600 ; steps/s = 1.61, tokens/s = 42653 (42653 target) ; Learning rate = 0.000256 ; Loss = 1.489409\n",
      "2024-12-11 08:46:59.278000: I runner.py:310] Step = 119700 ; steps/s = 1.62, tokens/s = 42141 (42141 target) ; Learning rate = 0.000255 ; Loss = 1.490134\n",
      "2024-12-11 08:48:01.259000: I runner.py:310] Step = 119800 ; steps/s = 1.61, tokens/s = 42678 (42678 target) ; Learning rate = 0.000255 ; Loss = 1.490618\n",
      "2024-12-11 08:49:03.223000: I runner.py:310] Step = 119900 ; steps/s = 1.61, tokens/s = 42674 (42674 target) ; Learning rate = 0.000255 ; Loss = 1.495953\n",
      "2024-12-11 08:50:05.248000: I runner.py:310] Step = 120000 ; steps/s = 1.61, tokens/s = 42616 (42616 target) ; Learning rate = 0.000255 ; Loss = 1.497300\n",
      "2024-12-11 08:50:07.448000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-120000\n",
      "2024-12-11 08:50:07.448000: I training.py:192] Running evaluation for step 120000\n",
      "2024-12-11 08:54:37.954000: I training.py:192] Evaluation result for step 120000: loss = 1.167061 ; perplexity = 3.212536\n",
      "2024-12-11 08:55:39.360000: I runner.py:310] Step = 120100 ; steps/s = 1.63, tokens/s = 42303 (42303 target) ; Learning rate = 0.000255 ; Loss = 1.497187\n",
      "2024-12-11 08:56:41.357000: I runner.py:310] Step = 120200 ; steps/s = 1.61, tokens/s = 42630 (42630 target) ; Learning rate = 0.000255 ; Loss = 1.490197\n",
      "2024-12-11 08:57:43.330000: I runner.py:310] Step = 120300 ; steps/s = 1.61, tokens/s = 42675 (42675 target) ; Learning rate = 0.000255 ; Loss = 1.486921\n",
      "2024-12-11 08:58:45.356000: I runner.py:310] Step = 120400 ; steps/s = 1.61, tokens/s = 42641 (42641 target) ; Learning rate = 0.000255 ; Loss = 1.488897\n",
      "2024-12-11 08:59:46.965000: I runner.py:310] Step = 120500 ; steps/s = 1.62, tokens/s = 42137 (42137 target) ; Learning rate = 0.000255 ; Loss = 1.486217\n",
      "2024-12-11 09:00:48.920000: I runner.py:310] Step = 120600 ; steps/s = 1.61, tokens/s = 42697 (42697 target) ; Learning rate = 0.000255 ; Loss = 1.492845\n",
      "2024-12-11 09:01:50.972000: I runner.py:310] Step = 120700 ; steps/s = 1.61, tokens/s = 42629 (42629 target) ; Learning rate = 0.000254 ; Loss = 1.493837\n",
      "2024-12-11 09:02:52.996000: I runner.py:310] Step = 120800 ; steps/s = 1.61, tokens/s = 42630 (42630 target) ; Learning rate = 0.000254 ; Loss = 1.498082\n",
      "2024-12-11 09:03:54.485000: I runner.py:310] Step = 120900 ; steps/s = 1.63, tokens/s = 42205 (42205 target) ; Learning rate = 0.000254 ; Loss = 1.489351\n",
      "2024-12-11 09:04:56.489000: I runner.py:310] Step = 121000 ; steps/s = 1.61, tokens/s = 42656 (42656 target) ; Learning rate = 0.000254 ; Loss = 1.491969\n",
      "2024-12-11 09:05:58.446000: I runner.py:310] Step = 121100 ; steps/s = 1.61, tokens/s = 42681 (42681 target) ; Learning rate = 0.000254 ; Loss = 1.489514\n",
      "2024-12-11 09:07:00.502000: I runner.py:310] Step = 121200 ; steps/s = 1.61, tokens/s = 42597 (42597 target) ; Learning rate = 0.000254 ; Loss = 1.492271\n",
      "2024-12-11 09:08:02.072000: I runner.py:310] Step = 121300 ; steps/s = 1.62, tokens/s = 42170 (42170 target) ; Learning rate = 0.000254 ; Loss = 1.485925\n",
      "2024-12-11 09:09:04.056000: I runner.py:310] Step = 121400 ; steps/s = 1.61, tokens/s = 42655 (42655 target) ; Learning rate = 0.000254 ; Loss = 1.488486\n",
      "2024-12-11 09:10:06.081000: I runner.py:310] Step = 121500 ; steps/s = 1.61, tokens/s = 42609 (42609 target) ; Learning rate = 0.000254 ; Loss = 1.491619\n",
      "2024-12-11 09:11:08.081000: I runner.py:310] Step = 121600 ; steps/s = 1.61, tokens/s = 42698 (42698 target) ; Learning rate = 0.000253 ; Loss = 1.502303\n",
      "2024-12-11 09:12:09.682000: I runner.py:310] Step = 121700 ; steps/s = 1.62, tokens/s = 42152 (42152 target) ; Learning rate = 0.000253 ; Loss = 1.495098\n",
      "2024-12-11 09:13:11.678000: I runner.py:310] Step = 121800 ; steps/s = 1.61, tokens/s = 42637 (42637 target) ; Learning rate = 0.000253 ; Loss = 1.486194\n",
      "2024-12-11 09:14:13.631000: I runner.py:310] Step = 121900 ; steps/s = 1.61, tokens/s = 42712 (42712 target) ; Learning rate = 0.000253 ; Loss = 1.492005\n",
      "2024-12-11 09:15:15.654000: I runner.py:310] Step = 122000 ; steps/s = 1.61, tokens/s = 42638 (42638 target) ; Learning rate = 0.000253 ; Loss = 1.488313\n",
      "2024-12-11 09:16:17.122000: I runner.py:310] Step = 122100 ; steps/s = 1.63, tokens/s = 42215 (42215 target) ; Learning rate = 0.000253 ; Loss = 1.497632\n",
      "2024-12-11 09:17:19.096000: I runner.py:310] Step = 122200 ; steps/s = 1.61, tokens/s = 42668 (42668 target) ; Learning rate = 0.000253 ; Loss = 1.483898\n",
      "2024-12-11 09:18:21.044000: I runner.py:310] Step = 122300 ; steps/s = 1.61, tokens/s = 42680 (42680 target) ; Learning rate = 0.000253 ; Loss = 1.488004\n",
      "2024-12-11 09:19:23.080000: I runner.py:310] Step = 122400 ; steps/s = 1.61, tokens/s = 42616 (42616 target) ; Learning rate = 0.000253 ; Loss = 1.491777\n",
      "2024-12-11 09:20:24.661000: I runner.py:310] Step = 122500 ; steps/s = 1.62, tokens/s = 42141 (42141 target) ; Learning rate = 0.000253 ; Loss = 1.497819\n",
      "2024-12-11 09:21:26.620000: I runner.py:310] Step = 122600 ; steps/s = 1.61, tokens/s = 42709 (42709 target) ; Learning rate = 0.000252 ; Loss = 1.491549\n",
      "2024-12-11 09:22:28.594000: I runner.py:310] Step = 122700 ; steps/s = 1.61, tokens/s = 42685 (42685 target) ; Learning rate = 0.000252 ; Loss = 1.488550\n",
      "2024-12-11 09:23:30.600000: I runner.py:310] Step = 122800 ; steps/s = 1.61, tokens/s = 42647 (42647 target) ; Learning rate = 0.000252 ; Loss = 1.490327\n",
      "2024-12-11 09:24:32.065000: I runner.py:310] Step = 122900 ; steps/s = 1.63, tokens/s = 42227 (42227 target) ; Learning rate = 0.000252 ; Loss = 1.492450\n",
      "2024-12-11 09:25:34.049000: I runner.py:310] Step = 123000 ; steps/s = 1.61, tokens/s = 42650 (42650 target) ; Learning rate = 0.000252 ; Loss = 1.486668\n",
      "2024-12-11 09:26:36.045000: I runner.py:310] Step = 123100 ; steps/s = 1.61, tokens/s = 42674 (42674 target) ; Learning rate = 0.000252 ; Loss = 1.486050\n",
      "2024-12-11 09:27:38.013000: I runner.py:310] Step = 123200 ; steps/s = 1.61, tokens/s = 42665 (42665 target) ; Learning rate = 0.000252 ; Loss = 1.492080\n",
      "2024-12-11 09:28:39.558000: I runner.py:310] Step = 123300 ; steps/s = 1.63, tokens/s = 42200 (42200 target) ; Learning rate = 0.000252 ; Loss = 1.494544\n",
      "2024-12-11 09:29:41.528000: I runner.py:310] Step = 123400 ; steps/s = 1.61, tokens/s = 42690 (42690 target) ; Learning rate = 0.000252 ; Loss = 1.484493\n",
      "2024-12-11 09:30:43.556000: I runner.py:310] Step = 123500 ; steps/s = 1.61, tokens/s = 42613 (42613 target) ; Learning rate = 0.000252 ; Loss = 1.490043\n",
      "2024-12-11 09:31:45.541000: I runner.py:310] Step = 123600 ; steps/s = 1.61, tokens/s = 42662 (42662 target) ; Learning rate = 0.000251 ; Loss = 1.487742\n",
      "2024-12-11 09:32:47.086000: I runner.py:310] Step = 123700 ; steps/s = 1.62, tokens/s = 42160 (42160 target) ; Learning rate = 0.000251 ; Loss = 1.495697\n",
      "2024-12-11 09:33:49.079000: I runner.py:310] Step = 123800 ; steps/s = 1.61, tokens/s = 42671 (42671 target) ; Learning rate = 0.000251 ; Loss = 1.484301\n",
      "2024-12-11 09:34:51.096000: I runner.py:310] Step = 123900 ; steps/s = 1.61, tokens/s = 42630 (42630 target) ; Learning rate = 0.000251 ; Loss = 1.491015\n",
      "2024-12-11 09:35:53.082000: I runner.py:310] Step = 124000 ; steps/s = 1.61, tokens/s = 42685 (42685 target) ; Learning rate = 0.000251 ; Loss = 1.487382\n",
      "2024-12-11 09:36:54.643000: I runner.py:310] Step = 124100 ; steps/s = 1.62, tokens/s = 42174 (42174 target) ; Learning rate = 0.000251 ; Loss = 1.498240\n",
      "2024-12-11 09:37:56.628000: I runner.py:310] Step = 124200 ; steps/s = 1.61, tokens/s = 42671 (42671 target) ; Learning rate = 0.000251 ; Loss = 1.491823\n",
      "2024-12-11 09:38:58.648000: I runner.py:310] Step = 124300 ; steps/s = 1.61, tokens/s = 42646 (42646 target) ; Learning rate = 0.000251 ; Loss = 1.485347\n",
      "2024-12-11 09:40:00.652000: I runner.py:310] Step = 124400 ; steps/s = 1.61, tokens/s = 42627 (42627 target) ; Learning rate = 0.000251 ; Loss = 1.487227\n",
      "2024-12-11 09:41:02.155000: I runner.py:310] Step = 124500 ; steps/s = 1.63, tokens/s = 42174 (42174 target) ; Learning rate = 0.000251 ; Loss = 1.499616\n",
      "2024-12-11 09:42:04.202000: I runner.py:310] Step = 124600 ; steps/s = 1.61, tokens/s = 42651 (42651 target) ; Learning rate = 0.000250 ; Loss = 1.485706\n",
      "2024-12-11 09:43:06.137000: I runner.py:310] Step = 124700 ; steps/s = 1.61, tokens/s = 42695 (42695 target) ; Learning rate = 0.000250 ; Loss = 1.489098\n",
      "2024-12-11 09:44:08.164000: I runner.py:310] Step = 124800 ; steps/s = 1.61, tokens/s = 42629 (42629 target) ; Learning rate = 0.000250 ; Loss = 1.489768\n",
      "2024-12-11 09:45:09.739000: I runner.py:310] Step = 124900 ; steps/s = 1.62, tokens/s = 42191 (42191 target) ; Learning rate = 0.000250 ; Loss = 1.491851\n",
      "2024-12-11 09:46:11.730000: I runner.py:310] Step = 125000 ; steps/s = 1.61, tokens/s = 42640 (42640 target) ; Learning rate = 0.000250 ; Loss = 1.489041\n",
      "2024-12-11 09:46:11.731000: I training.py:192] Running evaluation for step 125000\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Tr-En (TED2020)(POS Tags) -> Kk-En (POS Tags)\n",
    "!onmt-main --model kk-tr-en-modelim.py --config kk-tr-en-pos-2.yml --auto_config --checkpoint_path POS_TR_KK_EN/ckpt-100000 train --with_eval --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1caece96-9fd7-4686-8a2d-6cbe039d7826",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-11 09:46:47.783269: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-11 09:46:48.933978: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-11 09:46:48.934076: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-11 09:46:48.934088: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-11 09:46:50.573000: I main.py:308] Loading model description from POS_TR_KK_EN-2/model_description.py\n",
      "2024-12-11 09:46:50.775000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-11 09:46:50.775000: I main.py:315] Using model:\n",
      "(model): MultiFeaturesTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): ParallelInputter(\n",
      "      (inputters): ListWrapper(\n",
      "        (0): WordEmbedder()\n",
      "        (1): WordEmbedder()\n",
      "      )\n",
      "      (reducer): ConcatReducer()\n",
      "    )\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): ParallelInputter(\n",
      "        (inputters): ListWrapper(\n",
      "          (0): WordEmbedder()\n",
      "          (1): WordEmbedder()\n",
      "        )\n",
      "        (reducer): ConcatReducer()\n",
      "      )\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-11 09:46:50.782000: I main.py:340] Using parameters:\n",
      "data:\n",
      "  eval_features_file:\n",
      "  - KK_tokens_valid_shared\n",
      "  - KK_pos_tags_valid_shared.txt\n",
      "  eval_labels_file: KK_valid_target_tokens_shared.txt\n",
      "  source_1_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  source_2_vocabulary: RoBERTa_KK_unique_pos\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file:\n",
      "  - KK_tokens_train_shared\n",
      "  - KK_pos_tags_train_shared.txt\n",
      "  train_labels_file: KK_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: POS_TR_KK_EN-2\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 205000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-11 09:46:50.975276: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-11 09:46:51.608545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-11 09:46:51.767000: I inputter.py:316] Initialized source_1 input layer:\n",
      "2024-12-11 09:46:51.767000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-11 09:46:51.767000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-11 09:46:51.772000: I inputter.py:316] Initialized source_2 input layer:\n",
      "2024-12-11 09:46:51.772000: I inputter.py:316]  - vocabulary size: 21\n",
      "2024-12-11 09:46:51.772000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-11 09:46:51.845000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-11 09:46:51.845000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-11 09:46:51.845000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-11 09:46:51.868000: I runner.py:462] Restored checkpoint POS_TR_KK_EN-2/ckpt-100000\n",
      "2024-12-11 09:46:51.912000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-11 09:46:52.668966: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-11 09:46:52.795000: I runner.py:471] Tracing and optimizing the inference graph...\n",
      "2024-12-11 09:47:06.519977: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-11 09:47:07.386674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-11 09:47:18.211000: I runner.py:471] 895 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-11 09:47:28.794000: I runner.py:471] 1695 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-11 09:47:38.871000: I runner.py:471] 2527 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-11 09:47:49.876000: I runner.py:471] 3263 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-11 09:47:59.907000: I runner.py:471] 4223 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-11 09:48:10.412000: I runner.py:471] 4831 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-11 09:48:20.684000: I runner.py:471] 5727 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-11 09:48:30.923000: I runner.py:471] 6431 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-11 09:48:41.459000: I runner.py:471] 7103 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-11 09:48:52.042000: I runner.py:471] 7999 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-11 09:49:02.909000: I runner.py:471] 8927 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-11 09:49:13.383000: I runner.py:471] 9759 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-11 09:49:23.906000: I runner.py:471] 10495 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-11 09:49:34.114000: I runner.py:471] 11295 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-11 09:49:49.869000: I runner.py:471] 12046 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-11 09:50:00.203000: I runner.py:471] 12750 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-11 09:50:10.624000: I runner.py:471] 13614 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-11 09:50:20.939000: I runner.py:471] 14478 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-11 09:50:31.073000: I runner.py:471] 15278 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-11 09:50:41.078000: I runner.py:471] 16046 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-11 09:50:51.144000: I runner.py:471] 16782 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-11 09:51:01.699000: I runner.py:471] 17262 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-11 09:51:12.292000: I runner.py:471] 17861 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-11 09:51:23.337000: I runner.py:471] 17948 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-11 09:51:37.102000: I runner.py:471] 17803 predictions are buffered, but waiting for the prediction of queued line 281 to advance the output...\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 onmt-main --config kk-tr-en-pos-2.yml --auto_config --checkpoint_path POS_TR_KK_EN-2/ckpt-100000 infer --features_file KK_tokens_test_shared KK_pos_tags_test_shared.txt --predictions_file output_tr_kk_en_pos.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa86b92-877e-4dde-bd2e-9e16038b71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 MT-Preparation/subwording/3-desubword.py en_shared_vocab.model output_tr_kk_en_pos.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25bc6d2e-2da1-4044-9cd5-545299bd702d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference first sentence: In the developed world, this figure is 35 25%\n",
      "Translated first sentence: In developed countries of the world , this figure is 35-25%\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "BLEU2:  BLEU = 50.30 70.0/54.4/44.7/37.6 (BP = 1.000 ratio = 1.161 hyp_len = 480977 ref_len = 414303)\n",
      "CHRF:  chrF2 = 76.38\n"
     ]
    }
   ],
   "source": [
    "# BLEU and chrF scores\n",
    "!python3 compute-bleu.py en_test_shuffled.txt-filtered.en output_tr_kk_en_pos.txt.desubword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1753970-ef96-4558-8a7f-1941f7e78b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama METEOR Puan: 0.7624199636634235\n"
     ]
    }
   ],
   "source": [
    "# Average METEOR score (Ortalama METEOR Puan)\n",
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "def read_and_tokenize_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return [nltk.word_tokenize(line.strip()) for line in lines]\n",
    "\n",
    "def calculate_meteor(reference_file, hypothesis_file):\n",
    "    references = read_and_tokenize_file(reference_file)\n",
    "    hypotheses = read_and_tokenize_file(hypothesis_file)\n",
    "    \n",
    "    if len(references) != len(hypotheses):\n",
    "        raise ValueError(\"Dosyalarn satr saylar elemiyor\")\n",
    "\n",
    "    total_meteor_score = 0.0\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        total_meteor_score += meteor_score([ref], hyp)\n",
    "\n",
    "    average_meteor_score = total_meteor_score / len(references)\n",
    "    return average_meteor_score\n",
    "\n",
    "reference_file = 'en_test_shuffled.txt-filtered.en'\n",
    "hypothesis_file = 'output_tr_kk_en_pos.txt.desubword'\n",
    "\n",
    "score = calculate_meteor(reference_file, hypothesis_file)\n",
    "print(f\"Ortalama METEOR Puan: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db9fe25c-2d71-40d1-ad1a-573ce32e7b94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-07 20:14:22.507252: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-07 20:14:23.340587: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-07 20:14:23.340652: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-07 20:14:23.340660: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-07 20:14:24.397000: I onmt-main:8] Creating model directory POS_KK_TR_EN\n",
      "2024-12-07 20:14:24.595000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-07 20:14:24.595000: I main.py:315] Using model:\n",
      "(model): MultiFeaturesTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): ParallelInputter(\n",
      "      (inputters): ListWrapper(\n",
      "        (0): WordEmbedder()\n",
      "        (1): WordEmbedder()\n",
      "      )\n",
      "      (reducer): ConcatReducer()\n",
      "    )\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): ParallelInputter(\n",
      "        (inputters): ListWrapper(\n",
      "          (0): WordEmbedder()\n",
      "          (1): WordEmbedder()\n",
      "        )\n",
      "        (reducer): ConcatReducer()\n",
      "      )\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-07 20:14:24.598722: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-07 20:14:26.143380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-07 20:14:26.144055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7795 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-12-07 20:14:26.144600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 6099 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "2024-12-07 20:14:26.148000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file:\n",
      "  - KK_tokens_valid_shared\n",
      "  - KK_pos_tags_valid_shared.txt\n",
      "  eval_labels_file: KK_valid_target_tokens_shared.txt\n",
      "  source_1_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  source_2_vocabulary: RoBERTa_KK_unique_pos\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file:\n",
      "  - KK_tokens_train_shared\n",
      "  - KK_pos_tags_train_shared.txt\n",
      "  train_labels_file: KK_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: POS_KK_TR_EN\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 205000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-07 20:14:26.480000: I inputter.py:316] Initialized source_1 input layer:\n",
      "2024-12-07 20:14:26.480000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-07 20:14:26.481000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-07 20:14:26.484000: I inputter.py:316] Initialized source_2 input layer:\n",
      "2024-12-07 20:14:26.484000: I inputter.py:316]  - vocabulary size: 21\n",
      "2024-12-07 20:14:26.484000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-07 20:14:26.562000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-07 20:14:26.562000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-07 20:14:26.562000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-07 20:14:26.567000: W runner.py:269] No checkpoint to restore in POS_KK_TR_EN\n",
      "2024-12-07 20:14:26.569000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "2024-12-07 20:14:26.620000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-07 20:14:27.628751: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-07 20:14:27.764000: I main.py:325] Accumulate gradients of 7 iterations to reach effective batch size of 25000\n",
      "2024-12-07 20:14:27.897000: I mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2024-12-07 20:14:28.421000: I dataset_ops.py:2542] Training on 318032 examples\n",
      "2024-12-07 20:15:33.741717: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 65538048 exceeds 10% of free system memory.\n",
      "2024-12-07 20:15:33.748125: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 65538048 exceeds 10% of free system memory.\n",
      "2024-12-07 20:15:33.776042: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 65538048 exceeds 10% of free system memory.\n",
      "2024-12-07 20:15:33.784646: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 65538048 exceeds 10% of free system memory.\n",
      "2024-12-07 20:15:33.927087: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 65538048 exceeds 10% of free system memory.\n",
      "2024-12-07 20:15:35.455191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-07 20:15:36.524904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-07 20:15:36.780173: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-07 20:15:46.198000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-07 20:15:46.219000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-07 20:15:47.772000: I cross_device_ops.py:897] batch_all_reduce: 261 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-07 20:15:52.734000: I cross_device_ops.py:897] batch_all_reduce: 261 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-07 20:15:59.440000: I runner.py:310] Number of model parameters: 93357261\n",
      "2024-12-07 20:15:59.444000: I runner.py:310] Number of model weights: 261 (trainable = 261, non trainable = 0)\n",
      "2024-12-07 20:15:59.480000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-07 20:16:01.655000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-1\n",
      "2024-12-07 20:16:02.202000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-07 20:16:02.225000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-07 20:16:02.876000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-07 20:16:02.899000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-07 20:16:03.496000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-07 20:16:03.520000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-07 20:16:04.109000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-07 20:17:03.531000: I runner.py:310] Step = 100 ; steps/s = 1.60, tokens/s = 42410 (42410 target) ; Learning rate = 0.000009 ; Loss = 9.741229\n",
      "2024-12-07 20:18:07.360000: I runner.py:310] Step = 200 ; steps/s = 1.57, tokens/s = 41437 (41437 target) ; Learning rate = 0.000018 ; Loss = 8.952544\n",
      "2024-12-07 20:19:10.316000: I runner.py:310] Step = 300 ; steps/s = 1.59, tokens/s = 42006 (42006 target) ; Learning rate = 0.000027 ; Loss = 8.013070\n",
      "2024-12-07 20:20:12.722000: I runner.py:310] Step = 400 ; steps/s = 1.60, tokens/s = 41567 (41567 target) ; Learning rate = 0.000035 ; Loss = 7.353461\n",
      "2024-12-07 20:21:14.663000: I runner.py:310] Step = 500 ; steps/s = 1.61, tokens/s = 42695 (42695 target) ; Learning rate = 0.000044 ; Loss = 6.981738\n",
      "2024-12-07 20:22:16.414000: I runner.py:310] Step = 600 ; steps/s = 1.62, tokens/s = 42848 (42848 target) ; Learning rate = 0.000053 ; Loss = 6.706898\n",
      "2024-12-07 20:23:18.086000: I runner.py:310] Step = 700 ; steps/s = 1.62, tokens/s = 42881 (42881 target) ; Learning rate = 0.000062 ; Loss = 6.400028\n",
      "2024-12-07 20:24:19.536000: I runner.py:310] Step = 800 ; steps/s = 1.63, tokens/s = 42234 (42234 target) ; Learning rate = 0.000071 ; Loss = 6.086067\n",
      "2024-12-07 20:25:21.196000: I runner.py:310] Step = 900 ; steps/s = 1.62, tokens/s = 42921 (42921 target) ; Learning rate = 0.000080 ; Loss = 5.925841\n",
      "2024-12-07 20:26:22.831000: I runner.py:310] Step = 1000 ; steps/s = 1.62, tokens/s = 42909 (42909 target) ; Learning rate = 0.000088 ; Loss = 5.798011\n",
      "2024-12-07 20:27:24.520000: I runner.py:310] Step = 1100 ; steps/s = 1.62, tokens/s = 42843 (42843 target) ; Learning rate = 0.000097 ; Loss = 5.672926\n",
      "2024-12-07 20:28:25.713000: I runner.py:310] Step = 1200 ; steps/s = 1.63, tokens/s = 42399 (42399 target) ; Learning rate = 0.000106 ; Loss = 5.579026\n",
      "2024-12-07 20:29:27.342000: I runner.py:310] Step = 1300 ; steps/s = 1.62, tokens/s = 42936 (42936 target) ; Learning rate = 0.000115 ; Loss = 5.416812\n",
      "2024-12-07 20:30:28.963000: I runner.py:310] Step = 1400 ; steps/s = 1.62, tokens/s = 42928 (42928 target) ; Learning rate = 0.000124 ; Loss = 5.356210\n",
      "2024-12-07 20:31:30.563000: I runner.py:310] Step = 1500 ; steps/s = 1.62, tokens/s = 42908 (42908 target) ; Learning rate = 0.000133 ; Loss = 5.152258\n",
      "2024-12-07 20:32:31.680000: I runner.py:310] Step = 1600 ; steps/s = 1.64, tokens/s = 42463 (42463 target) ; Learning rate = 0.000142 ; Loss = 5.080559\n",
      "2024-12-07 20:33:33.318000: I runner.py:310] Step = 1700 ; steps/s = 1.62, tokens/s = 42912 (42912 target) ; Learning rate = 0.000150 ; Loss = 4.952680\n",
      "2024-12-07 20:34:34.994000: I runner.py:310] Step = 1800 ; steps/s = 1.62, tokens/s = 42854 (42854 target) ; Learning rate = 0.000159 ; Loss = 4.918290\n",
      "2024-12-07 20:35:36.595000: I runner.py:310] Step = 1900 ; steps/s = 1.62, tokens/s = 42940 (42940 target) ; Learning rate = 0.000168 ; Loss = 4.824439\n",
      "2024-12-07 20:36:37.693000: I runner.py:310] Step = 2000 ; steps/s = 1.64, tokens/s = 42499 (42499 target) ; Learning rate = 0.000177 ; Loss = 4.744052\n",
      "2024-12-07 20:37:39.288000: I runner.py:310] Step = 2100 ; steps/s = 1.62, tokens/s = 42951 (42951 target) ; Learning rate = 0.000186 ; Loss = 4.737990\n",
      "2024-12-07 20:38:40.914000: I runner.py:310] Step = 2200 ; steps/s = 1.62, tokens/s = 42871 (42871 target) ; Learning rate = 0.000195 ; Loss = 4.558025\n",
      "2024-12-07 20:39:42.564000: I runner.py:310] Step = 2300 ; steps/s = 1.62, tokens/s = 42903 (42903 target) ; Learning rate = 0.000203 ; Loss = 4.443030\n",
      "2024-12-07 20:40:43.737000: I runner.py:310] Step = 2400 ; steps/s = 1.63, tokens/s = 42456 (42456 target) ; Learning rate = 0.000212 ; Loss = 4.301792\n",
      "2024-12-07 20:41:45.333000: I runner.py:310] Step = 2500 ; steps/s = 1.62, tokens/s = 42925 (42925 target) ; Learning rate = 0.000221 ; Loss = 4.192680\n",
      "2024-12-07 20:42:46.988000: I runner.py:310] Step = 2600 ; steps/s = 1.62, tokens/s = 42914 (42914 target) ; Learning rate = 0.000230 ; Loss = 4.045869\n",
      "2024-12-07 20:43:48.647000: I runner.py:310] Step = 2700 ; steps/s = 1.62, tokens/s = 42872 (42872 target) ; Learning rate = 0.000239 ; Loss = 3.961405\n",
      "2024-12-07 20:44:49.776000: I runner.py:310] Step = 2800 ; steps/s = 1.64, tokens/s = 42474 (42474 target) ; Learning rate = 0.000248 ; Loss = 3.710571\n",
      "2024-12-07 20:45:51.385000: I runner.py:310] Step = 2900 ; steps/s = 1.62, tokens/s = 42901 (42901 target) ; Learning rate = 0.000256 ; Loss = 3.674640\n",
      "2024-12-07 20:46:53.076000: I runner.py:310] Step = 3000 ; steps/s = 1.62, tokens/s = 42896 (42896 target) ; Learning rate = 0.000265 ; Loss = 3.481898\n",
      "2024-12-07 20:47:54.831000: I runner.py:310] Step = 3100 ; steps/s = 1.62, tokens/s = 42838 (42838 target) ; Learning rate = 0.000274 ; Loss = 3.354308\n",
      "2024-12-07 20:48:56.075000: I runner.py:310] Step = 3200 ; steps/s = 1.63, tokens/s = 42345 (42345 target) ; Learning rate = 0.000283 ; Loss = 3.279054\n",
      "2024-12-07 20:49:57.795000: I runner.py:310] Step = 3300 ; steps/s = 1.62, tokens/s = 42850 (42850 target) ; Learning rate = 0.000292 ; Loss = 3.262821\n",
      "2024-12-07 20:50:59.455000: I runner.py:310] Step = 3400 ; steps/s = 1.62, tokens/s = 42882 (42882 target) ; Learning rate = 0.000301 ; Loss = 3.175765\n",
      "2024-12-07 20:52:01.123000: I runner.py:310] Step = 3500 ; steps/s = 1.62, tokens/s = 42898 (42898 target) ; Learning rate = 0.000309 ; Loss = 3.097658\n",
      "2024-12-07 20:53:02.388000: I runner.py:310] Step = 3600 ; steps/s = 1.63, tokens/s = 42372 (42372 target) ; Learning rate = 0.000318 ; Loss = 2.993086\n",
      "2024-12-07 20:54:04.074000: I runner.py:310] Step = 3700 ; steps/s = 1.62, tokens/s = 42861 (42861 target) ; Learning rate = 0.000327 ; Loss = 2.985908\n",
      "2024-12-07 20:55:05.766000: I runner.py:310] Step = 3800 ; steps/s = 1.62, tokens/s = 42897 (42897 target) ; Learning rate = 0.000336 ; Loss = 2.954632\n",
      "2024-12-07 20:56:07.471000: I runner.py:310] Step = 3900 ; steps/s = 1.62, tokens/s = 42853 (42853 target) ; Learning rate = 0.000345 ; Loss = 2.854665\n",
      "2024-12-07 20:57:08.747000: I runner.py:310] Step = 4000 ; steps/s = 1.63, tokens/s = 42353 (42353 target) ; Learning rate = 0.000354 ; Loss = 2.874072\n",
      "2024-12-07 20:58:10.414000: I runner.py:310] Step = 4100 ; steps/s = 1.62, tokens/s = 42881 (42881 target) ; Learning rate = 0.000362 ; Loss = 2.844027\n",
      "2024-12-07 20:59:12.147000: I runner.py:310] Step = 4200 ; steps/s = 1.62, tokens/s = 42827 (42827 target) ; Learning rate = 0.000371 ; Loss = 2.720231\n",
      "2024-12-07 21:00:13.834000: I runner.py:310] Step = 4300 ; steps/s = 1.62, tokens/s = 42861 (42861 target) ; Learning rate = 0.000380 ; Loss = 2.683294\n",
      "2024-12-07 21:01:15.079000: I runner.py:310] Step = 4400 ; steps/s = 1.63, tokens/s = 42392 (42392 target) ; Learning rate = 0.000389 ; Loss = 2.623137\n",
      "2024-12-07 21:02:16.769000: I runner.py:310] Step = 4500 ; steps/s = 1.62, tokens/s = 42883 (42883 target) ; Learning rate = 0.000398 ; Loss = 2.643838\n",
      "2024-12-07 21:03:18.469000: I runner.py:310] Step = 4600 ; steps/s = 1.62, tokens/s = 42864 (42864 target) ; Learning rate = 0.000407 ; Loss = 2.649066\n",
      "2024-12-07 21:04:20.167000: I runner.py:310] Step = 4700 ; steps/s = 1.62, tokens/s = 42863 (42863 target) ; Learning rate = 0.000416 ; Loss = 2.627963\n",
      "2024-12-07 21:05:21.454000: I runner.py:310] Step = 4800 ; steps/s = 1.63, tokens/s = 42374 (42374 target) ; Learning rate = 0.000424 ; Loss = 2.680884\n",
      "2024-12-07 21:06:23.090000: I runner.py:310] Step = 4900 ; steps/s = 1.62, tokens/s = 42898 (42898 target) ; Learning rate = 0.000433 ; Loss = 2.565138\n",
      "2024-12-07 21:07:24.744000: I runner.py:310] Step = 5000 ; steps/s = 1.62, tokens/s = 42918 (42918 target) ; Learning rate = 0.000442 ; Loss = 2.558701\n",
      "2024-12-07 21:07:24.746000: I training.py:192] Running evaluation for step 5000\n",
      "2024-12-07 21:16:02.041000: I training.py:192] Evaluation result for step 5000: loss = 1.350929 ; perplexity = 3.861010\n",
      "2024-12-07 21:17:03.632000: I runner.py:310] Step = 5100 ; steps/s = 1.62, tokens/s = 42915 (42915 target) ; Learning rate = 0.000451 ; Loss = 2.606567\n",
      "2024-12-07 21:18:04.918000: I runner.py:310] Step = 5200 ; steps/s = 1.63, tokens/s = 42349 (42349 target) ; Learning rate = 0.000460 ; Loss = 2.531289\n",
      "2024-12-07 21:19:06.671000: I runner.py:310] Step = 5300 ; steps/s = 1.62, tokens/s = 42833 (42833 target) ; Learning rate = 0.000469 ; Loss = 2.423503\n",
      "2024-12-07 21:20:08.432000: I runner.py:310] Step = 5400 ; steps/s = 1.62, tokens/s = 42792 (42792 target) ; Learning rate = 0.000477 ; Loss = 2.438896\n",
      "2024-12-07 21:21:10.285000: I runner.py:310] Step = 5500 ; steps/s = 1.62, tokens/s = 42759 (42759 target) ; Learning rate = 0.000486 ; Loss = 2.446388\n",
      "2024-12-07 21:22:11.600000: I runner.py:310] Step = 5600 ; steps/s = 1.63, tokens/s = 42348 (42348 target) ; Learning rate = 0.000495 ; Loss = 2.372737\n",
      "2024-12-07 21:23:13.358000: I runner.py:310] Step = 5700 ; steps/s = 1.62, tokens/s = 42836 (42836 target) ; Learning rate = 0.000504 ; Loss = 2.375564\n",
      "2024-12-07 21:24:15.167000: I runner.py:310] Step = 5800 ; steps/s = 1.62, tokens/s = 42784 (42784 target) ; Learning rate = 0.000513 ; Loss = 2.407672\n",
      "2024-12-07 21:25:16.927000: I runner.py:310] Step = 5900 ; steps/s = 1.62, tokens/s = 42795 (42795 target) ; Learning rate = 0.000522 ; Loss = 2.431862\n",
      "2024-12-07 21:26:18.230000: I runner.py:310] Step = 6000 ; steps/s = 1.63, tokens/s = 42352 (42352 target) ; Learning rate = 0.000530 ; Loss = 2.312421\n",
      "2024-12-07 21:27:20.005000: I runner.py:310] Step = 6100 ; steps/s = 1.62, tokens/s = 42827 (42827 target) ; Learning rate = 0.000539 ; Loss = 2.288518\n",
      "2024-12-07 21:28:21.751000: I runner.py:310] Step = 6200 ; steps/s = 1.62, tokens/s = 42841 (42841 target) ; Learning rate = 0.000548 ; Loss = 2.306309\n",
      "2024-12-07 21:29:23.504000: I runner.py:310] Step = 6300 ; steps/s = 1.62, tokens/s = 42795 (42795 target) ; Learning rate = 0.000557 ; Loss = 2.316153\n",
      "2024-12-07 21:30:24.860000: I runner.py:310] Step = 6400 ; steps/s = 1.63, tokens/s = 42302 (42302 target) ; Learning rate = 0.000566 ; Loss = 2.237719\n",
      "2024-12-07 21:31:26.573000: I runner.py:310] Step = 6500 ; steps/s = 1.62, tokens/s = 42849 (42849 target) ; Learning rate = 0.000575 ; Loss = 2.291179\n",
      "2024-12-07 21:32:28.279000: I runner.py:310] Step = 6600 ; steps/s = 1.62, tokens/s = 42847 (42847 target) ; Learning rate = 0.000583 ; Loss = 2.287473\n",
      "2024-12-07 21:33:29.966000: I runner.py:310] Step = 6700 ; steps/s = 1.62, tokens/s = 42879 (42879 target) ; Learning rate = 0.000592 ; Loss = 2.251295\n",
      "2024-12-07 21:34:31.271000: I runner.py:310] Step = 6800 ; steps/s = 1.63, tokens/s = 42353 (42353 target) ; Learning rate = 0.000601 ; Loss = 2.259732\n",
      "2024-12-07 21:35:32.961000: I runner.py:310] Step = 6900 ; steps/s = 1.62, tokens/s = 42854 (42854 target) ; Learning rate = 0.000610 ; Loss = 2.197643\n",
      "2024-12-07 21:36:34.623000: I runner.py:310] Step = 7000 ; steps/s = 1.62, tokens/s = 42862 (42862 target) ; Learning rate = 0.000619 ; Loss = 2.205896\n",
      "2024-12-07 21:37:36.306000: I runner.py:310] Step = 7100 ; steps/s = 1.62, tokens/s = 42913 (42913 target) ; Learning rate = 0.000628 ; Loss = 2.258610\n",
      "2024-12-07 21:38:37.532000: I runner.py:310] Step = 7200 ; steps/s = 1.63, tokens/s = 42395 (42395 target) ; Learning rate = 0.000636 ; Loss = 2.150881\n",
      "2024-12-07 21:39:39.194000: I runner.py:310] Step = 7300 ; steps/s = 1.62, tokens/s = 42888 (42888 target) ; Learning rate = 0.000645 ; Loss = 2.171761\n",
      "2024-12-07 21:40:40.885000: I runner.py:310] Step = 7400 ; steps/s = 1.62, tokens/s = 42859 (42859 target) ; Learning rate = 0.000654 ; Loss = 2.195786\n",
      "2024-12-07 21:41:42.540000: I runner.py:310] Step = 7500 ; steps/s = 1.62, tokens/s = 42917 (42917 target) ; Learning rate = 0.000663 ; Loss = 2.212858\n",
      "2024-12-07 21:42:43.709000: I runner.py:310] Step = 7600 ; steps/s = 1.64, tokens/s = 42417 (42417 target) ; Learning rate = 0.000672 ; Loss = 2.191609\n",
      "2024-12-07 21:43:45.365000: I runner.py:310] Step = 7700 ; steps/s = 1.62, tokens/s = 42871 (42871 target) ; Learning rate = 0.000681 ; Loss = 2.178344\n",
      "2024-12-07 21:44:47.106000: I runner.py:310] Step = 7800 ; steps/s = 1.62, tokens/s = 42841 (42841 target) ; Learning rate = 0.000690 ; Loss = 2.138379\n",
      "2024-12-07 21:45:48.819000: I runner.py:310] Step = 7900 ; steps/s = 1.62, tokens/s = 42857 (42857 target) ; Learning rate = 0.000698 ; Loss = 2.173732\n",
      "2024-12-07 21:46:50.023000: I runner.py:310] Step = 8000 ; steps/s = 1.63, tokens/s = 42423 (42423 target) ; Learning rate = 0.000707 ; Loss = 2.132057\n",
      "2024-12-07 21:47:51.668000: I runner.py:310] Step = 8100 ; steps/s = 1.62, tokens/s = 42891 (42891 target) ; Learning rate = 0.000716 ; Loss = 2.126154\n",
      "2024-12-07 21:48:53.340000: I runner.py:310] Step = 8200 ; steps/s = 1.62, tokens/s = 42892 (42892 target) ; Learning rate = 0.000725 ; Loss = 2.099071\n",
      "2024-12-07 21:49:54.963000: I runner.py:310] Step = 8300 ; steps/s = 1.62, tokens/s = 42930 (42930 target) ; Learning rate = 0.000734 ; Loss = 2.151249\n",
      "2024-12-07 21:50:56.181000: I runner.py:310] Step = 8400 ; steps/s = 1.63, tokens/s = 42376 (42376 target) ; Learning rate = 0.000743 ; Loss = 2.084424\n",
      "2024-12-07 21:51:57.870000: I runner.py:310] Step = 8500 ; steps/s = 1.62, tokens/s = 42887 (42887 target) ; Learning rate = 0.000751 ; Loss = 2.066979\n",
      "2024-12-07 21:52:59.542000: I runner.py:310] Step = 8600 ; steps/s = 1.62, tokens/s = 42864 (42864 target) ; Learning rate = 0.000760 ; Loss = 2.130730\n",
      "2024-12-07 21:54:01.253000: I runner.py:310] Step = 8700 ; steps/s = 1.62, tokens/s = 42843 (42843 target) ; Learning rate = 0.000769 ; Loss = 2.079604\n",
      "2024-12-07 21:55:02.534000: I runner.py:310] Step = 8800 ; steps/s = 1.63, tokens/s = 42372 (42372 target) ; Learning rate = 0.000778 ; Loss = 1.995902\n",
      "2024-12-07 21:56:04.261000: I runner.py:310] Step = 8900 ; steps/s = 1.62, tokens/s = 42829 (42829 target) ; Learning rate = 0.000787 ; Loss = 2.067148\n",
      "2024-12-07 21:57:05.952000: I runner.py:310] Step = 9000 ; steps/s = 1.62, tokens/s = 42869 (42869 target) ; Learning rate = 0.000796 ; Loss = 2.083955\n",
      "2024-12-07 21:58:07.690000: I runner.py:310] Step = 9100 ; steps/s = 1.62, tokens/s = 42848 (42848 target) ; Learning rate = 0.000804 ; Loss = 2.099800\n",
      "2024-12-07 21:59:08.911000: I runner.py:310] Step = 9200 ; steps/s = 1.63, tokens/s = 42382 (42382 target) ; Learning rate = 0.000813 ; Loss = 2.054587\n",
      "2024-12-07 22:00:10.635000: I runner.py:310] Step = 9300 ; steps/s = 1.62, tokens/s = 42838 (42838 target) ; Learning rate = 0.000822 ; Loss = 2.062399\n",
      "2024-12-07 22:01:12.271000: I runner.py:310] Step = 9400 ; steps/s = 1.62, tokens/s = 42919 (42919 target) ; Learning rate = 0.000831 ; Loss = 2.035936\n",
      "2024-12-07 22:02:13.918000: I runner.py:310] Step = 9500 ; steps/s = 1.62, tokens/s = 42910 (42910 target) ; Learning rate = 0.000840 ; Loss = 2.070897\n",
      "2024-12-07 22:03:15.189000: I runner.py:310] Step = 9600 ; steps/s = 1.63, tokens/s = 42356 (42356 target) ; Learning rate = 0.000849 ; Loss = 2.028286\n",
      "2024-12-07 22:04:16.948000: I runner.py:310] Step = 9700 ; steps/s = 1.62, tokens/s = 42797 (42797 target) ; Learning rate = 0.000857 ; Loss = 1.986445\n",
      "2024-12-07 22:05:18.635000: I runner.py:310] Step = 9800 ; steps/s = 1.62, tokens/s = 42893 (42893 target) ; Learning rate = 0.000866 ; Loss = 2.022849\n",
      "2024-12-07 22:06:20.308000: I runner.py:310] Step = 9900 ; steps/s = 1.62, tokens/s = 42902 (42902 target) ; Learning rate = 0.000875 ; Loss = 2.027344\n",
      "2024-12-07 22:07:21.654000: I runner.py:310] Step = 10000 ; steps/s = 1.63, tokens/s = 42307 (42307 target) ; Learning rate = 0.000884 ; Loss = 2.018181\n",
      "2024-12-07 22:07:23.457000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-10000\n",
      "2024-12-07 22:07:23.457000: I training.py:192] Running evaluation for step 10000\n",
      "2024-12-07 22:19:21.437000: I training.py:192] Evaluation result for step 10000: loss = 1.077963 ; perplexity = 2.938686\n",
      "2024-12-07 22:20:23.058000: I runner.py:310] Step = 10100 ; steps/s = 1.62, tokens/s = 42904 (42904 target) ; Learning rate = 0.000879 ; Loss = 2.026935\n",
      "2024-12-07 22:21:24.723000: I runner.py:310] Step = 10200 ; steps/s = 1.62, tokens/s = 42861 (42861 target) ; Learning rate = 0.000875 ; Loss = 2.028656\n",
      "2024-12-07 22:22:26.187000: I runner.py:310] Step = 10300 ; steps/s = 1.63, tokens/s = 42538 (42538 target) ; Learning rate = 0.000871 ; Loss = 2.003648\n",
      "2024-12-07 22:23:27.728000: I runner.py:310] Step = 10400 ; steps/s = 1.63, tokens/s = 42709 (42709 target) ; Learning rate = 0.000867 ; Loss = 1.951055\n",
      "2024-12-07 22:24:29.458000: I runner.py:310] Step = 10500 ; steps/s = 1.62, tokens/s = 42842 (42842 target) ; Learning rate = 0.000863 ; Loss = 1.966193\n",
      "2024-12-07 22:25:31.087000: I runner.py:310] Step = 10600 ; steps/s = 1.62, tokens/s = 42912 (42912 target) ; Learning rate = 0.000858 ; Loss = 1.989500\n",
      "2024-12-07 22:26:32.453000: I runner.py:310] Step = 10700 ; steps/s = 1.63, tokens/s = 42271 (42271 target) ; Learning rate = 0.000854 ; Loss = 1.964442\n",
      "2024-12-07 22:27:34.141000: I runner.py:310] Step = 10800 ; steps/s = 1.62, tokens/s = 42858 (42858 target) ; Learning rate = 0.000850 ; Loss = 1.960768\n",
      "2024-12-07 22:28:35.772000: I runner.py:310] Step = 10900 ; steps/s = 1.62, tokens/s = 42902 (42902 target) ; Learning rate = 0.000847 ; Loss = 1.952826\n",
      "2024-12-07 22:29:37.469000: I runner.py:310] Step = 11000 ; steps/s = 1.62, tokens/s = 42901 (42901 target) ; Learning rate = 0.000843 ; Loss = 1.952147\n",
      "2024-12-07 22:30:38.704000: I runner.py:310] Step = 11100 ; steps/s = 1.63, tokens/s = 42377 (42377 target) ; Learning rate = 0.000839 ; Loss = 1.939319\n",
      "2024-12-07 22:31:40.338000: I runner.py:310] Step = 11200 ; steps/s = 1.62, tokens/s = 42925 (42925 target) ; Learning rate = 0.000835 ; Loss = 1.921685\n",
      "2024-12-07 22:32:42.000000: I runner.py:310] Step = 11300 ; steps/s = 1.62, tokens/s = 42903 (42903 target) ; Learning rate = 0.000831 ; Loss = 1.915965\n",
      "2024-12-07 22:33:43.694000: I runner.py:310] Step = 11400 ; steps/s = 1.62, tokens/s = 42847 (42847 target) ; Learning rate = 0.000828 ; Loss = 1.931408\n",
      "2024-12-07 22:34:44.948000: I runner.py:310] Step = 11500 ; steps/s = 1.63, tokens/s = 42362 (42362 target) ; Learning rate = 0.000824 ; Loss = 1.928024\n",
      "2024-12-07 22:35:46.543000: I runner.py:310] Step = 11600 ; steps/s = 1.62, tokens/s = 42975 (42975 target) ; Learning rate = 0.000821 ; Loss = 1.926191\n",
      "2024-12-07 22:36:48.231000: I runner.py:310] Step = 11700 ; steps/s = 1.62, tokens/s = 42845 (42845 target) ; Learning rate = 0.000817 ; Loss = 1.922584\n",
      "2024-12-07 22:37:49.898000: I runner.py:310] Step = 11800 ; steps/s = 1.62, tokens/s = 42871 (42871 target) ; Learning rate = 0.000814 ; Loss = 1.894301\n",
      "2024-12-07 22:38:51.142000: I runner.py:310] Step = 11900 ; steps/s = 1.63, tokens/s = 42397 (42397 target) ; Learning rate = 0.000810 ; Loss = 1.863020\n",
      "2024-12-07 22:39:52.796000: I runner.py:310] Step = 12000 ; steps/s = 1.62, tokens/s = 42898 (42898 target) ; Learning rate = 0.000807 ; Loss = 1.874762\n",
      "2024-12-07 22:40:54.427000: I runner.py:310] Step = 12100 ; steps/s = 1.62, tokens/s = 42895 (42895 target) ; Learning rate = 0.000803 ; Loss = 1.902621\n",
      "2024-12-07 22:41:56.115000: I runner.py:310] Step = 12200 ; steps/s = 1.62, tokens/s = 42885 (42885 target) ; Learning rate = 0.000800 ; Loss = 1.898942\n",
      "2024-12-07 22:42:57.345000: I runner.py:310] Step = 12300 ; steps/s = 1.63, tokens/s = 42381 (42381 target) ; Learning rate = 0.000797 ; Loss = 1.865868\n",
      "2024-12-07 22:43:59.070000: I runner.py:310] Step = 12400 ; steps/s = 1.62, tokens/s = 42831 (42831 target) ; Learning rate = 0.000794 ; Loss = 1.844298\n",
      "2024-12-07 22:45:00.701000: I runner.py:310] Step = 12500 ; steps/s = 1.62, tokens/s = 42919 (42919 target) ; Learning rate = 0.000791 ; Loss = 1.890984\n",
      "2024-12-07 22:46:02.306000: I runner.py:310] Step = 12600 ; steps/s = 1.62, tokens/s = 42926 (42926 target) ; Learning rate = 0.000787 ; Loss = 1.897184\n",
      "2024-12-07 22:47:03.574000: I runner.py:310] Step = 12700 ; steps/s = 1.63, tokens/s = 42382 (42382 target) ; Learning rate = 0.000784 ; Loss = 1.845183\n",
      "2024-12-07 22:48:05.239000: I runner.py:310] Step = 12800 ; steps/s = 1.62, tokens/s = 42908 (42908 target) ; Learning rate = 0.000781 ; Loss = 1.842994\n",
      "2024-12-07 22:49:06.902000: I runner.py:310] Step = 12900 ; steps/s = 1.62, tokens/s = 42868 (42868 target) ; Learning rate = 0.000778 ; Loss = 1.850568\n",
      "2024-12-07 22:50:08.542000: I runner.py:310] Step = 13000 ; steps/s = 1.62, tokens/s = 42908 (42908 target) ; Learning rate = 0.000775 ; Loss = 1.866676\n",
      "2024-12-07 22:51:09.783000: I runner.py:310] Step = 13100 ; steps/s = 1.63, tokens/s = 42373 (42373 target) ; Learning rate = 0.000772 ; Loss = 1.834387\n",
      "2024-12-07 22:52:11.436000: I runner.py:310] Step = 13200 ; steps/s = 1.62, tokens/s = 42896 (42896 target) ; Learning rate = 0.000769 ; Loss = 1.837206\n",
      "2024-12-07 22:53:13.108000: I runner.py:310] Step = 13300 ; steps/s = 1.62, tokens/s = 42870 (42870 target) ; Learning rate = 0.000766 ; Loss = 1.837769\n",
      "2024-12-07 22:54:14.719000: I runner.py:310] Step = 13400 ; steps/s = 1.62, tokens/s = 42924 (42924 target) ; Learning rate = 0.000764 ; Loss = 1.846283\n",
      "2024-12-07 22:55:16.019000: I runner.py:310] Step = 13500 ; steps/s = 1.63, tokens/s = 42361 (42361 target) ; Learning rate = 0.000761 ; Loss = 1.842524\n",
      "2024-12-07 22:56:17.690000: I runner.py:310] Step = 13600 ; steps/s = 1.62, tokens/s = 42887 (42887 target) ; Learning rate = 0.000758 ; Loss = 1.811916\n",
      "2024-12-07 22:57:19.421000: I runner.py:310] Step = 13700 ; steps/s = 1.62, tokens/s = 42862 (42862 target) ; Learning rate = 0.000755 ; Loss = 1.807184\n",
      "2024-12-07 22:58:21.042000: I runner.py:310] Step = 13800 ; steps/s = 1.62, tokens/s = 42919 (42919 target) ; Learning rate = 0.000752 ; Loss = 1.830781\n",
      "2024-12-07 22:59:22.322000: I runner.py:310] Step = 13900 ; steps/s = 1.63, tokens/s = 42295 (42295 target) ; Learning rate = 0.000750 ; Loss = 1.803518\n",
      "2024-12-07 23:00:23.972000: I runner.py:310] Step = 14000 ; steps/s = 1.62, tokens/s = 42897 (42897 target) ; Learning rate = 0.000747 ; Loss = 1.824252\n",
      "2024-12-07 23:01:25.666000: I runner.py:310] Step = 14100 ; steps/s = 1.62, tokens/s = 42885 (42885 target) ; Learning rate = 0.000744 ; Loss = 1.783779\n",
      "2024-12-07 23:02:27.401000: I runner.py:310] Step = 14200 ; steps/s = 1.62, tokens/s = 42862 (42862 target) ; Learning rate = 0.000742 ; Loss = 1.790035\n",
      "2024-12-07 23:03:28.655000: I runner.py:310] Step = 14300 ; steps/s = 1.63, tokens/s = 42365 (42365 target) ; Learning rate = 0.000739 ; Loss = 1.781150\n",
      "2024-12-07 23:04:30.389000: I runner.py:310] Step = 14400 ; steps/s = 1.62, tokens/s = 42833 (42833 target) ; Learning rate = 0.000737 ; Loss = 1.792021\n",
      "2024-12-07 23:05:32.084000: I runner.py:310] Step = 14500 ; steps/s = 1.62, tokens/s = 42840 (42840 target) ; Learning rate = 0.000734 ; Loss = 1.818277\n",
      "2024-12-07 23:06:33.717000: I runner.py:310] Step = 14600 ; steps/s = 1.62, tokens/s = 42921 (42921 target) ; Learning rate = 0.000731 ; Loss = 1.818948\n",
      "2024-12-07 23:07:34.931000: I runner.py:310] Step = 14700 ; steps/s = 1.63, tokens/s = 42413 (42413 target) ; Learning rate = 0.000729 ; Loss = 1.782974\n",
      "2024-12-07 23:08:36.567000: I runner.py:310] Step = 14800 ; steps/s = 1.62, tokens/s = 42924 (42924 target) ; Learning rate = 0.000727 ; Loss = 1.768186\n",
      "2024-12-07 23:09:38.162000: I runner.py:310] Step = 14900 ; steps/s = 1.62, tokens/s = 42923 (42923 target) ; Learning rate = 0.000724 ; Loss = 1.799465\n",
      "2024-12-07 23:10:39.864000: I runner.py:310] Step = 15000 ; steps/s = 1.62, tokens/s = 42856 (42856 target) ; Learning rate = 0.000722 ; Loss = 1.817996\n",
      "2024-12-07 23:10:39.865000: I training.py:192] Running evaluation for step 15000\n",
      "2024-12-07 23:18:48.339000: I training.py:192] Evaluation result for step 15000: loss = 1.050887 ; perplexity = 2.860187\n",
      "2024-12-07 23:19:49.483000: I runner.py:310] Step = 15100 ; steps/s = 1.64, tokens/s = 42470 (42470 target) ; Learning rate = 0.000719 ; Loss = 1.785594\n",
      "2024-12-07 23:20:51.178000: I runner.py:310] Step = 15200 ; steps/s = 1.62, tokens/s = 42877 (42877 target) ; Learning rate = 0.000717 ; Loss = 1.769690\n",
      "2024-12-07 23:21:52.814000: I runner.py:310] Step = 15300 ; steps/s = 1.62, tokens/s = 42924 (42924 target) ; Learning rate = 0.000715 ; Loss = 1.791329\n",
      "2024-12-07 23:22:54.526000: I runner.py:310] Step = 15400 ; steps/s = 1.62, tokens/s = 42823 (42823 target) ; Learning rate = 0.000712 ; Loss = 1.783577\n",
      "2024-12-07 23:23:55.759000: I runner.py:310] Step = 15500 ; steps/s = 1.63, tokens/s = 42376 (42376 target) ; Learning rate = 0.000710 ; Loss = 1.763850\n",
      "2024-12-07 23:24:57.478000: I runner.py:310] Step = 15600 ; steps/s = 1.62, tokens/s = 42886 (42886 target) ; Learning rate = 0.000708 ; Loss = 1.758493\n",
      "2024-12-07 23:25:59.209000: I runner.py:310] Step = 15700 ; steps/s = 1.62, tokens/s = 42819 (42819 target) ; Learning rate = 0.000705 ; Loss = 1.783441\n",
      "2024-12-07 23:27:00.861000: I runner.py:310] Step = 15800 ; steps/s = 1.62, tokens/s = 42902 (42902 target) ; Learning rate = 0.000703 ; Loss = 1.769461\n",
      "2024-12-07 23:28:02.156000: I runner.py:310] Step = 15900 ; steps/s = 1.63, tokens/s = 42321 (42321 target) ; Learning rate = 0.000701 ; Loss = 1.759291\n",
      "2024-12-07 23:29:03.834000: I runner.py:310] Step = 16000 ; steps/s = 1.62, tokens/s = 42901 (42901 target) ; Learning rate = 0.000699 ; Loss = 1.751306\n",
      "2024-12-07 23:30:05.517000: I runner.py:310] Step = 16100 ; steps/s = 1.62, tokens/s = 42880 (42880 target) ; Learning rate = 0.000697 ; Loss = 1.763041\n",
      "2024-12-07 23:31:07.190000: I runner.py:310] Step = 16200 ; steps/s = 1.62, tokens/s = 42885 (42885 target) ; Learning rate = 0.000694 ; Loss = 1.752062\n",
      "2024-12-07 23:32:08.485000: I runner.py:310] Step = 16300 ; steps/s = 1.63, tokens/s = 42318 (42318 target) ; Learning rate = 0.000692 ; Loss = 1.744203\n",
      "2024-12-07 23:33:10.185000: I runner.py:310] Step = 16400 ; steps/s = 1.62, tokens/s = 42897 (42897 target) ; Learning rate = 0.000690 ; Loss = 1.759933\n",
      "2024-12-07 23:34:11.844000: I runner.py:310] Step = 16500 ; steps/s = 1.62, tokens/s = 42884 (42884 target) ; Learning rate = 0.000688 ; Loss = 1.786497\n",
      "2024-12-07 23:35:13.510000: I runner.py:310] Step = 16600 ; steps/s = 1.62, tokens/s = 42874 (42874 target) ; Learning rate = 0.000686 ; Loss = 1.766763\n",
      "2024-12-07 23:36:14.783000: I runner.py:310] Step = 16700 ; steps/s = 1.63, tokens/s = 42361 (42361 target) ; Learning rate = 0.000684 ; Loss = 1.722882\n",
      "2024-12-07 23:37:16.429000: I runner.py:310] Step = 16800 ; steps/s = 1.62, tokens/s = 42886 (42886 target) ; Learning rate = 0.000682 ; Loss = 1.746423\n",
      "2024-12-07 23:38:18.145000: I runner.py:310] Step = 16900 ; steps/s = 1.62, tokens/s = 42844 (42844 target) ; Learning rate = 0.000680 ; Loss = 1.747208\n",
      "2024-12-07 23:39:19.863000: I runner.py:310] Step = 17000 ; steps/s = 1.62, tokens/s = 42865 (42865 target) ; Learning rate = 0.000678 ; Loss = 1.768073\n",
      "2024-12-07 23:40:21.139000: I runner.py:310] Step = 17100 ; steps/s = 1.63, tokens/s = 42389 (42389 target) ; Learning rate = 0.000676 ; Loss = 1.745240\n",
      "2024-12-07 23:41:22.764000: I runner.py:310] Step = 17200 ; steps/s = 1.62, tokens/s = 42913 (42913 target) ; Learning rate = 0.000674 ; Loss = 1.726303\n",
      "2024-12-07 23:42:24.400000: I runner.py:310] Step = 17300 ; steps/s = 1.62, tokens/s = 42914 (42914 target) ; Learning rate = 0.000672 ; Loss = 1.734626\n",
      "2024-12-07 23:43:26.050000: I runner.py:310] Step = 17400 ; steps/s = 1.62, tokens/s = 42883 (42883 target) ; Learning rate = 0.000670 ; Loss = 1.734664\n",
      "2024-12-07 23:44:27.345000: I runner.py:310] Step = 17500 ; steps/s = 1.63, tokens/s = 42356 (42356 target) ; Learning rate = 0.000668 ; Loss = 1.744269\n",
      "2024-12-07 23:45:29.035000: I runner.py:310] Step = 17600 ; steps/s = 1.62, tokens/s = 42877 (42877 target) ; Learning rate = 0.000666 ; Loss = 1.722094\n",
      "2024-12-07 23:46:30.671000: I runner.py:310] Step = 17700 ; steps/s = 1.62, tokens/s = 42891 (42891 target) ; Learning rate = 0.000664 ; Loss = 1.740404\n",
      "2024-12-07 23:47:32.383000: I runner.py:310] Step = 17800 ; steps/s = 1.62, tokens/s = 42850 (42850 target) ; Learning rate = 0.000662 ; Loss = 1.731275\n",
      "2024-12-07 23:48:33.632000: I runner.py:310] Step = 17900 ; steps/s = 1.63, tokens/s = 42375 (42375 target) ; Learning rate = 0.000661 ; Loss = 1.732794\n",
      "2024-12-07 23:49:35.318000: I runner.py:310] Step = 18000 ; steps/s = 1.62, tokens/s = 42898 (42898 target) ; Learning rate = 0.000659 ; Loss = 1.726028\n",
      "2024-12-07 23:50:37.010000: I runner.py:310] Step = 18100 ; steps/s = 1.62, tokens/s = 42844 (42844 target) ; Learning rate = 0.000657 ; Loss = 1.726311\n",
      "2024-12-07 23:51:38.659000: I runner.py:310] Step = 18200 ; steps/s = 1.62, tokens/s = 42885 (42885 target) ; Learning rate = 0.000655 ; Loss = 1.722567\n",
      "2024-12-07 23:52:39.825000: I runner.py:310] Step = 18300 ; steps/s = 1.64, tokens/s = 42463 (42463 target) ; Learning rate = 0.000653 ; Loss = 1.680004\n",
      "2024-12-07 23:53:41.478000: I runner.py:310] Step = 18400 ; steps/s = 1.62, tokens/s = 42908 (42908 target) ; Learning rate = 0.000652 ; Loss = 1.719259\n",
      "2024-12-07 23:54:43.165000: I runner.py:310] Step = 18500 ; steps/s = 1.62, tokens/s = 42863 (42863 target) ; Learning rate = 0.000650 ; Loss = 1.726092\n",
      "2024-12-07 23:55:44.934000: I runner.py:310] Step = 18600 ; steps/s = 1.62, tokens/s = 42795 (42795 target) ; Learning rate = 0.000648 ; Loss = 1.740383\n",
      "2024-12-07 23:56:46.206000: I runner.py:310] Step = 18700 ; steps/s = 1.63, tokens/s = 42365 (42365 target) ; Learning rate = 0.000646 ; Loss = 1.681517\n",
      "2024-12-07 23:57:47.944000: I runner.py:310] Step = 18800 ; steps/s = 1.62, tokens/s = 42839 (42839 target) ; Learning rate = 0.000645 ; Loss = 1.711504\n",
      "2024-12-07 23:58:49.643000: I runner.py:310] Step = 18900 ; steps/s = 1.62, tokens/s = 42878 (42878 target) ; Learning rate = 0.000643 ; Loss = 1.713590\n",
      "2024-12-07 23:59:51.334000: I runner.py:310] Step = 19000 ; steps/s = 1.62, tokens/s = 42850 (42850 target) ; Learning rate = 0.000641 ; Loss = 1.729658\n",
      "2024-12-08 00:00:52.583000: I runner.py:310] Step = 19100 ; steps/s = 1.63, tokens/s = 42346 (42346 target) ; Learning rate = 0.000640 ; Loss = 1.717245\n",
      "2024-12-08 00:01:54.278000: I runner.py:310] Step = 19200 ; steps/s = 1.62, tokens/s = 42876 (42876 target) ; Learning rate = 0.000638 ; Loss = 1.715864\n",
      "2024-12-08 00:02:55.952000: I runner.py:310] Step = 19300 ; steps/s = 1.62, tokens/s = 42880 (42880 target) ; Learning rate = 0.000636 ; Loss = 1.722876\n",
      "2024-12-08 00:03:57.562000: I runner.py:310] Step = 19400 ; steps/s = 1.62, tokens/s = 42939 (42939 target) ; Learning rate = 0.000635 ; Loss = 1.717429\n",
      "2024-12-08 00:04:58.764000: I runner.py:310] Step = 19500 ; steps/s = 1.63, tokens/s = 42419 (42419 target) ; Learning rate = 0.000633 ; Loss = 1.696075\n",
      "2024-12-08 00:06:00.391000: I runner.py:310] Step = 19600 ; steps/s = 1.62, tokens/s = 42914 (42914 target) ; Learning rate = 0.000631 ; Loss = 1.703356\n",
      "2024-12-08 00:07:02.015000: I runner.py:310] Step = 19700 ; steps/s = 1.62, tokens/s = 42902 (42902 target) ; Learning rate = 0.000630 ; Loss = 1.708378\n",
      "2024-12-08 00:08:03.735000: I runner.py:310] Step = 19800 ; steps/s = 1.62, tokens/s = 42843 (42843 target) ; Learning rate = 0.000628 ; Loss = 1.716439\n",
      "2024-12-08 00:09:04.949000: I runner.py:310] Step = 19900 ; steps/s = 1.63, tokens/s = 42434 (42434 target) ; Learning rate = 0.000627 ; Loss = 1.695799\n",
      "2024-12-08 00:10:06.693000: I runner.py:310] Step = 20000 ; steps/s = 1.62, tokens/s = 42829 (42829 target) ; Learning rate = 0.000625 ; Loss = 1.692967\n",
      "2024-12-08 00:10:08.478000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-20000\n",
      "2024-12-08 00:10:08.478000: I training.py:192] Running evaluation for step 20000\n",
      "2024-12-08 00:16:17.373000: I training.py:192] Evaluation result for step 20000: loss = 1.067248 ; perplexity = 2.907366\n",
      "2024-12-08 00:17:18.954000: I runner.py:310] Step = 20100 ; steps/s = 1.62, tokens/s = 42919 (42919 target) ; Learning rate = 0.000623 ; Loss = 1.690550\n",
      "2024-12-08 00:18:20.777000: I runner.py:310] Step = 20200 ; steps/s = 1.62, tokens/s = 42791 (42791 target) ; Learning rate = 0.000622 ; Loss = 1.687278\n",
      "2024-12-08 00:19:21.961000: I runner.py:310] Step = 20300 ; steps/s = 1.63, tokens/s = 42449 (42449 target) ; Learning rate = 0.000620 ; Loss = 1.697438\n",
      "2024-12-08 00:20:23.773000: I runner.py:310] Step = 20400 ; steps/s = 1.62, tokens/s = 42780 (42780 target) ; Learning rate = 0.000619 ; Loss = 1.682098\n",
      "2024-12-08 00:21:25.543000: I runner.py:310] Step = 20500 ; steps/s = 1.62, tokens/s = 42777 (42777 target) ; Learning rate = 0.000617 ; Loss = 1.690419\n",
      "2024-12-08 00:22:27.233000: I runner.py:310] Step = 20600 ; steps/s = 1.62, tokens/s = 42763 (42763 target) ; Learning rate = 0.000616 ; Loss = 1.702988\n",
      "2024-12-08 00:23:28.621000: I runner.py:310] Step = 20700 ; steps/s = 1.63, tokens/s = 42362 (42362 target) ; Learning rate = 0.000614 ; Loss = 1.654908\n",
      "2024-12-08 00:24:30.337000: I runner.py:310] Step = 20800 ; steps/s = 1.62, tokens/s = 42854 (42854 target) ; Learning rate = 0.000613 ; Loss = 1.687233\n",
      "2024-12-08 00:25:32.042000: I runner.py:310] Step = 20900 ; steps/s = 1.62, tokens/s = 42883 (42883 target) ; Learning rate = 0.000611 ; Loss = 1.698642\n",
      "2024-12-08 00:26:33.360000: I runner.py:310] Step = 21000 ; steps/s = 1.63, tokens/s = 42324 (42324 target) ; Learning rate = 0.000610 ; Loss = 1.679798\n",
      "2024-12-08 00:27:35.082000: I runner.py:310] Step = 21100 ; steps/s = 1.62, tokens/s = 42882 (42882 target) ; Learning rate = 0.000608 ; Loss = 1.671233\n",
      "2024-12-08 00:28:36.789000: I runner.py:310] Step = 21200 ; steps/s = 1.62, tokens/s = 42854 (42854 target) ; Learning rate = 0.000607 ; Loss = 1.676670\n",
      "2024-12-08 00:29:38.500000: I runner.py:310] Step = 21300 ; steps/s = 1.62, tokens/s = 42888 (42888 target) ; Learning rate = 0.000606 ; Loss = 1.698420\n",
      "2024-12-08 00:30:39.862000: I runner.py:310] Step = 21400 ; steps/s = 1.63, tokens/s = 42242 (42242 target) ; Learning rate = 0.000604 ; Loss = 1.681590\n",
      "2024-12-08 00:31:41.597000: I runner.py:310] Step = 21500 ; steps/s = 1.62, tokens/s = 42884 (42884 target) ; Learning rate = 0.000603 ; Loss = 1.668776\n",
      "2024-12-08 00:32:43.297000: I runner.py:310] Step = 21600 ; steps/s = 1.62, tokens/s = 42856 (42856 target) ; Learning rate = 0.000601 ; Loss = 1.685002\n",
      "2024-12-08 00:33:45.048000: I runner.py:310] Step = 21700 ; steps/s = 1.62, tokens/s = 42842 (42842 target) ; Learning rate = 0.000600 ; Loss = 1.681236\n",
      "2024-12-08 00:34:46.320000: I runner.py:310] Step = 21800 ; steps/s = 1.63, tokens/s = 42307 (42307 target) ; Learning rate = 0.000599 ; Loss = 1.675364\n",
      "2024-12-08 00:35:48.068000: I runner.py:310] Step = 21900 ; steps/s = 1.62, tokens/s = 42816 (42816 target) ; Learning rate = 0.000597 ; Loss = 1.651710\n",
      "2024-12-08 00:36:49.780000: I runner.py:310] Step = 22000 ; steps/s = 1.62, tokens/s = 42856 (42856 target) ; Learning rate = 0.000596 ; Loss = 1.685062\n",
      "2024-12-08 00:37:51.515000: I runner.py:310] Step = 22100 ; steps/s = 1.62, tokens/s = 42837 (42837 target) ; Learning rate = 0.000595 ; Loss = 1.678221\n",
      "2024-12-08 00:38:52.774000: I runner.py:310] Step = 22200 ; steps/s = 1.63, tokens/s = 42382 (42382 target) ; Learning rate = 0.000593 ; Loss = 1.659165\n",
      "2024-12-08 00:39:54.477000: I runner.py:310] Step = 22300 ; steps/s = 1.62, tokens/s = 42884 (42884 target) ; Learning rate = 0.000592 ; Loss = 1.674343\n",
      "2024-12-08 00:40:56.230000: I runner.py:310] Step = 22400 ; steps/s = 1.62, tokens/s = 42828 (42828 target) ; Learning rate = 0.000591 ; Loss = 1.664137\n",
      "2024-12-08 00:41:57.907000: I runner.py:310] Step = 22500 ; steps/s = 1.62, tokens/s = 42863 (42863 target) ; Learning rate = 0.000589 ; Loss = 1.681566\n",
      "2024-12-08 00:42:59.287000: I runner.py:310] Step = 22600 ; steps/s = 1.63, tokens/s = 42298 (42298 target) ; Learning rate = 0.000588 ; Loss = 1.659759\n",
      "2024-12-08 00:44:01.019000: I runner.py:310] Step = 22700 ; steps/s = 1.62, tokens/s = 42819 (42819 target) ; Learning rate = 0.000587 ; Loss = 1.661089\n",
      "2024-12-08 00:45:02.719000: I runner.py:310] Step = 22800 ; steps/s = 1.62, tokens/s = 42884 (42884 target) ; Learning rate = 0.000585 ; Loss = 1.675919\n",
      "2024-12-08 00:46:04.488000: I runner.py:310] Step = 22900 ; steps/s = 1.62, tokens/s = 42847 (42847 target) ; Learning rate = 0.000584 ; Loss = 1.675152\n",
      "2024-12-08 00:47:05.755000: I runner.py:310] Step = 23000 ; steps/s = 1.63, tokens/s = 42310 (42310 target) ; Learning rate = 0.000583 ; Loss = 1.655205\n",
      "2024-12-08 00:48:07.443000: I runner.py:310] Step = 23100 ; steps/s = 1.62, tokens/s = 42871 (42871 target) ; Learning rate = 0.000582 ; Loss = 1.654265\n",
      "2024-12-08 00:49:09.152000: I runner.py:310] Step = 23200 ; steps/s = 1.62, tokens/s = 42830 (42830 target) ; Learning rate = 0.000580 ; Loss = 1.669239\n",
      "2024-12-08 00:50:10.843000: I runner.py:310] Step = 23300 ; steps/s = 1.62, tokens/s = 42908 (42908 target) ; Learning rate = 0.000579 ; Loss = 1.678883\n",
      "2024-12-08 00:51:12.034000: I runner.py:310] Step = 23400 ; steps/s = 1.63, tokens/s = 42424 (42424 target) ; Learning rate = 0.000578 ; Loss = 1.651164\n",
      "2024-12-08 00:52:13.802000: I runner.py:310] Step = 23500 ; steps/s = 1.62, tokens/s = 42833 (42833 target) ; Learning rate = 0.000577 ; Loss = 1.643955\n",
      "2024-12-08 00:53:15.545000: I runner.py:310] Step = 23600 ; steps/s = 1.62, tokens/s = 42838 (42838 target) ; Learning rate = 0.000575 ; Loss = 1.662113\n",
      "2024-12-08 00:54:17.305000: I runner.py:310] Step = 23700 ; steps/s = 1.62, tokens/s = 42787 (42787 target) ; Learning rate = 0.000574 ; Loss = 1.660251\n",
      "2024-12-08 00:55:18.508000: I runner.py:310] Step = 23800 ; steps/s = 1.63, tokens/s = 42421 (42421 target) ; Learning rate = 0.000573 ; Loss = 1.637299\n",
      "2024-12-08 00:56:20.250000: I runner.py:310] Step = 23900 ; steps/s = 1.62, tokens/s = 42848 (42848 target) ; Learning rate = 0.000572 ; Loss = 1.650056\n",
      "2024-12-08 00:57:21.866000: I runner.py:310] Step = 24000 ; steps/s = 1.62, tokens/s = 42914 (42914 target) ; Learning rate = 0.000571 ; Loss = 1.647222\n",
      "2024-12-08 00:58:23.636000: I runner.py:310] Step = 24100 ; steps/s = 1.62, tokens/s = 42783 (42783 target) ; Learning rate = 0.000569 ; Loss = 1.655968\n",
      "2024-12-08 00:59:24.844000: I runner.py:310] Step = 24200 ; steps/s = 1.63, tokens/s = 42432 (42432 target) ; Learning rate = 0.000568 ; Loss = 1.646251\n",
      "2024-12-08 01:00:26.564000: I runner.py:310] Step = 24300 ; steps/s = 1.62, tokens/s = 42842 (42842 target) ; Learning rate = 0.000567 ; Loss = 1.648875\n",
      "2024-12-08 01:01:28.287000: I runner.py:310] Step = 24400 ; steps/s = 1.62, tokens/s = 42844 (42844 target) ; Learning rate = 0.000566 ; Loss = 1.665418\n",
      "2024-12-08 01:02:30.094000: I runner.py:310] Step = 24500 ; steps/s = 1.62, tokens/s = 42793 (42793 target) ; Learning rate = 0.000565 ; Loss = 1.658889\n",
      "2024-12-08 01:03:31.360000: I runner.py:310] Step = 24600 ; steps/s = 1.63, tokens/s = 42363 (42363 target) ; Learning rate = 0.000564 ; Loss = 1.651345\n",
      "2024-12-08 01:04:33.054000: I runner.py:310] Step = 24700 ; steps/s = 1.62, tokens/s = 42902 (42902 target) ; Learning rate = 0.000562 ; Loss = 1.647789\n",
      "2024-12-08 01:05:34.733000: I runner.py:310] Step = 24800 ; steps/s = 1.62, tokens/s = 42862 (42862 target) ; Learning rate = 0.000561 ; Loss = 1.639109\n",
      "2024-12-08 01:06:36.427000: I runner.py:310] Step = 24900 ; steps/s = 1.62, tokens/s = 42845 (42845 target) ; Learning rate = 0.000560 ; Loss = 1.647855\n",
      "2024-12-08 01:07:37.692000: I runner.py:310] Step = 25000 ; steps/s = 1.63, tokens/s = 42360 (42360 target) ; Learning rate = 0.000559 ; Loss = 1.627967\n",
      "2024-12-08 01:07:37.693000: I training.py:192] Running evaluation for step 25000\n",
      "2024-12-08 01:13:26.459000: I training.py:192] Evaluation result for step 25000: loss = 1.084998 ; perplexity = 2.959435\n",
      "2024-12-08 01:14:28.048000: I runner.py:310] Step = 25100 ; steps/s = 1.62, tokens/s = 42977 (42977 target) ; Learning rate = 0.000558 ; Loss = 1.643926\n",
      "2024-12-08 01:15:29.851000: I runner.py:310] Step = 25200 ; steps/s = 1.62, tokens/s = 42774 (42774 target) ; Learning rate = 0.000557 ; Loss = 1.656671\n",
      "2024-12-08 01:16:31.558000: I runner.py:310] Step = 25300 ; steps/s = 1.62, tokens/s = 42868 (42868 target) ; Learning rate = 0.000556 ; Loss = 1.653177\n",
      "2024-12-08 01:17:32.872000: I runner.py:310] Step = 25400 ; steps/s = 1.63, tokens/s = 42310 (42310 target) ; Learning rate = 0.000555 ; Loss = 1.656007\n",
      "2024-12-08 01:18:34.685000: I runner.py:310] Step = 25500 ; steps/s = 1.62, tokens/s = 42801 (42801 target) ; Learning rate = 0.000553 ; Loss = 1.628968\n",
      "2024-12-08 01:19:36.362000: I runner.py:310] Step = 25600 ; steps/s = 1.62, tokens/s = 42878 (42878 target) ; Learning rate = 0.000552 ; Loss = 1.638464\n",
      "2024-12-08 01:20:38.105000: I runner.py:310] Step = 25700 ; steps/s = 1.62, tokens/s = 42824 (42824 target) ; Learning rate = 0.000551 ; Loss = 1.645919\n",
      "2024-12-08 01:21:39.398000: I runner.py:310] Step = 25800 ; steps/s = 1.63, tokens/s = 42362 (42362 target) ; Learning rate = 0.000550 ; Loss = 1.648173\n",
      "2024-12-08 01:22:41.139000: I runner.py:310] Step = 25900 ; steps/s = 1.62, tokens/s = 42831 (42831 target) ; Learning rate = 0.000549 ; Loss = 1.627479\n",
      "2024-12-08 01:23:42.794000: I runner.py:310] Step = 26000 ; steps/s = 1.62, tokens/s = 42885 (42885 target) ; Learning rate = 0.000548 ; Loss = 1.635704\n",
      "2024-12-08 01:24:44.549000: I runner.py:310] Step = 26100 ; steps/s = 1.62, tokens/s = 42823 (42823 target) ; Learning rate = 0.000547 ; Loss = 1.636397\n",
      "2024-12-08 01:25:45.882000: I runner.py:310] Step = 26200 ; steps/s = 1.63, tokens/s = 42340 (42340 target) ; Learning rate = 0.000546 ; Loss = 1.655454\n",
      "2024-12-08 01:26:47.579000: I runner.py:310] Step = 26300 ; steps/s = 1.62, tokens/s = 42869 (42869 target) ; Learning rate = 0.000545 ; Loss = 1.630518\n",
      "2024-12-08 01:27:49.374000: I runner.py:310] Step = 26400 ; steps/s = 1.62, tokens/s = 42774 (42774 target) ; Learning rate = 0.000544 ; Loss = 1.640964\n",
      "2024-12-08 01:28:51.157000: I runner.py:310] Step = 26500 ; steps/s = 1.62, tokens/s = 42790 (42790 target) ; Learning rate = 0.000543 ; Loss = 1.635644\n",
      "2024-12-08 01:29:52.468000: I runner.py:310] Step = 26600 ; steps/s = 1.63, tokens/s = 42347 (42347 target) ; Learning rate = 0.000542 ; Loss = 1.629096\n",
      "2024-12-08 01:30:54.163000: I runner.py:310] Step = 26700 ; steps/s = 1.62, tokens/s = 42902 (42902 target) ; Learning rate = 0.000541 ; Loss = 1.618934\n",
      "2024-12-08 01:31:55.870000: I runner.py:310] Step = 26800 ; steps/s = 1.62, tokens/s = 42846 (42846 target) ; Learning rate = 0.000540 ; Loss = 1.636022\n",
      "2024-12-08 01:32:57.601000: I runner.py:310] Step = 26900 ; steps/s = 1.62, tokens/s = 42825 (42825 target) ; Learning rate = 0.000539 ; Loss = 1.642015\n",
      "2024-12-08 01:33:58.893000: I runner.py:310] Step = 27000 ; steps/s = 1.63, tokens/s = 42353 (42353 target) ; Learning rate = 0.000538 ; Loss = 1.618910\n",
      "2024-12-08 01:35:00.688000: I runner.py:310] Step = 27100 ; steps/s = 1.62, tokens/s = 42812 (42812 target) ; Learning rate = 0.000537 ; Loss = 1.624105\n",
      "2024-12-08 01:36:02.434000: I runner.py:310] Step = 27200 ; steps/s = 1.62, tokens/s = 42803 (42803 target) ; Learning rate = 0.000536 ; Loss = 1.625121\n",
      "2024-12-08 01:37:04.148000: I runner.py:310] Step = 27300 ; steps/s = 1.62, tokens/s = 42854 (42854 target) ; Learning rate = 0.000535 ; Loss = 1.649879\n",
      "2024-12-08 01:38:05.374000: I runner.py:310] Step = 27400 ; steps/s = 1.63, tokens/s = 42390 (42390 target) ; Learning rate = 0.000534 ; Loss = 1.610747\n",
      "2024-12-08 01:39:07.111000: I runner.py:310] Step = 27500 ; steps/s = 1.62, tokens/s = 42852 (42852 target) ; Learning rate = 0.000533 ; Loss = 1.631051\n",
      "2024-12-08 01:40:08.866000: I runner.py:310] Step = 27600 ; steps/s = 1.62, tokens/s = 42814 (42814 target) ; Learning rate = 0.000532 ; Loss = 1.623632\n",
      "2024-12-08 01:41:10.662000: I runner.py:310] Step = 27700 ; steps/s = 1.62, tokens/s = 42797 (42797 target) ; Learning rate = 0.000531 ; Loss = 1.639174\n",
      "2024-12-08 01:42:11.910000: I runner.py:310] Step = 27800 ; steps/s = 1.63, tokens/s = 42369 (42369 target) ; Learning rate = 0.000530 ; Loss = 1.606708\n",
      "2024-12-08 01:43:13.620000: I runner.py:310] Step = 27900 ; steps/s = 1.62, tokens/s = 42819 (42819 target) ; Learning rate = 0.000529 ; Loss = 1.619850\n",
      "2024-12-08 01:44:15.359000: I runner.py:310] Step = 28000 ; steps/s = 1.62, tokens/s = 42837 (42837 target) ; Learning rate = 0.000528 ; Loss = 1.632920\n",
      "2024-12-08 01:45:17.046000: I runner.py:310] Step = 28100 ; steps/s = 1.62, tokens/s = 42896 (42896 target) ; Learning rate = 0.000527 ; Loss = 1.641332\n",
      "2024-12-08 01:46:18.303000: I runner.py:310] Step = 28200 ; steps/s = 1.63, tokens/s = 42382 (42382 target) ; Learning rate = 0.000526 ; Loss = 1.605120\n",
      "2024-12-08 01:47:20.078000: I runner.py:310] Step = 28300 ; steps/s = 1.62, tokens/s = 42821 (42821 target) ; Learning rate = 0.000525 ; Loss = 1.629730\n",
      "2024-12-08 01:48:21.833000: I runner.py:310] Step = 28400 ; steps/s = 1.62, tokens/s = 42802 (42802 target) ; Learning rate = 0.000524 ; Loss = 1.626026\n",
      "2024-12-08 01:49:23.577000: I runner.py:310] Step = 28500 ; steps/s = 1.62, tokens/s = 42845 (42845 target) ; Learning rate = 0.000524 ; Loss = 1.629935\n",
      "2024-12-08 01:50:24.830000: I runner.py:310] Step = 28600 ; steps/s = 1.63, tokens/s = 42380 (42380 target) ; Learning rate = 0.000523 ; Loss = 1.601970\n",
      "2024-12-08 01:51:26.560000: I runner.py:310] Step = 28700 ; steps/s = 1.62, tokens/s = 42837 (42837 target) ; Learning rate = 0.000522 ; Loss = 1.624755\n",
      "2024-12-08 01:52:28.215000: I runner.py:310] Step = 28800 ; steps/s = 1.62, tokens/s = 42887 (42887 target) ; Learning rate = 0.000521 ; Loss = 1.620151\n",
      "2024-12-08 01:53:29.940000: I runner.py:310] Step = 28900 ; steps/s = 1.62, tokens/s = 42866 (42866 target) ; Learning rate = 0.000520 ; Loss = 1.624582\n",
      "2024-12-08 01:54:31.249000: I runner.py:310] Step = 29000 ; steps/s = 1.63, tokens/s = 42323 (42323 target) ; Learning rate = 0.000519 ; Loss = 1.640247\n",
      "2024-12-08 01:55:32.953000: I runner.py:310] Step = 29100 ; steps/s = 1.62, tokens/s = 42906 (42906 target) ; Learning rate = 0.000518 ; Loss = 1.614378\n",
      "2024-12-08 01:56:34.671000: I runner.py:310] Step = 29200 ; steps/s = 1.62, tokens/s = 42826 (42826 target) ; Learning rate = 0.000517 ; Loss = 1.607351\n",
      "2024-12-08 01:57:36.334000: I runner.py:310] Step = 29300 ; steps/s = 1.62, tokens/s = 42870 (42870 target) ; Learning rate = 0.000516 ; Loss = 1.612237\n",
      "2024-12-08 01:58:37.578000: I runner.py:310] Step = 29400 ; steps/s = 1.63, tokens/s = 42427 (42427 target) ; Learning rate = 0.000515 ; Loss = 1.628577\n",
      "2024-12-08 01:59:39.298000: I runner.py:310] Step = 29500 ; steps/s = 1.62, tokens/s = 42830 (42830 target) ; Learning rate = 0.000515 ; Loss = 1.614174\n",
      "2024-12-08 02:00:40.994000: I runner.py:310] Step = 29600 ; steps/s = 1.62, tokens/s = 42870 (42870 target) ; Learning rate = 0.000514 ; Loss = 1.616038\n",
      "2024-12-08 02:01:42.649000: I runner.py:310] Step = 29700 ; steps/s = 1.62, tokens/s = 42874 (42874 target) ; Learning rate = 0.000513 ; Loss = 1.607265\n",
      "2024-12-08 02:02:43.911000: I runner.py:310] Step = 29800 ; steps/s = 1.63, tokens/s = 42341 (42341 target) ; Learning rate = 0.000512 ; Loss = 1.591209\n",
      "2024-12-08 02:03:45.609000: I runner.py:310] Step = 29900 ; steps/s = 1.62, tokens/s = 42846 (42846 target) ; Learning rate = 0.000511 ; Loss = 1.617214\n",
      "2024-12-08 02:04:47.394000: I runner.py:310] Step = 30000 ; steps/s = 1.62, tokens/s = 42822 (42822 target) ; Learning rate = 0.000510 ; Loss = 1.620843\n",
      "2024-12-08 02:04:49.244000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-30000\n",
      "2024-12-08 02:04:49.244000: I training.py:192] Running evaluation for step 30000\n",
      "2024-12-08 02:10:32.235000: I training.py:192] Evaluation result for step 30000: loss = 1.096252 ; perplexity = 2.992927\n",
      "2024-12-08 02:11:33.803000: I runner.py:310] Step = 30100 ; steps/s = 1.62, tokens/s = 42974 (42974 target) ; Learning rate = 0.000509 ; Loss = 1.625192\n",
      "2024-12-08 02:12:35.149000: I runner.py:310] Step = 30200 ; steps/s = 1.63, tokens/s = 42317 (42317 target) ; Learning rate = 0.000509 ; Loss = 1.632130\n",
      "2024-12-08 02:13:36.868000: I runner.py:310] Step = 30300 ; steps/s = 1.62, tokens/s = 42862 (42862 target) ; Learning rate = 0.000508 ; Loss = 1.608735\n",
      "2024-12-08 02:14:38.593000: I runner.py:310] Step = 30400 ; steps/s = 1.62, tokens/s = 42840 (42840 target) ; Learning rate = 0.000507 ; Loss = 1.618111\n",
      "2024-12-08 02:15:40.293000: I runner.py:310] Step = 30500 ; steps/s = 1.62, tokens/s = 42860 (42860 target) ; Learning rate = 0.000506 ; Loss = 1.613857\n",
      "2024-12-08 02:16:41.586000: I runner.py:310] Step = 30600 ; steps/s = 1.63, tokens/s = 42358 (42358 target) ; Learning rate = 0.000505 ; Loss = 1.602743\n",
      "2024-12-08 02:17:43.273000: I runner.py:310] Step = 30700 ; steps/s = 1.62, tokens/s = 42894 (42894 target) ; Learning rate = 0.000504 ; Loss = 1.604228\n",
      "2024-12-08 02:18:45.010000: I runner.py:310] Step = 30800 ; steps/s = 1.62, tokens/s = 42827 (42827 target) ; Learning rate = 0.000504 ; Loss = 1.617769\n",
      "2024-12-08 02:19:46.742000: I runner.py:310] Step = 30900 ; steps/s = 1.62, tokens/s = 42820 (42820 target) ; Learning rate = 0.000503 ; Loss = 1.626891\n",
      "2024-12-08 02:20:48.992000: I runner.py:310] Step = 31000 ; steps/s = 1.61, tokens/s = 41668 (41668 target) ; Learning rate = 0.000502 ; Loss = 1.615364\n",
      "2024-12-08 02:21:52.349000: I runner.py:310] Step = 31100 ; steps/s = 1.58, tokens/s = 41730 (41730 target) ; Learning rate = 0.000501 ; Loss = 1.606458\n",
      "2024-12-08 02:22:55.136000: I runner.py:310] Step = 31200 ; steps/s = 1.59, tokens/s = 42117 (42117 target) ; Learning rate = 0.000500 ; Loss = 1.610204\n",
      "2024-12-08 02:23:56.471000: I runner.py:310] Step = 31300 ; steps/s = 1.63, tokens/s = 42515 (42515 target) ; Learning rate = 0.000500 ; Loss = 1.612616\n",
      "2024-12-08 02:24:58.132000: I runner.py:310] Step = 31400 ; steps/s = 1.62, tokens/s = 42752 (42752 target) ; Learning rate = 0.000499 ; Loss = 1.615109\n",
      "2024-12-08 02:25:59.818000: I runner.py:310] Step = 31500 ; steps/s = 1.62, tokens/s = 42850 (42850 target) ; Learning rate = 0.000498 ; Loss = 1.599840\n",
      "2024-12-08 02:27:01.586000: I runner.py:310] Step = 31600 ; steps/s = 1.62, tokens/s = 42806 (42806 target) ; Learning rate = 0.000497 ; Loss = 1.603281\n",
      "2024-12-08 02:28:02.853000: I runner.py:310] Step = 31700 ; steps/s = 1.63, tokens/s = 42358 (42358 target) ; Learning rate = 0.000496 ; Loss = 1.607043\n",
      "2024-12-08 02:29:04.578000: I runner.py:310] Step = 31800 ; steps/s = 1.62, tokens/s = 42873 (42873 target) ; Learning rate = 0.000496 ; Loss = 1.600683\n",
      "2024-12-08 02:30:06.196000: I runner.py:310] Step = 31900 ; steps/s = 1.62, tokens/s = 42921 (42921 target) ; Learning rate = 0.000495 ; Loss = 1.596659\n",
      "2024-12-08 02:31:07.866000: I runner.py:310] Step = 32000 ; steps/s = 1.62, tokens/s = 42884 (42884 target) ; Learning rate = 0.000494 ; Loss = 1.608773\n",
      "2024-12-08 02:32:09.111000: I runner.py:310] Step = 32100 ; steps/s = 1.63, tokens/s = 42357 (42357 target) ; Learning rate = 0.000493 ; Loss = 1.593484\n",
      "2024-12-08 02:33:10.772000: I runner.py:310] Step = 32200 ; steps/s = 1.62, tokens/s = 42876 (42876 target) ; Learning rate = 0.000493 ; Loss = 1.593901\n",
      "2024-12-08 02:34:12.429000: I runner.py:310] Step = 32300 ; steps/s = 1.62, tokens/s = 42873 (42873 target) ; Learning rate = 0.000492 ; Loss = 1.601757\n",
      "2024-12-08 02:35:14.159000: I runner.py:310] Step = 32400 ; steps/s = 1.62, tokens/s = 42859 (42859 target) ; Learning rate = 0.000491 ; Loss = 1.596875\n",
      "2024-12-08 02:36:15.490000: I runner.py:310] Step = 32500 ; steps/s = 1.63, tokens/s = 42348 (42348 target) ; Learning rate = 0.000490 ; Loss = 1.604561\n",
      "2024-12-08 02:37:17.151000: I runner.py:310] Step = 32600 ; steps/s = 1.62, tokens/s = 42901 (42901 target) ; Learning rate = 0.000490 ; Loss = 1.602980\n",
      "2024-12-08 02:38:18.837000: I runner.py:310] Step = 32700 ; steps/s = 1.62, tokens/s = 42874 (42874 target) ; Learning rate = 0.000489 ; Loss = 1.593788\n",
      "2024-12-08 02:39:20.507000: I runner.py:310] Step = 32800 ; steps/s = 1.62, tokens/s = 42845 (42845 target) ; Learning rate = 0.000488 ; Loss = 1.607201\n",
      "2024-12-08 02:40:21.746000: I runner.py:310] Step = 32900 ; steps/s = 1.63, tokens/s = 42404 (42404 target) ; Learning rate = 0.000487 ; Loss = 1.608108\n",
      "2024-12-08 02:41:23.387000: I runner.py:310] Step = 33000 ; steps/s = 1.62, tokens/s = 42907 (42907 target) ; Learning rate = 0.000487 ; Loss = 1.612885\n",
      "2024-12-08 02:42:25.062000: I runner.py:310] Step = 33100 ; steps/s = 1.62, tokens/s = 42882 (42882 target) ; Learning rate = 0.000486 ; Loss = 1.595441\n",
      "2024-12-08 02:43:26.691000: I runner.py:310] Step = 33200 ; steps/s = 1.62, tokens/s = 42893 (42893 target) ; Learning rate = 0.000485 ; Loss = 1.605607\n",
      "2024-12-08 02:44:27.914000: I runner.py:310] Step = 33300 ; steps/s = 1.63, tokens/s = 42393 (42393 target) ; Learning rate = 0.000484 ; Loss = 1.592739\n",
      "2024-12-08 02:45:29.559000: I runner.py:310] Step = 33400 ; steps/s = 1.62, tokens/s = 42879 (42879 target) ; Learning rate = 0.000484 ; Loss = 1.598876\n",
      "2024-12-08 02:46:31.240000: I runner.py:310] Step = 33500 ; steps/s = 1.62, tokens/s = 42886 (42886 target) ; Learning rate = 0.000483 ; Loss = 1.587037\n",
      "2024-12-08 02:47:32.962000: I runner.py:310] Step = 33600 ; steps/s = 1.62, tokens/s = 42882 (42882 target) ; Learning rate = 0.000482 ; Loss = 1.589417\n",
      "2024-12-08 02:48:34.168000: I runner.py:310] Step = 33700 ; steps/s = 1.63, tokens/s = 42408 (42408 target) ; Learning rate = 0.000481 ; Loss = 1.586433\n",
      "2024-12-08 02:49:35.835000: I runner.py:310] Step = 33800 ; steps/s = 1.62, tokens/s = 42886 (42886 target) ; Learning rate = 0.000481 ; Loss = 1.591332\n",
      "2024-12-08 02:50:37.624000: I runner.py:310] Step = 33900 ; steps/s = 1.62, tokens/s = 42809 (42809 target) ; Learning rate = 0.000480 ; Loss = 1.606535\n",
      "2024-12-08 02:51:39.329000: I runner.py:310] Step = 34000 ; steps/s = 1.62, tokens/s = 42854 (42854 target) ; Learning rate = 0.000479 ; Loss = 1.596103\n",
      "2024-12-08 02:52:40.569000: I runner.py:310] Step = 34100 ; steps/s = 1.63, tokens/s = 42350 (42350 target) ; Learning rate = 0.000479 ; Loss = 1.592895\n",
      "2024-12-08 02:53:42.269000: I runner.py:310] Step = 34200 ; steps/s = 1.62, tokens/s = 42881 (42881 target) ; Learning rate = 0.000478 ; Loss = 1.586200\n",
      "2024-12-08 02:54:43.983000: I runner.py:310] Step = 34300 ; steps/s = 1.62, tokens/s = 42862 (42862 target) ; Learning rate = 0.000477 ; Loss = 1.594102\n",
      "2024-12-08 02:55:45.743000: I runner.py:310] Step = 34400 ; steps/s = 1.62, tokens/s = 42820 (42820 target) ; Learning rate = 0.000477 ; Loss = 1.601330\n",
      "2024-12-08 02:56:47.094000: I runner.py:310] Step = 34500 ; steps/s = 1.63, tokens/s = 42320 (42320 target) ; Learning rate = 0.000476 ; Loss = 1.579885\n",
      "2024-12-08 02:57:48.852000: I runner.py:310] Step = 34600 ; steps/s = 1.62, tokens/s = 42834 (42834 target) ; Learning rate = 0.000475 ; Loss = 1.583279\n",
      "2024-12-08 02:58:50.610000: I runner.py:310] Step = 34700 ; steps/s = 1.62, tokens/s = 42822 (42822 target) ; Learning rate = 0.000474 ; Loss = 1.600779\n",
      "2024-12-08 02:59:52.393000: I runner.py:310] Step = 34800 ; steps/s = 1.62, tokens/s = 42792 (42792 target) ; Learning rate = 0.000474 ; Loss = 1.598453\n",
      "2024-12-08 03:00:53.570000: I runner.py:310] Step = 34900 ; steps/s = 1.63, tokens/s = 42422 (42422 target) ; Learning rate = 0.000473 ; Loss = 1.595776\n",
      "2024-12-08 03:01:55.323000: I runner.py:310] Step = 35000 ; steps/s = 1.62, tokens/s = 42818 (42818 target) ; Learning rate = 0.000472 ; Loss = 1.589228\n",
      "2024-12-08 03:01:55.324000: I training.py:192] Running evaluation for step 35000\n",
      "2024-12-08 03:07:29.821000: I training.py:192] Evaluation result for step 35000: loss = 1.108696 ; perplexity = 3.030404\n",
      "2024-12-08 03:08:31.400000: I runner.py:310] Step = 35100 ; steps/s = 1.62, tokens/s = 42955 (42955 target) ; Learning rate = 0.000472 ; Loss = 1.598607\n",
      "2024-12-08 03:09:33.147000: I runner.py:310] Step = 35200 ; steps/s = 1.62, tokens/s = 42827 (42827 target) ; Learning rate = 0.000471 ; Loss = 1.605311\n",
      "2024-12-08 03:10:34.458000: I runner.py:310] Step = 35300 ; steps/s = 1.63, tokens/s = 42336 (42336 target) ; Learning rate = 0.000470 ; Loss = 1.598151\n",
      "2024-12-08 03:11:36.208000: I runner.py:310] Step = 35400 ; steps/s = 1.62, tokens/s = 42847 (42847 target) ; Learning rate = 0.000470 ; Loss = 1.579017\n",
      "2024-12-08 03:12:37.997000: I runner.py:310] Step = 35500 ; steps/s = 1.62, tokens/s = 42797 (42797 target) ; Learning rate = 0.000469 ; Loss = 1.590842\n",
      "2024-12-08 03:13:39.761000: I runner.py:310] Step = 35600 ; steps/s = 1.62, tokens/s = 42810 (42810 target) ; Learning rate = 0.000468 ; Loss = 1.592981\n",
      "2024-12-08 03:14:40.999000: I runner.py:310] Step = 35700 ; steps/s = 1.63, tokens/s = 42380 (42380 target) ; Learning rate = 0.000468 ; Loss = 1.580925\n",
      "2024-12-08 03:15:42.693000: I runner.py:310] Step = 35800 ; steps/s = 1.62, tokens/s = 42889 (42889 target) ; Learning rate = 0.000467 ; Loss = 1.580505\n",
      "2024-12-08 03:16:44.392000: I runner.py:310] Step = 35900 ; steps/s = 1.62, tokens/s = 42876 (42876 target) ; Learning rate = 0.000466 ; Loss = 1.585241\n",
      "2024-12-08 03:17:46.156000: I runner.py:310] Step = 36000 ; steps/s = 1.62, tokens/s = 42794 (42794 target) ; Learning rate = 0.000466 ; Loss = 1.593152\n",
      "2024-12-08 03:18:47.394000: I runner.py:310] Step = 36100 ; steps/s = 1.63, tokens/s = 42359 (42359 target) ; Learning rate = 0.000465 ; Loss = 1.600104\n",
      "2024-12-08 03:19:49.156000: I runner.py:310] Step = 36200 ; steps/s = 1.62, tokens/s = 42826 (42826 target) ; Learning rate = 0.000465 ; Loss = 1.580872\n",
      "2024-12-08 03:20:50.968000: I runner.py:310] Step = 36300 ; steps/s = 1.62, tokens/s = 42789 (42789 target) ; Learning rate = 0.000464 ; Loss = 1.594145\n",
      "2024-12-08 03:21:52.786000: I runner.py:310] Step = 36400 ; steps/s = 1.62, tokens/s = 42753 (42753 target) ; Learning rate = 0.000463 ; Loss = 1.586579\n",
      "2024-12-08 03:22:54.079000: I runner.py:310] Step = 36500 ; steps/s = 1.63, tokens/s = 42383 (42383 target) ; Learning rate = 0.000463 ; Loss = 1.584352\n",
      "2024-12-08 03:23:55.821000: I runner.py:310] Step = 36600 ; steps/s = 1.62, tokens/s = 42831 (42831 target) ; Learning rate = 0.000462 ; Loss = 1.584538\n",
      "2024-12-08 03:24:57.562000: I runner.py:310] Step = 36700 ; steps/s = 1.62, tokens/s = 42819 (42819 target) ; Learning rate = 0.000461 ; Loss = 1.597780\n",
      "2024-12-08 03:25:59.287000: I runner.py:310] Step = 36800 ; steps/s = 1.62, tokens/s = 42851 (42851 target) ; Learning rate = 0.000461 ; Loss = 1.595000\n",
      "2024-12-08 03:27:00.604000: I runner.py:310] Step = 36900 ; steps/s = 1.63, tokens/s = 42314 (42314 target) ; Learning rate = 0.000460 ; Loss = 1.592904\n",
      "2024-12-08 03:28:02.324000: I runner.py:310] Step = 37000 ; steps/s = 1.62, tokens/s = 42884 (42884 target) ; Learning rate = 0.000460 ; Loss = 1.577585\n",
      "2024-12-08 03:29:04.071000: I runner.py:310] Step = 37100 ; steps/s = 1.62, tokens/s = 42833 (42833 target) ; Learning rate = 0.000459 ; Loss = 1.595983\n",
      "2024-12-08 03:30:05.848000: I runner.py:310] Step = 37200 ; steps/s = 1.62, tokens/s = 42820 (42820 target) ; Learning rate = 0.000458 ; Loss = 1.585927\n",
      "2024-12-08 03:31:07.161000: I runner.py:310] Step = 37300 ; steps/s = 1.63, tokens/s = 42299 (42299 target) ; Learning rate = 0.000458 ; Loss = 1.572421\n",
      "2024-12-08 03:32:08.870000: I runner.py:310] Step = 37400 ; steps/s = 1.62, tokens/s = 42846 (42846 target) ; Learning rate = 0.000457 ; Loss = 1.584876\n",
      "2024-12-08 03:33:10.596000: I runner.py:310] Step = 37500 ; steps/s = 1.62, tokens/s = 42876 (42876 target) ; Learning rate = 0.000456 ; Loss = 1.587259\n",
      "2024-12-08 03:34:12.383000: I runner.py:310] Step = 37600 ; steps/s = 1.62, tokens/s = 42787 (42787 target) ; Learning rate = 0.000456 ; Loss = 1.601585\n",
      "2024-12-08 03:35:13.673000: I runner.py:310] Step = 37700 ; steps/s = 1.63, tokens/s = 42347 (42347 target) ; Learning rate = 0.000455 ; Loss = 1.591296\n",
      "2024-12-08 03:36:15.409000: I runner.py:310] Step = 37800 ; steps/s = 1.62, tokens/s = 42836 (42836 target) ; Learning rate = 0.000455 ; Loss = 1.582728\n",
      "2024-12-08 03:37:17.174000: I runner.py:310] Step = 37900 ; steps/s = 1.62, tokens/s = 42831 (42831 target) ; Learning rate = 0.000454 ; Loss = 1.578503\n",
      "2024-12-08 03:38:18.931000: I runner.py:310] Step = 38000 ; steps/s = 1.62, tokens/s = 42828 (42828 target) ; Learning rate = 0.000453 ; Loss = 1.581666\n",
      "2024-12-08 03:39:20.235000: I runner.py:310] Step = 38100 ; steps/s = 1.63, tokens/s = 42336 (42336 target) ; Learning rate = 0.000453 ; Loss = 1.566440\n",
      "2024-12-08 03:40:22.024000: I runner.py:310] Step = 38200 ; steps/s = 1.62, tokens/s = 42785 (42785 target) ; Learning rate = 0.000452 ; Loss = 1.580810\n",
      "2024-12-08 03:41:23.729000: I runner.py:310] Step = 38300 ; steps/s = 1.62, tokens/s = 42876 (42876 target) ; Learning rate = 0.000452 ; Loss = 1.586001\n",
      "2024-12-08 03:42:25.520000: I runner.py:310] Step = 38400 ; steps/s = 1.62, tokens/s = 42800 (42800 target) ; Learning rate = 0.000451 ; Loss = 1.585196\n",
      "2024-12-08 03:43:26.775000: I runner.py:310] Step = 38500 ; steps/s = 1.63, tokens/s = 42372 (42372 target) ; Learning rate = 0.000450 ; Loss = 1.571731\n",
      "2024-12-08 03:44:28.521000: I runner.py:310] Step = 38600 ; steps/s = 1.62, tokens/s = 42848 (42848 target) ; Learning rate = 0.000450 ; Loss = 1.573994\n",
      "2024-12-08 03:45:30.283000: I runner.py:310] Step = 38700 ; steps/s = 1.62, tokens/s = 42808 (42808 target) ; Learning rate = 0.000449 ; Loss = 1.595487\n",
      "2024-12-08 03:46:32.032000: I runner.py:310] Step = 38800 ; steps/s = 1.62, tokens/s = 42806 (42806 target) ; Learning rate = 0.000449 ; Loss = 1.580557\n",
      "2024-12-08 03:47:33.256000: I runner.py:310] Step = 38900 ; steps/s = 1.63, tokens/s = 42380 (42380 target) ; Learning rate = 0.000448 ; Loss = 1.577157\n",
      "2024-12-08 03:48:34.989000: I runner.py:310] Step = 39000 ; steps/s = 1.62, tokens/s = 42815 (42815 target) ; Learning rate = 0.000448 ; Loss = 1.570140\n",
      "2024-12-08 03:49:36.717000: I runner.py:310] Step = 39100 ; steps/s = 1.62, tokens/s = 42863 (42863 target) ; Learning rate = 0.000447 ; Loss = 1.579032\n",
      "2024-12-08 03:50:38.487000: I runner.py:310] Step = 39200 ; steps/s = 1.62, tokens/s = 42855 (42855 target) ; Learning rate = 0.000446 ; Loss = 1.587307\n",
      "2024-12-08 03:51:39.800000: I runner.py:310] Step = 39300 ; steps/s = 1.63, tokens/s = 42333 (42333 target) ; Learning rate = 0.000446 ; Loss = 1.582527\n",
      "2024-12-08 03:52:41.549000: I runner.py:310] Step = 39400 ; steps/s = 1.62, tokens/s = 42832 (42832 target) ; Learning rate = 0.000445 ; Loss = 1.567448\n",
      "2024-12-08 03:53:43.259000: I runner.py:310] Step = 39500 ; steps/s = 1.62, tokens/s = 42849 (42849 target) ; Learning rate = 0.000445 ; Loss = 1.584105\n",
      "2024-12-08 03:54:44.957000: I runner.py:310] Step = 39600 ; steps/s = 1.62, tokens/s = 42849 (42849 target) ; Learning rate = 0.000444 ; Loss = 1.568233\n",
      "2024-12-08 03:55:46.310000: I runner.py:310] Step = 39700 ; steps/s = 1.63, tokens/s = 42304 (42304 target) ; Learning rate = 0.000444 ; Loss = 1.551916\n",
      "2024-12-08 03:56:48.066000: I runner.py:310] Step = 39800 ; steps/s = 1.62, tokens/s = 42831 (42831 target) ; Learning rate = 0.000443 ; Loss = 1.580556\n",
      "2024-12-08 03:57:49.817000: I runner.py:310] Step = 39900 ; steps/s = 1.62, tokens/s = 42838 (42838 target) ; Learning rate = 0.000442 ; Loss = 1.582484\n",
      "2024-12-08 03:58:51.615000: I runner.py:310] Step = 40000 ; steps/s = 1.62, tokens/s = 42776 (42776 target) ; Learning rate = 0.000442 ; Loss = 1.574948\n",
      "2024-12-08 03:58:53.566000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-40000\n",
      "2024-12-08 03:58:53.566000: I training.py:192] Running evaluation for step 40000\n",
      "2024-12-08 04:04:05.561000: I training.py:192] Evaluation result for step 40000: loss = 1.115330 ; perplexity = 3.050576\n",
      "2024-12-08 04:05:06.712000: I runner.py:310] Step = 40100 ; steps/s = 1.64, tokens/s = 42464 (42464 target) ; Learning rate = 0.000441 ; Loss = 1.563616\n",
      "2024-12-08 04:06:08.435000: I runner.py:310] Step = 40200 ; steps/s = 1.62, tokens/s = 42861 (42861 target) ; Learning rate = 0.000441 ; Loss = 1.573558\n",
      "2024-12-08 04:07:10.157000: I runner.py:310] Step = 40300 ; steps/s = 1.62, tokens/s = 42814 (42814 target) ; Learning rate = 0.000440 ; Loss = 1.587626\n",
      "2024-12-08 04:08:11.901000: I runner.py:310] Step = 40400 ; steps/s = 1.62, tokens/s = 42839 (42839 target) ; Learning rate = 0.000440 ; Loss = 1.592134\n",
      "2024-12-08 04:09:13.171000: I runner.py:310] Step = 40500 ; steps/s = 1.63, tokens/s = 42364 (42364 target) ; Learning rate = 0.000439 ; Loss = 1.562648\n",
      "2024-12-08 04:10:14.921000: I runner.py:310] Step = 40600 ; steps/s = 1.62, tokens/s = 42828 (42828 target) ; Learning rate = 0.000439 ; Loss = 1.567110\n",
      "2024-12-08 04:11:16.721000: I runner.py:310] Step = 40700 ; steps/s = 1.62, tokens/s = 42827 (42827 target) ; Learning rate = 0.000438 ; Loss = 1.580145\n",
      "2024-12-08 04:12:18.495000: I runner.py:310] Step = 40800 ; steps/s = 1.62, tokens/s = 42771 (42771 target) ; Learning rate = 0.000438 ; Loss = 1.585148\n",
      "2024-12-08 04:13:19.791000: I runner.py:310] Step = 40900 ; steps/s = 1.63, tokens/s = 42355 (42355 target) ; Learning rate = 0.000437 ; Loss = 1.566997\n",
      "2024-12-08 04:14:21.532000: I runner.py:310] Step = 41000 ; steps/s = 1.62, tokens/s = 42824 (42824 target) ; Learning rate = 0.000437 ; Loss = 1.566189\n",
      "2024-12-08 04:15:23.309000: I runner.py:310] Step = 41100 ; steps/s = 1.62, tokens/s = 42793 (42793 target) ; Learning rate = 0.000436 ; Loss = 1.568066\n",
      "2024-12-08 04:16:25.111000: I runner.py:310] Step = 41200 ; steps/s = 1.62, tokens/s = 42817 (42817 target) ; Learning rate = 0.000435 ; Loss = 1.572510\n",
      "2024-12-08 04:17:26.449000: I runner.py:310] Step = 41300 ; steps/s = 1.63, tokens/s = 42303 (42303 target) ; Learning rate = 0.000435 ; Loss = 1.573817\n",
      "2024-12-08 04:18:28.219000: I runner.py:310] Step = 41400 ; steps/s = 1.62, tokens/s = 42836 (42836 target) ; Learning rate = 0.000434 ; Loss = 1.578115\n",
      "2024-12-08 04:19:29.972000: I runner.py:310] Step = 41500 ; steps/s = 1.62, tokens/s = 42819 (42819 target) ; Learning rate = 0.000434 ; Loss = 1.581753\n",
      "2024-12-08 04:20:31.632000: I runner.py:310] Step = 41600 ; steps/s = 1.62, tokens/s = 42632 (42632 target) ; Learning rate = 0.000433 ; Loss = 1.586269\n",
      "2024-12-08 04:21:33.111000: I runner.py:310] Step = 41700 ; steps/s = 1.63, tokens/s = 42504 (42504 target) ; Learning rate = 0.000433 ; Loss = 1.580024\n",
      "2024-12-08 04:22:34.867000: I runner.py:310] Step = 41800 ; steps/s = 1.62, tokens/s = 42816 (42816 target) ; Learning rate = 0.000432 ; Loss = 1.564252\n",
      "2024-12-08 04:23:36.523000: I runner.py:310] Step = 41900 ; steps/s = 1.62, tokens/s = 42910 (42910 target) ; Learning rate = 0.000432 ; Loss = 1.565790\n",
      "2024-12-08 04:24:37.889000: I runner.py:310] Step = 42000 ; steps/s = 1.63, tokens/s = 42248 (42248 target) ; Learning rate = 0.000431 ; Loss = 1.567260\n",
      "2024-12-08 04:25:39.615000: I runner.py:310] Step = 42100 ; steps/s = 1.62, tokens/s = 42885 (42885 target) ; Learning rate = 0.000431 ; Loss = 1.568619\n",
      "2024-12-08 04:26:41.322000: I runner.py:310] Step = 42200 ; steps/s = 1.62, tokens/s = 42827 (42827 target) ; Learning rate = 0.000430 ; Loss = 1.568574\n",
      "2024-12-08 04:27:43.024000: I runner.py:310] Step = 42300 ; steps/s = 1.62, tokens/s = 42860 (42860 target) ; Learning rate = 0.000430 ; Loss = 1.564452\n",
      "2024-12-08 04:28:44.407000: I runner.py:310] Step = 42400 ; steps/s = 1.63, tokens/s = 42283 (42283 target) ; Learning rate = 0.000429 ; Loss = 1.576004\n",
      "2024-12-08 04:29:46.123000: I runner.py:310] Step = 42500 ; steps/s = 1.62, tokens/s = 42883 (42883 target) ; Learning rate = 0.000429 ; Loss = 1.567204\n",
      "2024-12-08 04:30:47.911000: I runner.py:310] Step = 42600 ; steps/s = 1.62, tokens/s = 42820 (42820 target) ; Learning rate = 0.000428 ; Loss = 1.578426\n",
      "2024-12-08 04:31:49.687000: I runner.py:310] Step = 42700 ; steps/s = 1.62, tokens/s = 42772 (42772 target) ; Learning rate = 0.000428 ; Loss = 1.571201\n",
      "2024-12-08 04:32:50.926000: I runner.py:310] Step = 42800 ; steps/s = 1.63, tokens/s = 42377 (42377 target) ; Learning rate = 0.000427 ; Loss = 1.564987\n",
      "2024-12-08 04:33:52.727000: I runner.py:310] Step = 42900 ; steps/s = 1.62, tokens/s = 42791 (42791 target) ; Learning rate = 0.000427 ; Loss = 1.557470\n",
      "2024-12-08 04:34:54.484000: I runner.py:310] Step = 43000 ; steps/s = 1.62, tokens/s = 42846 (42846 target) ; Learning rate = 0.000426 ; Loss = 1.572204\n",
      "2024-12-08 04:35:56.200000: I runner.py:310] Step = 43100 ; steps/s = 1.62, tokens/s = 42854 (42854 target) ; Learning rate = 0.000426 ; Loss = 1.571538\n",
      "2024-12-08 04:36:57.558000: I runner.py:310] Step = 43200 ; steps/s = 1.63, tokens/s = 42287 (42287 target) ; Learning rate = 0.000425 ; Loss = 1.565211\n",
      "2024-12-08 04:37:59.278000: I runner.py:310] Step = 43300 ; steps/s = 1.62, tokens/s = 42833 (42833 target) ; Learning rate = 0.000425 ; Loss = 1.567087\n",
      "2024-12-08 04:39:00.980000: I runner.py:310] Step = 43400 ; steps/s = 1.62, tokens/s = 42833 (42833 target) ; Learning rate = 0.000424 ; Loss = 1.569024\n",
      "2024-12-08 04:40:02.657000: I runner.py:310] Step = 43500 ; steps/s = 1.62, tokens/s = 42922 (42922 target) ; Learning rate = 0.000424 ; Loss = 1.562579\n",
      "2024-12-08 04:41:04.000000: I runner.py:310] Step = 43600 ; steps/s = 1.63, tokens/s = 42298 (42298 target) ; Learning rate = 0.000423 ; Loss = 1.578075\n",
      "2024-12-08 04:42:05.742000: I runner.py:310] Step = 43700 ; steps/s = 1.62, tokens/s = 42856 (42856 target) ; Learning rate = 0.000423 ; Loss = 1.562183\n",
      "2024-12-08 04:43:07.427000: I runner.py:310] Step = 43800 ; steps/s = 1.62, tokens/s = 42865 (42865 target) ; Learning rate = 0.000422 ; Loss = 1.561687\n",
      "2024-12-08 04:44:09.097000: I runner.py:310] Step = 43900 ; steps/s = 1.62, tokens/s = 42883 (42883 target) ; Learning rate = 0.000422 ; Loss = 1.566284\n",
      "2024-12-08 04:45:10.486000: I runner.py:310] Step = 44000 ; steps/s = 1.63, tokens/s = 42289 (42289 target) ; Learning rate = 0.000421 ; Loss = 1.559161\n",
      "2024-12-08 04:46:12.141000: I runner.py:310] Step = 44100 ; steps/s = 1.62, tokens/s = 42898 (42898 target) ; Learning rate = 0.000421 ; Loss = 1.567200\n",
      "2024-12-08 04:47:13.980000: I runner.py:310] Step = 44200 ; steps/s = 1.62, tokens/s = 42780 (42780 target) ; Learning rate = 0.000420 ; Loss = 1.568383\n",
      "2024-12-08 04:48:15.706000: I runner.py:310] Step = 44300 ; steps/s = 1.62, tokens/s = 42817 (42817 target) ; Learning rate = 0.000420 ; Loss = 1.569210\n",
      "2024-12-08 04:49:16.985000: I runner.py:310] Step = 44400 ; steps/s = 1.63, tokens/s = 42355 (42355 target) ; Learning rate = 0.000419 ; Loss = 1.567476\n",
      "2024-12-08 04:50:18.697000: I runner.py:310] Step = 44500 ; steps/s = 1.62, tokens/s = 42847 (42847 target) ; Learning rate = 0.000419 ; Loss = 1.554023\n",
      "2024-12-08 04:51:20.453000: I runner.py:310] Step = 44600 ; steps/s = 1.62, tokens/s = 42812 (42812 target) ; Learning rate = 0.000419 ; Loss = 1.563284\n",
      "2024-12-08 04:52:22.258000: I runner.py:310] Step = 44700 ; steps/s = 1.62, tokens/s = 42822 (42822 target) ; Learning rate = 0.000418 ; Loss = 1.573098\n",
      "2024-12-08 04:53:23.546000: I runner.py:310] Step = 44800 ; steps/s = 1.63, tokens/s = 42351 (42351 target) ; Learning rate = 0.000418 ; Loss = 1.556296\n",
      "2024-12-08 04:54:25.270000: I runner.py:310] Step = 44900 ; steps/s = 1.62, tokens/s = 42845 (42845 target) ; Learning rate = 0.000417 ; Loss = 1.557077\n",
      "2024-12-08 04:55:27.024000: I runner.py:310] Step = 45000 ; steps/s = 1.62, tokens/s = 42812 (42812 target) ; Learning rate = 0.000417 ; Loss = 1.562547\n",
      "2024-12-08 04:55:27.025000: I training.py:192] Running evaluation for step 45000\n",
      "2024-12-08 05:00:24.792000: I training.py:192] Evaluation result for step 45000: loss = 1.126124 ; perplexity = 3.083680\n",
      "2024-12-08 05:01:26.355000: I runner.py:310] Step = 45100 ; steps/s = 1.62, tokens/s = 42988 (42988 target) ; Learning rate = 0.000416 ; Loss = 1.567403\n",
      "2024-12-08 05:02:27.531000: I runner.py:310] Step = 45200 ; steps/s = 1.63, tokens/s = 42416 (42416 target) ; Learning rate = 0.000416 ; Loss = 1.570089\n",
      "2024-12-08 05:03:29.242000: I runner.py:310] Step = 45300 ; steps/s = 1.62, tokens/s = 42834 (42834 target) ; Learning rate = 0.000415 ; Loss = 1.557053\n",
      "2024-12-08 05:04:31.086000: I runner.py:310] Step = 45400 ; steps/s = 1.62, tokens/s = 42804 (42804 target) ; Learning rate = 0.000415 ; Loss = 1.554901\n",
      "2024-12-08 05:05:32.819000: I runner.py:310] Step = 45500 ; steps/s = 1.62, tokens/s = 42820 (42820 target) ; Learning rate = 0.000414 ; Loss = 1.560697\n",
      "2024-12-08 05:06:34.111000: I runner.py:310] Step = 45600 ; steps/s = 1.63, tokens/s = 42336 (42336 target) ; Learning rate = 0.000414 ; Loss = 1.554886\n",
      "2024-12-08 05:07:35.850000: I runner.py:310] Step = 45700 ; steps/s = 1.62, tokens/s = 42841 (42841 target) ; Learning rate = 0.000413 ; Loss = 1.554828\n",
      "2024-12-08 05:08:37.651000: I runner.py:310] Step = 45800 ; steps/s = 1.62, tokens/s = 42789 (42789 target) ; Learning rate = 0.000413 ; Loss = 1.555257\n",
      "2024-12-08 05:09:39.491000: I runner.py:310] Step = 45900 ; steps/s = 1.62, tokens/s = 42761 (42761 target) ; Learning rate = 0.000413 ; Loss = 1.562281\n",
      "2024-12-08 05:10:40.727000: I runner.py:310] Step = 46000 ; steps/s = 1.63, tokens/s = 42382 (42382 target) ; Learning rate = 0.000412 ; Loss = 1.563003\n",
      "2024-12-08 05:11:42.534000: I runner.py:310] Step = 46100 ; steps/s = 1.62, tokens/s = 42786 (42786 target) ; Learning rate = 0.000412 ; Loss = 1.547698\n",
      "2024-12-08 05:12:44.342000: I runner.py:310] Step = 46200 ; steps/s = 1.62, tokens/s = 42816 (42816 target) ; Learning rate = 0.000411 ; Loss = 1.561459\n",
      "2024-12-08 05:13:46.051000: I runner.py:310] Step = 46300 ; steps/s = 1.62, tokens/s = 42842 (42842 target) ; Learning rate = 0.000411 ; Loss = 1.560287\n",
      "2024-12-08 05:14:47.356000: I runner.py:310] Step = 46400 ; steps/s = 1.63, tokens/s = 42310 (42310 target) ; Learning rate = 0.000410 ; Loss = 1.560205\n",
      "2024-12-08 05:15:49.076000: I runner.py:310] Step = 46500 ; steps/s = 1.62, tokens/s = 42884 (42884 target) ; Learning rate = 0.000410 ; Loss = 1.560315\n",
      "2024-12-08 05:16:50.856000: I runner.py:310] Step = 46600 ; steps/s = 1.62, tokens/s = 42790 (42790 target) ; Learning rate = 0.000409 ; Loss = 1.561360\n",
      "2024-12-08 05:17:52.568000: I runner.py:310] Step = 46700 ; steps/s = 1.62, tokens/s = 42875 (42875 target) ; Learning rate = 0.000409 ; Loss = 1.563037\n",
      "2024-12-08 05:18:53.910000: I runner.py:310] Step = 46800 ; steps/s = 1.63, tokens/s = 42323 (42323 target) ; Learning rate = 0.000409 ; Loss = 1.554154\n",
      "2024-12-08 05:19:55.627000: I runner.py:310] Step = 46900 ; steps/s = 1.62, tokens/s = 42812 (42812 target) ; Learning rate = 0.000408 ; Loss = 1.548846\n",
      "2024-12-08 05:20:57.405000: I runner.py:310] Step = 47000 ; steps/s = 1.62, tokens/s = 42819 (42819 target) ; Learning rate = 0.000408 ; Loss = 1.558926\n",
      "2024-12-08 05:21:59.200000: I runner.py:310] Step = 47100 ; steps/s = 1.62, tokens/s = 42783 (42783 target) ; Learning rate = 0.000407 ; Loss = 1.567789\n",
      "2024-12-08 05:23:00.549000: I runner.py:310] Step = 47200 ; steps/s = 1.63, tokens/s = 42333 (42333 target) ; Learning rate = 0.000407 ; Loss = 1.567387\n",
      "2024-12-08 05:24:02.281000: I runner.py:310] Step = 47300 ; steps/s = 1.62, tokens/s = 42849 (42849 target) ; Learning rate = 0.000406 ; Loss = 1.556814\n",
      "2024-12-08 05:25:04.063000: I runner.py:310] Step = 47400 ; steps/s = 1.62, tokens/s = 42807 (42807 target) ; Learning rate = 0.000406 ; Loss = 1.559459\n",
      "2024-12-08 05:26:05.867000: I runner.py:310] Step = 47500 ; steps/s = 1.62, tokens/s = 42772 (42772 target) ; Learning rate = 0.000406 ; Loss = 1.557056\n",
      "2024-12-08 05:27:07.202000: I runner.py:310] Step = 47600 ; steps/s = 1.63, tokens/s = 42329 (42329 target) ; Learning rate = 0.000405 ; Loss = 1.548159\n",
      "2024-12-08 05:28:09.018000: I runner.py:310] Step = 47700 ; steps/s = 1.62, tokens/s = 42804 (42804 target) ; Learning rate = 0.000405 ; Loss = 1.561148\n",
      "2024-12-08 05:29:10.767000: I runner.py:310] Step = 47800 ; steps/s = 1.62, tokens/s = 42850 (42850 target) ; Learning rate = 0.000404 ; Loss = 1.558929\n",
      "2024-12-08 05:30:12.454000: I runner.py:310] Step = 47900 ; steps/s = 1.62, tokens/s = 42842 (42842 target) ; Learning rate = 0.000404 ; Loss = 1.563299\n",
      "2024-12-08 05:31:13.710000: I runner.py:310] Step = 48000 ; steps/s = 1.63, tokens/s = 42343 (42343 target) ; Learning rate = 0.000403 ; Loss = 1.551652\n",
      "2024-12-08 05:32:15.456000: I runner.py:310] Step = 48100 ; steps/s = 1.62, tokens/s = 42833 (42833 target) ; Learning rate = 0.000403 ; Loss = 1.555861\n",
      "2024-12-08 05:33:17.191000: I runner.py:310] Step = 48200 ; steps/s = 1.62, tokens/s = 42841 (42841 target) ; Learning rate = 0.000403 ; Loss = 1.566758\n",
      "2024-12-08 05:34:18.930000: I runner.py:310] Step = 48300 ; steps/s = 1.62, tokens/s = 42818 (42818 target) ; Learning rate = 0.000402 ; Loss = 1.565035\n",
      "2024-12-08 05:35:20.267000: I runner.py:310] Step = 48400 ; steps/s = 1.63, tokens/s = 42335 (42335 target) ; Learning rate = 0.000402 ; Loss = 1.553489\n",
      "2024-12-08 05:36:22.050000: I runner.py:310] Step = 48500 ; steps/s = 1.62, tokens/s = 42817 (42817 target) ; Learning rate = 0.000401 ; Loss = 1.543525\n",
      "2024-12-08 05:37:23.722000: I runner.py:310] Step = 48600 ; steps/s = 1.62, tokens/s = 42888 (42888 target) ; Learning rate = 0.000401 ; Loss = 1.556532\n",
      "2024-12-08 05:38:25.457000: I runner.py:310] Step = 48700 ; steps/s = 1.62, tokens/s = 42822 (42822 target) ; Learning rate = 0.000401 ; Loss = 1.555903\n",
      "2024-12-08 05:39:26.779000: I runner.py:310] Step = 48800 ; steps/s = 1.63, tokens/s = 42340 (42340 target) ; Learning rate = 0.000400 ; Loss = 1.547020\n",
      "2024-12-08 05:40:28.481000: I runner.py:310] Step = 48900 ; steps/s = 1.62, tokens/s = 42859 (42859 target) ; Learning rate = 0.000400 ; Loss = 1.550278\n",
      "2024-12-08 05:41:30.195000: I runner.py:310] Step = 49000 ; steps/s = 1.62, tokens/s = 42852 (42852 target) ; Learning rate = 0.000399 ; Loss = 1.556358\n",
      "2024-12-08 05:42:32.020000: I runner.py:310] Step = 49100 ; steps/s = 1.62, tokens/s = 42765 (42765 target) ; Learning rate = 0.000399 ; Loss = 1.549770\n",
      "2024-12-08 05:43:33.306000: I runner.py:310] Step = 49200 ; steps/s = 1.63, tokens/s = 42366 (42366 target) ; Learning rate = 0.000398 ; Loss = 1.546770\n",
      "2024-12-08 05:44:35.109000: I runner.py:310] Step = 49300 ; steps/s = 1.62, tokens/s = 42790 (42790 target) ; Learning rate = 0.000398 ; Loss = 1.553717\n",
      "2024-12-08 05:45:36.861000: I runner.py:310] Step = 49400 ; steps/s = 1.62, tokens/s = 42818 (42818 target) ; Learning rate = 0.000398 ; Loss = 1.546896\n",
      "2024-12-08 05:46:38.549000: I runner.py:310] Step = 49500 ; steps/s = 1.62, tokens/s = 42860 (42860 target) ; Learning rate = 0.000397 ; Loss = 1.562395\n",
      "2024-12-08 05:47:39.840000: I runner.py:310] Step = 49600 ; steps/s = 1.63, tokens/s = 42358 (42358 target) ; Learning rate = 0.000397 ; Loss = 1.543290\n",
      "2024-12-08 05:48:41.614000: I runner.py:310] Step = 49700 ; steps/s = 1.62, tokens/s = 42792 (42792 target) ; Learning rate = 0.000396 ; Loss = 1.555583\n",
      "2024-12-08 05:49:43.268000: I runner.py:310] Step = 49800 ; steps/s = 1.62, tokens/s = 42902 (42902 target) ; Learning rate = 0.000396 ; Loss = 1.554305\n",
      "2024-12-08 05:50:45.037000: I runner.py:310] Step = 49900 ; steps/s = 1.62, tokens/s = 42826 (42826 target) ; Learning rate = 0.000396 ; Loss = 1.549331\n",
      "2024-12-08 05:51:46.300000: I runner.py:310] Step = 50000 ; steps/s = 1.63, tokens/s = 42371 (42371 target) ; Learning rate = 0.000395 ; Loss = 1.544512\n",
      "2024-12-08 05:51:48.319000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-50000\n",
      "2024-12-08 05:51:48.319000: I training.py:192] Running evaluation for step 50000\n",
      "2024-12-08 05:56:38.321000: I training.py:192] Evaluation result for step 50000: loss = 1.132833 ; perplexity = 3.104439\n",
      "2024-12-08 05:57:39.874000: I runner.py:310] Step = 50100 ; steps/s = 1.63, tokens/s = 42971 (42971 target) ; Learning rate = 0.000395 ; Loss = 1.554024\n",
      "2024-12-08 05:58:41.574000: I runner.py:310] Step = 50200 ; steps/s = 1.62, tokens/s = 42859 (42859 target) ; Learning rate = 0.000394 ; Loss = 1.561779\n",
      "2024-12-08 05:59:43.309000: I runner.py:310] Step = 50300 ; steps/s = 1.62, tokens/s = 42823 (42823 target) ; Learning rate = 0.000394 ; Loss = 1.558300\n",
      "2024-12-08 06:00:44.624000: I runner.py:310] Step = 50400 ; steps/s = 1.63, tokens/s = 42377 (42377 target) ; Learning rate = 0.000394 ; Loss = 1.535183\n",
      "2024-12-08 06:01:46.347000: I runner.py:310] Step = 50500 ; steps/s = 1.62, tokens/s = 42815 (42815 target) ; Learning rate = 0.000393 ; Loss = 1.547656\n",
      "2024-12-08 06:02:48.104000: I runner.py:310] Step = 50600 ; steps/s = 1.62, tokens/s = 42803 (42803 target) ; Learning rate = 0.000393 ; Loss = 1.554664\n",
      "2024-12-08 06:03:49.837000: I runner.py:310] Step = 50700 ; steps/s = 1.62, tokens/s = 42853 (42853 target) ; Learning rate = 0.000393 ; Loss = 1.560889\n",
      "2024-12-08 06:04:51.134000: I runner.py:310] Step = 50800 ; steps/s = 1.63, tokens/s = 42353 (42353 target) ; Learning rate = 0.000392 ; Loss = 1.541747\n",
      "2024-12-08 06:05:52.830000: I runner.py:310] Step = 50900 ; steps/s = 1.62, tokens/s = 42859 (42859 target) ; Learning rate = 0.000392 ; Loss = 1.554769\n",
      "2024-12-08 06:06:54.498000: I runner.py:310] Step = 51000 ; steps/s = 1.62, tokens/s = 42927 (42927 target) ; Learning rate = 0.000391 ; Loss = 1.552035\n",
      "2024-12-08 06:07:56.233000: I runner.py:310] Step = 51100 ; steps/s = 1.62, tokens/s = 42819 (42819 target) ; Learning rate = 0.000391 ; Loss = 1.561832\n",
      "2024-12-08 06:08:57.541000: I runner.py:310] Step = 51200 ; steps/s = 1.63, tokens/s = 42315 (42315 target) ; Learning rate = 0.000391 ; Loss = 1.546846\n",
      "2024-12-08 06:09:59.264000: I runner.py:310] Step = 51300 ; steps/s = 1.62, tokens/s = 42878 (42878 target) ; Learning rate = 0.000390 ; Loss = 1.554525\n",
      "2024-12-08 06:11:01.015000: I runner.py:310] Step = 51400 ; steps/s = 1.62, tokens/s = 42818 (42818 target) ; Learning rate = 0.000390 ; Loss = 1.547571\n",
      "2024-12-08 06:12:02.740000: I runner.py:310] Step = 51500 ; steps/s = 1.62, tokens/s = 42828 (42828 target) ; Learning rate = 0.000389 ; Loss = 1.541935\n",
      "2024-12-08 06:13:03.973000: I runner.py:310] Step = 51600 ; steps/s = 1.63, tokens/s = 42396 (42396 target) ; Learning rate = 0.000389 ; Loss = 1.555045\n",
      "2024-12-08 06:14:05.762000: I runner.py:310] Step = 51700 ; steps/s = 1.62, tokens/s = 42810 (42810 target) ; Learning rate = 0.000389 ; Loss = 1.549105\n",
      "2024-12-08 06:15:07.507000: I runner.py:310] Step = 51800 ; steps/s = 1.62, tokens/s = 42805 (42805 target) ; Learning rate = 0.000388 ; Loss = 1.550941\n",
      "2024-12-08 06:16:09.181000: I runner.py:310] Step = 51900 ; steps/s = 1.62, tokens/s = 42885 (42885 target) ; Learning rate = 0.000388 ; Loss = 1.555408\n",
      "2024-12-08 06:17:10.366000: I runner.py:310] Step = 52000 ; steps/s = 1.63, tokens/s = 42415 (42415 target) ; Learning rate = 0.000388 ; Loss = 1.540990\n",
      "2024-12-08 06:18:12.131000: I runner.py:310] Step = 52100 ; steps/s = 1.62, tokens/s = 42832 (42832 target) ; Learning rate = 0.000387 ; Loss = 1.551432\n",
      "2024-12-08 06:19:13.775000: I runner.py:310] Step = 52200 ; steps/s = 1.62, tokens/s = 42888 (42888 target) ; Learning rate = 0.000387 ; Loss = 1.561061\n",
      "2024-12-08 06:20:15.024000: I runner.py:310] Step = 52300 ; steps/s = 1.63, tokens/s = 42441 (42441 target) ; Learning rate = 0.000386 ; Loss = 1.560293\n",
      "2024-12-08 06:21:16.683000: I runner.py:310] Step = 52400 ; steps/s = 1.62, tokens/s = 42817 (42817 target) ; Learning rate = 0.000386 ; Loss = 1.543574\n",
      "2024-12-08 06:22:18.399000: I runner.py:310] Step = 52500 ; steps/s = 1.62, tokens/s = 42853 (42853 target) ; Learning rate = 0.000386 ; Loss = 1.549088\n",
      "2024-12-08 06:23:20.105000: I runner.py:310] Step = 52600 ; steps/s = 1.62, tokens/s = 42862 (42862 target) ; Learning rate = 0.000385 ; Loss = 1.545982\n",
      "2024-12-08 06:24:21.391000: I runner.py:310] Step = 52700 ; steps/s = 1.63, tokens/s = 42349 (42349 target) ; Learning rate = 0.000385 ; Loss = 1.547269\n",
      "2024-12-08 06:25:23.103000: I runner.py:310] Step = 52800 ; steps/s = 1.62, tokens/s = 42863 (42863 target) ; Learning rate = 0.000385 ; Loss = 1.531334\n",
      "2024-12-08 06:26:24.813000: I runner.py:310] Step = 52900 ; steps/s = 1.62, tokens/s = 42888 (42888 target) ; Learning rate = 0.000384 ; Loss = 1.554351\n",
      "2024-12-08 06:27:26.464000: I runner.py:310] Step = 53000 ; steps/s = 1.62, tokens/s = 42850 (42850 target) ; Learning rate = 0.000384 ; Loss = 1.552758\n",
      "2024-12-08 06:28:27.798000: I runner.py:310] Step = 53100 ; steps/s = 1.63, tokens/s = 42328 (42328 target) ; Learning rate = 0.000384 ; Loss = 1.541624\n",
      "2024-12-08 06:29:29.499000: I runner.py:310] Step = 53200 ; steps/s = 1.62, tokens/s = 42891 (42891 target) ; Learning rate = 0.000383 ; Loss = 1.539411\n",
      "2024-12-08 06:30:31.220000: I runner.py:310] Step = 53300 ; steps/s = 1.62, tokens/s = 42862 (42862 target) ; Learning rate = 0.000383 ; Loss = 1.552099\n",
      "2024-12-08 06:31:32.940000: I runner.py:310] Step = 53400 ; steps/s = 1.62, tokens/s = 42858 (42858 target) ; Learning rate = 0.000382 ; Loss = 1.549147\n",
      "2024-12-08 06:32:34.266000: I runner.py:310] Step = 53500 ; steps/s = 1.63, tokens/s = 42274 (42274 target) ; Learning rate = 0.000382 ; Loss = 1.546502\n",
      "2024-12-08 06:33:35.990000: I runner.py:310] Step = 53600 ; steps/s = 1.62, tokens/s = 42845 (42845 target) ; Learning rate = 0.000382 ; Loss = 1.544167\n",
      "2024-12-08 06:34:37.692000: I runner.py:310] Step = 53700 ; steps/s = 1.62, tokens/s = 42838 (42838 target) ; Learning rate = 0.000381 ; Loss = 1.545482\n",
      "2024-12-08 06:35:39.439000: I runner.py:310] Step = 53800 ; steps/s = 1.62, tokens/s = 42849 (42849 target) ; Learning rate = 0.000381 ; Loss = 1.546518\n",
      "2024-12-08 06:36:40.789000: I runner.py:310] Step = 53900 ; steps/s = 1.63, tokens/s = 42320 (42320 target) ; Learning rate = 0.000381 ; Loss = 1.544516\n",
      "2024-12-08 06:37:42.478000: I runner.py:310] Step = 54000 ; steps/s = 1.62, tokens/s = 42904 (42904 target) ; Learning rate = 0.000380 ; Loss = 1.539773\n",
      "2024-12-08 06:38:44.185000: I runner.py:310] Step = 54100 ; steps/s = 1.62, tokens/s = 42847 (42847 target) ; Learning rate = 0.000380 ; Loss = 1.541931\n",
      "2024-12-08 06:39:45.903000: I runner.py:310] Step = 54200 ; steps/s = 1.62, tokens/s = 42821 (42821 target) ; Learning rate = 0.000380 ; Loss = 1.557871\n",
      "2024-12-08 06:40:47.117000: I runner.py:310] Step = 54300 ; steps/s = 1.63, tokens/s = 42410 (42410 target) ; Learning rate = 0.000379 ; Loss = 1.543014\n",
      "2024-12-08 06:41:48.761000: I runner.py:310] Step = 54400 ; steps/s = 1.62, tokens/s = 42909 (42909 target) ; Learning rate = 0.000379 ; Loss = 1.547375\n",
      "2024-12-08 06:42:50.478000: I runner.py:310] Step = 54500 ; steps/s = 1.62, tokens/s = 42865 (42865 target) ; Learning rate = 0.000379 ; Loss = 1.547498\n",
      "2024-12-08 06:43:52.292000: I runner.py:310] Step = 54600 ; steps/s = 1.62, tokens/s = 42790 (42790 target) ; Learning rate = 0.000378 ; Loss = 1.541867\n",
      "2024-12-08 06:44:53.534000: I runner.py:310] Step = 54700 ; steps/s = 1.63, tokens/s = 42356 (42356 target) ; Learning rate = 0.000378 ; Loss = 1.539428\n",
      "2024-12-08 06:45:55.206000: I runner.py:310] Step = 54800 ; steps/s = 1.62, tokens/s = 42879 (42879 target) ; Learning rate = 0.000378 ; Loss = 1.530672\n",
      "2024-12-08 06:46:56.969000: I runner.py:310] Step = 54900 ; steps/s = 1.62, tokens/s = 42826 (42826 target) ; Learning rate = 0.000377 ; Loss = 1.543337\n",
      "2024-12-08 06:47:58.648000: I runner.py:310] Step = 55000 ; steps/s = 1.62, tokens/s = 42854 (42854 target) ; Learning rate = 0.000377 ; Loss = 1.547824\n",
      "2024-12-08 06:47:58.649000: I training.py:192] Running evaluation for step 55000\n",
      "2024-12-08 06:53:01.761000: I training.py:192] Evaluation result for step 55000: loss = 1.142040 ; perplexity = 3.133154\n",
      "2024-12-08 06:54:02.874000: I runner.py:310] Step = 55100 ; steps/s = 1.64, tokens/s = 42482 (42482 target) ; Learning rate = 0.000377 ; Loss = 1.548957\n",
      "2024-12-08 06:55:04.601000: I runner.py:310] Step = 55200 ; steps/s = 1.62, tokens/s = 42871 (42871 target) ; Learning rate = 0.000376 ; Loss = 1.543044\n",
      "2024-12-08 06:56:06.366000: I runner.py:310] Step = 55300 ; steps/s = 1.62, tokens/s = 42809 (42809 target) ; Learning rate = 0.000376 ; Loss = 1.538344\n",
      "2024-12-08 06:57:08.110000: I runner.py:310] Step = 55400 ; steps/s = 1.62, tokens/s = 42809 (42809 target) ; Learning rate = 0.000376 ; Loss = 1.545115\n",
      "2024-12-08 06:58:09.421000: I runner.py:310] Step = 55500 ; steps/s = 1.63, tokens/s = 42352 (42352 target) ; Learning rate = 0.000375 ; Loss = 1.545787\n",
      "2024-12-08 06:59:11.101000: I runner.py:310] Step = 55600 ; steps/s = 1.62, tokens/s = 42885 (42885 target) ; Learning rate = 0.000375 ; Loss = 1.535685\n",
      "2024-12-08 07:00:12.734000: I runner.py:310] Step = 55700 ; steps/s = 1.62, tokens/s = 42903 (42903 target) ; Learning rate = 0.000375 ; Loss = 1.533768\n",
      "2024-12-08 07:01:14.479000: I runner.py:310] Step = 55800 ; steps/s = 1.62, tokens/s = 42824 (42824 target) ; Learning rate = 0.000374 ; Loss = 1.534523\n",
      "2024-12-08 07:02:15.836000: I runner.py:310] Step = 55900 ; steps/s = 1.63, tokens/s = 42289 (42289 target) ; Learning rate = 0.000374 ; Loss = 1.550134\n",
      "2024-12-08 07:03:17.535000: I runner.py:310] Step = 56000 ; steps/s = 1.62, tokens/s = 42906 (42906 target) ; Learning rate = 0.000374 ; Loss = 1.545042\n",
      "2024-12-08 07:04:19.232000: I runner.py:310] Step = 56100 ; steps/s = 1.62, tokens/s = 42853 (42853 target) ; Learning rate = 0.000373 ; Loss = 1.539023\n",
      "2024-12-08 07:05:21.048000: I runner.py:310] Step = 56200 ; steps/s = 1.62, tokens/s = 42773 (42773 target) ; Learning rate = 0.000373 ; Loss = 1.542913\n",
      "2024-12-08 07:06:22.387000: I runner.py:310] Step = 56300 ; steps/s = 1.63, tokens/s = 42299 (42299 target) ; Learning rate = 0.000373 ; Loss = 1.537179\n",
      "2024-12-08 07:07:24.162000: I runner.py:310] Step = 56400 ; steps/s = 1.62, tokens/s = 42808 (42808 target) ; Learning rate = 0.000372 ; Loss = 1.539685\n",
      "2024-12-08 07:08:25.925000: I runner.py:310] Step = 56500 ; steps/s = 1.62, tokens/s = 42847 (42847 target) ; Learning rate = 0.000372 ; Loss = 1.543410\n",
      "2024-12-08 07:09:27.589000: I runner.py:310] Step = 56600 ; steps/s = 1.62, tokens/s = 42884 (42884 target) ; Learning rate = 0.000372 ; Loss = 1.545398\n",
      "2024-12-08 07:10:28.969000: I runner.py:310] Step = 56700 ; steps/s = 1.63, tokens/s = 42282 (42282 target) ; Learning rate = 0.000371 ; Loss = 1.551820\n",
      "2024-12-08 07:11:30.688000: I runner.py:310] Step = 56800 ; steps/s = 1.62, tokens/s = 42808 (42808 target) ; Learning rate = 0.000371 ; Loss = 1.535560\n",
      "2024-12-08 07:12:32.483000: I runner.py:310] Step = 56900 ; steps/s = 1.62, tokens/s = 42817 (42817 target) ; Learning rate = 0.000371 ; Loss = 1.537326\n",
      "2024-12-08 07:13:34.201000: I runner.py:310] Step = 57000 ; steps/s = 1.62, tokens/s = 42854 (42854 target) ; Learning rate = 0.000370 ; Loss = 1.545258\n",
      "2024-12-08 07:14:35.492000: I runner.py:310] Step = 57100 ; steps/s = 1.63, tokens/s = 42357 (42357 target) ; Learning rate = 0.000370 ; Loss = 1.543958\n",
      "2024-12-08 07:15:37.196000: I runner.py:310] Step = 57200 ; steps/s = 1.62, tokens/s = 42874 (42874 target) ; Learning rate = 0.000370 ; Loss = 1.537421\n",
      "2024-12-08 07:16:38.958000: I runner.py:310] Step = 57300 ; steps/s = 1.62, tokens/s = 42809 (42809 target) ; Learning rate = 0.000369 ; Loss = 1.529862\n",
      "2024-12-08 07:17:40.727000: I runner.py:310] Step = 57400 ; steps/s = 1.62, tokens/s = 42803 (42803 target) ; Learning rate = 0.000369 ; Loss = 1.537421\n",
      "2024-12-08 07:18:42.009000: I runner.py:310] Step = 57500 ; steps/s = 1.63, tokens/s = 42371 (42371 target) ; Learning rate = 0.000369 ; Loss = 1.542848\n",
      "2024-12-08 07:19:43.671000: I runner.py:310] Step = 57600 ; steps/s = 1.62, tokens/s = 42883 (42883 target) ; Learning rate = 0.000368 ; Loss = 1.528852\n",
      "2024-12-08 07:20:45.470000: I runner.py:310] Step = 57700 ; steps/s = 1.62, tokens/s = 42785 (42785 target) ; Learning rate = 0.000368 ; Loss = 1.536788\n",
      "2024-12-08 07:21:47.283000: I runner.py:310] Step = 57800 ; steps/s = 1.62, tokens/s = 42761 (42761 target) ; Learning rate = 0.000368 ; Loss = 1.539451\n",
      "2024-12-08 07:22:48.634000: I runner.py:310] Step = 57900 ; steps/s = 1.63, tokens/s = 42349 (42349 target) ; Learning rate = 0.000367 ; Loss = 1.545920\n",
      "2024-12-08 07:23:50.387000: I runner.py:310] Step = 58000 ; steps/s = 1.62, tokens/s = 42824 (42824 target) ; Learning rate = 0.000367 ; Loss = 1.542714\n",
      "2024-12-08 07:24:52.132000: I runner.py:310] Step = 58100 ; steps/s = 1.62, tokens/s = 42813 (42813 target) ; Learning rate = 0.000367 ; Loss = 1.542047\n",
      "2024-12-08 07:25:53.872000: I runner.py:310] Step = 58200 ; steps/s = 1.62, tokens/s = 42845 (42845 target) ; Learning rate = 0.000366 ; Loss = 1.540139\n",
      "2024-12-08 07:26:55.249000: I runner.py:310] Step = 58300 ; steps/s = 1.63, tokens/s = 42305 (42305 target) ; Learning rate = 0.000366 ; Loss = 1.534875\n",
      "2024-12-08 07:27:57.015000: I runner.py:310] Step = 58400 ; steps/s = 1.62, tokens/s = 42794 (42794 target) ; Learning rate = 0.000366 ; Loss = 1.537533\n",
      "2024-12-08 07:28:58.769000: I runner.py:310] Step = 58500 ; steps/s = 1.62, tokens/s = 42822 (42822 target) ; Learning rate = 0.000365 ; Loss = 1.544394\n",
      "2024-12-08 07:30:00.573000: I runner.py:310] Step = 58600 ; steps/s = 1.62, tokens/s = 42800 (42800 target) ; Learning rate = 0.000365 ; Loss = 1.548841\n",
      "2024-12-08 07:31:01.895000: I runner.py:310] Step = 58700 ; steps/s = 1.63, tokens/s = 42326 (42326 target) ; Learning rate = 0.000365 ; Loss = 1.545537\n",
      "2024-12-08 07:32:03.638000: I runner.py:310] Step = 58800 ; steps/s = 1.62, tokens/s = 42847 (42847 target) ; Learning rate = 0.000365 ; Loss = 1.544662\n",
      "2024-12-08 07:33:05.429000: I runner.py:310] Step = 58900 ; steps/s = 1.62, tokens/s = 42808 (42808 target) ; Learning rate = 0.000364 ; Loss = 1.535755\n",
      "2024-12-08 07:34:07.199000: I runner.py:310] Step = 59000 ; steps/s = 1.62, tokens/s = 42802 (42802 target) ; Learning rate = 0.000364 ; Loss = 1.534700\n",
      "2024-12-08 07:35:08.465000: I runner.py:310] Step = 59100 ; steps/s = 1.63, tokens/s = 42371 (42371 target) ; Learning rate = 0.000364 ; Loss = 1.541714\n",
      "2024-12-08 07:36:10.246000: I runner.py:310] Step = 59200 ; steps/s = 1.62, tokens/s = 42799 (42799 target) ; Learning rate = 0.000363 ; Loss = 1.531597\n",
      "2024-12-08 07:37:11.969000: I runner.py:310] Step = 59300 ; steps/s = 1.62, tokens/s = 42838 (42838 target) ; Learning rate = 0.000363 ; Loss = 1.539090\n",
      "2024-12-08 07:38:13.729000: I runner.py:310] Step = 59400 ; steps/s = 1.62, tokens/s = 42828 (42828 target) ; Learning rate = 0.000363 ; Loss = 1.540118\n",
      "2024-12-08 07:39:15.031000: I runner.py:310] Step = 59500 ; steps/s = 1.63, tokens/s = 42339 (42339 target) ; Learning rate = 0.000362 ; Loss = 1.533320\n",
      "2024-12-08 07:40:16.738000: I runner.py:310] Step = 59600 ; steps/s = 1.62, tokens/s = 42851 (42851 target) ; Learning rate = 0.000362 ; Loss = 1.531734\n",
      "2024-12-08 07:41:18.476000: I runner.py:310] Step = 59700 ; steps/s = 1.62, tokens/s = 42850 (42850 target) ; Learning rate = 0.000362 ; Loss = 1.539124\n",
      "2024-12-08 07:42:20.224000: I runner.py:310] Step = 59800 ; steps/s = 1.62, tokens/s = 42837 (42837 target) ; Learning rate = 0.000361 ; Loss = 1.544804\n",
      "2024-12-08 07:43:21.526000: I runner.py:310] Step = 59900 ; steps/s = 1.63, tokens/s = 42332 (42332 target) ; Learning rate = 0.000361 ; Loss = 1.543032\n",
      "2024-12-08 07:44:23.289000: I runner.py:310] Step = 60000 ; steps/s = 1.62, tokens/s = 42807 (42807 target) ; Learning rate = 0.000361 ; Loss = 1.530513\n",
      "2024-12-08 07:44:25.341000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-60000\n",
      "2024-12-08 07:44:25.341000: I training.py:192] Running evaluation for step 60000\n",
      "2024-12-08 07:49:26.319000: I training.py:192] Evaluation result for step 60000: loss = 1.148856 ; perplexity = 3.154581\n",
      "2024-12-08 07:50:27.883000: I runner.py:310] Step = 60100 ; steps/s = 1.62, tokens/s = 42966 (42966 target) ; Learning rate = 0.000361 ; Loss = 1.535412\n",
      "2024-12-08 07:51:29.678000: I runner.py:310] Step = 60200 ; steps/s = 1.62, tokens/s = 42803 (42803 target) ; Learning rate = 0.000360 ; Loss = 1.543455\n",
      "2024-12-08 07:52:30.998000: I runner.py:310] Step = 60300 ; steps/s = 1.63, tokens/s = 42330 (42330 target) ; Learning rate = 0.000360 ; Loss = 1.528018\n",
      "2024-12-08 07:53:32.765000: I runner.py:310] Step = 60400 ; steps/s = 1.62, tokens/s = 42809 (42809 target) ; Learning rate = 0.000360 ; Loss = 1.540099\n",
      "2024-12-08 07:54:34.464000: I runner.py:310] Step = 60500 ; steps/s = 1.62, tokens/s = 42878 (42878 target) ; Learning rate = 0.000359 ; Loss = 1.540651\n",
      "2024-12-08 07:55:36.171000: I runner.py:310] Step = 60600 ; steps/s = 1.62, tokens/s = 42850 (42850 target) ; Learning rate = 0.000359 ; Loss = 1.535076\n",
      "2024-12-08 07:56:37.492000: I runner.py:310] Step = 60700 ; steps/s = 1.63, tokens/s = 42359 (42359 target) ; Learning rate = 0.000359 ; Loss = 1.538616\n",
      "2024-12-08 07:57:39.180000: I runner.py:310] Step = 60800 ; steps/s = 1.62, tokens/s = 42874 (42874 target) ; Learning rate = 0.000358 ; Loss = 1.535281\n",
      "2024-12-08 07:58:40.855000: I runner.py:310] Step = 60900 ; steps/s = 1.62, tokens/s = 42861 (42861 target) ; Learning rate = 0.000358 ; Loss = 1.539102\n",
      "2024-12-08 07:59:42.491000: I runner.py:310] Step = 61000 ; steps/s = 1.62, tokens/s = 42882 (42882 target) ; Learning rate = 0.000358 ; Loss = 1.528154\n",
      "2024-12-08 08:00:43.707000: I runner.py:310] Step = 61100 ; steps/s = 1.63, tokens/s = 42401 (42401 target) ; Learning rate = 0.000358 ; Loss = 1.538670\n",
      "2024-12-08 08:01:45.376000: I runner.py:310] Step = 61200 ; steps/s = 1.62, tokens/s = 42909 (42909 target) ; Learning rate = 0.000357 ; Loss = 1.534707\n",
      "2024-12-08 08:02:47.033000: I runner.py:310] Step = 61300 ; steps/s = 1.62, tokens/s = 42877 (42877 target) ; Learning rate = 0.000357 ; Loss = 1.533823\n",
      "2024-12-08 08:03:48.762000: I runner.py:310] Step = 61400 ; steps/s = 1.62, tokens/s = 42825 (42825 target) ; Learning rate = 0.000357 ; Loss = 1.532832\n",
      "2024-12-08 08:04:50.100000: I runner.py:310] Step = 61500 ; steps/s = 1.63, tokens/s = 42344 (42344 target) ; Learning rate = 0.000356 ; Loss = 1.533909\n",
      "2024-12-08 08:05:51.730000: I runner.py:310] Step = 61600 ; steps/s = 1.62, tokens/s = 42939 (42939 target) ; Learning rate = 0.000356 ; Loss = 1.535431\n",
      "2024-12-08 08:06:53.429000: I runner.py:310] Step = 61700 ; steps/s = 1.62, tokens/s = 42843 (42843 target) ; Learning rate = 0.000356 ; Loss = 1.532949\n",
      "2024-12-08 08:07:55.143000: I runner.py:310] Step = 61800 ; steps/s = 1.62, tokens/s = 42833 (42833 target) ; Learning rate = 0.000356 ; Loss = 1.545933\n",
      "2024-12-08 08:08:56.498000: I runner.py:310] Step = 61900 ; steps/s = 1.63, tokens/s = 42312 (42312 target) ; Learning rate = 0.000355 ; Loss = 1.522851\n",
      "2024-12-08 08:09:58.214000: I runner.py:310] Step = 62000 ; steps/s = 1.62, tokens/s = 42812 (42812 target) ; Learning rate = 0.000355 ; Loss = 1.534373\n",
      "2024-12-08 08:10:59.878000: I runner.py:310] Step = 62100 ; steps/s = 1.62, tokens/s = 42901 (42901 target) ; Learning rate = 0.000355 ; Loss = 1.541603\n",
      "2024-12-08 08:12:01.630000: I runner.py:310] Step = 62200 ; steps/s = 1.62, tokens/s = 42841 (42841 target) ; Learning rate = 0.000354 ; Loss = 1.543420\n",
      "2024-12-08 08:13:02.900000: I runner.py:310] Step = 62300 ; steps/s = 1.63, tokens/s = 42354 (42354 target) ; Learning rate = 0.000354 ; Loss = 1.526819\n",
      "2024-12-08 08:14:04.662000: I runner.py:310] Step = 62400 ; steps/s = 1.62, tokens/s = 42846 (42846 target) ; Learning rate = 0.000354 ; Loss = 1.540058\n",
      "2024-12-08 08:15:06.391000: I runner.py:310] Step = 62500 ; steps/s = 1.62, tokens/s = 42847 (42847 target) ; Learning rate = 0.000354 ; Loss = 1.542895\n",
      "2024-12-08 08:16:07.839000: I runner.py:310] Step = 62600 ; steps/s = 1.63, tokens/s = 42554 (42554 target) ; Learning rate = 0.000353 ; Loss = 1.533323\n",
      "2024-12-08 08:17:09.445000: I runner.py:310] Step = 62700 ; steps/s = 1.62, tokens/s = 42582 (42582 target) ; Learning rate = 0.000353 ; Loss = 1.534779\n",
      "2024-12-08 08:18:11.196000: I runner.py:310] Step = 62800 ; steps/s = 1.62, tokens/s = 42834 (42834 target) ; Learning rate = 0.000353 ; Loss = 1.526605\n",
      "2024-12-08 08:19:12.996000: I runner.py:310] Step = 62900 ; steps/s = 1.62, tokens/s = 42783 (42783 target) ; Learning rate = 0.000352 ; Loss = 1.531847\n",
      "2024-12-08 08:20:14.304000: I runner.py:310] Step = 63000 ; steps/s = 1.63, tokens/s = 42323 (42323 target) ; Learning rate = 0.000352 ; Loss = 1.531775\n",
      "2024-12-08 08:21:16.041000: I runner.py:310] Step = 63100 ; steps/s = 1.62, tokens/s = 42865 (42865 target) ; Learning rate = 0.000352 ; Loss = 1.535407\n",
      "2024-12-08 08:22:17.728000: I runner.py:310] Step = 63200 ; steps/s = 1.62, tokens/s = 42869 (42869 target) ; Learning rate = 0.000352 ; Loss = 1.532953\n",
      "2024-12-08 08:23:19.506000: I runner.py:310] Step = 63300 ; steps/s = 1.62, tokens/s = 42824 (42824 target) ; Learning rate = 0.000351 ; Loss = 1.531847\n",
      "2024-12-08 08:24:20.773000: I runner.py:310] Step = 63400 ; steps/s = 1.63, tokens/s = 42332 (42332 target) ; Learning rate = 0.000351 ; Loss = 1.525682\n",
      "2024-12-08 08:25:22.508000: I runner.py:310] Step = 63500 ; steps/s = 1.62, tokens/s = 42837 (42837 target) ; Learning rate = 0.000351 ; Loss = 1.524901\n",
      "2024-12-08 08:26:24.209000: I runner.py:310] Step = 63600 ; steps/s = 1.62, tokens/s = 42874 (42874 target) ; Learning rate = 0.000350 ; Loss = 1.540293\n",
      "2024-12-08 08:27:25.952000: I runner.py:310] Step = 63700 ; steps/s = 1.62, tokens/s = 42823 (42823 target) ; Learning rate = 0.000350 ; Loss = 1.538442\n",
      "2024-12-08 08:28:27.293000: I runner.py:310] Step = 63800 ; steps/s = 1.63, tokens/s = 42326 (42326 target) ; Learning rate = 0.000350 ; Loss = 1.535166\n",
      "2024-12-08 08:29:29.057000: I runner.py:310] Step = 63900 ; steps/s = 1.62, tokens/s = 42826 (42826 target) ; Learning rate = 0.000350 ; Loss = 1.530521\n",
      "2024-12-08 08:30:30.798000: I runner.py:310] Step = 64000 ; steps/s = 1.62, tokens/s = 42852 (42852 target) ; Learning rate = 0.000349 ; Loss = 1.534104\n",
      "2024-12-08 08:31:32.454000: I runner.py:310] Step = 64100 ; steps/s = 1.62, tokens/s = 42858 (42858 target) ; Learning rate = 0.000349 ; Loss = 1.532913\n",
      "2024-12-08 08:32:33.785000: I runner.py:310] Step = 64200 ; steps/s = 1.63, tokens/s = 42339 (42339 target) ; Learning rate = 0.000349 ; Loss = 1.537532\n",
      "2024-12-08 08:33:35.550000: I runner.py:310] Step = 64300 ; steps/s = 1.62, tokens/s = 42827 (42827 target) ; Learning rate = 0.000349 ; Loss = 1.527071\n",
      "2024-12-08 08:34:37.276000: I runner.py:310] Step = 64400 ; steps/s = 1.62, tokens/s = 42840 (42840 target) ; Learning rate = 0.000348 ; Loss = 1.523956\n",
      "2024-12-08 08:35:38.985000: I runner.py:310] Step = 64500 ; steps/s = 1.62, tokens/s = 42853 (42853 target) ; Learning rate = 0.000348 ; Loss = 1.532219\n",
      "2024-12-08 08:36:40.196000: I runner.py:310] Step = 64600 ; steps/s = 1.63, tokens/s = 42398 (42398 target) ; Learning rate = 0.000348 ; Loss = 1.528216\n",
      "2024-12-08 08:37:41.905000: I runner.py:310] Step = 64700 ; steps/s = 1.62, tokens/s = 42852 (42852 target) ; Learning rate = 0.000347 ; Loss = 1.527727\n",
      "2024-12-08 08:38:43.593000: I runner.py:310] Step = 64800 ; steps/s = 1.62, tokens/s = 42855 (42855 target) ; Learning rate = 0.000347 ; Loss = 1.531157\n",
      "2024-12-08 08:39:45.292000: I runner.py:310] Step = 64900 ; steps/s = 1.62, tokens/s = 42856 (42856 target) ; Learning rate = 0.000347 ; Loss = 1.536434\n",
      "2024-12-08 08:40:46.541000: I runner.py:310] Step = 65000 ; steps/s = 1.63, tokens/s = 42408 (42408 target) ; Learning rate = 0.000347 ; Loss = 1.527478\n",
      "2024-12-08 08:40:46.542000: I training.py:192] Running evaluation for step 65000\n",
      "2024-12-08 08:45:36.032000: I training.py:192] Evaluation result for step 65000: loss = 1.152362 ; perplexity = 3.165662\n",
      "2024-12-08 08:46:37.597000: I runner.py:310] Step = 65100 ; steps/s = 1.62, tokens/s = 42967 (42967 target) ; Learning rate = 0.000346 ; Loss = 1.529513\n",
      "2024-12-08 08:47:39.327000: I runner.py:310] Step = 65200 ; steps/s = 1.62, tokens/s = 42840 (42840 target) ; Learning rate = 0.000346 ; Loss = 1.531525\n",
      "2024-12-08 08:48:41.081000: I runner.py:310] Step = 65300 ; steps/s = 1.62, tokens/s = 42821 (42821 target) ; Learning rate = 0.000346 ; Loss = 1.540151\n",
      "2024-12-08 08:49:42.330000: I runner.py:310] Step = 65400 ; steps/s = 1.63, tokens/s = 42373 (42373 target) ; Learning rate = 0.000346 ; Loss = 1.528846\n",
      "2024-12-08 08:50:44.020000: I runner.py:310] Step = 65500 ; steps/s = 1.62, tokens/s = 42899 (42899 target) ; Learning rate = 0.000345 ; Loss = 1.526552\n",
      "2024-12-08 08:51:45.650000: I runner.py:310] Step = 65600 ; steps/s = 1.62, tokens/s = 42901 (42901 target) ; Learning rate = 0.000345 ; Loss = 1.528061\n",
      "2024-12-08 08:52:47.325000: I runner.py:310] Step = 65700 ; steps/s = 1.62, tokens/s = 42848 (42848 target) ; Learning rate = 0.000345 ; Loss = 1.537555\n",
      "2024-12-08 08:53:48.648000: I runner.py:310] Step = 65800 ; steps/s = 1.63, tokens/s = 42342 (42342 target) ; Learning rate = 0.000345 ; Loss = 1.535398\n",
      "2024-12-08 08:54:50.344000: I runner.py:310] Step = 65900 ; steps/s = 1.62, tokens/s = 42838 (42838 target) ; Learning rate = 0.000344 ; Loss = 1.533333\n",
      "2024-12-08 08:55:52.028000: I runner.py:310] Step = 66000 ; steps/s = 1.62, tokens/s = 42887 (42887 target) ; Learning rate = 0.000344 ; Loss = 1.532381\n",
      "2024-12-08 08:56:53.765000: I runner.py:310] Step = 66100 ; steps/s = 1.62, tokens/s = 42819 (42819 target) ; Learning rate = 0.000344 ; Loss = 1.526946\n",
      "2024-12-08 08:57:54.993000: I runner.py:310] Step = 66200 ; steps/s = 1.63, tokens/s = 42399 (42399 target) ; Learning rate = 0.000344 ; Loss = 1.531053\n",
      "2024-12-08 08:58:56.677000: I runner.py:310] Step = 66300 ; steps/s = 1.62, tokens/s = 42889 (42889 target) ; Learning rate = 0.000343 ; Loss = 1.525158\n",
      "2024-12-08 08:59:58.354000: I runner.py:310] Step = 66400 ; steps/s = 1.62, tokens/s = 42860 (42860 target) ; Learning rate = 0.000343 ; Loss = 1.527427\n",
      "2024-12-08 09:01:00.042000: I runner.py:310] Step = 66500 ; steps/s = 1.62, tokens/s = 42880 (42880 target) ; Learning rate = 0.000343 ; Loss = 1.529649\n",
      "2024-12-08 09:02:01.257000: I runner.py:310] Step = 66600 ; steps/s = 1.63, tokens/s = 42410 (42410 target) ; Learning rate = 0.000342 ; Loss = 1.526009\n",
      "2024-12-08 09:03:02.949000: I runner.py:310] Step = 66700 ; steps/s = 1.62, tokens/s = 42852 (42852 target) ; Learning rate = 0.000342 ; Loss = 1.527737\n",
      "2024-12-08 09:04:04.628000: I runner.py:310] Step = 66800 ; steps/s = 1.62, tokens/s = 42895 (42895 target) ; Learning rate = 0.000342 ; Loss = 1.537127\n",
      "2024-12-08 09:05:06.285000: I runner.py:310] Step = 66900 ; steps/s = 1.62, tokens/s = 42883 (42883 target) ; Learning rate = 0.000342 ; Loss = 1.536082\n",
      "2024-12-08 09:06:07.631000: I runner.py:310] Step = 67000 ; steps/s = 1.63, tokens/s = 42327 (42327 target) ; Learning rate = 0.000341 ; Loss = 1.522232\n",
      "2024-12-08 09:07:09.357000: I runner.py:310] Step = 67100 ; steps/s = 1.62, tokens/s = 42842 (42842 target) ; Learning rate = 0.000341 ; Loss = 1.527248\n",
      "2024-12-08 09:08:11.022000: I runner.py:310] Step = 67200 ; steps/s = 1.62, tokens/s = 42911 (42911 target) ; Learning rate = 0.000341 ; Loss = 1.532485\n",
      "2024-12-08 09:09:12.681000: I runner.py:310] Step = 67300 ; steps/s = 1.62, tokens/s = 42880 (42880 target) ; Learning rate = 0.000341 ; Loss = 1.532604\n",
      "2024-12-08 09:10:13.912000: I runner.py:310] Step = 67400 ; steps/s = 1.63, tokens/s = 42368 (42368 target) ; Learning rate = 0.000340 ; Loss = 1.530499\n",
      "2024-12-08 09:11:15.590000: I runner.py:310] Step = 67500 ; steps/s = 1.62, tokens/s = 42909 (42909 target) ; Learning rate = 0.000340 ; Loss = 1.523438\n",
      "2024-12-08 09:12:17.296000: I runner.py:310] Step = 67600 ; steps/s = 1.62, tokens/s = 42829 (42829 target) ; Learning rate = 0.000340 ; Loss = 1.521900\n",
      "2024-12-08 09:13:18.957000: I runner.py:310] Step = 67700 ; steps/s = 1.62, tokens/s = 42910 (42910 target) ; Learning rate = 0.000340 ; Loss = 1.523604\n",
      "2024-12-08 09:14:20.276000: I runner.py:310] Step = 67800 ; steps/s = 1.63, tokens/s = 42327 (42327 target) ; Learning rate = 0.000339 ; Loss = 1.528545\n",
      "2024-12-08 09:15:21.962000: I runner.py:310] Step = 67900 ; steps/s = 1.62, tokens/s = 42852 (42852 target) ; Learning rate = 0.000339 ; Loss = 1.524411\n",
      "2024-12-08 09:16:23.622000: I runner.py:310] Step = 68000 ; steps/s = 1.62, tokens/s = 42890 (42890 target) ; Learning rate = 0.000339 ; Loss = 1.530674\n",
      "2024-12-08 09:17:25.377000: I runner.py:310] Step = 68100 ; steps/s = 1.62, tokens/s = 42840 (42840 target) ; Learning rate = 0.000339 ; Loss = 1.529630\n",
      "2024-12-08 09:18:26.563000: I runner.py:310] Step = 68200 ; steps/s = 1.63, tokens/s = 42411 (42411 target) ; Learning rate = 0.000338 ; Loss = 1.524690\n",
      "2024-12-08 09:19:28.303000: I runner.py:310] Step = 68300 ; steps/s = 1.62, tokens/s = 42854 (42854 target) ; Learning rate = 0.000338 ; Loss = 1.530732\n",
      "2024-12-08 09:20:30.024000: I runner.py:310] Step = 68400 ; steps/s = 1.62, tokens/s = 42836 (42836 target) ; Learning rate = 0.000338 ; Loss = 1.523064\n",
      "2024-12-08 09:21:31.827000: I runner.py:310] Step = 68500 ; steps/s = 1.62, tokens/s = 42801 (42801 target) ; Learning rate = 0.000338 ; Loss = 1.523901\n",
      "2024-12-08 09:22:33.044000: I runner.py:310] Step = 68600 ; steps/s = 1.63, tokens/s = 42391 (42391 target) ; Learning rate = 0.000337 ; Loss = 1.523218\n",
      "2024-12-08 09:23:34.702000: I runner.py:310] Step = 68700 ; steps/s = 1.62, tokens/s = 42884 (42884 target) ; Learning rate = 0.000337 ; Loss = 1.523277\n",
      "2024-12-08 09:24:36.415000: I runner.py:310] Step = 68800 ; steps/s = 1.62, tokens/s = 42845 (42845 target) ; Learning rate = 0.000337 ; Loss = 1.538659\n",
      "2024-12-08 09:25:38.117000: I runner.py:310] Step = 68900 ; steps/s = 1.62, tokens/s = 42875 (42875 target) ; Learning rate = 0.000337 ; Loss = 1.540817\n",
      "2024-12-08 09:26:39.380000: I runner.py:310] Step = 69000 ; steps/s = 1.63, tokens/s = 42376 (42376 target) ; Learning rate = 0.000336 ; Loss = 1.529495\n",
      "2024-12-08 09:27:41.053000: I runner.py:310] Step = 69100 ; steps/s = 1.62, tokens/s = 42862 (42862 target) ; Learning rate = 0.000336 ; Loss = 1.521642\n",
      "2024-12-08 09:28:42.744000: I runner.py:310] Step = 69200 ; steps/s = 1.62, tokens/s = 42875 (42875 target) ; Learning rate = 0.000336 ; Loss = 1.524234\n",
      "2024-12-08 09:29:44.457000: I runner.py:310] Step = 69300 ; steps/s = 1.62, tokens/s = 42873 (42873 target) ; Learning rate = 0.000336 ; Loss = 1.528759\n",
      "2024-12-08 09:30:45.730000: I runner.py:310] Step = 69400 ; steps/s = 1.63, tokens/s = 42361 (42361 target) ; Learning rate = 0.000336 ; Loss = 1.537679\n",
      "2024-12-08 09:31:47.441000: I runner.py:310] Step = 69500 ; steps/s = 1.62, tokens/s = 42852 (42852 target) ; Learning rate = 0.000335 ; Loss = 1.521236\n",
      "2024-12-08 09:32:49.154000: I runner.py:310] Step = 69600 ; steps/s = 1.62, tokens/s = 42866 (42866 target) ; Learning rate = 0.000335 ; Loss = 1.520909\n",
      "2024-12-08 09:33:50.898000: I runner.py:310] Step = 69700 ; steps/s = 1.62, tokens/s = 42806 (42806 target) ; Learning rate = 0.000335 ; Loss = 1.521574\n",
      "2024-12-08 09:34:52.131000: I runner.py:310] Step = 69800 ; steps/s = 1.63, tokens/s = 42418 (42418 target) ; Learning rate = 0.000335 ; Loss = 1.536414\n",
      "2024-12-08 09:35:53.859000: I runner.py:310] Step = 69900 ; steps/s = 1.62, tokens/s = 42857 (42857 target) ; Learning rate = 0.000334 ; Loss = 1.527884\n",
      "2024-12-08 09:36:55.597000: I runner.py:310] Step = 70000 ; steps/s = 1.62, tokens/s = 42825 (42825 target) ; Learning rate = 0.000334 ; Loss = 1.522289\n",
      "2024-12-08 09:36:58.303000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-70000\n",
      "2024-12-08 09:36:58.303000: I training.py:192] Running evaluation for step 70000\n",
      "2024-12-08 09:41:52.549000: I training.py:192] Evaluation result for step 70000: loss = 1.160457 ; perplexity = 3.191391\n",
      "2024-12-08 09:42:54.143000: I runner.py:310] Step = 70100 ; steps/s = 1.62, tokens/s = 42934 (42934 target) ; Learning rate = 0.000334 ; Loss = 1.526415\n",
      "2024-12-08 09:43:55.494000: I runner.py:310] Step = 70200 ; steps/s = 1.63, tokens/s = 42274 (42274 target) ; Learning rate = 0.000334 ; Loss = 1.521121\n",
      "2024-12-08 09:44:57.235000: I runner.py:310] Step = 70300 ; steps/s = 1.62, tokens/s = 42847 (42847 target) ; Learning rate = 0.000333 ; Loss = 1.532964\n",
      "2024-12-08 09:45:58.990000: I runner.py:310] Step = 70400 ; steps/s = 1.62, tokens/s = 42823 (42823 target) ; Learning rate = 0.000333 ; Loss = 1.533668\n",
      "2024-12-08 09:47:00.743000: I runner.py:310] Step = 70500 ; steps/s = 1.62, tokens/s = 42842 (42842 target) ; Learning rate = 0.000333 ; Loss = 1.528603\n",
      "2024-12-08 09:48:01.979000: I runner.py:310] Step = 70600 ; steps/s = 1.63, tokens/s = 42368 (42368 target) ; Learning rate = 0.000333 ; Loss = 1.529604\n",
      "2024-12-08 09:49:03.720000: I runner.py:310] Step = 70700 ; steps/s = 1.62, tokens/s = 42825 (42825 target) ; Learning rate = 0.000332 ; Loss = 1.516867\n",
      "2024-12-08 09:50:05.368000: I runner.py:310] Step = 70800 ; steps/s = 1.62, tokens/s = 42921 (42921 target) ; Learning rate = 0.000332 ; Loss = 1.523998\n",
      "2024-12-08 09:51:07.058000: I runner.py:310] Step = 70900 ; steps/s = 1.62, tokens/s = 42873 (42873 target) ; Learning rate = 0.000332 ; Loss = 1.529562\n",
      "2024-12-08 09:52:08.340000: I runner.py:310] Step = 71000 ; steps/s = 1.63, tokens/s = 42357 (42357 target) ; Learning rate = 0.000332 ; Loss = 1.516114\n",
      "2024-12-08 09:53:10.031000: I runner.py:310] Step = 71100 ; steps/s = 1.62, tokens/s = 42854 (42854 target) ; Learning rate = 0.000331 ; Loss = 1.526357\n",
      "2024-12-08 09:54:11.754000: I runner.py:310] Step = 71200 ; steps/s = 1.62, tokens/s = 42845 (42845 target) ; Learning rate = 0.000331 ; Loss = 1.524886\n",
      "2024-12-08 09:55:13.498000: I runner.py:310] Step = 71300 ; steps/s = 1.62, tokens/s = 42820 (42820 target) ; Learning rate = 0.000331 ; Loss = 1.532119\n",
      "2024-12-08 09:56:14.810000: I runner.py:310] Step = 71400 ; steps/s = 1.63, tokens/s = 42357 (42357 target) ; Learning rate = 0.000331 ; Loss = 1.524691\n",
      "2024-12-08 09:57:16.494000: I runner.py:310] Step = 71500 ; steps/s = 1.62, tokens/s = 42860 (42860 target) ; Learning rate = 0.000331 ; Loss = 1.515694\n",
      "2024-12-08 09:58:18.216000: I runner.py:310] Step = 71600 ; steps/s = 1.62, tokens/s = 42846 (42846 target) ; Learning rate = 0.000330 ; Loss = 1.523546\n",
      "2024-12-08 09:59:19.990000: I runner.py:310] Step = 71700 ; steps/s = 1.62, tokens/s = 42811 (42811 target) ; Learning rate = 0.000330 ; Loss = 1.519373\n",
      "2024-12-08 10:00:21.325000: I runner.py:310] Step = 71800 ; steps/s = 1.63, tokens/s = 42315 (42315 target) ; Learning rate = 0.000330 ; Loss = 1.516303\n",
      "2024-12-08 10:01:23.016000: I runner.py:310] Step = 71900 ; steps/s = 1.62, tokens/s = 42878 (42878 target) ; Learning rate = 0.000330 ; Loss = 1.532388\n",
      "2024-12-08 10:02:24.799000: I runner.py:310] Step = 72000 ; steps/s = 1.62, tokens/s = 42815 (42815 target) ; Learning rate = 0.000329 ; Loss = 1.523346\n",
      "2024-12-08 10:03:26.549000: I runner.py:310] Step = 72100 ; steps/s = 1.62, tokens/s = 42822 (42822 target) ; Learning rate = 0.000329 ; Loss = 1.515798\n",
      "2024-12-08 10:04:27.856000: I runner.py:310] Step = 72200 ; steps/s = 1.63, tokens/s = 42325 (42325 target) ; Learning rate = 0.000329 ; Loss = 1.531467\n",
      "2024-12-08 10:05:29.527000: I runner.py:310] Step = 72300 ; steps/s = 1.62, tokens/s = 42887 (42887 target) ; Learning rate = 0.000329 ; Loss = 1.523702\n",
      "2024-12-08 10:06:31.212000: I runner.py:310] Step = 72400 ; steps/s = 1.62, tokens/s = 42877 (42877 target) ; Learning rate = 0.000328 ; Loss = 1.521082\n",
      "2024-12-08 10:07:32.890000: I runner.py:310] Step = 72500 ; steps/s = 1.62, tokens/s = 42876 (42876 target) ; Learning rate = 0.000328 ; Loss = 1.533409\n",
      "2024-12-08 10:08:34.121000: I runner.py:310] Step = 72600 ; steps/s = 1.63, tokens/s = 42395 (42395 target) ; Learning rate = 0.000328 ; Loss = 1.518001\n",
      "2024-12-08 10:09:35.832000: I runner.py:310] Step = 72700 ; steps/s = 1.62, tokens/s = 42890 (42890 target) ; Learning rate = 0.000328 ; Loss = 1.516541\n",
      "2024-12-08 10:10:37.501000: I runner.py:310] Step = 72800 ; steps/s = 1.62, tokens/s = 42848 (42848 target) ; Learning rate = 0.000328 ; Loss = 1.526863\n",
      "2024-12-08 10:11:39.128000: I runner.py:310] Step = 72900 ; steps/s = 1.62, tokens/s = 42893 (42893 target) ; Learning rate = 0.000327 ; Loss = 1.529835\n",
      "2024-12-08 10:12:40.520000: I runner.py:310] Step = 73000 ; steps/s = 1.63, tokens/s = 42297 (42297 target) ; Learning rate = 0.000327 ; Loss = 1.521057\n",
      "2024-12-08 10:13:42.191000: I runner.py:310] Step = 73100 ; steps/s = 1.62, tokens/s = 42863 (42863 target) ; Learning rate = 0.000327 ; Loss = 1.524908\n",
      "2024-12-08 10:14:44.024000: I runner.py:310] Step = 73200 ; steps/s = 1.62, tokens/s = 42795 (42795 target) ; Learning rate = 0.000327 ; Loss = 1.522185\n",
      "2024-12-08 10:15:45.331000: I runner.py:310] Step = 73300 ; steps/s = 1.63, tokens/s = 42304 (42304 target) ; Learning rate = 0.000326 ; Loss = 1.535910\n",
      "2024-12-08 10:16:47.047000: I runner.py:310] Step = 73400 ; steps/s = 1.62, tokens/s = 42878 (42878 target) ; Learning rate = 0.000326 ; Loss = 1.522212\n",
      "2024-12-08 10:17:48.799000: I runner.py:310] Step = 73500 ; steps/s = 1.62, tokens/s = 42840 (42840 target) ; Learning rate = 0.000326 ; Loss = 1.519562\n",
      "2024-12-08 10:18:50.524000: I runner.py:310] Step = 73600 ; steps/s = 1.62, tokens/s = 42844 (42844 target) ; Learning rate = 0.000326 ; Loss = 1.521505\n",
      "2024-12-08 10:19:51.817000: I runner.py:310] Step = 73700 ; steps/s = 1.63, tokens/s = 42310 (42310 target) ; Learning rate = 0.000326 ; Loss = 1.518194\n",
      "2024-12-08 10:20:53.477000: I runner.py:310] Step = 73800 ; steps/s = 1.62, tokens/s = 42926 (42926 target) ; Learning rate = 0.000325 ; Loss = 1.513819\n",
      "2024-12-08 10:21:55.207000: I runner.py:310] Step = 73900 ; steps/s = 1.62, tokens/s = 42841 (42841 target) ; Learning rate = 0.000325 ; Loss = 1.523299\n",
      "2024-12-08 10:22:56.889000: I runner.py:310] Step = 74000 ; steps/s = 1.62, tokens/s = 42874 (42874 target) ; Learning rate = 0.000325 ; Loss = 1.524062\n",
      "2024-12-08 10:23:58.094000: I runner.py:310] Step = 74100 ; steps/s = 1.63, tokens/s = 42383 (42383 target) ; Learning rate = 0.000325 ; Loss = 1.525666\n",
      "2024-12-08 10:24:59.772000: I runner.py:310] Step = 74200 ; steps/s = 1.62, tokens/s = 42891 (42891 target) ; Learning rate = 0.000324 ; Loss = 1.527616\n",
      "2024-12-08 10:26:01.500000: I runner.py:310] Step = 74300 ; steps/s = 1.62, tokens/s = 42832 (42832 target) ; Learning rate = 0.000324 ; Loss = 1.524822\n",
      "2024-12-08 10:27:03.227000: I runner.py:310] Step = 74400 ; steps/s = 1.62, tokens/s = 42856 (42856 target) ; Learning rate = 0.000324 ; Loss = 1.533854\n",
      "2024-12-08 10:28:04.544000: I runner.py:310] Step = 74500 ; steps/s = 1.63, tokens/s = 42328 (42328 target) ; Learning rate = 0.000324 ; Loss = 1.524062\n",
      "2024-12-08 10:29:06.243000: I runner.py:310] Step = 74600 ; steps/s = 1.62, tokens/s = 42876 (42876 target) ; Learning rate = 0.000324 ; Loss = 1.510048\n",
      "2024-12-08 10:30:08.052000: I runner.py:310] Step = 74700 ; steps/s = 1.62, tokens/s = 42790 (42790 target) ; Learning rate = 0.000323 ; Loss = 1.523083\n",
      "2024-12-08 10:31:09.765000: I runner.py:310] Step = 74800 ; steps/s = 1.62, tokens/s = 42857 (42857 target) ; Learning rate = 0.000323 ; Loss = 1.520652\n",
      "2024-12-08 10:32:11.046000: I runner.py:310] Step = 74900 ; steps/s = 1.63, tokens/s = 42333 (42333 target) ; Learning rate = 0.000323 ; Loss = 1.524755\n",
      "2024-12-08 10:33:12.755000: I runner.py:310] Step = 75000 ; steps/s = 1.62, tokens/s = 42876 (42876 target) ; Learning rate = 0.000323 ; Loss = 1.518397\n",
      "2024-12-08 10:33:12.756000: I training.py:192] Running evaluation for step 75000\n",
      "2024-12-08 10:38:09.321000: I training.py:192] Evaluation result for step 75000: loss = 1.166245 ; perplexity = 3.209918\n",
      "2024-12-08 10:39:10.885000: I runner.py:310] Step = 75100 ; steps/s = 1.62, tokens/s = 42970 (42970 target) ; Learning rate = 0.000323 ; Loss = 1.532876\n",
      "2024-12-08 10:40:12.572000: I runner.py:310] Step = 75200 ; steps/s = 1.62, tokens/s = 42873 (42873 target) ; Learning rate = 0.000322 ; Loss = 1.529280\n",
      "2024-12-08 10:41:13.922000: I runner.py:310] Step = 75300 ; steps/s = 1.63, tokens/s = 42281 (42281 target) ; Learning rate = 0.000322 ; Loss = 1.526266\n",
      "2024-12-08 10:42:15.751000: I runner.py:310] Step = 75400 ; steps/s = 1.62, tokens/s = 42754 (42754 target) ; Learning rate = 0.000322 ; Loss = 1.517454\n",
      "2024-12-08 10:43:17.489000: I runner.py:310] Step = 75500 ; steps/s = 1.62, tokens/s = 42842 (42842 target) ; Learning rate = 0.000322 ; Loss = 1.519822\n",
      "2024-12-08 10:44:19.294000: I runner.py:310] Step = 75600 ; steps/s = 1.62, tokens/s = 42807 (42807 target) ; Learning rate = 0.000321 ; Loss = 1.523063\n",
      "2024-12-08 10:45:20.544000: I runner.py:310] Step = 75700 ; steps/s = 1.63, tokens/s = 42381 (42381 target) ; Learning rate = 0.000321 ; Loss = 1.515012\n",
      "2024-12-08 10:46:22.220000: I runner.py:310] Step = 75800 ; steps/s = 1.62, tokens/s = 42883 (42883 target) ; Learning rate = 0.000321 ; Loss = 1.515617\n",
      "2024-12-08 10:47:23.928000: I runner.py:310] Step = 75900 ; steps/s = 1.62, tokens/s = 42877 (42877 target) ; Learning rate = 0.000321 ; Loss = 1.515758\n",
      "2024-12-08 10:48:25.679000: I runner.py:310] Step = 76000 ; steps/s = 1.62, tokens/s = 42811 (42811 target) ; Learning rate = 0.000321 ; Loss = 1.526022\n",
      "2024-12-08 10:49:27.002000: I runner.py:310] Step = 76100 ; steps/s = 1.63, tokens/s = 42316 (42316 target) ; Learning rate = 0.000320 ; Loss = 1.526792\n",
      "2024-12-08 10:50:28.685000: I runner.py:310] Step = 76200 ; steps/s = 1.62, tokens/s = 42868 (42868 target) ; Learning rate = 0.000320 ; Loss = 1.519157\n",
      "2024-12-08 10:51:30.429000: I runner.py:310] Step = 76300 ; steps/s = 1.62, tokens/s = 42848 (42848 target) ; Learning rate = 0.000320 ; Loss = 1.518567\n",
      "2024-12-08 10:52:32.141000: I runner.py:310] Step = 76400 ; steps/s = 1.62, tokens/s = 42830 (42830 target) ; Learning rate = 0.000320 ; Loss = 1.520131\n",
      "2024-12-08 10:53:33.427000: I runner.py:310] Step = 76500 ; steps/s = 1.63, tokens/s = 42360 (42360 target) ; Learning rate = 0.000320 ; Loss = 1.520971\n",
      "2024-12-08 10:54:35.084000: I runner.py:310] Step = 76600 ; steps/s = 1.62, tokens/s = 42868 (42868 target) ; Learning rate = 0.000319 ; Loss = 1.523609\n",
      "2024-12-08 10:55:36.746000: I runner.py:310] Step = 76700 ; steps/s = 1.62, tokens/s = 42916 (42916 target) ; Learning rate = 0.000319 ; Loss = 1.522309\n",
      "2024-12-08 10:56:38.429000: I runner.py:310] Step = 76800 ; steps/s = 1.62, tokens/s = 42863 (42863 target) ; Learning rate = 0.000319 ; Loss = 1.525895\n",
      "2024-12-08 10:57:39.771000: I runner.py:310] Step = 76900 ; steps/s = 1.63, tokens/s = 42342 (42342 target) ; Learning rate = 0.000319 ; Loss = 1.517917\n",
      "2024-12-08 10:58:41.406000: I runner.py:310] Step = 77000 ; steps/s = 1.62, tokens/s = 42921 (42921 target) ; Learning rate = 0.000319 ; Loss = 1.518407\n",
      "2024-12-08 10:59:43.106000: I runner.py:310] Step = 77100 ; steps/s = 1.62, tokens/s = 42843 (42843 target) ; Learning rate = 0.000318 ; Loss = 1.519626\n",
      "2024-12-08 11:00:44.854000: I runner.py:310] Step = 77200 ; steps/s = 1.62, tokens/s = 42809 (42809 target) ; Learning rate = 0.000318 ; Loss = 1.526976\n",
      "2024-12-08 11:01:46.133000: I runner.py:310] Step = 77300 ; steps/s = 1.63, tokens/s = 42359 (42359 target) ; Learning rate = 0.000318 ; Loss = 1.521354\n",
      "2024-12-08 11:02:47.917000: I runner.py:310] Step = 77400 ; steps/s = 1.62, tokens/s = 42833 (42833 target) ; Learning rate = 0.000318 ; Loss = 1.521930\n",
      "2024-12-08 11:03:49.646000: I runner.py:310] Step = 77500 ; steps/s = 1.62, tokens/s = 42824 (42824 target) ; Learning rate = 0.000317 ; Loss = 1.515347\n",
      "2024-12-08 11:04:51.428000: I runner.py:310] Step = 77600 ; steps/s = 1.62, tokens/s = 42828 (42828 target) ; Learning rate = 0.000317 ; Loss = 1.517335\n",
      "2024-12-08 11:05:52.606000: I runner.py:310] Step = 77700 ; steps/s = 1.63, tokens/s = 42400 (42400 target) ; Learning rate = 0.000317 ; Loss = 1.522278\n",
      "2024-12-08 11:06:54.366000: I runner.py:310] Step = 77800 ; steps/s = 1.62, tokens/s = 42805 (42805 target) ; Learning rate = 0.000317 ; Loss = 1.518292\n",
      "2024-12-08 11:07:56.050000: I runner.py:310] Step = 77900 ; steps/s = 1.62, tokens/s = 42911 (42911 target) ; Learning rate = 0.000317 ; Loss = 1.520494\n",
      "2024-12-08 11:08:57.770000: I runner.py:310] Step = 78000 ; steps/s = 1.62, tokens/s = 42815 (42815 target) ; Learning rate = 0.000316 ; Loss = 1.512601\n",
      "2024-12-08 11:09:59.023000: I runner.py:310] Step = 78100 ; steps/s = 1.63, tokens/s = 42370 (42370 target) ; Learning rate = 0.000316 ; Loss = 1.513701\n",
      "2024-12-08 11:11:00.744000: I runner.py:310] Step = 78200 ; steps/s = 1.62, tokens/s = 42871 (42871 target) ; Learning rate = 0.000316 ; Loss = 1.507140\n",
      "2024-12-08 11:12:02.424000: I runner.py:310] Step = 78300 ; steps/s = 1.62, tokens/s = 42882 (42882 target) ; Learning rate = 0.000316 ; Loss = 1.518360\n",
      "2024-12-08 11:13:04.115000: I runner.py:310] Step = 78400 ; steps/s = 1.62, tokens/s = 42859 (42859 target) ; Learning rate = 0.000316 ; Loss = 1.522834\n",
      "2024-12-08 11:14:05.292000: I runner.py:310] Step = 78500 ; steps/s = 1.63, tokens/s = 42417 (42417 target) ; Learning rate = 0.000315 ; Loss = 1.528927\n",
      "2024-12-08 11:15:07.062000: I runner.py:310] Step = 78600 ; steps/s = 1.62, tokens/s = 42834 (42834 target) ; Learning rate = 0.000315 ; Loss = 1.522202\n",
      "2024-12-08 11:16:08.769000: I runner.py:310] Step = 78700 ; steps/s = 1.62, tokens/s = 42840 (42840 target) ; Learning rate = 0.000315 ; Loss = 1.519578\n",
      "2024-12-08 11:17:10.565000: I runner.py:310] Step = 78800 ; steps/s = 1.62, tokens/s = 42803 (42803 target) ; Learning rate = 0.000315 ; Loss = 1.514885\n",
      "2024-12-08 11:18:11.802000: I runner.py:310] Step = 78900 ; steps/s = 1.63, tokens/s = 42364 (42364 target) ; Learning rate = 0.000315 ; Loss = 1.509938\n",
      "2024-12-08 11:19:13.464000: I runner.py:310] Step = 79000 ; steps/s = 1.62, tokens/s = 42910 (42910 target) ; Learning rate = 0.000314 ; Loss = 1.521485\n",
      "2024-12-08 11:20:15.182000: I runner.py:310] Step = 79100 ; steps/s = 1.62, tokens/s = 42847 (42847 target) ; Learning rate = 0.000314 ; Loss = 1.520902\n",
      "2024-12-08 11:21:16.865000: I runner.py:310] Step = 79200 ; steps/s = 1.62, tokens/s = 42857 (42857 target) ; Learning rate = 0.000314 ; Loss = 1.517288\n",
      "2024-12-08 11:22:18.101000: I runner.py:310] Step = 79300 ; steps/s = 1.63, tokens/s = 42392 (42392 target) ; Learning rate = 0.000314 ; Loss = 1.523516\n",
      "2024-12-08 11:23:19.825000: I runner.py:310] Step = 79400 ; steps/s = 1.62, tokens/s = 42879 (42879 target) ; Learning rate = 0.000314 ; Loss = 1.517397\n",
      "2024-12-08 11:24:21.499000: I runner.py:310] Step = 79500 ; steps/s = 1.62, tokens/s = 42865 (42865 target) ; Learning rate = 0.000313 ; Loss = 1.513789\n",
      "2024-12-08 11:25:23.215000: I runner.py:310] Step = 79600 ; steps/s = 1.62, tokens/s = 42824 (42824 target) ; Learning rate = 0.000313 ; Loss = 1.524226\n",
      "2024-12-08 11:26:24.459000: I runner.py:310] Step = 79700 ; steps/s = 1.63, tokens/s = 42412 (42412 target) ; Learning rate = 0.000313 ; Loss = 1.517427\n",
      "2024-12-08 11:27:26.108000: I runner.py:310] Step = 79800 ; steps/s = 1.62, tokens/s = 42866 (42866 target) ; Learning rate = 0.000313 ; Loss = 1.519129\n",
      "2024-12-08 11:28:27.840000: I runner.py:310] Step = 79900 ; steps/s = 1.62, tokens/s = 42858 (42858 target) ; Learning rate = 0.000313 ; Loss = 1.512963\n",
      "2024-12-08 11:29:29.601000: I runner.py:310] Step = 80000 ; steps/s = 1.62, tokens/s = 42823 (42823 target) ; Learning rate = 0.000312 ; Loss = 1.520272\n",
      "2024-12-08 11:29:31.709000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-80000\n",
      "2024-12-08 11:29:31.709000: I training.py:192] Running evaluation for step 80000\n",
      "2024-12-08 11:34:18.776000: I training.py:192] Evaluation result for step 80000: loss = 1.168198 ; perplexity = 3.216193\n",
      "2024-12-08 11:35:19.894000: I runner.py:310] Step = 80100 ; steps/s = 1.64, tokens/s = 42469 (42469 target) ; Learning rate = 0.000312 ; Loss = 1.508520\n",
      "2024-12-08 11:36:21.583000: I runner.py:310] Step = 80200 ; steps/s = 1.62, tokens/s = 42857 (42857 target) ; Learning rate = 0.000312 ; Loss = 1.511489\n",
      "2024-12-08 11:37:23.245000: I runner.py:310] Step = 80300 ; steps/s = 1.62, tokens/s = 42915 (42915 target) ; Learning rate = 0.000312 ; Loss = 1.517590\n",
      "2024-12-08 11:38:24.958000: I runner.py:310] Step = 80400 ; steps/s = 1.62, tokens/s = 42831 (42831 target) ; Learning rate = 0.000312 ; Loss = 1.523611\n",
      "2024-12-08 11:39:26.246000: I runner.py:310] Step = 80500 ; steps/s = 1.63, tokens/s = 42353 (42353 target) ; Learning rate = 0.000312 ; Loss = 1.520970\n",
      "2024-12-08 11:40:27.974000: I runner.py:310] Step = 80600 ; steps/s = 1.62, tokens/s = 42852 (42852 target) ; Learning rate = 0.000311 ; Loss = 1.508292\n",
      "2024-12-08 11:41:29.704000: I runner.py:310] Step = 80700 ; steps/s = 1.62, tokens/s = 42807 (42807 target) ; Learning rate = 0.000311 ; Loss = 1.517652\n",
      "2024-12-08 11:42:31.388000: I runner.py:310] Step = 80800 ; steps/s = 1.62, tokens/s = 42899 (42899 target) ; Learning rate = 0.000311 ; Loss = 1.511133\n",
      "2024-12-08 11:43:32.642000: I runner.py:310] Step = 80900 ; steps/s = 1.63, tokens/s = 42398 (42398 target) ; Learning rate = 0.000311 ; Loss = 1.506527\n",
      "2024-12-08 11:44:34.328000: I runner.py:310] Step = 81000 ; steps/s = 1.62, tokens/s = 42870 (42870 target) ; Learning rate = 0.000311 ; Loss = 1.520761\n",
      "2024-12-08 11:45:36.026000: I runner.py:310] Step = 81100 ; steps/s = 1.62, tokens/s = 42864 (42864 target) ; Learning rate = 0.000310 ; Loss = 1.529359\n",
      "2024-12-08 11:46:37.772000: I runner.py:310] Step = 81200 ; steps/s = 1.62, tokens/s = 42826 (42826 target) ; Learning rate = 0.000310 ; Loss = 1.528276\n",
      "2024-12-08 11:47:39.098000: I runner.py:310] Step = 81300 ; steps/s = 1.63, tokens/s = 42323 (42323 target) ; Learning rate = 0.000310 ; Loss = 1.504854\n",
      "2024-12-08 11:48:40.802000: I runner.py:310] Step = 81400 ; steps/s = 1.62, tokens/s = 42848 (42848 target) ; Learning rate = 0.000310 ; Loss = 1.511224\n",
      "2024-12-08 11:49:42.492000: I runner.py:310] Step = 81500 ; steps/s = 1.62, tokens/s = 42879 (42879 target) ; Learning rate = 0.000310 ; Loss = 1.513040\n",
      "2024-12-08 11:50:44.206000: I runner.py:310] Step = 81600 ; steps/s = 1.62, tokens/s = 42841 (42841 target) ; Learning rate = 0.000309 ; Loss = 1.518848\n",
      "2024-12-08 11:51:45.446000: I runner.py:310] Step = 81700 ; steps/s = 1.63, tokens/s = 42405 (42405 target) ; Learning rate = 0.000309 ; Loss = 1.509897\n",
      "2024-12-08 11:52:47.130000: I runner.py:310] Step = 81800 ; steps/s = 1.62, tokens/s = 42863 (42863 target) ; Learning rate = 0.000309 ; Loss = 1.518694\n",
      "2024-12-08 11:53:48.848000: I runner.py:310] Step = 81900 ; steps/s = 1.62, tokens/s = 42838 (42838 target) ; Learning rate = 0.000309 ; Loss = 1.510601\n",
      "2024-12-08 11:54:50.552000: I runner.py:310] Step = 82000 ; steps/s = 1.62, tokens/s = 42858 (42858 target) ; Learning rate = 0.000309 ; Loss = 1.517530\n",
      "2024-12-08 11:55:51.807000: I runner.py:310] Step = 82100 ; steps/s = 1.63, tokens/s = 42372 (42372 target) ; Learning rate = 0.000308 ; Loss = 1.513080\n",
      "2024-12-08 11:56:53.484000: I runner.py:310] Step = 82200 ; steps/s = 1.62, tokens/s = 42896 (42896 target) ; Learning rate = 0.000308 ; Loss = 1.514800\n",
      "2024-12-08 11:57:55.186000: I runner.py:310] Step = 82300 ; steps/s = 1.62, tokens/s = 42885 (42885 target) ; Learning rate = 0.000308 ; Loss = 1.517674\n",
      "2024-12-08 11:58:56.859000: I runner.py:310] Step = 82400 ; steps/s = 1.62, tokens/s = 42846 (42846 target) ; Learning rate = 0.000308 ; Loss = 1.521333\n",
      "2024-12-08 11:59:58.090000: I runner.py:310] Step = 82500 ; steps/s = 1.63, tokens/s = 42403 (42403 target) ; Learning rate = 0.000308 ; Loss = 1.523619\n",
      "2024-12-08 12:00:59.819000: I runner.py:310] Step = 82600 ; steps/s = 1.62, tokens/s = 42805 (42805 target) ; Learning rate = 0.000308 ; Loss = 1.519395\n",
      "2024-12-08 12:02:01.470000: I runner.py:310] Step = 82700 ; steps/s = 1.62, tokens/s = 42899 (42899 target) ; Learning rate = 0.000307 ; Loss = 1.518202\n",
      "2024-12-08 12:03:03.185000: I runner.py:310] Step = 82800 ; steps/s = 1.62, tokens/s = 42869 (42869 target) ; Learning rate = 0.000307 ; Loss = 1.518755\n",
      "2024-12-08 12:04:04.522000: I runner.py:310] Step = 82900 ; steps/s = 1.63, tokens/s = 42336 (42336 target) ; Learning rate = 0.000307 ; Loss = 1.506160\n",
      "2024-12-08 12:05:06.291000: I runner.py:310] Step = 83000 ; steps/s = 1.62, tokens/s = 42823 (42823 target) ; Learning rate = 0.000307 ; Loss = 1.512678\n",
      "2024-12-08 12:06:08.055000: I runner.py:310] Step = 83100 ; steps/s = 1.62, tokens/s = 42809 (42809 target) ; Learning rate = 0.000307 ; Loss = 1.522435\n",
      "2024-12-08 12:07:09.767000: I runner.py:310] Step = 83200 ; steps/s = 1.62, tokens/s = 42844 (42844 target) ; Learning rate = 0.000306 ; Loss = 1.513465\n",
      "2024-12-08 12:08:11.041000: I runner.py:310] Step = 83300 ; steps/s = 1.63, tokens/s = 42353 (42353 target) ; Learning rate = 0.000306 ; Loss = 1.518409\n",
      "2024-12-08 12:09:12.771000: I runner.py:310] Step = 83400 ; steps/s = 1.62, tokens/s = 42877 (42877 target) ; Learning rate = 0.000306 ; Loss = 1.509680\n",
      "2024-12-08 12:10:14.504000: I runner.py:310] Step = 83500 ; steps/s = 1.62, tokens/s = 42837 (42837 target) ; Learning rate = 0.000306 ; Loss = 1.510310\n",
      "2024-12-08 12:11:15.936000: I runner.py:310] Step = 83600 ; steps/s = 1.63, tokens/s = 42437 (42437 target) ; Learning rate = 0.000306 ; Loss = 1.509276\n",
      "2024-12-08 12:12:17.493000: I runner.py:310] Step = 83700 ; steps/s = 1.62, tokens/s = 42778 (42778 target) ; Learning rate = 0.000306 ; Loss = 1.512665\n",
      "2024-12-08 12:13:19.216000: I runner.py:310] Step = 83800 ; steps/s = 1.62, tokens/s = 42840 (42840 target) ; Learning rate = 0.000305 ; Loss = 1.518663\n",
      "2024-12-08 12:14:20.899000: I runner.py:310] Step = 83900 ; steps/s = 1.62, tokens/s = 42870 (42870 target) ; Learning rate = 0.000305 ; Loss = 1.514559\n",
      "2024-12-08 12:15:22.166000: I runner.py:310] Step = 84000 ; steps/s = 1.63, tokens/s = 42331 (42331 target) ; Learning rate = 0.000305 ; Loss = 1.508373\n",
      "2024-12-08 12:16:23.912000: I runner.py:310] Step = 84100 ; steps/s = 1.62, tokens/s = 42862 (42862 target) ; Learning rate = 0.000305 ; Loss = 1.510352\n",
      "2024-12-08 12:17:25.623000: I runner.py:310] Step = 84200 ; steps/s = 1.62, tokens/s = 42860 (42860 target) ; Learning rate = 0.000305 ; Loss = 1.510150\n",
      "2024-12-08 12:18:27.407000: I runner.py:310] Step = 84300 ; steps/s = 1.62, tokens/s = 42794 (42794 target) ; Learning rate = 0.000304 ; Loss = 1.514630\n",
      "2024-12-08 12:19:28.665000: I runner.py:310] Step = 84400 ; steps/s = 1.63, tokens/s = 42353 (42353 target) ; Learning rate = 0.000304 ; Loss = 1.513741\n",
      "2024-12-08 12:20:30.356000: I runner.py:310] Step = 84500 ; steps/s = 1.62, tokens/s = 42873 (42873 target) ; Learning rate = 0.000304 ; Loss = 1.508704\n",
      "2024-12-08 12:21:32.048000: I runner.py:310] Step = 84600 ; steps/s = 1.62, tokens/s = 42852 (42852 target) ; Learning rate = 0.000304 ; Loss = 1.519178\n",
      "2024-12-08 12:22:33.753000: I runner.py:310] Step = 84700 ; steps/s = 1.62, tokens/s = 42860 (42860 target) ; Learning rate = 0.000304 ; Loss = 1.513660\n",
      "2024-12-08 12:23:34.994000: I runner.py:310] Step = 84800 ; steps/s = 1.63, tokens/s = 42394 (42394 target) ; Learning rate = 0.000304 ; Loss = 1.519636\n",
      "2024-12-08 12:24:36.705000: I runner.py:310] Step = 84900 ; steps/s = 1.62, tokens/s = 42857 (42857 target) ; Learning rate = 0.000303 ; Loss = 1.512350\n",
      "2024-12-08 12:25:38.495000: I runner.py:310] Step = 85000 ; steps/s = 1.62, tokens/s = 42775 (42775 target) ; Learning rate = 0.000303 ; Loss = 1.513156\n",
      "2024-12-08 12:25:38.498000: I training.py:192] Running evaluation for step 85000\n",
      "2024-12-08 12:30:23.312000: I training.py:192] Evaluation result for step 85000: loss = 1.171415 ; perplexity = 3.226554\n",
      "2024-12-08 12:31:24.954000: I runner.py:310] Step = 85100 ; steps/s = 1.62, tokens/s = 42918 (42918 target) ; Learning rate = 0.000303 ; Loss = 1.516999\n",
      "2024-12-08 12:32:26.254000: I runner.py:310] Step = 85200 ; steps/s = 1.63, tokens/s = 42351 (42351 target) ; Learning rate = 0.000303 ; Loss = 1.514944\n",
      "2024-12-08 12:33:27.980000: I runner.py:310] Step = 85300 ; steps/s = 1.62, tokens/s = 42877 (42877 target) ; Learning rate = 0.000303 ; Loss = 1.512592\n",
      "2024-12-08 12:34:29.762000: I runner.py:310] Step = 85400 ; steps/s = 1.62, tokens/s = 42808 (42808 target) ; Learning rate = 0.000302 ; Loss = 1.516393\n",
      "2024-12-08 12:35:31.462000: I runner.py:310] Step = 85500 ; steps/s = 1.62, tokens/s = 42878 (42878 target) ; Learning rate = 0.000302 ; Loss = 1.512889\n",
      "2024-12-08 12:36:32.714000: I runner.py:310] Step = 85600 ; steps/s = 1.63, tokens/s = 42331 (42331 target) ; Learning rate = 0.000302 ; Loss = 1.513905\n",
      "2024-12-08 12:37:34.391000: I runner.py:310] Step = 85700 ; steps/s = 1.62, tokens/s = 42905 (42905 target) ; Learning rate = 0.000302 ; Loss = 1.513572\n",
      "2024-12-08 12:38:36.075000: I runner.py:310] Step = 85800 ; steps/s = 1.62, tokens/s = 42862 (42862 target) ; Learning rate = 0.000302 ; Loss = 1.510645\n",
      "2024-12-08 12:39:37.898000: I runner.py:310] Step = 85900 ; steps/s = 1.62, tokens/s = 42759 (42759 target) ; Learning rate = 0.000302 ; Loss = 1.518899\n",
      "2024-12-08 12:40:39.167000: I runner.py:310] Step = 86000 ; steps/s = 1.63, tokens/s = 42373 (42373 target) ; Learning rate = 0.000301 ; Loss = 1.514375\n",
      "2024-12-08 12:41:40.871000: I runner.py:310] Step = 86100 ; steps/s = 1.62, tokens/s = 42900 (42900 target) ; Learning rate = 0.000301 ; Loss = 1.511717\n",
      "2024-12-08 12:42:42.660000: I runner.py:310] Step = 86200 ; steps/s = 1.62, tokens/s = 42805 (42805 target) ; Learning rate = 0.000301 ; Loss = 1.510858\n",
      "2024-12-08 12:43:44.347000: I runner.py:310] Step = 86300 ; steps/s = 1.62, tokens/s = 42849 (42849 target) ; Learning rate = 0.000301 ; Loss = 1.510073\n",
      "2024-12-08 12:44:45.668000: I runner.py:310] Step = 86400 ; steps/s = 1.63, tokens/s = 42296 (42296 target) ; Learning rate = 0.000301 ; Loss = 1.514796\n",
      "2024-12-08 12:45:47.391000: I runner.py:310] Step = 86500 ; steps/s = 1.62, tokens/s = 42867 (42867 target) ; Learning rate = 0.000301 ; Loss = 1.511091\n",
      "2024-12-08 12:46:49.137000: I runner.py:310] Step = 86600 ; steps/s = 1.62, tokens/s = 42812 (42812 target) ; Learning rate = 0.000300 ; Loss = 1.517798\n",
      "2024-12-08 12:47:50.866000: I runner.py:310] Step = 86700 ; steps/s = 1.62, tokens/s = 42865 (42865 target) ; Learning rate = 0.000300 ; Loss = 1.514880\n",
      "2024-12-08 12:48:52.133000: I runner.py:310] Step = 86800 ; steps/s = 1.63, tokens/s = 42346 (42346 target) ; Learning rate = 0.000300 ; Loss = 1.512388\n",
      "2024-12-08 12:49:53.874000: I runner.py:310] Step = 86900 ; steps/s = 1.62, tokens/s = 42872 (42872 target) ; Learning rate = 0.000300 ; Loss = 1.508383\n",
      "2024-12-08 12:50:55.634000: I runner.py:310] Step = 87000 ; steps/s = 1.62, tokens/s = 42769 (42769 target) ; Learning rate = 0.000300 ; Loss = 1.510659\n",
      "2024-12-08 12:51:57.409000: I runner.py:310] Step = 87100 ; steps/s = 1.62, tokens/s = 42821 (42821 target) ; Learning rate = 0.000299 ; Loss = 1.521548\n",
      "2024-12-08 12:53:00.021000: I runner.py:310] Step = 87200 ; steps/s = 1.60, tokens/s = 41495 (41495 target) ; Learning rate = 0.000299 ; Loss = 1.516151\n",
      "2024-12-08 12:54:01.941000: I runner.py:310] Step = 87300 ; steps/s = 1.62, tokens/s = 42722 (42722 target) ; Learning rate = 0.000299 ; Loss = 1.512274\n",
      "2024-12-08 12:55:03.732000: I runner.py:310] Step = 87400 ; steps/s = 1.62, tokens/s = 42785 (42785 target) ; Learning rate = 0.000299 ; Loss = 1.508004\n",
      "2024-12-08 12:56:05.480000: I runner.py:310] Step = 87500 ; steps/s = 1.62, tokens/s = 42798 (42798 target) ; Learning rate = 0.000299 ; Loss = 1.516063\n",
      "2024-12-08 12:57:06.837000: I runner.py:310] Step = 87600 ; steps/s = 1.63, tokens/s = 42321 (42321 target) ; Learning rate = 0.000299 ; Loss = 1.516125\n",
      "2024-12-08 12:58:08.570000: I runner.py:310] Step = 87700 ; steps/s = 1.62, tokens/s = 42843 (42843 target) ; Learning rate = 0.000298 ; Loss = 1.514061\n",
      "2024-12-08 12:59:10.329000: I runner.py:310] Step = 87800 ; steps/s = 1.62, tokens/s = 42829 (42829 target) ; Learning rate = 0.000298 ; Loss = 1.517600\n",
      "2024-12-08 13:00:12.112000: I runner.py:310] Step = 87900 ; steps/s = 1.62, tokens/s = 42784 (42784 target) ; Learning rate = 0.000298 ; Loss = 1.509171\n",
      "2024-12-08 13:01:13.402000: I runner.py:310] Step = 88000 ; steps/s = 1.63, tokens/s = 42327 (42327 target) ; Learning rate = 0.000298 ; Loss = 1.516671\n",
      "2024-12-08 13:02:15.133000: I runner.py:310] Step = 88100 ; steps/s = 1.62, tokens/s = 42868 (42868 target) ; Learning rate = 0.000298 ; Loss = 1.510325\n",
      "2024-12-08 13:03:16.838000: I runner.py:310] Step = 88200 ; steps/s = 1.62, tokens/s = 42842 (42842 target) ; Learning rate = 0.000298 ; Loss = 1.508579\n",
      "2024-12-08 13:04:18.548000: I runner.py:310] Step = 88300 ; steps/s = 1.62, tokens/s = 42867 (42867 target) ; Learning rate = 0.000297 ; Loss = 1.508285\n",
      "2024-12-08 13:05:19.915000: I runner.py:310] Step = 88400 ; steps/s = 1.63, tokens/s = 42293 (42293 target) ; Learning rate = 0.000297 ; Loss = 1.504341\n",
      "2024-12-08 13:06:21.681000: I runner.py:310] Step = 88500 ; steps/s = 1.62, tokens/s = 42824 (42824 target) ; Learning rate = 0.000297 ; Loss = 1.508860\n",
      "2024-12-08 13:07:23.460000: I runner.py:310] Step = 88600 ; steps/s = 1.62, tokens/s = 42808 (42808 target) ; Learning rate = 0.000297 ; Loss = 1.510988\n",
      "2024-12-08 13:08:25.115000: I runner.py:310] Step = 88700 ; steps/s = 1.62, tokens/s = 42891 (42891 target) ; Learning rate = 0.000297 ; Loss = 1.513931\n",
      "2024-12-08 13:09:26.322000: I runner.py:310] Step = 88800 ; steps/s = 1.63, tokens/s = 42402 (42402 target) ; Learning rate = 0.000297 ; Loss = 1.502175\n",
      "2024-12-08 13:10:28.016000: I runner.py:310] Step = 88900 ; steps/s = 1.62, tokens/s = 42898 (42898 target) ; Learning rate = 0.000296 ; Loss = 1.515345\n",
      "2024-12-08 13:11:29.704000: I runner.py:310] Step = 89000 ; steps/s = 1.62, tokens/s = 42861 (42861 target) ; Learning rate = 0.000296 ; Loss = 1.508895\n",
      "2024-12-08 13:12:31.375000: I runner.py:310] Step = 89100 ; steps/s = 1.62, tokens/s = 42885 (42885 target) ; Learning rate = 0.000296 ; Loss = 1.517009\n",
      "2024-12-08 13:13:32.640000: I runner.py:310] Step = 89200 ; steps/s = 1.63, tokens/s = 42337 (42337 target) ; Learning rate = 0.000296 ; Loss = 1.515174\n",
      "2024-12-08 13:14:34.311000: I runner.py:310] Step = 89300 ; steps/s = 1.62, tokens/s = 42864 (42864 target) ; Learning rate = 0.000296 ; Loss = 1.509831\n",
      "2024-12-08 13:15:35.986000: I runner.py:310] Step = 89400 ; steps/s = 1.62, tokens/s = 42892 (42892 target) ; Learning rate = 0.000296 ; Loss = 1.514081\n",
      "2024-12-08 13:16:37.691000: I runner.py:310] Step = 89500 ; steps/s = 1.62, tokens/s = 42862 (42862 target) ; Learning rate = 0.000295 ; Loss = 1.506036\n",
      "2024-12-08 13:17:38.996000: I runner.py:310] Step = 89600 ; steps/s = 1.63, tokens/s = 42351 (42351 target) ; Learning rate = 0.000295 ; Loss = 1.503644\n",
      "2024-12-08 13:18:40.669000: I runner.py:310] Step = 89700 ; steps/s = 1.62, tokens/s = 42887 (42887 target) ; Learning rate = 0.000295 ; Loss = 1.508787\n",
      "2024-12-08 13:19:42.343000: I runner.py:310] Step = 89800 ; steps/s = 1.62, tokens/s = 42880 (42880 target) ; Learning rate = 0.000295 ; Loss = 1.510161\n",
      "2024-12-08 13:20:44.057000: I runner.py:310] Step = 89900 ; steps/s = 1.62, tokens/s = 42852 (42852 target) ; Learning rate = 0.000295 ; Loss = 1.512470\n",
      "2024-12-08 13:21:45.316000: I runner.py:310] Step = 90000 ; steps/s = 1.63, tokens/s = 42383 (42383 target) ; Learning rate = 0.000295 ; Loss = 1.499690\n",
      "2024-12-08 13:21:47.504000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-90000\n",
      "2024-12-08 13:21:47.505000: I training.py:192] Running evaluation for step 90000\n",
      "2024-12-08 13:26:29.597000: I training.py:192] Evaluation result for step 90000: loss = 1.178613 ; perplexity = 3.249865\n",
      "2024-12-08 13:27:31.142000: I runner.py:310] Step = 90100 ; steps/s = 1.63, tokens/s = 42997 (42997 target) ; Learning rate = 0.000294 ; Loss = 1.510434\n",
      "2024-12-08 13:28:32.881000: I runner.py:310] Step = 90200 ; steps/s = 1.62, tokens/s = 42829 (42829 target) ; Learning rate = 0.000294 ; Loss = 1.508276\n",
      "2024-12-08 13:29:34.605000: I runner.py:310] Step = 90300 ; steps/s = 1.62, tokens/s = 42832 (42832 target) ; Learning rate = 0.000294 ; Loss = 1.509966\n",
      "2024-12-08 13:30:35.844000: I runner.py:310] Step = 90400 ; steps/s = 1.63, tokens/s = 42367 (42367 target) ; Learning rate = 0.000294 ; Loss = 1.520329\n",
      "2024-12-08 13:31:37.526000: I runner.py:310] Step = 90500 ; steps/s = 1.62, tokens/s = 42867 (42867 target) ; Learning rate = 0.000294 ; Loss = 1.503776\n",
      "2024-12-08 13:32:39.297000: I runner.py:310] Step = 90600 ; steps/s = 1.62, tokens/s = 42838 (42838 target) ; Learning rate = 0.000294 ; Loss = 1.505866\n",
      "2024-12-08 13:33:41.089000: I runner.py:310] Step = 90700 ; steps/s = 1.62, tokens/s = 42770 (42770 target) ; Learning rate = 0.000293 ; Loss = 1.510289\n",
      "2024-12-08 13:34:42.480000: I runner.py:310] Step = 90800 ; steps/s = 1.63, tokens/s = 42270 (42270 target) ; Learning rate = 0.000293 ; Loss = 1.498228\n",
      "2024-12-08 13:35:44.229000: I runner.py:310] Step = 90900 ; steps/s = 1.62, tokens/s = 42808 (42808 target) ; Learning rate = 0.000293 ; Loss = 1.517860\n",
      "2024-12-08 13:36:46.043000: I runner.py:310] Step = 91000 ; steps/s = 1.62, tokens/s = 42805 (42805 target) ; Learning rate = 0.000293 ; Loss = 1.513158\n",
      "2024-12-08 13:37:47.838000: I runner.py:310] Step = 91100 ; steps/s = 1.62, tokens/s = 42830 (42830 target) ; Learning rate = 0.000293 ; Loss = 1.510285\n",
      "2024-12-08 13:38:49.197000: I runner.py:310] Step = 91200 ; steps/s = 1.63, tokens/s = 42308 (42308 target) ; Learning rate = 0.000293 ; Loss = 1.500063\n",
      "2024-12-08 13:39:50.895000: I runner.py:310] Step = 91300 ; steps/s = 1.62, tokens/s = 42844 (42844 target) ; Learning rate = 0.000293 ; Loss = 1.512164\n",
      "2024-12-08 13:40:52.598000: I runner.py:310] Step = 91400 ; steps/s = 1.62, tokens/s = 42815 (42815 target) ; Learning rate = 0.000292 ; Loss = 1.509315\n",
      "2024-12-08 13:41:54.423000: I runner.py:310] Step = 91500 ; steps/s = 1.62, tokens/s = 42821 (42821 target) ; Learning rate = 0.000292 ; Loss = 1.515906\n",
      "2024-12-08 13:42:55.664000: I runner.py:310] Step = 91600 ; steps/s = 1.63, tokens/s = 42401 (42401 target) ; Learning rate = 0.000292 ; Loss = 1.497402\n",
      "2024-12-08 13:43:57.423000: I runner.py:310] Step = 91700 ; steps/s = 1.62, tokens/s = 42840 (42840 target) ; Learning rate = 0.000292 ; Loss = 1.506419\n",
      "2024-12-08 13:44:59.182000: I runner.py:310] Step = 91800 ; steps/s = 1.62, tokens/s = 42808 (42808 target) ; Learning rate = 0.000292 ; Loss = 1.512391\n",
      "2024-12-08 13:46:00.964000: I runner.py:310] Step = 91900 ; steps/s = 1.62, tokens/s = 42785 (42785 target) ; Learning rate = 0.000292 ; Loss = 1.517322\n",
      "2024-12-08 13:47:02.259000: I runner.py:310] Step = 92000 ; steps/s = 1.63, tokens/s = 42366 (42366 target) ; Learning rate = 0.000291 ; Loss = 1.516339\n",
      "2024-12-08 13:48:03.999000: I runner.py:310] Step = 92100 ; steps/s = 1.62, tokens/s = 42823 (42823 target) ; Learning rate = 0.000291 ; Loss = 1.513426\n",
      "2024-12-08 13:49:05.754000: I runner.py:310] Step = 92200 ; steps/s = 1.62, tokens/s = 42788 (42788 target) ; Learning rate = 0.000291 ; Loss = 1.501929\n",
      "2024-12-08 13:50:07.509000: I runner.py:310] Step = 92300 ; steps/s = 1.62, tokens/s = 42858 (42858 target) ; Learning rate = 0.000291 ; Loss = 1.504698\n",
      "2024-12-08 13:51:08.799000: I runner.py:310] Step = 92400 ; steps/s = 1.63, tokens/s = 42329 (42329 target) ; Learning rate = 0.000291 ; Loss = 1.500324\n",
      "2024-12-08 13:52:10.625000: I runner.py:310] Step = 92500 ; steps/s = 1.62, tokens/s = 42783 (42783 target) ; Learning rate = 0.000291 ; Loss = 1.509414\n",
      "2024-12-08 13:53:12.332000: I runner.py:310] Step = 92600 ; steps/s = 1.62, tokens/s = 42852 (42852 target) ; Learning rate = 0.000290 ; Loss = 1.513901\n",
      "2024-12-08 13:54:14.143000: I runner.py:310] Step = 92700 ; steps/s = 1.62, tokens/s = 42786 (42786 target) ; Learning rate = 0.000290 ; Loss = 1.515231\n",
      "2024-12-08 13:55:15.485000: I runner.py:310] Step = 92800 ; steps/s = 1.63, tokens/s = 42330 (42330 target) ; Learning rate = 0.000290 ; Loss = 1.507462\n",
      "2024-12-08 13:56:17.264000: I runner.py:310] Step = 92900 ; steps/s = 1.62, tokens/s = 42790 (42790 target) ; Learning rate = 0.000290 ; Loss = 1.514712\n",
      "2024-12-08 13:57:19.012000: I runner.py:310] Step = 93000 ; steps/s = 1.62, tokens/s = 42853 (42853 target) ; Learning rate = 0.000290 ; Loss = 1.506046\n",
      "2024-12-08 13:58:20.724000: I runner.py:310] Step = 93100 ; steps/s = 1.62, tokens/s = 42847 (42847 target) ; Learning rate = 0.000290 ; Loss = 1.517773\n",
      "2024-12-08 13:59:22.098000: I runner.py:310] Step = 93200 ; steps/s = 1.63, tokens/s = 42281 (42281 target) ; Learning rate = 0.000290 ; Loss = 1.510102\n",
      "2024-12-08 14:00:23.873000: I runner.py:310] Step = 93300 ; steps/s = 1.62, tokens/s = 42830 (42830 target) ; Learning rate = 0.000289 ; Loss = 1.500819\n",
      "2024-12-08 14:01:25.630000: I runner.py:310] Step = 93400 ; steps/s = 1.62, tokens/s = 42818 (42818 target) ; Learning rate = 0.000289 ; Loss = 1.510475\n",
      "2024-12-08 14:02:27.376000: I runner.py:310] Step = 93500 ; steps/s = 1.62, tokens/s = 42814 (42814 target) ; Learning rate = 0.000289 ; Loss = 1.509754\n",
      "2024-12-08 14:03:28.702000: I runner.py:310] Step = 93600 ; steps/s = 1.63, tokens/s = 42329 (42329 target) ; Learning rate = 0.000289 ; Loss = 1.513896\n",
      "2024-12-08 14:04:30.528000: I runner.py:310] Step = 93700 ; steps/s = 1.62, tokens/s = 42775 (42775 target) ; Learning rate = 0.000289 ; Loss = 1.506118\n",
      "2024-12-08 14:05:32.318000: I runner.py:310] Step = 93800 ; steps/s = 1.62, tokens/s = 42825 (42825 target) ; Learning rate = 0.000289 ; Loss = 1.507668\n",
      "2024-12-08 14:06:33.946000: I runner.py:310] Step = 93900 ; steps/s = 1.62, tokens/s = 42695 (42695 target) ; Learning rate = 0.000288 ; Loss = 1.530920\n",
      "2024-12-08 14:07:35.372000: I runner.py:310] Step = 94000 ; steps/s = 1.63, tokens/s = 42460 (42460 target) ; Learning rate = 0.000288 ; Loss = 1.513715\n",
      "2024-12-08 14:08:37.123000: I runner.py:310] Step = 94100 ; steps/s = 1.62, tokens/s = 42834 (42834 target) ; Learning rate = 0.000288 ; Loss = 1.503021\n",
      "2024-12-08 14:09:38.812000: I runner.py:310] Step = 94200 ; steps/s = 1.62, tokens/s = 42843 (42843 target) ; Learning rate = 0.000288 ; Loss = 1.507855\n",
      "2024-12-08 14:10:40.035000: I runner.py:310] Step = 94300 ; steps/s = 1.63, tokens/s = 42385 (42385 target) ; Learning rate = 0.000288 ; Loss = 1.503211\n",
      "2024-12-08 14:11:41.818000: I runner.py:310] Step = 94400 ; steps/s = 1.62, tokens/s = 42817 (42817 target) ; Learning rate = 0.000288 ; Loss = 1.511547\n",
      "2024-12-08 14:12:43.616000: I runner.py:310] Step = 94500 ; steps/s = 1.62, tokens/s = 42808 (42808 target) ; Learning rate = 0.000288 ; Loss = 1.500016\n",
      "2024-12-08 14:13:45.367000: I runner.py:310] Step = 94600 ; steps/s = 1.62, tokens/s = 42813 (42813 target) ; Learning rate = 0.000287 ; Loss = 1.513186\n",
      "2024-12-08 14:14:46.779000: I runner.py:310] Step = 94700 ; steps/s = 1.63, tokens/s = 42263 (42263 target) ; Learning rate = 0.000287 ; Loss = 1.510232\n",
      "2024-12-08 14:15:48.477000: I runner.py:310] Step = 94800 ; steps/s = 1.62, tokens/s = 42864 (42864 target) ; Learning rate = 0.000287 ; Loss = 1.507033\n",
      "2024-12-08 14:16:50.269000: I runner.py:310] Step = 94900 ; steps/s = 1.62, tokens/s = 42791 (42791 target) ; Learning rate = 0.000287 ; Loss = 1.499805\n",
      "2024-12-08 14:17:52.059000: I runner.py:310] Step = 95000 ; steps/s = 1.62, tokens/s = 42791 (42791 target) ; Learning rate = 0.000287 ; Loss = 1.511142\n",
      "2024-12-08 14:17:52.060000: I training.py:192] Running evaluation for step 95000\n",
      "2024-12-08 14:22:28.007000: I training.py:192] Evaluation result for step 95000: loss = 1.179886 ; perplexity = 3.254003\n",
      "2024-12-08 14:23:29.116000: I runner.py:310] Step = 95100 ; steps/s = 1.64, tokens/s = 42514 (42514 target) ; Learning rate = 0.000287 ; Loss = 1.504714\n",
      "2024-12-08 14:24:30.879000: I runner.py:310] Step = 95200 ; steps/s = 1.62, tokens/s = 42849 (42849 target) ; Learning rate = 0.000286 ; Loss = 1.502903\n",
      "2024-12-08 14:25:32.638000: I runner.py:310] Step = 95300 ; steps/s = 1.62, tokens/s = 42805 (42805 target) ; Learning rate = 0.000286 ; Loss = 1.508891\n",
      "2024-12-08 14:26:34.477000: I runner.py:310] Step = 95400 ; steps/s = 1.62, tokens/s = 42753 (42753 target) ; Learning rate = 0.000286 ; Loss = 1.506282\n",
      "2024-12-08 14:27:35.843000: I runner.py:310] Step = 95500 ; steps/s = 1.63, tokens/s = 42291 (42291 target) ; Learning rate = 0.000286 ; Loss = 1.506041\n",
      "2024-12-08 14:28:37.633000: I runner.py:310] Step = 95600 ; steps/s = 1.62, tokens/s = 42822 (42822 target) ; Learning rate = 0.000286 ; Loss = 1.513590\n",
      "2024-12-08 14:29:39.407000: I runner.py:310] Step = 95700 ; steps/s = 1.62, tokens/s = 42793 (42793 target) ; Learning rate = 0.000286 ; Loss = 1.504205\n",
      "2024-12-08 14:30:41.162000: I runner.py:310] Step = 95800 ; steps/s = 1.62, tokens/s = 42799 (42799 target) ; Learning rate = 0.000286 ; Loss = 1.507121\n",
      "2024-12-08 14:31:42.465000: I runner.py:310] Step = 95900 ; steps/s = 1.63, tokens/s = 42364 (42364 target) ; Learning rate = 0.000285 ; Loss = 1.508359\n",
      "2024-12-08 14:32:44.263000: I runner.py:310] Step = 96000 ; steps/s = 1.62, tokens/s = 42805 (42805 target) ; Learning rate = 0.000285 ; Loss = 1.507903\n",
      "2024-12-08 14:33:46.003000: I runner.py:310] Step = 96100 ; steps/s = 1.62, tokens/s = 42831 (42831 target) ; Learning rate = 0.000285 ; Loss = 1.511049\n",
      "2024-12-08 14:34:47.821000: I runner.py:310] Step = 96200 ; steps/s = 1.62, tokens/s = 42774 (42774 target) ; Learning rate = 0.000285 ; Loss = 1.503855\n",
      "2024-12-08 14:35:49.167000: I runner.py:310] Step = 96300 ; steps/s = 1.63, tokens/s = 42297 (42297 target) ; Learning rate = 0.000285 ; Loss = 1.510105\n",
      "2024-12-08 14:36:50.965000: I runner.py:310] Step = 96400 ; steps/s = 1.62, tokens/s = 42844 (42844 target) ; Learning rate = 0.000285 ; Loss = 1.504176\n",
      "2024-12-08 14:37:52.783000: I runner.py:310] Step = 96500 ; steps/s = 1.62, tokens/s = 42749 (42749 target) ; Learning rate = 0.000285 ; Loss = 1.508666\n",
      "2024-12-08 14:38:54.564000: I runner.py:310] Step = 96600 ; steps/s = 1.62, tokens/s = 42802 (42802 target) ; Learning rate = 0.000284 ; Loss = 1.505201\n",
      "2024-12-08 14:39:55.939000: I runner.py:310] Step = 96700 ; steps/s = 1.63, tokens/s = 42290 (42290 target) ; Learning rate = 0.000284 ; Loss = 1.507329\n",
      "2024-12-08 14:40:57.690000: I runner.py:310] Step = 96800 ; steps/s = 1.62, tokens/s = 42860 (42860 target) ; Learning rate = 0.000284 ; Loss = 1.508013\n",
      "2024-12-08 14:41:59.463000: I runner.py:310] Step = 96900 ; steps/s = 1.62, tokens/s = 42778 (42778 target) ; Learning rate = 0.000284 ; Loss = 1.506325\n",
      "2024-12-08 14:43:01.316000: I runner.py:310] Step = 97000 ; steps/s = 1.62, tokens/s = 42788 (42788 target) ; Learning rate = 0.000284 ; Loss = 1.513687\n",
      "2024-12-08 14:44:02.718000: I runner.py:310] Step = 97100 ; steps/s = 1.63, tokens/s = 42240 (42240 target) ; Learning rate = 0.000284 ; Loss = 1.507345\n",
      "2024-12-08 14:45:04.533000: I runner.py:310] Step = 97200 ; steps/s = 1.62, tokens/s = 42818 (42818 target) ; Learning rate = 0.000284 ; Loss = 1.505282\n",
      "2024-12-08 14:46:06.237000: I runner.py:310] Step = 97300 ; steps/s = 1.62, tokens/s = 42862 (42862 target) ; Learning rate = 0.000283 ; Loss = 1.506064\n",
      "2024-12-08 14:47:08.003000: I runner.py:310] Step = 97400 ; steps/s = 1.62, tokens/s = 42805 (42805 target) ; Learning rate = 0.000283 ; Loss = 1.506494\n",
      "2024-12-08 14:48:09.360000: I runner.py:310] Step = 97500 ; steps/s = 1.63, tokens/s = 42263 (42263 target) ; Learning rate = 0.000283 ; Loss = 1.507982\n",
      "2024-12-08 14:49:11.147000: I runner.py:310] Step = 97600 ; steps/s = 1.62, tokens/s = 42779 (42779 target) ; Learning rate = 0.000283 ; Loss = 1.496324\n",
      "2024-12-08 14:50:12.925000: I runner.py:310] Step = 97700 ; steps/s = 1.62, tokens/s = 42852 (42852 target) ; Learning rate = 0.000283 ; Loss = 1.506091\n",
      "2024-12-08 14:51:14.716000: I runner.py:310] Step = 97800 ; steps/s = 1.62, tokens/s = 42771 (42771 target) ; Learning rate = 0.000283 ; Loss = 1.506727\n",
      "2024-12-08 14:52:16.093000: I runner.py:310] Step = 97900 ; steps/s = 1.63, tokens/s = 42305 (42305 target) ; Learning rate = 0.000282 ; Loss = 1.501575\n",
      "2024-12-08 14:53:17.910000: I runner.py:310] Step = 98000 ; steps/s = 1.62, tokens/s = 42778 (42778 target) ; Learning rate = 0.000282 ; Loss = 1.499487\n",
      "2024-12-08 14:54:19.688000: I runner.py:310] Step = 98100 ; steps/s = 1.62, tokens/s = 42786 (42786 target) ; Learning rate = 0.000282 ; Loss = 1.511529\n",
      "2024-12-08 14:55:21.506000: I runner.py:310] Step = 98200 ; steps/s = 1.62, tokens/s = 42823 (42823 target) ; Learning rate = 0.000282 ; Loss = 1.508271\n",
      "2024-12-08 14:56:22.827000: I runner.py:310] Step = 98300 ; steps/s = 1.63, tokens/s = 42311 (42311 target) ; Learning rate = 0.000282 ; Loss = 1.516306\n",
      "2024-12-08 14:57:24.612000: I runner.py:310] Step = 98400 ; steps/s = 1.62, tokens/s = 42806 (42806 target) ; Learning rate = 0.000282 ; Loss = 1.496006\n",
      "2024-12-08 14:58:26.351000: I runner.py:310] Step = 98500 ; steps/s = 1.62, tokens/s = 42834 (42834 target) ; Learning rate = 0.000282 ; Loss = 1.496599\n",
      "2024-12-08 14:59:28.136000: I runner.py:310] Step = 98600 ; steps/s = 1.62, tokens/s = 42799 (42799 target) ; Learning rate = 0.000281 ; Loss = 1.515957\n",
      "2024-12-08 15:00:29.389000: I runner.py:310] Step = 98700 ; steps/s = 1.63, tokens/s = 42380 (42380 target) ; Learning rate = 0.000281 ; Loss = 1.505947\n",
      "2024-12-08 15:01:31.129000: I runner.py:310] Step = 98800 ; steps/s = 1.62, tokens/s = 42863 (42863 target) ; Learning rate = 0.000281 ; Loss = 1.503692\n",
      "2024-12-08 15:02:32.869000: I runner.py:310] Step = 98900 ; steps/s = 1.62, tokens/s = 42828 (42828 target) ; Learning rate = 0.000281 ; Loss = 1.502785\n",
      "2024-12-08 15:03:34.710000: I runner.py:310] Step = 99000 ; steps/s = 1.62, tokens/s = 42740 (42740 target) ; Learning rate = 0.000281 ; Loss = 1.512356\n",
      "2024-12-08 15:04:36.061000: I runner.py:310] Step = 99100 ; steps/s = 1.63, tokens/s = 42309 (42309 target) ; Learning rate = 0.000281 ; Loss = 1.512534\n",
      "2024-12-08 15:05:37.890000: I runner.py:310] Step = 99200 ; steps/s = 1.62, tokens/s = 42799 (42799 target) ; Learning rate = 0.000281 ; Loss = 1.505516\n",
      "2024-12-08 15:06:39.662000: I runner.py:310] Step = 99300 ; steps/s = 1.62, tokens/s = 42803 (42803 target) ; Learning rate = 0.000280 ; Loss = 1.506186\n",
      "2024-12-08 15:07:41.473000: I runner.py:310] Step = 99400 ; steps/s = 1.62, tokens/s = 42752 (42752 target) ; Learning rate = 0.000280 ; Loss = 1.506334\n",
      "2024-12-08 15:08:42.854000: I runner.py:310] Step = 99500 ; steps/s = 1.63, tokens/s = 42303 (42303 target) ; Learning rate = 0.000280 ; Loss = 1.495198\n",
      "2024-12-08 15:09:44.579000: I runner.py:310] Step = 99600 ; steps/s = 1.62, tokens/s = 42845 (42845 target) ; Learning rate = 0.000280 ; Loss = 1.500646\n",
      "2024-12-08 15:10:46.350000: I runner.py:310] Step = 99700 ; steps/s = 1.62, tokens/s = 42810 (42810 target) ; Learning rate = 0.000280 ; Loss = 1.505246\n",
      "2024-12-08 15:11:48.126000: I runner.py:310] Step = 99800 ; steps/s = 1.62, tokens/s = 42792 (42792 target) ; Learning rate = 0.000280 ; Loss = 1.504275\n",
      "2024-12-08 15:12:49.482000: I runner.py:310] Step = 99900 ; steps/s = 1.63, tokens/s = 42320 (42320 target) ; Learning rate = 0.000280 ; Loss = 1.497152\n",
      "2024-12-08 15:13:51.227000: I runner.py:310] Step = 100000 ; steps/s = 1.62, tokens/s = 42852 (42852 target) ; Learning rate = 0.000280 ; Loss = 1.510871\n",
      "2024-12-08 15:13:53.339000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-100000\n",
      "2024-12-08 15:13:53.339000: I training.py:192] Running evaluation for step 100000\n",
      "2024-12-08 15:18:34.486000: I training.py:192] Evaluation result for step 100000: loss = 1.184140 ; perplexity = 3.267874\n",
      "2024-12-08 15:19:36.104000: I runner.py:310] Step = 100100 ; steps/s = 1.62, tokens/s = 42938 (42938 target) ; Learning rate = 0.000279 ; Loss = 1.505674\n",
      "2024-12-08 15:20:37.850000: I runner.py:310] Step = 100200 ; steps/s = 1.62, tokens/s = 42830 (42830 target) ; Learning rate = 0.000279 ; Loss = 1.509227\n",
      "2024-12-08 15:21:39.233000: I runner.py:310] Step = 100300 ; steps/s = 1.63, tokens/s = 42271 (42271 target) ; Learning rate = 0.000279 ; Loss = 1.508580\n",
      "2024-12-08 15:22:41.048000: I runner.py:310] Step = 100400 ; steps/s = 1.62, tokens/s = 42793 (42793 target) ; Learning rate = 0.000279 ; Loss = 1.501701\n",
      "2024-12-08 15:23:42.943000: I runner.py:310] Step = 100500 ; steps/s = 1.62, tokens/s = 42681 (42681 target) ; Learning rate = 0.000279 ; Loss = 1.501970\n",
      "2024-12-08 15:24:44.768000: I runner.py:310] Step = 100600 ; steps/s = 1.62, tokens/s = 42794 (42794 target) ; Learning rate = 0.000279 ; Loss = 1.509348\n",
      "2024-12-08 15:25:46.081000: I runner.py:310] Step = 100700 ; steps/s = 1.63, tokens/s = 42347 (42347 target) ; Learning rate = 0.000279 ; Loss = 1.498879\n",
      "2024-12-08 15:26:47.907000: I runner.py:310] Step = 100800 ; steps/s = 1.62, tokens/s = 42814 (42814 target) ; Learning rate = 0.000278 ; Loss = 1.505080\n",
      "2024-12-08 15:27:49.683000: I runner.py:310] Step = 100900 ; steps/s = 1.62, tokens/s = 42771 (42771 target) ; Learning rate = 0.000278 ; Loss = 1.503376\n",
      "2024-12-08 15:28:51.452000: I runner.py:310] Step = 101000 ; steps/s = 1.62, tokens/s = 42826 (42826 target) ; Learning rate = 0.000278 ; Loss = 1.510254\n",
      "2024-12-08 15:29:52.721000: I runner.py:310] Step = 101100 ; steps/s = 1.63, tokens/s = 42350 (42350 target) ; Learning rate = 0.000278 ; Loss = 1.502255\n",
      "2024-12-08 15:30:54.497000: I runner.py:310] Step = 101200 ; steps/s = 1.62, tokens/s = 42829 (42829 target) ; Learning rate = 0.000278 ; Loss = 1.506209\n",
      "2024-12-08 15:31:56.291000: I runner.py:310] Step = 101300 ; steps/s = 1.62, tokens/s = 42785 (42785 target) ; Learning rate = 0.000278 ; Loss = 1.510658\n",
      "2024-12-08 15:32:58.076000: I runner.py:310] Step = 101400 ; steps/s = 1.62, tokens/s = 42800 (42800 target) ; Learning rate = 0.000278 ; Loss = 1.503971\n",
      "2024-12-08 15:33:59.369000: I runner.py:310] Step = 101500 ; steps/s = 1.63, tokens/s = 42351 (42351 target) ; Learning rate = 0.000277 ; Loss = 1.507818\n",
      "2024-12-08 15:35:01.166000: I runner.py:310] Step = 101600 ; steps/s = 1.62, tokens/s = 42784 (42784 target) ; Learning rate = 0.000277 ; Loss = 1.496847\n",
      "2024-12-08 15:36:02.922000: I runner.py:310] Step = 101700 ; steps/s = 1.62, tokens/s = 42827 (42827 target) ; Learning rate = 0.000277 ; Loss = 1.510423\n",
      "2024-12-08 15:37:04.718000: I runner.py:310] Step = 101800 ; steps/s = 1.62, tokens/s = 42802 (42802 target) ; Learning rate = 0.000277 ; Loss = 1.510668\n",
      "2024-12-08 15:38:06.046000: I runner.py:310] Step = 101900 ; steps/s = 1.63, tokens/s = 42324 (42324 target) ; Learning rate = 0.000277 ; Loss = 1.510604\n",
      "2024-12-08 15:39:07.826000: I runner.py:310] Step = 102000 ; steps/s = 1.62, tokens/s = 42793 (42793 target) ; Learning rate = 0.000277 ; Loss = 1.498952\n",
      "2024-12-08 15:40:09.607000: I runner.py:310] Step = 102100 ; steps/s = 1.62, tokens/s = 42829 (42829 target) ; Learning rate = 0.000277 ; Loss = 1.506852\n",
      "2024-12-08 15:41:11.402000: I runner.py:310] Step = 102200 ; steps/s = 1.62, tokens/s = 42765 (42765 target) ; Learning rate = 0.000276 ; Loss = 1.506355\n",
      "2024-12-08 15:42:12.785000: I runner.py:310] Step = 102300 ; steps/s = 1.63, tokens/s = 42281 (42281 target) ; Learning rate = 0.000276 ; Loss = 1.508157\n",
      "2024-12-08 15:43:14.528000: I runner.py:310] Step = 102400 ; steps/s = 1.62, tokens/s = 42852 (42852 target) ; Learning rate = 0.000276 ; Loss = 1.505660\n",
      "2024-12-08 15:44:16.326000: I runner.py:310] Step = 102500 ; steps/s = 1.62, tokens/s = 42819 (42819 target) ; Learning rate = 0.000276 ; Loss = 1.499324\n",
      "2024-12-08 15:45:18.185000: I runner.py:310] Step = 102600 ; steps/s = 1.62, tokens/s = 42728 (42728 target) ; Learning rate = 0.000276 ; Loss = 1.496992\n",
      "2024-12-08 15:46:19.525000: I runner.py:310] Step = 102700 ; steps/s = 1.63, tokens/s = 42321 (42321 target) ; Learning rate = 0.000276 ; Loss = 1.500885\n",
      "2024-12-08 15:47:21.314000: I runner.py:310] Step = 102800 ; steps/s = 1.62, tokens/s = 42841 (42841 target) ; Learning rate = 0.000276 ; Loss = 1.503348\n",
      "2024-12-08 15:48:23.118000: I runner.py:310] Step = 102900 ; steps/s = 1.62, tokens/s = 42794 (42794 target) ; Learning rate = 0.000276 ; Loss = 1.506900\n",
      "2024-12-08 15:49:24.966000: I runner.py:310] Step = 103000 ; steps/s = 1.62, tokens/s = 42713 (42713 target) ; Learning rate = 0.000275 ; Loss = 1.507964\n",
      "2024-12-08 15:50:26.284000: I runner.py:310] Step = 103100 ; steps/s = 1.63, tokens/s = 42336 (42336 target) ; Learning rate = 0.000275 ; Loss = 1.499023\n",
      "2024-12-08 15:51:28.044000: I runner.py:310] Step = 103200 ; steps/s = 1.62, tokens/s = 42815 (42815 target) ; Learning rate = 0.000275 ; Loss = 1.502846\n",
      "2024-12-08 15:52:29.828000: I runner.py:310] Step = 103300 ; steps/s = 1.62, tokens/s = 42812 (42812 target) ; Learning rate = 0.000275 ; Loss = 1.512169\n",
      "2024-12-08 15:53:31.573000: I runner.py:310] Step = 103400 ; steps/s = 1.62, tokens/s = 42819 (42819 target) ; Learning rate = 0.000275 ; Loss = 1.504335\n",
      "2024-12-08 15:54:32.895000: I runner.py:310] Step = 103500 ; steps/s = 1.63, tokens/s = 42356 (42356 target) ; Learning rate = 0.000275 ; Loss = 1.509402\n",
      "2024-12-08 15:55:34.729000: I runner.py:310] Step = 103600 ; steps/s = 1.62, tokens/s = 42759 (42759 target) ; Learning rate = 0.000275 ; Loss = 1.504975\n",
      "2024-12-08 15:56:36.484000: I runner.py:310] Step = 103700 ; steps/s = 1.62, tokens/s = 42796 (42796 target) ; Learning rate = 0.000274 ; Loss = 1.500611\n",
      "2024-12-08 15:57:38.240000: I runner.py:310] Step = 103800 ; steps/s = 1.62, tokens/s = 42837 (42837 target) ; Learning rate = 0.000274 ; Loss = 1.499451\n",
      "2024-12-08 15:58:39.515000: I runner.py:310] Step = 103900 ; steps/s = 1.63, tokens/s = 42387 (42387 target) ; Learning rate = 0.000274 ; Loss = 1.505166\n",
      "2024-12-08 15:59:41.279000: I runner.py:310] Step = 104000 ; steps/s = 1.62, tokens/s = 42826 (42826 target) ; Learning rate = 0.000274 ; Loss = 1.501797\n",
      "2024-12-08 16:00:43.063000: I runner.py:310] Step = 104100 ; steps/s = 1.62, tokens/s = 42792 (42792 target) ; Learning rate = 0.000274 ; Loss = 1.501042\n",
      "2024-12-08 16:01:44.820000: I runner.py:310] Step = 104200 ; steps/s = 1.62, tokens/s = 42795 (42795 target) ; Learning rate = 0.000274 ; Loss = 1.509541\n",
      "2024-12-08 16:02:46.198000: I runner.py:310] Step = 104300 ; steps/s = 1.63, tokens/s = 42307 (42307 target) ; Learning rate = 0.000274 ; Loss = 1.498902\n",
      "2024-12-08 16:03:48.043000: I runner.py:310] Step = 104400 ; steps/s = 1.62, tokens/s = 42735 (42735 target) ; Learning rate = 0.000274 ; Loss = 1.503248\n",
      "2024-12-08 16:04:49.862000: I runner.py:310] Step = 104500 ; steps/s = 1.62, tokens/s = 42785 (42785 target) ; Learning rate = 0.000273 ; Loss = 1.507803\n",
      "2024-12-08 16:05:51.301000: I runner.py:310] Step = 104600 ; steps/s = 1.63, tokens/s = 42343 (42343 target) ; Learning rate = 0.000273 ; Loss = 1.514937\n",
      "2024-12-08 16:06:53.063000: I runner.py:310] Step = 104700 ; steps/s = 1.62, tokens/s = 42722 (42722 target) ; Learning rate = 0.000273 ; Loss = 1.503413\n",
      "2024-12-08 16:07:54.927000: I runner.py:310] Step = 104800 ; steps/s = 1.62, tokens/s = 42764 (42764 target) ; Learning rate = 0.000273 ; Loss = 1.501799\n",
      "2024-12-08 16:08:56.697000: I runner.py:310] Step = 104900 ; steps/s = 1.62, tokens/s = 42813 (42813 target) ; Learning rate = 0.000273 ; Loss = 1.497979\n",
      "2024-12-08 16:09:58.118000: I runner.py:310] Step = 105000 ; steps/s = 1.63, tokens/s = 42232 (42232 target) ; Learning rate = 0.000273 ; Loss = 1.501330\n",
      "2024-12-08 16:09:58.119000: I training.py:192] Running evaluation for step 105000\n",
      "2024-12-08 16:14:39.727000: I training.py:192] Evaluation result for step 105000: loss = 1.187189 ; perplexity = 3.277853\n",
      "2024-12-08 16:15:41.386000: I runner.py:310] Step = 105100 ; steps/s = 1.62, tokens/s = 42900 (42900 target) ; Learning rate = 0.000273 ; Loss = 1.503049\n",
      "2024-12-08 16:16:43.162000: I runner.py:310] Step = 105200 ; steps/s = 1.62, tokens/s = 42841 (42841 target) ; Learning rate = 0.000273 ; Loss = 1.502392\n",
      "2024-12-08 16:17:45.054000: I runner.py:310] Step = 105300 ; steps/s = 1.62, tokens/s = 42714 (42714 target) ; Learning rate = 0.000272 ; Loss = 1.502684\n",
      "2024-12-08 16:18:46.371000: I runner.py:310] Step = 105400 ; steps/s = 1.63, tokens/s = 42327 (42327 target) ; Learning rate = 0.000272 ; Loss = 1.502845\n",
      "2024-12-08 16:19:48.159000: I runner.py:310] Step = 105500 ; steps/s = 1.62, tokens/s = 42810 (42810 target) ; Learning rate = 0.000272 ; Loss = 1.494327\n",
      "2024-12-08 16:20:49.904000: I runner.py:310] Step = 105600 ; steps/s = 1.62, tokens/s = 42825 (42825 target) ; Learning rate = 0.000272 ; Loss = 1.501396\n",
      "2024-12-08 16:21:51.753000: I runner.py:310] Step = 105700 ; steps/s = 1.62, tokens/s = 42737 (42737 target) ; Learning rate = 0.000272 ; Loss = 1.503141\n",
      "2024-12-08 16:22:53.146000: I runner.py:310] Step = 105800 ; steps/s = 1.63, tokens/s = 42296 (42296 target) ; Learning rate = 0.000272 ; Loss = 1.500274\n",
      "2024-12-08 16:23:54.909000: I runner.py:310] Step = 105900 ; steps/s = 1.62, tokens/s = 42825 (42825 target) ; Learning rate = 0.000272 ; Loss = 1.502616\n",
      "2024-12-08 16:24:56.711000: I runner.py:310] Step = 106000 ; steps/s = 1.62, tokens/s = 42791 (42791 target) ; Learning rate = 0.000271 ; Loss = 1.503794\n",
      "2024-12-08 16:25:58.509000: I runner.py:310] Step = 106100 ; steps/s = 1.62, tokens/s = 42791 (42791 target) ; Learning rate = 0.000271 ; Loss = 1.496966\n",
      "2024-12-08 16:26:59.826000: I runner.py:310] Step = 106200 ; steps/s = 1.63, tokens/s = 42325 (42325 target) ; Learning rate = 0.000271 ; Loss = 1.510126\n",
      "2024-12-08 16:28:01.538000: I runner.py:310] Step = 106300 ; steps/s = 1.62, tokens/s = 42866 (42866 target) ; Learning rate = 0.000271 ; Loss = 1.495005\n",
      "2024-12-08 16:29:03.307000: I runner.py:310] Step = 106400 ; steps/s = 1.62, tokens/s = 42831 (42831 target) ; Learning rate = 0.000271 ; Loss = 1.497023\n",
      "2024-12-08 16:30:05.094000: I runner.py:310] Step = 106500 ; steps/s = 1.62, tokens/s = 42803 (42803 target) ; Learning rate = 0.000271 ; Loss = 1.502953\n",
      "2024-12-08 16:31:06.469000: I runner.py:310] Step = 106600 ; steps/s = 1.63, tokens/s = 42255 (42255 target) ; Learning rate = 0.000271 ; Loss = 1.497865\n",
      "2024-12-08 16:32:08.284000: I runner.py:310] Step = 106700 ; steps/s = 1.62, tokens/s = 42791 (42791 target) ; Learning rate = 0.000271 ; Loss = 1.500390\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Kk-En (POS Tags) -> Tr-En (TED2020)(POS Tags)\n",
    "!onmt-main --model kk-tr-en-modelim.py --config kk-tr-en-pos-asl.yml --auto_config train --with_eval --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbacf489-b87a-42f5-b94a-e2c16a6bb6b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-08 16:33:07.713258: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-08 16:33:08.902164: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-08 16:33:08.902228: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-08 16:33:08.902236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-08 16:33:10.640000: I onmt-main:8] Creating model directory POS_KK_TR_EN_2\n",
      "2024-12-08 16:33:10.842000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-08 16:33:10.843000: I main.py:315] Using model:\n",
      "(model): MultiFeaturesTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): ParallelInputter(\n",
      "      (inputters): ListWrapper(\n",
      "        (0): WordEmbedder()\n",
      "        (1): WordEmbedder()\n",
      "      )\n",
      "      (reducer): ConcatReducer()\n",
      "    )\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): ParallelInputter(\n",
      "        (inputters): ListWrapper(\n",
      "          (0): WordEmbedder()\n",
      "          (1): WordEmbedder()\n",
      "        )\n",
      "        (reducer): ConcatReducer()\n",
      "      )\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-08 16:33:10.846142: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-08 16:33:13.686195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-08 16:33:13.686896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7742 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-12-08 16:33:13.687279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 674 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "2024-12-08 16:33:13.691000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file:\n",
      "  - TED2020_tokens_dev_shared\n",
      "  - TED2020_pos_tags_dev_shared.txt\n",
      "  eval_labels_file: TED2020_dev_target_tokens_shared.txt\n",
      "  source_1_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  source_2_vocabulary: Zemberek_TR_unique_pos\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file:\n",
      "  - TED2020_tokens_train_shared\n",
      "  - TED2020_pos_tags_train_shared.txt\n",
      "  train_labels_file: TED2020_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: POS_KK_TR_EN_2\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 205000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-08 16:33:14.021000: I inputter.py:316] Initialized source_1 input layer:\n",
      "2024-12-08 16:33:14.021000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-08 16:33:14.021000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-08 16:33:14.025000: I inputter.py:316] Initialized source_2 input layer:\n",
      "2024-12-08 16:33:14.025000: I inputter.py:316]  - vocabulary size: 21\n",
      "2024-12-08 16:33:14.025000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-08 16:33:14.097000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-08 16:33:14.097000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-08 16:33:14.097000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-08 16:33:14.126000: I runner.py:269] Restored checkpoint POS_KK_TR_EN/ckpt-100000\n",
      "2024-12-08 16:33:14.128000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "2024-12-08 16:33:14.184000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-08 16:33:15.320461: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-08 16:33:15.544000: I main.py:325] Accumulate gradients of 7 iterations to reach effective batch size of 25000\n",
      "2024-12-08 16:33:15.668000: I mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2024-12-08 16:33:16.038000: I dataset_ops.py:2542] Training on 337547 examples\n",
      "2024-12-08 16:34:25.059603: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-08 16:34:26.154658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-08 16:34:26.506927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-08 16:34:36.004000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-08 16:34:36.021000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-08 16:34:37.629000: I cross_device_ops.py:897] batch_all_reduce: 261 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-08 16:34:42.717000: I cross_device_ops.py:897] batch_all_reduce: 261 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-08 16:34:49.699000: I runner.py:310] Number of model parameters: 93357261\n",
      "2024-12-08 16:34:49.704000: I runner.py:310] Number of model weights: 261 (trainable = 261, non trainable = 0)\n",
      "2024-12-08 16:34:49.741000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-08 16:34:51.816000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-1\n",
      "2024-12-08 16:34:52.462000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-08 16:34:52.486000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-08 16:34:53.099000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-08 16:34:53.122000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-08 16:34:53.720000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-08 16:34:53.742000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-08 16:34:54.345000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-08 16:35:52.921000: I runner.py:310] Step = 100 ; steps/s = 1.63, tokens/s = 44366 (44366 target) ; Learning rate = 0.000009 ; Loss = 7.931133\n",
      "2024-12-08 16:36:54.563000: I runner.py:310] Step = 200 ; steps/s = 1.62, tokens/s = 44260 (44260 target) ; Learning rate = 0.000018 ; Loss = 6.588465\n",
      "2024-12-08 16:37:56.232000: I runner.py:310] Step = 300 ; steps/s = 1.62, tokens/s = 44256 (44256 target) ; Learning rate = 0.000027 ; Loss = 6.310205\n",
      "2024-12-08 16:38:58.567000: I runner.py:310] Step = 400 ; steps/s = 1.60, tokens/s = 43083 (43083 target) ; Learning rate = 0.000035 ; Loss = 5.918151\n",
      "2024-12-08 16:40:00.250000: I runner.py:310] Step = 500 ; steps/s = 1.62, tokens/s = 44254 (44254 target) ; Learning rate = 0.000044 ; Loss = 5.829993\n",
      "2024-12-08 16:41:01.882000: I runner.py:310] Step = 600 ; steps/s = 1.62, tokens/s = 44272 (44272 target) ; Learning rate = 0.000053 ; Loss = 5.782826\n",
      "2024-12-08 16:42:03.830000: I runner.py:310] Step = 700 ; steps/s = 1.61, tokens/s = 43321 (43321 target) ; Learning rate = 0.000062 ; Loss = 5.572983\n",
      "2024-12-08 16:43:05.534000: I runner.py:310] Step = 800 ; steps/s = 1.62, tokens/s = 44221 (44221 target) ; Learning rate = 0.000071 ; Loss = 5.329712\n",
      "2024-12-08 16:44:07.193000: I runner.py:310] Step = 900 ; steps/s = 1.62, tokens/s = 44267 (44267 target) ; Learning rate = 0.000080 ; Loss = 5.069835\n",
      "2024-12-08 16:45:08.849000: I runner.py:310] Step = 1000 ; steps/s = 1.62, tokens/s = 44268 (44268 target) ; Learning rate = 0.000088 ; Loss = 4.835525\n",
      "2024-12-08 16:46:10.116000: I runner.py:310] Step = 1100 ; steps/s = 1.63, tokens/s = 43813 (43813 target) ; Learning rate = 0.000097 ; Loss = 4.670836\n",
      "2024-12-08 16:47:11.792000: I runner.py:310] Step = 1200 ; steps/s = 1.62, tokens/s = 44252 (44252 target) ; Learning rate = 0.000106 ; Loss = 4.367313\n",
      "2024-12-08 16:48:13.428000: I runner.py:310] Step = 1300 ; steps/s = 1.62, tokens/s = 44274 (44274 target) ; Learning rate = 0.000115 ; Loss = 4.181284\n",
      "2024-12-08 16:49:14.700000: I runner.py:310] Step = 1400 ; steps/s = 1.63, tokens/s = 43805 (43805 target) ; Learning rate = 0.000124 ; Loss = 3.997266\n",
      "2024-12-08 16:50:16.379000: I runner.py:310] Step = 1500 ; steps/s = 1.62, tokens/s = 44252 (44252 target) ; Learning rate = 0.000133 ; Loss = 3.962934\n",
      "2024-12-08 16:51:17.939000: I runner.py:310] Step = 1600 ; steps/s = 1.62, tokens/s = 44350 (44350 target) ; Learning rate = 0.000142 ; Loss = 3.985122\n",
      "2024-12-08 16:52:19.515000: I runner.py:310] Step = 1700 ; steps/s = 1.62, tokens/s = 44310 (44310 target) ; Learning rate = 0.000150 ; Loss = 3.878867\n",
      "2024-12-08 16:53:20.712000: I runner.py:310] Step = 1800 ; steps/s = 1.63, tokens/s = 43863 (43863 target) ; Learning rate = 0.000159 ; Loss = 3.791052\n",
      "2024-12-08 16:54:22.373000: I runner.py:310] Step = 1900 ; steps/s = 1.62, tokens/s = 44257 (44257 target) ; Learning rate = 0.000168 ; Loss = 3.744422\n",
      "2024-12-08 16:55:24.072000: I runner.py:310] Step = 2000 ; steps/s = 1.62, tokens/s = 44235 (44235 target) ; Learning rate = 0.000177 ; Loss = 3.590332\n",
      "2024-12-08 16:56:25.274000: I runner.py:310] Step = 2100 ; steps/s = 1.63, tokens/s = 43857 (43857 target) ; Learning rate = 0.000186 ; Loss = 3.583790\n",
      "2024-12-08 16:57:26.968000: I runner.py:310] Step = 2200 ; steps/s = 1.62, tokens/s = 44230 (44230 target) ; Learning rate = 0.000195 ; Loss = 3.769753\n",
      "2024-12-08 16:58:28.625000: I runner.py:310] Step = 2300 ; steps/s = 1.62, tokens/s = 44257 (44257 target) ; Learning rate = 0.000203 ; Loss = 3.570308\n",
      "2024-12-08 16:59:30.338000: I runner.py:310] Step = 2400 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000212 ; Loss = 3.497929\n",
      "2024-12-08 17:00:31.616000: I runner.py:310] Step = 2500 ; steps/s = 1.63, tokens/s = 43814 (43814 target) ; Learning rate = 0.000221 ; Loss = 3.485779\n",
      "2024-12-08 17:01:33.281000: I runner.py:310] Step = 2600 ; steps/s = 1.62, tokens/s = 44263 (44263 target) ; Learning rate = 0.000230 ; Loss = 3.499475\n",
      "2024-12-08 17:02:34.912000: I runner.py:310] Step = 2700 ; steps/s = 1.62, tokens/s = 44278 (44278 target) ; Learning rate = 0.000239 ; Loss = 3.479013\n",
      "2024-12-08 17:03:36.154000: I runner.py:310] Step = 2800 ; steps/s = 1.63, tokens/s = 43823 (43823 target) ; Learning rate = 0.000248 ; Loss = 3.370999\n",
      "2024-12-08 17:04:37.808000: I runner.py:310] Step = 2900 ; steps/s = 1.62, tokens/s = 44258 (44258 target) ; Learning rate = 0.000256 ; Loss = 3.335224\n",
      "2024-12-08 17:05:39.744000: I runner.py:310] Step = 3000 ; steps/s = 1.61, tokens/s = 44063 (44063 target) ; Learning rate = 0.000265 ; Loss = 3.262786\n",
      "2024-12-08 17:06:41.350000: I runner.py:310] Step = 3100 ; steps/s = 1.62, tokens/s = 44302 (44302 target) ; Learning rate = 0.000274 ; Loss = 3.341674\n",
      "2024-12-08 17:07:42.622000: I runner.py:310] Step = 3200 ; steps/s = 1.63, tokens/s = 43811 (43811 target) ; Learning rate = 0.000283 ; Loss = 3.331091\n",
      "2024-12-08 17:08:44.276000: I runner.py:310] Step = 3300 ; steps/s = 1.62, tokens/s = 44251 (44251 target) ; Learning rate = 0.000292 ; Loss = 3.255584\n",
      "2024-12-08 17:09:45.907000: I runner.py:310] Step = 3400 ; steps/s = 1.62, tokens/s = 44296 (44296 target) ; Learning rate = 0.000301 ; Loss = 3.252257\n",
      "2024-12-08 17:10:47.204000: I runner.py:310] Step = 3500 ; steps/s = 1.63, tokens/s = 43797 (43797 target) ; Learning rate = 0.000309 ; Loss = 3.234787\n",
      "2024-12-08 17:11:48.835000: I runner.py:310] Step = 3600 ; steps/s = 1.62, tokens/s = 44284 (44284 target) ; Learning rate = 0.000318 ; Loss = 3.256559\n",
      "2024-12-08 17:12:50.479000: I runner.py:310] Step = 3700 ; steps/s = 1.62, tokens/s = 44274 (44274 target) ; Learning rate = 0.000327 ; Loss = 3.191599\n",
      "2024-12-08 17:13:52.067000: I runner.py:310] Step = 3800 ; steps/s = 1.62, tokens/s = 44301 (44301 target) ; Learning rate = 0.000336 ; Loss = 3.112041\n",
      "2024-12-08 17:14:53.426000: I runner.py:310] Step = 3900 ; steps/s = 1.63, tokens/s = 43762 (43762 target) ; Learning rate = 0.000345 ; Loss = 3.225368\n",
      "2024-12-08 17:15:55.053000: I runner.py:310] Step = 4000 ; steps/s = 1.62, tokens/s = 44280 (44280 target) ; Learning rate = 0.000354 ; Loss = 3.071753\n",
      "2024-12-08 17:16:56.697000: I runner.py:310] Step = 4100 ; steps/s = 1.62, tokens/s = 44258 (44258 target) ; Learning rate = 0.000362 ; Loss = 3.116117\n",
      "2024-12-08 17:17:57.884000: I runner.py:310] Step = 4200 ; steps/s = 1.63, tokens/s = 43877 (43877 target) ; Learning rate = 0.000371 ; Loss = 3.002526\n",
      "2024-12-08 17:18:59.535000: I runner.py:310] Step = 4300 ; steps/s = 1.62, tokens/s = 44270 (44270 target) ; Learning rate = 0.000380 ; Loss = 3.124845\n",
      "2024-12-08 17:20:01.268000: I runner.py:310] Step = 4400 ; steps/s = 1.62, tokens/s = 44208 (44208 target) ; Learning rate = 0.000389 ; Loss = 3.038398\n",
      "2024-12-08 17:21:02.911000: I runner.py:310] Step = 4500 ; steps/s = 1.62, tokens/s = 44277 (44277 target) ; Learning rate = 0.000398 ; Loss = 2.997541\n",
      "2024-12-08 17:22:04.109000: I runner.py:310] Step = 4600 ; steps/s = 1.63, tokens/s = 43855 (43855 target) ; Learning rate = 0.000407 ; Loss = 2.985283\n",
      "2024-12-08 17:23:05.705000: I runner.py:310] Step = 4700 ; steps/s = 1.62, tokens/s = 44307 (44307 target) ; Learning rate = 0.000416 ; Loss = 2.968205\n",
      "2024-12-08 17:24:07.346000: I runner.py:310] Step = 4800 ; steps/s = 1.62, tokens/s = 44272 (44272 target) ; Learning rate = 0.000424 ; Loss = 3.018484\n",
      "2024-12-08 17:25:08.553000: I runner.py:310] Step = 4900 ; steps/s = 1.63, tokens/s = 43850 (43850 target) ; Learning rate = 0.000433 ; Loss = 2.916334\n",
      "2024-12-08 17:26:10.217000: I runner.py:310] Step = 5000 ; steps/s = 1.62, tokens/s = 44274 (44274 target) ; Learning rate = 0.000442 ; Loss = 2.959425\n",
      "2024-12-08 17:26:10.218000: I training.py:192] Running evaluation for step 5000\n",
      "2024-12-08 17:31:59.198000: I training.py:192] Evaluation result for step 5000: loss = 1.991620 ; perplexity = 7.327392\n",
      "2024-12-08 17:33:00.886000: I runner.py:310] Step = 5100 ; steps/s = 1.62, tokens/s = 44243 (44243 target) ; Learning rate = 0.000451 ; Loss = 2.910529\n",
      "2024-12-08 17:34:02.577000: I runner.py:310] Step = 5200 ; steps/s = 1.62, tokens/s = 44233 (44233 target) ; Learning rate = 0.000460 ; Loss = 2.876925\n",
      "2024-12-08 17:35:03.943000: I runner.py:310] Step = 5300 ; steps/s = 1.63, tokens/s = 43750 (43750 target) ; Learning rate = 0.000469 ; Loss = 2.834447\n",
      "2024-12-08 17:36:05.700000: I runner.py:310] Step = 5400 ; steps/s = 1.62, tokens/s = 44189 (44189 target) ; Learning rate = 0.000477 ; Loss = 2.900731\n",
      "2024-12-08 17:37:07.483000: I runner.py:310] Step = 5500 ; steps/s = 1.62, tokens/s = 44173 (44173 target) ; Learning rate = 0.000486 ; Loss = 2.894689\n",
      "2024-12-08 17:38:08.866000: I runner.py:310] Step = 5600 ; steps/s = 1.63, tokens/s = 43730 (43730 target) ; Learning rate = 0.000495 ; Loss = 2.906530\n",
      "2024-12-08 17:39:10.601000: I runner.py:310] Step = 5700 ; steps/s = 1.62, tokens/s = 44219 (44219 target) ; Learning rate = 0.000504 ; Loss = 2.799345\n",
      "2024-12-08 17:40:12.282000: I runner.py:310] Step = 5800 ; steps/s = 1.62, tokens/s = 44248 (44248 target) ; Learning rate = 0.000513 ; Loss = 2.893468\n",
      "2024-12-08 17:41:14.078000: I runner.py:310] Step = 5900 ; steps/s = 1.62, tokens/s = 44151 (44151 target) ; Learning rate = 0.000522 ; Loss = 2.864216\n",
      "2024-12-08 17:42:15.519000: I runner.py:310] Step = 6000 ; steps/s = 1.63, tokens/s = 43691 (43691 target) ; Learning rate = 0.000530 ; Loss = 2.815891\n",
      "2024-12-08 17:43:17.301000: I runner.py:310] Step = 6100 ; steps/s = 1.62, tokens/s = 44171 (44171 target) ; Learning rate = 0.000539 ; Loss = 2.789758\n",
      "2024-12-08 17:44:19.031000: I runner.py:310] Step = 6200 ; steps/s = 1.62, tokens/s = 44211 (44211 target) ; Learning rate = 0.000548 ; Loss = 2.739146\n",
      "2024-12-08 17:45:20.458000: I runner.py:310] Step = 6300 ; steps/s = 1.63, tokens/s = 43684 (43684 target) ; Learning rate = 0.000557 ; Loss = 2.751381\n",
      "2024-12-08 17:46:22.200000: I runner.py:310] Step = 6400 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000566 ; Loss = 2.723732\n",
      "2024-12-08 17:47:23.915000: I runner.py:310] Step = 6500 ; steps/s = 1.62, tokens/s = 44231 (44231 target) ; Learning rate = 0.000575 ; Loss = 2.812094\n",
      "2024-12-08 17:48:25.659000: I runner.py:310] Step = 6600 ; steps/s = 1.62, tokens/s = 44190 (44190 target) ; Learning rate = 0.000583 ; Loss = 2.795123\n",
      "2024-12-08 17:49:26.985000: I runner.py:310] Step = 6700 ; steps/s = 1.63, tokens/s = 43756 (43756 target) ; Learning rate = 0.000592 ; Loss = 2.766193\n",
      "2024-12-08 17:50:28.703000: I runner.py:310] Step = 6800 ; steps/s = 1.62, tokens/s = 44223 (44223 target) ; Learning rate = 0.000601 ; Loss = 2.697294\n",
      "2024-12-08 17:51:30.379000: I runner.py:310] Step = 6900 ; steps/s = 1.62, tokens/s = 44257 (44257 target) ; Learning rate = 0.000610 ; Loss = 2.728101\n",
      "2024-12-08 17:52:31.706000: I runner.py:310] Step = 7000 ; steps/s = 1.63, tokens/s = 43770 (43770 target) ; Learning rate = 0.000619 ; Loss = 2.726891\n",
      "2024-12-08 17:53:33.384000: I runner.py:310] Step = 7100 ; steps/s = 1.62, tokens/s = 44251 (44251 target) ; Learning rate = 0.000628 ; Loss = 2.636166\n",
      "2024-12-08 17:54:35.032000: I runner.py:310] Step = 7200 ; steps/s = 1.62, tokens/s = 44275 (44275 target) ; Learning rate = 0.000636 ; Loss = 2.690891\n",
      "2024-12-08 17:55:36.645000: I runner.py:310] Step = 7300 ; steps/s = 1.62, tokens/s = 44285 (44285 target) ; Learning rate = 0.000645 ; Loss = 2.701367\n",
      "2024-12-08 17:56:37.941000: I runner.py:310] Step = 7400 ; steps/s = 1.63, tokens/s = 43785 (43785 target) ; Learning rate = 0.000654 ; Loss = 2.672398\n",
      "2024-12-08 17:57:39.575000: I runner.py:310] Step = 7500 ; steps/s = 1.62, tokens/s = 44285 (44285 target) ; Learning rate = 0.000663 ; Loss = 2.637544\n",
      "2024-12-08 17:58:41.194000: I runner.py:310] Step = 7600 ; steps/s = 1.62, tokens/s = 44296 (44296 target) ; Learning rate = 0.000672 ; Loss = 2.739751\n",
      "2024-12-08 17:59:42.393000: I runner.py:310] Step = 7700 ; steps/s = 1.63, tokens/s = 43863 (43863 target) ; Learning rate = 0.000681 ; Loss = 2.667365\n",
      "2024-12-08 18:00:44.081000: I runner.py:310] Step = 7800 ; steps/s = 1.62, tokens/s = 44239 (44239 target) ; Learning rate = 0.000690 ; Loss = 2.604717\n",
      "2024-12-08 18:01:45.751000: I runner.py:310] Step = 7900 ; steps/s = 1.62, tokens/s = 44264 (44264 target) ; Learning rate = 0.000698 ; Loss = 2.693033\n",
      "2024-12-08 18:02:47.236000: I runner.py:310] Step = 8000 ; steps/s = 1.63, tokens/s = 44132 (44132 target) ; Learning rate = 0.000707 ; Loss = 2.548807\n",
      "2024-12-08 18:03:48.655000: I runner.py:310] Step = 8100 ; steps/s = 1.63, tokens/s = 43941 (43941 target) ; Learning rate = 0.000716 ; Loss = 2.637168\n",
      "2024-12-08 18:04:50.285000: I runner.py:310] Step = 8200 ; steps/s = 1.62, tokens/s = 44312 (44312 target) ; Learning rate = 0.000725 ; Loss = 2.588446\n",
      "2024-12-08 18:05:51.896000: I runner.py:310] Step = 8300 ; steps/s = 1.62, tokens/s = 44277 (44277 target) ; Learning rate = 0.000734 ; Loss = 2.582547\n",
      "2024-12-08 18:06:53.067000: I runner.py:310] Step = 8400 ; steps/s = 1.63, tokens/s = 43868 (43868 target) ; Learning rate = 0.000743 ; Loss = 2.530615\n",
      "2024-12-08 18:07:54.703000: I runner.py:310] Step = 8500 ; steps/s = 1.62, tokens/s = 44274 (44274 target) ; Learning rate = 0.000751 ; Loss = 2.524193\n",
      "2024-12-08 18:08:56.417000: I runner.py:310] Step = 8600 ; steps/s = 1.62, tokens/s = 44229 (44229 target) ; Learning rate = 0.000760 ; Loss = 2.578199\n",
      "2024-12-08 18:09:57.616000: I runner.py:310] Step = 8700 ; steps/s = 1.63, tokens/s = 43863 (43863 target) ; Learning rate = 0.000769 ; Loss = 2.550836\n",
      "2024-12-08 18:10:59.271000: I runner.py:310] Step = 8800 ; steps/s = 1.62, tokens/s = 44260 (44260 target) ; Learning rate = 0.000778 ; Loss = 2.535655\n",
      "2024-12-08 18:12:00.916000: I runner.py:310] Step = 8900 ; steps/s = 1.62, tokens/s = 44287 (44287 target) ; Learning rate = 0.000787 ; Loss = 2.580191\n",
      "2024-12-08 18:13:02.545000: I runner.py:310] Step = 9000 ; steps/s = 1.62, tokens/s = 44281 (44281 target) ; Learning rate = 0.000796 ; Loss = 2.580425\n",
      "2024-12-08 18:14:03.800000: I runner.py:310] Step = 9100 ; steps/s = 1.63, tokens/s = 43808 (43808 target) ; Learning rate = 0.000804 ; Loss = 2.531827\n",
      "2024-12-08 18:15:05.458000: I runner.py:310] Step = 9200 ; steps/s = 1.62, tokens/s = 44261 (44261 target) ; Learning rate = 0.000813 ; Loss = 2.535139\n",
      "2024-12-08 18:16:07.117000: I runner.py:310] Step = 9300 ; steps/s = 1.62, tokens/s = 44273 (44273 target) ; Learning rate = 0.000822 ; Loss = 2.570264\n",
      "2024-12-08 18:17:08.355000: I runner.py:310] Step = 9400 ; steps/s = 1.63, tokens/s = 43828 (43828 target) ; Learning rate = 0.000831 ; Loss = 2.523160\n",
      "2024-12-08 18:18:10.006000: I runner.py:310] Step = 9500 ; steps/s = 1.62, tokens/s = 44279 (44279 target) ; Learning rate = 0.000840 ; Loss = 2.450677\n",
      "2024-12-08 18:19:11.642000: I runner.py:310] Step = 9600 ; steps/s = 1.62, tokens/s = 44274 (44274 target) ; Learning rate = 0.000849 ; Loss = 2.501325\n",
      "2024-12-08 18:20:13.333000: I runner.py:310] Step = 9700 ; steps/s = 1.62, tokens/s = 44230 (44230 target) ; Learning rate = 0.000857 ; Loss = 2.538439\n",
      "2024-12-08 18:21:14.576000: I runner.py:310] Step = 9800 ; steps/s = 1.63, tokens/s = 43840 (43840 target) ; Learning rate = 0.000866 ; Loss = 2.453962\n",
      "2024-12-08 18:22:16.236000: I runner.py:310] Step = 9900 ; steps/s = 1.62, tokens/s = 44259 (44259 target) ; Learning rate = 0.000875 ; Loss = 2.503383\n",
      "2024-12-08 18:23:17.998000: I runner.py:310] Step = 10000 ; steps/s = 1.62, tokens/s = 44184 (44184 target) ; Learning rate = 0.000884 ; Loss = 2.567169\n",
      "2024-12-08 18:23:19.895000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-10000\n",
      "2024-12-08 18:23:19.895000: I training.py:192] Running evaluation for step 10000\n",
      "2024-12-08 18:28:26.430000: I training.py:192] Evaluation result for step 10000: loss = 1.942351 ; perplexity = 6.975129\n",
      "2024-12-08 18:29:27.571000: I runner.py:310] Step = 10100 ; steps/s = 1.64, tokens/s = 43910 (43910 target) ; Learning rate = 0.000879 ; Loss = 2.431238\n",
      "2024-12-08 18:30:29.194000: I runner.py:310] Step = 10200 ; steps/s = 1.62, tokens/s = 44281 (44281 target) ; Learning rate = 0.000875 ; Loss = 2.451090\n",
      "2024-12-08 18:31:30.917000: I runner.py:310] Step = 10300 ; steps/s = 1.62, tokens/s = 44223 (44223 target) ; Learning rate = 0.000871 ; Loss = 2.479247\n",
      "2024-12-08 18:32:32.591000: I runner.py:310] Step = 10400 ; steps/s = 1.62, tokens/s = 44249 (44249 target) ; Learning rate = 0.000867 ; Loss = 2.418725\n",
      "2024-12-08 18:33:33.863000: I runner.py:310] Step = 10500 ; steps/s = 1.63, tokens/s = 43818 (43818 target) ; Learning rate = 0.000863 ; Loss = 2.409445\n",
      "2024-12-08 18:34:35.563000: I runner.py:310] Step = 10600 ; steps/s = 1.62, tokens/s = 44237 (44237 target) ; Learning rate = 0.000858 ; Loss = 2.485542\n",
      "2024-12-08 18:35:37.223000: I runner.py:310] Step = 10700 ; steps/s = 1.62, tokens/s = 44254 (44254 target) ; Learning rate = 0.000854 ; Loss = 2.496083\n",
      "2024-12-08 18:36:38.456000: I runner.py:310] Step = 10800 ; steps/s = 1.63, tokens/s = 43833 (43833 target) ; Learning rate = 0.000850 ; Loss = 2.429611\n",
      "2024-12-08 18:37:40.080000: I runner.py:310] Step = 10900 ; steps/s = 1.62, tokens/s = 44297 (44297 target) ; Learning rate = 0.000847 ; Loss = 2.424521\n",
      "2024-12-08 18:38:41.784000: I runner.py:310] Step = 11000 ; steps/s = 1.62, tokens/s = 44224 (44224 target) ; Learning rate = 0.000843 ; Loss = 2.441503\n",
      "2024-12-08 18:39:43.457000: I runner.py:310] Step = 11100 ; steps/s = 1.62, tokens/s = 44253 (44253 target) ; Learning rate = 0.000839 ; Loss = 2.498138\n",
      "2024-12-08 18:40:44.731000: I runner.py:310] Step = 11200 ; steps/s = 1.63, tokens/s = 43802 (43802 target) ; Learning rate = 0.000835 ; Loss = 2.419670\n",
      "2024-12-08 18:41:46.357000: I runner.py:310] Step = 11300 ; steps/s = 1.62, tokens/s = 44287 (44287 target) ; Learning rate = 0.000831 ; Loss = 2.383937\n",
      "2024-12-08 18:42:48.067000: I runner.py:310] Step = 11400 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000828 ; Loss = 2.385881\n",
      "2024-12-08 18:43:49.349000: I runner.py:310] Step = 11500 ; steps/s = 1.63, tokens/s = 43790 (43790 target) ; Learning rate = 0.000824 ; Loss = 2.422498\n",
      "2024-12-08 18:44:51.012000: I runner.py:310] Step = 11600 ; steps/s = 1.62, tokens/s = 44251 (44251 target) ; Learning rate = 0.000821 ; Loss = 2.342944\n",
      "2024-12-08 18:45:52.663000: I runner.py:310] Step = 11700 ; steps/s = 1.62, tokens/s = 44274 (44274 target) ; Learning rate = 0.000817 ; Loss = 2.406530\n",
      "2024-12-08 18:46:54.340000: I runner.py:310] Step = 11800 ; steps/s = 1.62, tokens/s = 44242 (44242 target) ; Learning rate = 0.000814 ; Loss = 2.450023\n",
      "2024-12-08 18:47:55.619000: I runner.py:310] Step = 11900 ; steps/s = 1.63, tokens/s = 43820 (43820 target) ; Learning rate = 0.000810 ; Loss = 2.320215\n",
      "2024-12-08 18:48:57.291000: I runner.py:310] Step = 12000 ; steps/s = 1.62, tokens/s = 44249 (44249 target) ; Learning rate = 0.000807 ; Loss = 2.346582\n",
      "2024-12-08 18:49:58.970000: I runner.py:310] Step = 12100 ; steps/s = 1.62, tokens/s = 44247 (44247 target) ; Learning rate = 0.000803 ; Loss = 2.422298\n",
      "2024-12-08 18:51:00.330000: I runner.py:310] Step = 12200 ; steps/s = 1.63, tokens/s = 43738 (43738 target) ; Learning rate = 0.000800 ; Loss = 2.377561\n",
      "2024-12-08 18:52:01.935000: I runner.py:310] Step = 12300 ; steps/s = 1.62, tokens/s = 44308 (44308 target) ; Learning rate = 0.000797 ; Loss = 2.365262\n",
      "2024-12-08 18:53:03.665000: I runner.py:310] Step = 12400 ; steps/s = 1.62, tokens/s = 44211 (44211 target) ; Learning rate = 0.000794 ; Loss = 2.389546\n",
      "2024-12-08 18:54:05.270000: I runner.py:310] Step = 12500 ; steps/s = 1.62, tokens/s = 44302 (44302 target) ; Learning rate = 0.000791 ; Loss = 2.407945\n",
      "2024-12-08 18:55:06.553000: I runner.py:310] Step = 12600 ; steps/s = 1.63, tokens/s = 43797 (43797 target) ; Learning rate = 0.000787 ; Loss = 2.291727\n",
      "2024-12-08 18:56:08.175000: I runner.py:310] Step = 12700 ; steps/s = 1.62, tokens/s = 44273 (44273 target) ; Learning rate = 0.000784 ; Loss = 2.359061\n",
      "2024-12-08 18:57:09.802000: I runner.py:310] Step = 12800 ; steps/s = 1.62, tokens/s = 44297 (44297 target) ; Learning rate = 0.000781 ; Loss = 2.354208\n",
      "2024-12-08 18:58:11.173000: I runner.py:310] Step = 12900 ; steps/s = 1.63, tokens/s = 43732 (43732 target) ; Learning rate = 0.000778 ; Loss = 2.337543\n",
      "2024-12-08 18:59:12.802000: I runner.py:310] Step = 13000 ; steps/s = 1.62, tokens/s = 44283 (44283 target) ; Learning rate = 0.000775 ; Loss = 2.284421\n",
      "2024-12-08 19:00:14.424000: I runner.py:310] Step = 13100 ; steps/s = 1.62, tokens/s = 44299 (44299 target) ; Learning rate = 0.000772 ; Loss = 2.368252\n",
      "2024-12-08 19:01:16.123000: I runner.py:310] Step = 13200 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000769 ; Loss = 2.308566\n",
      "2024-12-08 19:02:17.431000: I runner.py:310] Step = 13300 ; steps/s = 1.63, tokens/s = 43788 (43788 target) ; Learning rate = 0.000766 ; Loss = 2.261213\n",
      "2024-12-08 19:03:19.165000: I runner.py:310] Step = 13400 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000764 ; Loss = 2.320210\n",
      "2024-12-08 19:04:20.884000: I runner.py:310] Step = 13500 ; steps/s = 1.62, tokens/s = 44221 (44221 target) ; Learning rate = 0.000761 ; Loss = 2.358880\n",
      "2024-12-08 19:05:22.128000: I runner.py:310] Step = 13600 ; steps/s = 1.63, tokens/s = 43823 (43823 target) ; Learning rate = 0.000758 ; Loss = 2.259172\n",
      "2024-12-08 19:06:23.826000: I runner.py:310] Step = 13700 ; steps/s = 1.62, tokens/s = 44235 (44235 target) ; Learning rate = 0.000755 ; Loss = 2.316404\n",
      "2024-12-08 19:07:25.533000: I runner.py:310] Step = 13800 ; steps/s = 1.62, tokens/s = 44224 (44224 target) ; Learning rate = 0.000752 ; Loss = 2.317874\n",
      "2024-12-08 19:08:27.195000: I runner.py:310] Step = 13900 ; steps/s = 1.62, tokens/s = 44270 (44270 target) ; Learning rate = 0.000750 ; Loss = 2.333558\n",
      "2024-12-08 19:09:28.519000: I runner.py:310] Step = 14000 ; steps/s = 1.63, tokens/s = 43763 (43763 target) ; Learning rate = 0.000747 ; Loss = 2.289569\n",
      "2024-12-08 19:10:30.172000: I runner.py:310] Step = 14100 ; steps/s = 1.62, tokens/s = 44277 (44277 target) ; Learning rate = 0.000744 ; Loss = 2.286638\n",
      "2024-12-08 19:11:31.869000: I runner.py:310] Step = 14200 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000742 ; Loss = 2.291148\n",
      "2024-12-08 19:12:33.136000: I runner.py:310] Step = 14300 ; steps/s = 1.63, tokens/s = 43825 (43825 target) ; Learning rate = 0.000739 ; Loss = 2.245690\n",
      "2024-12-08 19:13:34.780000: I runner.py:310] Step = 14400 ; steps/s = 1.62, tokens/s = 44274 (44274 target) ; Learning rate = 0.000737 ; Loss = 2.271151\n",
      "2024-12-08 19:14:36.492000: I runner.py:310] Step = 14500 ; steps/s = 1.62, tokens/s = 44223 (44223 target) ; Learning rate = 0.000734 ; Loss = 2.288660\n",
      "2024-12-08 19:15:38.154000: I runner.py:310] Step = 14600 ; steps/s = 1.62, tokens/s = 44252 (44252 target) ; Learning rate = 0.000731 ; Loss = 2.320975\n",
      "2024-12-08 19:16:39.486000: I runner.py:310] Step = 14700 ; steps/s = 1.63, tokens/s = 43769 (43769 target) ; Learning rate = 0.000729 ; Loss = 2.310807\n",
      "2024-12-08 19:17:41.136000: I runner.py:310] Step = 14800 ; steps/s = 1.62, tokens/s = 44265 (44265 target) ; Learning rate = 0.000727 ; Loss = 2.256166\n",
      "2024-12-08 19:18:42.756000: I runner.py:310] Step = 14900 ; steps/s = 1.62, tokens/s = 44282 (44282 target) ; Learning rate = 0.000724 ; Loss = 2.262231\n",
      "2024-12-08 19:19:44.004000: I runner.py:310] Step = 15000 ; steps/s = 1.63, tokens/s = 43828 (43828 target) ; Learning rate = 0.000722 ; Loss = 2.286296\n",
      "2024-12-08 19:19:44.005000: I training.py:192] Running evaluation for step 15000\n",
      "2024-12-08 19:23:55.134000: I training.py:192] Evaluation result for step 15000: loss = 2.028474 ; perplexity = 7.602473\n",
      "2024-12-08 19:24:56.712000: I runner.py:310] Step = 15100 ; steps/s = 1.62, tokens/s = 44308 (44308 target) ; Learning rate = 0.000719 ; Loss = 2.230833\n",
      "2024-12-08 19:25:58.320000: I runner.py:310] Step = 15200 ; steps/s = 1.62, tokens/s = 44309 (44309 target) ; Learning rate = 0.000717 ; Loss = 2.242451\n",
      "2024-12-08 19:27:00.003000: I runner.py:310] Step = 15300 ; steps/s = 1.62, tokens/s = 44256 (44256 target) ; Learning rate = 0.000715 ; Loss = 2.244090\n",
      "2024-12-08 19:28:01.343000: I runner.py:310] Step = 15400 ; steps/s = 1.63, tokens/s = 43772 (43772 target) ; Learning rate = 0.000712 ; Loss = 2.262520\n",
      "2024-12-08 19:29:03.020000: I runner.py:310] Step = 15500 ; steps/s = 1.62, tokens/s = 44261 (44261 target) ; Learning rate = 0.000710 ; Loss = 2.219835\n",
      "2024-12-08 19:30:04.735000: I runner.py:310] Step = 15600 ; steps/s = 1.62, tokens/s = 44206 (44206 target) ; Learning rate = 0.000708 ; Loss = 2.244010\n",
      "2024-12-08 19:31:06.081000: I runner.py:310] Step = 15700 ; steps/s = 1.63, tokens/s = 43768 (43768 target) ; Learning rate = 0.000705 ; Loss = 2.269670\n",
      "2024-12-08 19:32:07.840000: I runner.py:310] Step = 15800 ; steps/s = 1.62, tokens/s = 44187 (44187 target) ; Learning rate = 0.000703 ; Loss = 2.229673\n",
      "2024-12-08 19:33:09.487000: I runner.py:310] Step = 15900 ; steps/s = 1.62, tokens/s = 44254 (44254 target) ; Learning rate = 0.000701 ; Loss = 2.208509\n",
      "2024-12-08 19:34:11.230000: I runner.py:310] Step = 16000 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000699 ; Loss = 2.210734\n",
      "2024-12-08 19:35:12.560000: I runner.py:310] Step = 16100 ; steps/s = 1.63, tokens/s = 43760 (43760 target) ; Learning rate = 0.000697 ; Loss = 2.257633\n",
      "2024-12-08 19:36:14.229000: I runner.py:310] Step = 16200 ; steps/s = 1.62, tokens/s = 44247 (44247 target) ; Learning rate = 0.000694 ; Loss = 2.197079\n",
      "2024-12-08 19:37:15.925000: I runner.py:310] Step = 16300 ; steps/s = 1.62, tokens/s = 44245 (44245 target) ; Learning rate = 0.000692 ; Loss = 2.207851\n",
      "2024-12-08 19:38:17.236000: I runner.py:310] Step = 16400 ; steps/s = 1.63, tokens/s = 43786 (43786 target) ; Learning rate = 0.000690 ; Loss = 2.229847\n",
      "2024-12-08 19:39:18.938000: I runner.py:310] Step = 16500 ; steps/s = 1.62, tokens/s = 44227 (44227 target) ; Learning rate = 0.000688 ; Loss = 2.184093\n",
      "2024-12-08 19:40:20.692000: I runner.py:310] Step = 16600 ; steps/s = 1.62, tokens/s = 44216 (44216 target) ; Learning rate = 0.000686 ; Loss = 2.166486\n",
      "2024-12-08 19:41:22.095000: I runner.py:310] Step = 16700 ; steps/s = 1.63, tokens/s = 43823 (43823 target) ; Learning rate = 0.000684 ; Loss = 2.315878\n",
      "2024-12-08 19:42:23.739000: I runner.py:310] Step = 16800 ; steps/s = 1.62, tokens/s = 44157 (44157 target) ; Learning rate = 0.000682 ; Loss = 2.186686\n",
      "2024-12-08 19:43:25.398000: I runner.py:310] Step = 16900 ; steps/s = 1.62, tokens/s = 44268 (44268 target) ; Learning rate = 0.000680 ; Loss = 2.225230\n",
      "2024-12-08 19:44:27.138000: I runner.py:310] Step = 17000 ; steps/s = 1.62, tokens/s = 44184 (44184 target) ; Learning rate = 0.000678 ; Loss = 2.230087\n",
      "2024-12-08 19:45:28.456000: I runner.py:310] Step = 17100 ; steps/s = 1.63, tokens/s = 43781 (43781 target) ; Learning rate = 0.000676 ; Loss = 2.160196\n",
      "2024-12-08 19:46:30.149000: I runner.py:310] Step = 17200 ; steps/s = 1.62, tokens/s = 44243 (44243 target) ; Learning rate = 0.000674 ; Loss = 2.209562\n",
      "2024-12-08 19:47:31.876000: I runner.py:310] Step = 17300 ; steps/s = 1.62, tokens/s = 44200 (44200 target) ; Learning rate = 0.000672 ; Loss = 2.199539\n",
      "2024-12-08 19:48:33.165000: I runner.py:310] Step = 17400 ; steps/s = 1.63, tokens/s = 43804 (43804 target) ; Learning rate = 0.000670 ; Loss = 2.160688\n",
      "2024-12-08 19:49:34.891000: I runner.py:310] Step = 17500 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000668 ; Loss = 2.221562\n",
      "2024-12-08 19:50:36.588000: I runner.py:310] Step = 17600 ; steps/s = 1.62, tokens/s = 44233 (44233 target) ; Learning rate = 0.000666 ; Loss = 2.158759\n",
      "2024-12-08 19:51:38.337000: I runner.py:310] Step = 17700 ; steps/s = 1.62, tokens/s = 44196 (44196 target) ; Learning rate = 0.000664 ; Loss = 2.169256\n",
      "2024-12-08 19:52:39.729000: I runner.py:310] Step = 17800 ; steps/s = 1.63, tokens/s = 43732 (43732 target) ; Learning rate = 0.000662 ; Loss = 2.189297\n",
      "2024-12-08 19:53:41.429000: I runner.py:310] Step = 17900 ; steps/s = 1.62, tokens/s = 44229 (44229 target) ; Learning rate = 0.000661 ; Loss = 2.180713\n",
      "2024-12-08 19:54:43.105000: I runner.py:310] Step = 18000 ; steps/s = 1.62, tokens/s = 44236 (44236 target) ; Learning rate = 0.000659 ; Loss = 2.141213\n",
      "2024-12-08 19:55:44.413000: I runner.py:310] Step = 18100 ; steps/s = 1.63, tokens/s = 43804 (43804 target) ; Learning rate = 0.000657 ; Loss = 2.174844\n",
      "2024-12-08 19:56:46.152000: I runner.py:310] Step = 18200 ; steps/s = 1.62, tokens/s = 44197 (44197 target) ; Learning rate = 0.000655 ; Loss = 2.159242\n",
      "2024-12-08 19:57:47.884000: I runner.py:310] Step = 18300 ; steps/s = 1.62, tokens/s = 44215 (44215 target) ; Learning rate = 0.000653 ; Loss = 2.154911\n",
      "2024-12-08 19:58:49.608000: I runner.py:310] Step = 18400 ; steps/s = 1.62, tokens/s = 44213 (44213 target) ; Learning rate = 0.000652 ; Loss = 2.183108\n",
      "2024-12-08 19:59:51.000000: I runner.py:310] Step = 18500 ; steps/s = 1.63, tokens/s = 43734 (43734 target) ; Learning rate = 0.000650 ; Loss = 2.191399\n",
      "2024-12-08 20:00:52.741000: I runner.py:310] Step = 18600 ; steps/s = 1.62, tokens/s = 44199 (44199 target) ; Learning rate = 0.000648 ; Loss = 2.156205\n",
      "2024-12-08 20:01:54.488000: I runner.py:310] Step = 18700 ; steps/s = 1.62, tokens/s = 44192 (44192 target) ; Learning rate = 0.000646 ; Loss = 2.162364\n",
      "2024-12-08 20:02:55.795000: I runner.py:310] Step = 18800 ; steps/s = 1.63, tokens/s = 43789 (43789 target) ; Learning rate = 0.000645 ; Loss = 2.148389\n",
      "2024-12-08 20:03:57.465000: I runner.py:310] Step = 18900 ; steps/s = 1.62, tokens/s = 44250 (44250 target) ; Learning rate = 0.000643 ; Loss = 2.127515\n",
      "2024-12-08 20:04:59.244000: I runner.py:310] Step = 19000 ; steps/s = 1.62, tokens/s = 44184 (44184 target) ; Learning rate = 0.000641 ; Loss = 2.151859\n",
      "2024-12-08 20:06:00.974000: I runner.py:310] Step = 19100 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000640 ; Loss = 2.203542\n",
      "2024-12-08 20:07:02.286000: I runner.py:310] Step = 19200 ; steps/s = 1.63, tokens/s = 43772 (43772 target) ; Learning rate = 0.000638 ; Loss = 2.083680\n",
      "2024-12-08 20:08:03.983000: I runner.py:310] Step = 19300 ; steps/s = 1.62, tokens/s = 44240 (44240 target) ; Learning rate = 0.000636 ; Loss = 2.154382\n",
      "2024-12-08 20:09:05.621000: I runner.py:310] Step = 19400 ; steps/s = 1.62, tokens/s = 44277 (44277 target) ; Learning rate = 0.000635 ; Loss = 2.165891\n",
      "2024-12-08 20:10:06.838000: I runner.py:310] Step = 19500 ; steps/s = 1.63, tokens/s = 43849 (43849 target) ; Learning rate = 0.000633 ; Loss = 2.135968\n",
      "2024-12-08 20:11:08.461000: I runner.py:310] Step = 19600 ; steps/s = 1.62, tokens/s = 44285 (44285 target) ; Learning rate = 0.000631 ; Loss = 2.122264\n",
      "2024-12-08 20:12:10.210000: I runner.py:310] Step = 19700 ; steps/s = 1.62, tokens/s = 44196 (44196 target) ; Learning rate = 0.000630 ; Loss = 2.117214\n",
      "2024-12-08 20:13:11.943000: I runner.py:310] Step = 19800 ; steps/s = 1.62, tokens/s = 44215 (44215 target) ; Learning rate = 0.000628 ; Loss = 2.165970\n",
      "2024-12-08 20:14:13.234000: I runner.py:310] Step = 19900 ; steps/s = 1.63, tokens/s = 43797 (43797 target) ; Learning rate = 0.000627 ; Loss = 2.180982\n",
      "2024-12-08 20:15:14.948000: I runner.py:310] Step = 20000 ; steps/s = 1.62, tokens/s = 44241 (44241 target) ; Learning rate = 0.000625 ; Loss = 2.124819\n",
      "2024-12-08 20:15:16.704000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-20000\n",
      "2024-12-08 20:15:16.704000: I training.py:192] Running evaluation for step 20000\n",
      "2024-12-08 20:18:28.848000: I training.py:192] Evaluation result for step 20000: loss = 2.108678 ; perplexity = 8.237342\n",
      "2024-12-08 20:19:30.393000: I runner.py:310] Step = 20100 ; steps/s = 1.63, tokens/s = 44336 (44336 target) ; Learning rate = 0.000623 ; Loss = 2.119615\n",
      "2024-12-08 20:20:31.650000: I runner.py:310] Step = 20200 ; steps/s = 1.63, tokens/s = 43810 (43810 target) ; Learning rate = 0.000622 ; Loss = 2.123023\n",
      "2024-12-08 20:21:33.377000: I runner.py:310] Step = 20300 ; steps/s = 1.62, tokens/s = 44215 (44215 target) ; Learning rate = 0.000620 ; Loss = 2.124281\n",
      "2024-12-08 20:22:35.054000: I runner.py:310] Step = 20400 ; steps/s = 1.62, tokens/s = 44258 (44258 target) ; Learning rate = 0.000619 ; Loss = 2.102695\n",
      "2024-12-08 20:23:36.854000: I runner.py:310] Step = 20500 ; steps/s = 1.62, tokens/s = 44155 (44155 target) ; Learning rate = 0.000617 ; Loss = 2.117899\n",
      "2024-12-08 20:24:38.172000: I runner.py:310] Step = 20600 ; steps/s = 1.63, tokens/s = 43778 (43778 target) ; Learning rate = 0.000616 ; Loss = 2.145132\n",
      "2024-12-08 20:25:39.841000: I runner.py:310] Step = 20700 ; steps/s = 1.62, tokens/s = 44265 (44265 target) ; Learning rate = 0.000614 ; Loss = 2.102404\n",
      "2024-12-08 20:26:41.488000: I runner.py:310] Step = 20800 ; steps/s = 1.62, tokens/s = 44267 (44267 target) ; Learning rate = 0.000613 ; Loss = 2.110725\n",
      "2024-12-08 20:27:42.774000: I runner.py:310] Step = 20900 ; steps/s = 1.63, tokens/s = 43788 (43788 target) ; Learning rate = 0.000611 ; Loss = 2.130239\n",
      "2024-12-08 20:28:44.482000: I runner.py:310] Step = 21000 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000610 ; Loss = 2.107251\n",
      "2024-12-08 20:29:46.243000: I runner.py:310] Step = 21100 ; steps/s = 1.62, tokens/s = 44197 (44197 target) ; Learning rate = 0.000608 ; Loss = 2.115412\n",
      "2024-12-08 20:30:47.912000: I runner.py:310] Step = 21200 ; steps/s = 1.62, tokens/s = 44251 (44251 target) ; Learning rate = 0.000607 ; Loss = 2.118066\n",
      "2024-12-08 20:31:49.213000: I runner.py:310] Step = 21300 ; steps/s = 1.63, tokens/s = 43786 (43786 target) ; Learning rate = 0.000606 ; Loss = 2.073350\n",
      "2024-12-08 20:32:50.936000: I runner.py:310] Step = 21400 ; steps/s = 1.62, tokens/s = 44236 (44236 target) ; Learning rate = 0.000604 ; Loss = 2.134146\n",
      "2024-12-08 20:33:52.582000: I runner.py:310] Step = 21500 ; steps/s = 1.62, tokens/s = 44252 (44252 target) ; Learning rate = 0.000603 ; Loss = 2.131931\n",
      "2024-12-08 20:34:53.859000: I runner.py:310] Step = 21600 ; steps/s = 1.63, tokens/s = 43797 (43797 target) ; Learning rate = 0.000601 ; Loss = 2.135765\n",
      "2024-12-08 20:35:55.588000: I runner.py:310] Step = 21700 ; steps/s = 1.62, tokens/s = 44214 (44214 target) ; Learning rate = 0.000600 ; Loss = 2.128675\n",
      "2024-12-08 20:36:57.329000: I runner.py:310] Step = 21800 ; steps/s = 1.62, tokens/s = 44211 (44211 target) ; Learning rate = 0.000599 ; Loss = 2.123701\n",
      "2024-12-08 20:37:59.066000: I runner.py:310] Step = 21900 ; steps/s = 1.62, tokens/s = 44206 (44206 target) ; Learning rate = 0.000597 ; Loss = 2.100209\n",
      "2024-12-08 20:39:00.332000: I runner.py:310] Step = 22000 ; steps/s = 1.63, tokens/s = 43805 (43805 target) ; Learning rate = 0.000596 ; Loss = 2.129879\n",
      "2024-12-08 20:40:02.031000: I runner.py:310] Step = 22100 ; steps/s = 1.62, tokens/s = 44232 (44232 target) ; Learning rate = 0.000595 ; Loss = 2.080486\n",
      "2024-12-08 20:41:03.718000: I runner.py:310] Step = 22200 ; steps/s = 1.62, tokens/s = 44246 (44246 target) ; Learning rate = 0.000593 ; Loss = 2.104886\n",
      "2024-12-08 20:42:05.121000: I runner.py:310] Step = 22300 ; steps/s = 1.63, tokens/s = 43716 (43716 target) ; Learning rate = 0.000592 ; Loss = 2.092987\n",
      "2024-12-08 20:43:06.836000: I runner.py:310] Step = 22400 ; steps/s = 1.62, tokens/s = 44223 (44223 target) ; Learning rate = 0.000591 ; Loss = 2.080359\n",
      "2024-12-08 20:44:08.498000: I runner.py:310] Step = 22500 ; steps/s = 1.62, tokens/s = 44255 (44255 target) ; Learning rate = 0.000589 ; Loss = 2.123977\n",
      "2024-12-08 20:45:10.214000: I runner.py:310] Step = 22600 ; steps/s = 1.62, tokens/s = 44227 (44227 target) ; Learning rate = 0.000588 ; Loss = 2.109947\n",
      "2024-12-08 20:46:11.536000: I runner.py:310] Step = 22700 ; steps/s = 1.63, tokens/s = 43778 (43778 target) ; Learning rate = 0.000587 ; Loss = 2.094536\n",
      "2024-12-08 20:47:13.240000: I runner.py:310] Step = 22800 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000585 ; Loss = 2.082841\n",
      "2024-12-08 20:48:15.005000: I runner.py:310] Step = 22900 ; steps/s = 1.62, tokens/s = 44177 (44177 target) ; Learning rate = 0.000584 ; Loss = 2.065632\n",
      "2024-12-08 20:49:16.369000: I runner.py:310] Step = 23000 ; steps/s = 1.63, tokens/s = 43754 (43754 target) ; Learning rate = 0.000583 ; Loss = 2.106591\n",
      "2024-12-08 20:50:18.075000: I runner.py:310] Step = 23100 ; steps/s = 1.62, tokens/s = 44246 (44246 target) ; Learning rate = 0.000582 ; Loss = 2.062372\n",
      "2024-12-08 20:51:19.859000: I runner.py:310] Step = 23200 ; steps/s = 1.62, tokens/s = 44161 (44161 target) ; Learning rate = 0.000580 ; Loss = 2.085643\n",
      "2024-12-08 20:52:21.578000: I runner.py:310] Step = 23300 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000579 ; Loss = 2.087630\n",
      "2024-12-08 20:53:22.923000: I runner.py:310] Step = 23400 ; steps/s = 1.63, tokens/s = 43748 (43748 target) ; Learning rate = 0.000578 ; Loss = 2.123326\n",
      "2024-12-08 20:54:24.604000: I runner.py:310] Step = 23500 ; steps/s = 1.62, tokens/s = 44245 (44245 target) ; Learning rate = 0.000577 ; Loss = 2.053252\n",
      "2024-12-08 20:55:26.340000: I runner.py:310] Step = 23600 ; steps/s = 1.62, tokens/s = 44213 (44213 target) ; Learning rate = 0.000575 ; Loss = 2.068639\n",
      "2024-12-08 20:56:27.610000: I runner.py:310] Step = 23700 ; steps/s = 1.63, tokens/s = 43809 (43809 target) ; Learning rate = 0.000574 ; Loss = 2.049820\n",
      "2024-12-08 20:57:30.139000: I runner.py:310] Step = 23800 ; steps/s = 1.60, tokens/s = 43643 (43643 target) ; Learning rate = 0.000573 ; Loss = 2.092212\n",
      "2024-12-08 20:58:31.893000: I runner.py:310] Step = 23900 ; steps/s = 1.62, tokens/s = 44202 (44202 target) ; Learning rate = 0.000572 ; Loss = 2.099036\n",
      "2024-12-08 20:59:33.602000: I runner.py:310] Step = 24000 ; steps/s = 1.62, tokens/s = 44223 (44223 target) ; Learning rate = 0.000571 ; Loss = 2.089842\n",
      "2024-12-08 21:00:34.942000: I runner.py:310] Step = 24100 ; steps/s = 1.63, tokens/s = 43761 (43761 target) ; Learning rate = 0.000569 ; Loss = 2.016794\n",
      "2024-12-08 21:01:36.643000: I runner.py:310] Step = 24200 ; steps/s = 1.62, tokens/s = 44230 (44230 target) ; Learning rate = 0.000568 ; Loss = 2.093496\n",
      "2024-12-08 21:02:38.407000: I runner.py:310] Step = 24300 ; steps/s = 1.62, tokens/s = 44189 (44189 target) ; Learning rate = 0.000567 ; Loss = 2.103659\n",
      "2024-12-08 21:03:39.677000: I runner.py:310] Step = 24400 ; steps/s = 1.63, tokens/s = 43813 (43813 target) ; Learning rate = 0.000566 ; Loss = 2.104961\n",
      "2024-12-08 21:04:41.362000: I runner.py:310] Step = 24500 ; steps/s = 1.62, tokens/s = 44241 (44241 target) ; Learning rate = 0.000565 ; Loss = 2.055079\n",
      "2024-12-08 21:05:43.106000: I runner.py:310] Step = 24600 ; steps/s = 1.62, tokens/s = 44188 (44188 target) ; Learning rate = 0.000564 ; Loss = 2.063294\n",
      "2024-12-08 21:06:44.679000: I runner.py:310] Step = 24700 ; steps/s = 1.62, tokens/s = 44186 (44186 target) ; Learning rate = 0.000562 ; Loss = 2.022811\n",
      "2024-12-08 21:07:46.099000: I runner.py:310] Step = 24800 ; steps/s = 1.63, tokens/s = 43831 (43831 target) ; Learning rate = 0.000561 ; Loss = 2.090128\n",
      "2024-12-08 21:08:47.830000: I runner.py:310] Step = 24900 ; steps/s = 1.62, tokens/s = 44207 (44207 target) ; Learning rate = 0.000560 ; Loss = 2.067626\n",
      "2024-12-08 21:09:49.528000: I runner.py:310] Step = 25000 ; steps/s = 1.62, tokens/s = 44250 (44250 target) ; Learning rate = 0.000559 ; Loss = 2.065297\n",
      "2024-12-08 21:09:49.530000: I training.py:192] Running evaluation for step 25000\n",
      "2024-12-08 21:12:42.588000: I training.py:192] Evaluation result for step 25000: loss = 2.172262 ; perplexity = 8.778117\n",
      "2024-12-08 21:13:43.734000: I runner.py:310] Step = 25100 ; steps/s = 1.64, tokens/s = 43911 (43911 target) ; Learning rate = 0.000558 ; Loss = 2.099141\n",
      "2024-12-08 21:14:45.407000: I runner.py:310] Step = 25200 ; steps/s = 1.62, tokens/s = 44252 (44252 target) ; Learning rate = 0.000557 ; Loss = 2.044398\n",
      "2024-12-08 21:15:47.157000: I runner.py:310] Step = 25300 ; steps/s = 1.62, tokens/s = 44188 (44188 target) ; Learning rate = 0.000556 ; Loss = 2.022181\n",
      "2024-12-08 21:16:48.549000: I runner.py:310] Step = 25400 ; steps/s = 1.63, tokens/s = 43735 (43735 target) ; Learning rate = 0.000555 ; Loss = 2.044999\n",
      "2024-12-08 21:17:50.254000: I runner.py:310] Step = 25500 ; steps/s = 1.62, tokens/s = 44240 (44240 target) ; Learning rate = 0.000553 ; Loss = 2.037145\n",
      "2024-12-08 21:18:52.006000: I runner.py:310] Step = 25600 ; steps/s = 1.62, tokens/s = 44194 (44194 target) ; Learning rate = 0.000552 ; Loss = 2.071299\n",
      "2024-12-08 21:19:53.749000: I runner.py:310] Step = 25700 ; steps/s = 1.62, tokens/s = 44186 (44186 target) ; Learning rate = 0.000551 ; Loss = 2.087430\n",
      "2024-12-08 21:20:55.059000: I runner.py:310] Step = 25800 ; steps/s = 1.63, tokens/s = 43788 (43788 target) ; Learning rate = 0.000550 ; Loss = 2.084515\n",
      "2024-12-08 21:21:56.794000: I runner.py:310] Step = 25900 ; steps/s = 1.62, tokens/s = 44206 (44206 target) ; Learning rate = 0.000549 ; Loss = 2.042373\n",
      "2024-12-08 21:22:58.502000: I runner.py:310] Step = 26000 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000548 ; Loss = 2.019281\n",
      "2024-12-08 21:23:59.901000: I runner.py:310] Step = 26100 ; steps/s = 1.63, tokens/s = 43734 (43734 target) ; Learning rate = 0.000547 ; Loss = 2.035283\n",
      "2024-12-08 21:25:01.673000: I runner.py:310] Step = 26200 ; steps/s = 1.62, tokens/s = 44174 (44174 target) ; Learning rate = 0.000546 ; Loss = 2.015025\n",
      "2024-12-08 21:26:03.392000: I runner.py:310] Step = 26300 ; steps/s = 1.62, tokens/s = 44230 (44230 target) ; Learning rate = 0.000545 ; Loss = 2.056314\n",
      "2024-12-08 21:27:05.156000: I runner.py:310] Step = 26400 ; steps/s = 1.62, tokens/s = 44184 (44184 target) ; Learning rate = 0.000544 ; Loss = 2.068253\n",
      "2024-12-08 21:28:06.503000: I runner.py:310] Step = 26500 ; steps/s = 1.63, tokens/s = 43755 (43755 target) ; Learning rate = 0.000543 ; Loss = 1.998702\n",
      "2024-12-08 21:29:08.285000: I runner.py:310] Step = 26600 ; steps/s = 1.62, tokens/s = 44181 (44181 target) ; Learning rate = 0.000542 ; Loss = 2.027812\n",
      "2024-12-08 21:30:10.026000: I runner.py:310] Step = 26700 ; steps/s = 1.62, tokens/s = 44197 (44197 target) ; Learning rate = 0.000541 ; Loss = 2.046919\n",
      "2024-12-08 21:31:11.370000: I runner.py:310] Step = 26800 ; steps/s = 1.63, tokens/s = 43753 (43753 target) ; Learning rate = 0.000540 ; Loss = 2.054139\n",
      "2024-12-08 21:32:13.083000: I runner.py:310] Step = 26900 ; steps/s = 1.62, tokens/s = 44232 (44232 target) ; Learning rate = 0.000539 ; Loss = 2.054080\n",
      "2024-12-08 21:33:14.811000: I runner.py:310] Step = 27000 ; steps/s = 1.62, tokens/s = 44203 (44203 target) ; Learning rate = 0.000538 ; Loss = 2.026920\n",
      "2024-12-08 21:34:16.517000: I runner.py:310] Step = 27100 ; steps/s = 1.62, tokens/s = 44242 (44242 target) ; Learning rate = 0.000537 ; Loss = 2.043792\n",
      "2024-12-08 21:35:17.871000: I runner.py:310] Step = 27200 ; steps/s = 1.63, tokens/s = 43745 (43745 target) ; Learning rate = 0.000536 ; Loss = 1.979059\n",
      "2024-12-08 21:36:19.664000: I runner.py:310] Step = 27300 ; steps/s = 1.62, tokens/s = 44171 (44171 target) ; Learning rate = 0.000535 ; Loss = 2.054178\n",
      "2024-12-08 21:37:21.379000: I runner.py:310] Step = 27400 ; steps/s = 1.62, tokens/s = 44218 (44218 target) ; Learning rate = 0.000534 ; Loss = 2.067930\n",
      "2024-12-08 21:38:22.721000: I runner.py:310] Step = 27500 ; steps/s = 1.63, tokens/s = 43758 (43758 target) ; Learning rate = 0.000533 ; Loss = 2.060457\n",
      "2024-12-08 21:39:24.434000: I runner.py:310] Step = 27600 ; steps/s = 1.62, tokens/s = 44230 (44230 target) ; Learning rate = 0.000532 ; Loss = 2.023222\n",
      "2024-12-08 21:40:26.105000: I runner.py:310] Step = 27700 ; steps/s = 1.62, tokens/s = 44240 (44240 target) ; Learning rate = 0.000531 ; Loss = 2.001622\n",
      "2024-12-08 21:41:27.780000: I runner.py:310] Step = 27800 ; steps/s = 1.62, tokens/s = 44258 (44258 target) ; Learning rate = 0.000530 ; Loss = 2.010527\n",
      "2024-12-08 21:42:29.136000: I runner.py:310] Step = 27900 ; steps/s = 1.63, tokens/s = 43747 (43747 target) ; Learning rate = 0.000529 ; Loss = 1.985219\n",
      "2024-12-08 21:43:30.920000: I runner.py:310] Step = 28000 ; steps/s = 1.62, tokens/s = 44168 (44168 target) ; Learning rate = 0.000528 ; Loss = 2.051424\n",
      "2024-12-08 21:44:32.643000: I runner.py:310] Step = 28100 ; steps/s = 1.62, tokens/s = 44218 (44218 target) ; Learning rate = 0.000527 ; Loss = 2.054358\n",
      "2024-12-08 21:45:34.056000: I runner.py:310] Step = 28200 ; steps/s = 1.63, tokens/s = 43719 (43719 target) ; Learning rate = 0.000526 ; Loss = 2.011912\n",
      "2024-12-08 21:46:35.761000: I runner.py:310] Step = 28300 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000525 ; Loss = 2.017523\n",
      "2024-12-08 21:47:37.531000: I runner.py:310] Step = 28400 ; steps/s = 1.62, tokens/s = 44174 (44174 target) ; Learning rate = 0.000524 ; Loss = 2.043814\n",
      "2024-12-08 21:48:39.246000: I runner.py:310] Step = 28500 ; steps/s = 1.62, tokens/s = 44231 (44231 target) ; Learning rate = 0.000524 ; Loss = 2.053433\n",
      "2024-12-08 21:49:40.493000: I runner.py:310] Step = 28600 ; steps/s = 1.63, tokens/s = 43825 (43825 target) ; Learning rate = 0.000523 ; Loss = 1.975704\n",
      "2024-12-08 21:50:42.253000: I runner.py:310] Step = 28700 ; steps/s = 1.62, tokens/s = 44188 (44188 target) ; Learning rate = 0.000522 ; Loss = 2.049176\n",
      "2024-12-08 21:51:43.975000: I runner.py:310] Step = 28800 ; steps/s = 1.62, tokens/s = 44217 (44217 target) ; Learning rate = 0.000521 ; Loss = 2.055119\n",
      "2024-12-08 21:52:45.288000: I runner.py:310] Step = 28900 ; steps/s = 1.63, tokens/s = 43777 (43777 target) ; Learning rate = 0.000520 ; Loss = 1.987618\n",
      "2024-12-08 21:53:47.062000: I runner.py:310] Step = 29000 ; steps/s = 1.62, tokens/s = 44188 (44188 target) ; Learning rate = 0.000519 ; Loss = 2.038492\n",
      "2024-12-08 21:54:48.760000: I runner.py:310] Step = 29100 ; steps/s = 1.62, tokens/s = 44232 (44232 target) ; Learning rate = 0.000518 ; Loss = 2.021987\n",
      "2024-12-08 21:55:50.481000: I runner.py:310] Step = 29200 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000517 ; Loss = 2.023158\n",
      "2024-12-08 21:56:51.811000: I runner.py:310] Step = 29300 ; steps/s = 1.63, tokens/s = 43758 (43758 target) ; Learning rate = 0.000516 ; Loss = 2.044316\n",
      "2024-12-08 21:57:53.518000: I runner.py:310] Step = 29400 ; steps/s = 1.62, tokens/s = 44240 (44240 target) ; Learning rate = 0.000515 ; Loss = 1.998409\n",
      "2024-12-08 21:58:55.236000: I runner.py:310] Step = 29500 ; steps/s = 1.62, tokens/s = 44232 (44232 target) ; Learning rate = 0.000515 ; Loss = 2.009595\n",
      "2024-12-08 21:59:56.584000: I runner.py:310] Step = 29600 ; steps/s = 1.63, tokens/s = 43737 (43737 target) ; Learning rate = 0.000514 ; Loss = 2.015303\n",
      "2024-12-08 22:00:58.316000: I runner.py:310] Step = 29700 ; steps/s = 1.62, tokens/s = 44209 (44209 target) ; Learning rate = 0.000513 ; Loss = 2.002834\n",
      "2024-12-08 22:01:59.956000: I runner.py:310] Step = 29800 ; steps/s = 1.62, tokens/s = 44284 (44284 target) ; Learning rate = 0.000512 ; Loss = 1.983397\n",
      "2024-12-08 22:03:01.696000: I runner.py:310] Step = 29900 ; steps/s = 1.62, tokens/s = 44199 (44199 target) ; Learning rate = 0.000511 ; Loss = 2.011534\n",
      "2024-12-08 22:04:02.977000: I runner.py:310] Step = 30000 ; steps/s = 1.63, tokens/s = 43798 (43798 target) ; Learning rate = 0.000510 ; Loss = 2.052915\n",
      "2024-12-08 22:04:04.857000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-30000\n",
      "2024-12-08 22:04:04.857000: I training.py:192] Running evaluation for step 30000\n",
      "2024-12-08 22:06:54.276000: I training.py:192] Evaluation result for step 30000: loss = 2.210375 ; perplexity = 9.119139\n",
      "2024-12-08 22:07:55.790000: I runner.py:310] Step = 30100 ; steps/s = 1.63, tokens/s = 44386 (44386 target) ; Learning rate = 0.000509 ; Loss = 2.013657\n",
      "2024-12-08 22:08:57.494000: I runner.py:310] Step = 30200 ; steps/s = 1.62, tokens/s = 44222 (44222 target) ; Learning rate = 0.000509 ; Loss = 2.008120\n",
      "2024-12-08 22:09:58.872000: I runner.py:310] Step = 30300 ; steps/s = 1.63, tokens/s = 43731 (43731 target) ; Learning rate = 0.000508 ; Loss = 1.964373\n",
      "2024-12-08 22:11:00.602000: I runner.py:310] Step = 30400 ; steps/s = 1.62, tokens/s = 44208 (44208 target) ; Learning rate = 0.000507 ; Loss = 2.014440\n",
      "2024-12-08 22:12:02.311000: I runner.py:310] Step = 30500 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000506 ; Loss = 2.033132\n",
      "2024-12-08 22:13:04.075000: I runner.py:310] Step = 30600 ; steps/s = 1.62, tokens/s = 44187 (44187 target) ; Learning rate = 0.000505 ; Loss = 2.038844\n",
      "2024-12-08 22:14:05.355000: I runner.py:310] Step = 30700 ; steps/s = 1.63, tokens/s = 43805 (43805 target) ; Learning rate = 0.000504 ; Loss = 1.978497\n",
      "2024-12-08 22:15:07.111000: I runner.py:310] Step = 30800 ; steps/s = 1.62, tokens/s = 44187 (44187 target) ; Learning rate = 0.000504 ; Loss = 2.000106\n",
      "2024-12-08 22:16:08.862000: I runner.py:310] Step = 30900 ; steps/s = 1.62, tokens/s = 44196 (44196 target) ; Learning rate = 0.000503 ; Loss = 2.026498\n",
      "2024-12-08 22:17:10.235000: I runner.py:310] Step = 31000 ; steps/s = 1.63, tokens/s = 43757 (43757 target) ; Learning rate = 0.000502 ; Loss = 1.977592\n",
      "2024-12-08 22:18:11.966000: I runner.py:310] Step = 31100 ; steps/s = 1.62, tokens/s = 44217 (44217 target) ; Learning rate = 0.000501 ; Loss = 1.998341\n",
      "2024-12-08 22:19:13.701000: I runner.py:310] Step = 31200 ; steps/s = 1.62, tokens/s = 44206 (44206 target) ; Learning rate = 0.000500 ; Loss = 2.029801\n",
      "2024-12-08 22:20:15.419000: I runner.py:310] Step = 31300 ; steps/s = 1.62, tokens/s = 44207 (44207 target) ; Learning rate = 0.000500 ; Loss = 2.018712\n",
      "2024-12-08 22:21:16.696000: I runner.py:310] Step = 31400 ; steps/s = 1.63, tokens/s = 43796 (43796 target) ; Learning rate = 0.000499 ; Loss = 2.029254\n",
      "2024-12-08 22:22:18.437000: I runner.py:310] Step = 31500 ; steps/s = 1.62, tokens/s = 44202 (44202 target) ; Learning rate = 0.000498 ; Loss = 1.994024\n",
      "2024-12-08 22:23:20.132000: I runner.py:310] Step = 31600 ; steps/s = 1.62, tokens/s = 44233 (44233 target) ; Learning rate = 0.000497 ; Loss = 1.997440\n",
      "2024-12-08 22:24:21.461000: I runner.py:310] Step = 31700 ; steps/s = 1.63, tokens/s = 43769 (43769 target) ; Learning rate = 0.000496 ; Loss = 2.019208\n",
      "2024-12-08 22:25:23.193000: I runner.py:310] Step = 31800 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000496 ; Loss = 1.968467\n",
      "2024-12-08 22:26:24.927000: I runner.py:310] Step = 31900 ; steps/s = 1.62, tokens/s = 44209 (44209 target) ; Learning rate = 0.000495 ; Loss = 1.983199\n",
      "2024-12-08 22:27:26.685000: I runner.py:310] Step = 32000 ; steps/s = 1.62, tokens/s = 44199 (44199 target) ; Learning rate = 0.000494 ; Loss = 1.986206\n",
      "2024-12-08 22:28:27.944000: I runner.py:310] Step = 32100 ; steps/s = 1.63, tokens/s = 43824 (43824 target) ; Learning rate = 0.000493 ; Loss = 1.971204\n",
      "2024-12-08 22:29:29.694000: I runner.py:310] Step = 32200 ; steps/s = 1.62, tokens/s = 44203 (44203 target) ; Learning rate = 0.000493 ; Loss = 2.007401\n",
      "2024-12-08 22:30:31.430000: I runner.py:310] Step = 32300 ; steps/s = 1.62, tokens/s = 44196 (44196 target) ; Learning rate = 0.000492 ; Loss = 2.016734\n",
      "2024-12-08 22:31:32.816000: I runner.py:310] Step = 32400 ; steps/s = 1.63, tokens/s = 43718 (43718 target) ; Learning rate = 0.000491 ; Loss = 2.018398\n",
      "2024-12-08 22:32:34.534000: I runner.py:310] Step = 32500 ; steps/s = 1.62, tokens/s = 44222 (44222 target) ; Learning rate = 0.000490 ; Loss = 1.970893\n",
      "2024-12-08 22:33:36.264000: I runner.py:310] Step = 32600 ; steps/s = 1.62, tokens/s = 44203 (44203 target) ; Learning rate = 0.000490 ; Loss = 1.978399\n",
      "2024-12-08 22:34:37.993000: I runner.py:310] Step = 32700 ; steps/s = 1.62, tokens/s = 44223 (44223 target) ; Learning rate = 0.000489 ; Loss = 1.978353\n",
      "2024-12-08 22:35:39.323000: I runner.py:310] Step = 32800 ; steps/s = 1.63, tokens/s = 43763 (43763 target) ; Learning rate = 0.000488 ; Loss = 1.964544\n",
      "2024-12-08 22:36:41.064000: I runner.py:310] Step = 32900 ; steps/s = 1.62, tokens/s = 44198 (44198 target) ; Learning rate = 0.000487 ; Loss = 2.004071\n",
      "2024-12-08 22:37:42.844000: I runner.py:310] Step = 33000 ; steps/s = 1.62, tokens/s = 44189 (44189 target) ; Learning rate = 0.000487 ; Loss = 2.016597\n",
      "2024-12-08 22:38:44.212000: I runner.py:310] Step = 33100 ; steps/s = 1.63, tokens/s = 43739 (43739 target) ; Learning rate = 0.000486 ; Loss = 1.994391\n",
      "2024-12-08 22:39:45.988000: I runner.py:310] Step = 33200 ; steps/s = 1.62, tokens/s = 44177 (44177 target) ; Learning rate = 0.000485 ; Loss = 1.963503\n",
      "2024-12-08 22:40:47.686000: I runner.py:310] Step = 33300 ; steps/s = 1.62, tokens/s = 44232 (44232 target) ; Learning rate = 0.000484 ; Loss = 1.987929\n",
      "2024-12-08 22:41:49.144000: I runner.py:310] Step = 33400 ; steps/s = 1.63, tokens/s = 43893 (43893 target) ; Learning rate = 0.000484 ; Loss = 2.008533\n",
      "2024-12-08 22:42:50.692000: I runner.py:310] Step = 33500 ; steps/s = 1.62, tokens/s = 44142 (44142 target) ; Learning rate = 0.000483 ; Loss = 1.955619\n",
      "2024-12-08 22:43:52.434000: I runner.py:310] Step = 33600 ; steps/s = 1.62, tokens/s = 44197 (44197 target) ; Learning rate = 0.000482 ; Loss = 1.998023\n",
      "2024-12-08 22:44:54.076000: I runner.py:310] Step = 33700 ; steps/s = 1.62, tokens/s = 44276 (44276 target) ; Learning rate = 0.000481 ; Loss = 2.013862\n",
      "2024-12-08 22:45:55.436000: I runner.py:310] Step = 33800 ; steps/s = 1.63, tokens/s = 43729 (43729 target) ; Learning rate = 0.000481 ; Loss = 2.022109\n",
      "2024-12-08 22:46:57.197000: I runner.py:310] Step = 33900 ; steps/s = 1.62, tokens/s = 44182 (44182 target) ; Learning rate = 0.000480 ; Loss = 1.966711\n",
      "2024-12-08 22:47:58.905000: I runner.py:310] Step = 34000 ; steps/s = 1.62, tokens/s = 44235 (44235 target) ; Learning rate = 0.000479 ; Loss = 1.953948\n",
      "2024-12-08 22:49:00.207000: I runner.py:310] Step = 34100 ; steps/s = 1.63, tokens/s = 43788 (43788 target) ; Learning rate = 0.000479 ; Loss = 1.957417\n",
      "2024-12-08 22:50:01.941000: I runner.py:310] Step = 34200 ; steps/s = 1.62, tokens/s = 44215 (44215 target) ; Learning rate = 0.000478 ; Loss = 1.965013\n",
      "2024-12-08 22:51:03.610000: I runner.py:310] Step = 34300 ; steps/s = 1.62, tokens/s = 44250 (44250 target) ; Learning rate = 0.000477 ; Loss = 1.985934\n",
      "2024-12-08 22:52:05.344000: I runner.py:310] Step = 34400 ; steps/s = 1.62, tokens/s = 44208 (44208 target) ; Learning rate = 0.000477 ; Loss = 2.009814\n",
      "2024-12-08 22:53:06.666000: I runner.py:310] Step = 34500 ; steps/s = 1.63, tokens/s = 43763 (43763 target) ; Learning rate = 0.000476 ; Loss = 1.933867\n",
      "2024-12-08 22:54:08.455000: I runner.py:310] Step = 34600 ; steps/s = 1.62, tokens/s = 44179 (44179 target) ; Learning rate = 0.000475 ; Loss = 1.974503\n",
      "2024-12-08 22:55:10.230000: I runner.py:310] Step = 34700 ; steps/s = 1.62, tokens/s = 44169 (44169 target) ; Learning rate = 0.000474 ; Loss = 1.994149\n",
      "2024-12-08 22:56:11.508000: I runner.py:310] Step = 34800 ; steps/s = 1.63, tokens/s = 43813 (43813 target) ; Learning rate = 0.000474 ; Loss = 1.985598\n",
      "2024-12-08 22:57:13.234000: I runner.py:310] Step = 34900 ; steps/s = 1.62, tokens/s = 44214 (44214 target) ; Learning rate = 0.000473 ; Loss = 1.986185\n",
      "2024-12-08 22:58:14.956000: I runner.py:310] Step = 35000 ; steps/s = 1.62, tokens/s = 44218 (44218 target) ; Learning rate = 0.000472 ; Loss = 1.961057\n",
      "2024-12-08 22:58:14.957000: I training.py:192] Running evaluation for step 35000\n",
      "2024-12-08 23:01:01.115000: I training.py:192] Evaluation result for step 35000: loss = 2.260981 ; perplexity = 9.592496\n",
      "2024-12-08 23:02:02.756000: I runner.py:310] Step = 35100 ; steps/s = 1.62, tokens/s = 44301 (44301 target) ; Learning rate = 0.000472 ; Loss = 1.976962\n",
      "2024-12-08 23:03:04.089000: I runner.py:310] Step = 35200 ; steps/s = 1.63, tokens/s = 43745 (43745 target) ; Learning rate = 0.000471 ; Loss = 2.012777\n",
      "2024-12-08 23:04:05.862000: I runner.py:310] Step = 35300 ; steps/s = 1.62, tokens/s = 44163 (44163 target) ; Learning rate = 0.000470 ; Loss = 1.950391\n",
      "2024-12-08 23:05:07.618000: I runner.py:310] Step = 35400 ; steps/s = 1.62, tokens/s = 44222 (44222 target) ; Learning rate = 0.000470 ; Loss = 1.965254\n",
      "2024-12-08 23:06:08.965000: I runner.py:310] Step = 35500 ; steps/s = 1.63, tokens/s = 43744 (43744 target) ; Learning rate = 0.000469 ; Loss = 1.993383\n",
      "2024-12-08 23:07:10.707000: I runner.py:310] Step = 35600 ; steps/s = 1.62, tokens/s = 44199 (44199 target) ; Learning rate = 0.000468 ; Loss = 1.981269\n",
      "2024-12-08 23:08:12.416000: I runner.py:310] Step = 35700 ; steps/s = 1.62, tokens/s = 44219 (44219 target) ; Learning rate = 0.000468 ; Loss = 1.961262\n",
      "2024-12-08 23:09:14.118000: I runner.py:310] Step = 35800 ; steps/s = 1.62, tokens/s = 44229 (44229 target) ; Learning rate = 0.000467 ; Loss = 1.975412\n",
      "2024-12-08 23:10:15.412000: I runner.py:310] Step = 35900 ; steps/s = 1.63, tokens/s = 43802 (43802 target) ; Learning rate = 0.000466 ; Loss = 2.026723\n",
      "2024-12-08 23:11:17.197000: I runner.py:310] Step = 36000 ; steps/s = 1.62, tokens/s = 44179 (44179 target) ; Learning rate = 0.000466 ; Loss = 1.938811\n",
      "2024-12-08 23:12:18.899000: I runner.py:310] Step = 36100 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000465 ; Loss = 1.965258\n",
      "2024-12-08 23:13:20.226000: I runner.py:310] Step = 36200 ; steps/s = 1.63, tokens/s = 43767 (43767 target) ; Learning rate = 0.000465 ; Loss = 1.959646\n",
      "2024-12-08 23:14:21.945000: I runner.py:310] Step = 36300 ; steps/s = 1.62, tokens/s = 44231 (44231 target) ; Learning rate = 0.000464 ; Loss = 1.965350\n",
      "2024-12-08 23:15:23.721000: I runner.py:310] Step = 36400 ; steps/s = 1.62, tokens/s = 44181 (44181 target) ; Learning rate = 0.000463 ; Loss = 1.973402\n",
      "2024-12-08 23:16:25.491000: I runner.py:310] Step = 36500 ; steps/s = 1.62, tokens/s = 44161 (44161 target) ; Learning rate = 0.000463 ; Loss = 2.011826\n",
      "2024-12-08 23:17:26.882000: I runner.py:310] Step = 36600 ; steps/s = 1.63, tokens/s = 43731 (43731 target) ; Learning rate = 0.000462 ; Loss = 1.995794\n",
      "2024-12-08 23:18:28.653000: I runner.py:310] Step = 36700 ; steps/s = 1.62, tokens/s = 44180 (44180 target) ; Learning rate = 0.000461 ; Loss = 1.944775\n",
      "2024-12-08 23:19:30.402000: I runner.py:310] Step = 36800 ; steps/s = 1.62, tokens/s = 44194 (44194 target) ; Learning rate = 0.000461 ; Loss = 1.965109\n",
      "2024-12-08 23:20:31.756000: I runner.py:310] Step = 36900 ; steps/s = 1.63, tokens/s = 43743 (43743 target) ; Learning rate = 0.000460 ; Loss = 1.975465\n",
      "2024-12-08 23:21:33.516000: I runner.py:310] Step = 37000 ; steps/s = 1.62, tokens/s = 44195 (44195 target) ; Learning rate = 0.000460 ; Loss = 1.962090\n",
      "2024-12-08 23:22:35.193000: I runner.py:310] Step = 37100 ; steps/s = 1.62, tokens/s = 44254 (44254 target) ; Learning rate = 0.000459 ; Loss = 1.952928\n",
      "2024-12-08 23:23:36.917000: I runner.py:310] Step = 37200 ; steps/s = 1.62, tokens/s = 44209 (44209 target) ; Learning rate = 0.000458 ; Loss = 1.963025\n",
      "2024-12-08 23:24:38.280000: I runner.py:310] Step = 37300 ; steps/s = 1.63, tokens/s = 43746 (43746 target) ; Learning rate = 0.000458 ; Loss = 1.994873\n",
      "2024-12-08 23:25:40.097000: I runner.py:310] Step = 37400 ; steps/s = 1.62, tokens/s = 44144 (44144 target) ; Learning rate = 0.000457 ; Loss = 1.937793\n",
      "2024-12-08 23:26:41.824000: I runner.py:310] Step = 37500 ; steps/s = 1.62, tokens/s = 44211 (44211 target) ; Learning rate = 0.000456 ; Loss = 1.965088\n",
      "2024-12-08 23:27:43.115000: I runner.py:310] Step = 37600 ; steps/s = 1.63, tokens/s = 43798 (43798 target) ; Learning rate = 0.000456 ; Loss = 1.975310\n",
      "2024-12-08 23:28:44.861000: I runner.py:310] Step = 37700 ; steps/s = 1.62, tokens/s = 44201 (44201 target) ; Learning rate = 0.000455 ; Loss = 1.949916\n",
      "2024-12-08 23:29:46.623000: I runner.py:310] Step = 37800 ; steps/s = 1.62, tokens/s = 44187 (44187 target) ; Learning rate = 0.000455 ; Loss = 1.957282\n",
      "2024-12-08 23:30:48.303000: I runner.py:310] Step = 37900 ; steps/s = 1.62, tokens/s = 44250 (44250 target) ; Learning rate = 0.000454 ; Loss = 1.946867\n",
      "2024-12-08 23:31:49.629000: I runner.py:310] Step = 38000 ; steps/s = 1.63, tokens/s = 43765 (43765 target) ; Learning rate = 0.000453 ; Loss = 1.995773\n",
      "2024-12-08 23:32:51.334000: I runner.py:310] Step = 38100 ; steps/s = 1.62, tokens/s = 44243 (44243 target) ; Learning rate = 0.000453 ; Loss = 1.933801\n",
      "2024-12-08 23:33:53.109000: I runner.py:310] Step = 38200 ; steps/s = 1.62, tokens/s = 44164 (44164 target) ; Learning rate = 0.000452 ; Loss = 1.941484\n",
      "2024-12-08 23:34:54.422000: I runner.py:310] Step = 38300 ; steps/s = 1.63, tokens/s = 43777 (43777 target) ; Learning rate = 0.000452 ; Loss = 1.968626\n",
      "2024-12-08 23:35:56.097000: I runner.py:310] Step = 38400 ; steps/s = 1.62, tokens/s = 44259 (44259 target) ; Learning rate = 0.000451 ; Loss = 1.949057\n",
      "2024-12-08 23:36:57.825000: I runner.py:310] Step = 38500 ; steps/s = 1.62, tokens/s = 44205 (44205 target) ; Learning rate = 0.000450 ; Loss = 1.950553\n",
      "2024-12-08 23:37:59.600000: I runner.py:310] Step = 38600 ; steps/s = 1.62, tokens/s = 44179 (44179 target) ; Learning rate = 0.000450 ; Loss = 1.947374\n",
      "2024-12-08 23:39:00.936000: I runner.py:310] Step = 38700 ; steps/s = 1.63, tokens/s = 43782 (43782 target) ; Learning rate = 0.000449 ; Loss = 1.993131\n",
      "2024-12-08 23:40:02.577000: I runner.py:310] Step = 38800 ; steps/s = 1.62, tokens/s = 44269 (44269 target) ; Learning rate = 0.000449 ; Loss = 1.950273\n",
      "2024-12-08 23:41:04.329000: I runner.py:310] Step = 38900 ; steps/s = 1.62, tokens/s = 44190 (44190 target) ; Learning rate = 0.000448 ; Loss = 1.947012\n",
      "2024-12-08 23:42:05.708000: I runner.py:310] Step = 39000 ; steps/s = 1.63, tokens/s = 43726 (43726 target) ; Learning rate = 0.000448 ; Loss = 1.973242\n",
      "2024-12-08 23:43:07.445000: I runner.py:310] Step = 39100 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000447 ; Loss = 1.933407\n",
      "2024-12-08 23:44:09.181000: I runner.py:310] Step = 39200 ; steps/s = 1.62, tokens/s = 44213 (44213 target) ; Learning rate = 0.000446 ; Loss = 1.934670\n",
      "2024-12-08 23:45:10.871000: I runner.py:310] Step = 39300 ; steps/s = 1.62, tokens/s = 44238 (44238 target) ; Learning rate = 0.000446 ; Loss = 1.944593\n",
      "2024-12-08 23:46:12.197000: I runner.py:310] Step = 39400 ; steps/s = 1.63, tokens/s = 43772 (43772 target) ; Learning rate = 0.000445 ; Loss = 1.971261\n",
      "2024-12-08 23:47:13.970000: I runner.py:310] Step = 39500 ; steps/s = 1.62, tokens/s = 44187 (44187 target) ; Learning rate = 0.000445 ; Loss = 1.944048\n",
      "2024-12-08 23:48:15.743000: I runner.py:310] Step = 39600 ; steps/s = 1.62, tokens/s = 44171 (44171 target) ; Learning rate = 0.000444 ; Loss = 1.944211\n",
      "2024-12-08 23:49:17.077000: I runner.py:310] Step = 39700 ; steps/s = 1.63, tokens/s = 43768 (43768 target) ; Learning rate = 0.000444 ; Loss = 1.903541\n",
      "2024-12-08 23:50:18.808000: I runner.py:310] Step = 39800 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000443 ; Loss = 1.939634\n",
      "2024-12-08 23:51:20.573000: I runner.py:310] Step = 39900 ; steps/s = 1.62, tokens/s = 44178 (44178 target) ; Learning rate = 0.000442 ; Loss = 1.967107\n",
      "2024-12-08 23:52:22.314000: I runner.py:310] Step = 40000 ; steps/s = 1.62, tokens/s = 44205 (44205 target) ; Learning rate = 0.000442 ; Loss = 1.974193\n",
      "2024-12-08 23:52:24.247000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-40000\n",
      "2024-12-08 23:52:24.247000: I training.py:192] Running evaluation for step 40000\n",
      "2024-12-08 23:55:03.104000: I training.py:192] Evaluation result for step 40000: loss = 2.286690 ; perplexity = 9.842301\n",
      "2024-12-08 23:56:04.292000: I runner.py:310] Step = 40100 ; steps/s = 1.63, tokens/s = 43876 (43876 target) ; Learning rate = 0.000441 ; Loss = 1.909971\n",
      "2024-12-08 23:57:06.028000: I runner.py:310] Step = 40200 ; steps/s = 1.62, tokens/s = 44213 (44213 target) ; Learning rate = 0.000441 ; Loss = 1.957553\n",
      "2024-12-08 23:58:07.796000: I runner.py:310] Step = 40300 ; steps/s = 1.62, tokens/s = 44185 (44185 target) ; Learning rate = 0.000440 ; Loss = 1.977783\n",
      "2024-12-08 23:59:09.162000: I runner.py:310] Step = 40400 ; steps/s = 1.63, tokens/s = 43737 (43737 target) ; Learning rate = 0.000440 ; Loss = 1.911176\n",
      "2024-12-09 00:00:10.937000: I runner.py:310] Step = 40500 ; steps/s = 1.62, tokens/s = 44179 (44179 target) ; Learning rate = 0.000439 ; Loss = 1.946909\n",
      "2024-12-09 00:01:12.658000: I runner.py:310] Step = 40600 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000439 ; Loss = 1.964961\n",
      "2024-12-09 00:02:14.441000: I runner.py:310] Step = 40700 ; steps/s = 1.62, tokens/s = 44172 (44172 target) ; Learning rate = 0.000438 ; Loss = 1.986082\n",
      "2024-12-09 00:03:15.789000: I runner.py:310] Step = 40800 ; steps/s = 1.63, tokens/s = 43761 (43761 target) ; Learning rate = 0.000438 ; Loss = 1.940651\n",
      "2024-12-09 00:04:17.465000: I runner.py:310] Step = 40900 ; steps/s = 1.62, tokens/s = 44253 (44253 target) ; Learning rate = 0.000437 ; Loss = 1.918125\n",
      "2024-12-09 00:05:19.279000: I runner.py:310] Step = 41000 ; steps/s = 1.62, tokens/s = 44137 (44137 target) ; Learning rate = 0.000437 ; Loss = 1.937216\n",
      "2024-12-09 00:06:20.631000: I runner.py:310] Step = 41100 ; steps/s = 1.63, tokens/s = 43757 (43757 target) ; Learning rate = 0.000436 ; Loss = 1.951472\n",
      "2024-12-09 00:07:22.355000: I runner.py:310] Step = 41200 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000435 ; Loss = 1.931881\n",
      "2024-12-09 00:08:24.071000: I runner.py:310] Step = 41300 ; steps/s = 1.62, tokens/s = 44226 (44226 target) ; Learning rate = 0.000435 ; Loss = 1.925713\n",
      "2024-12-09 00:09:25.806000: I runner.py:310] Step = 41400 ; steps/s = 1.62, tokens/s = 44206 (44206 target) ; Learning rate = 0.000434 ; Loss = 1.935037\n",
      "2024-12-09 00:10:27.218000: I runner.py:310] Step = 41500 ; steps/s = 1.63, tokens/s = 43726 (43726 target) ; Learning rate = 0.000434 ; Loss = 1.958724\n",
      "2024-12-09 00:11:28.993000: I runner.py:310] Step = 41600 ; steps/s = 1.62, tokens/s = 44180 (44180 target) ; Learning rate = 0.000433 ; Loss = 1.940591\n",
      "2024-12-09 00:12:30.719000: I runner.py:310] Step = 41700 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000433 ; Loss = 1.950832\n",
      "2024-12-09 00:13:32.043000: I runner.py:310] Step = 41800 ; steps/s = 1.63, tokens/s = 43765 (43765 target) ; Learning rate = 0.000432 ; Loss = 1.971392\n",
      "2024-12-09 00:14:33.755000: I runner.py:310] Step = 41900 ; steps/s = 1.62, tokens/s = 44223 (44223 target) ; Learning rate = 0.000432 ; Loss = 1.913739\n",
      "2024-12-09 00:15:35.467000: I runner.py:310] Step = 42000 ; steps/s = 1.62, tokens/s = 44220 (44220 target) ; Learning rate = 0.000431 ; Loss = 1.921180\n",
      "2024-12-09 00:16:36.824000: I runner.py:310] Step = 42100 ; steps/s = 1.63, tokens/s = 43750 (43750 target) ; Learning rate = 0.000431 ; Loss = 1.939060\n",
      "2024-12-09 00:17:38.602000: I runner.py:310] Step = 42200 ; steps/s = 1.62, tokens/s = 44161 (44161 target) ; Learning rate = 0.000430 ; Loss = 1.945997\n",
      "2024-12-09 00:18:40.362000: I runner.py:310] Step = 42300 ; steps/s = 1.62, tokens/s = 44200 (44200 target) ; Learning rate = 0.000430 ; Loss = 1.933210\n",
      "2024-12-09 00:19:42.084000: I runner.py:310] Step = 42400 ; steps/s = 1.62, tokens/s = 44217 (44217 target) ; Learning rate = 0.000429 ; Loss = 1.926022\n",
      "2024-12-09 00:20:43.454000: I runner.py:310] Step = 42500 ; steps/s = 1.63, tokens/s = 43733 (43733 target) ; Learning rate = 0.000429 ; Loss = 1.905660\n",
      "2024-12-09 00:21:45.171000: I runner.py:310] Step = 42600 ; steps/s = 1.62, tokens/s = 44230 (44230 target) ; Learning rate = 0.000428 ; Loss = 1.929033\n",
      "2024-12-09 00:22:46.941000: I runner.py:310] Step = 42700 ; steps/s = 1.62, tokens/s = 44172 (44172 target) ; Learning rate = 0.000428 ; Loss = 1.941820\n",
      "2024-12-09 00:23:48.285000: I runner.py:310] Step = 42800 ; steps/s = 1.63, tokens/s = 43762 (43762 target) ; Learning rate = 0.000427 ; Loss = 1.923801\n",
      "2024-12-09 00:24:50.049000: I runner.py:310] Step = 42900 ; steps/s = 1.62, tokens/s = 44196 (44196 target) ; Learning rate = 0.000427 ; Loss = 1.940180\n",
      "2024-12-09 00:25:51.819000: I runner.py:310] Step = 43000 ; steps/s = 1.62, tokens/s = 44182 (44182 target) ; Learning rate = 0.000426 ; Loss = 1.915156\n",
      "2024-12-09 00:26:53.529000: I runner.py:310] Step = 43100 ; steps/s = 1.62, tokens/s = 44230 (44230 target) ; Learning rate = 0.000426 ; Loss = 1.914086\n",
      "2024-12-09 00:27:54.845000: I runner.py:310] Step = 43200 ; steps/s = 1.63, tokens/s = 43757 (43757 target) ; Learning rate = 0.000425 ; Loss = 1.959634\n",
      "2024-12-09 00:28:56.606000: I runner.py:310] Step = 43300 ; steps/s = 1.62, tokens/s = 44192 (44192 target) ; Learning rate = 0.000425 ; Loss = 1.899408\n",
      "2024-12-09 00:29:58.360000: I runner.py:310] Step = 43400 ; steps/s = 1.62, tokens/s = 44206 (44206 target) ; Learning rate = 0.000424 ; Loss = 1.912167\n",
      "2024-12-09 00:30:59.691000: I runner.py:310] Step = 43500 ; steps/s = 1.63, tokens/s = 43759 (43759 target) ; Learning rate = 0.000424 ; Loss = 1.929303\n",
      "2024-12-09 00:32:01.465000: I runner.py:310] Step = 43600 ; steps/s = 1.62, tokens/s = 44181 (44181 target) ; Learning rate = 0.000423 ; Loss = 1.912383\n",
      "2024-12-09 00:33:03.113000: I runner.py:310] Step = 43700 ; steps/s = 1.62, tokens/s = 44270 (44270 target) ; Learning rate = 0.000423 ; Loss = 1.946693\n",
      "2024-12-09 00:34:04.869000: I runner.py:310] Step = 43800 ; steps/s = 1.62, tokens/s = 44197 (44197 target) ; Learning rate = 0.000422 ; Loss = 1.934946\n",
      "2024-12-09 00:35:06.284000: I runner.py:310] Step = 43900 ; steps/s = 1.63, tokens/s = 43698 (43698 target) ; Learning rate = 0.000422 ; Loss = 1.945022\n",
      "2024-12-09 00:36:08.018000: I runner.py:310] Step = 44000 ; steps/s = 1.62, tokens/s = 44207 (44207 target) ; Learning rate = 0.000421 ; Loss = 1.903666\n",
      "2024-12-09 00:37:09.748000: I runner.py:310] Step = 44100 ; steps/s = 1.62, tokens/s = 44221 (44221 target) ; Learning rate = 0.000421 ; Loss = 1.913183\n",
      "2024-12-09 00:38:11.121000: I runner.py:310] Step = 44200 ; steps/s = 1.63, tokens/s = 43733 (43733 target) ; Learning rate = 0.000420 ; Loss = 1.915906\n",
      "2024-12-09 00:39:12.833000: I runner.py:310] Step = 44300 ; steps/s = 1.62, tokens/s = 44226 (44226 target) ; Learning rate = 0.000420 ; Loss = 1.919632\n",
      "2024-12-09 00:40:14.674000: I runner.py:310] Step = 44400 ; steps/s = 1.62, tokens/s = 44142 (44142 target) ; Learning rate = 0.000419 ; Loss = 1.913736\n",
      "2024-12-09 00:41:16.466000: I runner.py:310] Step = 44500 ; steps/s = 1.62, tokens/s = 44146 (44146 target) ; Learning rate = 0.000419 ; Loss = 1.925950\n",
      "2024-12-09 00:42:17.781000: I runner.py:310] Step = 44600 ; steps/s = 1.63, tokens/s = 43781 (43781 target) ; Learning rate = 0.000419 ; Loss = 1.936651\n",
      "2024-12-09 00:43:19.602000: I runner.py:310] Step = 44700 ; steps/s = 1.62, tokens/s = 44158 (44158 target) ; Learning rate = 0.000418 ; Loss = 1.900599\n",
      "2024-12-09 00:44:21.335000: I runner.py:310] Step = 44800 ; steps/s = 1.62, tokens/s = 44213 (44213 target) ; Learning rate = 0.000418 ; Loss = 1.918425\n",
      "2024-12-09 00:45:22.700000: I runner.py:310] Step = 44900 ; steps/s = 1.63, tokens/s = 43730 (43730 target) ; Learning rate = 0.000417 ; Loss = 1.928313\n",
      "2024-12-09 00:46:24.443000: I runner.py:310] Step = 45000 ; steps/s = 1.62, tokens/s = 44200 (44200 target) ; Learning rate = 0.000417 ; Loss = 1.908862\n",
      "2024-12-09 00:46:24.444000: I training.py:192] Running evaluation for step 45000\n",
      "2024-12-09 00:49:00.393000: I training.py:192] Evaluation result for step 45000: loss = 2.318952 ; perplexity = 10.165019\n",
      "2024-12-09 00:50:01.949000: I runner.py:310] Step = 45100 ; steps/s = 1.62, tokens/s = 44349 (44349 target) ; Learning rate = 0.000416 ; Loss = 1.903879\n",
      "2024-12-09 00:51:03.713000: I runner.py:310] Step = 45200 ; steps/s = 1.62, tokens/s = 44182 (44182 target) ; Learning rate = 0.000416 ; Loss = 1.907015\n",
      "2024-12-09 00:52:05.028000: I runner.py:310] Step = 45300 ; steps/s = 1.63, tokens/s = 43779 (43779 target) ; Learning rate = 0.000415 ; Loss = 1.946725\n",
      "2024-12-09 00:53:06.796000: I runner.py:310] Step = 45400 ; steps/s = 1.62, tokens/s = 44179 (44179 target) ; Learning rate = 0.000415 ; Loss = 1.903757\n",
      "2024-12-09 00:54:08.537000: I runner.py:310] Step = 45500 ; steps/s = 1.62, tokens/s = 44205 (44205 target) ; Learning rate = 0.000414 ; Loss = 1.911687\n",
      "2024-12-09 00:55:09.954000: I runner.py:310] Step = 45600 ; steps/s = 1.63, tokens/s = 43708 (43708 target) ; Learning rate = 0.000414 ; Loss = 1.910468\n",
      "2024-12-09 00:56:11.716000: I runner.py:310] Step = 45700 ; steps/s = 1.62, tokens/s = 44195 (44195 target) ; Learning rate = 0.000413 ; Loss = 1.910630\n",
      "2024-12-09 00:57:13.411000: I runner.py:310] Step = 45800 ; steps/s = 1.62, tokens/s = 44239 (44239 target) ; Learning rate = 0.000413 ; Loss = 1.940679\n",
      "2024-12-09 00:58:15.153000: I runner.py:310] Step = 45900 ; steps/s = 1.62, tokens/s = 44193 (44193 target) ; Learning rate = 0.000413 ; Loss = 1.945286\n",
      "2024-12-09 00:59:16.559000: I runner.py:310] Step = 46000 ; steps/s = 1.63, tokens/s = 43718 (43718 target) ; Learning rate = 0.000412 ; Loss = 1.890882\n",
      "2024-12-09 01:00:18.229000: I runner.py:310] Step = 46100 ; steps/s = 1.62, tokens/s = 44254 (44254 target) ; Learning rate = 0.000412 ; Loss = 1.925044\n",
      "2024-12-09 01:01:19.937000: I runner.py:310] Step = 46200 ; steps/s = 1.62, tokens/s = 44229 (44229 target) ; Learning rate = 0.000411 ; Loss = 1.916703\n",
      "2024-12-09 01:02:21.326000: I runner.py:310] Step = 46300 ; steps/s = 1.63, tokens/s = 43725 (43725 target) ; Learning rate = 0.000411 ; Loss = 1.903207\n",
      "2024-12-09 01:03:23.064000: I runner.py:310] Step = 46400 ; steps/s = 1.62, tokens/s = 44195 (44195 target) ; Learning rate = 0.000410 ; Loss = 1.918596\n",
      "2024-12-09 01:04:24.821000: I runner.py:310] Step = 46500 ; steps/s = 1.62, tokens/s = 44188 (44188 target) ; Learning rate = 0.000410 ; Loss = 1.922395\n",
      "2024-12-09 01:05:26.574000: I runner.py:310] Step = 46600 ; steps/s = 1.62, tokens/s = 44193 (44193 target) ; Learning rate = 0.000409 ; Loss = 1.926486\n",
      "2024-12-09 01:06:27.911000: I runner.py:310] Step = 46700 ; steps/s = 1.63, tokens/s = 43775 (43775 target) ; Learning rate = 0.000409 ; Loss = 1.929830\n",
      "2024-12-09 01:07:29.638000: I runner.py:310] Step = 46800 ; steps/s = 1.62, tokens/s = 44200 (44200 target) ; Learning rate = 0.000409 ; Loss = 1.899219\n",
      "2024-12-09 01:08:31.353000: I runner.py:310] Step = 46900 ; steps/s = 1.62, tokens/s = 44222 (44222 target) ; Learning rate = 0.000408 ; Loss = 1.911445\n",
      "2024-12-09 01:09:32.759000: I runner.py:310] Step = 47000 ; steps/s = 1.63, tokens/s = 43726 (43726 target) ; Learning rate = 0.000408 ; Loss = 1.902802\n",
      "2024-12-09 01:10:34.504000: I runner.py:310] Step = 47100 ; steps/s = 1.62, tokens/s = 44185 (44185 target) ; Learning rate = 0.000407 ; Loss = 1.915548\n",
      "2024-12-09 01:11:36.258000: I runner.py:310] Step = 47200 ; steps/s = 1.62, tokens/s = 44198 (44198 target) ; Learning rate = 0.000407 ; Loss = 1.926984\n",
      "2024-12-09 01:12:37.927000: I runner.py:310] Step = 47300 ; steps/s = 1.62, tokens/s = 44256 (44256 target) ; Learning rate = 0.000406 ; Loss = 1.916635\n",
      "2024-12-09 01:13:39.257000: I runner.py:310] Step = 47400 ; steps/s = 1.63, tokens/s = 43773 (43773 target) ; Learning rate = 0.000406 ; Loss = 1.877272\n",
      "2024-12-09 01:14:41.004000: I runner.py:310] Step = 47500 ; steps/s = 1.62, tokens/s = 44209 (44209 target) ; Learning rate = 0.000406 ; Loss = 1.904988\n",
      "2024-12-09 01:15:42.737000: I runner.py:310] Step = 47600 ; steps/s = 1.62, tokens/s = 44193 (44193 target) ; Learning rate = 0.000405 ; Loss = 1.920866\n",
      "2024-12-09 01:16:44.134000: I runner.py:310] Step = 47700 ; steps/s = 1.63, tokens/s = 43722 (43722 target) ; Learning rate = 0.000405 ; Loss = 1.935215\n",
      "2024-12-09 01:17:45.796000: I runner.py:310] Step = 47800 ; steps/s = 1.62, tokens/s = 44253 (44253 target) ; Learning rate = 0.000404 ; Loss = 1.912895\n",
      "2024-12-09 01:18:47.526000: I runner.py:310] Step = 47900 ; steps/s = 1.62, tokens/s = 44218 (44218 target) ; Learning rate = 0.000404 ; Loss = 1.902467\n",
      "2024-12-09 01:19:49.248000: I runner.py:310] Step = 48000 ; steps/s = 1.62, tokens/s = 44219 (44219 target) ; Learning rate = 0.000403 ; Loss = 1.896767\n",
      "2024-12-09 01:20:50.592000: I runner.py:310] Step = 48100 ; steps/s = 1.63, tokens/s = 43759 (43759 target) ; Learning rate = 0.000403 ; Loss = 1.867387\n",
      "2024-12-09 01:21:52.303000: I runner.py:310] Step = 48200 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000403 ; Loss = 1.921154\n",
      "2024-12-09 01:22:53.973000: I runner.py:310] Step = 48300 ; steps/s = 1.62, tokens/s = 44255 (44255 target) ; Learning rate = 0.000402 ; Loss = 1.948414\n",
      "2024-12-09 01:23:55.297000: I runner.py:310] Step = 48400 ; steps/s = 1.63, tokens/s = 43771 (43771 target) ; Learning rate = 0.000402 ; Loss = 1.908156\n",
      "2024-12-09 01:24:57.016000: I runner.py:310] Step = 48500 ; steps/s = 1.62, tokens/s = 44224 (44224 target) ; Learning rate = 0.000401 ; Loss = 1.896960\n",
      "2024-12-09 01:25:58.741000: I runner.py:310] Step = 48600 ; steps/s = 1.62, tokens/s = 44216 (44216 target) ; Learning rate = 0.000401 ; Loss = 1.895777\n",
      "2024-12-09 01:27:00.473000: I runner.py:310] Step = 48700 ; steps/s = 1.62, tokens/s = 44208 (44208 target) ; Learning rate = 0.000401 ; Loss = 1.894448\n",
      "2024-12-09 01:28:01.774000: I runner.py:310] Step = 48800 ; steps/s = 1.63, tokens/s = 43796 (43796 target) ; Learning rate = 0.000400 ; Loss = 1.875054\n",
      "2024-12-09 01:29:03.416000: I runner.py:310] Step = 48900 ; steps/s = 1.62, tokens/s = 44273 (44273 target) ; Learning rate = 0.000400 ; Loss = 1.909570\n",
      "2024-12-09 01:30:05.132000: I runner.py:310] Step = 49000 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000399 ; Loss = 1.930980\n",
      "2024-12-09 01:31:06.498000: I runner.py:310] Step = 49100 ; steps/s = 1.63, tokens/s = 43745 (43745 target) ; Learning rate = 0.000399 ; Loss = 1.890130\n",
      "2024-12-09 01:32:08.254000: I runner.py:310] Step = 49200 ; steps/s = 1.62, tokens/s = 44185 (44185 target) ; Learning rate = 0.000398 ; Loss = 1.908754\n",
      "2024-12-09 01:33:09.967000: I runner.py:310] Step = 49300 ; steps/s = 1.62, tokens/s = 44213 (44213 target) ; Learning rate = 0.000398 ; Loss = 1.921457\n",
      "2024-12-09 01:34:11.740000: I runner.py:310] Step = 49400 ; steps/s = 1.62, tokens/s = 44191 (44191 target) ; Learning rate = 0.000398 ; Loss = 1.937132\n",
      "2024-12-09 01:35:13.019000: I runner.py:310] Step = 49500 ; steps/s = 1.63, tokens/s = 43811 (43811 target) ; Learning rate = 0.000397 ; Loss = 1.896956\n",
      "2024-12-09 01:36:14.800000: I runner.py:310] Step = 49600 ; steps/s = 1.62, tokens/s = 44170 (44170 target) ; Learning rate = 0.000397 ; Loss = 1.885989\n",
      "2024-12-09 01:37:16.561000: I runner.py:310] Step = 49700 ; steps/s = 1.62, tokens/s = 44178 (44178 target) ; Learning rate = 0.000396 ; Loss = 1.892539\n",
      "2024-12-09 01:38:17.902000: I runner.py:310] Step = 49800 ; steps/s = 1.63, tokens/s = 43766 (43766 target) ; Learning rate = 0.000396 ; Loss = 1.915992\n",
      "2024-12-09 01:39:19.652000: I runner.py:310] Step = 49900 ; steps/s = 1.62, tokens/s = 44201 (44201 target) ; Learning rate = 0.000396 ; Loss = 1.897785\n",
      "2024-12-09 01:40:21.426000: I runner.py:310] Step = 50000 ; steps/s = 1.62, tokens/s = 44176 (44176 target) ; Learning rate = 0.000395 ; Loss = 1.887457\n",
      "2024-12-09 01:40:23.355000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-50000\n",
      "2024-12-09 01:40:23.355000: I training.py:192] Running evaluation for step 50000\n",
      "2024-12-09 01:43:08.395000: I training.py:192] Evaluation result for step 50000: loss = 2.353482 ; perplexity = 10.522144\n",
      "2024-12-09 01:44:09.752000: I runner.py:310] Step = 50100 ; steps/s = 1.63, tokens/s = 44078 (44078 target) ; Learning rate = 0.000395 ; Loss = 1.895224\n",
      "2024-12-09 01:45:11.355000: I runner.py:310] Step = 50200 ; steps/s = 1.62, tokens/s = 44000 (44000 target) ; Learning rate = 0.000394 ; Loss = 1.881363\n",
      "2024-12-09 01:46:13.056000: I runner.py:310] Step = 50300 ; steps/s = 1.62, tokens/s = 44231 (44231 target) ; Learning rate = 0.000394 ; Loss = 1.914571\n",
      "2024-12-09 01:47:14.875000: I runner.py:310] Step = 50400 ; steps/s = 1.62, tokens/s = 44141 (44141 target) ; Learning rate = 0.000394 ; Loss = 1.919593\n",
      "2024-12-09 01:48:16.203000: I runner.py:310] Step = 50500 ; steps/s = 1.63, tokens/s = 43759 (43759 target) ; Learning rate = 0.000393 ; Loss = 1.871816\n",
      "2024-12-09 01:49:17.988000: I runner.py:310] Step = 50600 ; steps/s = 1.62, tokens/s = 44177 (44177 target) ; Learning rate = 0.000393 ; Loss = 1.899393\n",
      "2024-12-09 01:50:19.684000: I runner.py:310] Step = 50700 ; steps/s = 1.62, tokens/s = 44238 (44238 target) ; Learning rate = 0.000393 ; Loss = 1.904365\n",
      "2024-12-09 01:51:21.096000: I runner.py:310] Step = 50800 ; steps/s = 1.63, tokens/s = 43703 (43703 target) ; Learning rate = 0.000392 ; Loss = 1.899050\n",
      "2024-12-09 01:52:22.822000: I runner.py:310] Step = 50900 ; steps/s = 1.62, tokens/s = 44207 (44207 target) ; Learning rate = 0.000392 ; Loss = 1.901046\n",
      "2024-12-09 01:53:24.565000: I runner.py:310] Step = 51000 ; steps/s = 1.62, tokens/s = 44201 (44201 target) ; Learning rate = 0.000391 ; Loss = 1.900461\n",
      "2024-12-09 01:54:26.247000: I runner.py:310] Step = 51100 ; steps/s = 1.62, tokens/s = 44253 (44253 target) ; Learning rate = 0.000391 ; Loss = 1.881314\n",
      "2024-12-09 01:55:27.596000: I runner.py:310] Step = 51200 ; steps/s = 1.63, tokens/s = 43755 (43755 target) ; Learning rate = 0.000391 ; Loss = 1.860294\n",
      "2024-12-09 01:56:29.421000: I runner.py:310] Step = 51300 ; steps/s = 1.62, tokens/s = 44149 (44149 target) ; Learning rate = 0.000390 ; Loss = 1.892839\n",
      "2024-12-09 01:57:31.188000: I runner.py:310] Step = 51400 ; steps/s = 1.62, tokens/s = 44191 (44191 target) ; Learning rate = 0.000390 ; Loss = 1.914972\n",
      "2024-12-09 01:58:32.552000: I runner.py:310] Step = 51500 ; steps/s = 1.63, tokens/s = 43726 (43726 target) ; Learning rate = 0.000389 ; Loss = 1.886471\n",
      "2024-12-09 01:59:34.379000: I runner.py:310] Step = 51600 ; steps/s = 1.62, tokens/s = 44141 (44141 target) ; Learning rate = 0.000389 ; Loss = 1.920256\n",
      "2024-12-09 02:00:36.125000: I runner.py:310] Step = 51700 ; steps/s = 1.62, tokens/s = 44205 (44205 target) ; Learning rate = 0.000389 ; Loss = 1.887473\n",
      "2024-12-09 02:01:37.916000: I runner.py:310] Step = 51800 ; steps/s = 1.62, tokens/s = 44174 (44174 target) ; Learning rate = 0.000388 ; Loss = 1.880206\n",
      "2024-12-09 02:02:39.336000: I runner.py:310] Step = 51900 ; steps/s = 1.63, tokens/s = 43703 (43703 target) ; Learning rate = 0.000388 ; Loss = 1.863316\n",
      "2024-12-09 02:03:41.062000: I runner.py:310] Step = 52000 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000388 ; Loss = 1.891955\n",
      "2024-12-09 02:04:42.846000: I runner.py:310] Step = 52100 ; steps/s = 1.62, tokens/s = 44176 (44176 target) ; Learning rate = 0.000387 ; Loss = 1.906193\n",
      "2024-12-09 02:05:44.239000: I runner.py:310] Step = 52200 ; steps/s = 1.63, tokens/s = 43723 (43723 target) ; Learning rate = 0.000387 ; Loss = 1.879017\n",
      "2024-12-09 02:06:45.950000: I runner.py:310] Step = 52300 ; steps/s = 1.62, tokens/s = 44223 (44223 target) ; Learning rate = 0.000386 ; Loss = 1.887355\n",
      "2024-12-09 02:07:47.678000: I runner.py:310] Step = 52400 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000386 ; Loss = 1.894834\n",
      "2024-12-09 02:08:49.436000: I runner.py:310] Step = 52500 ; steps/s = 1.62, tokens/s = 44187 (44187 target) ; Learning rate = 0.000386 ; Loss = 1.899711\n",
      "2024-12-09 02:09:50.820000: I runner.py:310] Step = 52600 ; steps/s = 1.63, tokens/s = 43714 (43714 target) ; Learning rate = 0.000385 ; Loss = 1.906292\n",
      "2024-12-09 02:10:52.537000: I runner.py:310] Step = 52700 ; steps/s = 1.62, tokens/s = 44213 (44213 target) ; Learning rate = 0.000385 ; Loss = 1.888657\n",
      "2024-12-09 02:11:54.274000: I runner.py:310] Step = 52800 ; steps/s = 1.62, tokens/s = 44219 (44219 target) ; Learning rate = 0.000385 ; Loss = 1.880283\n",
      "2024-12-09 02:12:55.673000: I runner.py:310] Step = 52900 ; steps/s = 1.63, tokens/s = 43722 (43722 target) ; Learning rate = 0.000384 ; Loss = 1.894704\n",
      "2024-12-09 02:13:57.517000: I runner.py:310] Step = 53000 ; steps/s = 1.62, tokens/s = 44143 (44143 target) ; Learning rate = 0.000384 ; Loss = 1.900199\n",
      "2024-12-09 02:14:59.289000: I runner.py:310] Step = 53100 ; steps/s = 1.62, tokens/s = 44185 (44185 target) ; Learning rate = 0.000384 ; Loss = 1.871831\n",
      "2024-12-09 02:16:01.021000: I runner.py:310] Step = 53200 ; steps/s = 1.62, tokens/s = 44189 (44189 target) ; Learning rate = 0.000383 ; Loss = 1.896575\n",
      "2024-12-09 02:17:02.405000: I runner.py:310] Step = 53300 ; steps/s = 1.63, tokens/s = 43735 (43735 target) ; Learning rate = 0.000383 ; Loss = 1.905810\n",
      "2024-12-09 02:18:04.056000: I runner.py:310] Step = 53400 ; steps/s = 1.62, tokens/s = 44269 (44269 target) ; Learning rate = 0.000382 ; Loss = 1.878533\n",
      "2024-12-09 02:19:05.807000: I runner.py:310] Step = 53500 ; steps/s = 1.62, tokens/s = 44184 (44184 target) ; Learning rate = 0.000382 ; Loss = 1.882655\n",
      "2024-12-09 02:20:07.162000: I runner.py:310] Step = 53600 ; steps/s = 1.63, tokens/s = 43751 (43751 target) ; Learning rate = 0.000382 ; Loss = 1.880496\n",
      "2024-12-09 02:21:08.913000: I runner.py:310] Step = 53700 ; steps/s = 1.62, tokens/s = 44188 (44188 target) ; Learning rate = 0.000381 ; Loss = 1.899115\n",
      "2024-12-09 02:22:10.675000: I runner.py:310] Step = 53800 ; steps/s = 1.62, tokens/s = 44183 (44183 target) ; Learning rate = 0.000381 ; Loss = 1.904656\n",
      "2024-12-09 02:23:12.467000: I runner.py:310] Step = 53900 ; steps/s = 1.62, tokens/s = 44179 (44179 target) ; Learning rate = 0.000381 ; Loss = 1.909093\n",
      "2024-12-09 02:24:13.779000: I runner.py:310] Step = 54000 ; steps/s = 1.63, tokens/s = 43773 (43773 target) ; Learning rate = 0.000380 ; Loss = 1.906812\n",
      "2024-12-09 02:25:15.535000: I runner.py:310] Step = 54100 ; steps/s = 1.62, tokens/s = 44192 (44192 target) ; Learning rate = 0.000380 ; Loss = 1.864278\n",
      "2024-12-09 02:26:17.293000: I runner.py:310] Step = 54200 ; steps/s = 1.62, tokens/s = 44192 (44192 target) ; Learning rate = 0.000380 ; Loss = 1.868367\n",
      "2024-12-09 02:27:18.618000: I runner.py:310] Step = 54300 ; steps/s = 1.63, tokens/s = 43788 (43788 target) ; Learning rate = 0.000379 ; Loss = 1.870167\n",
      "2024-12-09 02:28:20.362000: I runner.py:310] Step = 54400 ; steps/s = 1.62, tokens/s = 44198 (44198 target) ; Learning rate = 0.000379 ; Loss = 1.880394\n",
      "2024-12-09 02:29:22.097000: I runner.py:310] Step = 54500 ; steps/s = 1.62, tokens/s = 44203 (44203 target) ; Learning rate = 0.000379 ; Loss = 1.897754\n",
      "2024-12-09 02:30:23.882000: I runner.py:310] Step = 54600 ; steps/s = 1.62, tokens/s = 44176 (44176 target) ; Learning rate = 0.000378 ; Loss = 1.913165\n",
      "2024-12-09 02:31:25.236000: I runner.py:310] Step = 54700 ; steps/s = 1.63, tokens/s = 43747 (43747 target) ; Learning rate = 0.000378 ; Loss = 1.911748\n",
      "2024-12-09 02:32:27.002000: I runner.py:310] Step = 54800 ; steps/s = 1.62, tokens/s = 44188 (44188 target) ; Learning rate = 0.000378 ; Loss = 1.871333\n",
      "2024-12-09 02:33:28.780000: I runner.py:310] Step = 54900 ; steps/s = 1.62, tokens/s = 44174 (44174 target) ; Learning rate = 0.000377 ; Loss = 1.892981\n",
      "2024-12-09 02:34:30.085000: I runner.py:310] Step = 55000 ; steps/s = 1.63, tokens/s = 43782 (43782 target) ; Learning rate = 0.000377 ; Loss = 1.903965\n",
      "2024-12-09 02:34:30.086000: I training.py:192] Running evaluation for step 55000\n",
      "2024-12-09 02:37:09.501000: I training.py:192] Evaluation result for step 55000: loss = 2.360031 ; perplexity = 10.591279\n",
      "2024-12-09 02:38:11.016000: I runner.py:310] Step = 55100 ; steps/s = 1.63, tokens/s = 44378 (44378 target) ; Learning rate = 0.000377 ; Loss = 1.873597\n",
      "2024-12-09 02:39:12.696000: I runner.py:310] Step = 55200 ; steps/s = 1.62, tokens/s = 44250 (44250 target) ; Learning rate = 0.000376 ; Loss = 1.859399\n",
      "2024-12-09 02:40:14.403000: I runner.py:310] Step = 55300 ; steps/s = 1.62, tokens/s = 44226 (44226 target) ; Learning rate = 0.000376 ; Loss = 1.886361\n",
      "2024-12-09 02:41:15.753000: I runner.py:310] Step = 55400 ; steps/s = 1.63, tokens/s = 43746 (43746 target) ; Learning rate = 0.000376 ; Loss = 1.922228\n",
      "2024-12-09 02:42:17.448000: I runner.py:310] Step = 55500 ; steps/s = 1.62, tokens/s = 44255 (44255 target) ; Learning rate = 0.000375 ; Loss = 1.876478\n",
      "2024-12-09 02:43:19.178000: I runner.py:310] Step = 55600 ; steps/s = 1.62, tokens/s = 44216 (44216 target) ; Learning rate = 0.000375 ; Loss = 1.887739\n",
      "2024-12-09 02:44:20.480000: I runner.py:310] Step = 55700 ; steps/s = 1.63, tokens/s = 43766 (43766 target) ; Learning rate = 0.000375 ; Loss = 1.849575\n",
      "2024-12-09 02:45:22.178000: I runner.py:310] Step = 55800 ; steps/s = 1.62, tokens/s = 44223 (44223 target) ; Learning rate = 0.000374 ; Loss = 1.878263\n",
      "2024-12-09 02:46:23.875000: I runner.py:310] Step = 55900 ; steps/s = 1.62, tokens/s = 44246 (44246 target) ; Learning rate = 0.000374 ; Loss = 1.884915\n",
      "2024-12-09 02:47:25.678000: I runner.py:310] Step = 56000 ; steps/s = 1.62, tokens/s = 44160 (44160 target) ; Learning rate = 0.000374 ; Loss = 1.908476\n",
      "2024-12-09 02:48:27.028000: I runner.py:310] Step = 56100 ; steps/s = 1.63, tokens/s = 43758 (43758 target) ; Learning rate = 0.000373 ; Loss = 1.914028\n",
      "2024-12-09 02:49:28.805000: I runner.py:310] Step = 56200 ; steps/s = 1.62, tokens/s = 44179 (44179 target) ; Learning rate = 0.000373 ; Loss = 1.860094\n",
      "2024-12-09 02:50:30.529000: I runner.py:310] Step = 56300 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000373 ; Loss = 1.857198\n",
      "2024-12-09 02:51:31.818000: I runner.py:310] Step = 56400 ; steps/s = 1.63, tokens/s = 43799 (43799 target) ; Learning rate = 0.000372 ; Loss = 1.902952\n",
      "2024-12-09 02:52:33.488000: I runner.py:310] Step = 56500 ; steps/s = 1.62, tokens/s = 44254 (44254 target) ; Learning rate = 0.000372 ; Loss = 1.868911\n",
      "2024-12-09 02:53:35.246000: I runner.py:310] Step = 56600 ; steps/s = 1.62, tokens/s = 44195 (44195 target) ; Learning rate = 0.000372 ; Loss = 1.868454\n",
      "2024-12-09 02:54:36.952000: I runner.py:310] Step = 56700 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000371 ; Loss = 1.882711\n",
      "2024-12-09 02:55:38.313000: I runner.py:310] Step = 56800 ; steps/s = 1.63, tokens/s = 43725 (43725 target) ; Learning rate = 0.000371 ; Loss = 1.881495\n",
      "2024-12-09 02:56:40.019000: I runner.py:310] Step = 56900 ; steps/s = 1.62, tokens/s = 44221 (44221 target) ; Learning rate = 0.000371 ; Loss = 1.858348\n",
      "2024-12-09 02:57:41.724000: I runner.py:310] Step = 57000 ; steps/s = 1.62, tokens/s = 44247 (44247 target) ; Learning rate = 0.000370 ; Loss = 1.874755\n",
      "2024-12-09 02:58:43.037000: I runner.py:310] Step = 57100 ; steps/s = 1.63, tokens/s = 43791 (43791 target) ; Learning rate = 0.000370 ; Loss = 1.888148\n",
      "2024-12-09 02:59:44.748000: I runner.py:310] Step = 57200 ; steps/s = 1.62, tokens/s = 44220 (44220 target) ; Learning rate = 0.000370 ; Loss = 1.864983\n",
      "2024-12-09 03:00:46.453000: I runner.py:310] Step = 57300 ; steps/s = 1.62, tokens/s = 44232 (44232 target) ; Learning rate = 0.000369 ; Loss = 1.872725\n",
      "2024-12-09 03:01:48.162000: I runner.py:310] Step = 57400 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000369 ; Loss = 1.872339\n",
      "2024-12-09 03:02:49.536000: I runner.py:310] Step = 57500 ; steps/s = 1.63, tokens/s = 43741 (43741 target) ; Learning rate = 0.000369 ; Loss = 1.849065\n",
      "2024-12-09 03:03:51.208000: I runner.py:310] Step = 57600 ; steps/s = 1.62, tokens/s = 44244 (44244 target) ; Learning rate = 0.000368 ; Loss = 1.887476\n",
      "2024-12-09 03:04:52.916000: I runner.py:310] Step = 57700 ; steps/s = 1.62, tokens/s = 44226 (44226 target) ; Learning rate = 0.000368 ; Loss = 1.888976\n",
      "2024-12-09 03:05:54.306000: I runner.py:310] Step = 57800 ; steps/s = 1.63, tokens/s = 43735 (43735 target) ; Learning rate = 0.000368 ; Loss = 1.886540\n",
      "2024-12-09 03:06:55.982000: I runner.py:310] Step = 57900 ; steps/s = 1.62, tokens/s = 44246 (44246 target) ; Learning rate = 0.000367 ; Loss = 1.867081\n",
      "2024-12-09 03:07:57.665000: I runner.py:310] Step = 58000 ; steps/s = 1.62, tokens/s = 44232 (44232 target) ; Learning rate = 0.000367 ; Loss = 1.861019\n",
      "2024-12-09 03:08:59.333000: I runner.py:310] Step = 58100 ; steps/s = 1.62, tokens/s = 44260 (44260 target) ; Learning rate = 0.000367 ; Loss = 1.875620\n",
      "2024-12-09 03:10:00.656000: I runner.py:310] Step = 58200 ; steps/s = 1.63, tokens/s = 43780 (43780 target) ; Learning rate = 0.000366 ; Loss = 1.894580\n",
      "2024-12-09 03:11:02.430000: I runner.py:310] Step = 58300 ; steps/s = 1.62, tokens/s = 44177 (44177 target) ; Learning rate = 0.000366 ; Loss = 1.853819\n",
      "2024-12-09 03:12:04.144000: I runner.py:310] Step = 58400 ; steps/s = 1.62, tokens/s = 44221 (44221 target) ; Learning rate = 0.000366 ; Loss = 1.875798\n",
      "2024-12-09 03:13:05.454000: I runner.py:310] Step = 58500 ; steps/s = 1.63, tokens/s = 43767 (43767 target) ; Learning rate = 0.000365 ; Loss = 1.836031\n",
      "2024-12-09 03:14:07.220000: I runner.py:310] Step = 58600 ; steps/s = 1.62, tokens/s = 44189 (44189 target) ; Learning rate = 0.000365 ; Loss = 1.880427\n",
      "2024-12-09 03:15:08.931000: I runner.py:310] Step = 58700 ; steps/s = 1.62, tokens/s = 44221 (44221 target) ; Learning rate = 0.000365 ; Loss = 1.890555\n",
      "2024-12-09 03:16:10.240000: I runner.py:310] Step = 58800 ; steps/s = 1.63, tokens/s = 43790 (43790 target) ; Learning rate = 0.000365 ; Loss = 1.940562\n",
      "2024-12-09 03:17:11.923000: I runner.py:310] Step = 58900 ; steps/s = 1.62, tokens/s = 44221 (44221 target) ; Learning rate = 0.000364 ; Loss = 1.874503\n",
      "2024-12-09 03:18:13.682000: I runner.py:310] Step = 59000 ; steps/s = 1.62, tokens/s = 44205 (44205 target) ; Learning rate = 0.000364 ; Loss = 1.859799\n",
      "2024-12-09 03:19:15.434000: I runner.py:310] Step = 59100 ; steps/s = 1.62, tokens/s = 44199 (44199 target) ; Learning rate = 0.000364 ; Loss = 1.861200\n",
      "2024-12-09 03:20:16.807000: I runner.py:310] Step = 59200 ; steps/s = 1.63, tokens/s = 43742 (43742 target) ; Learning rate = 0.000363 ; Loss = 1.889673\n",
      "2024-12-09 03:21:18.547000: I runner.py:310] Step = 59300 ; steps/s = 1.62, tokens/s = 44205 (44205 target) ; Learning rate = 0.000363 ; Loss = 1.856492\n",
      "2024-12-09 03:22:20.347000: I runner.py:310] Step = 59400 ; steps/s = 1.62, tokens/s = 44167 (44167 target) ; Learning rate = 0.000363 ; Loss = 1.856143\n",
      "2024-12-09 03:23:21.633000: I runner.py:310] Step = 59500 ; steps/s = 1.63, tokens/s = 43788 (43788 target) ; Learning rate = 0.000362 ; Loss = 1.869018\n",
      "2024-12-09 03:24:23.229000: I runner.py:310] Step = 59600 ; steps/s = 1.62, tokens/s = 44307 (44307 target) ; Learning rate = 0.000362 ; Loss = 1.859807\n",
      "2024-12-09 03:25:24.931000: I runner.py:310] Step = 59700 ; steps/s = 1.62, tokens/s = 44235 (44235 target) ; Learning rate = 0.000362 ; Loss = 1.894704\n",
      "2024-12-09 03:26:26.677000: I runner.py:310] Step = 59800 ; steps/s = 1.62, tokens/s = 44190 (44190 target) ; Learning rate = 0.000361 ; Loss = 1.870863\n",
      "2024-12-09 03:27:27.959000: I runner.py:310] Step = 59900 ; steps/s = 1.63, tokens/s = 43808 (43808 target) ; Learning rate = 0.000361 ; Loss = 1.911475\n",
      "2024-12-09 03:28:29.682000: I runner.py:310] Step = 60000 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000361 ; Loss = 1.854299\n",
      "2024-12-09 03:28:31.662000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-60000\n",
      "2024-12-09 03:28:31.662000: I training.py:192] Running evaluation for step 60000\n",
      "2024-12-09 03:31:17.816000: I training.py:192] Evaluation result for step 60000: loss = 2.384784 ; perplexity = 10.856714\n",
      "2024-12-09 03:32:19.415000: I runner.py:310] Step = 60100 ; steps/s = 1.62, tokens/s = 44318 (44318 target) ; Learning rate = 0.000361 ; Loss = 1.858215\n",
      "2024-12-09 03:33:20.675000: I runner.py:310] Step = 60200 ; steps/s = 1.63, tokens/s = 43812 (43812 target) ; Learning rate = 0.000360 ; Loss = 1.868317\n",
      "2024-12-09 03:34:22.348000: I runner.py:310] Step = 60300 ; steps/s = 1.62, tokens/s = 44261 (44261 target) ; Learning rate = 0.000360 ; Loss = 1.856223\n",
      "2024-12-09 03:35:24.003000: I runner.py:310] Step = 60400 ; steps/s = 1.62, tokens/s = 44274 (44274 target) ; Learning rate = 0.000360 ; Loss = 1.878927\n",
      "2024-12-09 03:36:25.778000: I runner.py:310] Step = 60500 ; steps/s = 1.62, tokens/s = 44161 (44161 target) ; Learning rate = 0.000359 ; Loss = 1.892987\n",
      "2024-12-09 03:37:27.088000: I runner.py:310] Step = 60600 ; steps/s = 1.63, tokens/s = 43781 (43781 target) ; Learning rate = 0.000359 ; Loss = 1.888401\n",
      "2024-12-09 03:38:28.767000: I runner.py:310] Step = 60700 ; steps/s = 1.62, tokens/s = 44244 (44244 target) ; Learning rate = 0.000359 ; Loss = 1.849437\n",
      "2024-12-09 03:39:30.469000: I runner.py:310] Step = 60800 ; steps/s = 1.62, tokens/s = 44242 (44242 target) ; Learning rate = 0.000358 ; Loss = 1.856018\n",
      "2024-12-09 03:40:31.820000: I runner.py:310] Step = 60900 ; steps/s = 1.63, tokens/s = 43742 (43742 target) ; Learning rate = 0.000358 ; Loss = 1.863490\n",
      "2024-12-09 03:41:33.476000: I runner.py:310] Step = 61000 ; steps/s = 1.62, tokens/s = 44269 (44269 target) ; Learning rate = 0.000358 ; Loss = 1.843427\n",
      "2024-12-09 03:42:35.078000: I runner.py:310] Step = 61100 ; steps/s = 1.62, tokens/s = 44302 (44302 target) ; Learning rate = 0.000358 ; Loss = 1.869751\n",
      "2024-12-09 03:43:36.787000: I runner.py:310] Step = 61200 ; steps/s = 1.62, tokens/s = 44227 (44227 target) ; Learning rate = 0.000357 ; Loss = 1.883547\n",
      "2024-12-09 03:44:38.078000: I runner.py:310] Step = 61300 ; steps/s = 1.63, tokens/s = 43808 (43808 target) ; Learning rate = 0.000357 ; Loss = 1.817106\n",
      "2024-12-09 03:45:39.814000: I runner.py:310] Step = 61400 ; steps/s = 1.62, tokens/s = 44197 (44197 target) ; Learning rate = 0.000357 ; Loss = 1.862515\n",
      "2024-12-09 03:46:41.504000: I runner.py:310] Step = 61500 ; steps/s = 1.62, tokens/s = 44244 (44244 target) ; Learning rate = 0.000356 ; Loss = 1.876450\n",
      "2024-12-09 03:47:42.863000: I runner.py:310] Step = 61600 ; steps/s = 1.63, tokens/s = 43734 (43734 target) ; Learning rate = 0.000356 ; Loss = 1.853985\n",
      "2024-12-09 03:48:44.585000: I runner.py:310] Step = 61700 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000356 ; Loss = 1.854164\n",
      "2024-12-09 03:49:46.287000: I runner.py:310] Step = 61800 ; steps/s = 1.62, tokens/s = 44232 (44232 target) ; Learning rate = 0.000356 ; Loss = 1.865638\n",
      "2024-12-09 03:50:47.989000: I runner.py:310] Step = 61900 ; steps/s = 1.62, tokens/s = 44238 (44238 target) ; Learning rate = 0.000355 ; Loss = 1.872671\n",
      "2024-12-09 03:51:49.269000: I runner.py:310] Step = 62000 ; steps/s = 1.63, tokens/s = 43807 (43807 target) ; Learning rate = 0.000355 ; Loss = 1.831120\n",
      "2024-12-09 03:52:50.869000: I runner.py:310] Step = 62100 ; steps/s = 1.62, tokens/s = 44308 (44308 target) ; Learning rate = 0.000355 ; Loss = 1.867887\n",
      "2024-12-09 03:53:52.535000: I runner.py:310] Step = 62200 ; steps/s = 1.62, tokens/s = 44257 (44257 target) ; Learning rate = 0.000354 ; Loss = 1.887218\n",
      "2024-12-09 03:54:53.792000: I runner.py:310] Step = 62300 ; steps/s = 1.63, tokens/s = 43820 (43820 target) ; Learning rate = 0.000354 ; Loss = 1.861061\n",
      "2024-12-09 03:55:55.508000: I runner.py:310] Step = 62400 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000354 ; Loss = 1.846346\n",
      "2024-12-09 03:56:57.165000: I runner.py:310] Step = 62500 ; steps/s = 1.62, tokens/s = 44258 (44258 target) ; Learning rate = 0.000354 ; Loss = 1.877770\n",
      "2024-12-09 03:57:58.857000: I runner.py:310] Step = 62600 ; steps/s = 1.62, tokens/s = 44237 (44237 target) ; Learning rate = 0.000353 ; Loss = 1.876822\n",
      "2024-12-09 03:59:00.144000: I runner.py:310] Step = 62700 ; steps/s = 1.63, tokens/s = 43803 (43803 target) ; Learning rate = 0.000353 ; Loss = 1.823016\n",
      "2024-12-09 04:00:01.904000: I runner.py:310] Step = 62800 ; steps/s = 1.62, tokens/s = 44191 (44191 target) ; Learning rate = 0.000353 ; Loss = 1.854574\n",
      "2024-12-09 04:01:03.651000: I runner.py:310] Step = 62900 ; steps/s = 1.62, tokens/s = 44193 (44193 target) ; Learning rate = 0.000352 ; Loss = 1.884206\n",
      "2024-12-09 04:02:04.970000: I runner.py:310] Step = 63000 ; steps/s = 1.63, tokens/s = 43773 (43773 target) ; Learning rate = 0.000352 ; Loss = 1.851274\n",
      "2024-12-09 04:03:06.691000: I runner.py:310] Step = 63100 ; steps/s = 1.62, tokens/s = 44222 (44222 target) ; Learning rate = 0.000352 ; Loss = 1.843774\n",
      "2024-12-09 04:04:08.377000: I runner.py:310] Step = 63200 ; steps/s = 1.62, tokens/s = 44231 (44231 target) ; Learning rate = 0.000352 ; Loss = 1.864262\n",
      "2024-12-09 04:05:10.072000: I runner.py:310] Step = 63300 ; steps/s = 1.62, tokens/s = 44244 (44244 target) ; Learning rate = 0.000351 ; Loss = 1.881299\n",
      "2024-12-09 04:06:11.348000: I runner.py:310] Step = 63400 ; steps/s = 1.63, tokens/s = 43815 (43815 target) ; Learning rate = 0.000351 ; Loss = 1.882410\n",
      "2024-12-09 04:07:13.012000: I runner.py:310] Step = 63500 ; steps/s = 1.62, tokens/s = 44254 (44254 target) ; Learning rate = 0.000351 ; Loss = 1.848557\n",
      "2024-12-09 04:08:14.611000: I runner.py:310] Step = 63600 ; steps/s = 1.62, tokens/s = 44308 (44308 target) ; Learning rate = 0.000350 ; Loss = 1.847387\n",
      "2024-12-09 04:09:15.903000: I runner.py:310] Step = 63700 ; steps/s = 1.63, tokens/s = 43775 (43775 target) ; Learning rate = 0.000350 ; Loss = 1.836599\n",
      "2024-12-09 04:10:17.627000: I runner.py:310] Step = 63800 ; steps/s = 1.62, tokens/s = 44222 (44222 target) ; Learning rate = 0.000350 ; Loss = 1.872565\n",
      "2024-12-09 04:11:19.381000: I runner.py:310] Step = 63900 ; steps/s = 1.62, tokens/s = 44192 (44192 target) ; Learning rate = 0.000350 ; Loss = 1.878410\n",
      "2024-12-09 04:12:21.103000: I runner.py:310] Step = 64000 ; steps/s = 1.62, tokens/s = 44218 (44218 target) ; Learning rate = 0.000349 ; Loss = 1.880150\n",
      "2024-12-09 04:13:22.355000: I runner.py:310] Step = 64100 ; steps/s = 1.63, tokens/s = 43819 (43819 target) ; Learning rate = 0.000349 ; Loss = 1.863036\n",
      "2024-12-09 04:14:24.034000: I runner.py:310] Step = 64200 ; steps/s = 1.62, tokens/s = 44243 (44243 target) ; Learning rate = 0.000349 ; Loss = 1.849683\n",
      "2024-12-09 04:15:25.657000: I runner.py:310] Step = 64300 ; steps/s = 1.62, tokens/s = 44291 (44291 target) ; Learning rate = 0.000349 ; Loss = 1.844442\n",
      "2024-12-09 04:16:26.963000: I runner.py:310] Step = 64400 ; steps/s = 1.63, tokens/s = 43792 (43792 target) ; Learning rate = 0.000348 ; Loss = 1.845491\n",
      "2024-12-09 04:17:28.678000: I runner.py:310] Step = 64500 ; steps/s = 1.62, tokens/s = 44245 (44245 target) ; Learning rate = 0.000348 ; Loss = 1.851683\n",
      "2024-12-09 04:18:30.477000: I runner.py:310] Step = 64600 ; steps/s = 1.62, tokens/s = 44146 (44146 target) ; Learning rate = 0.000348 ; Loss = 1.858216\n",
      "2024-12-09 04:19:32.270000: I runner.py:310] Step = 64700 ; steps/s = 1.62, tokens/s = 44161 (44161 target) ; Learning rate = 0.000347 ; Loss = 1.874896\n",
      "2024-12-09 04:20:33.596000: I runner.py:310] Step = 64800 ; steps/s = 1.63, tokens/s = 43768 (43768 target) ; Learning rate = 0.000347 ; Loss = 1.835480\n",
      "2024-12-09 04:21:35.365000: I runner.py:310] Step = 64900 ; steps/s = 1.62, tokens/s = 44184 (44184 target) ; Learning rate = 0.000347 ; Loss = 1.870617\n",
      "2024-12-09 04:22:37.076000: I runner.py:310] Step = 65000 ; steps/s = 1.62, tokens/s = 44229 (44229 target) ; Learning rate = 0.000347 ; Loss = 1.881189\n",
      "2024-12-09 04:22:37.077000: I training.py:192] Running evaluation for step 65000\n",
      "2024-12-09 04:25:25.123000: I training.py:192] Evaluation result for step 65000: loss = 2.396150 ; perplexity = 10.980820\n",
      "2024-12-09 04:26:26.335000: I runner.py:310] Step = 65100 ; steps/s = 1.63, tokens/s = 43847 (43847 target) ; Learning rate = 0.000346 ; Loss = 1.842886\n",
      "2024-12-09 04:27:28.031000: I runner.py:310] Step = 65200 ; steps/s = 1.62, tokens/s = 44233 (44233 target) ; Learning rate = 0.000346 ; Loss = 1.853720\n",
      "2024-12-09 04:28:29.670000: I runner.py:310] Step = 65300 ; steps/s = 1.62, tokens/s = 44278 (44278 target) ; Learning rate = 0.000346 ; Loss = 1.868900\n",
      "2024-12-09 04:29:31.399000: I runner.py:310] Step = 65400 ; steps/s = 1.62, tokens/s = 44221 (44221 target) ; Learning rate = 0.000346 ; Loss = 1.851823\n",
      "2024-12-09 04:30:32.729000: I runner.py:310] Step = 65500 ; steps/s = 1.63, tokens/s = 43767 (43767 target) ; Learning rate = 0.000345 ; Loss = 1.828074\n",
      "2024-12-09 04:31:34.421000: I runner.py:310] Step = 65600 ; steps/s = 1.62, tokens/s = 44236 (44236 target) ; Learning rate = 0.000345 ; Loss = 1.854154\n",
      "2024-12-09 04:32:36.143000: I runner.py:310] Step = 65700 ; steps/s = 1.62, tokens/s = 44211 (44211 target) ; Learning rate = 0.000345 ; Loss = 1.868438\n",
      "2024-12-09 04:33:37.526000: I runner.py:310] Step = 65800 ; steps/s = 1.63, tokens/s = 43726 (43726 target) ; Learning rate = 0.000345 ; Loss = 1.860545\n",
      "2024-12-09 04:34:39.168000: I runner.py:310] Step = 65900 ; steps/s = 1.62, tokens/s = 44283 (44283 target) ; Learning rate = 0.000344 ; Loss = 1.835901\n",
      "2024-12-09 04:35:40.891000: I runner.py:310] Step = 66000 ; steps/s = 1.62, tokens/s = 44217 (44217 target) ; Learning rate = 0.000344 ; Loss = 1.844190\n",
      "2024-12-09 04:36:42.612000: I runner.py:310] Step = 66100 ; steps/s = 1.62, tokens/s = 44220 (44220 target) ; Learning rate = 0.000344 ; Loss = 1.853667\n",
      "2024-12-09 04:37:43.885000: I runner.py:310] Step = 66200 ; steps/s = 1.63, tokens/s = 43812 (43812 target) ; Learning rate = 0.000344 ; Loss = 1.831788\n",
      "2024-12-09 04:38:45.551000: I runner.py:310] Step = 66300 ; steps/s = 1.62, tokens/s = 44266 (44266 target) ; Learning rate = 0.000343 ; Loss = 1.863294\n",
      "2024-12-09 04:39:47.266000: I runner.py:310] Step = 66400 ; steps/s = 1.62, tokens/s = 44211 (44211 target) ; Learning rate = 0.000343 ; Loss = 1.860516\n",
      "2024-12-09 04:40:48.562000: I runner.py:310] Step = 66500 ; steps/s = 1.63, tokens/s = 43784 (43784 target) ; Learning rate = 0.000343 ; Loss = 1.837253\n",
      "2024-12-09 04:41:50.282000: I runner.py:310] Step = 66600 ; steps/s = 1.62, tokens/s = 44224 (44224 target) ; Learning rate = 0.000342 ; Loss = 1.847481\n",
      "2024-12-09 04:42:51.873000: I runner.py:310] Step = 66700 ; steps/s = 1.62, tokens/s = 44313 (44313 target) ; Learning rate = 0.000342 ; Loss = 1.878279\n",
      "2024-12-09 04:43:53.507000: I runner.py:310] Step = 66800 ; steps/s = 1.62, tokens/s = 44015 (44015 target) ; Learning rate = 0.000342 ; Loss = 1.850979\n",
      "2024-12-09 04:44:54.996000: I runner.py:310] Step = 66900 ; steps/s = 1.63, tokens/s = 43915 (43915 target) ; Learning rate = 0.000342 ; Loss = 1.852303\n",
      "2024-12-09 04:45:56.697000: I runner.py:310] Step = 67000 ; steps/s = 1.62, tokens/s = 44231 (44231 target) ; Learning rate = 0.000341 ; Loss = 1.854305\n",
      "2024-12-09 04:46:58.464000: I runner.py:310] Step = 67100 ; steps/s = 1.62, tokens/s = 44180 (44180 target) ; Learning rate = 0.000341 ; Loss = 1.856596\n",
      "2024-12-09 04:47:59.720000: I runner.py:310] Step = 67200 ; steps/s = 1.63, tokens/s = 43831 (43831 target) ; Learning rate = 0.000341 ; Loss = 1.878620\n",
      "2024-12-09 04:49:01.408000: I runner.py:310] Step = 67300 ; steps/s = 1.62, tokens/s = 44252 (44252 target) ; Learning rate = 0.000341 ; Loss = 1.844424\n",
      "2024-12-09 04:50:03.144000: I runner.py:310] Step = 67400 ; steps/s = 1.62, tokens/s = 44201 (44201 target) ; Learning rate = 0.000340 ; Loss = 1.838783\n",
      "2024-12-09 04:51:04.452000: I runner.py:310] Step = 67500 ; steps/s = 1.63, tokens/s = 43777 (43777 target) ; Learning rate = 0.000340 ; Loss = 1.844575\n",
      "2024-12-09 04:52:06.125000: I runner.py:310] Step = 67600 ; steps/s = 1.62, tokens/s = 44243 (44243 target) ; Learning rate = 0.000340 ; Loss = 1.822855\n",
      "2024-12-09 04:53:07.896000: I runner.py:310] Step = 67700 ; steps/s = 1.62, tokens/s = 44182 (44182 target) ; Learning rate = 0.000340 ; Loss = 1.859752\n",
      "2024-12-09 04:54:09.617000: I runner.py:310] Step = 67800 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000339 ; Loss = 1.852895\n",
      "2024-12-09 04:55:10.887000: I runner.py:310] Step = 67900 ; steps/s = 1.63, tokens/s = 43802 (43802 target) ; Learning rate = 0.000339 ; Loss = 1.823225\n",
      "2024-12-09 04:56:12.590000: I runner.py:310] Step = 68000 ; steps/s = 1.62, tokens/s = 44227 (44227 target) ; Learning rate = 0.000339 ; Loss = 1.836836\n",
      "2024-12-09 04:57:14.357000: I runner.py:310] Step = 68100 ; steps/s = 1.62, tokens/s = 44187 (44187 target) ; Learning rate = 0.000339 ; Loss = 1.853171\n",
      "2024-12-09 04:58:15.625000: I runner.py:310] Step = 68200 ; steps/s = 1.63, tokens/s = 43813 (43813 target) ; Learning rate = 0.000338 ; Loss = 1.854224\n",
      "2024-12-09 04:59:17.247000: I runner.py:310] Step = 68300 ; steps/s = 1.62, tokens/s = 44289 (44289 target) ; Learning rate = 0.000338 ; Loss = 1.855177\n",
      "2024-12-09 05:00:18.935000: I runner.py:310] Step = 68400 ; steps/s = 1.62, tokens/s = 44254 (44254 target) ; Learning rate = 0.000338 ; Loss = 1.825939\n",
      "2024-12-09 05:01:20.630000: I runner.py:310] Step = 68500 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000338 ; Loss = 1.853271\n",
      "2024-12-09 05:02:21.972000: I runner.py:310] Step = 68600 ; steps/s = 1.63, tokens/s = 43764 (43764 target) ; Learning rate = 0.000337 ; Loss = 1.811660\n",
      "2024-12-09 05:03:23.586000: I runner.py:310] Step = 68700 ; steps/s = 1.62, tokens/s = 44296 (44296 target) ; Learning rate = 0.000337 ; Loss = 1.841752\n",
      "2024-12-09 05:04:25.261000: I runner.py:310] Step = 68800 ; steps/s = 1.62, tokens/s = 44252 (44252 target) ; Learning rate = 0.000337 ; Loss = 1.853047\n",
      "2024-12-09 05:05:26.559000: I runner.py:310] Step = 68900 ; steps/s = 1.63, tokens/s = 43785 (43785 target) ; Learning rate = 0.000337 ; Loss = 1.856531\n",
      "2024-12-09 05:06:28.254000: I runner.py:310] Step = 69000 ; steps/s = 1.62, tokens/s = 44232 (44232 target) ; Learning rate = 0.000336 ; Loss = 1.850494\n",
      "2024-12-09 05:07:29.991000: I runner.py:310] Step = 69100 ; steps/s = 1.62, tokens/s = 44217 (44217 target) ; Learning rate = 0.000336 ; Loss = 1.840056\n",
      "2024-12-09 05:08:31.714000: I runner.py:310] Step = 69200 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000336 ; Loss = 1.847920\n",
      "2024-12-09 05:09:33.062000: I runner.py:310] Step = 69300 ; steps/s = 1.63, tokens/s = 43750 (43750 target) ; Learning rate = 0.000336 ; Loss = 1.879281\n",
      "2024-12-09 05:10:34.851000: I runner.py:310] Step = 69400 ; steps/s = 1.62, tokens/s = 44171 (44171 target) ; Learning rate = 0.000336 ; Loss = 1.831123\n",
      "2024-12-09 05:11:36.568000: I runner.py:310] Step = 69500 ; steps/s = 1.62, tokens/s = 44218 (44218 target) ; Learning rate = 0.000335 ; Loss = 1.824964\n",
      "2024-12-09 05:12:37.811000: I runner.py:310] Step = 69600 ; steps/s = 1.63, tokens/s = 43823 (43823 target) ; Learning rate = 0.000335 ; Loss = 1.865006\n",
      "2024-12-09 05:13:39.552000: I runner.py:310] Step = 69700 ; steps/s = 1.62, tokens/s = 44199 (44199 target) ; Learning rate = 0.000335 ; Loss = 1.832504\n",
      "2024-12-09 05:14:41.275000: I runner.py:310] Step = 69800 ; steps/s = 1.62, tokens/s = 44230 (44230 target) ; Learning rate = 0.000335 ; Loss = 1.829620\n",
      "2024-12-09 05:15:43.072000: I runner.py:310] Step = 69900 ; steps/s = 1.62, tokens/s = 44160 (44160 target) ; Learning rate = 0.000334 ; Loss = 1.838609\n",
      "2024-12-09 05:16:44.363000: I runner.py:310] Step = 70000 ; steps/s = 1.63, tokens/s = 43793 (43793 target) ; Learning rate = 0.000334 ; Loss = 1.877637\n",
      "2024-12-09 05:16:46.878000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-70000\n",
      "2024-12-09 05:16:46.878000: I training.py:192] Running evaluation for step 70000\n",
      "2024-12-09 05:19:32.785000: I training.py:192] Evaluation result for step 70000: loss = 2.412082 ; perplexity = 11.157168\n",
      "2024-12-09 05:20:34.337000: I runner.py:310] Step = 70100 ; steps/s = 1.63, tokens/s = 44337 (44337 target) ; Learning rate = 0.000334 ; Loss = 1.833240\n",
      "2024-12-09 05:21:36.041000: I runner.py:310] Step = 70200 ; steps/s = 1.62, tokens/s = 44234 (44234 target) ; Learning rate = 0.000334 ; Loss = 1.838722\n",
      "2024-12-09 05:22:37.353000: I runner.py:310] Step = 70300 ; steps/s = 1.63, tokens/s = 43776 (43776 target) ; Learning rate = 0.000333 ; Loss = 1.872353\n",
      "2024-12-09 05:23:39.137000: I runner.py:310] Step = 70400 ; steps/s = 1.62, tokens/s = 44172 (44172 target) ; Learning rate = 0.000333 ; Loss = 1.838217\n",
      "2024-12-09 05:24:40.833000: I runner.py:310] Step = 70500 ; steps/s = 1.62, tokens/s = 44241 (44241 target) ; Learning rate = 0.000333 ; Loss = 1.829147\n",
      "2024-12-09 05:25:42.568000: I runner.py:310] Step = 70600 ; steps/s = 1.62, tokens/s = 44207 (44207 target) ; Learning rate = 0.000333 ; Loss = 1.840842\n",
      "2024-12-09 05:26:43.946000: I runner.py:310] Step = 70700 ; steps/s = 1.63, tokens/s = 43727 (43727 target) ; Learning rate = 0.000332 ; Loss = 1.867472\n",
      "2024-12-09 05:27:45.650000: I runner.py:310] Step = 70800 ; steps/s = 1.62, tokens/s = 44240 (44240 target) ; Learning rate = 0.000332 ; Loss = 1.819258\n",
      "2024-12-09 05:28:47.350000: I runner.py:310] Step = 70900 ; steps/s = 1.62, tokens/s = 44241 (44241 target) ; Learning rate = 0.000332 ; Loss = 1.843362\n",
      "2024-12-09 05:29:48.637000: I runner.py:310] Step = 71000 ; steps/s = 1.63, tokens/s = 43786 (43786 target) ; Learning rate = 0.000332 ; Loss = 1.825605\n",
      "2024-12-09 05:30:50.344000: I runner.py:310] Step = 71100 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000331 ; Loss = 1.824337\n",
      "2024-12-09 05:31:52.092000: I runner.py:310] Step = 71200 ; steps/s = 1.62, tokens/s = 44196 (44196 target) ; Learning rate = 0.000331 ; Loss = 1.854635\n",
      "2024-12-09 05:32:53.877000: I runner.py:310] Step = 71300 ; steps/s = 1.62, tokens/s = 44175 (44175 target) ; Learning rate = 0.000331 ; Loss = 1.863554\n",
      "2024-12-09 05:33:55.110000: I runner.py:310] Step = 71400 ; steps/s = 1.63, tokens/s = 43827 (43827 target) ; Learning rate = 0.000331 ; Loss = 1.864707\n",
      "2024-12-09 05:34:56.813000: I runner.py:310] Step = 71500 ; steps/s = 1.62, tokens/s = 44242 (44242 target) ; Learning rate = 0.000331 ; Loss = 1.840178\n",
      "2024-12-09 05:35:58.539000: I runner.py:310] Step = 71600 ; steps/s = 1.62, tokens/s = 44203 (44203 target) ; Learning rate = 0.000330 ; Loss = 1.819254\n",
      "2024-12-09 05:36:59.913000: I runner.py:310] Step = 71700 ; steps/s = 1.63, tokens/s = 43734 (43734 target) ; Learning rate = 0.000330 ; Loss = 1.857605\n",
      "2024-12-09 05:38:01.640000: I runner.py:310] Step = 71800 ; steps/s = 1.62, tokens/s = 44218 (44218 target) ; Learning rate = 0.000330 ; Loss = 1.830406\n",
      "2024-12-09 05:39:03.340000: I runner.py:310] Step = 71900 ; steps/s = 1.62, tokens/s = 44245 (44245 target) ; Learning rate = 0.000330 ; Loss = 1.827949\n",
      "2024-12-09 05:40:05.001000: I runner.py:310] Step = 72000 ; steps/s = 1.62, tokens/s = 44249 (44249 target) ; Learning rate = 0.000329 ; Loss = 1.832449\n",
      "2024-12-09 05:41:06.307000: I runner.py:310] Step = 72100 ; steps/s = 1.63, tokens/s = 43782 (43782 target) ; Learning rate = 0.000329 ; Loss = 1.798876\n",
      "2024-12-09 05:42:08.097000: I runner.py:310] Step = 72200 ; steps/s = 1.62, tokens/s = 44165 (44165 target) ; Learning rate = 0.000329 ; Loss = 1.859221\n",
      "2024-12-09 05:43:09.818000: I runner.py:310] Step = 72300 ; steps/s = 1.62, tokens/s = 44218 (44218 target) ; Learning rate = 0.000329 ; Loss = 1.862035\n",
      "2024-12-09 05:44:11.134000: I runner.py:310] Step = 72400 ; steps/s = 1.63, tokens/s = 43782 (43782 target) ; Learning rate = 0.000328 ; Loss = 1.853420\n",
      "2024-12-09 05:45:12.889000: I runner.py:310] Step = 72500 ; steps/s = 1.62, tokens/s = 44195 (44195 target) ; Learning rate = 0.000328 ; Loss = 1.837826\n",
      "2024-12-09 05:46:14.679000: I runner.py:310] Step = 72600 ; steps/s = 1.62, tokens/s = 44160 (44160 target) ; Learning rate = 0.000328 ; Loss = 1.832283\n",
      "2024-12-09 05:47:16.413000: I runner.py:310] Step = 72700 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000328 ; Loss = 1.832823\n",
      "2024-12-09 05:48:17.770000: I runner.py:310] Step = 72800 ; steps/s = 1.63, tokens/s = 43755 (43755 target) ; Learning rate = 0.000328 ; Loss = 1.852678\n",
      "2024-12-09 05:49:19.496000: I runner.py:310] Step = 72900 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000327 ; Loss = 1.828060\n",
      "2024-12-09 05:50:21.199000: I runner.py:310] Step = 73000 ; steps/s = 1.62, tokens/s = 44239 (44239 target) ; Learning rate = 0.000327 ; Loss = 1.831014\n",
      "2024-12-09 05:51:22.476000: I runner.py:310] Step = 73100 ; steps/s = 1.63, tokens/s = 43797 (43797 target) ; Learning rate = 0.000327 ; Loss = 1.849329\n",
      "2024-12-09 05:52:24.213000: I runner.py:310] Step = 73200 ; steps/s = 1.62, tokens/s = 44214 (44214 target) ; Learning rate = 0.000327 ; Loss = 1.823005\n",
      "2024-12-09 05:53:26.004000: I runner.py:310] Step = 73300 ; steps/s = 1.62, tokens/s = 44161 (44161 target) ; Learning rate = 0.000326 ; Loss = 1.843934\n",
      "2024-12-09 05:54:27.730000: I runner.py:310] Step = 73400 ; steps/s = 1.62, tokens/s = 44217 (44217 target) ; Learning rate = 0.000326 ; Loss = 1.847324\n",
      "2024-12-09 05:55:29.102000: I runner.py:310] Step = 73500 ; steps/s = 1.63, tokens/s = 43728 (43728 target) ; Learning rate = 0.000326 ; Loss = 1.850219\n",
      "2024-12-09 05:56:30.739000: I runner.py:310] Step = 73600 ; steps/s = 1.62, tokens/s = 44272 (44272 target) ; Learning rate = 0.000326 ; Loss = 1.815765\n",
      "2024-12-09 05:57:32.534000: I runner.py:310] Step = 73700 ; steps/s = 1.62, tokens/s = 44167 (44167 target) ; Learning rate = 0.000326 ; Loss = 1.827148\n",
      "2024-12-09 05:58:33.881000: I runner.py:310] Step = 73800 ; steps/s = 1.63, tokens/s = 43759 (43759 target) ; Learning rate = 0.000325 ; Loss = 1.849959\n",
      "2024-12-09 05:59:35.609000: I runner.py:310] Step = 73900 ; steps/s = 1.62, tokens/s = 44219 (44219 target) ; Learning rate = 0.000325 ; Loss = 1.810113\n",
      "2024-12-09 06:00:37.304000: I runner.py:310] Step = 74000 ; steps/s = 1.62, tokens/s = 44233 (44233 target) ; Learning rate = 0.000325 ; Loss = 1.823708\n",
      "2024-12-09 06:01:39.019000: I runner.py:310] Step = 74100 ; steps/s = 1.62, tokens/s = 44229 (44229 target) ; Learning rate = 0.000325 ; Loss = 1.832716\n",
      "2024-12-09 06:02:40.358000: I runner.py:310] Step = 74200 ; steps/s = 1.63, tokens/s = 43757 (43757 target) ; Learning rate = 0.000324 ; Loss = 1.852994\n",
      "2024-12-09 06:03:42.093000: I runner.py:310] Step = 74300 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000324 ; Loss = 1.815763\n",
      "2024-12-09 06:04:43.819000: I runner.py:310] Step = 74400 ; steps/s = 1.62, tokens/s = 44209 (44209 target) ; Learning rate = 0.000324 ; Loss = 1.834835\n",
      "2024-12-09 06:05:45.150000: I runner.py:310] Step = 74500 ; steps/s = 1.63, tokens/s = 43762 (43762 target) ; Learning rate = 0.000324 ; Loss = 1.840197\n",
      "2024-12-09 06:06:46.838000: I runner.py:310] Step = 74600 ; steps/s = 1.62, tokens/s = 44247 (44247 target) ; Learning rate = 0.000324 ; Loss = 1.822655\n",
      "2024-12-09 06:07:48.635000: I runner.py:310] Step = 74700 ; steps/s = 1.62, tokens/s = 44156 (44156 target) ; Learning rate = 0.000323 ; Loss = 1.825967\n",
      "2024-12-09 06:08:50.335000: I runner.py:310] Step = 74800 ; steps/s = 1.62, tokens/s = 44236 (44236 target) ; Learning rate = 0.000323 ; Loss = 1.828081\n",
      "2024-12-09 06:09:51.714000: I runner.py:310] Step = 74900 ; steps/s = 1.63, tokens/s = 43729 (43729 target) ; Learning rate = 0.000323 ; Loss = 1.824686\n",
      "2024-12-09 06:10:53.473000: I runner.py:310] Step = 75000 ; steps/s = 1.62, tokens/s = 44197 (44197 target) ; Learning rate = 0.000323 ; Loss = 1.851193\n",
      "2024-12-09 06:10:53.475000: I training.py:192] Running evaluation for step 75000\n",
      "2024-12-09 06:13:49.711000: I training.py:192] Evaluation result for step 75000: loss = 2.434884 ; perplexity = 11.414496\n",
      "2024-12-09 06:14:51.271000: I runner.py:310] Step = 75100 ; steps/s = 1.62, tokens/s = 44339 (44339 target) ; Learning rate = 0.000323 ; Loss = 1.842740\n",
      "2024-12-09 06:15:52.579000: I runner.py:310] Step = 75200 ; steps/s = 1.63, tokens/s = 43770 (43770 target) ; Learning rate = 0.000322 ; Loss = 1.854653\n",
      "2024-12-09 06:16:54.325000: I runner.py:310] Step = 75300 ; steps/s = 1.62, tokens/s = 44197 (44197 target) ; Learning rate = 0.000322 ; Loss = 1.813238\n",
      "2024-12-09 06:17:56.125000: I runner.py:310] Step = 75400 ; steps/s = 1.62, tokens/s = 44174 (44174 target) ; Learning rate = 0.000322 ; Loss = 1.828961\n",
      "2024-12-09 06:18:57.513000: I runner.py:310] Step = 75500 ; steps/s = 1.63, tokens/s = 43821 (43821 target) ; Learning rate = 0.000322 ; Loss = 1.862525\n",
      "2024-12-09 06:19:59.167000: I runner.py:310] Step = 75600 ; steps/s = 1.62, tokens/s = 44161 (44161 target) ; Learning rate = 0.000321 ; Loss = 1.834271\n",
      "2024-12-09 06:21:00.882000: I runner.py:310] Step = 75700 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000321 ; Loss = 1.815158\n",
      "2024-12-09 06:22:02.674000: I runner.py:310] Step = 75800 ; steps/s = 1.62, tokens/s = 44177 (44177 target) ; Learning rate = 0.000321 ; Loss = 1.835477\n",
      "2024-12-09 06:23:04.017000: I runner.py:310] Step = 75900 ; steps/s = 1.63, tokens/s = 43771 (43771 target) ; Learning rate = 0.000321 ; Loss = 1.859215\n",
      "2024-12-09 06:24:05.728000: I runner.py:310] Step = 76000 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000321 ; Loss = 1.819377\n",
      "2024-12-09 06:25:07.499000: I runner.py:310] Step = 76100 ; steps/s = 1.62, tokens/s = 44189 (44189 target) ; Learning rate = 0.000320 ; Loss = 1.828363\n",
      "2024-12-09 06:26:08.888000: I runner.py:310] Step = 76200 ; steps/s = 1.63, tokens/s = 43731 (43731 target) ; Learning rate = 0.000320 ; Loss = 1.830196\n",
      "2024-12-09 06:27:10.654000: I runner.py:310] Step = 76300 ; steps/s = 1.62, tokens/s = 44178 (44178 target) ; Learning rate = 0.000320 ; Loss = 1.851514\n",
      "2024-12-09 06:28:12.438000: I runner.py:310] Step = 76400 ; steps/s = 1.62, tokens/s = 44175 (44175 target) ; Learning rate = 0.000320 ; Loss = 1.817334\n",
      "2024-12-09 06:29:14.149000: I runner.py:310] Step = 76500 ; steps/s = 1.62, tokens/s = 44240 (44240 target) ; Learning rate = 0.000320 ; Loss = 1.817230\n",
      "2024-12-09 06:30:15.520000: I runner.py:310] Step = 76600 ; steps/s = 1.63, tokens/s = 43727 (43727 target) ; Learning rate = 0.000319 ; Loss = 1.804150\n",
      "2024-12-09 06:31:17.291000: I runner.py:310] Step = 76700 ; steps/s = 1.62, tokens/s = 44178 (44178 target) ; Learning rate = 0.000319 ; Loss = 1.828155\n",
      "2024-12-09 06:32:18.998000: I runner.py:310] Step = 76800 ; steps/s = 1.62, tokens/s = 44222 (44222 target) ; Learning rate = 0.000319 ; Loss = 1.824707\n",
      "2024-12-09 06:33:20.344000: I runner.py:310] Step = 76900 ; steps/s = 1.63, tokens/s = 43764 (43764 target) ; Learning rate = 0.000319 ; Loss = 1.825534\n",
      "2024-12-09 06:34:22.143000: I runner.py:310] Step = 77000 ; steps/s = 1.62, tokens/s = 44158 (44158 target) ; Learning rate = 0.000319 ; Loss = 1.816871\n",
      "2024-12-09 06:35:23.862000: I runner.py:310] Step = 77100 ; steps/s = 1.62, tokens/s = 44208 (44208 target) ; Learning rate = 0.000318 ; Loss = 1.831243\n",
      "2024-12-09 06:36:25.627000: I runner.py:310] Step = 77200 ; steps/s = 1.62, tokens/s = 44198 (44198 target) ; Learning rate = 0.000318 ; Loss = 1.833758\n",
      "2024-12-09 06:37:26.980000: I runner.py:310] Step = 77300 ; steps/s = 1.63, tokens/s = 43757 (43757 target) ; Learning rate = 0.000318 ; Loss = 1.850400\n",
      "2024-12-09 06:38:28.713000: I runner.py:310] Step = 77400 ; steps/s = 1.62, tokens/s = 44208 (44208 target) ; Learning rate = 0.000318 ; Loss = 1.824210\n",
      "2024-12-09 06:39:30.392000: I runner.py:310] Step = 77500 ; steps/s = 1.62, tokens/s = 44254 (44254 target) ; Learning rate = 0.000317 ; Loss = 1.804395\n",
      "2024-12-09 06:40:31.760000: I runner.py:310] Step = 77600 ; steps/s = 1.63, tokens/s = 43735 (43735 target) ; Learning rate = 0.000317 ; Loss = 1.839034\n",
      "2024-12-09 06:41:33.501000: I runner.py:310] Step = 77700 ; steps/s = 1.62, tokens/s = 44200 (44200 target) ; Learning rate = 0.000317 ; Loss = 1.825068\n",
      "2024-12-09 06:42:35.279000: I runner.py:310] Step = 77800 ; steps/s = 1.62, tokens/s = 44186 (44186 target) ; Learning rate = 0.000317 ; Loss = 1.814158\n",
      "2024-12-09 06:43:37.056000: I runner.py:310] Step = 77900 ; steps/s = 1.62, tokens/s = 44174 (44174 target) ; Learning rate = 0.000317 ; Loss = 1.829603\n",
      "2024-12-09 06:44:38.290000: I runner.py:310] Step = 78000 ; steps/s = 1.63, tokens/s = 43832 (43832 target) ; Learning rate = 0.000316 ; Loss = 1.842187\n",
      "2024-12-09 06:45:40.019000: I runner.py:310] Step = 78100 ; steps/s = 1.62, tokens/s = 44214 (44214 target) ; Learning rate = 0.000316 ; Loss = 1.809820\n",
      "2024-12-09 06:46:41.794000: I runner.py:310] Step = 78200 ; steps/s = 1.62, tokens/s = 44177 (44177 target) ; Learning rate = 0.000316 ; Loss = 1.818642\n",
      "2024-12-09 06:47:43.161000: I runner.py:310] Step = 78300 ; steps/s = 1.63, tokens/s = 43738 (43738 target) ; Learning rate = 0.000316 ; Loss = 1.828987\n",
      "2024-12-09 06:48:44.899000: I runner.py:310] Step = 78400 ; steps/s = 1.62, tokens/s = 44207 (44207 target) ; Learning rate = 0.000316 ; Loss = 1.814422\n",
      "2024-12-09 06:49:46.540000: I runner.py:310] Step = 78500 ; steps/s = 1.62, tokens/s = 44261 (44261 target) ; Learning rate = 0.000315 ; Loss = 1.841133\n",
      "2024-12-09 06:50:48.342000: I runner.py:310] Step = 78600 ; steps/s = 1.62, tokens/s = 44175 (44175 target) ; Learning rate = 0.000315 ; Loss = 1.838570\n",
      "2024-12-09 06:51:49.615000: I runner.py:310] Step = 78700 ; steps/s = 1.63, tokens/s = 43812 (43812 target) ; Learning rate = 0.000315 ; Loss = 1.794003\n",
      "2024-12-09 06:52:51.337000: I runner.py:310] Step = 78800 ; steps/s = 1.62, tokens/s = 44215 (44215 target) ; Learning rate = 0.000315 ; Loss = 1.838878\n",
      "2024-12-09 06:53:53.114000: I runner.py:310] Step = 78900 ; steps/s = 1.62, tokens/s = 44170 (44170 target) ; Learning rate = 0.000315 ; Loss = 1.856040\n",
      "2024-12-09 06:54:54.486000: I runner.py:310] Step = 79000 ; steps/s = 1.63, tokens/s = 43743 (43743 target) ; Learning rate = 0.000314 ; Loss = 1.809120\n",
      "2024-12-09 06:55:56.245000: I runner.py:310] Step = 79100 ; steps/s = 1.62, tokens/s = 44196 (44196 target) ; Learning rate = 0.000314 ; Loss = 1.819720\n",
      "2024-12-09 06:56:58.009000: I runner.py:310] Step = 79200 ; steps/s = 1.62, tokens/s = 44189 (44189 target) ; Learning rate = 0.000314 ; Loss = 1.837691\n",
      "2024-12-09 06:57:59.725000: I runner.py:310] Step = 79300 ; steps/s = 1.62, tokens/s = 44214 (44214 target) ; Learning rate = 0.000314 ; Loss = 1.847245\n",
      "2024-12-09 06:59:01.074000: I runner.py:310] Step = 79400 ; steps/s = 1.63, tokens/s = 43773 (43773 target) ; Learning rate = 0.000314 ; Loss = 1.789921\n",
      "2024-12-09 07:00:02.840000: I runner.py:310] Step = 79500 ; steps/s = 1.62, tokens/s = 44179 (44179 target) ; Learning rate = 0.000313 ; Loss = 1.839970\n",
      "2024-12-09 07:01:04.553000: I runner.py:310] Step = 79600 ; steps/s = 1.62, tokens/s = 44210 (44210 target) ; Learning rate = 0.000313 ; Loss = 1.843482\n",
      "2024-12-09 07:02:05.899000: I runner.py:310] Step = 79700 ; steps/s = 1.63, tokens/s = 43754 (43754 target) ; Learning rate = 0.000313 ; Loss = 1.818372\n",
      "2024-12-09 07:03:07.695000: I runner.py:310] Step = 79800 ; steps/s = 1.62, tokens/s = 44185 (44185 target) ; Learning rate = 0.000313 ; Loss = 1.803314\n",
      "2024-12-09 07:04:09.416000: I runner.py:310] Step = 79900 ; steps/s = 1.62, tokens/s = 44201 (44201 target) ; Learning rate = 0.000313 ; Loss = 1.833588\n",
      "2024-12-09 07:05:11.159000: I runner.py:310] Step = 80000 ; steps/s = 1.62, tokens/s = 44199 (44199 target) ; Learning rate = 0.000312 ; Loss = 1.849048\n",
      "2024-12-09 07:05:13.230000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-80000\n",
      "2024-12-09 07:05:13.230000: I training.py:192] Running evaluation for step 80000\n",
      "2024-12-09 07:08:04.182000: I training.py:192] Evaluation result for step 80000: loss = 2.439368 ; perplexity = 11.465798\n",
      "2024-12-09 07:09:05.368000: I runner.py:310] Step = 80100 ; steps/s = 1.63, tokens/s = 43867 (43867 target) ; Learning rate = 0.000312 ; Loss = 1.849256\n",
      "2024-12-09 07:10:07.105000: I runner.py:310] Step = 80200 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000312 ; Loss = 1.800847\n",
      "2024-12-09 07:11:08.875000: I runner.py:310] Step = 80300 ; steps/s = 1.62, tokens/s = 44179 (44179 target) ; Learning rate = 0.000312 ; Loss = 1.814501\n",
      "2024-12-09 07:12:10.250000: I runner.py:310] Step = 80400 ; steps/s = 1.63, tokens/s = 43744 (43744 target) ; Learning rate = 0.000312 ; Loss = 1.809624\n",
      "2024-12-09 07:13:11.967000: I runner.py:310] Step = 80500 ; steps/s = 1.62, tokens/s = 44228 (44228 target) ; Learning rate = 0.000312 ; Loss = 1.831987\n",
      "2024-12-09 07:14:13.664000: I runner.py:310] Step = 80600 ; steps/s = 1.62, tokens/s = 44220 (44220 target) ; Learning rate = 0.000311 ; Loss = 1.837551\n",
      "2024-12-09 07:15:15.377000: I runner.py:310] Step = 80700 ; steps/s = 1.62, tokens/s = 44235 (44235 target) ; Learning rate = 0.000311 ; Loss = 1.838815\n",
      "2024-12-09 07:16:16.695000: I runner.py:310] Step = 80800 ; steps/s = 1.63, tokens/s = 43771 (43771 target) ; Learning rate = 0.000311 ; Loss = 1.832462\n",
      "2024-12-09 07:17:18.417000: I runner.py:310] Step = 80900 ; steps/s = 1.62, tokens/s = 44211 (44211 target) ; Learning rate = 0.000311 ; Loss = 1.820963\n",
      "2024-12-09 07:18:20.217000: I runner.py:310] Step = 81000 ; steps/s = 1.62, tokens/s = 44163 (44163 target) ; Learning rate = 0.000311 ; Loss = 1.823758\n",
      "2024-12-09 07:19:21.470000: I runner.py:310] Step = 81100 ; steps/s = 1.63, tokens/s = 43821 (43821 target) ; Learning rate = 0.000310 ; Loss = 1.814194\n",
      "2024-12-09 07:20:23.227000: I runner.py:310] Step = 81200 ; steps/s = 1.62, tokens/s = 44190 (44190 target) ; Learning rate = 0.000310 ; Loss = 1.818707\n",
      "2024-12-09 07:21:24.926000: I runner.py:310] Step = 81300 ; steps/s = 1.62, tokens/s = 44244 (44244 target) ; Learning rate = 0.000310 ; Loss = 1.833799\n",
      "2024-12-09 07:22:26.659000: I runner.py:310] Step = 81400 ; steps/s = 1.62, tokens/s = 44208 (44208 target) ; Learning rate = 0.000310 ; Loss = 1.844074\n",
      "2024-12-09 07:23:28.002000: I runner.py:310] Step = 81500 ; steps/s = 1.63, tokens/s = 43757 (43757 target) ; Learning rate = 0.000310 ; Loss = 1.853517\n",
      "2024-12-09 07:24:29.777000: I runner.py:310] Step = 81600 ; steps/s = 1.62, tokens/s = 44175 (44175 target) ; Learning rate = 0.000309 ; Loss = 1.810380\n",
      "2024-12-09 07:25:31.500000: I runner.py:310] Step = 81700 ; steps/s = 1.62, tokens/s = 44221 (44221 target) ; Learning rate = 0.000309 ; Loss = 1.811441\n",
      "2024-12-09 07:26:32.796000: I runner.py:310] Step = 81800 ; steps/s = 1.63, tokens/s = 43789 (43789 target) ; Learning rate = 0.000309 ; Loss = 1.813207\n",
      "2024-12-09 07:27:34.549000: I runner.py:310] Step = 81900 ; steps/s = 1.62, tokens/s = 44190 (44190 target) ; Learning rate = 0.000309 ; Loss = 1.812806\n",
      "2024-12-09 07:28:36.232000: I runner.py:310] Step = 82000 ; steps/s = 1.62, tokens/s = 44260 (44260 target) ; Learning rate = 0.000309 ; Loss = 1.839586\n",
      "2024-12-09 07:29:37.924000: I runner.py:310] Step = 82100 ; steps/s = 1.62, tokens/s = 44226 (44226 target) ; Learning rate = 0.000308 ; Loss = 1.838495\n",
      "2024-12-09 07:30:39.139000: I runner.py:310] Step = 82200 ; steps/s = 1.63, tokens/s = 43842 (43842 target) ; Learning rate = 0.000308 ; Loss = 1.854114\n",
      "2024-12-09 07:31:40.905000: I runner.py:310] Step = 82300 ; steps/s = 1.62, tokens/s = 44203 (44203 target) ; Learning rate = 0.000308 ; Loss = 1.820875\n",
      "2024-12-09 07:32:42.675000: I runner.py:310] Step = 82400 ; steps/s = 1.62, tokens/s = 44171 (44171 target) ; Learning rate = 0.000308 ; Loss = 1.805001\n",
      "2024-12-09 07:33:44.027000: I runner.py:310] Step = 82500 ; steps/s = 1.63, tokens/s = 43756 (43756 target) ; Learning rate = 0.000308 ; Loss = 1.831945\n",
      "2024-12-09 07:34:45.721000: I runner.py:310] Step = 82600 ; steps/s = 1.62, tokens/s = 44254 (44254 target) ; Learning rate = 0.000308 ; Loss = 1.797754\n",
      "2024-12-09 07:35:47.351000: I runner.py:310] Step = 82700 ; steps/s = 1.62, tokens/s = 44270 (44270 target) ; Learning rate = 0.000307 ; Loss = 1.817098\n",
      "2024-12-09 07:36:49.119000: I runner.py:310] Step = 82800 ; steps/s = 1.62, tokens/s = 44174 (44174 target) ; Learning rate = 0.000307 ; Loss = 1.812115\n",
      "2024-12-09 07:37:50.367000: I runner.py:310] Step = 82900 ; steps/s = 1.63, tokens/s = 43831 (43831 target) ; Learning rate = 0.000307 ; Loss = 1.836328\n",
      "2024-12-09 07:38:52.121000: I runner.py:310] Step = 83000 ; steps/s = 1.62, tokens/s = 44180 (44180 target) ; Learning rate = 0.000307 ; Loss = 1.811932\n",
      "2024-12-09 07:39:53.769000: I runner.py:310] Step = 83100 ; steps/s = 1.62, tokens/s = 44269 (44269 target) ; Learning rate = 0.000307 ; Loss = 1.817949\n",
      "2024-12-09 07:40:55.062000: I runner.py:310] Step = 83200 ; steps/s = 1.63, tokens/s = 43806 (43806 target) ; Learning rate = 0.000306 ; Loss = 1.847422\n",
      "2024-12-09 07:41:56.782000: I runner.py:310] Step = 83300 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000306 ; Loss = 1.788144\n",
      "2024-12-09 07:42:58.445000: I runner.py:310] Step = 83400 ; steps/s = 1.62, tokens/s = 44269 (44269 target) ; Learning rate = 0.000306 ; Loss = 1.820316\n",
      "2024-12-09 07:44:00.094000: I runner.py:310] Step = 83500 ; steps/s = 1.62, tokens/s = 44111 (44111 target) ; Learning rate = 0.000306 ; Loss = 1.791731\n",
      "2024-12-09 07:45:01.452000: I runner.py:310] Step = 83600 ; steps/s = 1.63, tokens/s = 43892 (43892 target) ; Learning rate = 0.000306 ; Loss = 1.837094\n",
      "2024-12-09 07:46:03.169000: I runner.py:310] Step = 83700 ; steps/s = 1.62, tokens/s = 44235 (44235 target) ; Learning rate = 0.000306 ; Loss = 1.814268\n",
      "2024-12-09 07:47:04.955000: I runner.py:310] Step = 83800 ; steps/s = 1.62, tokens/s = 44172 (44172 target) ; Learning rate = 0.000305 ; Loss = 1.811588\n",
      "2024-12-09 07:48:06.306000: I runner.py:310] Step = 83900 ; steps/s = 1.63, tokens/s = 43756 (43756 target) ; Learning rate = 0.000305 ; Loss = 1.797655\n",
      "2024-12-09 07:49:08.026000: I runner.py:310] Step = 84000 ; steps/s = 1.62, tokens/s = 44211 (44211 target) ; Learning rate = 0.000305 ; Loss = 1.821860\n",
      "2024-12-09 07:50:09.706000: I runner.py:310] Step = 84100 ; steps/s = 1.62, tokens/s = 44251 (44251 target) ; Learning rate = 0.000305 ; Loss = 1.834351\n",
      "2024-12-09 07:51:10.933000: I runner.py:310] Step = 84200 ; steps/s = 1.63, tokens/s = 43837 (43837 target) ; Learning rate = 0.000305 ; Loss = 1.814452\n",
      "2024-12-09 07:52:12.642000: I runner.py:310] Step = 84300 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000304 ; Loss = 1.830483\n",
      "2024-12-09 07:53:14.399000: I runner.py:310] Step = 84400 ; steps/s = 1.62, tokens/s = 44202 (44202 target) ; Learning rate = 0.000304 ; Loss = 1.801697\n",
      "2024-12-09 07:54:16.143000: I runner.py:310] Step = 84500 ; steps/s = 1.62, tokens/s = 44189 (44189 target) ; Learning rate = 0.000304 ; Loss = 1.806149\n",
      "2024-12-09 07:55:17.490000: I runner.py:310] Step = 84600 ; steps/s = 1.63, tokens/s = 43757 (43757 target) ; Learning rate = 0.000304 ; Loss = 1.783336\n",
      "2024-12-09 07:56:19.194000: I runner.py:310] Step = 84700 ; steps/s = 1.62, tokens/s = 44236 (44236 target) ; Learning rate = 0.000304 ; Loss = 1.822838\n",
      "2024-12-09 07:57:20.871000: I runner.py:310] Step = 84800 ; steps/s = 1.62, tokens/s = 44245 (44245 target) ; Learning rate = 0.000304 ; Loss = 1.838421\n",
      "2024-12-09 07:58:22.122000: I runner.py:310] Step = 84900 ; steps/s = 1.63, tokens/s = 43818 (43818 target) ; Learning rate = 0.000303 ; Loss = 1.830449\n",
      "2024-12-09 07:59:23.843000: I runner.py:310] Step = 85000 ; steps/s = 1.62, tokens/s = 44221 (44221 target) ; Learning rate = 0.000303 ; Loss = 1.834659\n",
      "2024-12-09 07:59:23.844000: I training.py:192] Running evaluation for step 85000\n",
      "2024-12-09 08:02:15.390000: I training.py:192] Evaluation result for step 85000: loss = 2.455022 ; perplexity = 11.646691\n",
      "2024-12-09 08:03:16.949000: I runner.py:310] Step = 85100 ; steps/s = 1.62, tokens/s = 44354 (44354 target) ; Learning rate = 0.000303 ; Loss = 1.802586\n",
      "2024-12-09 08:04:18.741000: I runner.py:310] Step = 85200 ; steps/s = 1.62, tokens/s = 44144 (44144 target) ; Learning rate = 0.000303 ; Loss = 1.809577\n",
      "2024-12-09 08:05:20.143000: I runner.py:310] Step = 85300 ; steps/s = 1.63, tokens/s = 43729 (43729 target) ; Learning rate = 0.000303 ; Loss = 1.781167\n",
      "2024-12-09 08:06:21.965000: I runner.py:310] Step = 85400 ; steps/s = 1.62, tokens/s = 44143 (44143 target) ; Learning rate = 0.000302 ; Loss = 1.817677\n",
      "2024-12-09 08:07:23.667000: I runner.py:310] Step = 85500 ; steps/s = 1.62, tokens/s = 44237 (44237 target) ; Learning rate = 0.000302 ; Loss = 1.820797\n",
      "2024-12-09 08:08:25.069000: I runner.py:310] Step = 85600 ; steps/s = 1.63, tokens/s = 43711 (43711 target) ; Learning rate = 0.000302 ; Loss = 1.809302\n",
      "2024-12-09 08:09:26.780000: I runner.py:310] Step = 85700 ; steps/s = 1.62, tokens/s = 44220 (44220 target) ; Learning rate = 0.000302 ; Loss = 1.796781\n",
      "2024-12-09 08:10:28.585000: I runner.py:310] Step = 85800 ; steps/s = 1.62, tokens/s = 44160 (44160 target) ; Learning rate = 0.000302 ; Loss = 1.817173\n",
      "2024-12-09 08:11:30.307000: I runner.py:310] Step = 85900 ; steps/s = 1.62, tokens/s = 44229 (44229 target) ; Learning rate = 0.000302 ; Loss = 1.840129\n",
      "2024-12-09 08:12:31.676000: I runner.py:310] Step = 86000 ; steps/s = 1.63, tokens/s = 43723 (43723 target) ; Learning rate = 0.000301 ; Loss = 1.846820\n",
      "2024-12-09 08:13:33.452000: I runner.py:310] Step = 86100 ; steps/s = 1.62, tokens/s = 44198 (44198 target) ; Learning rate = 0.000301 ; Loss = 1.807327\n",
      "2024-12-09 08:14:35.183000: I runner.py:310] Step = 86200 ; steps/s = 1.62, tokens/s = 44200 (44200 target) ; Learning rate = 0.000301 ; Loss = 1.812970\n",
      "2024-12-09 08:15:36.544000: I runner.py:310] Step = 86300 ; steps/s = 1.63, tokens/s = 43739 (43739 target) ; Learning rate = 0.000301 ; Loss = 1.820986\n",
      "2024-12-09 08:16:38.261000: I runner.py:310] Step = 86400 ; steps/s = 1.62, tokens/s = 44232 (44232 target) ; Learning rate = 0.000301 ; Loss = 1.798086\n",
      "2024-12-09 08:17:40.014000: I runner.py:310] Step = 86500 ; steps/s = 1.62, tokens/s = 44194 (44194 target) ; Learning rate = 0.000301 ; Loss = 1.828901\n",
      "2024-12-09 08:18:41.706000: I runner.py:310] Step = 86600 ; steps/s = 1.62, tokens/s = 44231 (44231 target) ; Learning rate = 0.000300 ; Loss = 1.831890\n",
      "2024-12-09 08:19:43.035000: I runner.py:310] Step = 86700 ; steps/s = 1.63, tokens/s = 43762 (43762 target) ; Learning rate = 0.000300 ; Loss = 1.774773\n",
      "2024-12-09 08:20:44.778000: I runner.py:310] Step = 86800 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000300 ; Loss = 1.833509\n",
      "2024-12-09 08:21:46.472000: I runner.py:310] Step = 86900 ; steps/s = 1.62, tokens/s = 44238 (44238 target) ; Learning rate = 0.000300 ; Loss = 1.836237\n",
      "2024-12-09 08:22:47.783000: I runner.py:310] Step = 87000 ; steps/s = 1.63, tokens/s = 43783 (43783 target) ; Learning rate = 0.000300 ; Loss = 1.808440\n",
      "2024-12-09 08:23:49.479000: I runner.py:310] Step = 87100 ; steps/s = 1.62, tokens/s = 44244 (44244 target) ; Learning rate = 0.000299 ; Loss = 1.808585\n",
      "2024-12-09 08:24:51.218000: I runner.py:310] Step = 87200 ; steps/s = 1.62, tokens/s = 44197 (44197 target) ; Learning rate = 0.000299 ; Loss = 1.823025\n",
      "2024-12-09 08:25:52.937000: I runner.py:310] Step = 87300 ; steps/s = 1.62, tokens/s = 44214 (44214 target) ; Learning rate = 0.000299 ; Loss = 1.820613\n",
      "2024-12-09 08:26:54.237000: I runner.py:310] Step = 87400 ; steps/s = 1.63, tokens/s = 43792 (43792 target) ; Learning rate = 0.000299 ; Loss = 1.793930\n",
      "2024-12-09 08:27:55.970000: I runner.py:310] Step = 87500 ; steps/s = 1.62, tokens/s = 44211 (44211 target) ; Learning rate = 0.000299 ; Loss = 1.828196\n",
      "2024-12-09 08:28:57.726000: I runner.py:310] Step = 87600 ; steps/s = 1.62, tokens/s = 44182 (44182 target) ; Learning rate = 0.000299 ; Loss = 1.825495\n",
      "2024-12-09 08:29:59.085000: I runner.py:310] Step = 87700 ; steps/s = 1.63, tokens/s = 43755 (43755 target) ; Learning rate = 0.000298 ; Loss = 1.801279\n",
      "2024-12-09 08:31:00.765000: I runner.py:310] Step = 87800 ; steps/s = 1.62, tokens/s = 44250 (44250 target) ; Learning rate = 0.000298 ; Loss = 1.803964\n",
      "2024-12-09 08:32:02.466000: I runner.py:310] Step = 87900 ; steps/s = 1.62, tokens/s = 44234 (44234 target) ; Learning rate = 0.000298 ; Loss = 1.819495\n",
      "2024-12-09 08:33:04.215000: I runner.py:310] Step = 88000 ; steps/s = 1.62, tokens/s = 44194 (44194 target) ; Learning rate = 0.000298 ; Loss = 1.820225\n",
      "2024-12-09 08:34:05.560000: I runner.py:310] Step = 88100 ; steps/s = 1.63, tokens/s = 43755 (43755 target) ; Learning rate = 0.000298 ; Loss = 1.767717\n",
      "2024-12-09 08:35:07.281000: I runner.py:310] Step = 88200 ; steps/s = 1.62, tokens/s = 44220 (44220 target) ; Learning rate = 0.000298 ; Loss = 1.823299\n",
      "2024-12-09 08:36:09.001000: I runner.py:310] Step = 88300 ; steps/s = 1.62, tokens/s = 44221 (44221 target) ; Learning rate = 0.000297 ; Loss = 1.815185\n",
      "2024-12-09 08:37:10.349000: I runner.py:310] Step = 88400 ; steps/s = 1.63, tokens/s = 43757 (43757 target) ; Learning rate = 0.000297 ; Loss = 1.829247\n",
      "2024-12-09 08:38:12.045000: I runner.py:310] Step = 88500 ; steps/s = 1.62, tokens/s = 44240 (44240 target) ; Learning rate = 0.000297 ; Loss = 1.816270\n",
      "2024-12-09 08:39:13.741000: I runner.py:310] Step = 88600 ; steps/s = 1.62, tokens/s = 44235 (44235 target) ; Learning rate = 0.000297 ; Loss = 1.798733\n",
      "2024-12-09 08:40:15.502000: I runner.py:310] Step = 88700 ; steps/s = 1.62, tokens/s = 44178 (44178 target) ; Learning rate = 0.000297 ; Loss = 1.804278\n",
      "2024-12-09 08:41:16.891000: I runner.py:310] Step = 88800 ; steps/s = 1.63, tokens/s = 43724 (43724 target) ; Learning rate = 0.000297 ; Loss = 1.794928\n",
      "2024-12-09 08:42:18.656000: I runner.py:310] Step = 88900 ; steps/s = 1.62, tokens/s = 44180 (44180 target) ; Learning rate = 0.000296 ; Loss = 1.822685\n",
      "2024-12-09 08:43:20.363000: I runner.py:310] Step = 89000 ; steps/s = 1.62, tokens/s = 44246 (44246 target) ; Learning rate = 0.000296 ; Loss = 1.820195\n",
      "2024-12-09 08:44:21.637000: I runner.py:310] Step = 89100 ; steps/s = 1.63, tokens/s = 43796 (43796 target) ; Learning rate = 0.000296 ; Loss = 1.801105\n",
      "2024-12-09 08:45:23.406000: I runner.py:310] Step = 89200 ; steps/s = 1.62, tokens/s = 44184 (44184 target) ; Learning rate = 0.000296 ; Loss = 1.811326\n",
      "2024-12-09 08:46:25.150000: I runner.py:310] Step = 89300 ; steps/s = 1.62, tokens/s = 44206 (44206 target) ; Learning rate = 0.000296 ; Loss = 1.824664\n",
      "2024-12-09 08:47:26.972000: I runner.py:310] Step = 89400 ; steps/s = 1.62, tokens/s = 44138 (44138 target) ; Learning rate = 0.000296 ; Loss = 1.819902\n",
      "2024-12-09 08:48:28.308000: I runner.py:310] Step = 89500 ; steps/s = 1.63, tokens/s = 43771 (43771 target) ; Learning rate = 0.000295 ; Loss = 1.835060\n",
      "2024-12-09 08:49:30.063000: I runner.py:310] Step = 89600 ; steps/s = 1.62, tokens/s = 44198 (44198 target) ; Learning rate = 0.000295 ; Loss = 1.805128\n",
      "2024-12-09 08:50:31.780000: I runner.py:310] Step = 89700 ; steps/s = 1.62, tokens/s = 44226 (44226 target) ; Learning rate = 0.000295 ; Loss = 1.823743\n",
      "2024-12-09 08:51:33.100000: I runner.py:310] Step = 89800 ; steps/s = 1.63, tokens/s = 43767 (43767 target) ; Learning rate = 0.000295 ; Loss = 1.797235\n",
      "2024-12-09 08:52:34.843000: I runner.py:310] Step = 89900 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000295 ; Loss = 1.816787\n",
      "2024-12-09 08:53:36.578000: I runner.py:310] Step = 90000 ; steps/s = 1.62, tokens/s = 44201 (44201 target) ; Learning rate = 0.000295 ; Loss = 1.819380\n",
      "2024-12-09 08:53:38.734000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-90000\n",
      "2024-12-09 08:53:38.734000: I training.py:192] Running evaluation for step 90000\n",
      "2024-12-09 08:56:36.200000: I training.py:192] Evaluation result for step 90000: loss = 2.468871 ; perplexity = 11.809106\n",
      "2024-12-09 08:57:37.705000: I runner.py:310] Step = 90100 ; steps/s = 1.63, tokens/s = 44381 (44381 target) ; Learning rate = 0.000294 ; Loss = 1.840862\n",
      "2024-12-09 08:58:39.021000: I runner.py:310] Step = 90200 ; steps/s = 1.63, tokens/s = 43775 (43775 target) ; Learning rate = 0.000294 ; Loss = 1.833158\n",
      "2024-12-09 08:59:40.750000: I runner.py:310] Step = 90300 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000294 ; Loss = 1.788631\n",
      "2024-12-09 09:00:42.496000: I runner.py:310] Step = 90400 ; steps/s = 1.62, tokens/s = 44200 (44200 target) ; Learning rate = 0.000294 ; Loss = 1.796456\n",
      "2024-12-09 09:01:43.857000: I runner.py:310] Step = 90500 ; steps/s = 1.63, tokens/s = 43732 (43732 target) ; Learning rate = 0.000294 ; Loss = 1.792615\n",
      "2024-12-09 09:02:45.537000: I runner.py:310] Step = 90600 ; steps/s = 1.62, tokens/s = 44252 (44252 target) ; Learning rate = 0.000294 ; Loss = 1.805044\n",
      "2024-12-09 09:03:47.262000: I runner.py:310] Step = 90700 ; steps/s = 1.62, tokens/s = 44223 (44223 target) ; Learning rate = 0.000293 ; Loss = 1.815263\n",
      "2024-12-09 09:04:49.037000: I runner.py:310] Step = 90800 ; steps/s = 1.62, tokens/s = 44179 (44179 target) ; Learning rate = 0.000293 ; Loss = 1.826827\n",
      "2024-12-09 09:05:50.387000: I runner.py:310] Step = 90900 ; steps/s = 1.63, tokens/s = 43747 (43747 target) ; Learning rate = 0.000293 ; Loss = 1.789429\n",
      "2024-12-09 09:06:52.150000: I runner.py:310] Step = 91000 ; steps/s = 1.62, tokens/s = 44192 (44192 target) ; Learning rate = 0.000293 ; Loss = 1.815999\n",
      "2024-12-09 09:07:53.894000: I runner.py:310] Step = 91100 ; steps/s = 1.62, tokens/s = 44191 (44191 target) ; Learning rate = 0.000293 ; Loss = 1.815655\n",
      "2024-12-09 09:08:55.292000: I runner.py:310] Step = 91200 ; steps/s = 1.63, tokens/s = 43717 (43717 target) ; Learning rate = 0.000293 ; Loss = 1.823122\n",
      "2024-12-09 09:09:57.040000: I runner.py:310] Step = 91300 ; steps/s = 1.62, tokens/s = 44185 (44185 target) ; Learning rate = 0.000293 ; Loss = 1.798637\n",
      "2024-12-09 09:10:58.733000: I runner.py:310] Step = 91400 ; steps/s = 1.62, tokens/s = 44245 (44245 target) ; Learning rate = 0.000292 ; Loss = 1.808889\n",
      "2024-12-09 09:12:00.472000: I runner.py:310] Step = 91500 ; steps/s = 1.62, tokens/s = 44214 (44214 target) ; Learning rate = 0.000292 ; Loss = 1.796703\n",
      "2024-12-09 09:13:02.421000: I runner.py:310] Step = 91600 ; steps/s = 1.61, tokens/s = 43324 (43324 target) ; Learning rate = 0.000292 ; Loss = 1.795970\n",
      "2024-12-09 09:14:04.123000: I runner.py:310] Step = 91700 ; steps/s = 1.62, tokens/s = 44221 (44221 target) ; Learning rate = 0.000292 ; Loss = 1.813505\n",
      "2024-12-09 09:15:05.813000: I runner.py:310] Step = 91800 ; steps/s = 1.62, tokens/s = 44256 (44256 target) ; Learning rate = 0.000292 ; Loss = 1.810063\n",
      "2024-12-09 09:16:07.192000: I runner.py:310] Step = 91900 ; steps/s = 1.63, tokens/s = 43731 (43731 target) ; Learning rate = 0.000292 ; Loss = 1.794291\n",
      "2024-12-09 09:17:08.919000: I runner.py:310] Step = 92000 ; steps/s = 1.62, tokens/s = 44233 (44233 target) ; Learning rate = 0.000291 ; Loss = 1.803208\n",
      "2024-12-09 09:18:10.668000: I runner.py:310] Step = 92100 ; steps/s = 1.62, tokens/s = 44186 (44186 target) ; Learning rate = 0.000291 ; Loss = 1.813266\n",
      "2024-12-09 09:19:12.075000: I runner.py:310] Step = 92200 ; steps/s = 1.63, tokens/s = 43925 (43925 target) ; Learning rate = 0.000291 ; Loss = 1.855866\n",
      "2024-12-09 09:20:13.657000: I runner.py:310] Step = 92300 ; steps/s = 1.62, tokens/s = 44105 (44105 target) ; Learning rate = 0.000291 ; Loss = 1.796622\n",
      "2024-12-09 09:21:15.397000: I runner.py:310] Step = 92400 ; steps/s = 1.62, tokens/s = 44212 (44212 target) ; Learning rate = 0.000291 ; Loss = 1.818383\n",
      "2024-12-09 09:22:17.091000: I runner.py:310] Step = 92500 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000291 ; Loss = 1.804885\n",
      "2024-12-09 09:23:18.370000: I runner.py:310] Step = 92600 ; steps/s = 1.63, tokens/s = 43808 (43808 target) ; Learning rate = 0.000290 ; Loss = 1.817187\n",
      "2024-12-09 09:24:20.140000: I runner.py:310] Step = 92700 ; steps/s = 1.62, tokens/s = 44182 (44182 target) ; Learning rate = 0.000290 ; Loss = 1.789791\n",
      "2024-12-09 09:25:21.816000: I runner.py:310] Step = 92800 ; steps/s = 1.62, tokens/s = 44245 (44245 target) ; Learning rate = 0.000290 ; Loss = 1.794894\n",
      "2024-12-09 09:26:23.156000: I runner.py:310] Step = 92900 ; steps/s = 1.63, tokens/s = 43759 (43759 target) ; Learning rate = 0.000290 ; Loss = 1.818113\n",
      "2024-12-09 09:27:24.945000: I runner.py:310] Step = 93000 ; steps/s = 1.62, tokens/s = 44176 (44176 target) ; Learning rate = 0.000290 ; Loss = 1.793080\n",
      "2024-12-09 09:28:26.643000: I runner.py:310] Step = 93100 ; steps/s = 1.62, tokens/s = 44243 (44243 target) ; Learning rate = 0.000290 ; Loss = 1.818484\n",
      "2024-12-09 09:29:28.402000: I runner.py:310] Step = 93200 ; steps/s = 1.62, tokens/s = 44179 (44179 target) ; Learning rate = 0.000290 ; Loss = 1.819375\n",
      "2024-12-09 09:30:29.790000: I runner.py:310] Step = 93300 ; steps/s = 1.63, tokens/s = 43712 (43712 target) ; Learning rate = 0.000289 ; Loss = 1.776151\n",
      "2024-12-09 09:31:31.483000: I runner.py:310] Step = 93400 ; steps/s = 1.62, tokens/s = 44252 (44252 target) ; Learning rate = 0.000289 ; Loss = 1.794776\n",
      "2024-12-09 09:32:33.215000: I runner.py:310] Step = 93500 ; steps/s = 1.62, tokens/s = 44215 (44215 target) ; Learning rate = 0.000289 ; Loss = 1.816479\n",
      "2024-12-09 09:33:34.596000: I runner.py:310] Step = 93600 ; steps/s = 1.63, tokens/s = 43714 (43714 target) ; Learning rate = 0.000289 ; Loss = 1.799896\n",
      "2024-12-09 09:34:36.379000: I runner.py:310] Step = 93700 ; steps/s = 1.62, tokens/s = 44171 (44171 target) ; Learning rate = 0.000289 ; Loss = 1.801809\n",
      "2024-12-09 09:35:38.101000: I runner.py:310] Step = 93800 ; steps/s = 1.62, tokens/s = 44219 (44219 target) ; Learning rate = 0.000289 ; Loss = 1.807861\n",
      "2024-12-09 09:36:39.895000: I runner.py:310] Step = 93900 ; steps/s = 1.62, tokens/s = 44172 (44172 target) ; Learning rate = 0.000288 ; Loss = 1.809933\n",
      "2024-12-09 09:37:41.265000: I runner.py:310] Step = 94000 ; steps/s = 1.63, tokens/s = 43738 (43738 target) ; Learning rate = 0.000288 ; Loss = 1.827115\n",
      "2024-12-09 09:38:43.009000: I runner.py:310] Step = 94100 ; steps/s = 1.62, tokens/s = 44221 (44221 target) ; Learning rate = 0.000288 ; Loss = 1.782256\n",
      "2024-12-09 09:39:44.704000: I runner.py:310] Step = 94200 ; steps/s = 1.62, tokens/s = 44225 (44225 target) ; Learning rate = 0.000288 ; Loss = 1.783522\n",
      "2024-12-09 09:40:46.077000: I runner.py:310] Step = 94300 ; steps/s = 1.63, tokens/s = 43727 (43727 target) ; Learning rate = 0.000288 ; Loss = 1.800024\n",
      "2024-12-09 09:41:47.825000: I runner.py:310] Step = 94400 ; steps/s = 1.62, tokens/s = 44204 (44204 target) ; Learning rate = 0.000288 ; Loss = 1.793029\n",
      "2024-12-09 09:42:49.593000: I runner.py:310] Step = 94500 ; steps/s = 1.62, tokens/s = 44181 (44181 target) ; Learning rate = 0.000288 ; Loss = 1.795207\n",
      "2024-12-09 09:43:51.357000: I runner.py:310] Step = 94600 ; steps/s = 1.62, tokens/s = 44178 (44178 target) ; Learning rate = 0.000287 ; Loss = 1.788041\n",
      "2024-12-09 09:44:52.646000: I runner.py:310] Step = 94700 ; steps/s = 1.63, tokens/s = 43811 (43811 target) ; Learning rate = 0.000287 ; Loss = 1.818878\n",
      "2024-12-09 09:45:54.406000: I runner.py:310] Step = 94800 ; steps/s = 1.62, tokens/s = 44192 (44192 target) ; Learning rate = 0.000287 ; Loss = 1.792504\n",
      "2024-12-09 09:46:56.078000: I runner.py:310] Step = 94900 ; steps/s = 1.62, tokens/s = 44242 (44242 target) ; Learning rate = 0.000287 ; Loss = 1.793645\n",
      "2024-12-09 09:47:57.430000: I runner.py:310] Step = 95000 ; steps/s = 1.63, tokens/s = 43761 (43761 target) ; Learning rate = 0.000287 ; Loss = 1.787559\n",
      "2024-12-09 09:47:57.433000: I training.py:192] Running evaluation for step 95000\n",
      "2024-12-09 09:51:02.410000: I training.py:192] Evaluation result for step 95000: loss = 2.476862 ; perplexity = 11.903851\n",
      "2024-12-09 09:52:03.952000: I runner.py:310] Step = 95100 ; steps/s = 1.63, tokens/s = 44373 (44373 target) ; Learning rate = 0.000287 ; Loss = 1.799055\n",
      "2024-12-09 09:53:05.707000: I runner.py:310] Step = 95200 ; steps/s = 1.62, tokens/s = 44181 (44181 target) ; Learning rate = 0.000286 ; Loss = 1.820431\n",
      "2024-12-09 09:54:07.516000: I runner.py:310] Step = 95300 ; steps/s = 1.62, tokens/s = 44141 (44141 target) ; Learning rate = 0.000286 ; Loss = 1.812945\n",
      "2024-12-09 09:55:08.969000: I runner.py:310] Step = 95400 ; steps/s = 1.63, tokens/s = 43675 (43675 target) ; Learning rate = 0.000286 ; Loss = 1.774126\n",
      "2024-12-09 09:56:10.776000: I runner.py:310] Step = 95500 ; steps/s = 1.62, tokens/s = 44161 (44161 target) ; Learning rate = 0.000286 ; Loss = 1.811177\n",
      "2024-12-09 09:57:12.553000: I runner.py:310] Step = 95600 ; steps/s = 1.62, tokens/s = 44185 (44185 target) ; Learning rate = 0.000286 ; Loss = 1.812271\n",
      "2024-12-09 09:58:13.924000: I runner.py:310] Step = 95700 ; steps/s = 1.63, tokens/s = 43733 (43733 target) ; Learning rate = 0.000286 ; Loss = 1.788942\n",
      "2024-12-09 09:59:15.689000: I runner.py:310] Step = 95800 ; steps/s = 1.62, tokens/s = 44190 (44190 target) ; Learning rate = 0.000286 ; Loss = 1.787803\n",
      "2024-12-09 10:00:17.435000: I runner.py:310] Step = 95900 ; steps/s = 1.62, tokens/s = 44213 (44213 target) ; Learning rate = 0.000285 ; Loss = 1.808084\n",
      "2024-12-09 10:01:19.185000: I runner.py:310] Step = 96000 ; steps/s = 1.62, tokens/s = 44184 (44184 target) ; Learning rate = 0.000285 ; Loss = 1.822881\n",
      "2024-12-09 10:02:20.549000: I runner.py:310] Step = 96100 ; steps/s = 1.63, tokens/s = 43743 (43743 target) ; Learning rate = 0.000285 ; Loss = 1.765420\n",
      "2024-12-09 10:03:22.280000: I runner.py:310] Step = 96200 ; steps/s = 1.62, tokens/s = 44207 (44207 target) ; Learning rate = 0.000285 ; Loss = 1.806216\n",
      "2024-12-09 10:04:24.107000: I runner.py:310] Step = 96300 ; steps/s = 1.62, tokens/s = 44140 (44140 target) ; Learning rate = 0.000285 ; Loss = 1.800311\n",
      "2024-12-09 10:05:25.497000: I runner.py:310] Step = 96400 ; steps/s = 1.63, tokens/s = 43721 (43721 target) ; Learning rate = 0.000285 ; Loss = 1.819825\n",
      "2024-12-09 10:06:27.227000: I runner.py:310] Step = 96500 ; steps/s = 1.62, tokens/s = 44197 (44197 target) ; Learning rate = 0.000285 ; Loss = 1.807142\n",
      "2024-12-09 10:07:29.008000: I runner.py:310] Step = 96600 ; steps/s = 1.62, tokens/s = 44190 (44190 target) ; Learning rate = 0.000284 ; Loss = 1.789602\n",
      "2024-12-09 10:08:30.762000: I runner.py:310] Step = 96700 ; steps/s = 1.62, tokens/s = 44193 (44193 target) ; Learning rate = 0.000284 ; Loss = 1.798229\n",
      "2024-12-09 10:09:32.083000: I runner.py:310] Step = 96800 ; steps/s = 1.63, tokens/s = 43773 (43773 target) ; Learning rate = 0.000284 ; Loss = 1.790624\n",
      "2024-12-09 10:10:33.910000: I runner.py:310] Step = 96900 ; steps/s = 1.62, tokens/s = 44142 (44142 target) ; Learning rate = 0.000284 ; Loss = 1.807049\n",
      "2024-12-09 10:11:35.688000: I runner.py:310] Step = 97000 ; steps/s = 1.62, tokens/s = 44182 (44182 target) ; Learning rate = 0.000284 ; Loss = 1.808549\n",
      "2024-12-09 10:12:37.116000: I runner.py:310] Step = 97100 ; steps/s = 1.63, tokens/s = 43690 (43690 target) ; Learning rate = 0.000284 ; Loss = 1.783715\n",
      "2024-12-09 10:13:38.824000: I runner.py:310] Step = 97200 ; steps/s = 1.62, tokens/s = 44232 (44232 target) ; Learning rate = 0.000284 ; Loss = 1.780196\n",
      "2024-12-09 10:14:40.505000: I runner.py:310] Step = 97300 ; steps/s = 1.62, tokens/s = 44246 (44246 target) ; Learning rate = 0.000283 ; Loss = 1.808779\n",
      "2024-12-09 10:15:43.177000: I runner.py:310] Step = 97400 ; steps/s = 1.60, tokens/s = 43547 (43547 target) ; Learning rate = 0.000283 ; Loss = 1.813084\n",
      "2024-12-09 10:16:45.542000: I runner.py:310] Step = 97500 ; steps/s = 1.60, tokens/s = 43045 (43045 target) ; Learning rate = 0.000283 ; Loss = 1.816232\n",
      "2024-12-09 10:17:47.356000: I runner.py:310] Step = 97600 ; steps/s = 1.62, tokens/s = 44135 (44135 target) ; Learning rate = 0.000283 ; Loss = 1.788841\n",
      "2024-12-09 10:18:49.034000: I runner.py:310] Step = 97700 ; steps/s = 1.62, tokens/s = 44255 (44255 target) ; Learning rate = 0.000283 ; Loss = 1.791450\n",
      "2024-12-09 10:19:50.356000: I runner.py:310] Step = 97800 ; steps/s = 1.63, tokens/s = 43770 (43770 target) ; Learning rate = 0.000283 ; Loss = 1.785869\n",
      "2024-12-09 10:20:52.130000: I runner.py:310] Step = 97900 ; steps/s = 1.62, tokens/s = 44181 (44181 target) ; Learning rate = 0.000282 ; Loss = 1.804064\n",
      "2024-12-09 10:21:53.899000: I runner.py:310] Step = 98000 ; steps/s = 1.62, tokens/s = 44184 (44184 target) ; Learning rate = 0.000282 ; Loss = 1.817030\n",
      "2024-12-09 10:22:55.601000: I runner.py:310] Step = 98100 ; steps/s = 1.62, tokens/s = 44233 (44233 target) ; Learning rate = 0.000282 ; Loss = 1.815809\n",
      "2024-12-09 10:23:56.871000: I runner.py:310] Step = 98200 ; steps/s = 1.63, tokens/s = 43817 (43817 target) ; Learning rate = 0.000282 ; Loss = 1.815922\n",
      "2024-12-09 10:24:58.633000: I runner.py:310] Step = 98300 ; steps/s = 1.62, tokens/s = 44175 (44175 target) ; Learning rate = 0.000282 ; Loss = 1.792979\n",
      "2024-12-09 10:26:00.360000: I runner.py:310] Step = 98400 ; steps/s = 1.62, tokens/s = 44218 (44218 target) ; Learning rate = 0.000282 ; Loss = 1.785504\n",
      "2024-12-09 10:27:01.711000: I runner.py:310] Step = 98500 ; steps/s = 1.63, tokens/s = 43754 (43754 target) ; Learning rate = 0.000282 ; Loss = 1.774709\n",
      "2024-12-09 10:28:03.423000: I runner.py:310] Step = 98600 ; steps/s = 1.62, tokens/s = 44232 (44232 target) ; Learning rate = 0.000281 ; Loss = 1.805077\n",
      "2024-12-09 10:29:05.102000: I runner.py:310] Step = 98700 ; steps/s = 1.62, tokens/s = 44253 (44253 target) ; Learning rate = 0.000281 ; Loss = 1.805684\n",
      "2024-12-09 10:30:06.772000: I runner.py:310] Step = 98800 ; steps/s = 1.62, tokens/s = 44242 (44242 target) ; Learning rate = 0.000281 ; Loss = 1.810461\n",
      "2024-12-09 10:31:08.298000: I runner.py:310] Step = 98900 ; steps/s = 1.63, tokens/s = 43629 (43629 target) ; Learning rate = 0.000281 ; Loss = 1.764954\n",
      "2024-12-09 10:32:10.600000: I runner.py:310] Step = 99000 ; steps/s = 1.61, tokens/s = 43796 (43796 target) ; Learning rate = 0.000281 ; Loss = 1.814585\n",
      "2024-12-09 10:33:12.678000: I runner.py:310] Step = 99100 ; steps/s = 1.61, tokens/s = 43959 (43959 target) ; Learning rate = 0.000281 ; Loss = 1.805555\n",
      "2024-12-09 10:34:15.226000: I runner.py:310] Step = 99200 ; steps/s = 1.60, tokens/s = 42925 (42925 target) ; Learning rate = 0.000281 ; Loss = 1.807176\n",
      "2024-12-09 10:35:17.915000: I runner.py:310] Step = 99300 ; steps/s = 1.60, tokens/s = 43531 (43531 target) ; Learning rate = 0.000280 ; Loss = 1.788275\n",
      "2024-12-09 10:36:19.892000: I runner.py:310] Step = 99400 ; steps/s = 1.61, tokens/s = 44030 (44030 target) ; Learning rate = 0.000280 ; Loss = 1.788492\n",
      "2024-12-09 10:37:22.402000: I runner.py:310] Step = 99500 ; steps/s = 1.60, tokens/s = 43665 (43665 target) ; Learning rate = 0.000280 ; Loss = 1.802892\n",
      "2024-12-09 10:38:24.248000: I runner.py:310] Step = 99600 ; steps/s = 1.62, tokens/s = 43398 (43398 target) ; Learning rate = 0.000280 ; Loss = 1.792128\n",
      "2024-12-09 10:39:26.836000: I runner.py:310] Step = 99700 ; steps/s = 1.60, tokens/s = 43615 (43615 target) ; Learning rate = 0.000280 ; Loss = 1.779992\n",
      "2024-12-09 10:40:29.231000: I runner.py:310] Step = 99800 ; steps/s = 1.60, tokens/s = 43744 (43744 target) ; Learning rate = 0.000280 ; Loss = 1.806364\n",
      "2024-12-09 10:41:30.663000: I runner.py:310] Step = 99900 ; steps/s = 1.63, tokens/s = 43678 (43678 target) ; Learning rate = 0.000280 ; Loss = 1.782303\n",
      "2024-12-09 10:42:33.153000: I runner.py:310] Step = 100000 ; steps/s = 1.60, tokens/s = 43681 (43681 target) ; Learning rate = 0.000280 ; Loss = 1.799014\n",
      "2024-12-09 10:42:35.292000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-100000\n",
      "2024-12-09 10:42:35.292000: I training.py:192] Running evaluation for step 100000\n",
      "2024-12-09 10:45:36.345000: I training.py:192] Evaluation result for step 100000: loss = 2.488562 ; perplexity = 12.043943\n",
      "2024-12-09 10:46:39.603000: I runner.py:310] Step = 100100 ; steps/s = 1.58, tokens/s = 43151 (43151 target) ; Learning rate = 0.000279 ; Loss = 1.795247\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Kk-En (POS Tags) -> Tr-En (TED2020)(POS Tags)\n",
    "!onmt-main --model kk-tr-en-modelim.py --config kk-tr-en-pos-asl-2.yml --auto_config --checkpoint_path POS_KK_TR_EN/ckpt-100000 train --with_eval --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06c8304d-3343-4a77-8ab4-41687decf1e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-09 10:49:08.762012: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-09 10:49:09.555017: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-09 10:49:09.555111: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-09 10:49:09.555125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-09 10:49:10.568000: I main.py:308] Loading model description from POS_KK_TR_EN_2/model_description.py\n",
      "2024-12-09 10:49:10.765000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-09 10:49:10.765000: I main.py:315] Using model:\n",
      "(model): MultiFeaturesTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): ParallelInputter(\n",
      "      (inputters): ListWrapper(\n",
      "        (0): WordEmbedder()\n",
      "        (1): WordEmbedder()\n",
      "      )\n",
      "      (reducer): ConcatReducer()\n",
      "    )\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): ParallelInputter(\n",
      "        (inputters): ListWrapper(\n",
      "          (0): WordEmbedder()\n",
      "          (1): WordEmbedder()\n",
      "        )\n",
      "        (reducer): ConcatReducer()\n",
      "      )\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-09 10:49:10.771000: I main.py:340] Using parameters:\n",
      "data:\n",
      "  eval_features_file:\n",
      "  - TED2020_tokens_dev_shared\n",
      "  - TED2020_pos_tags_dev_shared.txt\n",
      "  eval_labels_file: TED2020_dev_target_tokens_shared.txt\n",
      "  source_1_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  source_2_vocabulary: Zemberek_TR_unique_pos\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file:\n",
      "  - TED2020_tokens_train_shared\n",
      "  - TED2020_pos_tags_train_shared.txt\n",
      "  train_labels_file: TED2020_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: POS_KK_TR_EN_2\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 205000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-09 10:49:10.951723: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-09 10:49:11.576684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-09 10:49:11.737000: I inputter.py:316] Initialized source_1 input layer:\n",
      "2024-12-09 10:49:11.737000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-09 10:49:11.737000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-09 10:49:11.741000: I inputter.py:316] Initialized source_2 input layer:\n",
      "2024-12-09 10:49:11.741000: I inputter.py:316]  - vocabulary size: 21\n",
      "2024-12-09 10:49:11.741000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-09 10:49:11.813000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-09 10:49:11.813000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-09 10:49:11.813000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-09 10:49:11.832000: I runner.py:462] Restored checkpoint POS_KK_TR_EN_2/ckpt-100000\n",
      "2024-12-09 10:49:11.877000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-09 10:49:12.654270: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-09 10:49:12.778000: I runner.py:471] Tracing and optimizing the inference graph...\n",
      "2024-12-09 10:49:26.398749: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-09 10:49:27.297974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-09 10:49:47.854000: I runner.py:471] 1699 predictions are buffered, but waiting for the prediction of queued line 31 to advance the output...\n",
      "2024-12-09 10:50:03.095000: I runner.py:471] 2703 predictions are buffered, but waiting for the prediction of queued line 51 to advance the output...\n",
      "2024-12-09 10:50:13.168000: I runner.py:471] 3439 predictions are buffered, but waiting for the prediction of queued line 51 to advance the output...\n",
      "2024-12-09 10:50:25.195000: I runner.py:471] 4175 predictions are buffered, but waiting for the prediction of queued line 51 to advance the output...\n",
      "2024-12-09 10:50:35.778000: I runner.py:471] 5167 predictions are buffered, but waiting for the prediction of queued line 51 to advance the output...\n",
      "2024-12-09 10:50:45.870000: I runner.py:471] 5743 predictions are buffered, but waiting for the prediction of queued line 51 to advance the output...\n",
      "2024-12-09 10:50:56.001000: I runner.py:471] 6479 predictions are buffered, but waiting for the prediction of queued line 51 to advance the output...\n",
      "2024-12-09 10:51:13.533000: I runner.py:471] 7603 predictions are buffered, but waiting for the prediction of queued line 111 to advance the output...\n",
      "2024-12-09 10:51:23.561000: I runner.py:471] 8403 predictions are buffered, but waiting for the prediction of queued line 111 to advance the output...\n",
      "2024-12-09 10:51:34.322000: I runner.py:471] 8915 predictions are buffered, but waiting for the prediction of queued line 111 to advance the output...\n",
      "2024-12-09 10:51:46.990000: I runner.py:471] 9587 predictions are buffered, but waiting for the prediction of queued line 111 to advance the output...\n",
      "2024-12-09 10:51:59.074000: I runner.py:471] 9834 predictions are buffered, but waiting for the prediction of queued line 111 to advance the output...\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 onmt-main --config kk-tr-en-pos-asl-2.yml --auto_config --checkpoint_path POS_KK_TR_EN_2/ckpt-100000 infer --features_file TED2020_tokens_test_shared TED2020_pos_tags_test_shared.txt --predictions_file output_kk_tr_en_pos.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7938ae-840f-4e4d-8e07-80cef6eda673",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 MT-Preparation/subwording/3-desubword.py en_shared_vocab.model output_kk_tr_en_pos.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94ec7ece-a042-4578-b13d-3cede972c7d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference first sentence: It's quite widespread.\n",
      "Translated first sentence: And that's a pretty common theme .\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "BLEU2:  BLEU = 21.50 50.2/26.8/16.1/9.9 (BP = 1.000 ratio = 1.193 hyp_len = 236101 ref_len = 197923)\n",
      "CHRF:  chrF2 = 50.69\n"
     ]
    }
   ],
   "source": [
    "# BLEU and chrF scores\n",
    "!python3 compute-bleu.py TED2020.en-tr.en-filtered.en.test output_kk_tr_en_pos.txt.desubword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "077a9bfd-e19f-48f7-a4cc-7880c39b82fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama METEOR Puan: 0.5678681502396242\n"
     ]
    }
   ],
   "source": [
    "# Average METEOR score (Ortalama METEOR Puan)\n",
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "def read_and_tokenize_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return [nltk.word_tokenize(line.strip()) for line in lines]\n",
    "\n",
    "def calculate_meteor(reference_file, hypothesis_file):\n",
    "    references = read_and_tokenize_file(reference_file)\n",
    "    hypotheses = read_and_tokenize_file(hypothesis_file)\n",
    "    \n",
    "    if len(references) != len(hypotheses):\n",
    "        raise ValueError(\"Dosyalarn satr saylar elemiyor\")\n",
    "\n",
    "    total_meteor_score = 0.0\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        total_meteor_score += meteor_score([ref], hyp)\n",
    "\n",
    "    average_meteor_score = total_meteor_score / len(references)\n",
    "    return average_meteor_score\n",
    "\n",
    "reference_file = 'TED2020.en-tr.en-filtered.en.test'\n",
    "hypothesis_file = 'output_kk_tr_en_pos.txt.desubword'\n",
    "\n",
    "score = calculate_meteor(reference_file, hypothesis_file)\n",
    "print(f\"Ortalama METEOR Puan: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e166941-79e0-4213-bcfd-7e6c98797afa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 10:35:15.838710: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-12 10:35:16.638176: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-12 10:35:16.638240: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-12 10:35:16.638248: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-12 10:35:17.664000: I onmt-main:8] Creating model directory TR-EN_std\n",
      "2024-12-12 10:35:17.869000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-12 10:35:17.869000: I main.py:315] Using model:\n",
      "(model): MyCustomTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): WordEmbedder()\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): WordEmbedder()\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-12 10:35:17.872129: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-12 10:35:20.735911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-12 10:35:20.736578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7745 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-12-12 10:35:20.737084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 674 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "2024-12-12 10:35:20.741000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file: TED2020_tokens_dev\n",
      "  eval_labels_file: TED2020_dev_target_tokens.txt\n",
      "  source_vocabulary: tr_vocab.vocab\n",
      "  target_vocabulary: en_vocab.vocab\n",
      "  train_features_file: TED2020_tokens_train\n",
      "  train_labels_file: TED2020_train_target_tokens.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: TR-EN_std\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 250000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-12 10:35:21.076000: I inputter.py:316] Initialized source input layer:\n",
      "2024-12-12 10:35:21.076000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-12 10:35:21.076000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-12 10:35:21.149000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-12 10:35:21.150000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-12 10:35:21.150000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-12 10:35:21.154000: W runner.py:269] No checkpoint to restore in TR-EN_std\n",
      "2024-12-12 10:35:21.157000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "2024-12-12 10:35:21.202000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-12 10:35:22.152923: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-12 10:35:22.274000: I main.py:325] Accumulate gradients of 7 iterations to reach effective batch size of 25000\n",
      "2024-12-12 10:35:22.398000: I mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2024-12-12 10:35:22.544000: I dataset_ops.py:2542] Training on 337547 examples\n",
      "2024-12-12 10:36:28.891099: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-12 10:36:30.006769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-12 10:36:30.299460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-12 10:36:39.742000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-12 10:36:39.764000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-12 10:36:41.335000: I cross_device_ops.py:897] batch_all_reduce: 260 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-12 10:36:46.366000: I cross_device_ops.py:897] batch_all_reduce: 260 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-12 10:36:52.319000: I runner.py:310] Number of model parameters: 93326081\n",
      "2024-12-12 10:36:52.323000: I runner.py:310] Number of model weights: 260 (trainable = 260, non trainable = 0)\n",
      "2024-12-12 10:36:52.357000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-12 10:36:52.364000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-12 10:36:54.404000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-1\n",
      "2024-12-12 10:36:55.001000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-12 10:36:55.009000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-12 10:36:55.635000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-12 10:36:55.644000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-12 10:36:56.230000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-12 10:36:56.253000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-12 10:37:55.003000: I runner.py:310] Step = 100 ; steps/s = 1.64, tokens/s = 78767 (33354 source, 45413 target) ; Learning rate = 0.000009 ; Loss = 9.747484\n",
      "2024-12-12 10:38:57.485000: I runner.py:310] Step = 200 ; steps/s = 1.60, tokens/s = 76960 (32599 source, 44361 target) ; Learning rate = 0.000018 ; Loss = 8.860649\n",
      "2024-12-12 10:40:00.212000: I runner.py:310] Step = 300 ; steps/s = 1.59, tokens/s = 76645 (32443 source, 44202 target) ; Learning rate = 0.000027 ; Loss = 7.741114\n",
      "2024-12-12 10:41:01.607000: I runner.py:310] Step = 400 ; steps/s = 1.63, tokens/s = 77019 (32633 source, 44386 target) ; Learning rate = 0.000035 ; Loss = 6.948563\n",
      "2024-12-12 10:42:02.900000: I runner.py:310] Step = 500 ; steps/s = 1.63, tokens/s = 78444 (33212 source, 45232 target) ; Learning rate = 0.000044 ; Loss = 6.604214\n",
      "2024-12-12 10:43:04.159000: I runner.py:310] Step = 600 ; steps/s = 1.63, tokens/s = 78485 (33232 source, 45253 target) ; Learning rate = 0.000053 ; Loss = 6.091356\n",
      "2024-12-12 10:44:05.458000: I runner.py:310] Step = 700 ; steps/s = 1.63, tokens/s = 78433 (33210 source, 45223 target) ; Learning rate = 0.000062 ; Loss = 5.905806\n",
      "2024-12-12 10:45:06.476000: I runner.py:310] Step = 800 ; steps/s = 1.64, tokens/s = 77499 (32825 source, 44674 target) ; Learning rate = 0.000071 ; Loss = 5.846711\n",
      "2024-12-12 10:46:07.675000: I runner.py:310] Step = 900 ; steps/s = 1.63, tokens/s = 78545 (33245 source, 45300 target) ; Learning rate = 0.000080 ; Loss = 5.671990\n",
      "2024-12-12 10:47:08.945000: I runner.py:310] Step = 1000 ; steps/s = 1.63, tokens/s = 78500 (33256 source, 45244 target) ; Learning rate = 0.000088 ; Loss = 5.526238\n",
      "2024-12-12 10:48:09.741000: I runner.py:310] Step = 1100 ; steps/s = 1.65, tokens/s = 77750 (32911 source, 44839 target) ; Learning rate = 0.000097 ; Loss = 5.419189\n",
      "2024-12-12 10:49:10.972000: I runner.py:310] Step = 1200 ; steps/s = 1.63, tokens/s = 78527 (33255 source, 45272 target) ; Learning rate = 0.000106 ; Loss = 5.390561\n",
      "2024-12-12 10:50:12.120000: I runner.py:310] Step = 1300 ; steps/s = 1.64, tokens/s = 78647 (33314 source, 45333 target) ; Learning rate = 0.000115 ; Loss = 5.231230\n",
      "2024-12-12 10:51:13.326000: I runner.py:310] Step = 1400 ; steps/s = 1.63, tokens/s = 78539 (33241 source, 45298 target) ; Learning rate = 0.000124 ; Loss = 5.160791\n",
      "2024-12-12 10:52:14.220000: I runner.py:310] Step = 1500 ; steps/s = 1.64, tokens/s = 77662 (32902 source, 44760 target) ; Learning rate = 0.000133 ; Loss = 4.974434\n",
      "2024-12-12 10:53:15.397000: I runner.py:310] Step = 1600 ; steps/s = 1.63, tokens/s = 78591 (33270 source, 45321 target) ; Learning rate = 0.000142 ; Loss = 4.923938\n",
      "2024-12-12 10:54:16.626000: I runner.py:310] Step = 1700 ; steps/s = 1.63, tokens/s = 78532 (33254 source, 45278 target) ; Learning rate = 0.000150 ; Loss = 4.892431\n",
      "2024-12-12 10:55:17.478000: I runner.py:310] Step = 1800 ; steps/s = 1.64, tokens/s = 77704 (32907 source, 44797 target) ; Learning rate = 0.000159 ; Loss = 4.730258\n",
      "2024-12-12 10:56:18.728000: I runner.py:310] Step = 1900 ; steps/s = 1.63, tokens/s = 78522 (33261 source, 45261 target) ; Learning rate = 0.000168 ; Loss = 4.560883\n",
      "2024-12-12 10:57:19.900000: I runner.py:310] Step = 2000 ; steps/s = 1.63, tokens/s = 78580 (33258 source, 45322 target) ; Learning rate = 0.000177 ; Loss = 4.474342\n",
      "2024-12-12 10:58:21.166000: I runner.py:310] Step = 2100 ; steps/s = 1.63, tokens/s = 78454 (33209 source, 45245 target) ; Learning rate = 0.000186 ; Loss = 4.362378\n",
      "2024-12-12 10:59:22.088000: I runner.py:310] Step = 2200 ; steps/s = 1.64, tokens/s = 77647 (32901 source, 44746 target) ; Learning rate = 0.000195 ; Loss = 4.350815\n",
      "2024-12-12 11:00:23.298000: I runner.py:310] Step = 2300 ; steps/s = 1.63, tokens/s = 78576 (33286 source, 45290 target) ; Learning rate = 0.000203 ; Loss = 4.219478\n",
      "2024-12-12 11:01:24.546000: I runner.py:310] Step = 2400 ; steps/s = 1.63, tokens/s = 78469 (33211 source, 45258 target) ; Learning rate = 0.000212 ; Loss = 4.150044\n",
      "2024-12-12 11:02:25.730000: I runner.py:310] Step = 2500 ; steps/s = 1.63, tokens/s = 78581 (33272 source, 45309 target) ; Learning rate = 0.000221 ; Loss = 4.005702\n",
      "2024-12-12 11:03:26.587000: I runner.py:310] Step = 2600 ; steps/s = 1.64, tokens/s = 77686 (32899 source, 44787 target) ; Learning rate = 0.000230 ; Loss = 3.942479\n",
      "2024-12-12 11:04:27.825000: I runner.py:310] Step = 2700 ; steps/s = 1.63, tokens/s = 78549 (33273 source, 45276 target) ; Learning rate = 0.000239 ; Loss = 3.817537\n",
      "2024-12-12 11:05:29.087000: I runner.py:310] Step = 2800 ; steps/s = 1.63, tokens/s = 78491 (33242 source, 45249 target) ; Learning rate = 0.000248 ; Loss = 3.817996\n",
      "2024-12-12 11:06:29.896000: I runner.py:310] Step = 2900 ; steps/s = 1.64, tokens/s = 77720 (32889 source, 44831 target) ; Learning rate = 0.000256 ; Loss = 3.666865\n",
      "2024-12-12 11:07:31.146000: I runner.py:310] Step = 3000 ; steps/s = 1.63, tokens/s = 78501 (33244 source, 45257 target) ; Learning rate = 0.000265 ; Loss = 3.693614\n",
      "2024-12-12 11:08:32.305000: I runner.py:310] Step = 3100 ; steps/s = 1.64, tokens/s = 78640 (33315 source, 45325 target) ; Learning rate = 0.000274 ; Loss = 3.683947\n",
      "2024-12-12 11:09:33.532000: I runner.py:310] Step = 3200 ; steps/s = 1.63, tokens/s = 78516 (33236 source, 45280 target) ; Learning rate = 0.000283 ; Loss = 3.675197\n",
      "2024-12-12 11:10:34.406000: I runner.py:310] Step = 3300 ; steps/s = 1.64, tokens/s = 77665 (32892 source, 44773 target) ; Learning rate = 0.000292 ; Loss = 3.542909\n",
      "2024-12-12 11:11:35.626000: I runner.py:310] Step = 3400 ; steps/s = 1.63, tokens/s = 78522 (33240 source, 45282 target) ; Learning rate = 0.000301 ; Loss = 3.470951\n",
      "2024-12-12 11:12:36.938000: I runner.py:310] Step = 3500 ; steps/s = 1.63, tokens/s = 78436 (33216 source, 45220 target) ; Learning rate = 0.000309 ; Loss = 3.480307\n",
      "2024-12-12 11:13:37.782000: I runner.py:310] Step = 3600 ; steps/s = 1.64, tokens/s = 77717 (32913 source, 44804 target) ; Learning rate = 0.000318 ; Loss = 3.420403\n",
      "2024-12-12 11:14:39.027000: I runner.py:310] Step = 3700 ; steps/s = 1.63, tokens/s = 78510 (33242 source, 45268 target) ; Learning rate = 0.000327 ; Loss = 3.351983\n",
      "2024-12-12 11:15:40.258000: I runner.py:310] Step = 3800 ; steps/s = 1.63, tokens/s = 78522 (33249 source, 45273 target) ; Learning rate = 0.000336 ; Loss = 3.266420\n",
      "2024-12-12 11:16:41.513000: I runner.py:310] Step = 3900 ; steps/s = 1.63, tokens/s = 78485 (33233 source, 45252 target) ; Learning rate = 0.000345 ; Loss = 3.284368\n",
      "2024-12-12 11:17:42.313000: I runner.py:310] Step = 4000 ; steps/s = 1.65, tokens/s = 77799 (32964 source, 44835 target) ; Learning rate = 0.000354 ; Loss = 3.311331\n",
      "2024-12-12 11:18:43.567000: I runner.py:310] Step = 4100 ; steps/s = 1.63, tokens/s = 78481 (33224 source, 45257 target) ; Learning rate = 0.000362 ; Loss = 3.248033\n",
      "2024-12-12 11:19:44.745000: I runner.py:310] Step = 4200 ; steps/s = 1.63, tokens/s = 78577 (33265 source, 45312 target) ; Learning rate = 0.000371 ; Loss = 3.229269\n",
      "2024-12-12 11:20:45.574000: I runner.py:310] Step = 4300 ; steps/s = 1.64, tokens/s = 77742 (32925 source, 44817 target) ; Learning rate = 0.000380 ; Loss = 3.386002\n",
      "2024-12-12 11:21:46.798000: I runner.py:310] Step = 4400 ; steps/s = 1.63, tokens/s = 78543 (33265 source, 45278 target) ; Learning rate = 0.000389 ; Loss = 3.037223\n",
      "2024-12-12 11:22:48.030000: I runner.py:310] Step = 4500 ; steps/s = 1.63, tokens/s = 78521 (33252 source, 45269 target) ; Learning rate = 0.000398 ; Loss = 3.120667\n",
      "2024-12-12 11:23:49.310000: I runner.py:310] Step = 4600 ; steps/s = 1.63, tokens/s = 78459 (33215 source, 45244 target) ; Learning rate = 0.000407 ; Loss = 3.133835\n",
      "2024-12-12 11:24:50.222000: I runner.py:310] Step = 4700 ; steps/s = 1.64, tokens/s = 77613 (32855 source, 44758 target) ; Learning rate = 0.000416 ; Loss = 3.103266\n",
      "2024-12-12 11:25:51.529000: I runner.py:310] Step = 4800 ; steps/s = 1.63, tokens/s = 78445 (33229 source, 45216 target) ; Learning rate = 0.000424 ; Loss = 3.042130\n",
      "2024-12-12 11:26:52.826000: I runner.py:310] Step = 4900 ; steps/s = 1.63, tokens/s = 78435 (33211 source, 45224 target) ; Learning rate = 0.000433 ; Loss = 3.124906\n",
      "2024-12-12 11:27:54.038000: I runner.py:310] Step = 5000 ; steps/s = 1.63, tokens/s = 78550 (33261 source, 45289 target) ; Learning rate = 0.000442 ; Loss = 2.988898\n",
      "2024-12-12 11:27:54.039000: I training.py:192] Running evaluation for step 5000\n",
      "2024-12-12 11:37:08.055000: I training.py:192] Evaluation result for step 5000: loss = 2.131578 ; perplexity = 8.428158\n",
      "2024-12-12 11:38:08.921000: I runner.py:310] Step = 5100 ; steps/s = 1.64, tokens/s = 77689 (32902 source, 44787 target) ; Learning rate = 0.000451 ; Loss = 2.957044\n",
      "2024-12-12 11:39:10.262000: I runner.py:310] Step = 5200 ; steps/s = 1.63, tokens/s = 78381 (33182 source, 45199 target) ; Learning rate = 0.000460 ; Loss = 2.912055\n",
      "2024-12-12 11:40:11.611000: I runner.py:310] Step = 5300 ; steps/s = 1.63, tokens/s = 78402 (33213 source, 45189 target) ; Learning rate = 0.000469 ; Loss = 2.955607\n",
      "2024-12-12 11:41:12.543000: I runner.py:310] Step = 5400 ; steps/s = 1.64, tokens/s = 77574 (32836 source, 44738 target) ; Learning rate = 0.000477 ; Loss = 2.968147\n",
      "2024-12-12 11:42:13.883000: I runner.py:310] Step = 5500 ; steps/s = 1.63, tokens/s = 78377 (33183 source, 45194 target) ; Learning rate = 0.000486 ; Loss = 2.947558\n",
      "2024-12-12 11:43:15.234000: I runner.py:310] Step = 5600 ; steps/s = 1.63, tokens/s = 78388 (33207 source, 45181 target) ; Learning rate = 0.000495 ; Loss = 2.925695\n",
      "2024-12-12 11:44:16.590000: I runner.py:310] Step = 5700 ; steps/s = 1.63, tokens/s = 78364 (33183 source, 45181 target) ; Learning rate = 0.000504 ; Loss = 2.837369\n",
      "2024-12-12 11:45:17.618000: I runner.py:310] Step = 5800 ; steps/s = 1.64, tokens/s = 77470 (32800 source, 44670 target) ; Learning rate = 0.000513 ; Loss = 2.708786\n",
      "2024-12-12 11:46:19.003000: I runner.py:310] Step = 5900 ; steps/s = 1.63, tokens/s = 78322 (33150 source, 45172 target) ; Learning rate = 0.000522 ; Loss = 2.875727\n",
      "2024-12-12 11:47:20.367000: I runner.py:310] Step = 6000 ; steps/s = 1.63, tokens/s = 78369 (33202 source, 45167 target) ; Learning rate = 0.000530 ; Loss = 2.866526\n",
      "2024-12-12 11:48:21.352000: I runner.py:310] Step = 6100 ; steps/s = 1.64, tokens/s = 77531 (32832 source, 44699 target) ; Learning rate = 0.000539 ; Loss = 2.836980\n",
      "2024-12-12 11:49:22.687000: I runner.py:310] Step = 6200 ; steps/s = 1.63, tokens/s = 78393 (33198 source, 45195 target) ; Learning rate = 0.000548 ; Loss = 2.846721\n",
      "2024-12-12 11:50:23.980000: I runner.py:310] Step = 6300 ; steps/s = 1.63, tokens/s = 78445 (33218 source, 45227 target) ; Learning rate = 0.000557 ; Loss = 2.779156\n",
      "2024-12-12 11:51:25.317000: I runner.py:310] Step = 6400 ; steps/s = 1.63, tokens/s = 78395 (33199 source, 45196 target) ; Learning rate = 0.000566 ; Loss = 2.807131\n",
      "2024-12-12 11:52:26.255000: I runner.py:310] Step = 6500 ; steps/s = 1.64, tokens/s = 77543 (32816 source, 44727 target) ; Learning rate = 0.000575 ; Loss = 2.702325\n",
      "2024-12-12 11:53:27.534000: I runner.py:310] Step = 6600 ; steps/s = 1.63, tokens/s = 78468 (33222 source, 45246 target) ; Learning rate = 0.000583 ; Loss = 2.740072\n",
      "2024-12-12 11:54:28.791000: I runner.py:310] Step = 6700 ; steps/s = 1.63, tokens/s = 78500 (33242 source, 45258 target) ; Learning rate = 0.000592 ; Loss = 2.745534\n",
      "2024-12-12 11:55:29.763000: I runner.py:310] Step = 6800 ; steps/s = 1.64, tokens/s = 77815 (32970 source, 44845 target) ; Learning rate = 0.000601 ; Loss = 2.973497\n",
      "2024-12-12 11:56:30.993000: I runner.py:310] Step = 6900 ; steps/s = 1.63, tokens/s = 78285 (33154 source, 45131 target) ; Learning rate = 0.000610 ; Loss = 2.718964\n",
      "2024-12-12 11:57:32.214000: I runner.py:310] Step = 7000 ; steps/s = 1.63, tokens/s = 78565 (33290 source, 45275 target) ; Learning rate = 0.000619 ; Loss = 2.738278\n",
      "2024-12-12 11:58:33.540000: I runner.py:310] Step = 7100 ; steps/s = 1.63, tokens/s = 78378 (33171 source, 45207 target) ; Learning rate = 0.000628 ; Loss = 2.707689\n",
      "2024-12-12 11:59:34.413000: I runner.py:310] Step = 7200 ; steps/s = 1.64, tokens/s = 77681 (32893 source, 44788 target) ; Learning rate = 0.000636 ; Loss = 2.631568\n",
      "2024-12-12 12:00:35.744000: I runner.py:310] Step = 7300 ; steps/s = 1.63, tokens/s = 78421 (33215 source, 45206 target) ; Learning rate = 0.000645 ; Loss = 2.668357\n",
      "2024-12-12 12:01:37.092000: I runner.py:310] Step = 7400 ; steps/s = 1.63, tokens/s = 78343 (33162 source, 45181 target) ; Learning rate = 0.000654 ; Loss = 2.622783\n",
      "2024-12-12 12:02:38.372000: I runner.py:310] Step = 7500 ; steps/s = 1.63, tokens/s = 78457 (33221 source, 45236 target) ; Learning rate = 0.000663 ; Loss = 2.668232\n",
      "2024-12-12 12:03:39.191000: I runner.py:310] Step = 7600 ; steps/s = 1.64, tokens/s = 77760 (32935 source, 44825 target) ; Learning rate = 0.000672 ; Loss = 2.529846\n",
      "2024-12-12 12:04:40.491000: I runner.py:310] Step = 7700 ; steps/s = 1.63, tokens/s = 78415 (33200 source, 45215 target) ; Learning rate = 0.000681 ; Loss = 2.572552\n",
      "2024-12-12 12:05:41.777000: I runner.py:310] Step = 7800 ; steps/s = 1.63, tokens/s = 78471 (33232 source, 45239 target) ; Learning rate = 0.000690 ; Loss = 2.689449\n",
      "2024-12-12 12:06:42.649000: I runner.py:310] Step = 7900 ; steps/s = 1.64, tokens/s = 77669 (32891 source, 44778 target) ; Learning rate = 0.000698 ; Loss = 2.493471\n",
      "2024-12-12 12:07:43.949000: I runner.py:310] Step = 8000 ; steps/s = 1.63, tokens/s = 78415 (33205 source, 45210 target) ; Learning rate = 0.000707 ; Loss = 2.561019\n",
      "2024-12-12 12:08:45.127000: I runner.py:310] Step = 8100 ; steps/s = 1.63, tokens/s = 78611 (33287 source, 45324 target) ; Learning rate = 0.000716 ; Loss = 2.624460\n",
      "2024-12-12 12:09:46.384000: I runner.py:310] Step = 8200 ; steps/s = 1.63, tokens/s = 78490 (33231 source, 45259 target) ; Learning rate = 0.000725 ; Loss = 2.643871\n",
      "2024-12-12 12:10:47.224000: I runner.py:310] Step = 8300 ; steps/s = 1.64, tokens/s = 77706 (32897 source, 44809 target) ; Learning rate = 0.000734 ; Loss = 2.532830\n",
      "2024-12-12 12:11:48.544000: I runner.py:310] Step = 8400 ; steps/s = 1.63, tokens/s = 78412 (33210 source, 45202 target) ; Learning rate = 0.000743 ; Loss = 2.554542\n",
      "2024-12-12 12:12:49.811000: I runner.py:310] Step = 8500 ; steps/s = 1.63, tokens/s = 78496 (33239 source, 45257 target) ; Learning rate = 0.000751 ; Loss = 2.532977\n",
      "2024-12-12 12:13:50.713000: I runner.py:310] Step = 8600 ; steps/s = 1.64, tokens/s = 77642 (32888 source, 44754 target) ; Learning rate = 0.000760 ; Loss = 2.513538\n",
      "2024-12-12 12:14:51.980000: I runner.py:310] Step = 8700 ; steps/s = 1.63, tokens/s = 78478 (33227 source, 45251 target) ; Learning rate = 0.000769 ; Loss = 2.509880\n",
      "2024-12-12 12:15:53.238000: I runner.py:310] Step = 8800 ; steps/s = 1.63, tokens/s = 78471 (33224 source, 45247 target) ; Learning rate = 0.000778 ; Loss = 2.461524\n",
      "2024-12-12 12:16:54.493000: I runner.py:310] Step = 8900 ; steps/s = 1.63, tokens/s = 78497 (33233 source, 45264 target) ; Learning rate = 0.000787 ; Loss = 2.537716\n",
      "2024-12-12 12:17:55.408000: I runner.py:310] Step = 9000 ; steps/s = 1.64, tokens/s = 77604 (32859 source, 44745 target) ; Learning rate = 0.000796 ; Loss = 2.463596\n",
      "2024-12-12 12:18:56.629000: I runner.py:310] Step = 9100 ; steps/s = 1.63, tokens/s = 78557 (33280 source, 45277 target) ; Learning rate = 0.000804 ; Loss = 2.486076\n",
      "2024-12-12 12:19:57.943000: I runner.py:310] Step = 9200 ; steps/s = 1.63, tokens/s = 78423 (33206 source, 45217 target) ; Learning rate = 0.000813 ; Loss = 2.537859\n",
      "2024-12-12 12:20:59.240000: I runner.py:310] Step = 9300 ; steps/s = 1.63, tokens/s = 78447 (33218 source, 45229 target) ; Learning rate = 0.000822 ; Loss = 2.523563\n",
      "2024-12-12 12:22:00.154000: I runner.py:310] Step = 9400 ; steps/s = 1.64, tokens/s = 77617 (32866 source, 44751 target) ; Learning rate = 0.000831 ; Loss = 2.471812\n",
      "2024-12-12 12:23:01.428000: I runner.py:310] Step = 9500 ; steps/s = 1.63, tokens/s = 78468 (33223 source, 45245 target) ; Learning rate = 0.000840 ; Loss = 2.465606\n",
      "2024-12-12 12:24:02.759000: I runner.py:310] Step = 9600 ; steps/s = 1.63, tokens/s = 78383 (33189 source, 45194 target) ; Learning rate = 0.000849 ; Loss = 2.443382\n",
      "2024-12-12 12:25:03.581000: I runner.py:310] Step = 9700 ; steps/s = 1.64, tokens/s = 77767 (32943 source, 44824 target) ; Learning rate = 0.000857 ; Loss = 2.363719\n",
      "2024-12-12 12:26:04.831000: I runner.py:310] Step = 9800 ; steps/s = 1.63, tokens/s = 78508 (33246 source, 45262 target) ; Learning rate = 0.000866 ; Loss = 2.437071\n",
      "2024-12-12 12:27:06.117000: I runner.py:310] Step = 9900 ; steps/s = 1.63, tokens/s = 78478 (33247 source, 45231 target) ; Learning rate = 0.000875 ; Loss = 2.479912\n",
      "2024-12-12 12:28:07.400000: I runner.py:310] Step = 10000 ; steps/s = 1.63, tokens/s = 78428 (33191 source, 45237 target) ; Learning rate = 0.000884 ; Loss = 2.459489\n",
      "2024-12-12 12:28:09.182000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-10000\n",
      "2024-12-12 12:28:09.182000: I training.py:192] Running evaluation for step 10000\n",
      "2024-12-12 12:30:52.317000: I training.py:192] Evaluation result for step 10000: loss = 2.045046 ; perplexity = 7.729517\n",
      "2024-12-12 12:31:53.178000: I runner.py:310] Step = 10100 ; steps/s = 1.64, tokens/s = 77690 (32889 source, 44801 target) ; Learning rate = 0.000879 ; Loss = 2.455270\n",
      "2024-12-12 12:32:54.408000: I runner.py:310] Step = 10200 ; steps/s = 1.63, tokens/s = 78510 (33243 source, 45267 target) ; Learning rate = 0.000875 ; Loss = 2.353786\n",
      "2024-12-12 12:33:55.731000: I runner.py:310] Step = 10300 ; steps/s = 1.63, tokens/s = 78416 (33209 source, 45207 target) ; Learning rate = 0.000871 ; Loss = 2.415669\n",
      "2024-12-12 12:34:56.592000: I runner.py:310] Step = 10400 ; steps/s = 1.64, tokens/s = 77698 (32906 source, 44792 target) ; Learning rate = 0.000867 ; Loss = 2.354443\n",
      "2024-12-12 12:35:57.895000: I runner.py:310] Step = 10500 ; steps/s = 1.63, tokens/s = 78433 (33212 source, 45221 target) ; Learning rate = 0.000863 ; Loss = 2.329939\n",
      "2024-12-12 12:36:59.164000: I runner.py:310] Step = 10600 ; steps/s = 1.63, tokens/s = 78499 (33258 source, 45241 target) ; Learning rate = 0.000858 ; Loss = 2.334859\n",
      "2024-12-12 12:38:00.442000: I runner.py:310] Step = 10700 ; steps/s = 1.63, tokens/s = 78450 (33209 source, 45241 target) ; Learning rate = 0.000854 ; Loss = 2.398583\n",
      "2024-12-12 12:39:01.351000: I runner.py:310] Step = 10800 ; steps/s = 1.64, tokens/s = 77625 (32867 source, 44758 target) ; Learning rate = 0.000850 ; Loss = 2.268272\n",
      "2024-12-12 12:40:02.651000: I runner.py:310] Step = 10900 ; steps/s = 1.63, tokens/s = 78446 (33225 source, 45221 target) ; Learning rate = 0.000847 ; Loss = 2.351412\n",
      "2024-12-12 12:41:04.012000: I runner.py:310] Step = 11000 ; steps/s = 1.63, tokens/s = 78328 (33149 source, 45179 target) ; Learning rate = 0.000843 ; Loss = 2.431127\n",
      "2024-12-12 12:42:04.862000: I runner.py:310] Step = 11100 ; steps/s = 1.64, tokens/s = 77713 (32915 source, 44798 target) ; Learning rate = 0.000839 ; Loss = 2.270430\n",
      "2024-12-12 12:43:06.175000: I runner.py:310] Step = 11200 ; steps/s = 1.63, tokens/s = 78402 (33187 source, 45215 target) ; Learning rate = 0.000835 ; Loss = 2.339017\n",
      "2024-12-12 12:44:07.432000: I runner.py:310] Step = 11300 ; steps/s = 1.63, tokens/s = 78534 (33282 source, 45252 target) ; Learning rate = 0.000831 ; Loss = 2.289609\n",
      "2024-12-12 12:45:08.715000: I runner.py:310] Step = 11400 ; steps/s = 1.63, tokens/s = 78448 (33218 source, 45230 target) ; Learning rate = 0.000828 ; Loss = 2.335460\n",
      "2024-12-12 12:46:09.588000: I runner.py:310] Step = 11500 ; steps/s = 1.64, tokens/s = 77669 (32880 source, 44789 target) ; Learning rate = 0.000824 ; Loss = 2.288825\n",
      "2024-12-12 12:47:10.859000: I runner.py:310] Step = 11600 ; steps/s = 1.63, tokens/s = 78491 (33236 source, 45255 target) ; Learning rate = 0.000821 ; Loss = 2.289576\n",
      "2024-12-12 12:48:12.204000: I runner.py:310] Step = 11700 ; steps/s = 1.63, tokens/s = 78354 (33174 source, 45180 target) ; Learning rate = 0.000817 ; Loss = 2.307622\n",
      "2024-12-12 12:49:13.492000: I runner.py:310] Step = 11800 ; steps/s = 1.63, tokens/s = 78454 (33226 source, 45228 target) ; Learning rate = 0.000814 ; Loss = 2.324492\n",
      "2024-12-12 12:50:14.374000: I runner.py:310] Step = 11900 ; steps/s = 1.64, tokens/s = 77644 (32868 source, 44776 target) ; Learning rate = 0.000810 ; Loss = 2.257400\n",
      "2024-12-12 12:51:15.657000: I runner.py:310] Step = 12000 ; steps/s = 1.63, tokens/s = 78443 (33200 source, 45243 target) ; Learning rate = 0.000807 ; Loss = 2.249509\n",
      "2024-12-12 12:52:16.995000: I runner.py:310] Step = 12100 ; steps/s = 1.63, tokens/s = 78407 (33211 source, 45196 target) ; Learning rate = 0.000803 ; Loss = 2.260952\n",
      "2024-12-12 12:53:17.961000: I runner.py:310] Step = 12200 ; steps/s = 1.64, tokens/s = 77567 (32862 source, 44705 target) ; Learning rate = 0.000800 ; Loss = 2.241291\n",
      "2024-12-12 12:54:19.226000: I runner.py:310] Step = 12300 ; steps/s = 1.63, tokens/s = 78484 (33237 source, 45247 target) ; Learning rate = 0.000797 ; Loss = 2.212544\n",
      "2024-12-12 12:55:20.496000: I runner.py:310] Step = 12400 ; steps/s = 1.63, tokens/s = 78473 (33224 source, 45249 target) ; Learning rate = 0.000794 ; Loss = 2.210028\n",
      "2024-12-12 12:56:21.812000: I runner.py:310] Step = 12500 ; steps/s = 1.63, tokens/s = 78421 (33219 source, 45202 target) ; Learning rate = 0.000791 ; Loss = 2.238686\n",
      "2024-12-12 12:57:22.762000: I runner.py:310] Step = 12600 ; steps/s = 1.64, tokens/s = 77557 (32825 source, 44732 target) ; Learning rate = 0.000787 ; Loss = 2.107229\n",
      "2024-12-12 12:58:24.064000: I runner.py:310] Step = 12700 ; steps/s = 1.63, tokens/s = 78447 (33233 source, 45214 target) ; Learning rate = 0.000784 ; Loss = 2.228505\n",
      "2024-12-12 12:59:25.354000: I runner.py:310] Step = 12800 ; steps/s = 1.63, tokens/s = 78454 (33222 source, 45232 target) ; Learning rate = 0.000781 ; Loss = 2.243325\n",
      "2024-12-12 13:00:26.275000: I runner.py:310] Step = 12900 ; steps/s = 1.64, tokens/s = 77608 (32859 source, 44749 target) ; Learning rate = 0.000778 ; Loss = 2.146378\n",
      "2024-12-12 13:01:27.524000: I runner.py:310] Step = 13000 ; steps/s = 1.63, tokens/s = 78494 (33227 source, 45267 target) ; Learning rate = 0.000775 ; Loss = 2.164846\n",
      "2024-12-12 13:02:28.844000: I runner.py:310] Step = 13100 ; steps/s = 1.63, tokens/s = 78419 (33220 source, 45199 target) ; Learning rate = 0.000772 ; Loss = 2.224712\n",
      "2024-12-12 13:03:30.176000: I runner.py:310] Step = 13200 ; steps/s = 1.63, tokens/s = 78395 (33199 source, 45196 target) ; Learning rate = 0.000769 ; Loss = 2.282849\n",
      "2024-12-12 13:04:31.011000: I runner.py:310] Step = 13300 ; steps/s = 1.64, tokens/s = 77731 (32918 source, 44813 target) ; Learning rate = 0.000766 ; Loss = 2.126952\n",
      "2024-12-12 13:05:32.389000: I runner.py:310] Step = 13400 ; steps/s = 1.63, tokens/s = 78333 (33175 source, 45158 target) ; Learning rate = 0.000764 ; Loss = 2.175053\n",
      "2024-12-12 13:06:33.742000: I runner.py:310] Step = 13500 ; steps/s = 1.63, tokens/s = 78357 (33172 source, 45185 target) ; Learning rate = 0.000761 ; Loss = 2.179560\n",
      "2024-12-12 13:07:34.771000: I runner.py:310] Step = 13600 ; steps/s = 1.64, tokens/s = 77979 (33017 source, 44962 target) ; Learning rate = 0.000758 ; Loss = 2.279837\n",
      "2024-12-12 13:08:35.923000: I runner.py:310] Step = 13700 ; steps/s = 1.64, tokens/s = 78145 (33098 source, 45047 target) ; Learning rate = 0.000755 ; Loss = 2.179659\n",
      "2024-12-12 13:09:37.193000: I runner.py:310] Step = 13800 ; steps/s = 1.63, tokens/s = 78479 (33236 source, 45243 target) ; Learning rate = 0.000752 ; Loss = 2.154905\n",
      "2024-12-12 13:10:38.551000: I runner.py:310] Step = 13900 ; steps/s = 1.63, tokens/s = 78364 (33180 source, 45184 target) ; Learning rate = 0.000750 ; Loss = 2.199094\n",
      "2024-12-12 13:11:39.418000: I runner.py:310] Step = 14000 ; steps/s = 1.64, tokens/s = 77669 (32888 source, 44781 target) ; Learning rate = 0.000747 ; Loss = 2.160033\n",
      "2024-12-12 13:12:40.700000: I runner.py:310] Step = 14100 ; steps/s = 1.63, tokens/s = 78463 (33227 source, 45236 target) ; Learning rate = 0.000744 ; Loss = 2.113997\n",
      "2024-12-12 13:13:41.887000: I runner.py:310] Step = 14200 ; steps/s = 1.63, tokens/s = 78592 (33283 source, 45309 target) ; Learning rate = 0.000742 ; Loss = 2.141483\n",
      "2024-12-12 13:14:43.158000: I runner.py:310] Step = 14300 ; steps/s = 1.63, tokens/s = 78473 (33221 source, 45252 target) ; Learning rate = 0.000739 ; Loss = 2.163495\n",
      "2024-12-12 13:15:44.071000: I runner.py:310] Step = 14400 ; steps/s = 1.64, tokens/s = 77607 (32852 source, 44755 target) ; Learning rate = 0.000737 ; Loss = 2.176490\n",
      "2024-12-12 13:16:45.419000: I runner.py:310] Step = 14500 ; steps/s = 1.63, tokens/s = 78394 (33210 source, 45184 target) ; Learning rate = 0.000734 ; Loss = 2.089731\n",
      "2024-12-12 13:17:46.690000: I runner.py:310] Step = 14600 ; steps/s = 1.63, tokens/s = 78469 (33222 source, 45247 target) ; Learning rate = 0.000731 ; Loss = 2.130753\n",
      "2024-12-12 13:18:47.552000: I runner.py:310] Step = 14700 ; steps/s = 1.64, tokens/s = 77688 (32902 source, 44786 target) ; Learning rate = 0.000729 ; Loss = 2.053762\n",
      "2024-12-12 13:19:48.845000: I runner.py:310] Step = 14800 ; steps/s = 1.63, tokens/s = 78416 (33189 source, 45227 target) ; Learning rate = 0.000727 ; Loss = 2.094484\n",
      "2024-12-12 13:20:50.089000: I runner.py:310] Step = 14900 ; steps/s = 1.63, tokens/s = 78538 (33267 source, 45271 target) ; Learning rate = 0.000724 ; Loss = 2.138703\n",
      "2024-12-12 13:21:51.372000: I runner.py:310] Step = 15000 ; steps/s = 1.63, tokens/s = 78440 (33207 source, 45233 target) ; Learning rate = 0.000722 ; Loss = 2.156802\n",
      "2024-12-12 13:21:51.375000: I training.py:192] Running evaluation for step 15000\n",
      "2024-12-12 13:24:11.484000: I training.py:192] Evaluation result for step 15000: loss = 2.158311 ; perplexity = 8.656507\n",
      "2024-12-12 13:25:12.271000: I runner.py:310] Step = 15100 ; steps/s = 1.65, tokens/s = 77788 (32942 source, 44846 target) ; Learning rate = 0.000719 ; Loss = 2.037617\n",
      "2024-12-12 13:26:13.637000: I runner.py:310] Step = 15200 ; steps/s = 1.63, tokens/s = 78364 (33197 source, 45167 target) ; Learning rate = 0.000717 ; Loss = 2.135622\n",
      "2024-12-12 13:27:14.950000: I runner.py:310] Step = 15300 ; steps/s = 1.63, tokens/s = 78418 (33207 source, 45211 target) ; Learning rate = 0.000715 ; Loss = 2.116712\n",
      "2024-12-12 13:28:15.936000: I runner.py:310] Step = 15400 ; steps/s = 1.64, tokens/s = 77531 (32823 source, 44708 target) ; Learning rate = 0.000712 ; Loss = 2.060121\n",
      "2024-12-12 13:29:17.309000: I runner.py:310] Step = 15500 ; steps/s = 1.63, tokens/s = 78296 (33128 source, 45168 target) ; Learning rate = 0.000710 ; Loss = 2.101412\n",
      "2024-12-12 13:30:18.649000: I runner.py:310] Step = 15600 ; steps/s = 1.63, tokens/s = 78396 (33204 source, 45192 target) ; Learning rate = 0.000708 ; Loss = 2.086255\n",
      "2024-12-12 13:31:19.938000: I runner.py:310] Step = 15700 ; steps/s = 1.63, tokens/s = 78472 (33240 source, 45232 target) ; Learning rate = 0.000705 ; Loss = 2.089204\n",
      "2024-12-12 13:32:20.802000: I runner.py:310] Step = 15800 ; steps/s = 1.64, tokens/s = 77695 (32906 source, 44789 target) ; Learning rate = 0.000703 ; Loss = 2.014976\n",
      "2024-12-12 13:33:22.076000: I runner.py:310] Step = 15900 ; steps/s = 1.63, tokens/s = 78449 (33212 source, 45237 target) ; Learning rate = 0.000701 ; Loss = 2.057917\n",
      "2024-12-12 13:34:23.380000: I runner.py:310] Step = 16000 ; steps/s = 1.63, tokens/s = 78450 (33233 source, 45217 target) ; Learning rate = 0.000699 ; Loss = 2.111781\n",
      "2024-12-12 13:35:24.635000: I runner.py:310] Step = 16100 ; steps/s = 1.63, tokens/s = 78499 (33236 source, 45263 target) ; Learning rate = 0.000697 ; Loss = 2.129523\n",
      "2024-12-12 13:36:25.585000: I runner.py:310] Step = 16200 ; steps/s = 1.64, tokens/s = 77561 (32843 source, 44718 target) ; Learning rate = 0.000694 ; Loss = 2.017794\n",
      "2024-12-12 13:37:26.966000: I runner.py:310] Step = 16300 ; steps/s = 1.63, tokens/s = 78342 (33176 source, 45166 target) ; Learning rate = 0.000692 ; Loss = 2.072831\n",
      "2024-12-12 13:38:28.268000: I runner.py:310] Step = 16400 ; steps/s = 1.63, tokens/s = 78424 (33200 source, 45224 target) ; Learning rate = 0.000690 ; Loss = 2.078400\n",
      "2024-12-12 13:39:29.174000: I runner.py:310] Step = 16500 ; steps/s = 1.64, tokens/s = 77645 (32888 source, 44757 target) ; Learning rate = 0.000688 ; Loss = 2.036541\n",
      "2024-12-12 13:40:30.492000: I runner.py:310] Step = 16600 ; steps/s = 1.63, tokens/s = 78411 (33201 source, 45210 target) ; Learning rate = 0.000686 ; Loss = 2.030576\n",
      "2024-12-12 13:41:31.811000: I runner.py:310] Step = 16700 ; steps/s = 1.63, tokens/s = 78448 (33240 source, 45208 target) ; Learning rate = 0.000684 ; Loss = 2.051806\n",
      "2024-12-12 13:42:33.169000: I runner.py:310] Step = 16800 ; steps/s = 1.63, tokens/s = 78353 (33165 source, 45188 target) ; Learning rate = 0.000682 ; Loss = 2.040245\n",
      "2024-12-12 13:43:34.095000: I runner.py:310] Step = 16900 ; steps/s = 1.64, tokens/s = 77615 (32869 source, 44746 target) ; Learning rate = 0.000680 ; Loss = 1.976910\n",
      "2024-12-12 13:44:35.455000: I runner.py:310] Step = 17000 ; steps/s = 1.63, tokens/s = 78351 (33171 source, 45180 target) ; Learning rate = 0.000678 ; Loss = 2.034940\n",
      "2024-12-12 13:45:36.796000: I runner.py:310] Step = 17100 ; steps/s = 1.63, tokens/s = 78382 (33197 source, 45185 target) ; Learning rate = 0.000676 ; Loss = 2.065723\n",
      "2024-12-12 13:46:37.741000: I runner.py:310] Step = 17200 ; steps/s = 1.64, tokens/s = 77583 (32850 source, 44733 target) ; Learning rate = 0.000674 ; Loss = 2.003964\n",
      "2024-12-12 13:47:39.063000: I runner.py:310] Step = 17300 ; steps/s = 1.63, tokens/s = 78414 (33210 source, 45204 target) ; Learning rate = 0.000672 ; Loss = 1.989886\n",
      "2024-12-12 13:48:40.359000: I runner.py:310] Step = 17400 ; steps/s = 1.63, tokens/s = 78422 (33196 source, 45226 target) ; Learning rate = 0.000670 ; Loss = 2.036159\n",
      "2024-12-12 13:49:41.694000: I runner.py:310] Step = 17500 ; steps/s = 1.63, tokens/s = 78416 (33220 source, 45196 target) ; Learning rate = 0.000668 ; Loss = 2.066273\n",
      "2024-12-12 13:50:42.521000: I runner.py:310] Step = 17600 ; steps/s = 1.64, tokens/s = 77722 (32905 source, 44817 target) ; Learning rate = 0.000666 ; Loss = 2.031685\n",
      "2024-12-12 13:51:43.792000: I runner.py:310] Step = 17700 ; steps/s = 1.63, tokens/s = 78467 (33218 source, 45249 target) ; Learning rate = 0.000664 ; Loss = 2.010418\n",
      "2024-12-12 13:52:45.149000: I runner.py:310] Step = 17800 ; steps/s = 1.63, tokens/s = 78354 (33168 source, 45186 target) ; Learning rate = 0.000662 ; Loss = 2.019179\n",
      "2024-12-12 13:53:46.145000: I runner.py:310] Step = 17900 ; steps/s = 1.64, tokens/s = 77539 (32848 source, 44691 target) ; Learning rate = 0.000661 ; Loss = 1.996712\n",
      "2024-12-12 13:54:47.477000: I runner.py:310] Step = 18000 ; steps/s = 1.63, tokens/s = 78375 (33176 source, 45199 target) ; Learning rate = 0.000659 ; Loss = 2.006089\n",
      "2024-12-12 13:55:48.821000: I runner.py:310] Step = 18100 ; steps/s = 1.63, tokens/s = 78391 (33198 source, 45193 target) ; Learning rate = 0.000657 ; Loss = 2.002005\n",
      "2024-12-12 13:56:50.076000: I runner.py:310] Step = 18200 ; steps/s = 1.63, tokens/s = 78513 (33258 source, 45255 target) ; Learning rate = 0.000655 ; Loss = 1.988340\n",
      "2024-12-12 13:57:50.947000: I runner.py:310] Step = 18300 ; steps/s = 1.64, tokens/s = 77635 (32853 source, 44782 target) ; Learning rate = 0.000653 ; Loss = 1.937627\n",
      "2024-12-12 13:58:52.270000: I runner.py:310] Step = 18400 ; steps/s = 1.63, tokens/s = 78447 (33246 source, 45201 target) ; Learning rate = 0.000652 ; Loss = 1.990823\n",
      "2024-12-12 13:59:53.628000: I runner.py:310] Step = 18500 ; steps/s = 1.63, tokens/s = 78345 (33165 source, 45180 target) ; Learning rate = 0.000650 ; Loss = 2.054859\n",
      "2024-12-12 14:00:54.927000: I runner.py:310] Step = 18600 ; steps/s = 1.63, tokens/s = 78459 (33229 source, 45230 target) ; Learning rate = 0.000648 ; Loss = 2.045736\n",
      "2024-12-12 14:01:55.888000: I runner.py:310] Step = 18700 ; steps/s = 1.64, tokens/s = 77550 (32836 source, 44714 target) ; Learning rate = 0.000646 ; Loss = 1.953689\n",
      "2024-12-12 14:02:57.225000: I runner.py:310] Step = 18800 ; steps/s = 1.63, tokens/s = 78384 (33184 source, 45200 target) ; Learning rate = 0.000645 ; Loss = 1.998478\n",
      "2024-12-12 14:03:58.573000: I runner.py:310] Step = 18900 ; steps/s = 1.63, tokens/s = 78404 (33218 source, 45186 target) ; Learning rate = 0.000643 ; Loss = 2.000216\n",
      "2024-12-12 14:04:59.469000: I runner.py:310] Step = 19000 ; steps/s = 1.64, tokens/s = 77637 (32876 source, 44761 target) ; Learning rate = 0.000641 ; Loss = 1.960711\n",
      "2024-12-12 14:06:00.698000: I runner.py:310] Step = 19100 ; steps/s = 1.63, tokens/s = 78521 (33238 source, 45283 target) ; Learning rate = 0.000640 ; Loss = 1.984336\n",
      "2024-12-12 14:07:02.011000: I runner.py:310] Step = 19200 ; steps/s = 1.63, tokens/s = 78431 (33223 source, 45208 target) ; Learning rate = 0.000638 ; Loss = 1.994731\n",
      "2024-12-12 14:08:03.377000: I runner.py:310] Step = 19300 ; steps/s = 1.63, tokens/s = 78346 (33167 source, 45179 target) ; Learning rate = 0.000636 ; Loss = 2.016563\n",
      "2024-12-12 14:09:04.343000: I runner.py:310] Step = 19400 ; steps/s = 1.64, tokens/s = 77545 (32833 source, 44712 target) ; Learning rate = 0.000635 ; Loss = 1.930132\n",
      "2024-12-12 14:10:05.643000: I runner.py:310] Step = 19500 ; steps/s = 1.63, tokens/s = 78481 (33248 source, 45233 target) ; Learning rate = 0.000633 ; Loss = 1.974530\n",
      "2024-12-12 14:11:06.992000: I runner.py:310] Step = 19600 ; steps/s = 1.63, tokens/s = 78382 (33202 source, 45180 target) ; Learning rate = 0.000631 ; Loss = 2.017243\n",
      "2024-12-12 14:12:07.898000: I runner.py:310] Step = 19700 ; steps/s = 1.64, tokens/s = 77587 (32833 source, 44754 target) ; Learning rate = 0.000630 ; Loss = 1.927144\n",
      "2024-12-12 14:13:09.245000: I runner.py:310] Step = 19800 ; steps/s = 1.63, tokens/s = 78375 (33185 source, 45190 target) ; Learning rate = 0.000628 ; Loss = 1.933935\n",
      "2024-12-12 14:14:10.544000: I runner.py:310] Step = 19900 ; steps/s = 1.63, tokens/s = 78401 (33188 source, 45213 target) ; Learning rate = 0.000627 ; Loss = 1.986882\n",
      "2024-12-12 14:15:11.838000: I runner.py:310] Step = 20000 ; steps/s = 1.63, tokens/s = 78474 (33240 source, 45234 target) ; Learning rate = 0.000625 ; Loss = 1.981098\n",
      "2024-12-12 14:15:13.606000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-20000\n",
      "2024-12-12 14:15:13.606000: I training.py:192] Running evaluation for step 20000\n",
      "2024-12-12 14:17:32.213000: I training.py:192] Evaluation result for step 20000: loss = 2.275236 ; perplexity = 9.730214\n",
      "2024-12-12 14:18:32.952000: I runner.py:310] Step = 20100 ; steps/s = 1.65, tokens/s = 77907 (32989 source, 44918 target) ; Learning rate = 0.000623 ; Loss = 1.966437\n",
      "2024-12-12 14:19:34.237000: I runner.py:310] Step = 20200 ; steps/s = 1.63, tokens/s = 78459 (33220 source, 45239 target) ; Learning rate = 0.000622 ; Loss = 1.923155\n",
      "2024-12-12 14:20:35.524000: I runner.py:310] Step = 20300 ; steps/s = 1.63, tokens/s = 78471 (33226 source, 45245 target) ; Learning rate = 0.000620 ; Loss = 1.961653\n",
      "2024-12-12 14:21:36.713000: I runner.py:310] Step = 20400 ; steps/s = 1.63, tokens/s = 78041 (33057 source, 44984 target) ; Learning rate = 0.000619 ; Loss = 1.985352\n",
      "2024-12-12 14:22:37.794000: I runner.py:310] Step = 20500 ; steps/s = 1.64, tokens/s = 77985 (33047 source, 44938 target) ; Learning rate = 0.000617 ; Loss = 1.915123\n",
      "2024-12-12 14:23:39.099000: I runner.py:310] Step = 20600 ; steps/s = 1.63, tokens/s = 78422 (33200 source, 45222 target) ; Learning rate = 0.000616 ; Loss = 1.951207\n",
      "2024-12-12 14:24:40.477000: I runner.py:310] Step = 20700 ; steps/s = 1.63, tokens/s = 78316 (33146 source, 45170 target) ; Learning rate = 0.000614 ; Loss = 1.963101\n",
      "2024-12-12 14:25:41.487000: I runner.py:310] Step = 20800 ; steps/s = 1.64, tokens/s = 77476 (32808 source, 44668 target) ; Learning rate = 0.000613 ; Loss = 1.887957\n",
      "2024-12-12 14:26:42.813000: I runner.py:310] Step = 20900 ; steps/s = 1.63, tokens/s = 78415 (33204 source, 45211 target) ; Learning rate = 0.000611 ; Loss = 1.947592\n",
      "2024-12-12 14:27:44.147000: I runner.py:310] Step = 21000 ; steps/s = 1.63, tokens/s = 78394 (33191 source, 45203 target) ; Learning rate = 0.000610 ; Loss = 1.959606\n",
      "2024-12-12 14:28:45.558000: I runner.py:310] Step = 21100 ; steps/s = 1.63, tokens/s = 78299 (33165 source, 45134 target) ; Learning rate = 0.000608 ; Loss = 1.971477\n",
      "2024-12-12 14:29:46.470000: I runner.py:310] Step = 21200 ; steps/s = 1.64, tokens/s = 77590 (32849 source, 44741 target) ; Learning rate = 0.000607 ; Loss = 1.977437\n",
      "2024-12-12 14:30:47.749000: I runner.py:310] Step = 21300 ; steps/s = 1.63, tokens/s = 78471 (33225 source, 45246 target) ; Learning rate = 0.000606 ; Loss = 1.920902\n",
      "2024-12-12 14:31:49.061000: I runner.py:310] Step = 21400 ; steps/s = 1.63, tokens/s = 78430 (33214 source, 45216 target) ; Learning rate = 0.000604 ; Loss = 1.930384\n",
      "2024-12-12 14:32:50.008000: I runner.py:310] Step = 21500 ; steps/s = 1.64, tokens/s = 77582 (32853 source, 44729 target) ; Learning rate = 0.000603 ; Loss = 1.934276\n",
      "2024-12-12 14:33:51.357000: I runner.py:310] Step = 21600 ; steps/s = 1.63, tokens/s = 78387 (33199 source, 45188 target) ; Learning rate = 0.000601 ; Loss = 1.926845\n",
      "2024-12-12 14:34:52.679000: I runner.py:310] Step = 21700 ; steps/s = 1.63, tokens/s = 78398 (33194 source, 45204 target) ; Learning rate = 0.000600 ; Loss = 1.924707\n",
      "2024-12-12 14:35:53.981000: I runner.py:310] Step = 21800 ; steps/s = 1.63, tokens/s = 78442 (33216 source, 45226 target) ; Learning rate = 0.000599 ; Loss = 1.950134\n",
      "2024-12-12 14:36:54.893000: I runner.py:310] Step = 21900 ; steps/s = 1.64, tokens/s = 77576 (32827 source, 44749 target) ; Learning rate = 0.000597 ; Loss = 1.892507\n",
      "2024-12-12 14:37:56.236000: I runner.py:310] Step = 22000 ; steps/s = 1.63, tokens/s = 78391 (33200 source, 45191 target) ; Learning rate = 0.000596 ; Loss = 1.928466\n",
      "2024-12-12 14:38:57.538000: I runner.py:310] Step = 22100 ; steps/s = 1.63, tokens/s = 78448 (33222 source, 45226 target) ; Learning rate = 0.000595 ; Loss = 1.932605\n",
      "2024-12-12 14:39:58.524000: I runner.py:310] Step = 22200 ; steps/s = 1.64, tokens/s = 77542 (32849 source, 44693 target) ; Learning rate = 0.000593 ; Loss = 1.892307\n",
      "2024-12-12 14:40:59.909000: I runner.py:310] Step = 22300 ; steps/s = 1.63, tokens/s = 78324 (33162 source, 45162 target) ; Learning rate = 0.000592 ; Loss = 1.899989\n",
      "2024-12-12 14:42:01.214000: I runner.py:310] Step = 22400 ; steps/s = 1.63, tokens/s = 78414 (33203 source, 45211 target) ; Learning rate = 0.000591 ; Loss = 1.952038\n",
      "2024-12-12 14:43:02.592000: I runner.py:310] Step = 22500 ; steps/s = 1.63, tokens/s = 78345 (33171 source, 45174 target) ; Learning rate = 0.000589 ; Loss = 1.936431\n",
      "2024-12-12 14:44:03.555000: I runner.py:310] Step = 22600 ; steps/s = 1.64, tokens/s = 77588 (32874 source, 44714 target) ; Learning rate = 0.000588 ; Loss = 1.877751\n",
      "2024-12-12 14:45:04.833000: I runner.py:310] Step = 22700 ; steps/s = 1.63, tokens/s = 78446 (33198 source, 45248 target) ; Learning rate = 0.000587 ; Loss = 1.907805\n",
      "2024-12-12 14:46:06.215000: I runner.py:310] Step = 22800 ; steps/s = 1.63, tokens/s = 78311 (33149 source, 45162 target) ; Learning rate = 0.000585 ; Loss = 1.937136\n",
      "2024-12-12 14:47:07.599000: I runner.py:310] Step = 22900 ; steps/s = 1.63, tokens/s = 78365 (33208 source, 45157 target) ; Learning rate = 0.000584 ; Loss = 1.947846\n",
      "2024-12-12 14:48:08.574000: I runner.py:310] Step = 23000 ; steps/s = 1.64, tokens/s = 77532 (32822 source, 44710 target) ; Learning rate = 0.000583 ; Loss = 1.874920\n",
      "2024-12-12 14:49:09.995000: I runner.py:310] Step = 23100 ; steps/s = 1.63, tokens/s = 78282 (33155 source, 45127 target) ; Learning rate = 0.000582 ; Loss = 1.907817\n",
      "2024-12-12 14:50:11.702000: I runner.py:310] Step = 23200 ; steps/s = 1.62, tokens/s = 77889 (32964 source, 44925 target) ; Learning rate = 0.000580 ; Loss = 1.939495\n",
      "2024-12-12 14:51:12.608000: I runner.py:310] Step = 23300 ; steps/s = 1.64, tokens/s = 77657 (32897 source, 44760 target) ; Learning rate = 0.000579 ; Loss = 1.907804\n",
      "2024-12-12 14:52:13.987000: I runner.py:310] Step = 23400 ; steps/s = 1.63, tokens/s = 78318 (33153 source, 45165 target) ; Learning rate = 0.000578 ; Loss = 1.895560\n",
      "2024-12-12 14:53:15.242000: I runner.py:310] Step = 23500 ; steps/s = 1.63, tokens/s = 78526 (33267 source, 45259 target) ; Learning rate = 0.000577 ; Loss = 1.904119\n",
      "2024-12-12 14:54:16.587000: I runner.py:310] Step = 23600 ; steps/s = 1.63, tokens/s = 78371 (33188 source, 45183 target) ; Learning rate = 0.000575 ; Loss = 1.915480\n",
      "2024-12-12 14:55:17.594000: I runner.py:310] Step = 23700 ; steps/s = 1.64, tokens/s = 77500 (32809 source, 44691 target) ; Learning rate = 0.000574 ; Loss = 1.850041\n",
      "2024-12-12 14:56:18.895000: I runner.py:310] Step = 23800 ; steps/s = 1.63, tokens/s = 78476 (33253 source, 45223 target) ; Learning rate = 0.000573 ; Loss = 1.914624\n",
      "2024-12-12 14:57:20.251000: I runner.py:310] Step = 23900 ; steps/s = 1.63, tokens/s = 78372 (33195 source, 45177 target) ; Learning rate = 0.000572 ; Loss = 1.910301\n",
      "2024-12-12 14:58:21.127000: I runner.py:310] Step = 24000 ; steps/s = 1.64, tokens/s = 77627 (32846 source, 44781 target) ; Learning rate = 0.000571 ; Loss = 1.878438\n",
      "2024-12-12 14:59:22.491000: I runner.py:310] Step = 24100 ; steps/s = 1.63, tokens/s = 78354 (33182 source, 45172 target) ; Learning rate = 0.000569 ; Loss = 1.880051\n",
      "2024-12-12 15:00:23.858000: I runner.py:310] Step = 24200 ; steps/s = 1.63, tokens/s = 78360 (33190 source, 45170 target) ; Learning rate = 0.000568 ; Loss = 1.906440\n",
      "2024-12-12 15:01:25.166000: I runner.py:310] Step = 24300 ; steps/s = 1.63, tokens/s = 78435 (33216 source, 45219 target) ; Learning rate = 0.000567 ; Loss = 1.914586\n",
      "2024-12-12 15:02:26.119000: I runner.py:310] Step = 24400 ; steps/s = 1.64, tokens/s = 77564 (32844 source, 44720 target) ; Learning rate = 0.000566 ; Loss = 1.846829\n",
      "2024-12-12 15:03:27.525000: I runner.py:310] Step = 24500 ; steps/s = 1.63, tokens/s = 78303 (33166 source, 45137 target) ; Learning rate = 0.000565 ; Loss = 1.895475\n",
      "2024-12-12 15:04:28.825000: I runner.py:310] Step = 24600 ; steps/s = 1.63, tokens/s = 78425 (33197 source, 45228 target) ; Learning rate = 0.000564 ; Loss = 1.916416\n",
      "2024-12-12 15:05:29.700000: I runner.py:310] Step = 24700 ; steps/s = 1.64, tokens/s = 77661 (32878 source, 44783 target) ; Learning rate = 0.000562 ; Loss = 1.879749\n",
      "2024-12-12 15:06:31.072000: I runner.py:310] Step = 24800 ; steps/s = 1.63, tokens/s = 78322 (33158 source, 45164 target) ; Learning rate = 0.000561 ; Loss = 1.871763\n",
      "2024-12-12 15:07:32.460000: I runner.py:310] Step = 24900 ; steps/s = 1.63, tokens/s = 78343 (33179 source, 45164 target) ; Learning rate = 0.000560 ; Loss = 1.882533\n",
      "2024-12-12 15:08:33.744000: I runner.py:310] Step = 25000 ; steps/s = 1.63, tokens/s = 78448 (33215 source, 45233 target) ; Learning rate = 0.000559 ; Loss = 1.887489\n",
      "2024-12-12 15:08:33.745000: I training.py:192] Running evaluation for step 25000\n",
      "2024-12-12 15:10:49.296000: I training.py:192] Evaluation result for step 25000: loss = 2.371342 ; perplexity = 10.711758\n",
      "2024-12-12 15:11:50.048000: I runner.py:310] Step = 25100 ; steps/s = 1.65, tokens/s = 77865 (32986 source, 44879 target) ; Learning rate = 0.000558 ; Loss = 1.834565\n",
      "2024-12-12 15:12:51.503000: I runner.py:310] Step = 25200 ; steps/s = 1.63, tokens/s = 78240 (33140 source, 45100 target) ; Learning rate = 0.000557 ; Loss = 1.880126\n",
      "2024-12-12 15:13:52.914000: I runner.py:310] Step = 25300 ; steps/s = 1.63, tokens/s = 78305 (33162 source, 45143 target) ; Learning rate = 0.000556 ; Loss = 1.898337\n",
      "2024-12-12 15:14:54.266000: I runner.py:310] Step = 25400 ; steps/s = 1.63, tokens/s = 78360 (33163 source, 45197 target) ; Learning rate = 0.000555 ; Loss = 1.896489\n",
      "2024-12-12 15:15:55.185000: I runner.py:310] Step = 25500 ; steps/s = 1.64, tokens/s = 77595 (32843 source, 44752 target) ; Learning rate = 0.000553 ; Loss = 1.902909\n",
      "2024-12-12 15:16:56.517000: I runner.py:310] Step = 25600 ; steps/s = 1.63, tokens/s = 78432 (33231 source, 45201 target) ; Learning rate = 0.000552 ; Loss = 1.866073\n",
      "2024-12-12 15:17:57.853000: I runner.py:310] Step = 25700 ; steps/s = 1.63, tokens/s = 78418 (33226 source, 45192 target) ; Learning rate = 0.000551 ; Loss = 1.872345\n",
      "2024-12-12 15:18:58.844000: I runner.py:310] Step = 25800 ; steps/s = 1.64, tokens/s = 77476 (32783 source, 44693 target) ; Learning rate = 0.000550 ; Loss = 1.859402\n",
      "2024-12-12 15:20:00.161000: I runner.py:310] Step = 25900 ; steps/s = 1.63, tokens/s = 78411 (33192 source, 45219 target) ; Learning rate = 0.000549 ; Loss = 1.855716\n",
      "2024-12-12 15:21:01.467000: I runner.py:310] Step = 26000 ; steps/s = 1.63, tokens/s = 78435 (33215 source, 45220 target) ; Learning rate = 0.000548 ; Loss = 1.857774\n",
      "2024-12-12 15:22:02.794000: I runner.py:310] Step = 26100 ; steps/s = 1.63, tokens/s = 78410 (33214 source, 45196 target) ; Learning rate = 0.000547 ; Loss = 1.874048\n",
      "2024-12-12 15:23:03.687000: I runner.py:310] Step = 26200 ; steps/s = 1.64, tokens/s = 77638 (32871 source, 44767 target) ; Learning rate = 0.000546 ; Loss = 1.889875\n",
      "2024-12-12 15:24:05.052000: I runner.py:310] Step = 26300 ; steps/s = 1.63, tokens/s = 78359 (33184 source, 45175 target) ; Learning rate = 0.000545 ; Loss = 1.860179\n",
      "2024-12-12 15:25:06.423000: I runner.py:310] Step = 26400 ; steps/s = 1.63, tokens/s = 78368 (33197 source, 45171 target) ; Learning rate = 0.000544 ; Loss = 1.851270\n",
      "2024-12-12 15:26:07.410000: I runner.py:310] Step = 26500 ; steps/s = 1.64, tokens/s = 77513 (32816 source, 44697 target) ; Learning rate = 0.000543 ; Loss = 1.863814\n",
      "2024-12-12 15:27:08.749000: I runner.py:310] Step = 26600 ; steps/s = 1.63, tokens/s = 78376 (33188 source, 45188 target) ; Learning rate = 0.000542 ; Loss = 1.861314\n",
      "2024-12-12 15:28:10.113000: I runner.py:310] Step = 26700 ; steps/s = 1.63, tokens/s = 78337 (33156 source, 45181 target) ; Learning rate = 0.000541 ; Loss = 1.858029\n",
      "2024-12-12 15:29:11.448000: I runner.py:310] Step = 26800 ; steps/s = 1.63, tokens/s = 78399 (33199 source, 45200 target) ; Learning rate = 0.000540 ; Loss = 1.871411\n",
      "2024-12-12 15:30:12.412000: I runner.py:310] Step = 26900 ; steps/s = 1.64, tokens/s = 77570 (32855 source, 44715 target) ; Learning rate = 0.000539 ; Loss = 1.867640\n",
      "2024-12-12 15:31:13.800000: I runner.py:310] Step = 27000 ; steps/s = 1.63, tokens/s = 78352 (33192 source, 45160 target) ; Learning rate = 0.000538 ; Loss = 1.852504\n",
      "2024-12-12 15:32:15.121000: I runner.py:310] Step = 27100 ; steps/s = 1.63, tokens/s = 78417 (33206 source, 45211 target) ; Learning rate = 0.000537 ; Loss = 1.843024\n",
      "2024-12-12 15:33:16.345000: I runner.py:310] Step = 27200 ; steps/s = 1.63, tokens/s = 78222 (33119 source, 45103 target) ; Learning rate = 0.000536 ; Loss = 1.852146\n",
      "2024-12-12 15:34:17.405000: I runner.py:310] Step = 27300 ; steps/s = 1.64, tokens/s = 77714 (32897 source, 44817 target) ; Learning rate = 0.000535 ; Loss = 1.821490\n",
      "2024-12-12 15:35:18.785000: I runner.py:310] Step = 27400 ; steps/s = 1.63, tokens/s = 78341 (33184 source, 45157 target) ; Learning rate = 0.000534 ; Loss = 1.858713\n",
      "2024-12-12 15:36:20.089000: I runner.py:310] Step = 27500 ; steps/s = 1.63, tokens/s = 78426 (33202 source, 45224 target) ; Learning rate = 0.000533 ; Loss = 1.880268\n",
      "2024-12-12 15:37:21.025000: I runner.py:310] Step = 27600 ; steps/s = 1.64, tokens/s = 77615 (32884 source, 44731 target) ; Learning rate = 0.000532 ; Loss = 1.802904\n",
      "2024-12-12 15:38:22.307000: I runner.py:310] Step = 27700 ; steps/s = 1.63, tokens/s = 78449 (33219 source, 45230 target) ; Learning rate = 0.000531 ; Loss = 1.854707\n",
      "2024-12-12 15:39:23.646000: I runner.py:310] Step = 27800 ; steps/s = 1.63, tokens/s = 78410 (33211 source, 45199 target) ; Learning rate = 0.000530 ; Loss = 1.850730\n",
      "2024-12-12 15:40:24.913000: I runner.py:310] Step = 27900 ; steps/s = 1.63, tokens/s = 78446 (33197 source, 45249 target) ; Learning rate = 0.000529 ; Loss = 1.876589\n",
      "2024-12-12 15:41:25.931000: I runner.py:310] Step = 28000 ; steps/s = 1.64, tokens/s = 77482 (32810 source, 44672 target) ; Learning rate = 0.000528 ; Loss = 1.813217\n",
      "2024-12-12 15:42:27.190000: I runner.py:310] Step = 28100 ; steps/s = 1.63, tokens/s = 78478 (33224 source, 45254 target) ; Learning rate = 0.000527 ; Loss = 1.834022\n",
      "2024-12-12 15:43:28.510000: I runner.py:310] Step = 28200 ; steps/s = 1.63, tokens/s = 78418 (33208 source, 45210 target) ; Learning rate = 0.000526 ; Loss = 1.874251\n",
      "2024-12-12 15:44:29.418000: I runner.py:310] Step = 28300 ; steps/s = 1.64, tokens/s = 77660 (32898 source, 44762 target) ; Learning rate = 0.000525 ; Loss = 1.817993\n",
      "2024-12-12 15:45:30.735000: I runner.py:310] Step = 28400 ; steps/s = 1.63, tokens/s = 78425 (33215 source, 45210 target) ; Learning rate = 0.000524 ; Loss = 1.832326\n",
      "2024-12-12 15:46:32.026000: I runner.py:310] Step = 28500 ; steps/s = 1.63, tokens/s = 78453 (33219 source, 45234 target) ; Learning rate = 0.000524 ; Loss = 1.859087\n",
      "2024-12-12 15:47:33.415000: I runner.py:310] Step = 28600 ; steps/s = 1.63, tokens/s = 78302 (33154 source, 45148 target) ; Learning rate = 0.000523 ; Loss = 1.859594\n",
      "2024-12-12 15:48:34.266000: I runner.py:310] Step = 28700 ; steps/s = 1.64, tokens/s = 77679 (32878 source, 44801 target) ; Learning rate = 0.000522 ; Loss = 1.802155\n",
      "2024-12-12 15:49:35.640000: I runner.py:310] Step = 28800 ; steps/s = 1.63, tokens/s = 78349 (33176 source, 45173 target) ; Learning rate = 0.000521 ; Loss = 1.840138\n",
      "2024-12-12 15:50:36.968000: I runner.py:310] Step = 28900 ; steps/s = 1.63, tokens/s = 78404 (33201 source, 45203 target) ; Learning rate = 0.000520 ; Loss = 1.868426\n",
      "2024-12-12 15:51:37.907000: I runner.py:310] Step = 29000 ; steps/s = 1.64, tokens/s = 77608 (32878 source, 44730 target) ; Learning rate = 0.000519 ; Loss = 1.825720\n",
      "2024-12-12 15:52:39.238000: I runner.py:310] Step = 29100 ; steps/s = 1.63, tokens/s = 78393 (33196 source, 45197 target) ; Learning rate = 0.000518 ; Loss = 1.834423\n",
      "2024-12-12 15:53:40.612000: I runner.py:310] Step = 29200 ; steps/s = 1.63, tokens/s = 78333 (33159 source, 45174 target) ; Learning rate = 0.000517 ; Loss = 1.836000\n",
      "2024-12-12 15:54:41.999000: I runner.py:310] Step = 29300 ; steps/s = 1.63, tokens/s = 78341 (33180 source, 45161 target) ; Learning rate = 0.000516 ; Loss = 1.841109\n",
      "2024-12-12 15:55:42.970000: I runner.py:310] Step = 29400 ; steps/s = 1.64, tokens/s = 77576 (32873 source, 44703 target) ; Learning rate = 0.000515 ; Loss = 1.806866\n",
      "2024-12-12 15:56:44.295000: I runner.py:310] Step = 29500 ; steps/s = 1.63, tokens/s = 78414 (33210 source, 45204 target) ; Learning rate = 0.000515 ; Loss = 1.836970\n",
      "2024-12-12 15:57:45.613000: I runner.py:310] Step = 29600 ; steps/s = 1.63, tokens/s = 78438 (33218 source, 45220 target) ; Learning rate = 0.000514 ; Loss = 1.836735\n",
      "2024-12-12 15:58:46.926000: I runner.py:310] Step = 29700 ; steps/s = 1.63, tokens/s = 78365 (33159 source, 45206 target) ; Learning rate = 0.000513 ; Loss = 1.865072\n",
      "2024-12-12 15:59:47.871000: I runner.py:310] Step = 29800 ; steps/s = 1.64, tokens/s = 77559 (32829 source, 44730 target) ; Learning rate = 0.000512 ; Loss = 1.838786\n",
      "2024-12-12 16:00:49.174000: I runner.py:310] Step = 29900 ; steps/s = 1.63, tokens/s = 78439 (33221 source, 45218 target) ; Learning rate = 0.000511 ; Loss = 1.829239\n",
      "2024-12-12 16:01:50.477000: I runner.py:310] Step = 30000 ; steps/s = 1.63, tokens/s = 78430 (33215 source, 45215 target) ; Learning rate = 0.000510 ; Loss = 1.821798\n",
      "2024-12-12 16:01:52.302000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-30000\n",
      "2024-12-12 16:01:52.302000: I training.py:192] Running evaluation for step 30000\n",
      "2024-12-12 16:04:05.428000: I training.py:192] Evaluation result for step 30000: loss = 2.435996 ; perplexity = 11.427190\n",
      "2024-12-12 16:05:06.227000: I runner.py:310] Step = 30100 ; steps/s = 1.65, tokens/s = 77793 (32946 source, 44847 target) ; Learning rate = 0.000509 ; Loss = 1.817951\n",
      "2024-12-12 16:06:07.453000: I runner.py:310] Step = 30200 ; steps/s = 1.63, tokens/s = 78535 (33258 source, 45277 target) ; Learning rate = 0.000509 ; Loss = 1.813697\n",
      "2024-12-12 16:07:08.759000: I runner.py:310] Step = 30300 ; steps/s = 1.63, tokens/s = 78409 (33187 source, 45222 target) ; Learning rate = 0.000508 ; Loss = 1.812225\n",
      "2024-12-12 16:08:10.096000: I runner.py:310] Step = 30400 ; steps/s = 1.63, tokens/s = 78410 (33214 source, 45196 target) ; Learning rate = 0.000507 ; Loss = 1.831048\n",
      "2024-12-12 16:09:10.992000: I runner.py:310] Step = 30500 ; steps/s = 1.64, tokens/s = 77651 (32891 source, 44760 target) ; Learning rate = 0.000506 ; Loss = 1.785991\n",
      "2024-12-12 16:10:12.350000: I runner.py:310] Step = 30600 ; steps/s = 1.63, tokens/s = 78379 (33197 source, 45182 target) ; Learning rate = 0.000505 ; Loss = 1.820883\n",
      "2024-12-12 16:11:13.712000: I runner.py:310] Step = 30700 ; steps/s = 1.63, tokens/s = 78376 (33197 source, 45179 target) ; Learning rate = 0.000504 ; Loss = 1.832279\n",
      "2024-12-12 16:12:14.729000: I runner.py:310] Step = 30800 ; steps/s = 1.64, tokens/s = 77450 (32767 source, 44683 target) ; Learning rate = 0.000504 ; Loss = 1.800561\n",
      "2024-12-12 16:13:16.035000: I runner.py:310] Step = 30900 ; steps/s = 1.63, tokens/s = 78453 (33241 source, 45212 target) ; Learning rate = 0.000503 ; Loss = 1.803581\n",
      "2024-12-12 16:14:17.370000: I runner.py:310] Step = 31000 ; steps/s = 1.63, tokens/s = 78364 (33171 source, 45193 target) ; Learning rate = 0.000502 ; Loss = 1.832341\n",
      "2024-12-12 16:15:18.673000: I runner.py:310] Step = 31100 ; steps/s = 1.63, tokens/s = 78437 (33209 source, 45228 target) ; Learning rate = 0.000501 ; Loss = 1.827300\n",
      "2024-12-12 16:16:19.618000: I runner.py:310] Step = 31200 ; steps/s = 1.64, tokens/s = 77584 (32850 source, 44734 target) ; Learning rate = 0.000500 ; Loss = 1.787648\n",
      "2024-12-12 16:17:20.994000: I runner.py:310] Step = 31300 ; steps/s = 1.63, tokens/s = 78386 (33227 source, 45159 target) ; Learning rate = 0.000500 ; Loss = 1.819982\n",
      "2024-12-12 16:18:22.322000: I runner.py:310] Step = 31400 ; steps/s = 1.63, tokens/s = 78377 (33169 source, 45208 target) ; Learning rate = 0.000499 ; Loss = 1.828879\n",
      "2024-12-12 16:19:23.214000: I runner.py:310] Step = 31500 ; steps/s = 1.64, tokens/s = 77625 (32859 source, 44766 target) ; Learning rate = 0.000498 ; Loss = 1.798802\n",
      "2024-12-12 16:20:24.555000: I runner.py:310] Step = 31600 ; steps/s = 1.63, tokens/s = 78370 (33178 source, 45192 target) ; Learning rate = 0.000497 ; Loss = 1.808506\n",
      "2024-12-12 16:21:25.877000: I runner.py:310] Step = 31700 ; steps/s = 1.63, tokens/s = 78410 (33199 source, 45211 target) ; Learning rate = 0.000496 ; Loss = 1.805284\n",
      "2024-12-12 16:22:27.134000: I runner.py:310] Step = 31800 ; steps/s = 1.63, tokens/s = 78490 (33238 source, 45252 target) ; Learning rate = 0.000496 ; Loss = 1.821635\n",
      "2024-12-12 16:23:28.036000: I runner.py:310] Step = 31900 ; steps/s = 1.64, tokens/s = 77649 (32887 source, 44762 target) ; Learning rate = 0.000495 ; Loss = 1.820691\n",
      "2024-12-12 16:24:30.054000: I runner.py:310] Step = 32000 ; steps/s = 1.61, tokens/s = 77520 (32825 source, 44695 target) ; Learning rate = 0.000494 ; Loss = 1.805017\n",
      "2024-12-12 16:25:31.440000: I runner.py:310] Step = 32100 ; steps/s = 1.63, tokens/s = 78324 (33163 source, 45161 target) ; Learning rate = 0.000493 ; Loss = 1.794744\n",
      "2024-12-12 16:26:32.739000: I runner.py:310] Step = 32200 ; steps/s = 1.63, tokens/s = 78454 (33227 source, 45227 target) ; Learning rate = 0.000493 ; Loss = 1.810478\n",
      "2024-12-12 16:27:33.727000: I runner.py:310] Step = 32300 ; steps/s = 1.64, tokens/s = 77523 (32827 source, 44696 target) ; Learning rate = 0.000492 ; Loss = 1.842914\n",
      "2024-12-12 16:28:35.059000: I runner.py:310] Step = 32400 ; steps/s = 1.63, tokens/s = 78407 (33207 source, 45200 target) ; Learning rate = 0.000491 ; Loss = 1.793253\n",
      "2024-12-12 16:29:36.402000: I runner.py:310] Step = 32500 ; steps/s = 1.63, tokens/s = 78392 (33205 source, 45187 target) ; Learning rate = 0.000490 ; Loss = 1.798299\n",
      "2024-12-12 16:30:37.277000: I runner.py:310] Step = 32600 ; steps/s = 1.64, tokens/s = 77645 (32864 source, 44781 target) ; Learning rate = 0.000490 ; Loss = 1.784727\n",
      "2024-12-12 16:31:38.591000: I runner.py:310] Step = 32700 ; steps/s = 1.63, tokens/s = 78432 (33223 source, 45209 target) ; Learning rate = 0.000489 ; Loss = 1.785087\n",
      "2024-12-12 16:32:39.928000: I runner.py:310] Step = 32800 ; steps/s = 1.63, tokens/s = 78394 (33200 source, 45194 target) ; Learning rate = 0.000488 ; Loss = 1.825269\n",
      "2024-12-12 16:33:41.340000: I runner.py:310] Step = 32900 ; steps/s = 1.63, tokens/s = 78293 (33150 source, 45143 target) ; Learning rate = 0.000487 ; Loss = 1.826487\n",
      "2024-12-12 16:34:42.342000: I runner.py:310] Step = 33000 ; steps/s = 1.64, tokens/s = 77479 (32791 source, 44688 target) ; Learning rate = 0.000487 ; Loss = 1.802499\n",
      "2024-12-12 16:35:43.654000: I runner.py:310] Step = 33100 ; steps/s = 1.63, tokens/s = 78427 (33208 source, 45219 target) ; Learning rate = 0.000486 ; Loss = 1.784860\n",
      "2024-12-12 16:36:44.912000: I runner.py:310] Step = 33200 ; steps/s = 1.63, tokens/s = 78499 (33251 source, 45248 target) ; Learning rate = 0.000485 ; Loss = 1.794684\n",
      "2024-12-12 16:37:45.875000: I runner.py:310] Step = 33300 ; steps/s = 1.64, tokens/s = 77584 (32863 source, 44721 target) ; Learning rate = 0.000484 ; Loss = 1.780418\n",
      "2024-12-12 16:38:47.158000: I runner.py:310] Step = 33400 ; steps/s = 1.63, tokens/s = 78434 (33192 source, 45242 target) ; Learning rate = 0.000484 ; Loss = 1.799018\n",
      "2024-12-12 16:39:48.433000: I runner.py:310] Step = 33500 ; steps/s = 1.63, tokens/s = 78479 (33237 source, 45242 target) ; Learning rate = 0.000483 ; Loss = 1.805357\n",
      "2024-12-12 16:40:49.795000: I runner.py:310] Step = 33600 ; steps/s = 1.63, tokens/s = 78375 (33197 source, 45178 target) ; Learning rate = 0.000482 ; Loss = 1.822166\n",
      "2024-12-12 16:41:50.720000: I runner.py:310] Step = 33700 ; steps/s = 1.64, tokens/s = 77602 (32859 source, 44743 target) ; Learning rate = 0.000481 ; Loss = 1.755295\n",
      "2024-12-12 16:42:52.095000: I runner.py:310] Step = 33800 ; steps/s = 1.63, tokens/s = 78332 (33166 source, 45166 target) ; Learning rate = 0.000481 ; Loss = 1.797155\n",
      "2024-12-12 16:43:53.401000: I runner.py:310] Step = 33900 ; steps/s = 1.63, tokens/s = 78432 (33209 source, 45223 target) ; Learning rate = 0.000480 ; Loss = 1.805037\n",
      "2024-12-12 16:44:54.738000: I runner.py:310] Step = 34000 ; steps/s = 1.63, tokens/s = 78385 (33199 source, 45186 target) ; Learning rate = 0.000479 ; Loss = 1.815662\n",
      "2024-12-12 16:45:55.635000: I runner.py:310] Step = 34100 ; steps/s = 1.64, tokens/s = 77639 (32873 source, 44766 target) ; Learning rate = 0.000479 ; Loss = 1.794242\n",
      "2024-12-12 16:46:56.998000: I runner.py:310] Step = 34200 ; steps/s = 1.63, tokens/s = 78395 (33218 source, 45177 target) ; Learning rate = 0.000478 ; Loss = 1.784115\n",
      "2024-12-12 16:47:58.336000: I runner.py:310] Step = 34300 ; steps/s = 1.63, tokens/s = 78360 (33161 source, 45199 target) ; Learning rate = 0.000477 ; Loss = 1.794399\n",
      "2024-12-12 16:48:59.310000: I runner.py:310] Step = 34400 ; steps/s = 1.64, tokens/s = 77566 (32864 source, 44702 target) ; Learning rate = 0.000477 ; Loss = 1.770379\n",
      "2024-12-12 16:50:00.646000: I runner.py:310] Step = 34500 ; steps/s = 1.63, tokens/s = 78380 (33178 source, 45202 target) ; Learning rate = 0.000476 ; Loss = 1.800935\n",
      "2024-12-12 16:51:01.950000: I runner.py:310] Step = 34600 ; steps/s = 1.63, tokens/s = 78429 (33210 source, 45219 target) ; Learning rate = 0.000475 ; Loss = 1.799514\n",
      "2024-12-12 16:52:03.341000: I runner.py:310] Step = 34700 ; steps/s = 1.63, tokens/s = 78304 (33146 source, 45158 target) ; Learning rate = 0.000474 ; Loss = 1.814189\n",
      "2024-12-12 16:53:04.269000: I runner.py:310] Step = 34800 ; steps/s = 1.64, tokens/s = 77597 (32867 source, 44730 target) ; Learning rate = 0.000474 ; Loss = 1.760340\n",
      "2024-12-12 16:54:05.670000: I runner.py:310] Step = 34900 ; steps/s = 1.63, tokens/s = 78309 (33164 source, 45145 target) ; Learning rate = 0.000473 ; Loss = 1.795288\n",
      "2024-12-12 16:55:06.985000: I runner.py:310] Step = 35000 ; steps/s = 1.63, tokens/s = 78414 (33198 source, 45216 target) ; Learning rate = 0.000472 ; Loss = 1.794896\n",
      "2024-12-12 16:55:06.986000: I training.py:192] Running evaluation for step 35000\n",
      "2024-12-12 16:57:19.664000: I training.py:192] Evaluation result for step 35000: loss = 2.490623 ; perplexity = 12.068788\n",
      "2024-12-12 16:58:20.417000: I runner.py:310] Step = 35100 ; steps/s = 1.65, tokens/s = 77852 (32976 source, 44876 target) ; Learning rate = 0.000472 ; Loss = 1.763295\n",
      "2024-12-12 16:59:21.750000: I runner.py:310] Step = 35200 ; steps/s = 1.63, tokens/s = 78428 (33225 source, 45203 target) ; Learning rate = 0.000471 ; Loss = 1.777437\n",
      "2024-12-12 17:00:23.051000: I runner.py:310] Step = 35300 ; steps/s = 1.63, tokens/s = 78421 (33185 source, 45236 target) ; Learning rate = 0.000470 ; Loss = 1.791908\n",
      "2024-12-12 17:01:24.406000: I runner.py:310] Step = 35400 ; steps/s = 1.63, tokens/s = 78353 (33181 source, 45172 target) ; Learning rate = 0.000470 ; Loss = 1.809878\n",
      "2024-12-12 17:02:25.237000: I runner.py:310] Step = 35500 ; steps/s = 1.64, tokens/s = 77708 (32897 source, 44811 target) ; Learning rate = 0.000469 ; Loss = 1.802358\n",
      "2024-12-12 17:03:26.488000: I runner.py:310] Step = 35600 ; steps/s = 1.63, tokens/s = 78496 (33240 source, 45256 target) ; Learning rate = 0.000468 ; Loss = 1.763454\n",
      "2024-12-12 17:04:27.797000: I runner.py:310] Step = 35700 ; steps/s = 1.63, tokens/s = 78425 (33206 source, 45219 target) ; Learning rate = 0.000468 ; Loss = 1.770067\n",
      "2024-12-12 17:05:28.742000: I runner.py:310] Step = 35800 ; steps/s = 1.64, tokens/s = 77589 (32861 source, 44728 target) ; Learning rate = 0.000467 ; Loss = 1.755056\n",
      "2024-12-12 17:06:30.002000: I runner.py:310] Step = 35900 ; steps/s = 1.63, tokens/s = 78513 (33257 source, 45256 target) ; Learning rate = 0.000466 ; Loss = 1.759760\n",
      "2024-12-12 17:07:31.288000: I runner.py:310] Step = 36000 ; steps/s = 1.63, tokens/s = 78464 (33237 source, 45227 target) ; Learning rate = 0.000466 ; Loss = 1.799715\n",
      "2024-12-12 17:08:32.577000: I runner.py:310] Step = 36100 ; steps/s = 1.63, tokens/s = 78423 (33191 source, 45232 target) ; Learning rate = 0.000465 ; Loss = 1.802477\n",
      "2024-12-12 17:09:33.434000: I runner.py:310] Step = 36200 ; steps/s = 1.64, tokens/s = 77696 (32900 source, 44796 target) ; Learning rate = 0.000465 ; Loss = 1.779019\n",
      "2024-12-12 17:10:34.744000: I runner.py:310] Step = 36300 ; steps/s = 1.63, tokens/s = 78414 (33194 source, 45220 target) ; Learning rate = 0.000464 ; Loss = 1.763417\n",
      "2024-12-12 17:11:36.033000: I runner.py:310] Step = 36400 ; steps/s = 1.63, tokens/s = 78469 (33242 source, 45227 target) ; Learning rate = 0.000463 ; Loss = 1.765229\n",
      "2024-12-12 17:12:37.360000: I runner.py:310] Step = 36500 ; steps/s = 1.63, tokens/s = 78388 (33186 source, 45202 target) ; Learning rate = 0.000463 ; Loss = 1.782291\n",
      "2024-12-12 17:13:38.264000: I runner.py:310] Step = 36600 ; steps/s = 1.64, tokens/s = 77610 (32858 source, 44752 target) ; Learning rate = 0.000462 ; Loss = 1.790780\n",
      "2024-12-12 17:14:39.513000: I runner.py:310] Step = 36700 ; steps/s = 1.63, tokens/s = 78526 (33264 source, 45262 target) ; Learning rate = 0.000461 ; Loss = 1.772150\n",
      "2024-12-12 17:15:40.868000: I runner.py:310] Step = 36800 ; steps/s = 1.63, tokens/s = 78361 (33174 source, 45187 target) ; Learning rate = 0.000461 ; Loss = 1.776516\n",
      "2024-12-12 17:16:41.713000: I runner.py:310] Step = 36900 ; steps/s = 1.64, tokens/s = 77707 (32907 source, 44800 target) ; Learning rate = 0.000460 ; Loss = 1.782827\n",
      "2024-12-12 17:17:42.917000: I runner.py:310] Step = 37000 ; steps/s = 1.63, tokens/s = 78573 (33275 source, 45298 target) ; Learning rate = 0.000460 ; Loss = 1.768617\n",
      "2024-12-12 17:18:44.221000: I runner.py:310] Step = 37100 ; steps/s = 1.63, tokens/s = 78443 (33227 source, 45216 target) ; Learning rate = 0.000459 ; Loss = 1.788439\n",
      "2024-12-12 17:19:45.500000: I runner.py:310] Step = 37200 ; steps/s = 1.63, tokens/s = 78451 (33211 source, 45240 target) ; Learning rate = 0.000458 ; Loss = 1.780361\n",
      "2024-12-12 17:20:46.365000: I runner.py:310] Step = 37300 ; steps/s = 1.64, tokens/s = 77687 (32892 source, 44795 target) ; Learning rate = 0.000458 ; Loss = 1.798703\n",
      "2024-12-12 17:21:47.593000: I runner.py:310] Step = 37400 ; steps/s = 1.63, tokens/s = 78517 (33243 source, 45274 target) ; Learning rate = 0.000457 ; Loss = 1.762547\n",
      "2024-12-12 17:22:48.902000: I runner.py:310] Step = 37500 ; steps/s = 1.63, tokens/s = 78434 (33228 source, 45206 target) ; Learning rate = 0.000456 ; Loss = 1.769016\n",
      "2024-12-12 17:23:49.724000: I runner.py:310] Step = 37600 ; steps/s = 1.64, tokens/s = 77742 (32922 source, 44820 target) ; Learning rate = 0.000456 ; Loss = 1.757907\n",
      "2024-12-12 17:24:50.979000: I runner.py:310] Step = 37700 ; steps/s = 1.63, tokens/s = 78472 (33221 source, 45251 target) ; Learning rate = 0.000455 ; Loss = 1.763380\n",
      "2024-12-12 17:25:52.276000: I runner.py:310] Step = 37800 ; steps/s = 1.63, tokens/s = 78455 (33230 source, 45225 target) ; Learning rate = 0.000455 ; Loss = 1.778052\n",
      "2024-12-12 17:26:53.501000: I runner.py:310] Step = 37900 ; steps/s = 1.63, tokens/s = 78555 (33269 source, 45286 target) ; Learning rate = 0.000454 ; Loss = 1.779931\n",
      "2024-12-12 17:27:54.385000: I runner.py:310] Step = 38000 ; steps/s = 1.64, tokens/s = 77629 (32860 source, 44769 target) ; Learning rate = 0.000453 ; Loss = 1.776643\n",
      "2024-12-12 17:28:55.611000: I runner.py:310] Step = 38100 ; steps/s = 1.63, tokens/s = 78528 (33253 source, 45275 target) ; Learning rate = 0.000453 ; Loss = 1.757930\n",
      "2024-12-12 17:29:56.943000: I runner.py:310] Step = 38200 ; steps/s = 1.63, tokens/s = 78402 (33203 source, 45199 target) ; Learning rate = 0.000452 ; Loss = 1.776733\n",
      "2024-12-12 17:30:57.805000: I runner.py:310] Step = 38300 ; steps/s = 1.64, tokens/s = 77686 (32891 source, 44795 target) ; Learning rate = 0.000452 ; Loss = 1.763521\n",
      "2024-12-12 17:31:59.070000: I runner.py:310] Step = 38400 ; steps/s = 1.63, tokens/s = 78497 (33249 source, 45248 target) ; Learning rate = 0.000451 ; Loss = 1.755167\n",
      "2024-12-12 17:33:00.359000: I runner.py:310] Step = 38500 ; steps/s = 1.63, tokens/s = 78424 (33201 source, 45223 target) ; Learning rate = 0.000450 ; Loss = 1.776745\n",
      "2024-12-12 17:34:01.604000: I runner.py:310] Step = 38600 ; steps/s = 1.63, tokens/s = 78514 (33247 source, 45267 target) ; Learning rate = 0.000450 ; Loss = 1.793661\n",
      "2024-12-12 17:35:02.513000: I runner.py:310] Step = 38700 ; steps/s = 1.64, tokens/s = 77617 (32858 source, 44759 target) ; Learning rate = 0.000449 ; Loss = 1.776342\n",
      "2024-12-12 17:36:03.740000: I runner.py:310] Step = 38800 ; steps/s = 1.63, tokens/s = 78556 (33271 source, 45285 target) ; Learning rate = 0.000449 ; Loss = 1.751194\n",
      "2024-12-12 17:37:05.054000: I runner.py:310] Step = 38900 ; steps/s = 1.63, tokens/s = 78433 (33219 source, 45214 target) ; Learning rate = 0.000448 ; Loss = 1.757195\n",
      "2024-12-12 17:38:06.335000: I runner.py:310] Step = 39000 ; steps/s = 1.63, tokens/s = 78444 (33212 source, 45232 target) ; Learning rate = 0.000448 ; Loss = 1.762135\n",
      "2024-12-12 17:39:07.225000: I runner.py:310] Step = 39100 ; steps/s = 1.64, tokens/s = 77646 (32879 source, 44767 target) ; Learning rate = 0.000447 ; Loss = 1.741879\n",
      "2024-12-12 17:40:08.488000: I runner.py:310] Step = 39200 ; steps/s = 1.63, tokens/s = 78490 (33233 source, 45257 target) ; Learning rate = 0.000446 ; Loss = 1.755516\n",
      "2024-12-12 17:41:09.751000: I runner.py:310] Step = 39300 ; steps/s = 1.63, tokens/s = 78496 (33244 source, 45252 target) ; Learning rate = 0.000446 ; Loss = 1.777765\n",
      "2024-12-12 17:42:10.603000: I runner.py:310] Step = 39400 ; steps/s = 1.64, tokens/s = 77696 (32902 source, 44794 target) ; Learning rate = 0.000445 ; Loss = 1.760023\n",
      "2024-12-12 17:43:11.953000: I runner.py:310] Step = 39500 ; steps/s = 1.63, tokens/s = 78335 (33149 source, 45186 target) ; Learning rate = 0.000445 ; Loss = 1.752364\n",
      "2024-12-12 17:44:13.210000: I runner.py:310] Step = 39600 ; steps/s = 1.63, tokens/s = 78473 (33215 source, 45258 target) ; Learning rate = 0.000444 ; Loss = 1.749070\n",
      "2024-12-12 17:45:14.522000: I runner.py:310] Step = 39700 ; steps/s = 1.63, tokens/s = 78475 (33271 source, 45204 target) ; Learning rate = 0.000444 ; Loss = 1.763177\n",
      "2024-12-12 17:46:15.340000: I runner.py:310] Step = 39800 ; steps/s = 1.64, tokens/s = 77744 (32922 source, 44822 target) ; Learning rate = 0.000443 ; Loss = 1.774162\n",
      "2024-12-12 17:47:16.620000: I runner.py:310] Step = 39900 ; steps/s = 1.63, tokens/s = 78458 (33215 source, 45243 target) ; Learning rate = 0.000442 ; Loss = 1.748350\n",
      "2024-12-12 17:48:17.931000: I runner.py:310] Step = 40000 ; steps/s = 1.63, tokens/s = 78444 (33232 source, 45212 target) ; Learning rate = 0.000442 ; Loss = 1.744649\n",
      "2024-12-12 17:48:20.213000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-40000\n",
      "2024-12-12 17:48:20.213000: I training.py:192] Running evaluation for step 40000\n",
      "2024-12-12 17:50:30.131000: I training.py:192] Evaluation result for step 40000: loss = 2.524855 ; perplexity = 12.489089\n",
      "2024-12-12 17:51:30.884000: I runner.py:310] Step = 40100 ; steps/s = 1.65, tokens/s = 77895 (32976 source, 44919 target) ; Learning rate = 0.000441 ; Loss = 1.738635\n",
      "2024-12-12 17:52:32.225000: I runner.py:310] Step = 40200 ; steps/s = 1.63, tokens/s = 78419 (33222 source, 45197 target) ; Learning rate = 0.000441 ; Loss = 1.755313\n",
      "2024-12-12 17:53:33.516000: I runner.py:310] Step = 40300 ; steps/s = 1.63, tokens/s = 78450 (33220 source, 45230 target) ; Learning rate = 0.000440 ; Loss = 1.773112\n",
      "2024-12-12 17:54:34.863000: I runner.py:310] Step = 40400 ; steps/s = 1.63, tokens/s = 78365 (33178 source, 45187 target) ; Learning rate = 0.000440 ; Loss = 1.763897\n",
      "2024-12-12 17:55:35.721000: I runner.py:310] Step = 40500 ; steps/s = 1.64, tokens/s = 77657 (32868 source, 44789 target) ; Learning rate = 0.000439 ; Loss = 1.730571\n",
      "2024-12-12 17:56:37.008000: I runner.py:310] Step = 40600 ; steps/s = 1.63, tokens/s = 78465 (33242 source, 45223 target) ; Learning rate = 0.000439 ; Loss = 1.747882\n",
      "2024-12-12 17:57:38.331000: I runner.py:310] Step = 40700 ; steps/s = 1.63, tokens/s = 78386 (33175 source, 45211 target) ; Learning rate = 0.000438 ; Loss = 1.764467\n",
      "2024-12-12 17:58:39.609000: I runner.py:310] Step = 40800 ; steps/s = 1.63, tokens/s = 78489 (33241 source, 45248 target) ; Learning rate = 0.000438 ; Loss = 1.766538\n",
      "2024-12-12 17:59:40.532000: I runner.py:310] Step = 40900 ; steps/s = 1.64, tokens/s = 77595 (32860 source, 44735 target) ; Learning rate = 0.000437 ; Loss = 1.738071\n",
      "2024-12-12 18:00:41.838000: I runner.py:310] Step = 41000 ; steps/s = 1.63, tokens/s = 78439 (33218 source, 45221 target) ; Learning rate = 0.000437 ; Loss = 1.763944\n",
      "2024-12-12 18:01:43.062000: I runner.py:310] Step = 41100 ; steps/s = 1.63, tokens/s = 78541 (33260 source, 45281 target) ; Learning rate = 0.000436 ; Loss = 1.770951\n",
      "2024-12-12 18:02:43.926000: I runner.py:310] Step = 41200 ; steps/s = 1.64, tokens/s = 77685 (32900 source, 44785 target) ; Learning rate = 0.000435 ; Loss = 1.728821\n",
      "2024-12-12 18:03:45.306000: I runner.py:310] Step = 41300 ; steps/s = 1.63, tokens/s = 78324 (33157 source, 45167 target) ; Learning rate = 0.000435 ; Loss = 1.747388\n",
      "2024-12-12 18:04:46.561000: I runner.py:310] Step = 41400 ; steps/s = 1.63, tokens/s = 78472 (33214 source, 45258 target) ; Learning rate = 0.000434 ; Loss = 1.769436\n",
      "2024-12-12 18:05:47.833000: I runner.py:310] Step = 41500 ; steps/s = 1.63, tokens/s = 78505 (33258 source, 45247 target) ; Learning rate = 0.000434 ; Loss = 1.776350\n",
      "2024-12-12 18:06:48.851000: I runner.py:310] Step = 41600 ; steps/s = 1.64, tokens/s = 77496 (32820 source, 44676 target) ; Learning rate = 0.000433 ; Loss = 1.724296\n",
      "2024-12-12 18:07:50.140000: I runner.py:310] Step = 41700 ; steps/s = 1.63, tokens/s = 78464 (33234 source, 45230 target) ; Learning rate = 0.000433 ; Loss = 1.752512\n",
      "2024-12-12 18:08:51.441000: I runner.py:310] Step = 41800 ; steps/s = 1.63, tokens/s = 78421 (33198 source, 45223 target) ; Learning rate = 0.000432 ; Loss = 1.779893\n",
      "2024-12-12 18:09:52.316000: I runner.py:310] Step = 41900 ; steps/s = 1.64, tokens/s = 77641 (32871 source, 44770 target) ; Learning rate = 0.000432 ; Loss = 1.742836\n",
      "2024-12-12 18:10:53.571000: I runner.py:310] Step = 42000 ; steps/s = 1.63, tokens/s = 78490 (33232 source, 45258 target) ; Learning rate = 0.000431 ; Loss = 1.747548\n",
      "2024-12-12 18:11:54.916000: I runner.py:310] Step = 42100 ; steps/s = 1.63, tokens/s = 78392 (33203 source, 45189 target) ; Learning rate = 0.000431 ; Loss = 1.733506\n",
      "2024-12-12 18:12:56.262000: I runner.py:310] Step = 42200 ; steps/s = 1.63, tokens/s = 78383 (33192 source, 45191 target) ; Learning rate = 0.000430 ; Loss = 1.751149\n",
      "2024-12-12 18:13:57.241000: I runner.py:310] Step = 42300 ; steps/s = 1.64, tokens/s = 77570 (32864 source, 44706 target) ; Learning rate = 0.000430 ; Loss = 1.715547\n",
      "2024-12-12 18:14:58.513000: I runner.py:310] Step = 42400 ; steps/s = 1.63, tokens/s = 78484 (33238 source, 45246 target) ; Learning rate = 0.000429 ; Loss = 1.752660\n",
      "2024-12-12 18:15:59.842000: I runner.py:310] Step = 42500 ; steps/s = 1.63, tokens/s = 78382 (33180 source, 45202 target) ; Learning rate = 0.000429 ; Loss = 1.760275\n",
      "2024-12-12 18:17:00.717000: I runner.py:310] Step = 42600 ; steps/s = 1.64, tokens/s = 77651 (32869 source, 44782 target) ; Learning rate = 0.000428 ; Loss = 1.738531\n",
      "2024-12-12 18:18:02.034000: I runner.py:310] Step = 42700 ; steps/s = 1.63, tokens/s = 78402 (33192 source, 45210 target) ; Learning rate = 0.000428 ; Loss = 1.747184\n",
      "2024-12-12 18:19:03.342000: I runner.py:310] Step = 42800 ; steps/s = 1.63, tokens/s = 78432 (33218 source, 45214 target) ; Learning rate = 0.000427 ; Loss = 1.740888\n",
      "2024-12-12 18:20:04.666000: I runner.py:310] Step = 42900 ; steps/s = 1.63, tokens/s = 78402 (33195 source, 45207 target) ; Learning rate = 0.000427 ; Loss = 1.748122\n",
      "2024-12-12 18:21:05.564000: I runner.py:310] Step = 43000 ; steps/s = 1.64, tokens/s = 77654 (32896 source, 44758 target) ; Learning rate = 0.000426 ; Loss = 1.720248\n",
      "2024-12-12 18:22:06.886000: I runner.py:310] Step = 43100 ; steps/s = 1.63, tokens/s = 78365 (33160 source, 45205 target) ; Learning rate = 0.000426 ; Loss = 1.746359\n",
      "2024-12-12 18:23:08.161000: I runner.py:310] Step = 43200 ; steps/s = 1.63, tokens/s = 78494 (33247 source, 45247 target) ; Learning rate = 0.000425 ; Loss = 1.753266\n",
      "2024-12-12 18:24:09.422000: I runner.py:310] Step = 43300 ; steps/s = 1.63, tokens/s = 78517 (33260 source, 45257 target) ; Learning rate = 0.000425 ; Loss = 1.766419\n",
      "2024-12-12 18:25:10.340000: I runner.py:310] Step = 43400 ; steps/s = 1.64, tokens/s = 77612 (32865 source, 44747 target) ; Learning rate = 0.000424 ; Loss = 1.727028\n",
      "2024-12-12 18:26:11.609000: I runner.py:310] Step = 43500 ; steps/s = 1.63, tokens/s = 78468 (33226 source, 45242 target) ; Learning rate = 0.000424 ; Loss = 1.759195\n",
      "2024-12-12 18:27:12.872000: I runner.py:310] Step = 43600 ; steps/s = 1.63, tokens/s = 78496 (33241 source, 45255 target) ; Learning rate = 0.000423 ; Loss = 1.757761\n",
      "2024-12-12 18:28:13.774000: I runner.py:310] Step = 43700 ; steps/s = 1.64, tokens/s = 77613 (32856 source, 44757 target) ; Learning rate = 0.000423 ; Loss = 1.751269\n",
      "2024-12-12 18:29:15.049000: I runner.py:310] Step = 43800 ; steps/s = 1.63, tokens/s = 78467 (33229 source, 45238 target) ; Learning rate = 0.000422 ; Loss = 1.734156\n",
      "2024-12-12 18:30:16.397000: I runner.py:310] Step = 43900 ; steps/s = 1.63, tokens/s = 78398 (33206 source, 45192 target) ; Learning rate = 0.000422 ; Loss = 1.733842\n",
      "2024-12-12 18:31:17.662000: I runner.py:310] Step = 44000 ; steps/s = 1.63, tokens/s = 78474 (33231 source, 45243 target) ; Learning rate = 0.000421 ; Loss = 1.745717\n",
      "2024-12-12 18:32:18.567000: I runner.py:310] Step = 44100 ; steps/s = 1.64, tokens/s = 77614 (32848 source, 44766 target) ; Learning rate = 0.000421 ; Loss = 1.753400\n",
      "2024-12-12 18:33:19.837000: I runner.py:310] Step = 44200 ; steps/s = 1.63, tokens/s = 78484 (33242 source, 45242 target) ; Learning rate = 0.000420 ; Loss = 1.739832\n",
      "2024-12-12 18:34:21.038000: I runner.py:310] Step = 44300 ; steps/s = 1.63, tokens/s = 78568 (33275 source, 45293 target) ; Learning rate = 0.000420 ; Loss = 1.743487\n",
      "2024-12-12 18:35:21.927000: I runner.py:310] Step = 44400 ; steps/s = 1.64, tokens/s = 77661 (32894 source, 44767 target) ; Learning rate = 0.000419 ; Loss = 1.733732\n",
      "2024-12-12 18:36:23.204000: I runner.py:310] Step = 44500 ; steps/s = 1.63, tokens/s = 78490 (33239 source, 45251 target) ; Learning rate = 0.000419 ; Loss = 1.732471\n",
      "2024-12-12 18:37:24.451000: I runner.py:310] Step = 44600 ; steps/s = 1.63, tokens/s = 78474 (33214 source, 45260 target) ; Learning rate = 0.000419 ; Loss = 1.740552\n",
      "2024-12-12 18:38:25.737000: I runner.py:310] Step = 44700 ; steps/s = 1.63, tokens/s = 78463 (33237 source, 45226 target) ; Learning rate = 0.000418 ; Loss = 1.759482\n",
      "2024-12-12 18:39:26.672000: I runner.py:310] Step = 44800 ; steps/s = 1.64, tokens/s = 77565 (32831 source, 44734 target) ; Learning rate = 0.000418 ; Loss = 1.749202\n",
      "2024-12-12 18:40:27.923000: I runner.py:310] Step = 44900 ; steps/s = 1.63, tokens/s = 78529 (33263 source, 45266 target) ; Learning rate = 0.000417 ; Loss = 1.722112\n",
      "2024-12-12 18:41:29.198000: I runner.py:310] Step = 45000 ; steps/s = 1.63, tokens/s = 78474 (33232 source, 45242 target) ; Learning rate = 0.000417 ; Loss = 1.731208\n",
      "2024-12-12 18:41:29.199000: I training.py:192] Running evaluation for step 45000\n",
      "2024-12-12 18:43:38.075000: I training.py:192] Evaluation result for step 45000: loss = 2.560331 ; perplexity = 12.940098\n",
      "2024-12-12 18:44:38.794000: I runner.py:310] Step = 45100 ; steps/s = 1.65, tokens/s = 77868 (32970 source, 44898 target) ; Learning rate = 0.000416 ; Loss = 1.716932\n",
      "2024-12-12 18:45:40.108000: I runner.py:310] Step = 45200 ; steps/s = 1.63, tokens/s = 78439 (33224 source, 45215 target) ; Learning rate = 0.000416 ; Loss = 1.734353\n",
      "2024-12-12 18:46:41.389000: I runner.py:310] Step = 45300 ; steps/s = 1.63, tokens/s = 78458 (33223 source, 45235 target) ; Learning rate = 0.000415 ; Loss = 1.729749\n",
      "2024-12-12 18:47:42.623000: I runner.py:310] Step = 45400 ; steps/s = 1.63, tokens/s = 78495 (33222 source, 45273 target) ; Learning rate = 0.000415 ; Loss = 1.732126\n",
      "2024-12-12 18:48:43.460000: I runner.py:310] Step = 45500 ; steps/s = 1.64, tokens/s = 77713 (32905 source, 44808 target) ; Learning rate = 0.000414 ; Loss = 1.707266\n",
      "2024-12-12 18:49:44.706000: I runner.py:310] Step = 45600 ; steps/s = 1.63, tokens/s = 78531 (33269 source, 45262 target) ; Learning rate = 0.000414 ; Loss = 1.734477\n",
      "2024-12-12 18:50:46.004000: I runner.py:310] Step = 45700 ; steps/s = 1.63, tokens/s = 78460 (33231 source, 45229 target) ; Learning rate = 0.000413 ; Loss = 1.747795\n",
      "2024-12-12 18:51:47.268000: I runner.py:310] Step = 45800 ; steps/s = 1.63, tokens/s = 78461 (33211 source, 45250 target) ; Learning rate = 0.000413 ; Loss = 1.744064\n",
      "2024-12-12 18:52:48.186000: I runner.py:310] Step = 45900 ; steps/s = 1.64, tokens/s = 77622 (32882 source, 44740 target) ; Learning rate = 0.000413 ; Loss = 1.707352\n",
      "2024-12-12 18:53:49.507000: I runner.py:310] Step = 46000 ; steps/s = 1.63, tokens/s = 78416 (33203 source, 45213 target) ; Learning rate = 0.000412 ; Loss = 1.731857\n",
      "2024-12-12 18:54:50.759000: I runner.py:310] Step = 46100 ; steps/s = 1.63, tokens/s = 78480 (33222 source, 45258 target) ; Learning rate = 0.000412 ; Loss = 1.736554\n",
      "2024-12-12 18:55:51.603000: I runner.py:310] Step = 46200 ; steps/s = 1.64, tokens/s = 77707 (32901 source, 44806 target) ; Learning rate = 0.000411 ; Loss = 1.735196\n",
      "2024-12-12 18:56:52.957000: I runner.py:310] Step = 46300 ; steps/s = 1.63, tokens/s = 78360 (33178 source, 45182 target) ; Learning rate = 0.000411 ; Loss = 1.711058\n",
      "2024-12-12 18:57:54.330000: I runner.py:310] Step = 46400 ; steps/s = 1.63, tokens/s = 78365 (33189 source, 45176 target) ; Learning rate = 0.000410 ; Loss = 1.722984\n",
      "2024-12-12 18:58:55.557000: I runner.py:310] Step = 46500 ; steps/s = 1.63, tokens/s = 78545 (33272 source, 45273 target) ; Learning rate = 0.000410 ; Loss = 1.735077\n",
      "2024-12-12 18:59:56.444000: I runner.py:310] Step = 46600 ; steps/s = 1.64, tokens/s = 77633 (32860 source, 44773 target) ; Learning rate = 0.000409 ; Loss = 1.741108\n",
      "2024-12-12 19:00:57.695000: I runner.py:310] Step = 46700 ; steps/s = 1.63, tokens/s = 78494 (33238 source, 45256 target) ; Learning rate = 0.000409 ; Loss = 1.714698\n",
      "2024-12-12 19:01:59.045000: I runner.py:310] Step = 46800 ; steps/s = 1.63, tokens/s = 78386 (33196 source, 45190 target) ; Learning rate = 0.000409 ; Loss = 1.729876\n",
      "2024-12-12 19:02:59.854000: I runner.py:310] Step = 46900 ; steps/s = 1.64, tokens/s = 77774 (32953 source, 44821 target) ; Learning rate = 0.000408 ; Loss = 1.708286\n",
      "2024-12-12 19:04:01.134000: I runner.py:310] Step = 47000 ; steps/s = 1.63, tokens/s = 78454 (33216 source, 45238 target) ; Learning rate = 0.000408 ; Loss = 1.705651\n",
      "2024-12-12 19:05:02.461000: I runner.py:310] Step = 47100 ; steps/s = 1.63, tokens/s = 78410 (33206 source, 45204 target) ; Learning rate = 0.000407 ; Loss = 1.735616\n",
      "2024-12-12 19:06:03.835000: I runner.py:310] Step = 47200 ; steps/s = 1.63, tokens/s = 78342 (33176 source, 45166 target) ; Learning rate = 0.000407 ; Loss = 1.736909\n",
      "2024-12-12 19:07:04.747000: I runner.py:310] Step = 47300 ; steps/s = 1.64, tokens/s = 77596 (32843 source, 44753 target) ; Learning rate = 0.000406 ; Loss = 1.702580\n",
      "2024-12-12 19:08:06.100000: I runner.py:310] Step = 47400 ; steps/s = 1.63, tokens/s = 78357 (33180 source, 45177 target) ; Learning rate = 0.000406 ; Loss = 1.728675\n",
      "2024-12-12 19:09:07.382000: I runner.py:310] Step = 47500 ; steps/s = 1.63, tokens/s = 78467 (33228 source, 45239 target) ; Learning rate = 0.000406 ; Loss = 1.746717\n",
      "2024-12-12 19:10:08.657000: I runner.py:310] Step = 47600 ; steps/s = 1.63, tokens/s = 78476 (33228 source, 45248 target) ; Learning rate = 0.000405 ; Loss = 1.741096\n",
      "2024-12-12 19:11:09.500000: I runner.py:310] Step = 47700 ; steps/s = 1.64, tokens/s = 77716 (32918 source, 44798 target) ; Learning rate = 0.000405 ; Loss = 1.703106\n",
      "2024-12-12 19:12:10.848000: I runner.py:310] Step = 47800 ; steps/s = 1.63, tokens/s = 78378 (33187 source, 45191 target) ; Learning rate = 0.000404 ; Loss = 1.734136\n",
      "2024-12-12 19:13:12.158000: I runner.py:310] Step = 47900 ; steps/s = 1.63, tokens/s = 78426 (33213 source, 45213 target) ; Learning rate = 0.000404 ; Loss = 1.733656\n",
      "2024-12-12 19:14:13.096000: I runner.py:310] Step = 48000 ; steps/s = 1.64, tokens/s = 77567 (32836 source, 44731 target) ; Learning rate = 0.000403 ; Loss = 1.702070\n",
      "2024-12-12 19:15:14.423000: I runner.py:310] Step = 48100 ; steps/s = 1.63, tokens/s = 78395 (33184 source, 45211 target) ; Learning rate = 0.000403 ; Loss = 1.726162\n",
      "2024-12-12 19:16:15.635000: I runner.py:310] Step = 48200 ; steps/s = 1.63, tokens/s = 78564 (33275 source, 45289 target) ; Learning rate = 0.000403 ; Loss = 1.732035\n",
      "2024-12-12 19:17:16.938000: I runner.py:310] Step = 48300 ; steps/s = 1.63, tokens/s = 78449 (33230 source, 45219 target) ; Learning rate = 0.000402 ; Loss = 1.740563\n",
      "2024-12-12 19:18:17.827000: I runner.py:310] Step = 48400 ; steps/s = 1.64, tokens/s = 77674 (32905 source, 44769 target) ; Learning rate = 0.000402 ; Loss = 1.736916\n",
      "2024-12-12 19:19:19.113000: I runner.py:310] Step = 48500 ; steps/s = 1.63, tokens/s = 78444 (33214 source, 45230 target) ; Learning rate = 0.000401 ; Loss = 1.710382\n",
      "2024-12-12 19:20:20.415000: I runner.py:310] Step = 48600 ; steps/s = 1.63, tokens/s = 78428 (33207 source, 45221 target) ; Learning rate = 0.000401 ; Loss = 1.719181\n",
      "2024-12-12 19:21:21.340000: I runner.py:310] Step = 48700 ; steps/s = 1.64, tokens/s = 77595 (32848 source, 44747 target) ; Learning rate = 0.000401 ; Loss = 1.701091\n",
      "2024-12-12 19:22:22.650000: I runner.py:310] Step = 48800 ; steps/s = 1.63, tokens/s = 78445 (33227 source, 45218 target) ; Learning rate = 0.000400 ; Loss = 1.711190\n",
      "2024-12-12 19:23:23.952000: I runner.py:310] Step = 48900 ; steps/s = 1.63, tokens/s = 78458 (33230 source, 45228 target) ; Learning rate = 0.000400 ; Loss = 1.732218\n",
      "2024-12-12 19:24:25.262000: I runner.py:310] Step = 49000 ; steps/s = 1.63, tokens/s = 78415 (33199 source, 45216 target) ; Learning rate = 0.000399 ; Loss = 1.734975\n",
      "2024-12-12 19:25:26.197000: I runner.py:310] Step = 49100 ; steps/s = 1.64, tokens/s = 77568 (32844 source, 44724 target) ; Learning rate = 0.000399 ; Loss = 1.727608\n",
      "2024-12-12 19:26:27.455000: I runner.py:310] Step = 49200 ; steps/s = 1.63, tokens/s = 78491 (33228 source, 45263 target) ; Learning rate = 0.000398 ; Loss = 1.719695\n",
      "2024-12-12 19:27:28.761000: I runner.py:310] Step = 49300 ; steps/s = 1.63, tokens/s = 78413 (33195 source, 45218 target) ; Learning rate = 0.000398 ; Loss = 1.716832\n",
      "2024-12-12 19:28:29.649000: I runner.py:310] Step = 49400 ; steps/s = 1.64, tokens/s = 77668 (32905 source, 44763 target) ; Learning rate = 0.000398 ; Loss = 1.706978\n",
      "2024-12-12 19:29:30.918000: I runner.py:310] Step = 49500 ; steps/s = 1.63, tokens/s = 78456 (33207 source, 45249 target) ; Learning rate = 0.000397 ; Loss = 1.719758\n",
      "2024-12-12 19:30:32.120000: I runner.py:310] Step = 49600 ; steps/s = 1.63, tokens/s = 78584 (33290 source, 45294 target) ; Learning rate = 0.000397 ; Loss = 1.712567\n",
      "2024-12-12 19:31:33.398000: I runner.py:310] Step = 49700 ; steps/s = 1.63, tokens/s = 78480 (33240 source, 45240 target) ; Learning rate = 0.000396 ; Loss = 1.719923\n",
      "2024-12-12 19:32:34.230000: I runner.py:310] Step = 49800 ; steps/s = 1.64, tokens/s = 77705 (32893 source, 44812 target) ; Learning rate = 0.000396 ; Loss = 1.693853\n",
      "2024-12-12 19:33:35.513000: I runner.py:310] Step = 49900 ; steps/s = 1.63, tokens/s = 78471 (33234 source, 45237 target) ; Learning rate = 0.000396 ; Loss = 1.712662\n",
      "2024-12-12 19:34:36.808000: I runner.py:310] Step = 50000 ; steps/s = 1.63, tokens/s = 78440 (33215 source, 45225 target) ; Learning rate = 0.000395 ; Loss = 1.730765\n",
      "2024-12-12 19:34:38.842000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-50000\n",
      "2024-12-12 19:34:38.843000: I training.py:192] Running evaluation for step 50000\n",
      "2024-12-12 19:36:48.862000: I training.py:192] Evaluation result for step 50000: loss = 2.576938 ; perplexity = 13.156786\n",
      "2024-12-12 19:37:49.990000: I runner.py:310] Step = 50100 ; steps/s = 1.64, tokens/s = 78673 (33314 source, 45359 target) ; Learning rate = 0.000395 ; Loss = 1.729769\n",
      "2024-12-12 19:38:50.935000: I runner.py:310] Step = 50200 ; steps/s = 1.64, tokens/s = 77565 (32830 source, 44735 target) ; Learning rate = 0.000394 ; Loss = 1.696372\n",
      "2024-12-12 19:39:52.294000: I runner.py:310] Step = 50300 ; steps/s = 1.63, tokens/s = 78361 (33191 source, 45170 target) ; Learning rate = 0.000394 ; Loss = 1.720672\n",
      "2024-12-12 19:40:53.551000: I runner.py:310] Step = 50400 ; steps/s = 1.63, tokens/s = 78501 (33242 source, 45259 target) ; Learning rate = 0.000394 ; Loss = 1.725408\n",
      "2024-12-12 19:41:54.423000: I runner.py:310] Step = 50500 ; steps/s = 1.64, tokens/s = 77673 (32896 source, 44777 target) ; Learning rate = 0.000393 ; Loss = 1.711512\n",
      "2024-12-12 19:42:55.702000: I runner.py:310] Step = 50600 ; steps/s = 1.63, tokens/s = 78476 (33241 source, 45235 target) ; Learning rate = 0.000393 ; Loss = 1.701005\n",
      "2024-12-12 19:43:56.972000: I runner.py:310] Step = 50700 ; steps/s = 1.63, tokens/s = 78485 (33234 source, 45251 target) ; Learning rate = 0.000393 ; Loss = 1.705889\n",
      "2024-12-12 19:44:58.248000: I runner.py:310] Step = 50800 ; steps/s = 1.63, tokens/s = 78455 (33212 source, 45243 target) ; Learning rate = 0.000392 ; Loss = 1.713111\n",
      "2024-12-12 19:45:59.157000: I runner.py:310] Step = 50900 ; steps/s = 1.64, tokens/s = 77617 (32854 source, 44763 target) ; Learning rate = 0.000392 ; Loss = 1.693873\n",
      "2024-12-12 19:47:00.386000: I runner.py:310] Step = 51000 ; steps/s = 1.63, tokens/s = 78555 (33281 source, 45274 target) ; Learning rate = 0.000391 ; Loss = 1.723294\n",
      "2024-12-12 19:48:01.631000: I runner.py:310] Step = 51100 ; steps/s = 1.63, tokens/s = 78499 (33238 source, 45261 target) ; Learning rate = 0.000391 ; Loss = 1.720862\n",
      "2024-12-12 19:49:02.547000: I runner.py:310] Step = 51200 ; steps/s = 1.64, tokens/s = 77616 (32871 source, 44745 target) ; Learning rate = 0.000391 ; Loss = 1.712489\n",
      "2024-12-12 19:50:03.802000: I runner.py:310] Step = 51300 ; steps/s = 1.63, tokens/s = 78459 (33209 source, 45250 target) ; Learning rate = 0.000390 ; Loss = 1.716080\n",
      "2024-12-12 19:51:05.086000: I runner.py:310] Step = 51400 ; steps/s = 1.63, tokens/s = 78476 (33241 source, 45235 target) ; Learning rate = 0.000390 ; Loss = 1.694595\n",
      "2024-12-12 19:52:06.417000: I runner.py:310] Step = 51500 ; steps/s = 1.63, tokens/s = 78408 (33197 source, 45211 target) ; Learning rate = 0.000389 ; Loss = 1.709881\n",
      "2024-12-12 19:53:07.317000: I runner.py:310] Step = 51600 ; steps/s = 1.64, tokens/s = 77635 (32872 source, 44763 target) ; Learning rate = 0.000389 ; Loss = 1.724098\n",
      "2024-12-12 19:54:08.643000: I runner.py:310] Step = 51700 ; steps/s = 1.63, tokens/s = 78421 (33217 source, 45204 target) ; Learning rate = 0.000389 ; Loss = 1.699814\n",
      "2024-12-12 19:55:09.939000: I runner.py:310] Step = 51800 ; steps/s = 1.63, tokens/s = 78406 (33174 source, 45232 target) ; Learning rate = 0.000388 ; Loss = 1.700601\n",
      "2024-12-12 19:56:10.838000: I runner.py:310] Step = 51900 ; steps/s = 1.64, tokens/s = 77653 (32903 source, 44750 target) ; Learning rate = 0.000388 ; Loss = 1.724174\n",
      "2024-12-12 19:57:12.071000: I runner.py:310] Step = 52000 ; steps/s = 1.63, tokens/s = 78505 (33223 source, 45282 target) ; Learning rate = 0.000388 ; Loss = 1.722599\n",
      "2024-12-12 19:58:13.433000: I runner.py:310] Step = 52100 ; steps/s = 1.63, tokens/s = 78361 (33193 source, 45168 target) ; Learning rate = 0.000387 ; Loss = 1.705044\n",
      "2024-12-12 19:59:14.702000: I runner.py:310] Step = 52200 ; steps/s = 1.63, tokens/s = 78498 (33253 source, 45245 target) ; Learning rate = 0.000387 ; Loss = 1.704649\n",
      "2024-12-12 20:00:15.616000: I runner.py:310] Step = 52300 ; steps/s = 1.64, tokens/s = 77614 (32869 source, 44745 target) ; Learning rate = 0.000386 ; Loss = 1.692264\n",
      "2024-12-12 20:01:16.904000: I runner.py:310] Step = 52400 ; steps/s = 1.63, tokens/s = 78464 (33227 source, 45237 target) ; Learning rate = 0.000386 ; Loss = 1.700313\n",
      "2024-12-12 20:02:18.196000: I runner.py:310] Step = 52500 ; steps/s = 1.63, tokens/s = 78446 (33220 source, 45226 target) ; Learning rate = 0.000386 ; Loss = 1.718835\n",
      "2024-12-12 20:03:19.528000: I runner.py:310] Step = 52600 ; steps/s = 1.63, tokens/s = 78395 (33190 source, 45205 target) ; Learning rate = 0.000385 ; Loss = 1.727827\n",
      "2024-12-12 20:04:20.400000: I runner.py:310] Step = 52700 ; steps/s = 1.64, tokens/s = 77686 (32911 source, 44775 target) ; Learning rate = 0.000385 ; Loss = 1.717863\n",
      "2024-12-12 20:05:21.703000: I runner.py:310] Step = 52800 ; steps/s = 1.63, tokens/s = 78413 (33188 source, 45225 target) ; Learning rate = 0.000385 ; Loss = 1.698861\n",
      "2024-12-12 20:06:22.969000: I runner.py:310] Step = 52900 ; steps/s = 1.63, tokens/s = 78481 (33235 source, 45246 target) ; Learning rate = 0.000384 ; Loss = 1.705583\n",
      "2024-12-12 20:07:23.925000: I runner.py:310] Step = 53000 ; steps/s = 1.64, tokens/s = 77577 (32851 source, 44726 target) ; Learning rate = 0.000384 ; Loss = 1.715872\n",
      "2024-12-12 20:08:25.231000: I runner.py:310] Step = 53100 ; steps/s = 1.63, tokens/s = 78403 (33184 source, 45219 target) ; Learning rate = 0.000384 ; Loss = 1.696598\n",
      "2024-12-12 20:09:26.491000: I runner.py:310] Step = 53200 ; steps/s = 1.63, tokens/s = 78487 (33243 source, 45244 target) ; Learning rate = 0.000383 ; Loss = 1.700758\n",
      "2024-12-12 20:10:27.757000: I runner.py:310] Step = 53300 ; steps/s = 1.63, tokens/s = 78481 (33237 source, 45244 target) ; Learning rate = 0.000383 ; Loss = 1.701290\n",
      "2024-12-12 20:11:28.628000: I runner.py:310] Step = 53400 ; steps/s = 1.64, tokens/s = 77690 (32905 source, 44785 target) ; Learning rate = 0.000382 ; Loss = 1.667253\n",
      "2024-12-12 20:12:29.908000: I runner.py:310] Step = 53500 ; steps/s = 1.63, tokens/s = 78462 (33219 source, 45243 target) ; Learning rate = 0.000382 ; Loss = 1.713100\n",
      "2024-12-12 20:13:31.237000: I runner.py:310] Step = 53600 ; steps/s = 1.63, tokens/s = 78394 (33189 source, 45205 target) ; Learning rate = 0.000382 ; Loss = 1.717918\n",
      "2024-12-12 20:14:32.065000: I runner.py:310] Step = 53700 ; steps/s = 1.64, tokens/s = 77735 (32929 source, 44806 target) ; Learning rate = 0.000381 ; Loss = 1.704926\n",
      "2024-12-12 20:15:33.345000: I runner.py:310] Step = 53800 ; steps/s = 1.63, tokens/s = 78449 (33214 source, 45235 target) ; Learning rate = 0.000381 ; Loss = 1.706701\n",
      "2024-12-12 20:16:34.613000: I runner.py:310] Step = 53900 ; steps/s = 1.63, tokens/s = 78470 (33218 source, 45252 target) ; Learning rate = 0.000381 ; Loss = 1.703230\n",
      "2024-12-12 20:17:35.869000: I runner.py:310] Step = 54000 ; steps/s = 1.63, tokens/s = 78507 (33253 source, 45254 target) ; Learning rate = 0.000380 ; Loss = 1.705591\n",
      "2024-12-12 20:18:36.783000: I runner.py:310] Step = 54100 ; steps/s = 1.64, tokens/s = 77616 (32865 source, 44751 target) ; Learning rate = 0.000380 ; Loss = 1.688452\n",
      "2024-12-12 20:19:38.002000: I runner.py:310] Step = 54200 ; steps/s = 1.63, tokens/s = 78538 (33259 source, 45279 target) ; Learning rate = 0.000380 ; Loss = 1.704567\n",
      "2024-12-12 20:20:39.236000: I runner.py:310] Step = 54300 ; steps/s = 1.63, tokens/s = 78530 (33259 source, 45271 target) ; Learning rate = 0.000379 ; Loss = 1.710203\n",
      "2024-12-12 20:21:40.495000: I runner.py:310] Step = 54400 ; steps/s = 1.63, tokens/s = 78492 (33234 source, 45258 target) ; Learning rate = 0.000379 ; Loss = 1.712935\n",
      "2024-12-12 20:22:41.300000: I runner.py:310] Step = 54500 ; steps/s = 1.64, tokens/s = 77737 (32908 source, 44829 target) ; Learning rate = 0.000379 ; Loss = 1.706280\n",
      "2024-12-12 20:23:42.573000: I runner.py:310] Step = 54600 ; steps/s = 1.63, tokens/s = 78444 (33200 source, 45244 target) ; Learning rate = 0.000378 ; Loss = 1.696736\n",
      "2024-12-12 20:24:43.823000: I runner.py:310] Step = 54700 ; steps/s = 1.63, tokens/s = 78542 (33282 source, 45260 target) ; Learning rate = 0.000378 ; Loss = 1.717114\n",
      "2024-12-12 20:25:44.661000: I runner.py:310] Step = 54800 ; steps/s = 1.64, tokens/s = 77719 (32909 source, 44810 target) ; Learning rate = 0.000378 ; Loss = 1.698268\n",
      "2024-12-12 20:26:45.964000: I runner.py:310] Step = 54900 ; steps/s = 1.63, tokens/s = 78447 (33229 source, 45218 target) ; Learning rate = 0.000377 ; Loss = 1.704482\n",
      "2024-12-12 20:27:47.267000: I runner.py:310] Step = 55000 ; steps/s = 1.63, tokens/s = 78435 (33207 source, 45228 target) ; Learning rate = 0.000377 ; Loss = 1.698750\n",
      "2024-12-12 20:27:47.269000: I training.py:192] Running evaluation for step 55000\n",
      "2024-12-12 20:29:57.047000: I training.py:192] Evaluation result for step 55000: loss = 2.609493 ; perplexity = 13.592155\n",
      "2024-12-12 20:30:58.253000: I runner.py:310] Step = 55100 ; steps/s = 1.63, tokens/s = 78561 (33266 source, 45295 target) ; Learning rate = 0.000377 ; Loss = 1.705358\n",
      "2024-12-12 20:31:59.205000: I runner.py:310] Step = 55200 ; steps/s = 1.64, tokens/s = 77571 (32852 source, 44719 target) ; Learning rate = 0.000376 ; Loss = 1.673014\n",
      "2024-12-12 20:33:00.526000: I runner.py:310] Step = 55300 ; steps/s = 1.63, tokens/s = 78418 (33210 source, 45208 target) ; Learning rate = 0.000376 ; Loss = 1.707080\n",
      "2024-12-12 20:34:01.826000: I runner.py:310] Step = 55400 ; steps/s = 1.63, tokens/s = 78439 (33214 source, 45225 target) ; Learning rate = 0.000376 ; Loss = 1.713319\n",
      "2024-12-12 20:35:02.698000: I runner.py:310] Step = 55500 ; steps/s = 1.64, tokens/s = 77653 (32875 source, 44778 target) ; Learning rate = 0.000375 ; Loss = 1.686391\n",
      "2024-12-12 20:36:03.918000: I runner.py:310] Step = 55600 ; steps/s = 1.63, tokens/s = 78544 (33253 source, 45291 target) ; Learning rate = 0.000375 ; Loss = 1.698836\n",
      "2024-12-12 20:37:05.190000: I runner.py:310] Step = 55700 ; steps/s = 1.63, tokens/s = 78477 (33234 source, 45243 target) ; Learning rate = 0.000375 ; Loss = 1.700108\n",
      "2024-12-12 20:38:06.458000: I runner.py:310] Step = 55800 ; steps/s = 1.63, tokens/s = 78472 (33231 source, 45241 target) ; Learning rate = 0.000374 ; Loss = 1.710221\n",
      "2024-12-12 20:39:07.447000: I runner.py:310] Step = 55900 ; steps/s = 1.64, tokens/s = 77494 (32803 source, 44691 target) ; Learning rate = 0.000374 ; Loss = 1.714234\n",
      "2024-12-12 20:40:08.750000: I runner.py:310] Step = 56000 ; steps/s = 1.63, tokens/s = 78438 (33220 source, 45218 target) ; Learning rate = 0.000374 ; Loss = 1.695794\n",
      "2024-12-12 20:41:09.958000: I runner.py:310] Step = 56100 ; steps/s = 1.63, tokens/s = 78586 (33293 source, 45293 target) ; Learning rate = 0.000373 ; Loss = 1.696048\n",
      "2024-12-12 20:42:10.883000: I runner.py:310] Step = 56200 ; steps/s = 1.64, tokens/s = 77607 (32860 source, 44747 target) ; Learning rate = 0.000373 ; Loss = 1.705759\n",
      "2024-12-12 20:43:12.150000: I runner.py:310] Step = 56300 ; steps/s = 1.63, tokens/s = 78475 (33231 source, 45244 target) ; Learning rate = 0.000373 ; Loss = 1.705811\n",
      "2024-12-12 20:44:13.433000: I runner.py:310] Step = 56400 ; steps/s = 1.63, tokens/s = 78467 (33221 source, 45246 target) ; Learning rate = 0.000372 ; Loss = 1.690298\n",
      "2024-12-12 20:45:14.722000: I runner.py:310] Step = 56500 ; steps/s = 1.63, tokens/s = 78442 (33217 source, 45225 target) ; Learning rate = 0.000372 ; Loss = 1.695328\n",
      "2024-12-12 20:46:15.692000: I runner.py:310] Step = 56600 ; steps/s = 1.64, tokens/s = 77534 (32827 source, 44707 target) ; Learning rate = 0.000372 ; Loss = 1.716234\n",
      "2024-12-12 20:47:16.980000: I runner.py:310] Step = 56700 ; steps/s = 1.63, tokens/s = 78452 (33223 source, 45229 target) ; Learning rate = 0.000371 ; Loss = 1.694590\n",
      "2024-12-12 20:48:18.237000: I runner.py:310] Step = 56800 ; steps/s = 1.63, tokens/s = 78510 (33258 source, 45252 target) ; Learning rate = 0.000371 ; Loss = 1.690919\n",
      "2024-12-12 20:49:19.496000: I runner.py:310] Step = 56900 ; steps/s = 1.63, tokens/s = 78483 (33221 source, 45262 target) ; Learning rate = 0.000371 ; Loss = 1.704911\n",
      "2024-12-12 20:50:20.292000: I runner.py:310] Step = 57000 ; steps/s = 1.65, tokens/s = 77742 (32903 source, 44839 target) ; Learning rate = 0.000370 ; Loss = 1.711397\n",
      "2024-12-12 20:51:21.573000: I runner.py:310] Step = 57100 ; steps/s = 1.63, tokens/s = 78475 (33240 source, 45235 target) ; Learning rate = 0.000370 ; Loss = 1.690071\n",
      "2024-12-12 20:52:22.821000: I runner.py:310] Step = 57200 ; steps/s = 1.63, tokens/s = 78523 (33260 source, 45263 target) ; Learning rate = 0.000370 ; Loss = 1.702382\n",
      "2024-12-12 20:53:23.713000: I runner.py:310] Step = 57300 ; steps/s = 1.64, tokens/s = 77639 (32869 source, 44770 target) ; Learning rate = 0.000369 ; Loss = 1.701250\n",
      "2024-12-12 20:54:24.975000: I runner.py:310] Step = 57400 ; steps/s = 1.63, tokens/s = 78507 (33251 source, 45256 target) ; Learning rate = 0.000369 ; Loss = 1.684673\n",
      "2024-12-12 20:55:26.260000: I runner.py:310] Step = 57500 ; steps/s = 1.63, tokens/s = 78445 (33219 source, 45226 target) ; Learning rate = 0.000369 ; Loss = 1.693142\n",
      "2024-12-12 20:56:27.494000: I runner.py:310] Step = 57600 ; steps/s = 1.63, tokens/s = 78528 (33255 source, 45273 target) ; Learning rate = 0.000368 ; Loss = 1.701753\n",
      "2024-12-12 20:57:28.312000: I runner.py:310] Step = 57700 ; steps/s = 1.64, tokens/s = 77724 (32905 source, 44819 target) ; Learning rate = 0.000368 ; Loss = 1.701188\n",
      "2024-12-12 20:58:29.629000: I runner.py:310] Step = 57800 ; steps/s = 1.63, tokens/s = 78441 (33225 source, 45216 target) ; Learning rate = 0.000368 ; Loss = 1.690622\n",
      "2024-12-12 20:59:30.904000: I runner.py:310] Step = 57900 ; steps/s = 1.63, tokens/s = 78463 (33214 source, 45249 target) ; Learning rate = 0.000367 ; Loss = 1.683689\n",
      "2024-12-12 21:00:31.896000: I runner.py:310] Step = 58000 ; steps/s = 1.64, tokens/s = 77502 (32827 source, 44675 target) ; Learning rate = 0.000367 ; Loss = 1.687107\n",
      "2024-12-12 21:01:33.188000: I runner.py:310] Step = 58100 ; steps/s = 1.63, tokens/s = 78467 (33231 source, 45236 target) ; Learning rate = 0.000367 ; Loss = 1.677354\n",
      "2024-12-12 21:02:34.523000: I runner.py:310] Step = 58200 ; steps/s = 1.63, tokens/s = 78402 (33199 source, 45203 target) ; Learning rate = 0.000366 ; Loss = 1.707716\n",
      "2024-12-12 21:03:35.765000: I runner.py:310] Step = 58300 ; steps/s = 1.63, tokens/s = 78503 (33241 source, 45262 target) ; Learning rate = 0.000366 ; Loss = 1.705432\n",
      "2024-12-12 21:04:36.667000: I runner.py:310] Step = 58400 ; steps/s = 1.64, tokens/s = 77606 (32856 source, 44750 target) ; Learning rate = 0.000366 ; Loss = 1.708690\n",
      "2024-12-12 21:05:37.962000: I runner.py:310] Step = 58500 ; steps/s = 1.63, tokens/s = 78455 (33222 source, 45233 target) ; Learning rate = 0.000365 ; Loss = 1.680545\n",
      "2024-12-12 21:06:39.302000: I runner.py:310] Step = 58600 ; steps/s = 1.63, tokens/s = 78386 (33190 source, 45196 target) ; Learning rate = 0.000365 ; Loss = 1.688216\n",
      "2024-12-12 21:07:40.270000: I runner.py:310] Step = 58700 ; steps/s = 1.64, tokens/s = 77675 (32897 source, 44778 target) ; Learning rate = 0.000365 ; Loss = 1.700293\n",
      "2024-12-12 21:08:41.493000: I runner.py:310] Step = 58800 ; steps/s = 1.63, tokens/s = 78406 (33195 source, 45211 target) ; Learning rate = 0.000365 ; Loss = 1.679937\n",
      "2024-12-12 21:09:42.725000: I runner.py:310] Step = 58900 ; steps/s = 1.63, tokens/s = 78546 (33267 source, 45279 target) ; Learning rate = 0.000364 ; Loss = 1.695114\n",
      "2024-12-12 21:10:43.996000: I runner.py:310] Step = 59000 ; steps/s = 1.63, tokens/s = 78466 (33223 source, 45243 target) ; Learning rate = 0.000364 ; Loss = 1.702112\n",
      "2024-12-12 21:11:44.945000: I runner.py:310] Step = 59100 ; steps/s = 1.64, tokens/s = 77579 (32857 source, 44722 target) ; Learning rate = 0.000364 ; Loss = 1.671279\n",
      "2024-12-12 21:12:46.234000: I runner.py:310] Step = 59200 ; steps/s = 1.63, tokens/s = 78443 (33210 source, 45233 target) ; Learning rate = 0.000363 ; Loss = 1.684948\n",
      "2024-12-12 21:13:47.524000: I runner.py:310] Step = 59300 ; steps/s = 1.63, tokens/s = 78458 (33226 source, 45232 target) ; Learning rate = 0.000363 ; Loss = 1.692000\n",
      "2024-12-12 21:14:48.868000: I runner.py:310] Step = 59400 ; steps/s = 1.63, tokens/s = 78395 (33197 source, 45198 target) ; Learning rate = 0.000363 ; Loss = 1.699793\n",
      "2024-12-12 21:15:49.789000: I runner.py:310] Step = 59500 ; steps/s = 1.64, tokens/s = 77598 (32850 source, 44748 target) ; Learning rate = 0.000362 ; Loss = 1.691671\n",
      "2024-12-12 21:16:51.112000: I runner.py:310] Step = 59600 ; steps/s = 1.63, tokens/s = 78422 (33214 source, 45208 target) ; Learning rate = 0.000362 ; Loss = 1.694301\n",
      "2024-12-12 21:17:52.379000: I runner.py:310] Step = 59700 ; steps/s = 1.63, tokens/s = 78475 (33233 source, 45242 target) ; Learning rate = 0.000362 ; Loss = 1.695019\n",
      "2024-12-12 21:18:53.206000: I runner.py:310] Step = 59800 ; steps/s = 1.64, tokens/s = 77736 (32925 source, 44811 target) ; Learning rate = 0.000361 ; Loss = 1.686882\n",
      "2024-12-12 21:19:54.463000: I runner.py:310] Step = 59900 ; steps/s = 1.63, tokens/s = 78510 (33252 source, 45258 target) ; Learning rate = 0.000361 ; Loss = 1.677138\n",
      "2024-12-12 21:20:55.768000: I runner.py:310] Step = 60000 ; steps/s = 1.63, tokens/s = 78421 (33197 source, 45224 target) ; Learning rate = 0.000361 ; Loss = 1.701396\n",
      "2024-12-12 21:20:57.813000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-60000\n",
      "2024-12-12 21:20:57.813000: I training.py:192] Running evaluation for step 60000\n",
      "2024-12-12 21:23:04.399000: I training.py:192] Evaluation result for step 60000: loss = 2.622800 ; perplexity = 13.774243\n",
      "2024-12-12 21:24:05.627000: I runner.py:310] Step = 60100 ; steps/s = 1.64, tokens/s = 78612 (33288 source, 45324 target) ; Learning rate = 0.000361 ; Loss = 1.702540\n",
      "2024-12-12 21:25:06.644000: I runner.py:310] Step = 60200 ; steps/s = 1.64, tokens/s = 77455 (32790 source, 44665 target) ; Learning rate = 0.000360 ; Loss = 1.666473\n",
      "2024-12-12 21:26:07.915000: I runner.py:310] Step = 60300 ; steps/s = 1.63, tokens/s = 78488 (33242 source, 45246 target) ; Learning rate = 0.000360 ; Loss = 1.685529\n",
      "2024-12-12 21:27:09.231000: I runner.py:310] Step = 60400 ; steps/s = 1.63, tokens/s = 78425 (33218 source, 45207 target) ; Learning rate = 0.000360 ; Loss = 1.688604\n",
      "2024-12-12 21:28:10.161000: I runner.py:310] Step = 60500 ; steps/s = 1.64, tokens/s = 77596 (32849 source, 44747 target) ; Learning rate = 0.000359 ; Loss = 1.678282\n",
      "2024-12-12 21:29:11.494000: I runner.py:310] Step = 60600 ; steps/s = 1.63, tokens/s = 78410 (33206 source, 45204 target) ; Learning rate = 0.000359 ; Loss = 1.679157\n",
      "2024-12-12 21:30:12.764000: I runner.py:310] Step = 60700 ; steps/s = 1.63, tokens/s = 78466 (33230 source, 45236 target) ; Learning rate = 0.000359 ; Loss = 1.693529\n",
      "2024-12-12 21:31:14.102000: I runner.py:310] Step = 60800 ; steps/s = 1.63, tokens/s = 78397 (33203 source, 45194 target) ; Learning rate = 0.000358 ; Loss = 1.711800\n",
      "2024-12-12 21:32:15.086000: I runner.py:310] Step = 60900 ; steps/s = 1.64, tokens/s = 77529 (32823 source, 44706 target) ; Learning rate = 0.000358 ; Loss = 1.710245\n",
      "2024-12-12 21:33:16.459000: I runner.py:310] Step = 61000 ; steps/s = 1.63, tokens/s = 78323 (33149 source, 45174 target) ; Learning rate = 0.000358 ; Loss = 1.685092\n",
      "2024-12-12 21:34:17.787000: I runner.py:310] Step = 61100 ; steps/s = 1.63, tokens/s = 78401 (33196 source, 45205 target) ; Learning rate = 0.000358 ; Loss = 1.689749\n",
      "2024-12-12 21:35:19.035000: I runner.py:310] Step = 61200 ; steps/s = 1.63, tokens/s = 78528 (33270 source, 45258 target) ; Learning rate = 0.000357 ; Loss = 1.678336\n",
      "2024-12-12 21:36:19.912000: I runner.py:310] Step = 61300 ; steps/s = 1.64, tokens/s = 77665 (32888 source, 44777 target) ; Learning rate = 0.000357 ; Loss = 1.670150\n",
      "2024-12-12 21:37:21.259000: I runner.py:310] Step = 61400 ; steps/s = 1.63, tokens/s = 78387 (33200 source, 45187 target) ; Learning rate = 0.000357 ; Loss = 1.689035\n",
      "2024-12-12 21:38:22.558000: I runner.py:310] Step = 61500 ; steps/s = 1.63, tokens/s = 78416 (33190 source, 45226 target) ; Learning rate = 0.000356 ; Loss = 1.691800\n",
      "2024-12-12 21:39:23.471000: I runner.py:310] Step = 61600 ; steps/s = 1.64, tokens/s = 77649 (32899 source, 44750 target) ; Learning rate = 0.000356 ; Loss = 1.691093\n",
      "2024-12-12 21:40:24.741000: I runner.py:310] Step = 61700 ; steps/s = 1.63, tokens/s = 78476 (33230 source, 45246 target) ; Learning rate = 0.000356 ; Loss = 1.675758\n",
      "2024-12-12 21:41:25.993000: I runner.py:310] Step = 61800 ; steps/s = 1.63, tokens/s = 78509 (33248 source, 45261 target) ; Learning rate = 0.000356 ; Loss = 1.682627\n",
      "2024-12-12 21:42:27.285000: I runner.py:310] Step = 61900 ; steps/s = 1.63, tokens/s = 78424 (33194 source, 45230 target) ; Learning rate = 0.000355 ; Loss = 1.678538\n",
      "2024-12-12 21:43:28.199000: I runner.py:310] Step = 62000 ; steps/s = 1.64, tokens/s = 77618 (32866 source, 44752 target) ; Learning rate = 0.000355 ; Loss = 1.705981\n",
      "2024-12-12 21:44:29.505000: I runner.py:310] Step = 62100 ; steps/s = 1.63, tokens/s = 78429 (33214 source, 45215 target) ; Learning rate = 0.000355 ; Loss = 1.667678\n",
      "2024-12-12 21:45:30.867000: I runner.py:310] Step = 62200 ; steps/s = 1.63, tokens/s = 78373 (33195 source, 45178 target) ; Learning rate = 0.000354 ; Loss = 1.689558\n",
      "2024-12-12 21:46:31.797000: I runner.py:310] Step = 62300 ; steps/s = 1.64, tokens/s = 77581 (32841 source, 44740 target) ; Learning rate = 0.000354 ; Loss = 1.684471\n",
      "2024-12-12 21:47:33.102000: I runner.py:310] Step = 62400 ; steps/s = 1.63, tokens/s = 78415 (33196 source, 45219 target) ; Learning rate = 0.000354 ; Loss = 1.685641\n",
      "2024-12-12 21:48:34.418000: I runner.py:310] Step = 62500 ; steps/s = 1.63, tokens/s = 78412 (33198 source, 45214 target) ; Learning rate = 0.000354 ; Loss = 1.680751\n",
      "2024-12-12 21:49:35.805000: I runner.py:310] Step = 62600 ; steps/s = 1.63, tokens/s = 78324 (33167 source, 45157 target) ; Learning rate = 0.000353 ; Loss = 1.688206\n",
      "2024-12-12 21:50:36.735000: I runner.py:310] Step = 62700 ; steps/s = 1.64, tokens/s = 77572 (32833 source, 44739 target) ; Learning rate = 0.000353 ; Loss = 1.693090\n",
      "2024-12-12 21:51:38.070000: I runner.py:310] Step = 62800 ; steps/s = 1.63, tokens/s = 78389 (33192 source, 45197 target) ; Learning rate = 0.000353 ; Loss = 1.673430\n",
      "2024-12-12 21:52:39.466000: I runner.py:310] Step = 62900 ; steps/s = 1.63, tokens/s = 78311 (33168 source, 45143 target) ; Learning rate = 0.000352 ; Loss = 1.684545\n",
      "2024-12-12 21:53:40.457000: I runner.py:310] Step = 63000 ; steps/s = 1.64, tokens/s = 77581 (32879 source, 44702 target) ; Learning rate = 0.000352 ; Loss = 1.675233\n",
      "2024-12-12 21:54:41.766000: I runner.py:310] Step = 63100 ; steps/s = 1.63, tokens/s = 78418 (33194 source, 45224 target) ; Learning rate = 0.000352 ; Loss = 1.683818\n",
      "2024-12-12 21:55:43.103000: I runner.py:310] Step = 63200 ; steps/s = 1.63, tokens/s = 78411 (33219 source, 45192 target) ; Learning rate = 0.000352 ; Loss = 1.670028\n",
      "2024-12-12 21:56:44.322000: I runner.py:310] Step = 63300 ; steps/s = 1.63, tokens/s = 78534 (33260 source, 45274 target) ; Learning rate = 0.000351 ; Loss = 1.683159\n",
      "2024-12-12 21:57:45.192000: I runner.py:310] Step = 63400 ; steps/s = 1.64, tokens/s = 77657 (32873 source, 44784 target) ; Learning rate = 0.000351 ; Loss = 1.661180\n",
      "2024-12-12 21:58:46.481000: I runner.py:310] Step = 63500 ; steps/s = 1.63, tokens/s = 78462 (33230 source, 45232 target) ; Learning rate = 0.000351 ; Loss = 1.677336\n",
      "2024-12-12 21:59:47.817000: I runner.py:310] Step = 63600 ; steps/s = 1.63, tokens/s = 78353 (33154 source, 45199 target) ; Learning rate = 0.000350 ; Loss = 1.693742\n",
      "2024-12-12 22:00:49.148000: I runner.py:310] Step = 63700 ; steps/s = 1.63, tokens/s = 78432 (33232 source, 45200 target) ; Learning rate = 0.000350 ; Loss = 1.690129\n",
      "2024-12-12 22:01:50.170000: I runner.py:310] Step = 63800 ; steps/s = 1.64, tokens/s = 77478 (32805 source, 44673 target) ; Learning rate = 0.000350 ; Loss = 1.681393\n",
      "2024-12-12 22:02:51.437000: I runner.py:310] Step = 63900 ; steps/s = 1.63, tokens/s = 78478 (33232 source, 45246 target) ; Learning rate = 0.000350 ; Loss = 1.682757\n",
      "2024-12-12 22:03:52.679000: I runner.py:310] Step = 64000 ; steps/s = 1.63, tokens/s = 78500 (33235 source, 45265 target) ; Learning rate = 0.000349 ; Loss = 1.683313\n",
      "2024-12-12 22:04:53.569000: I runner.py:310] Step = 64100 ; steps/s = 1.64, tokens/s = 77694 (32921 source, 44773 target) ; Learning rate = 0.000349 ; Loss = 1.669371\n",
      "2024-12-12 22:05:54.881000: I runner.py:310] Step = 64200 ; steps/s = 1.63, tokens/s = 78410 (33199 source, 45211 target) ; Learning rate = 0.000349 ; Loss = 1.675243\n",
      "2024-12-12 22:06:56.197000: I runner.py:310] Step = 64300 ; steps/s = 1.63, tokens/s = 78418 (33206 source, 45212 target) ; Learning rate = 0.000349 ; Loss = 1.681672\n",
      "2024-12-12 22:07:57.486000: I runner.py:310] Step = 64400 ; steps/s = 1.63, tokens/s = 78453 (33218 source, 45235 target) ; Learning rate = 0.000348 ; Loss = 1.685279\n",
      "2024-12-12 22:08:58.434000: I runner.py:310] Step = 64500 ; steps/s = 1.64, tokens/s = 77549 (32817 source, 44732 target) ; Learning rate = 0.000348 ; Loss = 1.689436\n",
      "2024-12-12 22:09:59.706000: I runner.py:310] Step = 64600 ; steps/s = 1.63, tokens/s = 78481 (33238 source, 45243 target) ; Learning rate = 0.000348 ; Loss = 1.672920\n",
      "2024-12-12 22:11:01.096000: I runner.py:310] Step = 64700 ; steps/s = 1.63, tokens/s = 78337 (33187 source, 45150 target) ; Learning rate = 0.000347 ; Loss = 1.673435\n",
      "2024-12-12 22:12:01.971000: I runner.py:310] Step = 64800 ; steps/s = 1.64, tokens/s = 77686 (32901 source, 44785 target) ; Learning rate = 0.000347 ; Loss = 1.673830\n",
      "2024-12-12 22:13:03.273000: I runner.py:310] Step = 64900 ; steps/s = 1.63, tokens/s = 78433 (33213 source, 45220 target) ; Learning rate = 0.000347 ; Loss = 1.681707\n",
      "2024-12-12 22:14:04.631000: I runner.py:310] Step = 65000 ; steps/s = 1.63, tokens/s = 78368 (33181 source, 45187 target) ; Learning rate = 0.000347 ; Loss = 1.677182\n",
      "2024-12-12 22:14:04.632000: I training.py:192] Running evaluation for step 65000\n",
      "2024-12-12 22:16:14.081000: I training.py:192] Evaluation result for step 65000: loss = 2.637555 ; perplexity = 13.978986\n",
      "2024-12-12 22:17:15.257000: I runner.py:310] Step = 65100 ; steps/s = 1.64, tokens/s = 78588 (33274 source, 45314 target) ; Learning rate = 0.000346 ; Loss = 1.676934\n",
      "2024-12-12 22:18:16.240000: I runner.py:310] Step = 65200 ; steps/s = 1.64, tokens/s = 77537 (32833 source, 44704 target) ; Learning rate = 0.000346 ; Loss = 1.681584\n",
      "2024-12-12 22:19:17.612000: I runner.py:310] Step = 65300 ; steps/s = 1.63, tokens/s = 78330 (33161 source, 45169 target) ; Learning rate = 0.000346 ; Loss = 1.670356\n",
      "2024-12-12 22:20:18.980000: I runner.py:310] Step = 65400 ; steps/s = 1.63, tokens/s = 78381 (33200 source, 45181 target) ; Learning rate = 0.000346 ; Loss = 1.673840\n",
      "2024-12-12 22:21:19.964000: I runner.py:310] Step = 65500 ; steps/s = 1.64, tokens/s = 77875 (32977 source, 44898 target) ; Learning rate = 0.000345 ; Loss = 1.686545\n",
      "2024-12-12 22:22:21.174000: I runner.py:310] Step = 65600 ; steps/s = 1.63, tokens/s = 78211 (33131 source, 45080 target) ; Learning rate = 0.000345 ; Loss = 1.661702\n",
      "2024-12-12 22:23:22.516000: I runner.py:310] Step = 65700 ; steps/s = 1.63, tokens/s = 78385 (33194 source, 45191 target) ; Learning rate = 0.000345 ; Loss = 1.695074\n",
      "2024-12-12 22:24:23.884000: I runner.py:310] Step = 65800 ; steps/s = 1.63, tokens/s = 78343 (33166 source, 45177 target) ; Learning rate = 0.000345 ; Loss = 1.686920\n",
      "2024-12-12 22:25:24.868000: I runner.py:310] Step = 65900 ; steps/s = 1.64, tokens/s = 77529 (32824 source, 44705 target) ; Learning rate = 0.000344 ; Loss = 1.664617\n",
      "2024-12-12 22:26:26.268000: I runner.py:310] Step = 66000 ; steps/s = 1.63, tokens/s = 78319 (33167 source, 45152 target) ; Learning rate = 0.000344 ; Loss = 1.679004\n",
      "2024-12-12 22:27:27.525000: I runner.py:310] Step = 66100 ; steps/s = 1.63, tokens/s = 78456 (33204 source, 45252 target) ; Learning rate = 0.000344 ; Loss = 1.690213\n",
      "2024-12-12 22:28:28.825000: I runner.py:310] Step = 66200 ; steps/s = 1.63, tokens/s = 78456 (33233 source, 45223 target) ; Learning rate = 0.000344 ; Loss = 1.703916\n",
      "2024-12-12 22:29:29.664000: I runner.py:310] Step = 66300 ; steps/s = 1.64, tokens/s = 77719 (32911 source, 44808 target) ; Learning rate = 0.000343 ; Loss = 1.687109\n",
      "2024-12-12 22:30:30.973000: I runner.py:310] Step = 66400 ; steps/s = 1.63, tokens/s = 78417 (33196 source, 45221 target) ; Learning rate = 0.000343 ; Loss = 1.673063\n",
      "2024-12-12 22:31:32.264000: I runner.py:310] Step = 66500 ; steps/s = 1.63, tokens/s = 78493 (33266 source, 45227 target) ; Learning rate = 0.000343 ; Loss = 1.678226\n",
      "2024-12-12 22:32:33.146000: I runner.py:310] Step = 66600 ; steps/s = 1.64, tokens/s = 77618 (32842 source, 44776 target) ; Learning rate = 0.000342 ; Loss = 1.677700\n",
      "2024-12-12 22:33:34.409000: I runner.py:310] Step = 66700 ; steps/s = 1.63, tokens/s = 78490 (33240 source, 45250 target) ; Learning rate = 0.000342 ; Loss = 1.673001\n",
      "2024-12-12 22:34:35.731000: I runner.py:310] Step = 66800 ; steps/s = 1.63, tokens/s = 78413 (33207 source, 45206 target) ; Learning rate = 0.000342 ; Loss = 1.665638\n",
      "2024-12-12 22:35:37.087000: I runner.py:310] Step = 66900 ; steps/s = 1.63, tokens/s = 78366 (33187 source, 45179 target) ; Learning rate = 0.000342 ; Loss = 1.669399\n",
      "2024-12-12 22:36:37.985000: I runner.py:310] Step = 67000 ; steps/s = 1.64, tokens/s = 77687 (32919 source, 44768 target) ; Learning rate = 0.000341 ; Loss = 1.696005\n",
      "2024-12-12 22:37:39.294000: I runner.py:310] Step = 67100 ; steps/s = 1.63, tokens/s = 78418 (33199 source, 45219 target) ; Learning rate = 0.000341 ; Loss = 1.667757\n",
      "2024-12-12 22:38:40.606000: I runner.py:310] Step = 67200 ; steps/s = 1.63, tokens/s = 78416 (33210 source, 45206 target) ; Learning rate = 0.000341 ; Loss = 1.665588\n",
      "2024-12-12 22:39:41.539000: I runner.py:310] Step = 67300 ; steps/s = 1.64, tokens/s = 77559 (32825 source, 44734 target) ; Learning rate = 0.000341 ; Loss = 1.666808\n",
      "2024-12-12 22:40:42.861000: I runner.py:310] Step = 67400 ; steps/s = 1.63, tokens/s = 78430 (33230 source, 45200 target) ; Learning rate = 0.000340 ; Loss = 1.657702\n",
      "2024-12-12 22:41:44.112000: I runner.py:310] Step = 67500 ; steps/s = 1.63, tokens/s = 78490 (33229 source, 45261 target) ; Learning rate = 0.000340 ; Loss = 1.692402\n",
      "2024-12-12 22:42:45.457000: I runner.py:310] Step = 67600 ; steps/s = 1.63, tokens/s = 78371 (33183 source, 45188 target) ; Learning rate = 0.000340 ; Loss = 1.689955\n",
      "2024-12-12 22:43:46.302000: I runner.py:310] Step = 67700 ; steps/s = 1.64, tokens/s = 77717 (32902 source, 44815 target) ; Learning rate = 0.000340 ; Loss = 1.657235\n",
      "2024-12-12 22:44:47.645000: I runner.py:310] Step = 67800 ; steps/s = 1.63, tokens/s = 78364 (33173 source, 45191 target) ; Learning rate = 0.000339 ; Loss = 1.668934\n",
      "2024-12-12 22:45:48.946000: I runner.py:310] Step = 67900 ; steps/s = 1.63, tokens/s = 78479 (33255 source, 45224 target) ; Learning rate = 0.000339 ; Loss = 1.684181\n",
      "2024-12-12 22:46:50.327000: I runner.py:310] Step = 68000 ; steps/s = 1.63, tokens/s = 78302 (33144 source, 45158 target) ; Learning rate = 0.000339 ; Loss = 1.688616\n",
      "2024-12-12 22:47:51.244000: I runner.py:310] Step = 68100 ; steps/s = 1.64, tokens/s = 77662 (32902 source, 44760 target) ; Learning rate = 0.000339 ; Loss = 1.646153\n",
      "2024-12-12 22:48:52.514000: I runner.py:310] Step = 68200 ; steps/s = 1.63, tokens/s = 78444 (33204 source, 45240 target) ; Learning rate = 0.000338 ; Loss = 1.667909\n",
      "2024-12-12 22:49:53.835000: I runner.py:310] Step = 68300 ; steps/s = 1.63, tokens/s = 78397 (33191 source, 45206 target) ; Learning rate = 0.000338 ; Loss = 1.675376\n",
      "2024-12-12 22:50:54.714000: I runner.py:310] Step = 68400 ; steps/s = 1.64, tokens/s = 77675 (32899 source, 44776 target) ; Learning rate = 0.000338 ; Loss = 1.671123\n",
      "2024-12-12 22:51:56.003000: I runner.py:310] Step = 68500 ; steps/s = 1.63, tokens/s = 78437 (33204 source, 45233 target) ; Learning rate = 0.000338 ; Loss = 1.670529\n",
      "2024-12-12 22:52:57.279000: I runner.py:310] Step = 68600 ; steps/s = 1.63, tokens/s = 78483 (33241 source, 45242 target) ; Learning rate = 0.000337 ; Loss = 1.663474\n",
      "2024-12-12 22:53:58.619000: I runner.py:310] Step = 68700 ; steps/s = 1.63, tokens/s = 78404 (33210 source, 45194 target) ; Learning rate = 0.000337 ; Loss = 1.669424\n",
      "2024-12-12 22:54:59.522000: I runner.py:310] Step = 68800 ; steps/s = 1.64, tokens/s = 77610 (32852 source, 44758 target) ; Learning rate = 0.000337 ; Loss = 1.683766\n",
      "2024-12-12 22:56:00.823000: I runner.py:310] Step = 68900 ; steps/s = 1.63, tokens/s = 78457 (33238 source, 45219 target) ; Learning rate = 0.000337 ; Loss = 1.668669\n",
      "2024-12-12 22:57:02.132000: I runner.py:310] Step = 69000 ; steps/s = 1.63, tokens/s = 78432 (33211 source, 45221 target) ; Learning rate = 0.000336 ; Loss = 1.673116\n",
      "2024-12-12 22:58:03.022000: I runner.py:310] Step = 69100 ; steps/s = 1.64, tokens/s = 77629 (32866 source, 44763 target) ; Learning rate = 0.000336 ; Loss = 1.668103\n",
      "2024-12-12 22:59:04.404000: I runner.py:310] Step = 69200 ; steps/s = 1.63, tokens/s = 78333 (33168 source, 45165 target) ; Learning rate = 0.000336 ; Loss = 1.669095\n",
      "2024-12-12 23:00:05.727000: I runner.py:310] Step = 69300 ; steps/s = 1.63, tokens/s = 78420 (33216 source, 45204 target) ; Learning rate = 0.000336 ; Loss = 1.682164\n",
      "2024-12-12 23:01:06.958000: I runner.py:310] Step = 69400 ; steps/s = 1.63, tokens/s = 78524 (33247 source, 45277 target) ; Learning rate = 0.000336 ; Loss = 1.674351\n",
      "2024-12-12 23:02:07.897000: I runner.py:310] Step = 69500 ; steps/s = 1.64, tokens/s = 77570 (32846 source, 44724 target) ; Learning rate = 0.000335 ; Loss = 1.687145\n",
      "2024-12-12 23:03:09.164000: I runner.py:310] Step = 69600 ; steps/s = 1.63, tokens/s = 78472 (33225 source, 45247 target) ; Learning rate = 0.000335 ; Loss = 1.657483\n",
      "2024-12-12 23:04:10.491000: I runner.py:310] Step = 69700 ; steps/s = 1.63, tokens/s = 78415 (33213 source, 45202 target) ; Learning rate = 0.000335 ; Loss = 1.661821\n",
      "2024-12-12 23:05:11.281000: I runner.py:310] Step = 69800 ; steps/s = 1.65, tokens/s = 77776 (32928 source, 44848 target) ; Learning rate = 0.000335 ; Loss = 1.673559\n",
      "2024-12-12 23:06:12.572000: I runner.py:310] Step = 69900 ; steps/s = 1.63, tokens/s = 78435 (33206 source, 45229 target) ; Learning rate = 0.000334 ; Loss = 1.671083\n",
      "2024-12-12 23:07:13.875000: I runner.py:310] Step = 70000 ; steps/s = 1.63, tokens/s = 78451 (33228 source, 45223 target) ; Learning rate = 0.000334 ; Loss = 1.674225\n",
      "2024-12-12 23:07:15.608000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-70000\n",
      "2024-12-12 23:07:15.608000: I training.py:192] Running evaluation for step 70000\n",
      "2024-12-12 23:09:22.471000: I training.py:192] Evaluation result for step 70000: loss = 2.653393 ; perplexity = 14.202148\n",
      "2024-12-12 23:10:23.615000: I runner.py:310] Step = 70100 ; steps/s = 1.64, tokens/s = 78687 (33312 source, 45375 target) ; Learning rate = 0.000334 ; Loss = 1.666339\n",
      "2024-12-12 23:11:24.534000: I runner.py:310] Step = 70200 ; steps/s = 1.64, tokens/s = 77608 (32871 source, 44737 target) ; Learning rate = 0.000334 ; Loss = 1.647426\n",
      "2024-12-12 23:12:25.763000: I runner.py:310] Step = 70300 ; steps/s = 1.63, tokens/s = 78532 (33259 source, 45273 target) ; Learning rate = 0.000333 ; Loss = 1.667826\n",
      "2024-12-12 23:13:27.092000: I runner.py:310] Step = 70400 ; steps/s = 1.63, tokens/s = 78429 (33225 source, 45204 target) ; Learning rate = 0.000333 ; Loss = 1.667390\n",
      "2024-12-12 23:14:28.472000: I runner.py:310] Step = 70500 ; steps/s = 1.63, tokens/s = 78310 (33139 source, 45171 target) ; Learning rate = 0.000333 ; Loss = 1.677200\n",
      "2024-12-12 23:15:29.384000: I runner.py:310] Step = 70600 ; steps/s = 1.64, tokens/s = 77583 (32836 source, 44747 target) ; Learning rate = 0.000333 ; Loss = 1.679099\n",
      "2024-12-12 23:16:30.706000: I runner.py:310] Step = 70700 ; steps/s = 1.63, tokens/s = 78425 (33220 source, 45205 target) ; Learning rate = 0.000332 ; Loss = 1.658123\n",
      "2024-12-12 23:17:32.054000: I runner.py:310] Step = 70800 ; steps/s = 1.63, tokens/s = 78401 (33214 source, 45187 target) ; Learning rate = 0.000332 ; Loss = 1.662235\n",
      "2024-12-12 23:18:33.044000: I runner.py:310] Step = 70900 ; steps/s = 1.64, tokens/s = 77505 (32810 source, 44695 target) ; Learning rate = 0.000332 ; Loss = 1.675505\n",
      "2024-12-12 23:19:34.327000: I runner.py:310] Step = 71000 ; steps/s = 1.63, tokens/s = 78451 (33215 source, 45236 target) ; Learning rate = 0.000332 ; Loss = 1.662865\n",
      "2024-12-12 23:20:35.668000: I runner.py:310] Step = 71100 ; steps/s = 1.63, tokens/s = 78420 (33224 source, 45196 target) ; Learning rate = 0.000331 ; Loss = 1.659905\n",
      "2024-12-12 23:21:36.970000: I runner.py:310] Step = 71200 ; steps/s = 1.63, tokens/s = 78427 (33198 source, 45229 target) ; Learning rate = 0.000331 ; Loss = 1.665308\n",
      "2024-12-12 23:22:37.910000: I runner.py:310] Step = 71300 ; steps/s = 1.64, tokens/s = 77586 (32859 source, 44727 target) ; Learning rate = 0.000331 ; Loss = 1.655718\n",
      "2024-12-12 23:23:39.233000: I runner.py:310] Step = 71400 ; steps/s = 1.63, tokens/s = 78433 (33222 source, 45211 target) ; Learning rate = 0.000331 ; Loss = 1.670863\n",
      "2024-12-12 23:24:40.583000: I runner.py:310] Step = 71500 ; steps/s = 1.63, tokens/s = 78334 (33164 source, 45170 target) ; Learning rate = 0.000331 ; Loss = 1.676330\n",
      "2024-12-12 23:25:41.516000: I runner.py:310] Step = 71600 ; steps/s = 1.64, tokens/s = 77596 (32847 source, 44749 target) ; Learning rate = 0.000330 ; Loss = 1.660592\n",
      "2024-12-12 23:26:42.913000: I runner.py:310] Step = 71700 ; steps/s = 1.63, tokens/s = 78287 (33147 source, 45140 target) ; Learning rate = 0.000330 ; Loss = 1.664412\n",
      "2024-12-12 23:27:44.181000: I runner.py:310] Step = 71800 ; steps/s = 1.63, tokens/s = 78493 (33238 source, 45255 target) ; Learning rate = 0.000330 ; Loss = 1.661773\n",
      "2024-12-12 23:28:45.441000: I runner.py:310] Step = 71900 ; steps/s = 1.63, tokens/s = 78512 (33260 source, 45252 target) ; Learning rate = 0.000330 ; Loss = 1.659987\n",
      "2024-12-12 23:29:46.277000: I runner.py:310] Step = 72000 ; steps/s = 1.64, tokens/s = 77717 (32904 source, 44813 target) ; Learning rate = 0.000329 ; Loss = 1.679031\n",
      "2024-12-12 23:30:47.607000: I runner.py:310] Step = 72100 ; steps/s = 1.63, tokens/s = 78407 (33202 source, 45205 target) ; Learning rate = 0.000329 ; Loss = 1.652947\n",
      "2024-12-12 23:31:48.988000: I runner.py:310] Step = 72200 ; steps/s = 1.63, tokens/s = 78323 (33164 source, 45159 target) ; Learning rate = 0.000329 ; Loss = 1.662315\n",
      "2024-12-12 23:32:50.173000: I runner.py:310] Step = 72300 ; steps/s = 1.63, tokens/s = 77940 (33007 source, 44933 target) ; Learning rate = 0.000329 ; Loss = 1.665802\n",
      "2024-12-12 23:33:51.303000: I runner.py:310] Step = 72400 ; steps/s = 1.64, tokens/s = 78016 (33047 source, 44969 target) ; Learning rate = 0.000328 ; Loss = 1.648866\n",
      "2024-12-12 23:34:52.633000: I runner.py:310] Step = 72500 ; steps/s = 1.63, tokens/s = 78371 (33175 source, 45196 target) ; Learning rate = 0.000328 ; Loss = 1.668805\n",
      "2024-12-12 23:35:53.986000: I runner.py:310] Step = 72600 ; steps/s = 1.63, tokens/s = 78359 (33172 source, 45187 target) ; Learning rate = 0.000328 ; Loss = 1.667078\n",
      "2024-12-12 23:36:54.838000: I runner.py:310] Step = 72700 ; steps/s = 1.64, tokens/s = 77732 (32937 source, 44795 target) ; Learning rate = 0.000328 ; Loss = 1.675158\n",
      "2024-12-12 23:37:56.103000: I runner.py:310] Step = 72800 ; steps/s = 1.63, tokens/s = 78469 (33217 source, 45252 target) ; Learning rate = 0.000328 ; Loss = 1.659412\n",
      "2024-12-12 23:38:57.381000: I runner.py:310] Step = 72900 ; steps/s = 1.63, tokens/s = 78479 (33237 source, 45242 target) ; Learning rate = 0.000327 ; Loss = 1.665251\n",
      "2024-12-12 23:39:58.732000: I runner.py:310] Step = 73000 ; steps/s = 1.63, tokens/s = 78355 (33174 source, 45181 target) ; Learning rate = 0.000327 ; Loss = 1.666976\n",
      "2024-12-12 23:40:59.682000: I runner.py:310] Step = 73100 ; steps/s = 1.64, tokens/s = 77577 (32839 source, 44738 target) ; Learning rate = 0.000327 ; Loss = 1.673685\n",
      "2024-12-12 23:42:01.033000: I runner.py:310] Step = 73200 ; steps/s = 1.63, tokens/s = 78367 (33187 source, 45180 target) ; Learning rate = 0.000327 ; Loss = 1.656841\n",
      "2024-12-12 23:43:02.403000: I runner.py:310] Step = 73300 ; steps/s = 1.63, tokens/s = 78369 (33196 source, 45173 target) ; Learning rate = 0.000326 ; Loss = 1.665322\n",
      "2024-12-12 23:44:03.279000: I runner.py:310] Step = 73400 ; steps/s = 1.64, tokens/s = 77647 (32872 source, 44775 target) ; Learning rate = 0.000326 ; Loss = 1.659114\n",
      "2024-12-12 23:45:04.556000: I runner.py:310] Step = 73500 ; steps/s = 1.63, tokens/s = 78485 (33239 source, 45246 target) ; Learning rate = 0.000326 ; Loss = 1.661259\n",
      "2024-12-12 23:46:05.838000: I runner.py:310] Step = 73600 ; steps/s = 1.63, tokens/s = 78446 (33218 source, 45228 target) ; Learning rate = 0.000326 ; Loss = 1.666114\n",
      "2024-12-12 23:47:07.159000: I runner.py:310] Step = 73700 ; steps/s = 1.63, tokens/s = 78408 (33203 source, 45205 target) ; Learning rate = 0.000326 ; Loss = 1.669943\n",
      "2024-12-12 23:48:08.105000: I runner.py:310] Step = 73800 ; steps/s = 1.64, tokens/s = 77589 (32859 source, 44730 target) ; Learning rate = 0.000325 ; Loss = 1.640582\n",
      "2024-12-12 23:49:09.398000: I runner.py:310] Step = 73900 ; steps/s = 1.63, tokens/s = 78434 (33211 source, 45223 target) ; Learning rate = 0.000325 ; Loss = 1.667695\n",
      "2024-12-12 23:50:10.699000: I runner.py:310] Step = 74000 ; steps/s = 1.63, tokens/s = 78432 (33212 source, 45220 target) ; Learning rate = 0.000325 ; Loss = 1.665014\n",
      "2024-12-12 23:51:11.575000: I runner.py:310] Step = 74100 ; steps/s = 1.64, tokens/s = 77675 (32890 source, 44785 target) ; Learning rate = 0.000325 ; Loss = 1.653832\n",
      "2024-12-12 23:52:12.927000: I runner.py:310] Step = 74200 ; steps/s = 1.63, tokens/s = 78375 (33191 source, 45184 target) ; Learning rate = 0.000324 ; Loss = 1.666232\n",
      "2024-12-12 23:53:14.134000: I runner.py:310] Step = 74300 ; steps/s = 1.63, tokens/s = 78558 (33265 source, 45293 target) ; Learning rate = 0.000324 ; Loss = 1.659977\n",
      "2024-12-12 23:54:15.418000: I runner.py:310] Step = 74400 ; steps/s = 1.63, tokens/s = 78450 (33214 source, 45236 target) ; Learning rate = 0.000324 ; Loss = 1.656096\n",
      "2024-12-12 23:55:16.320000: I runner.py:310] Step = 74500 ; steps/s = 1.64, tokens/s = 77629 (32869 source, 44760 target) ; Learning rate = 0.000324 ; Loss = 1.638915\n",
      "2024-12-12 23:56:17.691000: I runner.py:310] Step = 74600 ; steps/s = 1.63, tokens/s = 78371 (33202 source, 45169 target) ; Learning rate = 0.000324 ; Loss = 1.663249\n",
      "2024-12-12 23:57:18.977000: I runner.py:310] Step = 74700 ; steps/s = 1.63, tokens/s = 78437 (33203 source, 45234 target) ; Learning rate = 0.000323 ; Loss = 1.665884\n",
      "2024-12-12 23:58:20.314000: I runner.py:310] Step = 74800 ; steps/s = 1.63, tokens/s = 78386 (33188 source, 45198 target) ; Learning rate = 0.000323 ; Loss = 1.678999\n",
      "2024-12-12 23:59:21.242000: I runner.py:310] Step = 74900 ; steps/s = 1.64, tokens/s = 77630 (32895 source, 44735 target) ; Learning rate = 0.000323 ; Loss = 1.650123\n",
      "2024-12-13 00:00:22.492000: I runner.py:310] Step = 75000 ; steps/s = 1.63, tokens/s = 78519 (33256 source, 45263 target) ; Learning rate = 0.000323 ; Loss = 1.665214\n",
      "2024-12-13 00:00:22.493000: I training.py:192] Running evaluation for step 75000\n",
      "2024-12-13 00:02:28.656000: I training.py:192] Evaluation result for step 75000: loss = 2.663205 ; perplexity = 14.342185\n",
      "2024-12-13 00:03:29.809000: I runner.py:310] Step = 75100 ; steps/s = 1.64, tokens/s = 78642 (33296 source, 45346 target) ; Learning rate = 0.000323 ; Loss = 1.677047\n",
      "2024-12-13 00:04:30.755000: I runner.py:310] Step = 75200 ; steps/s = 1.64, tokens/s = 77531 (32808 source, 44723 target) ; Learning rate = 0.000322 ; Loss = 1.652684\n",
      "2024-12-13 00:05:32.024000: I runner.py:310] Step = 75300 ; steps/s = 1.63, tokens/s = 78481 (33234 source, 45247 target) ; Learning rate = 0.000322 ; Loss = 1.664578\n",
      "2024-12-13 00:06:33.375000: I runner.py:310] Step = 75400 ; steps/s = 1.63, tokens/s = 78375 (33192 source, 45183 target) ; Learning rate = 0.000322 ; Loss = 1.670217\n",
      "2024-12-13 00:07:34.699000: I runner.py:310] Step = 75500 ; steps/s = 1.63, tokens/s = 78424 (33216 source, 45208 target) ; Learning rate = 0.000322 ; Loss = 1.662625\n",
      "2024-12-13 00:08:35.606000: I runner.py:310] Step = 75600 ; steps/s = 1.64, tokens/s = 77621 (32867 source, 44754 target) ; Learning rate = 0.000321 ; Loss = 1.645280\n",
      "2024-12-13 00:09:36.952000: I runner.py:310] Step = 75700 ; steps/s = 1.63, tokens/s = 78385 (33204 source, 45181 target) ; Learning rate = 0.000321 ; Loss = 1.672326\n",
      "2024-12-13 00:10:38.298000: I runner.py:310] Step = 75800 ; steps/s = 1.63, tokens/s = 78348 (33156 source, 45192 target) ; Learning rate = 0.000321 ; Loss = 1.671636\n",
      "2024-12-13 00:11:39.242000: I runner.py:310] Step = 75900 ; steps/s = 1.64, tokens/s = 77592 (32856 source, 44736 target) ; Learning rate = 0.000321 ; Loss = 1.657435\n",
      "2024-12-13 00:12:40.546000: I runner.py:310] Step = 76000 ; steps/s = 1.63, tokens/s = 78453 (33243 source, 45210 target) ; Learning rate = 0.000321 ; Loss = 1.650947\n",
      "2024-12-13 00:13:41.868000: I runner.py:310] Step = 76100 ; steps/s = 1.63, tokens/s = 78393 (33181 source, 45212 target) ; Learning rate = 0.000320 ; Loss = 1.668063\n",
      "2024-12-13 00:14:43.270000: I runner.py:310] Step = 76200 ; steps/s = 1.63, tokens/s = 78311 (33162 source, 45149 target) ; Learning rate = 0.000320 ; Loss = 1.664589\n",
      "2024-12-13 00:15:44.164000: I runner.py:310] Step = 76300 ; steps/s = 1.64, tokens/s = 77613 (32850 source, 44763 target) ; Learning rate = 0.000320 ; Loss = 1.672787\n",
      "2024-12-13 00:16:45.450000: I runner.py:310] Step = 76400 ; steps/s = 1.63, tokens/s = 78441 (33218 source, 45223 target) ; Learning rate = 0.000320 ; Loss = 1.653527\n",
      "2024-12-13 00:17:46.699000: I runner.py:310] Step = 76500 ; steps/s = 1.63, tokens/s = 78531 (33263 source, 45268 target) ; Learning rate = 0.000320 ; Loss = 1.653742\n",
      "2024-12-13 00:18:47.670000: I runner.py:310] Step = 76600 ; steps/s = 1.64, tokens/s = 77547 (32834 source, 44713 target) ; Learning rate = 0.000319 ; Loss = 1.660927\n",
      "2024-12-13 00:19:48.930000: I runner.py:310] Step = 76700 ; steps/s = 1.63, tokens/s = 78516 (33260 source, 45256 target) ; Learning rate = 0.000319 ; Loss = 1.658872\n",
      "2024-12-13 00:20:50.262000: I runner.py:310] Step = 76800 ; steps/s = 1.63, tokens/s = 78384 (33178 source, 45206 target) ; Learning rate = 0.000319 ; Loss = 1.659770\n",
      "2024-12-13 00:21:51.617000: I runner.py:310] Step = 76900 ; steps/s = 1.63, tokens/s = 78360 (33181 source, 45179 target) ; Learning rate = 0.000319 ; Loss = 1.668046\n",
      "2024-12-13 00:22:52.458000: I runner.py:310] Step = 77000 ; steps/s = 1.64, tokens/s = 77702 (32907 source, 44795 target) ; Learning rate = 0.000319 ; Loss = 1.659510\n",
      "2024-12-13 00:23:53.776000: I runner.py:310] Step = 77100 ; steps/s = 1.63, tokens/s = 78384 (33183 source, 45201 target) ; Learning rate = 0.000318 ; Loss = 1.646521\n",
      "2024-12-13 00:24:55.058000: I runner.py:310] Step = 77200 ; steps/s = 1.63, tokens/s = 78481 (33233 source, 45248 target) ; Learning rate = 0.000318 ; Loss = 1.661269\n",
      "2024-12-13 00:25:56.426000: I runner.py:310] Step = 77300 ; steps/s = 1.63, tokens/s = 78356 (33181 source, 45175 target) ; Learning rate = 0.000318 ; Loss = 1.663983\n",
      "2024-12-13 00:26:57.366000: I runner.py:310] Step = 77400 ; steps/s = 1.64, tokens/s = 77580 (32852 source, 44728 target) ; Learning rate = 0.000318 ; Loss = 1.672516\n",
      "2024-12-13 00:27:58.665000: I runner.py:310] Step = 77500 ; steps/s = 1.63, tokens/s = 78451 (33234 source, 45217 target) ; Learning rate = 0.000317 ; Loss = 1.659329\n",
      "2024-12-13 00:28:59.954000: I runner.py:310] Step = 77600 ; steps/s = 1.63, tokens/s = 78454 (33219 source, 45235 target) ; Learning rate = 0.000317 ; Loss = 1.648722\n",
      "2024-12-13 00:30:00.813000: I runner.py:310] Step = 77700 ; steps/s = 1.64, tokens/s = 77697 (32904 source, 44793 target) ; Learning rate = 0.000317 ; Loss = 1.644042\n",
      "2024-12-13 00:31:02.091000: I runner.py:310] Step = 77800 ; steps/s = 1.63, tokens/s = 78482 (33246 source, 45236 target) ; Learning rate = 0.000317 ; Loss = 1.659619\n",
      "2024-12-13 00:32:03.350000: I runner.py:310] Step = 77900 ; steps/s = 1.63, tokens/s = 78463 (33201 source, 45262 target) ; Learning rate = 0.000317 ; Loss = 1.663369\n",
      "2024-12-13 00:33:04.630000: I runner.py:310] Step = 78000 ; steps/s = 1.63, tokens/s = 78469 (33227 source, 45242 target) ; Learning rate = 0.000316 ; Loss = 1.671220\n",
      "2024-12-13 00:34:05.611000: I runner.py:310] Step = 78100 ; steps/s = 1.64, tokens/s = 77546 (32860 source, 44686 target) ; Learning rate = 0.000316 ; Loss = 1.671498\n",
      "2024-12-13 00:35:06.963000: I runner.py:310] Step = 78200 ; steps/s = 1.63, tokens/s = 78333 (33148 source, 45185 target) ; Learning rate = 0.000316 ; Loss = 1.649109\n",
      "2024-12-13 00:36:08.362000: I runner.py:310] Step = 78300 ; steps/s = 1.63, tokens/s = 78317 (33162 source, 45155 target) ; Learning rate = 0.000316 ; Loss = 1.649664\n",
      "2024-12-13 00:37:09.252000: I runner.py:310] Step = 78400 ; steps/s = 1.64, tokens/s = 77658 (32889 source, 44769 target) ; Learning rate = 0.000316 ; Loss = 1.655878\n",
      "2024-12-13 00:38:10.552000: I runner.py:310] Step = 78500 ; steps/s = 1.63, tokens/s = 78447 (33224 source, 45223 target) ; Learning rate = 0.000315 ; Loss = 1.649391\n",
      "2024-12-13 00:39:11.843000: I runner.py:310] Step = 78600 ; steps/s = 1.63, tokens/s = 78456 (33230 source, 45226 target) ; Learning rate = 0.000315 ; Loss = 1.656508\n",
      "2024-12-13 00:40:13.128000: I runner.py:310] Step = 78700 ; steps/s = 1.63, tokens/s = 78442 (33209 source, 45233 target) ; Learning rate = 0.000315 ; Loss = 1.652004\n",
      "2024-12-13 00:41:14.038000: I runner.py:310] Step = 78800 ; steps/s = 1.64, tokens/s = 77618 (32847 source, 44771 target) ; Learning rate = 0.000315 ; Loss = 1.668176\n",
      "2024-12-13 00:42:15.360000: I runner.py:310] Step = 78900 ; steps/s = 1.63, tokens/s = 78427 (33212 source, 45215 target) ; Learning rate = 0.000315 ; Loss = 1.652916\n",
      "2024-12-13 00:43:16.657000: I runner.py:310] Step = 79000 ; steps/s = 1.63, tokens/s = 78441 (33230 source, 45211 target) ; Learning rate = 0.000314 ; Loss = 1.643846\n",
      "2024-12-13 00:44:17.858000: I runner.py:310] Step = 79100 ; steps/s = 1.63, tokens/s = 78175 (33108 source, 45067 target) ; Learning rate = 0.000314 ; Loss = 1.637953\n",
      "2024-12-13 00:45:18.777000: I runner.py:310] Step = 79200 ; steps/s = 1.64, tokens/s = 77999 (33026 source, 44973 target) ; Learning rate = 0.000314 ; Loss = 1.642126\n",
      "2024-12-13 00:46:20.024000: I runner.py:310] Step = 79300 ; steps/s = 1.63, tokens/s = 78507 (33246 source, 45261 target) ; Learning rate = 0.000314 ; Loss = 1.653388\n",
      "2024-12-13 00:47:21.445000: I runner.py:310] Step = 79400 ; steps/s = 1.63, tokens/s = 78278 (33143 source, 45135 target) ; Learning rate = 0.000314 ; Loss = 1.669409\n",
      "2024-12-13 00:48:22.450000: I runner.py:310] Step = 79500 ; steps/s = 1.64, tokens/s = 77527 (32850 source, 44677 target) ; Learning rate = 0.000313 ; Loss = 1.641381\n",
      "2024-12-13 00:49:23.732000: I runner.py:310] Step = 79600 ; steps/s = 1.63, tokens/s = 78456 (33219 source, 45237 target) ; Learning rate = 0.000313 ; Loss = 1.657893\n",
      "2024-12-13 00:50:24.972000: I runner.py:310] Step = 79700 ; steps/s = 1.63, tokens/s = 78479 (33209 source, 45270 target) ; Learning rate = 0.000313 ; Loss = 1.653010\n",
      "2024-12-13 00:51:26.324000: I runner.py:310] Step = 79800 ; steps/s = 1.63, tokens/s = 78376 (33189 source, 45187 target) ; Learning rate = 0.000313 ; Loss = 1.654323\n",
      "2024-12-13 00:52:27.172000: I runner.py:310] Step = 79900 ; steps/s = 1.64, tokens/s = 77698 (32895 source, 44803 target) ; Learning rate = 0.000313 ; Loss = 1.663058\n",
      "2024-12-13 00:53:28.518000: I runner.py:310] Step = 80000 ; steps/s = 1.63, tokens/s = 78398 (33213 source, 45185 target) ; Learning rate = 0.000312 ; Loss = 1.647543\n",
      "2024-12-13 00:53:30.683000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-80000\n",
      "2024-12-13 00:53:30.683000: I training.py:192] Running evaluation for step 80000\n",
      "2024-12-13 00:55:36.088000: I training.py:192] Evaluation result for step 80000: loss = 2.680426 ; perplexity = 14.591304\n",
      "2024-12-13 00:56:37.195000: I runner.py:310] Step = 80100 ; steps/s = 1.64, tokens/s = 78756 (33348 source, 45408 target) ; Learning rate = 0.000312 ; Loss = 1.652010\n",
      "2024-12-13 00:57:38.147000: I runner.py:310] Step = 80200 ; steps/s = 1.64, tokens/s = 77573 (32848 source, 44725 target) ; Learning rate = 0.000312 ; Loss = 1.643494\n",
      "2024-12-13 00:58:39.455000: I runner.py:310] Step = 80300 ; steps/s = 1.63, tokens/s = 78403 (33186 source, 45217 target) ; Learning rate = 0.000312 ; Loss = 1.650857\n",
      "2024-12-13 00:59:40.809000: I runner.py:310] Step = 80400 ; steps/s = 1.63, tokens/s = 78387 (33197 source, 45190 target) ; Learning rate = 0.000312 ; Loss = 1.655836\n",
      "2024-12-13 01:00:42.074000: I runner.py:310] Step = 80500 ; steps/s = 1.63, tokens/s = 78488 (33248 source, 45240 target) ; Learning rate = 0.000312 ; Loss = 1.652150\n",
      "2024-12-13 01:01:43.004000: I runner.py:310] Step = 80600 ; steps/s = 1.64, tokens/s = 77575 (32839 source, 44736 target) ; Learning rate = 0.000311 ; Loss = 1.631770\n",
      "2024-12-13 01:02:44.310000: I runner.py:310] Step = 80700 ; steps/s = 1.63, tokens/s = 78443 (33224 source, 45219 target) ; Learning rate = 0.000311 ; Loss = 1.652526\n",
      "2024-12-13 01:03:45.652000: I runner.py:310] Step = 80800 ; steps/s = 1.63, tokens/s = 78375 (33181 source, 45194 target) ; Learning rate = 0.000311 ; Loss = 1.653681\n",
      "2024-12-13 01:04:46.573000: I runner.py:310] Step = 80900 ; steps/s = 1.64, tokens/s = 77617 (32875 source, 44742 target) ; Learning rate = 0.000311 ; Loss = 1.649200\n",
      "2024-12-13 01:05:47.991000: I runner.py:310] Step = 81000 ; steps/s = 1.63, tokens/s = 78290 (33144 source, 45146 target) ; Learning rate = 0.000311 ; Loss = 1.647540\n",
      "2024-12-13 01:06:49.217000: I runner.py:310] Step = 81100 ; steps/s = 1.63, tokens/s = 78530 (33256 source, 45274 target) ; Learning rate = 0.000310 ; Loss = 1.663919\n",
      "2024-12-13 01:07:50.563000: I runner.py:310] Step = 81200 ; steps/s = 1.63, tokens/s = 78348 (33169 source, 45179 target) ; Learning rate = 0.000310 ; Loss = 1.661346\n",
      "2024-12-13 01:08:51.486000: I runner.py:310] Step = 81300 ; steps/s = 1.64, tokens/s = 77637 (32897 source, 44740 target) ; Learning rate = 0.000310 ; Loss = 1.629838\n",
      "2024-12-13 01:09:52.794000: I runner.py:310] Step = 81400 ; steps/s = 1.63, tokens/s = 78445 (33223 source, 45222 target) ; Learning rate = 0.000310 ; Loss = 1.651033\n",
      "2024-12-13 01:10:54.147000: I runner.py:310] Step = 81500 ; steps/s = 1.63, tokens/s = 78367 (33177 source, 45190 target) ; Learning rate = 0.000310 ; Loss = 1.657124\n",
      "2024-12-13 01:11:55.441000: I runner.py:310] Step = 81600 ; steps/s = 1.63, tokens/s = 78427 (33202 source, 45225 target) ; Learning rate = 0.000309 ; Loss = 1.657017\n",
      "2024-12-13 01:12:56.392000: I runner.py:310] Step = 81700 ; steps/s = 1.64, tokens/s = 77560 (32829 source, 44731 target) ; Learning rate = 0.000309 ; Loss = 1.640073\n",
      "2024-12-13 01:13:57.677000: I runner.py:310] Step = 81800 ; steps/s = 1.63, tokens/s = 78441 (33209 source, 45232 target) ; Learning rate = 0.000309 ; Loss = 1.652009\n",
      "2024-12-13 01:14:58.988000: I runner.py:310] Step = 81900 ; steps/s = 1.63, tokens/s = 78452 (33247 source, 45205 target) ; Learning rate = 0.000309 ; Loss = 1.661979\n",
      "2024-12-13 01:15:59.840000: I runner.py:310] Step = 82000 ; steps/s = 1.64, tokens/s = 77703 (32897 source, 44806 target) ; Learning rate = 0.000309 ; Loss = 1.642604\n",
      "2024-12-13 01:17:01.171000: I runner.py:310] Step = 82100 ; steps/s = 1.63, tokens/s = 78403 (33208 source, 45195 target) ; Learning rate = 0.000308 ; Loss = 1.647533\n",
      "2024-12-13 01:18:02.533000: I runner.py:310] Step = 82200 ; steps/s = 1.63, tokens/s = 78343 (33169 source, 45174 target) ; Learning rate = 0.000308 ; Loss = 1.650988\n",
      "2024-12-13 01:19:03.852000: I runner.py:310] Step = 82300 ; steps/s = 1.63, tokens/s = 78430 (33219 source, 45211 target) ; Learning rate = 0.000308 ; Loss = 1.666393\n",
      "2024-12-13 01:20:04.722000: I runner.py:310] Step = 82400 ; steps/s = 1.64, tokens/s = 77672 (32884 source, 44788 target) ; Learning rate = 0.000308 ; Loss = 1.628245\n",
      "2024-12-13 01:21:06.054000: I runner.py:310] Step = 82500 ; steps/s = 1.63, tokens/s = 78419 (33223 source, 45196 target) ; Learning rate = 0.000308 ; Loss = 1.659580\n",
      "2024-12-13 01:22:07.378000: I runner.py:310] Step = 82600 ; steps/s = 1.63, tokens/s = 78385 (33175 source, 45210 target) ; Learning rate = 0.000308 ; Loss = 1.658660\n",
      "2024-12-13 01:23:08.243000: I runner.py:310] Step = 82700 ; steps/s = 1.64, tokens/s = 77687 (32907 source, 44780 target) ; Learning rate = 0.000307 ; Loss = 1.633822\n",
      "2024-12-13 01:24:09.575000: I runner.py:310] Step = 82800 ; steps/s = 1.63, tokens/s = 78384 (33172 source, 45212 target) ; Learning rate = 0.000307 ; Loss = 1.653986\n",
      "2024-12-13 01:25:10.882000: I runner.py:310] Step = 82900 ; steps/s = 1.63, tokens/s = 78405 (33195 source, 45210 target) ; Learning rate = 0.000307 ; Loss = 1.656227\n",
      "2024-12-13 01:26:12.224000: I runner.py:310] Step = 83000 ; steps/s = 1.63, tokens/s = 78419 (33222 source, 45197 target) ; Learning rate = 0.000307 ; Loss = 1.657870\n",
      "2024-12-13 01:27:13.167000: I runner.py:310] Step = 83100 ; steps/s = 1.64, tokens/s = 77571 (32839 source, 44732 target) ; Learning rate = 0.000307 ; Loss = 1.670215\n",
      "2024-12-13 01:28:14.427000: I runner.py:310] Step = 83200 ; steps/s = 1.63, tokens/s = 78516 (33278 source, 45238 target) ; Learning rate = 0.000306 ; Loss = 1.640953\n",
      "2024-12-13 01:29:15.709000: I runner.py:310] Step = 83300 ; steps/s = 1.63, tokens/s = 78455 (33216 source, 45239 target) ; Learning rate = 0.000306 ; Loss = 1.650224\n",
      "2024-12-13 01:30:16.613000: I runner.py:310] Step = 83400 ; steps/s = 1.64, tokens/s = 77619 (32853 source, 44766 target) ; Learning rate = 0.000306 ; Loss = 1.649692\n",
      "2024-12-13 01:31:17.940000: I runner.py:310] Step = 83500 ; steps/s = 1.63, tokens/s = 78393 (33189 source, 45204 target) ; Learning rate = 0.000306 ; Loss = 1.648526\n",
      "2024-12-13 01:32:19.258000: I runner.py:310] Step = 83600 ; steps/s = 1.63, tokens/s = 78430 (33226 source, 45204 target) ; Learning rate = 0.000306 ; Loss = 1.641562\n",
      "2024-12-13 01:33:20.577000: I runner.py:310] Step = 83700 ; steps/s = 1.63, tokens/s = 78400 (33188 source, 45212 target) ; Learning rate = 0.000306 ; Loss = 1.637821\n",
      "2024-12-13 01:34:21.434000: I runner.py:310] Step = 83800 ; steps/s = 1.64, tokens/s = 77700 (32912 source, 44788 target) ; Learning rate = 0.000305 ; Loss = 1.631858\n",
      "2024-12-13 01:35:22.673000: I runner.py:310] Step = 83900 ; steps/s = 1.63, tokens/s = 78514 (33242 source, 45272 target) ; Learning rate = 0.000305 ; Loss = 1.652046\n",
      "2024-12-13 01:36:23.997000: I runner.py:310] Step = 84000 ; steps/s = 1.63, tokens/s = 78394 (33198 source, 45196 target) ; Learning rate = 0.000305 ; Loss = 1.662431\n",
      "2024-12-13 01:37:25.304000: I runner.py:310] Step = 84100 ; steps/s = 1.63, tokens/s = 78438 (33213 source, 45225 target) ; Learning rate = 0.000305 ; Loss = 1.665217\n",
      "2024-12-13 01:38:26.163000: I runner.py:310] Step = 84200 ; steps/s = 1.64, tokens/s = 77662 (32871 source, 44791 target) ; Learning rate = 0.000305 ; Loss = 1.666900\n",
      "2024-12-13 01:39:27.423000: I runner.py:310] Step = 84300 ; steps/s = 1.63, tokens/s = 78519 (33266 source, 45253 target) ; Learning rate = 0.000304 ; Loss = 1.635406\n",
      "2024-12-13 01:40:28.700000: I runner.py:310] Step = 84400 ; steps/s = 1.63, tokens/s = 78453 (33215 source, 45238 target) ; Learning rate = 0.000304 ; Loss = 1.644170\n",
      "2024-12-13 01:41:29.632000: I runner.py:310] Step = 84500 ; steps/s = 1.64, tokens/s = 77603 (32868 source, 44735 target) ; Learning rate = 0.000304 ; Loss = 1.651639\n",
      "2024-12-13 01:42:30.898000: I runner.py:310] Step = 84600 ; steps/s = 1.63, tokens/s = 78480 (33228 source, 45252 target) ; Learning rate = 0.000304 ; Loss = 1.649226\n",
      "2024-12-13 01:43:32.132000: I runner.py:310] Step = 84700 ; steps/s = 1.63, tokens/s = 78518 (33247 source, 45271 target) ; Learning rate = 0.000304 ; Loss = 1.644800\n",
      "2024-12-13 01:44:33.447000: I runner.py:310] Step = 84800 ; steps/s = 1.63, tokens/s = 78398 (33189 source, 45209 target) ; Learning rate = 0.000304 ; Loss = 1.649454\n",
      "2024-12-13 01:45:34.386000: I runner.py:310] Step = 84900 ; steps/s = 1.64, tokens/s = 77632 (32895 source, 44737 target) ; Learning rate = 0.000303 ; Loss = 1.622260\n",
      "2024-12-13 01:46:35.764000: I runner.py:310] Step = 85000 ; steps/s = 1.63, tokens/s = 78314 (33146 source, 45168 target) ; Learning rate = 0.000303 ; Loss = 1.655895\n",
      "2024-12-13 01:46:35.766000: I training.py:192] Running evaluation for step 85000\n",
      "2024-12-13 01:48:40.743000: I training.py:192] Evaluation result for step 85000: loss = 2.689749 ; perplexity = 14.727972\n",
      "2024-12-13 01:49:41.948000: I runner.py:310] Step = 85100 ; steps/s = 1.63, tokens/s = 78577 (33275 source, 45302 target) ; Learning rate = 0.000303 ; Loss = 1.647909\n",
      "2024-12-13 01:50:42.858000: I runner.py:310] Step = 85200 ; steps/s = 1.64, tokens/s = 77628 (32874 source, 44754 target) ; Learning rate = 0.000303 ; Loss = 1.644076\n",
      "2024-12-13 01:51:44.174000: I runner.py:310] Step = 85300 ; steps/s = 1.63, tokens/s = 78402 (33186 source, 45216 target) ; Learning rate = 0.000303 ; Loss = 1.633561\n",
      "2024-12-13 01:52:45.531000: I runner.py:310] Step = 85400 ; steps/s = 1.63, tokens/s = 78394 (33217 source, 45177 target) ; Learning rate = 0.000302 ; Loss = 1.649003\n",
      "2024-12-13 01:53:46.789000: I runner.py:310] Step = 85500 ; steps/s = 1.63, tokens/s = 78475 (33219 source, 45256 target) ; Learning rate = 0.000302 ; Loss = 1.649173\n",
      "2024-12-13 01:54:47.733000: I runner.py:310] Step = 85600 ; steps/s = 1.64, tokens/s = 77596 (32872 source, 44724 target) ; Learning rate = 0.000302 ; Loss = 1.626951\n",
      "2024-12-13 01:55:49.088000: I runner.py:310] Step = 85700 ; steps/s = 1.63, tokens/s = 78385 (33199 source, 45186 target) ; Learning rate = 0.000302 ; Loss = 1.645244\n",
      "2024-12-13 01:56:50.434000: I runner.py:310] Step = 85800 ; steps/s = 1.63, tokens/s = 78355 (33161 source, 45194 target) ; Learning rate = 0.000302 ; Loss = 1.659151\n",
      "2024-12-13 01:57:51.765000: I runner.py:310] Step = 85900 ; steps/s = 1.63, tokens/s = 78228 (33131 source, 45097 target) ; Learning rate = 0.000302 ; Loss = 1.654028\n",
      "2024-12-13 01:58:52.742000: I runner.py:310] Step = 86000 ; steps/s = 1.64, tokens/s = 77737 (32936 source, 44801 target) ; Learning rate = 0.000301 ; Loss = 1.627979\n",
      "2024-12-13 01:59:54.056000: I runner.py:310] Step = 86100 ; steps/s = 1.63, tokens/s = 78387 (33172 source, 45215 target) ; Learning rate = 0.000301 ; Loss = 1.645110\n",
      "2024-12-13 02:00:55.339000: I runner.py:310] Step = 86200 ; steps/s = 1.63, tokens/s = 78464 (33226 source, 45238 target) ; Learning rate = 0.000301 ; Loss = 1.651818\n",
      "2024-12-13 02:01:56.218000: I runner.py:310] Step = 86300 ; steps/s = 1.64, tokens/s = 77670 (32898 source, 44772 target) ; Learning rate = 0.000301 ; Loss = 1.625236\n",
      "2024-12-13 02:02:57.536000: I runner.py:310] Step = 86400 ; steps/s = 1.63, tokens/s = 78448 (33234 source, 45214 target) ; Learning rate = 0.000301 ; Loss = 1.643753\n",
      "2024-12-13 02:03:58.857000: I runner.py:310] Step = 86500 ; steps/s = 1.63, tokens/s = 78410 (33199 source, 45211 target) ; Learning rate = 0.000301 ; Loss = 1.652585\n",
      "2024-12-13 02:05:00.123000: I runner.py:310] Step = 86600 ; steps/s = 1.63, tokens/s = 78452 (33206 source, 45246 target) ; Learning rate = 0.000300 ; Loss = 1.655669\n",
      "2024-12-13 02:06:01.119000: I runner.py:310] Step = 86700 ; steps/s = 1.64, tokens/s = 77527 (32839 source, 44688 target) ; Learning rate = 0.000300 ; Loss = 1.631790\n",
      "2024-12-13 02:07:02.428000: I runner.py:310] Step = 86800 ; steps/s = 1.63, tokens/s = 78409 (33193 source, 45216 target) ; Learning rate = 0.000300 ; Loss = 1.646481\n",
      "2024-12-13 02:08:03.780000: I runner.py:310] Step = 86900 ; steps/s = 1.63, tokens/s = 78393 (33195 source, 45198 target) ; Learning rate = 0.000300 ; Loss = 1.657645\n",
      "2024-12-13 02:09:04.690000: I runner.py:310] Step = 87000 ; steps/s = 1.64, tokens/s = 77614 (32865 source, 44749 target) ; Learning rate = 0.000300 ; Loss = 1.637859\n",
      "2024-12-13 02:10:06.037000: I runner.py:310] Step = 87100 ; steps/s = 1.63, tokens/s = 78367 (33180 source, 45187 target) ; Learning rate = 0.000299 ; Loss = 1.646019\n",
      "2024-12-13 02:11:07.368000: I runner.py:310] Step = 87200 ; steps/s = 1.63, tokens/s = 78372 (33176 source, 45196 target) ; Learning rate = 0.000299 ; Loss = 1.653194\n",
      "2024-12-13 02:12:08.669000: I runner.py:310] Step = 87300 ; steps/s = 1.63, tokens/s = 78431 (33209 source, 45222 target) ; Learning rate = 0.000299 ; Loss = 1.659161\n",
      "2024-12-13 02:13:09.613000: I runner.py:310] Step = 87400 ; steps/s = 1.64, tokens/s = 77594 (32868 source, 44726 target) ; Learning rate = 0.000299 ; Loss = 1.628874\n",
      "2024-12-13 02:14:10.953000: I runner.py:310] Step = 87500 ; steps/s = 1.63, tokens/s = 78399 (33199 source, 45200 target) ; Learning rate = 0.000299 ; Loss = 1.642107\n",
      "2024-12-13 02:15:12.263000: I runner.py:310] Step = 87600 ; steps/s = 1.63, tokens/s = 78430 (33217 source, 45213 target) ; Learning rate = 0.000299 ; Loss = 1.646467\n",
      "2024-12-13 02:16:13.158000: I runner.py:310] Step = 87700 ; steps/s = 1.64, tokens/s = 77643 (32876 source, 44767 target) ; Learning rate = 0.000298 ; Loss = 1.638441\n",
      "2024-12-13 02:17:14.559000: I runner.py:310] Step = 87800 ; steps/s = 1.63, tokens/s = 78306 (33149 source, 45157 target) ; Learning rate = 0.000298 ; Loss = 1.635943\n",
      "2024-12-13 02:18:15.852000: I runner.py:310] Step = 87900 ; steps/s = 1.63, tokens/s = 78453 (33234 source, 45219 target) ; Learning rate = 0.000298 ; Loss = 1.641216\n",
      "2024-12-13 02:19:17.162000: I runner.py:310] Step = 88000 ; steps/s = 1.63, tokens/s = 78427 (33209 source, 45218 target) ; Learning rate = 0.000298 ; Loss = 1.658330\n",
      "2024-12-13 02:20:18.105000: I runner.py:310] Step = 88100 ; steps/s = 1.64, tokens/s = 77585 (32852 source, 44733 target) ; Learning rate = 0.000298 ; Loss = 1.628461\n",
      "2024-12-13 02:21:19.418000: I runner.py:310] Step = 88200 ; steps/s = 1.63, tokens/s = 78425 (33214 source, 45211 target) ; Learning rate = 0.000298 ; Loss = 1.646103\n",
      "2024-12-13 02:22:20.708000: I runner.py:310] Step = 88300 ; steps/s = 1.63, tokens/s = 78467 (33243 source, 45224 target) ; Learning rate = 0.000297 ; Loss = 1.640559\n",
      "2024-12-13 02:23:22.050000: I runner.py:310] Step = 88400 ; steps/s = 1.63, tokens/s = 78350 (33155 source, 45195 target) ; Learning rate = 0.000297 ; Loss = 1.643282\n",
      "2024-12-13 02:24:22.890000: I runner.py:310] Step = 88500 ; steps/s = 1.64, tokens/s = 77717 (32909 source, 44808 target) ; Learning rate = 0.000297 ; Loss = 1.627811\n",
      "2024-12-13 02:25:24.170000: I runner.py:310] Step = 88600 ; steps/s = 1.63, tokens/s = 78463 (33234 source, 45229 target) ; Learning rate = 0.000297 ; Loss = 1.645972\n",
      "2024-12-13 02:26:25.515000: I runner.py:310] Step = 88700 ; steps/s = 1.63, tokens/s = 78380 (33178 source, 45202 target) ; Learning rate = 0.000297 ; Loss = 1.658150\n",
      "2024-12-13 02:27:26.532000: I runner.py:310] Step = 88800 ; steps/s = 1.64, tokens/s = 77494 (32826 source, 44668 target) ; Learning rate = 0.000297 ; Loss = 1.643319\n",
      "2024-12-13 02:28:27.832000: I runner.py:310] Step = 88900 ; steps/s = 1.63, tokens/s = 78432 (33210 source, 45222 target) ; Learning rate = 0.000296 ; Loss = 1.630270\n",
      "2024-12-13 02:29:29.161000: I runner.py:310] Step = 89000 ; steps/s = 1.63, tokens/s = 78393 (33190 source, 45203 target) ; Learning rate = 0.000296 ; Loss = 1.631934\n",
      "2024-12-13 02:30:30.559000: I runner.py:310] Step = 89100 ; steps/s = 1.63, tokens/s = 78313 (33158 source, 45155 target) ; Learning rate = 0.000296 ; Loss = 1.639906\n",
      "2024-12-13 02:31:31.494000: I runner.py:310] Step = 89200 ; steps/s = 1.64, tokens/s = 77597 (32858 source, 44739 target) ; Learning rate = 0.000296 ; Loss = 1.662563\n",
      "2024-12-13 02:32:32.786000: I runner.py:310] Step = 89300 ; steps/s = 1.63, tokens/s = 78438 (33208 source, 45230 target) ; Learning rate = 0.000296 ; Loss = 1.634161\n",
      "2024-12-13 02:33:34.161000: I runner.py:310] Step = 89400 ; steps/s = 1.63, tokens/s = 78355 (33197 source, 45158 target) ; Learning rate = 0.000296 ; Loss = 1.632082\n",
      "2024-12-13 02:34:35.115000: I runner.py:310] Step = 89500 ; steps/s = 1.64, tokens/s = 77572 (32842 source, 44730 target) ; Learning rate = 0.000295 ; Loss = 1.638884\n",
      "2024-12-13 02:35:36.415000: I runner.py:310] Step = 89600 ; steps/s = 1.63, tokens/s = 78422 (33194 source, 45228 target) ; Learning rate = 0.000295 ; Loss = 1.651644\n",
      "2024-12-13 02:36:37.780000: I runner.py:310] Step = 89700 ; steps/s = 1.63, tokens/s = 78341 (33173 source, 45168 target) ; Learning rate = 0.000295 ; Loss = 1.643066\n",
      "2024-12-13 02:37:39.130000: I runner.py:310] Step = 89800 ; steps/s = 1.63, tokens/s = 78385 (33196 source, 45189 target) ; Learning rate = 0.000295 ; Loss = 1.647839\n",
      "2024-12-13 02:38:40.084000: I runner.py:310] Step = 89900 ; steps/s = 1.64, tokens/s = 77547 (32836 source, 44711 target) ; Learning rate = 0.000295 ; Loss = 1.657357\n",
      "2024-12-13 02:39:41.446000: I runner.py:310] Step = 90000 ; steps/s = 1.63, tokens/s = 78377 (33192 source, 45185 target) ; Learning rate = 0.000295 ; Loss = 1.628843\n",
      "2024-12-13 02:39:44.077000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-90000\n",
      "2024-12-13 02:39:44.077000: I training.py:192] Running evaluation for step 90000\n",
      "2024-12-13 02:41:48.799000: I training.py:192] Evaluation result for step 90000: loss = 2.704330 ; perplexity = 14.944300\n",
      "2024-12-13 02:42:49.982000: I runner.py:310] Step = 90100 ; steps/s = 1.64, tokens/s = 78594 (33286 source, 45308 target) ; Learning rate = 0.000294 ; Loss = 1.643369\n",
      "2024-12-13 02:43:50.906000: I runner.py:310] Step = 90200 ; steps/s = 1.64, tokens/s = 77636 (32879 source, 44757 target) ; Learning rate = 0.000294 ; Loss = 1.640210\n",
      "2024-12-13 02:44:52.308000: I runner.py:310] Step = 90300 ; steps/s = 1.63, tokens/s = 78317 (33166 source, 45151 target) ; Learning rate = 0.000294 ; Loss = 1.643453\n",
      "2024-12-13 02:45:53.665000: I runner.py:310] Step = 90400 ; steps/s = 1.63, tokens/s = 78374 (33192 source, 45182 target) ; Learning rate = 0.000294 ; Loss = 1.637548\n",
      "2024-12-13 02:46:54.998000: I runner.py:310] Step = 90500 ; steps/s = 1.63, tokens/s = 78391 (33191 source, 45200 target) ; Learning rate = 0.000294 ; Loss = 1.637377\n",
      "2024-12-13 02:47:55.873000: I runner.py:310] Step = 90600 ; steps/s = 1.64, tokens/s = 77673 (32901 source, 44772 target) ; Learning rate = 0.000294 ; Loss = 1.642848\n",
      "2024-12-13 02:48:57.193000: I runner.py:310] Step = 90700 ; steps/s = 1.63, tokens/s = 78386 (33173 source, 45213 target) ; Learning rate = 0.000293 ; Loss = 1.632072\n",
      "2024-12-13 02:49:58.571000: I runner.py:310] Step = 90800 ; steps/s = 1.63, tokens/s = 78345 (33183 source, 45162 target) ; Learning rate = 0.000293 ; Loss = 1.641599\n",
      "2024-12-13 02:50:59.944000: I runner.py:310] Step = 90900 ; steps/s = 1.63, tokens/s = 78350 (33173 source, 45177 target) ; Learning rate = 0.000293 ; Loss = 1.641497\n",
      "2024-12-13 02:52:00.937000: I runner.py:310] Step = 91000 ; steps/s = 1.64, tokens/s = 77525 (32827 source, 44698 target) ; Learning rate = 0.000293 ; Loss = 1.643838\n",
      "2024-12-13 02:53:02.333000: I runner.py:310] Step = 91100 ; steps/s = 1.63, tokens/s = 78328 (33173 source, 45155 target) ; Learning rate = 0.000293 ; Loss = 1.631830\n",
      "2024-12-13 02:54:03.684000: I runner.py:310] Step = 91200 ; steps/s = 1.63, tokens/s = 78373 (33192 source, 45181 target) ; Learning rate = 0.000293 ; Loss = 1.637710\n",
      "2024-12-13 02:55:04.550000: I runner.py:310] Step = 91300 ; steps/s = 1.64, tokens/s = 77663 (32880 source, 44783 target) ; Learning rate = 0.000293 ; Loss = 1.640051\n",
      "2024-12-13 02:56:05.906000: I runner.py:310] Step = 91400 ; steps/s = 1.63, tokens/s = 78361 (33182 source, 45179 target) ; Learning rate = 0.000292 ; Loss = 1.636682\n",
      "2024-12-13 02:57:07.173000: I runner.py:310] Step = 91500 ; steps/s = 1.63, tokens/s = 78478 (33231 source, 45247 target) ; Learning rate = 0.000292 ; Loss = 1.631287\n",
      "2024-12-13 02:58:08.536000: I runner.py:310] Step = 91600 ; steps/s = 1.63, tokens/s = 78357 (33181 source, 45176 target) ; Learning rate = 0.000292 ; Loss = 1.629445\n",
      "2024-12-13 02:59:09.525000: I runner.py:310] Step = 91700 ; steps/s = 1.64, tokens/s = 77541 (32847 source, 44694 target) ; Learning rate = 0.000292 ; Loss = 1.622403\n",
      "2024-12-13 03:00:10.801000: I runner.py:310] Step = 91800 ; steps/s = 1.63, tokens/s = 78463 (33225 source, 45238 target) ; Learning rate = 0.000292 ; Loss = 1.647436\n",
      "2024-12-13 03:01:12.120000: I runner.py:310] Step = 91900 ; steps/s = 1.63, tokens/s = 78395 (33179 source, 45216 target) ; Learning rate = 0.000292 ; Loss = 1.640065\n",
      "2024-12-13 03:02:13.032000: I runner.py:310] Step = 92000 ; steps/s = 1.64, tokens/s = 77636 (32893 source, 44743 target) ; Learning rate = 0.000291 ; Loss = 1.628340\n",
      "2024-12-13 03:03:14.314000: I runner.py:310] Step = 92100 ; steps/s = 1.63, tokens/s = 78453 (33211 source, 45242 target) ; Learning rate = 0.000291 ; Loss = 1.631243\n",
      "2024-12-13 03:04:15.643000: I runner.py:310] Step = 92200 ; steps/s = 1.63, tokens/s = 78417 (33204 source, 45213 target) ; Learning rate = 0.000291 ; Loss = 1.641217\n",
      "2024-12-13 03:05:16.940000: I runner.py:310] Step = 92300 ; steps/s = 1.63, tokens/s = 78430 (33214 source, 45216 target) ; Learning rate = 0.000291 ; Loss = 1.644986\n",
      "2024-12-13 03:06:17.872000: I runner.py:310] Step = 92400 ; steps/s = 1.64, tokens/s = 77616 (32876 source, 44740 target) ; Learning rate = 0.000291 ; Loss = 1.622611\n",
      "2024-12-13 03:07:19.194000: I runner.py:310] Step = 92500 ; steps/s = 1.63, tokens/s = 78385 (33193 source, 45192 target) ; Learning rate = 0.000291 ; Loss = 1.634721\n",
      "2024-12-13 03:08:20.455000: I runner.py:310] Step = 92600 ; steps/s = 1.63, tokens/s = 78501 (33244 source, 45257 target) ; Learning rate = 0.000290 ; Loss = 1.646453\n",
      "2024-12-13 03:09:21.796000: I runner.py:310] Step = 92700 ; steps/s = 1.63, tokens/s = 78376 (33175 source, 45201 target) ; Learning rate = 0.000290 ; Loss = 1.647837\n",
      "2024-12-13 03:10:22.656000: I runner.py:310] Step = 92800 ; steps/s = 1.64, tokens/s = 77710 (32914 source, 44796 target) ; Learning rate = 0.000290 ; Loss = 1.631152\n",
      "2024-12-13 03:11:23.960000: I runner.py:310] Step = 92900 ; steps/s = 1.63, tokens/s = 78434 (33223 source, 45211 target) ; Learning rate = 0.000290 ; Loss = 1.643256\n",
      "2024-12-13 03:12:25.287000: I runner.py:310] Step = 93000 ; steps/s = 1.63, tokens/s = 78390 (33185 source, 45205 target) ; Learning rate = 0.000290 ; Loss = 1.641899\n",
      "2024-12-13 03:13:26.263000: I runner.py:310] Step = 93100 ; steps/s = 1.64, tokens/s = 77533 (32832 source, 44701 target) ; Learning rate = 0.000290 ; Loss = 1.628342\n",
      "2024-12-13 03:14:27.545000: I runner.py:310] Step = 93200 ; steps/s = 1.63, tokens/s = 78458 (33225 source, 45233 target) ; Learning rate = 0.000290 ; Loss = 1.639827\n",
      "2024-12-13 03:15:28.891000: I runner.py:310] Step = 93300 ; steps/s = 1.63, tokens/s = 78392 (33196 source, 45196 target) ; Learning rate = 0.000289 ; Loss = 1.648284\n",
      "2024-12-13 03:16:30.295000: I runner.py:310] Step = 93400 ; steps/s = 1.63, tokens/s = 78298 (33150 source, 45148 target) ; Learning rate = 0.000289 ; Loss = 1.649958\n",
      "2024-12-13 03:17:31.169000: I runner.py:310] Step = 93500 ; steps/s = 1.64, tokens/s = 77664 (32886 source, 44778 target) ; Learning rate = 0.000289 ; Loss = 1.649323\n",
      "2024-12-13 03:18:32.455000: I runner.py:310] Step = 93600 ; steps/s = 1.63, tokens/s = 78461 (33235 source, 45226 target) ; Learning rate = 0.000289 ; Loss = 1.632206\n",
      "2024-12-13 03:19:33.753000: I runner.py:310] Step = 93700 ; steps/s = 1.63, tokens/s = 78458 (33223 source, 45235 target) ; Learning rate = 0.000289 ; Loss = 1.650827\n",
      "2024-12-13 03:20:34.710000: I runner.py:310] Step = 93800 ; steps/s = 1.64, tokens/s = 77558 (32835 source, 44723 target) ; Learning rate = 0.000289 ; Loss = 1.638500\n",
      "2024-12-13 03:21:35.984000: I runner.py:310] Step = 93900 ; steps/s = 1.63, tokens/s = 78431 (33185 source, 45246 target) ; Learning rate = 0.000288 ; Loss = 1.628206\n",
      "2024-12-13 03:22:37.343000: I runner.py:310] Step = 94000 ; steps/s = 1.63, tokens/s = 78406 (33220 source, 45186 target) ; Learning rate = 0.000288 ; Loss = 1.640796\n",
      "2024-12-13 03:23:38.690000: I runner.py:310] Step = 94100 ; steps/s = 1.63, tokens/s = 78356 (33174 source, 45182 target) ; Learning rate = 0.000288 ; Loss = 1.639727\n",
      "2024-12-13 03:24:39.639000: I runner.py:310] Step = 94200 ; steps/s = 1.64, tokens/s = 77579 (32857 source, 44722 target) ; Learning rate = 0.000288 ; Loss = 1.620164\n",
      "2024-12-13 03:25:40.946000: I runner.py:310] Step = 94300 ; steps/s = 1.63, tokens/s = 78433 (33218 source, 45215 target) ; Learning rate = 0.000288 ; Loss = 1.639910\n",
      "2024-12-13 03:26:42.320000: I runner.py:310] Step = 94400 ; steps/s = 1.63, tokens/s = 78342 (33173 source, 45169 target) ; Learning rate = 0.000288 ; Loss = 1.637823\n",
      "2024-12-13 03:27:43.253000: I runner.py:310] Step = 94500 ; steps/s = 1.64, tokens/s = 77604 (32866 source, 44738 target) ; Learning rate = 0.000288 ; Loss = 1.629816\n",
      "2024-12-13 03:28:44.615000: I runner.py:310] Step = 94600 ; steps/s = 1.63, tokens/s = 78331 (33143 source, 45188 target) ; Learning rate = 0.000287 ; Loss = 1.634929\n",
      "2024-12-13 03:29:45.946000: I runner.py:310] Step = 94700 ; steps/s = 1.63, tokens/s = 78418 (33210 source, 45208 target) ; Learning rate = 0.000287 ; Loss = 1.626992\n",
      "2024-12-13 03:30:47.275000: I runner.py:310] Step = 94800 ; steps/s = 1.63, tokens/s = 78423 (33229 source, 45194 target) ; Learning rate = 0.000287 ; Loss = 1.638554\n",
      "2024-12-13 03:31:48.138000: I runner.py:310] Step = 94900 ; steps/s = 1.64, tokens/s = 77678 (32894 source, 44784 target) ; Learning rate = 0.000287 ; Loss = 1.624152\n",
      "2024-12-13 03:32:49.497000: I runner.py:310] Step = 95000 ; steps/s = 1.63, tokens/s = 78365 (33184 source, 45181 target) ; Learning rate = 0.000287 ; Loss = 1.638339\n",
      "2024-12-13 03:32:49.500000: I training.py:192] Running evaluation for step 95000\n",
      "2024-12-13 03:34:55.424000: I training.py:192] Evaluation result for step 95000: loss = 2.710385 ; perplexity = 15.035057\n",
      "2024-12-13 03:35:56.564000: I runner.py:310] Step = 95100 ; steps/s = 1.64, tokens/s = 78661 (33312 source, 45349 target) ; Learning rate = 0.000287 ; Loss = 1.640442\n",
      "2024-12-13 03:36:57.803000: I runner.py:310] Step = 95200 ; steps/s = 1.63, tokens/s = 78494 (33225 source, 45269 target) ; Learning rate = 0.000286 ; Loss = 1.641661\n",
      "2024-12-13 03:37:58.754000: I runner.py:310] Step = 95300 ; steps/s = 1.64, tokens/s = 77550 (32826 source, 44724 target) ; Learning rate = 0.000286 ; Loss = 1.625063\n",
      "2024-12-13 03:39:00.120000: I runner.py:310] Step = 95400 ; steps/s = 1.63, tokens/s = 78353 (33176 source, 45177 target) ; Learning rate = 0.000286 ; Loss = 1.643036\n",
      "2024-12-13 03:40:01.438000: I runner.py:310] Step = 95500 ; steps/s = 1.63, tokens/s = 78432 (33230 source, 45202 target) ; Learning rate = 0.000286 ; Loss = 1.644374\n",
      "2024-12-13 03:41:02.357000: I runner.py:310] Step = 95600 ; steps/s = 1.64, tokens/s = 77632 (32877 source, 44755 target) ; Learning rate = 0.000286 ; Loss = 1.633424\n",
      "2024-12-13 03:42:03.650000: I runner.py:310] Step = 95700 ; steps/s = 1.63, tokens/s = 78442 (33218 source, 45224 target) ; Learning rate = 0.000286 ; Loss = 1.626920\n",
      "2024-12-13 03:43:04.971000: I runner.py:310] Step = 95800 ; steps/s = 1.63, tokens/s = 78414 (33199 source, 45215 target) ; Learning rate = 0.000286 ; Loss = 1.632564\n",
      "2024-12-13 03:44:06.305000: I runner.py:310] Step = 95900 ; steps/s = 1.63, tokens/s = 78398 (33204 source, 45194 target) ; Learning rate = 0.000285 ; Loss = 1.641476\n",
      "2024-12-13 03:45:07.218000: I runner.py:310] Step = 96000 ; steps/s = 1.64, tokens/s = 77590 (32842 source, 44748 target) ; Learning rate = 0.000285 ; Loss = 1.645136\n",
      "2024-12-13 03:46:08.496000: I runner.py:310] Step = 96100 ; steps/s = 1.63, tokens/s = 78493 (33243 source, 45250 target) ; Learning rate = 0.000285 ; Loss = 1.624382\n",
      "2024-12-13 03:47:09.840000: I runner.py:310] Step = 96200 ; steps/s = 1.63, tokens/s = 78376 (33191 source, 45185 target) ; Learning rate = 0.000285 ; Loss = 1.641244\n",
      "2024-12-13 03:48:10.763000: I runner.py:310] Step = 96300 ; steps/s = 1.64, tokens/s = 77616 (32873 source, 44743 target) ; Learning rate = 0.000285 ; Loss = 1.628517\n",
      "2024-12-13 03:49:12.060000: I runner.py:310] Step = 96400 ; steps/s = 1.63, tokens/s = 78479 (33255 source, 45224 target) ; Learning rate = 0.000285 ; Loss = 1.625244\n",
      "2024-12-13 03:50:13.322000: I runner.py:310] Step = 96500 ; steps/s = 1.63, tokens/s = 78441 (33197 source, 45244 target) ; Learning rate = 0.000285 ; Loss = 1.644488\n",
      "2024-12-13 03:51:14.595000: I runner.py:310] Step = 96600 ; steps/s = 1.63, tokens/s = 78469 (33220 source, 45249 target) ; Learning rate = 0.000284 ; Loss = 1.639510\n",
      "2024-12-13 03:52:15.595000: I runner.py:310] Step = 96700 ; steps/s = 1.64, tokens/s = 77530 (32836 source, 44694 target) ; Learning rate = 0.000284 ; Loss = 1.641697\n",
      "2024-12-13 03:53:16.932000: I runner.py:310] Step = 96800 ; steps/s = 1.63, tokens/s = 78348 (33156 source, 45192 target) ; Learning rate = 0.000284 ; Loss = 1.621338\n",
      "2024-12-13 03:54:18.255000: I runner.py:310] Step = 96900 ; steps/s = 1.63, tokens/s = 78402 (33199 source, 45203 target) ; Learning rate = 0.000284 ; Loss = 1.631718\n",
      "2024-12-13 03:55:19.169000: I runner.py:310] Step = 97000 ; steps/s = 1.64, tokens/s = 77650 (32896 source, 44754 target) ; Learning rate = 0.000284 ; Loss = 1.626778\n",
      "2024-12-13 03:56:20.621000: I runner.py:310] Step = 97100 ; steps/s = 1.63, tokens/s = 78224 (33111 source, 45113 target) ; Learning rate = 0.000284 ; Loss = 1.639310\n",
      "2024-12-13 03:57:21.961000: I runner.py:310] Step = 97200 ; steps/s = 1.63, tokens/s = 78382 (33184 source, 45198 target) ; Learning rate = 0.000284 ; Loss = 1.627270\n",
      "2024-12-13 03:58:23.277000: I runner.py:310] Step = 97300 ; steps/s = 1.63, tokens/s = 78445 (33240 source, 45205 target) ; Learning rate = 0.000283 ; Loss = 1.627602\n",
      "2024-12-13 03:59:24.213000: I runner.py:310] Step = 97400 ; steps/s = 1.64, tokens/s = 77577 (32844 source, 44733 target) ; Learning rate = 0.000283 ; Loss = 1.650269\n",
      "2024-12-13 04:00:25.449000: I runner.py:310] Step = 97500 ; steps/s = 1.63, tokens/s = 78525 (33255 source, 45270 target) ; Learning rate = 0.000283 ; Loss = 1.627103\n",
      "2024-12-13 04:01:26.808000: I runner.py:310] Step = 97600 ; steps/s = 1.63, tokens/s = 78366 (33187 source, 45179 target) ; Learning rate = 0.000283 ; Loss = 1.643603\n",
      "2024-12-13 04:02:28.182000: I runner.py:310] Step = 97700 ; steps/s = 1.63, tokens/s = 78349 (33179 source, 45170 target) ; Learning rate = 0.000283 ; Loss = 1.629022\n",
      "2024-12-13 04:03:29.113000: I runner.py:310] Step = 97800 ; steps/s = 1.64, tokens/s = 77583 (32843 source, 44740 target) ; Learning rate = 0.000283 ; Loss = 1.638758\n",
      "2024-12-13 04:04:30.420000: I runner.py:310] Step = 97900 ; steps/s = 1.63, tokens/s = 78447 (33224 source, 45223 target) ; Learning rate = 0.000282 ; Loss = 1.625310\n",
      "2024-12-13 04:05:31.746000: I runner.py:310] Step = 98000 ; steps/s = 1.63, tokens/s = 78409 (33212 source, 45197 target) ; Learning rate = 0.000282 ; Loss = 1.626500\n",
      "2024-12-13 04:06:32.718000: I runner.py:310] Step = 98100 ; steps/s = 1.64, tokens/s = 77524 (32818 source, 44706 target) ; Learning rate = 0.000282 ; Loss = 1.626998\n",
      "2024-12-13 04:07:34.014000: I runner.py:310] Step = 98200 ; steps/s = 1.63, tokens/s = 78426 (33199 source, 45227 target) ; Learning rate = 0.000282 ; Loss = 1.633003\n",
      "2024-12-13 04:08:35.423000: I runner.py:310] Step = 98300 ; steps/s = 1.63, tokens/s = 78298 (33166 source, 45132 target) ; Learning rate = 0.000282 ; Loss = 1.629240\n",
      "2024-12-13 04:09:36.732000: I runner.py:310] Step = 98400 ; steps/s = 1.63, tokens/s = 78442 (33213 source, 45229 target) ; Learning rate = 0.000282 ; Loss = 1.629597\n",
      "2024-12-13 04:10:37.621000: I runner.py:310] Step = 98500 ; steps/s = 1.64, tokens/s = 77648 (32873 source, 44775 target) ; Learning rate = 0.000282 ; Loss = 1.648402\n",
      "2024-12-13 04:11:38.907000: I runner.py:310] Step = 98600 ; steps/s = 1.63, tokens/s = 78462 (33235 source, 45227 target) ; Learning rate = 0.000281 ; Loss = 1.631068\n",
      "2024-12-13 04:12:40.169000: I runner.py:310] Step = 98700 ; steps/s = 1.63, tokens/s = 78484 (33230 source, 45254 target) ; Learning rate = 0.000281 ; Loss = 1.630283\n",
      "2024-12-13 04:13:41.120000: I runner.py:310] Step = 98800 ; steps/s = 1.64, tokens/s = 77581 (32857 source, 44724 target) ; Learning rate = 0.000281 ; Loss = 1.623658\n",
      "2024-12-13 04:14:42.479000: I runner.py:310] Step = 98900 ; steps/s = 1.63, tokens/s = 78370 (33199 source, 45171 target) ; Learning rate = 0.000281 ; Loss = 1.627726\n",
      "2024-12-13 04:15:43.818000: I runner.py:310] Step = 99000 ; steps/s = 1.63, tokens/s = 78369 (33173 source, 45196 target) ; Learning rate = 0.000281 ; Loss = 1.629644\n",
      "2024-12-13 04:16:45.193000: I runner.py:310] Step = 99100 ; steps/s = 1.63, tokens/s = 78342 (33170 source, 45172 target) ; Learning rate = 0.000281 ; Loss = 1.635563\n",
      "2024-12-13 04:17:46.139000: I runner.py:310] Step = 99200 ; steps/s = 1.64, tokens/s = 77590 (32861 source, 44729 target) ; Learning rate = 0.000281 ; Loss = 1.643817\n",
      "2024-12-13 04:18:47.531000: I runner.py:310] Step = 99300 ; steps/s = 1.63, tokens/s = 78292 (33135 source, 45157 target) ; Learning rate = 0.000280 ; Loss = 1.630120\n",
      "2024-12-13 04:19:48.838000: I runner.py:310] Step = 99400 ; steps/s = 1.63, tokens/s = 78453 (33228 source, 45225 target) ; Learning rate = 0.000280 ; Loss = 1.633583\n",
      "2024-12-13 04:20:50.141000: I runner.py:310] Step = 99500 ; steps/s = 1.63, tokens/s = 78435 (33222 source, 45213 target) ; Learning rate = 0.000280 ; Loss = 1.641715\n",
      "2024-12-13 04:21:51.100000: I runner.py:310] Step = 99600 ; steps/s = 1.64, tokens/s = 77533 (32827 source, 44706 target) ; Learning rate = 0.000280 ; Loss = 1.632396\n",
      "2024-12-13 04:22:52.419000: I runner.py:310] Step = 99700 ; steps/s = 1.63, tokens/s = 78407 (33203 source, 45204 target) ; Learning rate = 0.000280 ; Loss = 1.637548\n",
      "2024-12-13 04:23:53.688000: I runner.py:310] Step = 99800 ; steps/s = 1.63, tokens/s = 78491 (33230 source, 45261 target) ; Learning rate = 0.000280 ; Loss = 1.625466\n",
      "2024-12-13 04:24:54.647000: I runner.py:310] Step = 99900 ; steps/s = 1.64, tokens/s = 77578 (32862 source, 44716 target) ; Learning rate = 0.000280 ; Loss = 1.617365\n",
      "2024-12-13 04:25:55.948000: I runner.py:310] Step = 100000 ; steps/s = 1.63, tokens/s = 78440 (33226 source, 45214 target) ; Learning rate = 0.000280 ; Loss = 1.627281\n",
      "2024-12-13 04:25:58.133000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-100000\n",
      "2024-12-13 04:25:58.133000: I training.py:192] Running evaluation for step 100000\n",
      "2024-12-13 04:28:05.424000: I training.py:192] Evaluation result for step 100000: loss = 2.720315 ; perplexity = 15.185098\n",
      "2024-12-13 04:29:06.557000: I runner.py:310] Step = 100100 ; steps/s = 1.64, tokens/s = 78664 (33297 source, 45367 target) ; Learning rate = 0.000279 ; Loss = 1.643083\n",
      "2024-12-13 04:30:07.847000: I runner.py:310] Step = 100200 ; steps/s = 1.63, tokens/s = 78447 (33213 source, 45234 target) ; Learning rate = 0.000279 ; Loss = 1.642302\n",
      "2024-12-13 04:31:08.775000: I runner.py:310] Step = 100300 ; steps/s = 1.64, tokens/s = 77638 (32897 source, 44741 target) ; Learning rate = 0.000279 ; Loss = 1.618723\n",
      "2024-12-13 04:32:10.108000: I runner.py:310] Step = 100400 ; steps/s = 1.63, tokens/s = 78405 (33210 source, 45195 target) ; Learning rate = 0.000279 ; Loss = 1.634841\n",
      "2024-12-13 04:33:11.450000: I runner.py:310] Step = 100500 ; steps/s = 1.63, tokens/s = 78375 (33186 source, 45189 target) ; Learning rate = 0.000279 ; Loss = 1.637000\n",
      "2024-12-13 04:34:12.312000: I runner.py:310] Step = 100600 ; steps/s = 1.64, tokens/s = 77656 (32865 source, 44791 target) ; Learning rate = 0.000279 ; Loss = 1.617059\n",
      "2024-12-13 04:35:13.680000: I runner.py:310] Step = 100700 ; steps/s = 1.63, tokens/s = 78359 (33188 source, 45171 target) ; Learning rate = 0.000279 ; Loss = 1.621030\n",
      "2024-12-13 04:36:15.028000: I runner.py:310] Step = 100800 ; steps/s = 1.63, tokens/s = 78348 (33160 source, 45188 target) ; Learning rate = 0.000278 ; Loss = 1.634858\n",
      "2024-12-13 04:37:16.319000: I runner.py:310] Step = 100900 ; steps/s = 1.63, tokens/s = 78456 (33220 source, 45236 target) ; Learning rate = 0.000278 ; Loss = 1.640310\n",
      "2024-12-13 04:38:17.255000: I runner.py:310] Step = 101000 ; steps/s = 1.64, tokens/s = 77588 (32860 source, 44728 target) ; Learning rate = 0.000278 ; Loss = 1.643284\n",
      "2024-12-13 04:39:18.590000: I runner.py:310] Step = 101100 ; steps/s = 1.63, tokens/s = 78373 (33172 source, 45201 target) ; Learning rate = 0.000278 ; Loss = 1.629019\n",
      "2024-12-13 04:40:19.844000: I runner.py:310] Step = 101200 ; steps/s = 1.63, tokens/s = 78516 (33255 source, 45261 target) ; Learning rate = 0.000278 ; Loss = 1.625133\n",
      "2024-12-13 04:41:20.703000: I runner.py:310] Step = 101300 ; steps/s = 1.64, tokens/s = 77710 (32922 source, 44788 target) ; Learning rate = 0.000278 ; Loss = 1.624629\n",
      "2024-12-13 04:42:21.995000: I runner.py:310] Step = 101400 ; steps/s = 1.63, tokens/s = 78452 (33220 source, 45232 target) ; Learning rate = 0.000278 ; Loss = 1.625131\n",
      "2024-12-13 04:43:23.321000: I runner.py:310] Step = 101500 ; steps/s = 1.63, tokens/s = 78397 (33204 source, 45193 target) ; Learning rate = 0.000277 ; Loss = 1.627621\n",
      "2024-12-13 04:44:24.662000: I runner.py:310] Step = 101600 ; steps/s = 1.63, tokens/s = 78402 (33198 source, 45204 target) ; Learning rate = 0.000277 ; Loss = 1.640742\n",
      "2024-12-13 04:45:25.647000: I runner.py:310] Step = 101700 ; steps/s = 1.64, tokens/s = 77508 (32810 source, 44698 target) ; Learning rate = 0.000277 ; Loss = 1.616025\n",
      "2024-12-13 04:46:26.955000: I runner.py:310] Step = 101800 ; steps/s = 1.63, tokens/s = 78427 (33213 source, 45214 target) ; Learning rate = 0.000277 ; Loss = 1.635548\n",
      "2024-12-13 04:47:28.210000: I runner.py:310] Step = 101900 ; steps/s = 1.63, tokens/s = 78511 (33253 source, 45258 target) ; Learning rate = 0.000277 ; Loss = 1.637274\n",
      "2024-12-13 04:48:29.452000: I runner.py:310] Step = 102000 ; steps/s = 1.63, tokens/s = 78505 (33240 source, 45265 target) ; Learning rate = 0.000277 ; Loss = 1.634729\n",
      "2024-12-13 04:49:30.321000: I runner.py:310] Step = 102100 ; steps/s = 1.64, tokens/s = 77689 (32902 source, 44787 target) ; Learning rate = 0.000277 ; Loss = 1.613120\n",
      "2024-12-13 04:50:31.693000: I runner.py:310] Step = 102200 ; steps/s = 1.63, tokens/s = 78353 (33180 source, 45173 target) ; Learning rate = 0.000276 ; Loss = 1.635865\n",
      "2024-12-13 04:51:33.000000: I runner.py:310] Step = 102300 ; steps/s = 1.63, tokens/s = 78417 (33201 source, 45216 target) ; Learning rate = 0.000276 ; Loss = 1.637372\n",
      "2024-12-13 04:52:33.935000: I runner.py:310] Step = 102400 ; steps/s = 1.64, tokens/s = 77597 (32868 source, 44729 target) ; Learning rate = 0.000276 ; Loss = 1.622944\n",
      "2024-12-13 04:53:35.179000: I runner.py:310] Step = 102500 ; steps/s = 1.63, tokens/s = 78511 (33241 source, 45270 target) ; Learning rate = 0.000276 ; Loss = 1.627599\n",
      "2024-12-13 04:54:36.490000: I runner.py:310] Step = 102600 ; steps/s = 1.63, tokens/s = 78408 (33196 source, 45212 target) ; Learning rate = 0.000276 ; Loss = 1.638918\n",
      "2024-12-13 04:55:37.841000: I runner.py:310] Step = 102700 ; steps/s = 1.63, tokens/s = 78371 (33186 source, 45185 target) ; Learning rate = 0.000276 ; Loss = 1.637994\n",
      "2024-12-13 04:56:38.735000: I runner.py:310] Step = 102800 ; steps/s = 1.64, tokens/s = 77649 (32890 source, 44759 target) ; Learning rate = 0.000276 ; Loss = 1.613881\n",
      "2024-12-13 04:57:40.089000: I runner.py:310] Step = 102900 ; steps/s = 1.63, tokens/s = 78393 (33208 source, 45185 target) ; Learning rate = 0.000276 ; Loss = 1.627134\n",
      "2024-12-13 04:58:41.385000: I runner.py:310] Step = 103000 ; steps/s = 1.63, tokens/s = 78425 (33201 source, 45224 target) ; Learning rate = 0.000275 ; Loss = 1.637121\n",
      "2024-12-13 04:59:42.333000: I runner.py:310] Step = 103100 ; steps/s = 1.64, tokens/s = 77547 (32818 source, 44729 target) ; Learning rate = 0.000275 ; Loss = 1.626230\n",
      "2024-12-13 05:00:43.660000: I runner.py:310] Step = 103200 ; steps/s = 1.63, tokens/s = 78383 (33184 source, 45199 target) ; Learning rate = 0.000275 ; Loss = 1.632109\n",
      "2024-12-13 05:01:44.958000: I runner.py:310] Step = 103300 ; steps/s = 1.63, tokens/s = 78435 (33208 source, 45227 target) ; Learning rate = 0.000275 ; Loss = 1.627377\n",
      "2024-12-13 05:02:46.273000: I runner.py:310] Step = 103400 ; steps/s = 1.63, tokens/s = 78424 (33213 source, 45211 target) ; Learning rate = 0.000275 ; Loss = 1.635349\n",
      "2024-12-13 05:03:47.179000: I runner.py:310] Step = 103500 ; steps/s = 1.64, tokens/s = 77680 (32920 source, 44760 target) ; Learning rate = 0.000275 ; Loss = 1.643124\n",
      "2024-12-13 05:04:48.393000: I runner.py:310] Step = 103600 ; steps/s = 1.63, tokens/s = 78535 (33244 source, 45291 target) ; Learning rate = 0.000275 ; Loss = 1.616998\n",
      "2024-12-13 05:05:49.644000: I runner.py:310] Step = 103700 ; steps/s = 1.63, tokens/s = 78489 (33220 source, 45269 target) ; Learning rate = 0.000274 ; Loss = 1.626475\n",
      "2024-12-13 05:06:50.601000: I runner.py:310] Step = 103800 ; steps/s = 1.64, tokens/s = 77565 (32859 source, 44706 target) ; Learning rate = 0.000274 ; Loss = 1.638542\n",
      "2024-12-13 05:07:51.913000: I runner.py:310] Step = 103900 ; steps/s = 1.63, tokens/s = 78439 (33232 source, 45207 target) ; Learning rate = 0.000274 ; Loss = 1.617679\n",
      "2024-12-13 05:08:53.226000: I runner.py:310] Step = 104000 ; steps/s = 1.63, tokens/s = 78392 (33175 source, 45217 target) ; Learning rate = 0.000274 ; Loss = 1.635361\n",
      "2024-12-13 05:09:54.482000: I runner.py:310] Step = 104100 ; steps/s = 1.63, tokens/s = 78490 (33232 source, 45258 target) ; Learning rate = 0.000274 ; Loss = 1.630276\n",
      "2024-12-13 05:10:55.355000: I runner.py:310] Step = 104200 ; steps/s = 1.64, tokens/s = 77676 (32901 source, 44775 target) ; Learning rate = 0.000274 ; Loss = 1.633066\n",
      "2024-12-13 05:11:56.593000: I runner.py:310] Step = 104300 ; steps/s = 1.63, tokens/s = 78529 (33257 source, 45272 target) ; Learning rate = 0.000274 ; Loss = 1.625328\n",
      "2024-12-13 05:12:57.836000: I runner.py:310] Step = 104400 ; steps/s = 1.63, tokens/s = 78538 (33269 source, 45269 target) ; Learning rate = 0.000274 ; Loss = 1.624294\n",
      "2024-12-13 05:13:59.154000: I runner.py:310] Step = 104500 ; steps/s = 1.63, tokens/s = 78379 (33170 source, 45209 target) ; Learning rate = 0.000273 ; Loss = 1.630710\n",
      "2024-12-13 05:15:00.033000: I runner.py:310] Step = 104600 ; steps/s = 1.64, tokens/s = 77676 (32906 source, 44770 target) ; Learning rate = 0.000273 ; Loss = 1.642729\n",
      "2024-12-13 05:16:01.360000: I runner.py:310] Step = 104700 ; steps/s = 1.63, tokens/s = 78383 (33173 source, 45210 target) ; Learning rate = 0.000273 ; Loss = 1.623901\n",
      "2024-12-13 05:17:02.653000: I runner.py:310] Step = 104800 ; steps/s = 1.63, tokens/s = 78483 (33253 source, 45230 target) ; Learning rate = 0.000273 ; Loss = 1.627540\n",
      "2024-12-13 05:18:03.559000: I runner.py:310] Step = 104900 ; steps/s = 1.64, tokens/s = 77592 (32841 source, 44751 target) ; Learning rate = 0.000273 ; Loss = 1.630614\n",
      "2024-12-13 05:19:04.889000: I runner.py:310] Step = 105000 ; steps/s = 1.63, tokens/s = 78389 (33184 source, 45205 target) ; Learning rate = 0.000273 ; Loss = 1.622267\n",
      "2024-12-13 05:19:04.891000: I training.py:192] Running evaluation for step 105000\n",
      "2024-12-13 05:21:12.573000: I training.py:192] Evaluation result for step 105000: loss = 2.723394 ; perplexity = 15.231930\n",
      "2024-12-13 05:22:13.705000: I runner.py:310] Step = 105100 ; steps/s = 1.64, tokens/s = 78665 (33311 source, 45354 target) ; Learning rate = 0.000273 ; Loss = 1.629005\n",
      "2024-12-13 05:23:15.031000: I runner.py:310] Step = 105200 ; steps/s = 1.63, tokens/s = 78423 (33220 source, 45203 target) ; Learning rate = 0.000273 ; Loss = 1.625697\n",
      "2024-12-13 05:24:15.939000: I runner.py:310] Step = 105300 ; steps/s = 1.64, tokens/s = 77653 (32898 source, 44755 target) ; Learning rate = 0.000272 ; Loss = 1.617506\n",
      "2024-12-13 05:25:17.242000: I runner.py:310] Step = 105400 ; steps/s = 1.63, tokens/s = 78412 (33204 source, 45208 target) ; Learning rate = 0.000272 ; Loss = 1.632501\n",
      "2024-12-13 05:26:18.537000: I runner.py:310] Step = 105500 ; steps/s = 1.63, tokens/s = 78453 (33220 source, 45233 target) ; Learning rate = 0.000272 ; Loss = 1.631876\n",
      "2024-12-13 05:27:19.396000: I runner.py:310] Step = 105600 ; steps/s = 1.64, tokens/s = 77678 (32883 source, 44795 target) ; Learning rate = 0.000272 ; Loss = 1.622262\n",
      "2024-12-13 05:28:20.691000: I runner.py:310] Step = 105700 ; steps/s = 1.63, tokens/s = 78479 (33249 source, 45230 target) ; Learning rate = 0.000272 ; Loss = 1.622051\n",
      "2024-12-13 05:29:21.993000: I runner.py:310] Step = 105800 ; steps/s = 1.63, tokens/s = 78416 (33197 source, 45219 target) ; Learning rate = 0.000272 ; Loss = 1.628968\n",
      "2024-12-13 05:30:23.269000: I runner.py:310] Step = 105900 ; steps/s = 1.63, tokens/s = 78454 (33214 source, 45240 target) ; Learning rate = 0.000272 ; Loss = 1.635974\n",
      "2024-12-13 05:31:24.183000: I runner.py:310] Step = 106000 ; steps/s = 1.64, tokens/s = 77599 (32843 source, 44756 target) ; Learning rate = 0.000271 ; Loss = 1.635284\n",
      "2024-12-13 05:32:25.472000: I runner.py:310] Step = 106100 ; steps/s = 1.63, tokens/s = 78441 (33205 source, 45236 target) ; Learning rate = 0.000271 ; Loss = 1.622974\n",
      "2024-12-13 05:33:26.735000: I runner.py:310] Step = 106200 ; steps/s = 1.63, tokens/s = 78503 (33252 source, 45251 target) ; Learning rate = 0.000271 ; Loss = 1.620815\n",
      "2024-12-13 05:34:28.051000: I runner.py:310] Step = 106300 ; steps/s = 1.63, tokens/s = 78429 (33226 source, 45203 target) ; Learning rate = 0.000271 ; Loss = 1.623160\n",
      "2024-12-13 05:35:28.959000: I runner.py:310] Step = 106400 ; steps/s = 1.64, tokens/s = 77627 (32867 source, 44760 target) ; Learning rate = 0.000271 ; Loss = 1.627800\n",
      "2024-12-13 05:36:30.239000: I runner.py:310] Step = 106500 ; steps/s = 1.63, tokens/s = 78464 (33233 source, 45231 target) ; Learning rate = 0.000271 ; Loss = 1.618862\n",
      "2024-12-13 05:37:31.617000: I runner.py:310] Step = 106600 ; steps/s = 1.63, tokens/s = 78327 (33160 source, 45167 target) ; Learning rate = 0.000271 ; Loss = 1.628502\n",
      "2024-12-13 05:38:32.595000: I runner.py:310] Step = 106700 ; steps/s = 1.64, tokens/s = 77521 (32823 source, 44698 target) ; Learning rate = 0.000271 ; Loss = 1.633905\n",
      "2024-12-13 05:39:33.858000: I runner.py:310] Step = 106800 ; steps/s = 1.63, tokens/s = 78467 (33207 source, 45260 target) ; Learning rate = 0.000270 ; Loss = 1.619299\n",
      "2024-12-13 05:40:35.104000: I runner.py:310] Step = 106900 ; steps/s = 1.63, tokens/s = 78549 (33276 source, 45273 target) ; Learning rate = 0.000270 ; Loss = 1.618808\n",
      "2024-12-13 05:41:36.443000: I runner.py:310] Step = 107000 ; steps/s = 1.63, tokens/s = 78381 (33203 source, 45178 target) ; Learning rate = 0.000270 ; Loss = 1.627597\n",
      "2024-12-13 05:42:37.340000: I runner.py:310] Step = 107100 ; steps/s = 1.64, tokens/s = 77662 (32897 source, 44765 target) ; Learning rate = 0.000270 ; Loss = 1.610105\n",
      "2024-12-13 05:43:38.701000: I runner.py:310] Step = 107200 ; steps/s = 1.63, tokens/s = 78357 (33175 source, 45182 target) ; Learning rate = 0.000270 ; Loss = 1.634795\n",
      "2024-12-13 05:44:39.975000: I runner.py:310] Step = 107300 ; steps/s = 1.63, tokens/s = 78468 (33229 source, 45239 target) ; Learning rate = 0.000270 ; Loss = 1.628409\n",
      "2024-12-13 05:45:40.859000: I runner.py:310] Step = 107400 ; steps/s = 1.64, tokens/s = 77645 (32875 source, 44770 target) ; Learning rate = 0.000270 ; Loss = 1.626308\n",
      "2024-12-13 05:46:42.156000: I runner.py:310] Step = 107500 ; steps/s = 1.63, tokens/s = 78410 (33177 source, 45233 target) ; Learning rate = 0.000270 ; Loss = 1.629474\n",
      "2024-12-13 05:47:43.467000: I runner.py:310] Step = 107600 ; steps/s = 1.63, tokens/s = 78419 (33206 source, 45213 target) ; Learning rate = 0.000269 ; Loss = 1.623994\n",
      "2024-12-13 05:48:44.729000: I runner.py:310] Step = 107700 ; steps/s = 1.63, tokens/s = 78505 (33259 source, 45246 target) ; Learning rate = 0.000269 ; Loss = 1.624190\n",
      "2024-12-13 05:49:45.557000: I runner.py:310] Step = 107800 ; steps/s = 1.64, tokens/s = 77729 (32908 source, 44821 target) ; Learning rate = 0.000269 ; Loss = 1.636587\n",
      "2024-12-13 05:50:46.833000: I runner.py:310] Step = 107900 ; steps/s = 1.63, tokens/s = 78495 (33263 source, 45232 target) ; Learning rate = 0.000269 ; Loss = 1.623962\n",
      "2024-12-13 05:51:48.201000: I runner.py:310] Step = 108000 ; steps/s = 1.63, tokens/s = 78357 (33182 source, 45175 target) ; Learning rate = 0.000269 ; Loss = 1.627224\n",
      "2024-12-13 05:52:49.129000: I runner.py:310] Step = 108100 ; steps/s = 1.64, tokens/s = 77576 (32839 source, 44737 target) ; Learning rate = 0.000269 ; Loss = 1.626511\n",
      "2024-12-13 05:53:50.419000: I runner.py:310] Step = 108200 ; steps/s = 1.63, tokens/s = 78445 (33217 source, 45228 target) ; Learning rate = 0.000269 ; Loss = 1.621908\n",
      "2024-12-13 05:54:51.685000: I runner.py:310] Step = 108300 ; steps/s = 1.63, tokens/s = 78499 (33244 source, 45255 target) ; Learning rate = 0.000269 ; Loss = 1.621799\n",
      "2024-12-13 05:55:52.953000: I runner.py:310] Step = 108400 ; steps/s = 1.63, tokens/s = 78462 (33215 source, 45247 target) ; Learning rate = 0.000268 ; Loss = 1.623065\n",
      "2024-12-13 05:56:53.804000: I runner.py:310] Step = 108500 ; steps/s = 1.64, tokens/s = 77724 (32933 source, 44791 target) ; Learning rate = 0.000268 ; Loss = 1.615177\n",
      "2024-12-13 05:57:55.079000: I runner.py:310] Step = 108600 ; steps/s = 1.63, tokens/s = 78482 (33233 source, 45249 target) ; Learning rate = 0.000268 ; Loss = 1.624240\n",
      "2024-12-13 05:58:56.339000: I runner.py:310] Step = 108700 ; steps/s = 1.63, tokens/s = 78464 (33212 source, 45252 target) ; Learning rate = 0.000268 ; Loss = 1.624140\n",
      "2024-12-13 05:59:57.681000: I runner.py:310] Step = 108800 ; steps/s = 1.63, tokens/s = 78369 (33182 source, 45187 target) ; Learning rate = 0.000268 ; Loss = 1.620971\n",
      "2024-12-13 06:00:58.595000: I runner.py:310] Step = 108900 ; steps/s = 1.64, tokens/s = 77637 (32887 source, 44750 target) ; Learning rate = 0.000268 ; Loss = 1.620655\n",
      "2024-12-13 06:01:59.866000: I runner.py:310] Step = 109000 ; steps/s = 1.63, tokens/s = 78494 (33251 source, 45243 target) ; Learning rate = 0.000268 ; Loss = 1.629432\n",
      "2024-12-13 06:03:01.109000: I runner.py:310] Step = 109100 ; steps/s = 1.63, tokens/s = 78490 (33227 source, 45263 target) ; Learning rate = 0.000268 ; Loss = 1.624936\n",
      "2024-12-13 06:04:02.053000: I runner.py:310] Step = 109200 ; steps/s = 1.64, tokens/s = 77586 (32850 source, 44736 target) ; Learning rate = 0.000267 ; Loss = 1.608398\n",
      "2024-12-13 06:05:03.316000: I runner.py:310] Step = 109300 ; steps/s = 1.63, tokens/s = 78510 (33266 source, 45244 target) ; Learning rate = 0.000267 ; Loss = 1.622701\n",
      "2024-12-13 06:06:04.627000: I runner.py:310] Step = 109400 ; steps/s = 1.63, tokens/s = 78402 (33189 source, 45213 target) ; Learning rate = 0.000267 ; Loss = 1.628558\n",
      "2024-12-13 06:07:05.906000: I runner.py:310] Step = 109500 ; steps/s = 1.63, tokens/s = 78453 (33209 source, 45244 target) ; Learning rate = 0.000267 ; Loss = 1.624955\n",
      "2024-12-13 06:08:06.941000: I runner.py:310] Step = 109600 ; steps/s = 1.64, tokens/s = 77444 (32779 source, 44665 target) ; Learning rate = 0.000267 ; Loss = 1.643376\n",
      "2024-12-13 06:09:08.215000: I runner.py:310] Step = 109700 ; steps/s = 1.63, tokens/s = 78473 (33237 source, 45236 target) ; Learning rate = 0.000267 ; Loss = 1.621285\n",
      "2024-12-13 06:10:09.531000: I runner.py:310] Step = 109800 ; steps/s = 1.63, tokens/s = 78434 (33216 source, 45218 target) ; Learning rate = 0.000267 ; Loss = 1.628236\n",
      "2024-12-13 06:11:10.467000: I runner.py:310] Step = 109900 ; steps/s = 1.64, tokens/s = 77584 (32854 source, 44730 target) ; Learning rate = 0.000267 ; Loss = 1.623473\n",
      "2024-12-13 06:12:11.696000: I runner.py:310] Step = 110000 ; steps/s = 1.63, tokens/s = 78535 (33254 source, 45281 target) ; Learning rate = 0.000266 ; Loss = 1.615374\n",
      "2024-12-13 06:12:13.817000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-110000\n",
      "2024-12-13 06:12:13.818000: I training.py:192] Running evaluation for step 110000\n",
      "2024-12-13 06:14:19.621000: I training.py:192] Evaluation result for step 110000: loss = 2.725300 ; perplexity = 15.260993\n",
      "2024-12-13 06:15:20.781000: I runner.py:310] Step = 110100 ; steps/s = 1.64, tokens/s = 78657 (33330 source, 45327 target) ; Learning rate = 0.000266 ; Loss = 1.614997\n",
      "2024-12-13 06:16:22.153000: I runner.py:310] Step = 110200 ; steps/s = 1.63, tokens/s = 78315 (33141 source, 45174 target) ; Learning rate = 0.000266 ; Loss = 1.618208\n",
      "2024-12-13 06:17:23.201000: I runner.py:310] Step = 110300 ; steps/s = 1.64, tokens/s = 77450 (32798 source, 44652 target) ; Learning rate = 0.000266 ; Loss = 1.610111\n",
      "2024-12-13 06:18:24.471000: I runner.py:310] Step = 110400 ; steps/s = 1.63, tokens/s = 78483 (33235 source, 45248 target) ; Learning rate = 0.000266 ; Loss = 1.627599\n",
      "2024-12-13 06:19:25.782000: I runner.py:310] Step = 110500 ; steps/s = 1.63, tokens/s = 78429 (33219 source, 45210 target) ; Learning rate = 0.000266 ; Loss = 1.622805\n",
      "2024-12-13 06:20:26.697000: I runner.py:310] Step = 110600 ; steps/s = 1.64, tokens/s = 77604 (32850 source, 44754 target) ; Learning rate = 0.000266 ; Loss = 1.648697\n",
      "2024-12-13 06:21:28.022000: I runner.py:310] Step = 110700 ; steps/s = 1.63, tokens/s = 78408 (33198 source, 45210 target) ; Learning rate = 0.000266 ; Loss = 1.633335\n",
      "2024-12-13 06:22:29.344000: I runner.py:310] Step = 110800 ; steps/s = 1.63, tokens/s = 78420 (33220 source, 45200 target) ; Learning rate = 0.000266 ; Loss = 1.614962\n",
      "2024-12-13 06:23:30.720000: I runner.py:310] Step = 110900 ; steps/s = 1.63, tokens/s = 78338 (33174 source, 45164 target) ; Learning rate = 0.000265 ; Loss = 1.621579\n",
      "2024-12-13 06:24:31.735000: I runner.py:310] Step = 111000 ; steps/s = 1.64, tokens/s = 77473 (32787 source, 44686 target) ; Learning rate = 0.000265 ; Loss = 1.610469\n",
      "2024-12-13 06:25:33.063000: I runner.py:310] Step = 111100 ; steps/s = 1.63, tokens/s = 78404 (33204 source, 45200 target) ; Learning rate = 0.000265 ; Loss = 1.625732\n",
      "2024-12-13 06:26:34.320000: I runner.py:310] Step = 111200 ; steps/s = 1.63, tokens/s = 78492 (33227 source, 45265 target) ; Learning rate = 0.000265 ; Loss = 1.627259\n",
      "2024-12-13 06:27:35.686000: I runner.py:310] Step = 111300 ; steps/s = 1.63, tokens/s = 78364 (33199 source, 45165 target) ; Learning rate = 0.000265 ; Loss = 1.633987\n",
      "2024-12-13 06:28:36.582000: I runner.py:310] Step = 111400 ; steps/s = 1.64, tokens/s = 77649 (32893 source, 44756 target) ; Learning rate = 0.000265 ; Loss = 1.625534\n",
      "2024-12-13 06:29:37.927000: I runner.py:310] Step = 111500 ; steps/s = 1.63, tokens/s = 78382 (33190 source, 45192 target) ; Learning rate = 0.000265 ; Loss = 1.614632\n",
      "2024-12-13 06:30:39.262000: I runner.py:310] Step = 111600 ; steps/s = 1.63, tokens/s = 78386 (33185 source, 45201 target) ; Learning rate = 0.000265 ; Loss = 1.624186\n",
      "2024-12-13 06:31:40.242000: I runner.py:310] Step = 111700 ; steps/s = 1.64, tokens/s = 77519 (32818 source, 44701 target) ; Learning rate = 0.000264 ; Loss = 1.625637\n",
      "2024-12-13 06:32:41.550000: I runner.py:310] Step = 111800 ; steps/s = 1.63, tokens/s = 78423 (33212 source, 45211 target) ; Learning rate = 0.000264 ; Loss = 1.622596\n",
      "2024-12-13 06:33:42.886000: I runner.py:310] Step = 111900 ; steps/s = 1.63, tokens/s = 78393 (33190 source, 45203 target) ; Learning rate = 0.000264 ; Loss = 1.625970\n",
      "2024-12-13 06:34:44.249000: I runner.py:310] Step = 112000 ; steps/s = 1.63, tokens/s = 78365 (33185 source, 45180 target) ; Learning rate = 0.000264 ; Loss = 1.621529\n",
      "2024-12-13 06:35:45.166000: I runner.py:310] Step = 112100 ; steps/s = 1.64, tokens/s = 77610 (32871 source, 44739 target) ; Learning rate = 0.000264 ; Loss = 1.606641\n",
      "2024-12-13 06:36:46.520000: I runner.py:310] Step = 112200 ; steps/s = 1.63, tokens/s = 78382 (33187 source, 45195 target) ; Learning rate = 0.000264 ; Loss = 1.622471\n",
      "2024-12-13 06:37:47.835000: I runner.py:310] Step = 112300 ; steps/s = 1.63, tokens/s = 78394 (33187 source, 45207 target) ; Learning rate = 0.000264 ; Loss = 1.631793\n",
      "2024-12-13 06:38:48.795000: I runner.py:310] Step = 112400 ; steps/s = 1.64, tokens/s = 77580 (32864 source, 44716 target) ; Learning rate = 0.000264 ; Loss = 1.619757\n",
      "2024-12-13 06:39:50.116000: I runner.py:310] Step = 112500 ; steps/s = 1.63, tokens/s = 78390 (33178 source, 45212 target) ; Learning rate = 0.000264 ; Loss = 1.627460\n",
      "2024-12-13 06:40:51.480000: I runner.py:310] Step = 112600 ; steps/s = 1.63, tokens/s = 78335 (33162 source, 45173 target) ; Learning rate = 0.000263 ; Loss = 1.620832\n",
      "2024-12-13 06:41:52.810000: I runner.py:310] Step = 112700 ; steps/s = 1.63, tokens/s = 78430 (33232 source, 45198 target) ; Learning rate = 0.000263 ; Loss = 1.622873\n",
      "2024-12-13 06:42:53.688000: I runner.py:310] Step = 112800 ; steps/s = 1.64, tokens/s = 77699 (32915 source, 44784 target) ; Learning rate = 0.000263 ; Loss = 1.607147\n",
      "2024-12-13 06:43:55.012000: I runner.py:310] Step = 112900 ; steps/s = 1.63, tokens/s = 78435 (33239 source, 45196 target) ; Learning rate = 0.000263 ; Loss = 1.627146\n",
      "2024-12-13 06:44:56.307000: I runner.py:310] Step = 113000 ; steps/s = 1.63, tokens/s = 78416 (33187 source, 45229 target) ; Learning rate = 0.000263 ; Loss = 1.628898\n",
      "2024-12-13 06:45:57.661000: I runner.py:310] Step = 113100 ; steps/s = 1.63, tokens/s = 78350 (33162 source, 45188 target) ; Learning rate = 0.000263 ; Loss = 1.626559\n",
      "2024-12-13 06:46:58.629000: I runner.py:310] Step = 113200 ; steps/s = 1.64, tokens/s = 77567 (32861 source, 44706 target) ; Learning rate = 0.000263 ; Loss = 1.620442\n",
      "2024-12-13 06:47:59.989000: I runner.py:310] Step = 113300 ; steps/s = 1.63, tokens/s = 78383 (33196 source, 45187 target) ; Learning rate = 0.000263 ; Loss = 1.627927\n",
      "2024-12-13 06:49:01.259000: I runner.py:310] Step = 113400 ; steps/s = 1.63, tokens/s = 78439 (33194 source, 45245 target) ; Learning rate = 0.000262 ; Loss = 1.621864\n",
      "2024-12-13 06:50:02.149000: I runner.py:310] Step = 113500 ; steps/s = 1.64, tokens/s = 77638 (32874 source, 44764 target) ; Learning rate = 0.000262 ; Loss = 1.615540\n",
      "2024-12-13 06:51:03.517000: I runner.py:310] Step = 113600 ; steps/s = 1.63, tokens/s = 78346 (33176 source, 45170 target) ; Learning rate = 0.000262 ; Loss = 1.613361\n",
      "2024-12-13 06:52:04.844000: I runner.py:310] Step = 113700 ; steps/s = 1.63, tokens/s = 78411 (33202 source, 45209 target) ; Learning rate = 0.000262 ; Loss = 1.626069\n",
      "2024-12-13 06:53:06.145000: I runner.py:310] Step = 113800 ; steps/s = 1.63, tokens/s = 78436 (33216 source, 45220 target) ; Learning rate = 0.000262 ; Loss = 1.629023\n",
      "2024-12-13 06:54:07.059000: I runner.py:310] Step = 113900 ; steps/s = 1.64, tokens/s = 77615 (32867 source, 44748 target) ; Learning rate = 0.000262 ; Loss = 1.608883\n",
      "2024-12-13 06:55:08.364000: I runner.py:310] Step = 114000 ; steps/s = 1.63, tokens/s = 78412 (33190 source, 45222 target) ; Learning rate = 0.000262 ; Loss = 1.624468\n",
      "2024-12-13 06:56:09.730000: I runner.py:310] Step = 114100 ; steps/s = 1.63, tokens/s = 78392 (33212 source, 45180 target) ; Learning rate = 0.000262 ; Loss = 1.630871\n",
      "2024-12-13 06:57:10.631000: I runner.py:310] Step = 114200 ; steps/s = 1.64, tokens/s = 77627 (32869 source, 44758 target) ; Learning rate = 0.000262 ; Loss = 1.623860\n",
      "2024-12-13 06:58:12.002000: I runner.py:310] Step = 114300 ; steps/s = 1.63, tokens/s = 78324 (33154 source, 45170 target) ; Learning rate = 0.000261 ; Loss = 1.618627\n",
      "2024-12-13 06:59:13.322000: I runner.py:310] Step = 114400 ; steps/s = 1.63, tokens/s = 78430 (33222 source, 45208 target) ; Learning rate = 0.000261 ; Loss = 1.615142\n",
      "2024-12-13 07:00:14.577000: I runner.py:310] Step = 114500 ; steps/s = 1.63, tokens/s = 78505 (33248 source, 45257 target) ; Learning rate = 0.000261 ; Loss = 1.618429\n",
      "2024-12-13 07:01:15.505000: I runner.py:310] Step = 114600 ; steps/s = 1.64, tokens/s = 77578 (32843 source, 44735 target) ; Learning rate = 0.000261 ; Loss = 1.622595\n",
      "2024-12-13 07:02:16.880000: I runner.py:310] Step = 114700 ; steps/s = 1.63, tokens/s = 78340 (33177 source, 45163 target) ; Learning rate = 0.000261 ; Loss = 1.615118\n",
      "2024-12-13 07:03:18.187000: I runner.py:310] Step = 114800 ; steps/s = 1.63, tokens/s = 78427 (33210 source, 45217 target) ; Learning rate = 0.000261 ; Loss = 1.615488\n",
      "2024-12-13 07:04:19.093000: I runner.py:310] Step = 114900 ; steps/s = 1.64, tokens/s = 77645 (32877 source, 44768 target) ; Learning rate = 0.000261 ; Loss = 1.615171\n",
      "2024-12-13 07:05:20.411000: I runner.py:310] Step = 115000 ; steps/s = 1.63, tokens/s = 78397 (33187 source, 45210 target) ; Learning rate = 0.000261 ; Loss = 1.623307\n",
      "2024-12-13 07:05:20.413000: I training.py:192] Running evaluation for step 115000\n",
      "2024-12-13 07:07:27.567000: I training.py:192] Evaluation result for step 115000: loss = 2.733572 ; perplexity = 15.387761\n",
      "2024-12-13 07:08:28.696000: I runner.py:310] Step = 115100 ; steps/s = 1.64, tokens/s = 78673 (33309 source, 45364 target) ; Learning rate = 0.000261 ; Loss = 1.610073\n",
      "2024-12-13 07:09:29.977000: I runner.py:310] Step = 115200 ; steps/s = 1.63, tokens/s = 78476 (33238 source, 45238 target) ; Learning rate = 0.000260 ; Loss = 1.627565\n",
      "2024-12-13 07:10:30.984000: I runner.py:310] Step = 115300 ; steps/s = 1.64, tokens/s = 77522 (32843 source, 44679 target) ; Learning rate = 0.000260 ; Loss = 1.607236\n",
      "2024-12-13 07:11:32.270000: I runner.py:310] Step = 115400 ; steps/s = 1.63, tokens/s = 78482 (33244 source, 45238 target) ; Learning rate = 0.000260 ; Loss = 1.621819\n",
      "2024-12-13 07:12:33.581000: I runner.py:310] Step = 115500 ; steps/s = 1.63, tokens/s = 78399 (33185 source, 45214 target) ; Learning rate = 0.000260 ; Loss = 1.622673\n",
      "2024-12-13 07:13:34.863000: I runner.py:310] Step = 115600 ; steps/s = 1.63, tokens/s = 78449 (33215 source, 45234 target) ; Learning rate = 0.000260 ; Loss = 1.632819\n",
      "2024-12-13 07:14:35.729000: I runner.py:310] Step = 115700 ; steps/s = 1.64, tokens/s = 77699 (32908 source, 44791 target) ; Learning rate = 0.000260 ; Loss = 1.609571\n",
      "2024-12-13 07:15:37.067000: I runner.py:310] Step = 115800 ; steps/s = 1.63, tokens/s = 78386 (33190 source, 45196 target) ; Learning rate = 0.000260 ; Loss = 1.615045\n",
      "2024-12-13 07:16:38.203000: I runner.py:310] Step = 115900 ; steps/s = 1.64, tokens/s = 78640 (33301 source, 45339 target) ; Learning rate = 0.000260 ; Loss = 1.621876\n",
      "2024-12-13 07:17:39.112000: I runner.py:310] Step = 116000 ; steps/s = 1.64, tokens/s = 77613 (32856 source, 44757 target) ; Learning rate = 0.000260 ; Loss = 1.628316\n",
      "2024-12-13 07:18:40.459000: I runner.py:310] Step = 116100 ; steps/s = 1.63, tokens/s = 78336 (33136 source, 45200 target) ; Learning rate = 0.000259 ; Loss = 1.618495\n",
      "2024-12-13 07:19:41.710000: I runner.py:310] Step = 116200 ; steps/s = 1.63, tokens/s = 78507 (33251 source, 45256 target) ; Learning rate = 0.000259 ; Loss = 1.625710\n",
      "2024-12-13 07:20:42.929000: I runner.py:310] Step = 116300 ; steps/s = 1.63, tokens/s = 78583 (33303 source, 45280 target) ; Learning rate = 0.000259 ; Loss = 1.621880\n",
      "2024-12-13 07:21:43.753000: I runner.py:310] Step = 116400 ; steps/s = 1.64, tokens/s = 77742 (32938 source, 44804 target) ; Learning rate = 0.000259 ; Loss = 1.598561\n",
      "2024-12-13 07:22:45.043000: I runner.py:310] Step = 116500 ; steps/s = 1.63, tokens/s = 78454 (33224 source, 45230 target) ; Learning rate = 0.000259 ; Loss = 1.630928\n",
      "2024-12-13 07:23:46.359000: I runner.py:310] Step = 116600 ; steps/s = 1.63, tokens/s = 78437 (33224 source, 45213 target) ; Learning rate = 0.000259 ; Loss = 1.625091\n",
      "2024-12-13 07:24:47.282000: I runner.py:310] Step = 116700 ; steps/s = 1.64, tokens/s = 77577 (32829 source, 44748 target) ; Learning rate = 0.000259 ; Loss = 1.618382\n",
      "2024-12-13 07:25:48.529000: I runner.py:310] Step = 116800 ; steps/s = 1.63, tokens/s = 78491 (33229 source, 45262 target) ; Learning rate = 0.000259 ; Loss = 1.613236\n",
      "2024-12-13 07:26:49.781000: I runner.py:310] Step = 116900 ; steps/s = 1.63, tokens/s = 78494 (33233 source, 45261 target) ; Learning rate = 0.000259 ; Loss = 1.618012\n",
      "2024-12-13 07:27:51.066000: I runner.py:310] Step = 117000 ; steps/s = 1.63, tokens/s = 78463 (33229 source, 45234 target) ; Learning rate = 0.000258 ; Loss = 1.628507\n",
      "2024-12-13 07:28:51.929000: I runner.py:310] Step = 117100 ; steps/s = 1.64, tokens/s = 77680 (32894 source, 44786 target) ; Learning rate = 0.000258 ; Loss = 1.606144\n",
      "2024-12-13 07:29:53.202000: I runner.py:310] Step = 117200 ; steps/s = 1.63, tokens/s = 78497 (33251 source, 45246 target) ; Learning rate = 0.000258 ; Loss = 1.622340\n",
      "2024-12-13 07:30:54.551000: I runner.py:310] Step = 117300 ; steps/s = 1.63, tokens/s = 78365 (33179 source, 45186 target) ; Learning rate = 0.000258 ; Loss = 1.623889\n",
      "2024-12-13 07:31:55.543000: I runner.py:310] Step = 117400 ; steps/s = 1.64, tokens/s = 77740 (32920 source, 44820 target) ; Learning rate = 0.000258 ; Loss = 1.649368\n",
      "2024-12-13 07:32:56.651000: I runner.py:310] Step = 117500 ; steps/s = 1.64, tokens/s = 78456 (33229 source, 45227 target) ; Learning rate = 0.000258 ; Loss = 1.629475\n",
      "2024-12-13 07:33:57.919000: I runner.py:310] Step = 117600 ; steps/s = 1.63, tokens/s = 78443 (33195 source, 45248 target) ; Learning rate = 0.000258 ; Loss = 1.609594\n",
      "2024-12-13 07:34:59.204000: I runner.py:310] Step = 117700 ; steps/s = 1.63, tokens/s = 78467 (33234 source, 45233 target) ; Learning rate = 0.000258 ; Loss = 1.613299\n",
      "2024-12-13 07:35:59.953000: I runner.py:310] Step = 117800 ; steps/s = 1.65, tokens/s = 77862 (32982 source, 44880 target) ; Learning rate = 0.000258 ; Loss = 1.619777\n",
      "2024-12-13 07:37:01.199000: I runner.py:310] Step = 117900 ; steps/s = 1.63, tokens/s = 78506 (33248 source, 45258 target) ; Learning rate = 0.000257 ; Loss = 1.613619\n",
      "2024-12-13 07:38:02.491000: I runner.py:310] Step = 118000 ; steps/s = 1.63, tokens/s = 78462 (33237 source, 45225 target) ; Learning rate = 0.000257 ; Loss = 1.620541\n",
      "2024-12-13 07:39:03.743000: I runner.py:310] Step = 118100 ; steps/s = 1.63, tokens/s = 78484 (33215 source, 45269 target) ; Learning rate = 0.000257 ; Loss = 1.615854\n",
      "2024-12-13 07:40:04.606000: I runner.py:310] Step = 118200 ; steps/s = 1.64, tokens/s = 77673 (32889 source, 44784 target) ; Learning rate = 0.000257 ; Loss = 1.601408\n",
      "2024-12-13 07:41:05.862000: I runner.py:310] Step = 118300 ; steps/s = 1.63, tokens/s = 78481 (33221 source, 45260 target) ; Learning rate = 0.000257 ; Loss = 1.625422\n",
      "2024-12-13 07:42:07.214000: I runner.py:310] Step = 118400 ; steps/s = 1.63, tokens/s = 78390 (33208 source, 45182 target) ; Learning rate = 0.000257 ; Loss = 1.618339\n",
      "2024-12-13 07:43:08.112000: I runner.py:310] Step = 118500 ; steps/s = 1.64, tokens/s = 77631 (32870 source, 44761 target) ; Learning rate = 0.000257 ; Loss = 1.611706\n",
      "2024-12-13 07:44:09.386000: I runner.py:310] Step = 118600 ; steps/s = 1.63, tokens/s = 78480 (33251 source, 45229 target) ; Learning rate = 0.000257 ; Loss = 1.611084\n",
      "2024-12-13 07:45:10.599000: I runner.py:310] Step = 118700 ; steps/s = 1.63, tokens/s = 78547 (33258 source, 45289 target) ; Learning rate = 0.000257 ; Loss = 1.626359\n",
      "2024-12-13 07:46:11.867000: I runner.py:310] Step = 118800 ; steps/s = 1.63, tokens/s = 78469 (33216 source, 45253 target) ; Learning rate = 0.000256 ; Loss = 1.624802\n",
      "2024-12-13 07:47:12.773000: I runner.py:310] Step = 118900 ; steps/s = 1.64, tokens/s = 77631 (32871 source, 44760 target) ; Learning rate = 0.000256 ; Loss = 1.605610\n",
      "2024-12-13 07:48:14.061000: I runner.py:310] Step = 119000 ; steps/s = 1.63, tokens/s = 78447 (33215 source, 45232 target) ; Learning rate = 0.000256 ; Loss = 1.620766\n",
      "2024-12-13 07:49:15.325000: I runner.py:310] Step = 119100 ; steps/s = 1.63, tokens/s = 78481 (33224 source, 45257 target) ; Learning rate = 0.000256 ; Loss = 1.631195\n",
      "2024-12-13 07:50:16.212000: I runner.py:310] Step = 119200 ; steps/s = 1.64, tokens/s = 77659 (32896 source, 44763 target) ; Learning rate = 0.000256 ; Loss = 1.612470\n",
      "2024-12-13 07:51:17.508000: I runner.py:310] Step = 119300 ; steps/s = 1.63, tokens/s = 78453 (33233 source, 45220 target) ; Learning rate = 0.000256 ; Loss = 1.615605\n",
      "2024-12-13 07:52:18.824000: I runner.py:310] Step = 119400 ; steps/s = 1.63, tokens/s = 78407 (33193 source, 45214 target) ; Learning rate = 0.000256 ; Loss = 1.618569\n",
      "2024-12-13 07:53:20.083000: I runner.py:310] Step = 119500 ; steps/s = 1.63, tokens/s = 78504 (33249 source, 45255 target) ; Learning rate = 0.000256 ; Loss = 1.619358\n",
      "2024-12-13 07:54:20.979000: I runner.py:310] Step = 119600 ; steps/s = 1.64, tokens/s = 77624 (32859 source, 44765 target) ; Learning rate = 0.000256 ; Loss = 1.601023\n",
      "2024-12-13 07:55:22.201000: I runner.py:310] Step = 119700 ; steps/s = 1.63, tokens/s = 78512 (33240 source, 45272 target) ; Learning rate = 0.000255 ; Loss = 1.619648\n",
      "2024-12-13 07:56:23.543000: I runner.py:310] Step = 119800 ; steps/s = 1.63, tokens/s = 78403 (33204 source, 45199 target) ; Learning rate = 0.000255 ; Loss = 1.619276\n",
      "2024-12-13 07:57:24.759000: I runner.py:310] Step = 119900 ; steps/s = 1.63, tokens/s = 78567 (33278 source, 45289 target) ; Learning rate = 0.000255 ; Loss = 1.634149\n",
      "2024-12-13 07:58:25.597000: I runner.py:310] Step = 120000 ; steps/s = 1.64, tokens/s = 77695 (32896 source, 44799 target) ; Learning rate = 0.000255 ; Loss = 1.618845\n",
      "2024-12-13 07:58:27.738000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-120000\n",
      "2024-12-13 07:58:27.738000: I training.py:192] Running evaluation for step 120000\n",
      "2024-12-13 08:00:33.249000: I training.py:192] Evaluation result for step 120000: loss = 2.748265 ; perplexity = 15.615520\n",
      "2024-12-13 08:01:34.492000: I runner.py:310] Step = 120100 ; steps/s = 1.63, tokens/s = 78518 (33238 source, 45280 target) ; Learning rate = 0.000255 ; Loss = 1.606704\n",
      "2024-12-13 08:02:35.809000: I runner.py:310] Step = 120200 ; steps/s = 1.63, tokens/s = 78423 (33213 source, 45210 target) ; Learning rate = 0.000255 ; Loss = 1.624314\n",
      "2024-12-13 08:03:36.718000: I runner.py:310] Step = 120300 ; steps/s = 1.64, tokens/s = 77637 (32883 source, 44754 target) ; Learning rate = 0.000255 ; Loss = 1.611168\n",
      "2024-12-13 08:04:37.959000: I runner.py:310] Step = 120400 ; steps/s = 1.63, tokens/s = 78530 (33260 source, 45270 target) ; Learning rate = 0.000255 ; Loss = 1.620078\n",
      "2024-12-13 08:05:39.285000: I runner.py:310] Step = 120500 ; steps/s = 1.63, tokens/s = 78408 (33200 source, 45208 target) ; Learning rate = 0.000255 ; Loss = 1.614455\n",
      "2024-12-13 08:06:40.611000: I runner.py:310] Step = 120600 ; steps/s = 1.63, tokens/s = 78379 (33181 source, 45198 target) ; Learning rate = 0.000255 ; Loss = 1.621130\n",
      "2024-12-13 08:07:41.511000: I runner.py:310] Step = 120700 ; steps/s = 1.64, tokens/s = 77630 (32875 source, 44755 target) ; Learning rate = 0.000254 ; Loss = 1.627379\n",
      "2024-12-13 08:08:42.786000: I runner.py:310] Step = 120800 ; steps/s = 1.63, tokens/s = 78490 (33242 source, 45248 target) ; Learning rate = 0.000254 ; Loss = 1.616230\n",
      "2024-12-13 08:09:44.078000: I runner.py:310] Step = 120900 ; steps/s = 1.63, tokens/s = 78422 (33191 source, 45231 target) ; Learning rate = 0.000254 ; Loss = 1.615398\n",
      "2024-12-13 08:10:44.979000: I runner.py:310] Step = 121000 ; steps/s = 1.64, tokens/s = 77667 (32906 source, 44761 target) ; Learning rate = 0.000254 ; Loss = 1.619698\n",
      "2024-12-13 08:11:46.283000: I runner.py:310] Step = 121100 ; steps/s = 1.63, tokens/s = 78428 (33204 source, 45224 target) ; Learning rate = 0.000254 ; Loss = 1.605353\n",
      "2024-12-13 08:12:47.538000: I runner.py:310] Step = 121200 ; steps/s = 1.63, tokens/s = 78493 (33239 source, 45254 target) ; Learning rate = 0.000254 ; Loss = 1.624323\n",
      "2024-12-13 08:13:48.798000: I runner.py:310] Step = 121300 ; steps/s = 1.63, tokens/s = 78484 (33225 source, 45259 target) ; Learning rate = 0.000254 ; Loss = 1.627406\n",
      "2024-12-13 08:14:49.613000: I runner.py:310] Step = 121400 ; steps/s = 1.64, tokens/s = 77738 (32918 source, 44820 target) ; Learning rate = 0.000254 ; Loss = 1.601405\n",
      "2024-12-13 08:15:50.878000: I runner.py:310] Step = 121500 ; steps/s = 1.63, tokens/s = 78481 (33241 source, 45240 target) ; Learning rate = 0.000254 ; Loss = 1.616934\n",
      "2024-12-13 08:16:52.127000: I runner.py:310] Step = 121600 ; steps/s = 1.63, tokens/s = 78500 (33240 source, 45260 target) ; Learning rate = 0.000253 ; Loss = 1.618482\n",
      "2024-12-13 08:17:53.013000: I runner.py:310] Step = 121700 ; steps/s = 1.64, tokens/s = 77655 (32883 source, 44772 target) ; Learning rate = 0.000253 ; Loss = 1.603258\n",
      "2024-12-13 08:18:54.297000: I runner.py:310] Step = 121800 ; steps/s = 1.63, tokens/s = 78470 (33233 source, 45237 target) ; Learning rate = 0.000253 ; Loss = 1.613727\n",
      "2024-12-13 08:19:55.561000: I runner.py:310] Step = 121900 ; steps/s = 1.63, tokens/s = 78508 (33264 source, 45244 target) ; Learning rate = 0.000253 ; Loss = 1.605174\n",
      "2024-12-13 08:20:56.820000: I runner.py:310] Step = 122000 ; steps/s = 1.63, tokens/s = 78463 (33204 source, 45259 target) ; Learning rate = 0.000253 ; Loss = 1.616765\n",
      "2024-12-13 08:21:57.770000: I runner.py:310] Step = 122100 ; steps/s = 1.64, tokens/s = 77586 (32862 source, 44724 target) ; Learning rate = 0.000253 ; Loss = 1.601996\n",
      "2024-12-13 08:22:59.114000: I runner.py:310] Step = 122200 ; steps/s = 1.63, tokens/s = 78381 (33189 source, 45192 target) ; Learning rate = 0.000253 ; Loss = 1.613157\n",
      "2024-12-13 08:24:00.346000: I runner.py:310] Step = 122300 ; steps/s = 1.63, tokens/s = 78557 (33288 source, 45269 target) ; Learning rate = 0.000253 ; Loss = 1.621691\n",
      "2024-12-13 08:25:01.624000: I runner.py:310] Step = 122400 ; steps/s = 1.63, tokens/s = 78426 (33186 source, 45240 target) ; Learning rate = 0.000253 ; Loss = 1.622312\n",
      "2024-12-13 08:26:02.514000: I runner.py:310] Step = 122500 ; steps/s = 1.64, tokens/s = 77654 (32876 source, 44778 target) ; Learning rate = 0.000253 ; Loss = 1.623630\n",
      "2024-12-13 08:27:03.783000: I runner.py:310] Step = 122600 ; steps/s = 1.63, tokens/s = 78475 (33232 source, 45243 target) ; Learning rate = 0.000252 ; Loss = 1.611031\n",
      "2024-12-13 08:28:05.090000: I runner.py:310] Step = 122700 ; steps/s = 1.63, tokens/s = 78438 (33224 source, 45214 target) ; Learning rate = 0.000252 ; Loss = 1.612486\n",
      "2024-12-13 08:29:05.963000: I runner.py:310] Step = 122800 ; steps/s = 1.64, tokens/s = 77649 (32868 source, 44781 target) ; Learning rate = 0.000252 ; Loss = 1.624228\n",
      "2024-12-13 08:30:07.311000: I runner.py:310] Step = 122900 ; steps/s = 1.63, tokens/s = 78377 (33195 source, 45182 target) ; Learning rate = 0.000252 ; Loss = 1.608419\n",
      "2024-12-13 08:31:08.501000: I runner.py:310] Step = 123000 ; steps/s = 1.63, tokens/s = 78551 (33248 source, 45303 target) ; Learning rate = 0.000252 ; Loss = 1.608327\n",
      "2024-12-13 08:32:09.787000: I runner.py:310] Step = 123100 ; steps/s = 1.63, tokens/s = 78459 (33224 source, 45235 target) ; Learning rate = 0.000252 ; Loss = 1.615571\n",
      "2024-12-13 08:33:10.730000: I runner.py:310] Step = 123200 ; steps/s = 1.64, tokens/s = 77576 (32845 source, 44731 target) ; Learning rate = 0.000252 ; Loss = 1.620165\n",
      "2024-12-13 08:34:12.051000: I runner.py:310] Step = 123300 ; steps/s = 1.63, tokens/s = 78428 (33213 source, 45215 target) ; Learning rate = 0.000252 ; Loss = 1.614282\n",
      "2024-12-13 08:35:13.370000: I runner.py:310] Step = 123400 ; steps/s = 1.63, tokens/s = 78428 (33216 source, 45212 target) ; Learning rate = 0.000252 ; Loss = 1.611238\n",
      "2024-12-13 08:36:14.211000: I runner.py:310] Step = 123500 ; steps/s = 1.64, tokens/s = 77717 (32927 source, 44790 target) ; Learning rate = 0.000252 ; Loss = 1.605226\n",
      "2024-12-13 08:37:15.545000: I runner.py:310] Step = 123600 ; steps/s = 1.63, tokens/s = 78404 (33209 source, 45195 target) ; Learning rate = 0.000251 ; Loss = 1.615704\n",
      "2024-12-13 08:38:16.791000: I runner.py:310] Step = 123700 ; steps/s = 1.63, tokens/s = 78482 (33214 source, 45268 target) ; Learning rate = 0.000251 ; Loss = 1.617107\n",
      "2024-12-13 08:39:18.093000: I runner.py:310] Step = 123800 ; steps/s = 1.63, tokens/s = 78428 (33205 source, 45223 target) ; Learning rate = 0.000251 ; Loss = 1.619063\n",
      "2024-12-13 08:40:18.987000: I runner.py:310] Step = 123900 ; steps/s = 1.64, tokens/s = 77692 (32929 source, 44763 target) ; Learning rate = 0.000251 ; Loss = 1.596489\n",
      "2024-12-13 08:41:20.140000: I runner.py:310] Step = 124000 ; steps/s = 1.64, tokens/s = 78617 (33281 source, 45336 target) ; Learning rate = 0.000251 ; Loss = 1.607766\n",
      "2024-12-13 08:42:21.484000: I runner.py:310] Step = 124100 ; steps/s = 1.63, tokens/s = 78384 (33193 source, 45191 target) ; Learning rate = 0.000251 ; Loss = 1.628304\n",
      "2024-12-13 08:43:22.475000: I runner.py:310] Step = 124200 ; steps/s = 1.64, tokens/s = 77967 (33002 source, 44965 target) ; Learning rate = 0.000251 ; Loss = 1.614182\n",
      "2024-12-13 08:44:23.566000: I runner.py:310] Step = 124300 ; steps/s = 1.64, tokens/s = 78225 (33123 source, 45102 target) ; Learning rate = 0.000251 ; Loss = 1.603940\n",
      "2024-12-13 08:45:24.856000: I runner.py:310] Step = 124400 ; steps/s = 1.63, tokens/s = 78440 (33202 source, 45238 target) ; Learning rate = 0.000251 ; Loss = 1.624201\n",
      "2024-12-13 08:46:26.127000: I runner.py:310] Step = 124500 ; steps/s = 1.63, tokens/s = 78449 (33209 source, 45240 target) ; Learning rate = 0.000251 ; Loss = 1.622511\n",
      "2024-12-13 08:47:26.974000: I runner.py:310] Step = 124600 ; steps/s = 1.64, tokens/s = 77738 (32944 source, 44794 target) ; Learning rate = 0.000250 ; Loss = 1.618267\n",
      "2024-12-13 08:48:28.279000: I runner.py:310] Step = 124700 ; steps/s = 1.63, tokens/s = 78445 (33220 source, 45225 target) ; Learning rate = 0.000250 ; Loss = 1.598844\n",
      "2024-12-13 08:49:29.563000: I runner.py:310] Step = 124800 ; steps/s = 1.63, tokens/s = 78458 (33224 source, 45234 target) ; Learning rate = 0.000250 ; Loss = 1.609808\n",
      "2024-12-13 08:50:30.844000: I runner.py:310] Step = 124900 ; steps/s = 1.63, tokens/s = 78455 (33218 source, 45237 target) ; Learning rate = 0.000250 ; Loss = 1.608946\n",
      "2024-12-13 08:51:31.768000: I runner.py:310] Step = 125000 ; steps/s = 1.64, tokens/s = 77588 (32855 source, 44733 target) ; Learning rate = 0.000250 ; Loss = 1.600810\n",
      "2024-12-13 08:51:31.769000: I training.py:192] Running evaluation for step 125000\n",
      "2024-12-13 08:53:41.637000: I training.py:192] Evaluation result for step 125000: loss = 2.752150 ; perplexity = 15.676293\n",
      "2024-12-13 08:54:42.742000: I runner.py:310] Step = 125100 ; steps/s = 1.64, tokens/s = 78716 (33338 source, 45378 target) ; Learning rate = 0.000250 ; Loss = 1.612026\n",
      "2024-12-13 08:55:44.012000: I runner.py:310] Step = 125200 ; steps/s = 1.63, tokens/s = 78491 (33235 source, 45256 target) ; Learning rate = 0.000250 ; Loss = 1.616743\n",
      "2024-12-13 08:56:44.907000: I runner.py:310] Step = 125300 ; steps/s = 1.64, tokens/s = 77624 (32856 source, 44768 target) ; Learning rate = 0.000250 ; Loss = 1.605287\n",
      "2024-12-13 08:57:46.187000: I runner.py:310] Step = 125400 ; steps/s = 1.63, tokens/s = 78499 (33257 source, 45242 target) ; Learning rate = 0.000250 ; Loss = 1.603842\n",
      "2024-12-13 08:58:47.472000: I runner.py:310] Step = 125500 ; steps/s = 1.63, tokens/s = 78438 (33208 source, 45230 target) ; Learning rate = 0.000250 ; Loss = 1.611321\n",
      "2024-12-13 08:59:48.828000: I runner.py:310] Step = 125600 ; steps/s = 1.63, tokens/s = 78356 (33180 source, 45176 target) ; Learning rate = 0.000249 ; Loss = 1.625409\n",
      "2024-12-13 09:00:49.744000: I runner.py:310] Step = 125700 ; steps/s = 1.64, tokens/s = 77595 (32846 source, 44749 target) ; Learning rate = 0.000249 ; Loss = 1.623513\n",
      "2024-12-13 09:01:51.070000: I runner.py:310] Step = 125800 ; steps/s = 1.63, tokens/s = 78394 (33191 source, 45203 target) ; Learning rate = 0.000249 ; Loss = 1.599148\n",
      "2024-12-13 09:02:52.448000: I runner.py:310] Step = 125900 ; steps/s = 1.63, tokens/s = 78360 (33193 source, 45167 target) ; Learning rate = 0.000249 ; Loss = 1.613205\n",
      "2024-12-13 09:03:53.418000: I runner.py:310] Step = 126000 ; steps/s = 1.64, tokens/s = 77545 (32839 source, 44706 target) ; Learning rate = 0.000249 ; Loss = 1.607871\n",
      "2024-12-13 09:04:54.747000: I runner.py:310] Step = 126100 ; steps/s = 1.63, tokens/s = 78442 (33233 source, 45209 target) ; Learning rate = 0.000249 ; Loss = 1.614096\n",
      "2024-12-13 09:05:56.075000: I runner.py:310] Step = 126200 ; steps/s = 1.63, tokens/s = 78374 (33161 source, 45213 target) ; Learning rate = 0.000249 ; Loss = 1.611942\n",
      "2024-12-13 09:06:57.359000: I runner.py:310] Step = 126300 ; steps/s = 1.63, tokens/s = 78468 (33243 source, 45225 target) ; Learning rate = 0.000249 ; Loss = 1.607308\n",
      "2024-12-13 09:07:58.182000: I runner.py:310] Step = 126400 ; steps/s = 1.64, tokens/s = 77725 (32907 source, 44818 target) ; Learning rate = 0.000249 ; Loss = 1.595622\n",
      "2024-12-13 09:08:59.508000: I runner.py:310] Step = 126500 ; steps/s = 1.63, tokens/s = 78452 (33250 source, 45202 target) ; Learning rate = 0.000249 ; Loss = 1.610641\n",
      "2024-12-13 09:10:00.800000: I runner.py:310] Step = 126600 ; steps/s = 1.63, tokens/s = 78446 (33214 source, 45232 target) ; Learning rate = 0.000248 ; Loss = 1.618781\n",
      "2024-12-13 09:11:02.165000: I runner.py:310] Step = 126700 ; steps/s = 1.63, tokens/s = 78324 (33146 source, 45178 target) ; Learning rate = 0.000248 ; Loss = 1.626756\n",
      "2024-12-13 09:12:03.105000: I runner.py:310] Step = 126800 ; steps/s = 1.64, tokens/s = 77595 (32865 source, 44730 target) ; Learning rate = 0.000248 ; Loss = 1.602748\n",
      "2024-12-13 09:13:04.301000: I runner.py:310] Step = 126900 ; steps/s = 1.63, tokens/s = 78556 (33252 source, 45304 target) ; Learning rate = 0.000248 ; Loss = 1.609595\n",
      "2024-12-13 09:14:05.601000: I runner.py:310] Step = 127000 ; steps/s = 1.63, tokens/s = 78447 (33221 source, 45226 target) ; Learning rate = 0.000248 ; Loss = 1.619363\n",
      "2024-12-13 09:15:06.457000: I runner.py:310] Step = 127100 ; steps/s = 1.64, tokens/s = 77686 (32896 source, 44790 target) ; Learning rate = 0.000248 ; Loss = 1.604280\n",
      "2024-12-13 09:16:07.692000: I runner.py:310] Step = 127200 ; steps/s = 1.63, tokens/s = 78521 (33245 source, 45276 target) ; Learning rate = 0.000248 ; Loss = 1.619530\n",
      "2024-12-13 09:17:08.936000: I runner.py:310] Step = 127300 ; steps/s = 1.63, tokens/s = 78522 (33260 source, 45262 target) ; Learning rate = 0.000248 ; Loss = 1.614584\n",
      "2024-12-13 09:18:10.274000: I runner.py:310] Step = 127400 ; steps/s = 1.63, tokens/s = 78386 (33194 source, 45192 target) ; Learning rate = 0.000248 ; Loss = 1.618879\n",
      "2024-12-13 09:19:11.148000: I runner.py:310] Step = 127500 ; steps/s = 1.64, tokens/s = 77676 (32895 source, 44781 target) ; Learning rate = 0.000248 ; Loss = 1.599653\n",
      "2024-12-13 09:20:12.440000: I runner.py:310] Step = 127600 ; steps/s = 1.63, tokens/s = 78428 (33196 source, 45232 target) ; Learning rate = 0.000247 ; Loss = 1.610885\n",
      "2024-12-13 09:21:13.798000: I runner.py:310] Step = 127700 ; steps/s = 1.63, tokens/s = 78372 (33190 source, 45182 target) ; Learning rate = 0.000247 ; Loss = 1.611591\n",
      "2024-12-13 09:22:14.750000: I runner.py:310] Step = 127800 ; steps/s = 1.64, tokens/s = 77562 (32843 source, 44719 target) ; Learning rate = 0.000247 ; Loss = 1.608426\n",
      "2024-12-13 09:23:16.041000: I runner.py:310] Step = 127900 ; steps/s = 1.63, tokens/s = 78432 (33199 source, 45233 target) ; Learning rate = 0.000247 ; Loss = 1.614805\n",
      "2024-12-13 09:24:17.383000: I runner.py:310] Step = 128000 ; steps/s = 1.63, tokens/s = 78399 (33204 source, 45195 target) ; Learning rate = 0.000247 ; Loss = 1.616035\n",
      "2024-12-13 09:25:18.624000: I runner.py:310] Step = 128100 ; steps/s = 1.63, tokens/s = 78516 (33258 source, 45258 target) ; Learning rate = 0.000247 ; Loss = 1.603078\n",
      "2024-12-13 09:26:19.556000: I runner.py:310] Step = 128200 ; steps/s = 1.64, tokens/s = 77598 (32855 source, 44743 target) ; Learning rate = 0.000247 ; Loss = 1.598801\n",
      "2024-12-13 09:27:20.842000: I runner.py:310] Step = 128300 ; steps/s = 1.63, tokens/s = 78465 (33243 source, 45222 target) ; Learning rate = 0.000247 ; Loss = 1.616640\n",
      "2024-12-13 09:28:22.151000: I runner.py:310] Step = 128400 ; steps/s = 1.63, tokens/s = 78421 (33204 source, 45217 target) ; Learning rate = 0.000247 ; Loss = 1.615353\n",
      "2024-12-13 09:29:23.024000: I runner.py:310] Step = 128500 ; steps/s = 1.64, tokens/s = 77673 (32885 source, 44788 target) ; Learning rate = 0.000247 ; Loss = 1.610054\n",
      "2024-12-13 09:30:24.252000: I runner.py:310] Step = 128600 ; steps/s = 1.63, tokens/s = 78519 (33249 source, 45270 target) ; Learning rate = 0.000246 ; Loss = 1.624050\n",
      "2024-12-13 09:31:25.557000: I runner.py:310] Step = 128700 ; steps/s = 1.63, tokens/s = 78414 (33192 source, 45222 target) ; Learning rate = 0.000246 ; Loss = 1.608163\n",
      "2024-12-13 09:32:26.828000: I runner.py:310] Step = 128800 ; steps/s = 1.63, tokens/s = 78475 (33226 source, 45249 target) ; Learning rate = 0.000246 ; Loss = 1.610325\n",
      "2024-12-13 09:33:27.755000: I runner.py:310] Step = 128900 ; steps/s = 1.64, tokens/s = 77640 (32903 source, 44737 target) ; Learning rate = 0.000246 ; Loss = 1.610798\n",
      "2024-12-13 09:34:29.020000: I runner.py:310] Step = 129000 ; steps/s = 1.63, tokens/s = 78440 (33187 source, 45253 target) ; Learning rate = 0.000246 ; Loss = 1.609052\n",
      "2024-12-13 09:35:30.280000: I runner.py:310] Step = 129100 ; steps/s = 1.63, tokens/s = 78490 (33244 source, 45246 target) ; Learning rate = 0.000246 ; Loss = 1.603755\n",
      "2024-12-13 09:36:31.599000: I runner.py:310] Step = 129200 ; steps/s = 1.63, tokens/s = 78433 (33220 source, 45213 target) ; Learning rate = 0.000246 ; Loss = 1.613522\n",
      "2024-12-13 09:37:32.515000: I runner.py:310] Step = 129300 ; steps/s = 1.64, tokens/s = 77617 (32864 source, 44753 target) ; Learning rate = 0.000246 ; Loss = 1.594603\n",
      "2024-12-13 09:38:33.831000: I runner.py:310] Step = 129400 ; steps/s = 1.63, tokens/s = 78424 (33212 source, 45212 target) ; Learning rate = 0.000246 ; Loss = 1.613554\n",
      "2024-12-13 09:39:35.094000: I runner.py:310] Step = 129500 ; steps/s = 1.63, tokens/s = 78475 (33226 source, 45249 target) ; Learning rate = 0.000246 ; Loss = 1.616886\n",
      "2024-12-13 09:40:35.970000: I runner.py:310] Step = 129600 ; steps/s = 1.64, tokens/s = 77666 (32892 source, 44774 target) ; Learning rate = 0.000246 ; Loss = 1.602136\n",
      "2024-12-13 09:41:37.262000: I runner.py:310] Step = 129700 ; steps/s = 1.63, tokens/s = 78412 (33193 source, 45219 target) ; Learning rate = 0.000245 ; Loss = 1.610347\n",
      "2024-12-13 09:42:38.620000: I runner.py:310] Step = 129800 ; steps/s = 1.63, tokens/s = 78393 (33208 source, 45185 target) ; Learning rate = 0.000245 ; Loss = 1.612284\n",
      "2024-12-13 09:43:39.923000: I runner.py:310] Step = 129900 ; steps/s = 1.63, tokens/s = 78442 (33219 source, 45223 target) ; Learning rate = 0.000245 ; Loss = 1.622460\n",
      "2024-12-13 09:44:40.833000: I runner.py:310] Step = 130000 ; steps/s = 1.64, tokens/s = 77618 (32856 source, 44762 target) ; Learning rate = 0.000245 ; Loss = 1.592780\n",
      "2024-12-13 09:44:43.004000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-130000\n",
      "2024-12-13 09:44:43.004000: I training.py:192] Running evaluation for step 130000\n",
      "2024-12-13 09:46:49.523000: I training.py:192] Evaluation result for step 130000: loss = 2.761778 ; perplexity = 15.827967\n",
      "2024-12-13 09:47:50.649000: I runner.py:310] Step = 130100 ; steps/s = 1.64, tokens/s = 78676 (33319 source, 45357 target) ; Learning rate = 0.000245 ; Loss = 1.607337\n",
      "2024-12-13 09:48:51.885000: I runner.py:310] Step = 130200 ; steps/s = 1.63, tokens/s = 78551 (33280 source, 45271 target) ; Learning rate = 0.000245 ; Loss = 1.621816\n",
      "2024-12-13 09:49:52.872000: I runner.py:310] Step = 130300 ; steps/s = 1.64, tokens/s = 77517 (32815 source, 44702 target) ; Learning rate = 0.000245 ; Loss = 1.600721\n",
      "2024-12-13 09:50:54.132000: I runner.py:310] Step = 130400 ; steps/s = 1.63, tokens/s = 78510 (33268 source, 45242 target) ; Learning rate = 0.000245 ; Loss = 1.612346\n",
      "2024-12-13 09:51:55.399000: I runner.py:310] Step = 130500 ; steps/s = 1.63, tokens/s = 78474 (33219 source, 45255 target) ; Learning rate = 0.000245 ; Loss = 1.613517\n",
      "2024-12-13 09:52:56.744000: I runner.py:310] Step = 130600 ; steps/s = 1.63, tokens/s = 78352 (33162 source, 45190 target) ; Learning rate = 0.000245 ; Loss = 1.622805\n",
      "2024-12-13 09:53:57.661000: I runner.py:310] Step = 130700 ; steps/s = 1.64, tokens/s = 77617 (32859 source, 44758 target) ; Learning rate = 0.000244 ; Loss = 1.588959\n",
      "2024-12-13 09:54:58.993000: I runner.py:310] Step = 130800 ; steps/s = 1.63, tokens/s = 78411 (33219 source, 45192 target) ; Learning rate = 0.000244 ; Loss = 1.608630\n",
      "2024-12-13 09:56:00.242000: I runner.py:310] Step = 130900 ; steps/s = 1.63, tokens/s = 78520 (33257 source, 45263 target) ; Learning rate = 0.000244 ; Loss = 1.607156\n",
      "2024-12-13 09:57:01.443000: I runner.py:310] Step = 131000 ; steps/s = 1.63, tokens/s = 77981 (33009 source, 44972 target) ; Learning rate = 0.000244 ; Loss = 1.614506\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Tr-En (TED2020)(base model)\n",
    "!onmt-main --model kk-tr-en-shared.py --config tr-en.yml --auto_config train --with_eval --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e765433b-e7f2-4db7-8abf-709b4f9a86b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-13 10:40:57.470147: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-13 10:40:58.616259: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-13 10:40:58.616465: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-13 10:40:58.616476: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-13 10:41:00.263000: I main.py:308] Loading model description from TR-EN_std/model_description.py\n",
      "2024-12-13 10:41:00.467000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-13 10:41:00.467000: I main.py:315] Using model:\n",
      "(model): MyCustomTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): WordEmbedder()\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): WordEmbedder()\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-13 10:41:00.472000: I main.py:340] Using parameters:\n",
      "data:\n",
      "  eval_features_file: TED2020_tokens_dev\n",
      "  eval_labels_file: TED2020_dev_target_tokens.txt\n",
      "  source_vocabulary: tr_vocab.vocab\n",
      "  target_vocabulary: en_vocab.vocab\n",
      "  train_features_file: TED2020_tokens_train\n",
      "  train_labels_file: TED2020_train_target_tokens.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: TR-EN_std\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 250000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-13 10:41:00.668174: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-13 10:41:01.277148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-13 10:41:01.444000: I inputter.py:316] Initialized source input layer:\n",
      "2024-12-13 10:41:01.444000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-13 10:41:01.444000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-13 10:41:01.530000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-13 10:41:01.531000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-13 10:41:01.531000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-13 10:41:01.550000: I runner.py:462] Restored checkpoint TR-EN_std/ckpt-100000\n",
      "2024-12-13 10:41:01.588000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-13 10:41:02.109979: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-13 10:41:02.228000: I runner.py:471] Tracing and optimizing the inference graph...\n",
      "2024-12-13 10:41:15.791988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-13 10:41:16.699659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-13 10:41:30.151000: I runner.py:471] 1341 predictions are buffered, but waiting for the prediction of queued line 5 to advance the output...\n",
      "2024-12-13 10:41:49.108000: I runner.py:471] 3145 predictions are buffered, but waiting for the prediction of queued line 89 to advance the output...\n",
      "2024-12-13 10:41:59.363000: I runner.py:471] 4201 predictions are buffered, but waiting for the prediction of queued line 89 to advance the output...\n",
      "2024-12-13 10:42:09.525000: I runner.py:471] 5129 predictions are buffered, but waiting for the prediction of queued line 89 to advance the output...\n",
      "2024-12-13 10:42:19.916000: I runner.py:471] 6089 predictions are buffered, but waiting for the prediction of queued line 89 to advance the output...\n",
      "2024-12-13 10:42:30.028000: I runner.py:471] 7113 predictions are buffered, but waiting for the prediction of queued line 89 to advance the output...\n",
      "2024-12-13 10:42:40.102000: I runner.py:471] 8105 predictions are buffered, but waiting for the prediction of queued line 89 to advance the output...\n",
      "2024-12-13 10:42:50.280000: I runner.py:471] 9001 predictions are buffered, but waiting for the prediction of queued line 89 to advance the output...\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 onmt-main --config tr-en.yml --auto_config --checkpoint_path TR-EN_std/ckpt-100000 infer --features_file TED2020_tokens_test --predictions_file output_tr_en_std.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f61e5d7-1d0c-426f-90ce-982ea824b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 MT-Preparation/subwording/3-desubword.py en_vocab.model output_tr_en_std.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd9bca61-520e-4525-b56b-1b7b46d7ab96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference first sentence: It's quite widespread.\n",
      "Translated first sentence: This is very common .\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "BLEU2:  BLEU = 22.87 55.0/28.8/16.9/10.2 (BP = 1.000 ratio = 1.058 hyp_len = 209406 ref_len = 197923)\n",
      "CHRF:  chrF2 = 50.05\n"
     ]
    }
   ],
   "source": [
    "# BLEU and chrF scores\n",
    "!python3 compute-bleu.py TED2020.en-tr.en-filtered.en.test output_tr_en_std.txt.desubword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fb5b991-bcf5-4919-b433-a6c6a9f2773a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama METEOR Puan: 0.5538603230976438\n"
     ]
    }
   ],
   "source": [
    "# Average METEOR score (Ortalama METEOR Puan)\n",
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "def read_and_tokenize_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return [nltk.word_tokenize(line.strip()) for line in lines]\n",
    "\n",
    "def calculate_meteor(reference_file, hypothesis_file):\n",
    "    references = read_and_tokenize_file(reference_file)\n",
    "    hypotheses = read_and_tokenize_file(hypothesis_file)\n",
    "    \n",
    "    if len(references) != len(hypotheses):\n",
    "        raise ValueError(\"Dosyalarn satr saylar elemiyor\")\n",
    "\n",
    "    total_meteor_score = 0.0\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        total_meteor_score += meteor_score([ref], hyp)\n",
    "\n",
    "    average_meteor_score = total_meteor_score / len(references)\n",
    "    return average_meteor_score\n",
    "\n",
    "reference_file = 'TED2020.en-tr.en-filtered.en.test'\n",
    "hypothesis_file = 'output_tr_en_std.txt.desubword'\n",
    "\n",
    "score = calculate_meteor(reference_file, hypothesis_file)\n",
    "print(f\"Ortalama METEOR Puan: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e44696d2-675b-49be-8c65-baf2fe240c4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-13 10:48:12.354368: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-13 10:48:13.140151: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-13 10:48:13.140217: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-13 10:48:13.140225: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-13 10:48:14.128000: I onmt-main:8] Creating model directory POS_TR_EN\n",
      "2024-12-13 10:48:14.337000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-13 10:48:14.337000: I main.py:315] Using model:\n",
      "(model): MultiFeaturesTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): ParallelInputter(\n",
      "      (inputters): ListWrapper(\n",
      "        (0): WordEmbedder()\n",
      "        (1): WordEmbedder()\n",
      "      )\n",
      "      (reducer): ConcatReducer()\n",
      "    )\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): ParallelInputter(\n",
      "        (inputters): ListWrapper(\n",
      "          (0): WordEmbedder()\n",
      "          (1): WordEmbedder()\n",
      "        )\n",
      "        (reducer): ConcatReducer()\n",
      "      )\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-13 10:48:14.340198: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-13 10:48:14.935616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6099 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "2024-12-13 10:48:14.939000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file:\n",
      "  - TED2020_tokens_dev\n",
      "  - TED2020_pos_tags_dev.txt\n",
      "  eval_labels_file: TED2020_dev_target_tokens.txt\n",
      "  source_1_vocabulary: tr_vocab.vocab\n",
      "  source_2_vocabulary: Zemberek_TR_unique_pos\n",
      "  target_vocabulary: en_vocab.vocab\n",
      "  train_features_file:\n",
      "  - TED2020_tokens_train\n",
      "  - TED2020_pos_tags_train.txt\n",
      "  train_labels_file: TED2020_train_target_tokens.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: POS_TR_EN\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 205000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-13 10:48:15.244000: I inputter.py:316] Initialized source_1 input layer:\n",
      "2024-12-13 10:48:15.244000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-13 10:48:15.245000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-13 10:48:15.250000: I inputter.py:316] Initialized source_2 input layer:\n",
      "2024-12-13 10:48:15.250000: I inputter.py:316]  - vocabulary size: 21\n",
      "2024-12-13 10:48:15.250000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-13 10:48:15.323000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-13 10:48:15.324000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-13 10:48:15.324000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-13 10:48:15.328000: W runner.py:269] No checkpoint to restore in POS_TR_EN\n",
      "2024-12-13 10:48:15.328000: I main.py:325] Accumulate gradients of 13 iterations to reach effective batch size of 25000\n",
      "2024-12-13 10:48:15.558000: I dataset_ops.py:2542] Training on 337547 examples\n",
      "2024-12-13 10:48:15.581000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-13 10:48:39.832450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-13 10:48:40.730289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-13 10:48:45.760000: I runner.py:310] Number of model parameters: 93357261\n",
      "2024-12-13 10:48:45.765000: I runner.py:310] Number of model weights: 261 (trainable = 261, non trainable = 0)\n",
      "2024-12-13 10:48:48.389000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-1\n",
      "2024-12-13 10:51:05.941000: I runner.py:310] Step = 100 ; steps/s = 0.72, tokens/s = 18561 (18561 target) ; Learning rate = 0.000009 ; Loss = 9.775434\n",
      "2024-12-13 10:53:24.258000: I runner.py:310] Step = 200 ; steps/s = 0.72, tokens/s = 18604 (18604 target) ; Learning rate = 0.000018 ; Loss = 8.866173\n",
      "2024-12-13 10:55:42.937000: I runner.py:310] Step = 300 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000027 ; Loss = 7.734450\n",
      "2024-12-13 10:58:00.531000: I runner.py:310] Step = 400 ; steps/s = 0.73, tokens/s = 18373 (18373 target) ; Learning rate = 0.000035 ; Loss = 6.937834\n",
      "2024-12-13 11:00:19.213000: I runner.py:310] Step = 500 ; steps/s = 0.72, tokens/s = 18562 (18562 target) ; Learning rate = 0.000044 ; Loss = 6.548125\n",
      "2024-12-13 11:02:37.924000: I runner.py:310] Step = 600 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000053 ; Loss = 6.172677\n",
      "2024-12-13 11:04:56.598000: I runner.py:310] Step = 700 ; steps/s = 0.72, tokens/s = 18562 (18562 target) ; Learning rate = 0.000062 ; Loss = 5.928043\n",
      "2024-12-13 11:07:13.416000: I runner.py:310] Step = 800 ; steps/s = 0.73, tokens/s = 18473 (18473 target) ; Learning rate = 0.000071 ; Loss = 5.761339\n",
      "2024-12-13 11:09:32.111000: I runner.py:310] Step = 900 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000080 ; Loss = 5.630996\n",
      "2024-12-13 11:11:50.892000: I runner.py:310] Step = 1000 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000088 ; Loss = 5.632935\n",
      "2024-12-13 11:14:09.582000: I runner.py:310] Step = 1100 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000097 ; Loss = 5.416084\n",
      "2024-12-13 11:16:26.251000: I runner.py:310] Step = 1200 ; steps/s = 0.73, tokens/s = 18493 (18493 target) ; Learning rate = 0.000106 ; Loss = 5.341707\n",
      "2024-12-13 11:18:44.981000: I runner.py:310] Step = 1300 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000115 ; Loss = 5.241650\n",
      "2024-12-13 11:21:03.658000: I runner.py:310] Step = 1400 ; steps/s = 0.72, tokens/s = 18561 (18561 target) ; Learning rate = 0.000124 ; Loss = 5.171624\n",
      "2024-12-13 11:23:22.388000: I runner.py:310] Step = 1500 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000133 ; Loss = 5.138145\n",
      "2024-12-13 11:25:39.066000: I runner.py:310] Step = 1600 ; steps/s = 0.73, tokens/s = 18490 (18490 target) ; Learning rate = 0.000142 ; Loss = 5.013011\n",
      "2024-12-13 11:27:57.825000: I runner.py:310] Step = 1700 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000150 ; Loss = 5.096311\n",
      "2024-12-13 11:30:16.550000: I runner.py:310] Step = 1800 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000159 ; Loss = 4.998603\n",
      "2024-12-13 11:32:35.263000: I runner.py:310] Step = 1900 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000168 ; Loss = 4.921333\n",
      "2024-12-13 11:34:51.972000: I runner.py:310] Step = 2000 ; steps/s = 0.73, tokens/s = 18491 (18491 target) ; Learning rate = 0.000177 ; Loss = 4.644180\n",
      "2024-12-13 11:37:10.750000: I runner.py:310] Step = 2100 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000186 ; Loss = 4.644992\n",
      "2024-12-13 11:39:29.528000: I runner.py:310] Step = 2200 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000195 ; Loss = 4.646056\n",
      "2024-12-13 11:41:48.254000: I runner.py:310] Step = 2300 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000203 ; Loss = 4.498656\n",
      "2024-12-13 11:44:04.735000: I runner.py:310] Step = 2400 ; steps/s = 0.73, tokens/s = 18517 (18517 target) ; Learning rate = 0.000212 ; Loss = 4.393954\n",
      "2024-12-13 11:46:23.360000: I runner.py:310] Step = 2500 ; steps/s = 0.72, tokens/s = 18570 (18570 target) ; Learning rate = 0.000221 ; Loss = 4.143408\n",
      "2024-12-13 11:48:42.117000: I runner.py:310] Step = 2600 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000230 ; Loss = 4.230050\n",
      "2024-12-13 11:50:58.808000: I runner.py:310] Step = 2700 ; steps/s = 0.73, tokens/s = 18493 (18493 target) ; Learning rate = 0.000239 ; Loss = 4.154609\n",
      "2024-12-13 11:53:17.521000: I runner.py:310] Step = 2800 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000248 ; Loss = 4.073403\n",
      "2024-12-13 11:55:36.242000: I runner.py:310] Step = 2900 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000256 ; Loss = 3.873564\n",
      "2024-12-13 11:57:54.985000: I runner.py:310] Step = 3000 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000265 ; Loss = 3.796604\n",
      "2024-12-13 12:00:11.668000: I runner.py:310] Step = 3100 ; steps/s = 0.73, tokens/s = 18495 (18495 target) ; Learning rate = 0.000274 ; Loss = 3.691045\n",
      "2024-12-13 12:02:30.380000: I runner.py:310] Step = 3200 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000283 ; Loss = 3.659086\n",
      "2024-12-13 12:04:49.188000: I runner.py:310] Step = 3300 ; steps/s = 0.72, tokens/s = 18544 (18544 target) ; Learning rate = 0.000292 ; Loss = 3.673922\n",
      "2024-12-13 12:07:07.898000: I runner.py:310] Step = 3400 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000301 ; Loss = 3.596848\n",
      "2024-12-13 12:09:24.572000: I runner.py:310] Step = 3500 ; steps/s = 0.73, tokens/s = 18495 (18495 target) ; Learning rate = 0.000309 ; Loss = 3.441527\n",
      "2024-12-13 12:11:43.374000: I runner.py:310] Step = 3600 ; steps/s = 0.72, tokens/s = 18544 (18544 target) ; Learning rate = 0.000318 ; Loss = 3.572264\n",
      "2024-12-13 12:14:02.135000: I runner.py:310] Step = 3700 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000327 ; Loss = 3.564594\n",
      "2024-12-13 12:16:20.875000: I runner.py:310] Step = 3800 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000336 ; Loss = 3.338404\n",
      "2024-12-13 12:18:37.546000: I runner.py:310] Step = 3900 ; steps/s = 0.73, tokens/s = 18494 (18494 target) ; Learning rate = 0.000345 ; Loss = 3.332702\n",
      "2024-12-13 12:20:56.282000: I runner.py:310] Step = 4000 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000354 ; Loss = 3.421404\n",
      "2024-12-13 12:23:15.010000: I runner.py:310] Step = 4100 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000362 ; Loss = 3.349796\n",
      "2024-12-13 12:25:33.757000: I runner.py:310] Step = 4200 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000371 ; Loss = 3.371076\n",
      "2024-12-13 12:27:50.410000: I runner.py:310] Step = 4300 ; steps/s = 0.73, tokens/s = 18494 (18494 target) ; Learning rate = 0.000380 ; Loss = 3.207390\n",
      "2024-12-13 12:30:09.130000: I runner.py:310] Step = 4400 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000389 ; Loss = 3.247808\n",
      "2024-12-13 12:32:27.949000: I runner.py:310] Step = 4500 ; steps/s = 0.72, tokens/s = 18540 (18540 target) ; Learning rate = 0.000398 ; Loss = 3.245200\n",
      "2024-12-13 12:34:46.673000: I runner.py:310] Step = 4600 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000407 ; Loss = 3.246391\n",
      "2024-12-13 12:37:03.195000: I runner.py:310] Step = 4700 ; steps/s = 0.73, tokens/s = 18510 (18510 target) ; Learning rate = 0.000416 ; Loss = 3.123114\n",
      "2024-12-13 12:39:21.800000: I runner.py:310] Step = 4800 ; steps/s = 0.72, tokens/s = 18571 (18571 target) ; Learning rate = 0.000424 ; Loss = 3.101570\n",
      "2024-12-13 12:41:40.560000: I runner.py:310] Step = 4900 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000433 ; Loss = 3.181023\n",
      "2024-12-13 12:43:59.320000: I runner.py:310] Step = 5000 ; steps/s = 0.72, tokens/s = 18548 (18548 target) ; Learning rate = 0.000442 ; Loss = 3.147417\n",
      "2024-12-13 12:46:15.971000: I runner.py:310] Step = 5100 ; steps/s = 0.73, tokens/s = 18498 (18498 target) ; Learning rate = 0.000451 ; Loss = 3.125856\n",
      "2024-12-13 12:48:34.727000: I runner.py:310] Step = 5200 ; steps/s = 0.72, tokens/s = 18548 (18548 target) ; Learning rate = 0.000460 ; Loss = 3.124594\n",
      "2024-12-13 12:50:53.458000: I runner.py:310] Step = 5300 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000469 ; Loss = 3.030165\n",
      "2024-12-13 12:53:10.151000: I runner.py:310] Step = 5400 ; steps/s = 0.73, tokens/s = 18492 (18492 target) ; Learning rate = 0.000477 ; Loss = 2.962155\n",
      "2024-12-13 12:55:28.896000: I runner.py:310] Step = 5500 ; steps/s = 0.72, tokens/s = 18553 (18553 target) ; Learning rate = 0.000486 ; Loss = 2.982327\n",
      "2024-12-13 12:57:47.622000: I runner.py:310] Step = 5600 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000495 ; Loss = 2.948902\n",
      "2024-12-13 13:00:06.406000: I runner.py:310] Step = 5700 ; steps/s = 0.72, tokens/s = 18546 (18546 target) ; Learning rate = 0.000504 ; Loss = 3.037379\n",
      "2024-12-13 13:02:23.092000: I runner.py:310] Step = 5800 ; steps/s = 0.73, tokens/s = 18493 (18493 target) ; Learning rate = 0.000513 ; Loss = 3.012193\n",
      "2024-12-13 13:04:41.820000: I runner.py:310] Step = 5900 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000522 ; Loss = 2.891427\n",
      "2024-12-13 13:07:00.584000: I runner.py:310] Step = 6000 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000530 ; Loss = 2.886436\n",
      "2024-12-13 13:09:19.352000: I runner.py:310] Step = 6100 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000539 ; Loss = 2.911131\n",
      "2024-12-13 13:11:36.024000: I runner.py:310] Step = 6200 ; steps/s = 0.73, tokens/s = 18496 (18496 target) ; Learning rate = 0.000548 ; Loss = 2.891051\n",
      "2024-12-13 13:13:54.779000: I runner.py:310] Step = 6300 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000557 ; Loss = 2.868072\n",
      "2024-12-13 13:16:13.496000: I runner.py:310] Step = 6400 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000566 ; Loss = 2.816814\n",
      "2024-12-13 13:18:32.250000: I runner.py:310] Step = 6500 ; steps/s = 0.72, tokens/s = 18548 (18548 target) ; Learning rate = 0.000575 ; Loss = 2.873796\n",
      "2024-12-13 13:20:48.891000: I runner.py:310] Step = 6600 ; steps/s = 0.73, tokens/s = 18503 (18503 target) ; Learning rate = 0.000583 ; Loss = 2.805155\n",
      "2024-12-13 13:23:07.635000: I runner.py:310] Step = 6700 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000592 ; Loss = 2.782078\n",
      "2024-12-13 13:25:26.386000: I runner.py:310] Step = 6800 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000601 ; Loss = 2.819884\n",
      "2024-12-13 13:27:45.112000: I runner.py:310] Step = 6900 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000610 ; Loss = 2.829247\n",
      "2024-12-13 13:30:01.631000: I runner.py:310] Step = 7000 ; steps/s = 0.73, tokens/s = 18516 (18516 target) ; Learning rate = 0.000619 ; Loss = 2.730977\n",
      "2024-12-13 13:32:20.038000: I runner.py:310] Step = 7100 ; steps/s = 0.72, tokens/s = 18596 (18596 target) ; Learning rate = 0.000628 ; Loss = 2.742531\n",
      "2024-12-13 13:34:38.797000: I runner.py:310] Step = 7200 ; steps/s = 0.72, tokens/s = 18546 (18546 target) ; Learning rate = 0.000636 ; Loss = 2.748587\n",
      "2024-12-13 13:36:57.537000: I runner.py:310] Step = 7300 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000645 ; Loss = 2.849894\n",
      "2024-12-13 13:39:14.185000: I runner.py:310] Step = 7400 ; steps/s = 0.73, tokens/s = 18497 (18497 target) ; Learning rate = 0.000654 ; Loss = 2.685831\n",
      "2024-12-13 13:41:32.944000: I runner.py:310] Step = 7500 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000663 ; Loss = 2.786447\n",
      "2024-12-13 13:43:51.680000: I runner.py:310] Step = 7600 ; steps/s = 0.72, tokens/s = 18553 (18553 target) ; Learning rate = 0.000672 ; Loss = 2.757775\n",
      "2024-12-13 13:46:10.383000: I runner.py:310] Step = 7700 ; steps/s = 0.72, tokens/s = 18563 (18563 target) ; Learning rate = 0.000681 ; Loss = 2.747680\n",
      "2024-12-13 13:48:27.064000: I runner.py:310] Step = 7800 ; steps/s = 0.73, tokens/s = 18494 (18494 target) ; Learning rate = 0.000690 ; Loss = 2.720561\n",
      "2024-12-13 13:50:45.763000: I runner.py:310] Step = 7900 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000698 ; Loss = 2.662848\n",
      "2024-12-13 13:53:04.493000: I runner.py:310] Step = 8000 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000707 ; Loss = 2.698745\n",
      "2024-12-13 13:55:21.190000: I runner.py:310] Step = 8100 ; steps/s = 0.73, tokens/s = 18493 (18493 target) ; Learning rate = 0.000716 ; Loss = 2.707084\n",
      "2024-12-13 13:57:40.011000: I runner.py:310] Step = 8200 ; steps/s = 0.72, tokens/s = 18543 (18543 target) ; Learning rate = 0.000725 ; Loss = 2.712034\n",
      "2024-12-13 13:59:58.780000: I runner.py:310] Step = 8300 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000734 ; Loss = 2.618925\n",
      "2024-12-13 14:02:17.544000: I runner.py:310] Step = 8400 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000743 ; Loss = 2.676903\n",
      "2024-12-13 14:04:34.241000: I runner.py:310] Step = 8500 ; steps/s = 0.73, tokens/s = 18489 (18489 target) ; Learning rate = 0.000751 ; Loss = 2.560520\n",
      "2024-12-13 14:06:52.981000: I runner.py:310] Step = 8600 ; steps/s = 0.72, tokens/s = 18553 (18553 target) ; Learning rate = 0.000760 ; Loss = 2.592637\n",
      "2024-12-13 14:09:11.714000: I runner.py:310] Step = 8700 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000769 ; Loss = 2.682729\n",
      "2024-12-13 14:11:30.460000: I runner.py:310] Step = 8800 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000778 ; Loss = 2.700950\n",
      "2024-12-13 14:13:47.118000: I runner.py:310] Step = 8900 ; steps/s = 0.73, tokens/s = 18494 (18494 target) ; Learning rate = 0.000787 ; Loss = 2.536549\n",
      "2024-12-13 14:16:05.852000: I runner.py:310] Step = 9000 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000796 ; Loss = 2.603474\n",
      "2024-12-13 14:18:24.577000: I runner.py:310] Step = 9100 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000804 ; Loss = 2.627190\n",
      "2024-12-13 14:20:43.254000: I runner.py:310] Step = 9200 ; steps/s = 0.72, tokens/s = 18562 (18562 target) ; Learning rate = 0.000813 ; Loss = 2.679564\n",
      "2024-12-13 14:22:59.824000: I runner.py:310] Step = 9300 ; steps/s = 0.73, tokens/s = 18508 (18508 target) ; Learning rate = 0.000822 ; Loss = 2.542122\n",
      "2024-12-13 14:25:18.299000: I runner.py:310] Step = 9400 ; steps/s = 0.72, tokens/s = 18588 (18588 target) ; Learning rate = 0.000831 ; Loss = 2.541703\n",
      "2024-12-13 14:27:37.089000: I runner.py:310] Step = 9500 ; steps/s = 0.72, tokens/s = 18543 (18543 target) ; Learning rate = 0.000840 ; Loss = 2.583255\n",
      "2024-12-13 14:29:55.902000: I runner.py:310] Step = 9600 ; steps/s = 0.72, tokens/s = 18543 (18543 target) ; Learning rate = 0.000849 ; Loss = 2.612289\n",
      "2024-12-13 14:32:12.606000: I runner.py:310] Step = 9700 ; steps/s = 0.73, tokens/s = 18488 (18488 target) ; Learning rate = 0.000857 ; Loss = 2.547391\n",
      "2024-12-13 14:34:31.310000: I runner.py:310] Step = 9800 ; steps/s = 0.72, tokens/s = 18560 (18560 target) ; Learning rate = 0.000866 ; Loss = 2.592908\n",
      "2024-12-13 14:36:49.983000: I runner.py:310] Step = 9900 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000875 ; Loss = 2.530021\n",
      "2024-12-13 14:39:08.786000: I runner.py:310] Step = 10000 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000884 ; Loss = 2.547170\n",
      "2024-12-13 14:39:10.845000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-10000\n",
      "2024-12-13 14:41:27.413000: I runner.py:310] Step = 10100 ; steps/s = 0.73, tokens/s = 18511 (18511 target) ; Learning rate = 0.000879 ; Loss = 2.575147\n",
      "2024-12-13 14:43:46.117000: I runner.py:310] Step = 10200 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000875 ; Loss = 2.492494\n",
      "2024-12-13 14:46:04.845000: I runner.py:310] Step = 10300 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000871 ; Loss = 2.551951\n",
      "2024-12-13 14:48:23.544000: I runner.py:310] Step = 10400 ; steps/s = 0.72, tokens/s = 18562 (18562 target) ; Learning rate = 0.000867 ; Loss = 2.538902\n",
      "2024-12-13 14:50:40.176000: I runner.py:310] Step = 10500 ; steps/s = 0.73, tokens/s = 18499 (18499 target) ; Learning rate = 0.000863 ; Loss = 2.449115\n",
      "2024-12-13 14:52:58.959000: I runner.py:310] Step = 10600 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000858 ; Loss = 2.495574\n",
      "2024-12-13 14:55:17.713000: I runner.py:310] Step = 10700 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000854 ; Loss = 2.527408\n",
      "2024-12-13 14:57:34.434000: I runner.py:310] Step = 10800 ; steps/s = 0.73, tokens/s = 18490 (18490 target) ; Learning rate = 0.000850 ; Loss = 2.458453\n",
      "2024-12-13 14:59:53.200000: I runner.py:310] Step = 10900 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000847 ; Loss = 2.411251\n",
      "2024-12-13 15:02:11.950000: I runner.py:310] Step = 11000 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000843 ; Loss = 2.505393\n",
      "2024-12-13 15:04:30.726000: I runner.py:310] Step = 11100 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000839 ; Loss = 2.488771\n",
      "2024-12-13 15:06:47.471000: I runner.py:310] Step = 11200 ; steps/s = 0.73, tokens/s = 18484 (18484 target) ; Learning rate = 0.000835 ; Loss = 2.428846\n",
      "2024-12-13 15:09:06.173000: I runner.py:310] Step = 11300 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000831 ; Loss = 2.402608\n",
      "2024-12-13 15:11:24.896000: I runner.py:310] Step = 11400 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000828 ; Loss = 2.468831\n",
      "2024-12-13 15:13:43.587000: I runner.py:310] Step = 11500 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000824 ; Loss = 2.471839\n",
      "2024-12-13 15:16:00.229000: I runner.py:310] Step = 11600 ; steps/s = 0.73, tokens/s = 18493 (18493 target) ; Learning rate = 0.000821 ; Loss = 2.407121\n",
      "2024-12-13 15:18:18.688000: I runner.py:310] Step = 11700 ; steps/s = 0.72, tokens/s = 18589 (18589 target) ; Learning rate = 0.000817 ; Loss = 2.393236\n",
      "2024-12-13 15:20:37.457000: I runner.py:310] Step = 11800 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000814 ; Loss = 2.406578\n",
      "2024-12-13 15:22:56.159000: I runner.py:310] Step = 11900 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000810 ; Loss = 2.425387\n",
      "2024-12-13 15:25:12.885000: I runner.py:310] Step = 12000 ; steps/s = 0.73, tokens/s = 18487 (18487 target) ; Learning rate = 0.000807 ; Loss = 2.295937\n",
      "2024-12-13 15:27:31.657000: I runner.py:310] Step = 12100 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000803 ; Loss = 2.381655\n",
      "2024-12-13 15:29:50.369000: I runner.py:310] Step = 12200 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000800 ; Loss = 2.394642\n",
      "2024-12-13 15:32:09.156000: I runner.py:310] Step = 12300 ; steps/s = 0.72, tokens/s = 18546 (18546 target) ; Learning rate = 0.000797 ; Loss = 2.432606\n",
      "2024-12-13 15:34:25.820000: I runner.py:310] Step = 12400 ; steps/s = 0.73, tokens/s = 18491 (18491 target) ; Learning rate = 0.000794 ; Loss = 2.387455\n",
      "2024-12-13 15:36:44.532000: I runner.py:310] Step = 12500 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000791 ; Loss = 2.348443\n",
      "2024-12-13 15:39:03.254000: I runner.py:310] Step = 12600 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000787 ; Loss = 2.379900\n",
      "2024-12-13 15:41:22.015000: I runner.py:310] Step = 12700 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000784 ; Loss = 2.373337\n",
      "2024-12-13 15:43:38.646000: I runner.py:310] Step = 12800 ; steps/s = 0.73, tokens/s = 18502 (18502 target) ; Learning rate = 0.000781 ; Loss = 2.396866\n",
      "2024-12-13 15:45:57.349000: I runner.py:310] Step = 12900 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000778 ; Loss = 2.328263\n",
      "2024-12-13 15:48:16.053000: I runner.py:310] Step = 13000 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000775 ; Loss = 2.311393\n",
      "2024-12-13 15:50:34.815000: I runner.py:310] Step = 13100 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000772 ; Loss = 2.352775\n",
      "2024-12-13 15:52:51.565000: I runner.py:310] Step = 13200 ; steps/s = 0.73, tokens/s = 18481 (18481 target) ; Learning rate = 0.000769 ; Loss = 2.267734\n",
      "2024-12-13 15:55:10.303000: I runner.py:310] Step = 13300 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000766 ; Loss = 2.321128\n",
      "2024-12-13 15:57:29.006000: I runner.py:310] Step = 13400 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000764 ; Loss = 2.345656\n",
      "2024-12-13 15:59:45.715000: I runner.py:310] Step = 13500 ; steps/s = 0.73, tokens/s = 18492 (18492 target) ; Learning rate = 0.000761 ; Loss = 2.297240\n",
      "2024-12-13 16:02:04.508000: I runner.py:310] Step = 13600 ; steps/s = 0.72, tokens/s = 18546 (18546 target) ; Learning rate = 0.000758 ; Loss = 2.300918\n",
      "2024-12-13 16:04:23.264000: I runner.py:310] Step = 13700 ; steps/s = 0.72, tokens/s = 18553 (18553 target) ; Learning rate = 0.000755 ; Loss = 2.291499\n",
      "2024-12-13 16:06:42.050000: I runner.py:310] Step = 13800 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000752 ; Loss = 2.309127\n",
      "2024-12-13 16:08:58.681000: I runner.py:310] Step = 13900 ; steps/s = 0.73, tokens/s = 18500 (18500 target) ; Learning rate = 0.000750 ; Loss = 2.249926\n",
      "2024-12-13 16:11:17.206000: I runner.py:310] Step = 14000 ; steps/s = 0.72, tokens/s = 18582 (18582 target) ; Learning rate = 0.000747 ; Loss = 2.266664\n",
      "2024-12-13 16:13:35.998000: I runner.py:310] Step = 14100 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000744 ; Loss = 2.294674\n",
      "2024-12-13 16:15:54.758000: I runner.py:310] Step = 14200 ; steps/s = 0.72, tokens/s = 18548 (18548 target) ; Learning rate = 0.000742 ; Loss = 2.257423\n",
      "2024-12-13 16:18:11.420000: I runner.py:310] Step = 14300 ; steps/s = 0.73, tokens/s = 18492 (18492 target) ; Learning rate = 0.000739 ; Loss = 2.257728\n",
      "2024-12-13 16:20:30.187000: I runner.py:310] Step = 14400 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000737 ; Loss = 2.251212\n",
      "2024-12-13 16:22:48.950000: I runner.py:310] Step = 14500 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000734 ; Loss = 2.260659\n",
      "2024-12-13 16:25:07.684000: I runner.py:310] Step = 14600 ; steps/s = 0.72, tokens/s = 18553 (18553 target) ; Learning rate = 0.000731 ; Loss = 2.300549\n",
      "2024-12-13 16:27:24.362000: I runner.py:310] Step = 14700 ; steps/s = 0.73, tokens/s = 18496 (18496 target) ; Learning rate = 0.000729 ; Loss = 2.216820\n",
      "2024-12-13 16:29:43.100000: I runner.py:310] Step = 14800 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000727 ; Loss = 2.256138\n",
      "2024-12-13 16:32:01.771000: I runner.py:310] Step = 14900 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000724 ; Loss = 2.268699\n",
      "2024-12-13 16:34:20.539000: I runner.py:310] Step = 15000 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000722 ; Loss = 2.305722\n",
      "2024-12-13 16:36:37.172000: I runner.py:310] Step = 15100 ; steps/s = 0.73, tokens/s = 18500 (18500 target) ; Learning rate = 0.000719 ; Loss = 2.181488\n",
      "2024-12-13 16:38:55.895000: I runner.py:310] Step = 15200 ; steps/s = 0.72, tokens/s = 18553 (18553 target) ; Learning rate = 0.000717 ; Loss = 2.231843\n",
      "2024-12-13 16:41:14.627000: I runner.py:310] Step = 15300 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000715 ; Loss = 2.271856\n",
      "2024-12-13 16:43:33.312000: I runner.py:310] Step = 15400 ; steps/s = 0.72, tokens/s = 18563 (18563 target) ; Learning rate = 0.000712 ; Loss = 2.283983\n",
      "2024-12-13 16:45:49.980000: I runner.py:310] Step = 15500 ; steps/s = 0.73, tokens/s = 18493 (18493 target) ; Learning rate = 0.000710 ; Loss = 2.161999\n",
      "2024-12-13 16:48:08.727000: I runner.py:310] Step = 15600 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000708 ; Loss = 2.207873\n",
      "2024-12-13 16:50:27.430000: I runner.py:310] Step = 15700 ; steps/s = 0.72, tokens/s = 18560 (18560 target) ; Learning rate = 0.000705 ; Loss = 2.241560\n",
      "2024-12-13 16:52:46.110000: I runner.py:310] Step = 15800 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000703 ; Loss = 2.250952\n",
      "2024-12-13 16:55:02.813000: I runner.py:310] Step = 15900 ; steps/s = 0.73, tokens/s = 18489 (18489 target) ; Learning rate = 0.000701 ; Loss = 2.213821\n",
      "2024-12-13 16:57:21.631000: I runner.py:310] Step = 16000 ; steps/s = 0.72, tokens/s = 18542 (18542 target) ; Learning rate = 0.000699 ; Loss = 2.182442\n",
      "2024-12-13 16:59:40.265000: I runner.py:310] Step = 16100 ; steps/s = 0.72, tokens/s = 18569 (18569 target) ; Learning rate = 0.000697 ; Loss = 2.217407\n",
      "2024-12-13 17:01:56.852000: I runner.py:310] Step = 16200 ; steps/s = 0.73, tokens/s = 18505 (18505 target) ; Learning rate = 0.000694 ; Loss = 2.201618\n",
      "2024-12-13 17:04:15.296000: I runner.py:310] Step = 16300 ; steps/s = 0.72, tokens/s = 18590 (18590 target) ; Learning rate = 0.000692 ; Loss = 2.207864\n",
      "2024-12-13 17:06:34.060000: I runner.py:310] Step = 16400 ; steps/s = 0.72, tokens/s = 18548 (18548 target) ; Learning rate = 0.000690 ; Loss = 2.172327\n",
      "2024-12-13 17:08:52.734000: I runner.py:310] Step = 16500 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000688 ; Loss = 2.205999\n",
      "2024-12-13 17:11:09.367000: I runner.py:310] Step = 16600 ; steps/s = 0.73, tokens/s = 18498 (18498 target) ; Learning rate = 0.000686 ; Loss = 2.183420\n",
      "2024-12-13 17:13:28.086000: I runner.py:310] Step = 16700 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000684 ; Loss = 2.185365\n",
      "2024-12-13 17:15:46.844000: I runner.py:310] Step = 16800 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000682 ; Loss = 2.186294\n",
      "2024-12-13 17:18:05.546000: I runner.py:310] Step = 16900 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000680 ; Loss = 2.173245\n",
      "2024-12-13 17:20:22.258000: I runner.py:310] Step = 17000 ; steps/s = 0.73, tokens/s = 18489 (18489 target) ; Learning rate = 0.000678 ; Loss = 2.127466\n",
      "2024-12-13 17:22:41.038000: I runner.py:310] Step = 17100 ; steps/s = 0.72, tokens/s = 18542 (18542 target) ; Learning rate = 0.000676 ; Loss = 2.159062\n",
      "2024-12-13 17:24:59.803000: I runner.py:310] Step = 17200 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000674 ; Loss = 2.162636\n",
      "2024-12-13 17:27:18.579000: I runner.py:310] Step = 17300 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000672 ; Loss = 2.210667\n",
      "2024-12-13 17:29:35.295000: I runner.py:310] Step = 17400 ; steps/s = 0.73, tokens/s = 18491 (18491 target) ; Learning rate = 0.000670 ; Loss = 2.120235\n",
      "2024-12-13 17:31:53.991000: I runner.py:310] Step = 17500 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000668 ; Loss = 2.163924\n",
      "2024-12-13 17:34:12.695000: I runner.py:310] Step = 17600 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000666 ; Loss = 2.177956\n",
      "2024-12-13 17:36:31.415000: I runner.py:310] Step = 17700 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000664 ; Loss = 2.216857\n",
      "2024-12-13 17:38:48.064000: I runner.py:310] Step = 17800 ; steps/s = 0.73, tokens/s = 18496 (18496 target) ; Learning rate = 0.000662 ; Loss = 2.168279\n",
      "2024-12-13 17:41:06.795000: I runner.py:310] Step = 17900 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000661 ; Loss = 2.133257\n",
      "2024-12-13 17:43:25.580000: I runner.py:310] Step = 18000 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000659 ; Loss = 2.131986\n",
      "2024-12-13 17:45:44.402000: I runner.py:310] Step = 18100 ; steps/s = 0.72, tokens/s = 18541 (18541 target) ; Learning rate = 0.000657 ; Loss = 2.156993\n",
      "2024-12-13 17:48:01.048000: I runner.py:310] Step = 18200 ; steps/s = 0.73, tokens/s = 18497 (18497 target) ; Learning rate = 0.000655 ; Loss = 2.149084\n",
      "2024-12-13 17:50:19.784000: I runner.py:310] Step = 18300 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000653 ; Loss = 2.096432\n",
      "2024-12-13 17:52:38.649000: I runner.py:310] Step = 18400 ; steps/s = 0.72, tokens/s = 18534 (18534 target) ; Learning rate = 0.000652 ; Loss = 2.121188\n",
      "2024-12-13 17:54:56.610000: I runner.py:310] Step = 18500 ; steps/s = 0.72, tokens/s = 18532 (18532 target) ; Learning rate = 0.000650 ; Loss = 2.154643\n",
      "2024-12-13 17:57:13.677000: I runner.py:310] Step = 18600 ; steps/s = 0.73, tokens/s = 18570 (18570 target) ; Learning rate = 0.000648 ; Loss = 2.066201\n",
      "2024-12-13 17:59:32.455000: I runner.py:310] Step = 18700 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000646 ; Loss = 2.173226\n",
      "2024-12-13 18:01:51.205000: I runner.py:310] Step = 18800 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000645 ; Loss = 2.157155\n",
      "2024-12-13 18:04:07.901000: I runner.py:310] Step = 18900 ; steps/s = 0.73, tokens/s = 18489 (18489 target) ; Learning rate = 0.000643 ; Loss = 2.093221\n",
      "2024-12-13 18:06:26.662000: I runner.py:310] Step = 19000 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000641 ; Loss = 2.088036\n",
      "2024-12-13 18:08:45.358000: I runner.py:310] Step = 19100 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000640 ; Loss = 2.136297\n",
      "2024-12-13 18:11:04.104000: I runner.py:310] Step = 19200 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000638 ; Loss = 2.146010\n",
      "2024-12-13 18:13:20.788000: I runner.py:310] Step = 19300 ; steps/s = 0.73, tokens/s = 18493 (18493 target) ; Learning rate = 0.000636 ; Loss = 2.106548\n",
      "2024-12-13 18:15:39.505000: I runner.py:310] Step = 19400 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000635 ; Loss = 2.107326\n",
      "2024-12-13 18:17:58.295000: I runner.py:310] Step = 19500 ; steps/s = 0.72, tokens/s = 18546 (18546 target) ; Learning rate = 0.000633 ; Loss = 2.118673\n",
      "2024-12-13 18:20:17.056000: I runner.py:310] Step = 19600 ; steps/s = 0.72, tokens/s = 18548 (18548 target) ; Learning rate = 0.000631 ; Loss = 2.116532\n",
      "2024-12-13 18:22:33.765000: I runner.py:310] Step = 19700 ; steps/s = 0.73, tokens/s = 18490 (18490 target) ; Learning rate = 0.000630 ; Loss = 2.111497\n",
      "2024-12-13 18:24:52.453000: I runner.py:310] Step = 19800 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000628 ; Loss = 2.103580\n",
      "2024-12-13 18:27:11.194000: I runner.py:310] Step = 19900 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000627 ; Loss = 2.110894\n",
      "2024-12-13 18:29:29.957000: I runner.py:310] Step = 20000 ; steps/s = 0.72, tokens/s = 18546 (18546 target) ; Learning rate = 0.000625 ; Loss = 2.131411\n",
      "2024-12-13 18:29:31.900000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-20000\n",
      "2024-12-13 18:31:48.515000: I runner.py:310] Step = 20100 ; steps/s = 0.73, tokens/s = 18502 (18502 target) ; Learning rate = 0.000623 ; Loss = 2.038516\n",
      "2024-12-13 18:34:07.313000: I runner.py:310] Step = 20200 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000622 ; Loss = 2.112797\n",
      "2024-12-13 18:36:26.093000: I runner.py:310] Step = 20300 ; steps/s = 0.72, tokens/s = 18546 (18546 target) ; Learning rate = 0.000620 ; Loss = 2.121036\n",
      "2024-12-13 18:38:44.854000: I runner.py:310] Step = 20400 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000619 ; Loss = 2.108793\n",
      "2024-12-13 18:41:01.597000: I runner.py:310] Step = 20500 ; steps/s = 0.73, tokens/s = 18487 (18487 target) ; Learning rate = 0.000617 ; Loss = 2.024539\n",
      "2024-12-13 18:43:20.336000: I runner.py:310] Step = 20600 ; steps/s = 0.72, tokens/s = 18553 (18553 target) ; Learning rate = 0.000616 ; Loss = 2.105169\n",
      "2024-12-13 18:45:39.109000: I runner.py:310] Step = 20700 ; steps/s = 0.72, tokens/s = 18544 (18544 target) ; Learning rate = 0.000614 ; Loss = 2.129590\n",
      "2024-12-13 18:47:57.957000: I runner.py:310] Step = 20800 ; steps/s = 0.72, tokens/s = 18539 (18539 target) ; Learning rate = 0.000613 ; Loss = 2.129636\n",
      "2024-12-13 18:50:14.274000: I runner.py:310] Step = 20900 ; steps/s = 0.73, tokens/s = 18541 (18541 target) ; Learning rate = 0.000611 ; Loss = 2.020957\n",
      "2024-12-13 18:52:32.995000: I runner.py:310] Step = 21000 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000610 ; Loss = 2.115754\n",
      "2024-12-13 18:54:51.710000: I runner.py:310] Step = 21100 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000608 ; Loss = 2.101878\n",
      "2024-12-13 18:57:08.603000: I runner.py:310] Step = 21200 ; steps/s = 0.73, tokens/s = 18498 (18498 target) ; Learning rate = 0.000607 ; Loss = 2.158298\n",
      "2024-12-13 18:59:27.149000: I runner.py:310] Step = 21300 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000606 ; Loss = 2.092394\n",
      "2024-12-13 19:01:45.918000: I runner.py:310] Step = 21400 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000604 ; Loss = 2.061507\n",
      "2024-12-13 19:04:04.686000: I runner.py:310] Step = 21500 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000603 ; Loss = 2.081009\n",
      "2024-12-13 19:06:21.423000: I runner.py:310] Step = 21600 ; steps/s = 0.73, tokens/s = 18484 (18484 target) ; Learning rate = 0.000601 ; Loss = 2.045939\n",
      "2024-12-13 19:08:40.187000: I runner.py:310] Step = 21700 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000600 ; Loss = 2.014993\n",
      "2024-12-13 19:10:58.963000: I runner.py:310] Step = 21800 ; steps/s = 0.72, tokens/s = 18544 (18544 target) ; Learning rate = 0.000599 ; Loss = 2.086151\n",
      "2024-12-13 19:13:17.651000: I runner.py:310] Step = 21900 ; steps/s = 0.72, tokens/s = 18561 (18561 target) ; Learning rate = 0.000597 ; Loss = 2.103402\n",
      "2024-12-13 19:15:34.407000: I runner.py:310] Step = 22000 ; steps/s = 0.73, tokens/s = 18486 (18486 target) ; Learning rate = 0.000596 ; Loss = 2.044119\n",
      "2024-12-13 19:17:53.169000: I runner.py:310] Step = 22100 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000595 ; Loss = 2.027872\n",
      "2024-12-13 19:20:11.949000: I runner.py:310] Step = 22200 ; steps/s = 0.72, tokens/s = 18541 (18541 target) ; Learning rate = 0.000593 ; Loss = 2.062634\n",
      "2024-12-13 19:22:30.721000: I runner.py:310] Step = 22300 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000592 ; Loss = 2.088332\n",
      "2024-12-13 19:24:47.473000: I runner.py:310] Step = 22400 ; steps/s = 0.73, tokens/s = 18482 (18482 target) ; Learning rate = 0.000591 ; Loss = 2.009098\n",
      "2024-12-13 19:27:06.183000: I runner.py:310] Step = 22500 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000589 ; Loss = 2.044653\n",
      "2024-12-13 19:29:24.913000: I runner.py:310] Step = 22600 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000588 ; Loss = 2.061216\n",
      "2024-12-13 19:31:43.658000: I runner.py:310] Step = 22700 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000587 ; Loss = 2.077897\n",
      "2024-12-13 19:34:00.356000: I runner.py:310] Step = 22800 ; steps/s = 0.73, tokens/s = 18491 (18491 target) ; Learning rate = 0.000585 ; Loss = 2.083836\n",
      "2024-12-13 19:36:19.134000: I runner.py:310] Step = 22900 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000584 ; Loss = 2.030137\n",
      "2024-12-13 19:38:37.940000: I runner.py:310] Step = 23000 ; steps/s = 0.72, tokens/s = 18542 (18542 target) ; Learning rate = 0.000583 ; Loss = 2.035469\n",
      "2024-12-13 19:40:56.718000: I runner.py:310] Step = 23100 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000582 ; Loss = 2.064297\n",
      "2024-12-13 19:43:12.948000: I runner.py:310] Step = 23200 ; steps/s = 0.73, tokens/s = 18554 (18554 target) ; Learning rate = 0.000580 ; Loss = 2.070658\n",
      "2024-12-13 19:45:31.719000: I runner.py:310] Step = 23300 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000579 ; Loss = 2.014528\n",
      "2024-12-13 19:47:50.534000: I runner.py:310] Step = 23400 ; steps/s = 0.72, tokens/s = 18543 (18543 target) ; Learning rate = 0.000578 ; Loss = 2.016045\n",
      "2024-12-13 19:50:09.290000: I runner.py:310] Step = 23500 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000577 ; Loss = 2.052857\n",
      "2024-12-13 19:52:25.990000: I runner.py:310] Step = 23600 ; steps/s = 0.73, tokens/s = 18489 (18489 target) ; Learning rate = 0.000575 ; Loss = 1.994177\n",
      "2024-12-13 19:54:44.802000: I runner.py:310] Step = 23700 ; steps/s = 0.72, tokens/s = 18544 (18544 target) ; Learning rate = 0.000574 ; Loss = 2.042550\n",
      "2024-12-13 19:57:03.592000: I runner.py:310] Step = 23800 ; steps/s = 0.72, tokens/s = 18543 (18543 target) ; Learning rate = 0.000573 ; Loss = 2.049650\n",
      "2024-12-13 19:59:20.278000: I runner.py:310] Step = 23900 ; steps/s = 0.73, tokens/s = 18494 (18494 target) ; Learning rate = 0.000572 ; Loss = 2.028361\n",
      "2024-12-13 20:01:39.034000: I runner.py:310] Step = 24000 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000571 ; Loss = 2.056202\n",
      "2024-12-13 20:03:57.844000: I runner.py:310] Step = 24100 ; steps/s = 0.72, tokens/s = 18544 (18544 target) ; Learning rate = 0.000569 ; Loss = 2.026734\n",
      "2024-12-13 20:06:16.559000: I runner.py:310] Step = 24200 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000568 ; Loss = 2.008587\n",
      "2024-12-13 20:08:33.310000: I runner.py:310] Step = 24300 ; steps/s = 0.73, tokens/s = 18484 (18484 target) ; Learning rate = 0.000567 ; Loss = 2.030551\n",
      "2024-12-13 20:10:52.075000: I runner.py:310] Step = 24400 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000566 ; Loss = 2.036687\n",
      "2024-12-13 20:13:10.763000: I runner.py:310] Step = 24500 ; steps/s = 0.72, tokens/s = 18562 (18562 target) ; Learning rate = 0.000565 ; Loss = 2.013449\n",
      "2024-12-13 20:15:29.521000: I runner.py:310] Step = 24600 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000564 ; Loss = 2.020626\n",
      "2024-12-13 20:17:46.287000: I runner.py:310] Step = 24700 ; steps/s = 0.73, tokens/s = 18479 (18479 target) ; Learning rate = 0.000562 ; Loss = 1.990121\n",
      "2024-12-13 20:20:05.032000: I runner.py:310] Step = 24800 ; steps/s = 0.72, tokens/s = 18548 (18548 target) ; Learning rate = 0.000561 ; Loss = 2.002860\n",
      "2024-12-13 20:22:23.688000: I runner.py:310] Step = 24900 ; steps/s = 0.72, tokens/s = 18563 (18563 target) ; Learning rate = 0.000560 ; Loss = 2.025919\n",
      "2024-12-13 20:24:42.471000: I runner.py:310] Step = 25000 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000559 ; Loss = 2.051237\n",
      "2024-12-13 20:26:59.164000: I runner.py:310] Step = 25100 ; steps/s = 0.73, tokens/s = 18496 (18496 target) ; Learning rate = 0.000558 ; Loss = 1.991357\n",
      "2024-12-13 20:29:17.920000: I runner.py:310] Step = 25200 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000557 ; Loss = 2.013010\n",
      "2024-12-13 20:31:36.699000: I runner.py:310] Step = 25300 ; steps/s = 0.72, tokens/s = 18546 (18546 target) ; Learning rate = 0.000556 ; Loss = 2.023015\n",
      "2024-12-13 20:33:55.525000: I runner.py:310] Step = 25400 ; steps/s = 0.72, tokens/s = 18543 (18543 target) ; Learning rate = 0.000555 ; Loss = 2.051623\n",
      "2024-12-13 20:36:11.828000: I runner.py:310] Step = 25500 ; steps/s = 0.73, tokens/s = 18546 (18546 target) ; Learning rate = 0.000553 ; Loss = 1.968326\n",
      "2024-12-13 20:38:30.486000: I runner.py:310] Step = 25600 ; steps/s = 0.72, tokens/s = 18565 (18565 target) ; Learning rate = 0.000552 ; Loss = 2.027400\n",
      "2024-12-13 20:40:49.267000: I runner.py:310] Step = 25700 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000551 ; Loss = 2.039900\n",
      "2024-12-13 20:43:08.080000: I runner.py:310] Step = 25800 ; steps/s = 0.72, tokens/s = 18540 (18540 target) ; Learning rate = 0.000550 ; Loss = 2.023825\n",
      "2024-12-13 20:45:24.805000: I runner.py:310] Step = 25900 ; steps/s = 0.73, tokens/s = 18485 (18485 target) ; Learning rate = 0.000549 ; Loss = 2.036171\n",
      "2024-12-13 20:47:43.544000: I runner.py:310] Step = 26000 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000548 ; Loss = 1.977726\n",
      "2024-12-13 20:50:02.301000: I runner.py:310] Step = 26100 ; steps/s = 0.72, tokens/s = 18553 (18553 target) ; Learning rate = 0.000547 ; Loss = 2.011091\n",
      "2024-12-13 20:52:20.965000: I runner.py:310] Step = 26200 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000546 ; Loss = 2.029083\n",
      "2024-12-13 20:54:37.666000: I runner.py:310] Step = 26300 ; steps/s = 0.73, tokens/s = 18492 (18492 target) ; Learning rate = 0.000545 ; Loss = 2.026747\n",
      "2024-12-13 20:56:56.482000: I runner.py:310] Step = 26400 ; steps/s = 0.72, tokens/s = 18541 (18541 target) ; Learning rate = 0.000544 ; Loss = 1.981527\n",
      "2024-12-13 20:59:15.194000: I runner.py:310] Step = 26500 ; steps/s = 0.72, tokens/s = 18553 (18553 target) ; Learning rate = 0.000543 ; Loss = 2.006732\n",
      "2024-12-13 21:01:31.843000: I runner.py:310] Step = 26600 ; steps/s = 0.73, tokens/s = 18498 (18498 target) ; Learning rate = 0.000542 ; Loss = 1.986264\n",
      "2024-12-13 21:03:50.567000: I runner.py:310] Step = 26700 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000541 ; Loss = 2.006769\n",
      "2024-12-13 21:06:09.314000: I runner.py:310] Step = 26800 ; steps/s = 0.72, tokens/s = 18553 (18553 target) ; Learning rate = 0.000540 ; Loss = 1.963934\n",
      "2024-12-13 21:08:28.063000: I runner.py:310] Step = 26900 ; steps/s = 0.72, tokens/s = 18548 (18548 target) ; Learning rate = 0.000539 ; Loss = 1.999200\n",
      "2024-12-13 21:10:44.784000: I runner.py:310] Step = 27000 ; steps/s = 0.73, tokens/s = 18486 (18486 target) ; Learning rate = 0.000538 ; Loss = 1.964698\n",
      "2024-12-13 21:13:03.629000: I runner.py:310] Step = 27100 ; steps/s = 0.72, tokens/s = 18533 (18533 target) ; Learning rate = 0.000537 ; Loss = 1.972977\n",
      "2024-12-13 21:15:22.414000: I runner.py:310] Step = 27200 ; steps/s = 0.72, tokens/s = 18548 (18548 target) ; Learning rate = 0.000536 ; Loss = 2.004756\n",
      "2024-12-13 21:17:41.157000: I runner.py:310] Step = 27300 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000535 ; Loss = 2.018069\n",
      "2024-12-13 21:19:57.840000: I runner.py:310] Step = 27400 ; steps/s = 0.73, tokens/s = 18490 (18490 target) ; Learning rate = 0.000534 ; Loss = 1.995795\n",
      "2024-12-13 21:22:16.583000: I runner.py:310] Step = 27500 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000533 ; Loss = 1.988316\n",
      "2024-12-13 21:24:35.342000: I runner.py:310] Step = 27600 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000532 ; Loss = 1.974593\n",
      "2024-12-13 21:26:54.160000: I runner.py:310] Step = 27700 ; steps/s = 0.72, tokens/s = 18542 (18542 target) ; Learning rate = 0.000531 ; Loss = 1.988320\n",
      "2024-12-13 21:29:10.523000: I runner.py:310] Step = 27800 ; steps/s = 0.73, tokens/s = 18536 (18536 target) ; Learning rate = 0.000530 ; Loss = 2.007633\n",
      "2024-12-13 21:31:29.215000: I runner.py:310] Step = 27900 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000529 ; Loss = 1.951926\n",
      "2024-12-13 21:33:47.948000: I runner.py:310] Step = 28000 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000528 ; Loss = 1.962675\n",
      "2024-12-13 21:36:06.719000: I runner.py:310] Step = 28100 ; steps/s = 0.72, tokens/s = 18546 (18546 target) ; Learning rate = 0.000527 ; Loss = 1.975314\n",
      "2024-12-13 21:38:23.420000: I runner.py:310] Step = 28200 ; steps/s = 0.73, tokens/s = 18491 (18491 target) ; Learning rate = 0.000526 ; Loss = 1.931552\n",
      "2024-12-13 21:40:42.136000: I runner.py:310] Step = 28300 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000525 ; Loss = 1.971398\n",
      "2024-12-13 21:43:00.838000: I runner.py:310] Step = 28400 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000524 ; Loss = 1.986876\n",
      "2024-12-13 21:45:19.570000: I runner.py:310] Step = 28500 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000524 ; Loss = 2.013901\n",
      "2024-12-13 21:47:36.384000: I runner.py:310] Step = 28600 ; steps/s = 0.73, tokens/s = 18476 (18476 target) ; Learning rate = 0.000523 ; Loss = 1.916252\n",
      "2024-12-13 21:49:55.186000: I runner.py:310] Step = 28700 ; steps/s = 0.72, tokens/s = 18544 (18544 target) ; Learning rate = 0.000522 ; Loss = 1.983887\n",
      "2024-12-13 21:52:13.941000: I runner.py:310] Step = 28800 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000521 ; Loss = 2.003169\n",
      "2024-12-13 21:54:32.687000: I runner.py:310] Step = 28900 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000520 ; Loss = 2.001008\n",
      "2024-12-13 21:56:49.340000: I runner.py:310] Step = 29000 ; steps/s = 0.73, tokens/s = 18499 (18499 target) ; Learning rate = 0.000519 ; Loss = 1.935152\n",
      "2024-12-13 21:59:08.067000: I runner.py:310] Step = 29100 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000518 ; Loss = 1.971118\n",
      "2024-12-13 22:01:26.777000: I runner.py:310] Step = 29200 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000517 ; Loss = 1.968590\n",
      "2024-12-13 22:03:43.448000: I runner.py:310] Step = 29300 ; steps/s = 0.73, tokens/s = 18494 (18494 target) ; Learning rate = 0.000516 ; Loss = 1.954761\n",
      "2024-12-13 22:06:02.264000: I runner.py:310] Step = 29400 ; steps/s = 0.72, tokens/s = 18540 (18540 target) ; Learning rate = 0.000515 ; Loss = 1.929811\n",
      "2024-12-13 22:08:21.025000: I runner.py:310] Step = 29500 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000515 ; Loss = 1.976237\n",
      "2024-12-13 22:10:39.773000: I runner.py:310] Step = 29600 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000514 ; Loss = 1.977641\n",
      "2024-12-13 22:12:56.457000: I runner.py:310] Step = 29700 ; steps/s = 0.73, tokens/s = 18496 (18496 target) ; Learning rate = 0.000513 ; Loss = 1.955749\n",
      "2024-12-13 22:15:15.251000: I runner.py:310] Step = 29800 ; steps/s = 0.72, tokens/s = 18543 (18543 target) ; Learning rate = 0.000512 ; Loss = 1.946501\n",
      "2024-12-13 22:17:34.006000: I runner.py:310] Step = 29900 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000511 ; Loss = 1.952269\n",
      "2024-12-13 22:19:52.808000: I runner.py:310] Step = 30000 ; steps/s = 0.72, tokens/s = 18543 (18543 target) ; Learning rate = 0.000510 ; Loss = 1.976781\n",
      "2024-12-13 22:19:54.365000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-30000\n",
      "2024-12-13 22:22:10.678000: I runner.py:310] Step = 30100 ; steps/s = 0.73, tokens/s = 18544 (18544 target) ; Learning rate = 0.000509 ; Loss = 1.930311\n",
      "2024-12-13 22:24:29.375000: I runner.py:310] Step = 30200 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000509 ; Loss = 1.960006\n",
      "2024-12-13 22:26:48.122000: I runner.py:310] Step = 30300 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000508 ; Loss = 1.962204\n",
      "2024-12-13 22:29:06.814000: I runner.py:310] Step = 30400 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000507 ; Loss = 1.990492\n",
      "2024-12-13 22:31:23.529000: I runner.py:310] Step = 30500 ; steps/s = 0.73, tokens/s = 18491 (18491 target) ; Learning rate = 0.000506 ; Loss = 1.917779\n",
      "2024-12-13 22:33:42.226000: I runner.py:310] Step = 30600 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000505 ; Loss = 1.934308\n",
      "2024-12-13 22:36:00.933000: I runner.py:310] Step = 30700 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000504 ; Loss = 1.976410\n",
      "2024-12-13 22:38:19.686000: I runner.py:310] Step = 30800 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000504 ; Loss = 1.989345\n",
      "2024-12-13 22:40:36.463000: I runner.py:310] Step = 30900 ; steps/s = 0.73, tokens/s = 18480 (18480 target) ; Learning rate = 0.000503 ; Loss = 1.921348\n",
      "2024-12-13 22:42:55.215000: I runner.py:310] Step = 31000 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000502 ; Loss = 1.956009\n",
      "2024-12-13 22:45:13.892000: I runner.py:310] Step = 31100 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000501 ; Loss = 1.968572\n",
      "2024-12-13 22:47:32.645000: I runner.py:310] Step = 31200 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000500 ; Loss = 1.966995\n",
      "2024-12-13 22:49:49.328000: I runner.py:310] Step = 31300 ; steps/s = 0.73, tokens/s = 18494 (18494 target) ; Learning rate = 0.000500 ; Loss = 1.971974\n",
      "2024-12-13 22:52:08.111000: I runner.py:310] Step = 31400 ; steps/s = 0.72, tokens/s = 18543 (18543 target) ; Learning rate = 0.000499 ; Loss = 1.947315\n",
      "2024-12-13 22:54:26.847000: I runner.py:310] Step = 31500 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000498 ; Loss = 1.947168\n",
      "2024-12-13 22:56:45.601000: I runner.py:310] Step = 31600 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000497 ; Loss = 1.943656\n",
      "2024-12-13 22:59:02.312000: I runner.py:310] Step = 31700 ; steps/s = 0.73, tokens/s = 18492 (18492 target) ; Learning rate = 0.000496 ; Loss = 1.906828\n",
      "2024-12-13 23:01:21.016000: I runner.py:310] Step = 31800 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000496 ; Loss = 1.955134\n",
      "2024-12-13 23:03:39.684000: I runner.py:310] Step = 31900 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000495 ; Loss = 1.969823\n",
      "2024-12-13 23:05:56.420000: I runner.py:310] Step = 32000 ; steps/s = 0.73, tokens/s = 18486 (18486 target) ; Learning rate = 0.000494 ; Loss = 1.930500\n",
      "2024-12-13 23:08:15.156000: I runner.py:310] Step = 32100 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000493 ; Loss = 1.905101\n",
      "2024-12-13 23:10:33.901000: I runner.py:310] Step = 32200 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000493 ; Loss = 1.951883\n",
      "2024-12-13 23:12:52.663000: I runner.py:310] Step = 32300 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000492 ; Loss = 1.958718\n",
      "2024-12-13 23:15:09.022000: I runner.py:310] Step = 32400 ; steps/s = 0.73, tokens/s = 18536 (18536 target) ; Learning rate = 0.000491 ; Loss = 1.906698\n",
      "2024-12-13 23:17:27.735000: I runner.py:310] Step = 32500 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000490 ; Loss = 1.908658\n",
      "2024-12-13 23:19:46.479000: I runner.py:310] Step = 32600 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000490 ; Loss = 1.963328\n",
      "2024-12-13 23:22:05.214000: I runner.py:310] Step = 32700 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000489 ; Loss = 1.945445\n",
      "2024-12-13 23:24:21.931000: I runner.py:310] Step = 32800 ; steps/s = 0.73, tokens/s = 18489 (18489 target) ; Learning rate = 0.000488 ; Loss = 1.938640\n",
      "2024-12-13 23:26:40.646000: I runner.py:310] Step = 32900 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000487 ; Loss = 1.937875\n",
      "2024-12-13 23:28:59.359000: I runner.py:310] Step = 33000 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000487 ; Loss = 1.920394\n",
      "2024-12-13 23:31:18.104000: I runner.py:310] Step = 33100 ; steps/s = 0.72, tokens/s = 18553 (18553 target) ; Learning rate = 0.000486 ; Loss = 1.942562\n",
      "2024-12-13 23:33:34.870000: I runner.py:310] Step = 33200 ; steps/s = 0.73, tokens/s = 18479 (18479 target) ; Learning rate = 0.000485 ; Loss = 1.890470\n",
      "2024-12-13 23:35:53.594000: I runner.py:310] Step = 33300 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000484 ; Loss = 1.922459\n",
      "2024-12-13 23:38:12.317000: I runner.py:310] Step = 33400 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000484 ; Loss = 1.948781\n",
      "2024-12-13 23:40:31.087000: I runner.py:310] Step = 33500 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000483 ; Loss = 1.959325\n",
      "2024-12-13 23:42:47.728000: I runner.py:310] Step = 33600 ; steps/s = 0.73, tokens/s = 18498 (18498 target) ; Learning rate = 0.000482 ; Loss = 1.940329\n",
      "2024-12-13 23:45:06.538000: I runner.py:310] Step = 33700 ; steps/s = 0.72, tokens/s = 18543 (18543 target) ; Learning rate = 0.000481 ; Loss = 1.894205\n",
      "2024-12-13 23:47:25.290000: I runner.py:310] Step = 33800 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000481 ; Loss = 1.912419\n",
      "2024-12-13 23:49:44.064000: I runner.py:310] Step = 33900 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000480 ; Loss = 1.935190\n",
      "2024-12-13 23:52:00.743000: I runner.py:310] Step = 34000 ; steps/s = 0.73, tokens/s = 18494 (18494 target) ; Learning rate = 0.000479 ; Loss = 1.892129\n",
      "2024-12-13 23:54:19.446000: I runner.py:310] Step = 34100 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000479 ; Loss = 1.939027\n",
      "2024-12-13 23:56:38.297000: I runner.py:310] Step = 34200 ; steps/s = 0.72, tokens/s = 18534 (18534 target) ; Learning rate = 0.000478 ; Loss = 1.923590\n",
      "2024-12-13 23:58:57.079000: I runner.py:310] Step = 34300 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000477 ; Loss = 1.944986\n",
      "2024-12-14 00:01:13.755000: I runner.py:310] Step = 34400 ; steps/s = 0.73, tokens/s = 18494 (18494 target) ; Learning rate = 0.000477 ; Loss = 1.880432\n",
      "2024-12-14 00:03:32.571000: I runner.py:310] Step = 34500 ; steps/s = 0.72, tokens/s = 18543 (18543 target) ; Learning rate = 0.000476 ; Loss = 1.914355\n",
      "2024-12-14 00:05:51.303000: I runner.py:310] Step = 34600 ; steps/s = 0.72, tokens/s = 18553 (18553 target) ; Learning rate = 0.000475 ; Loss = 1.945923\n",
      "2024-12-14 00:08:07.730000: I runner.py:310] Step = 34700 ; steps/s = 0.73, tokens/s = 18531 (18531 target) ; Learning rate = 0.000474 ; Loss = 1.908695\n",
      "2024-12-14 00:10:26.452000: I runner.py:310] Step = 34800 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000474 ; Loss = 1.893538\n",
      "2024-12-14 00:12:45.214000: I runner.py:310] Step = 34900 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000473 ; Loss = 1.912327\n",
      "2024-12-14 00:15:03.945000: I runner.py:310] Step = 35000 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000472 ; Loss = 1.946619\n",
      "2024-12-14 00:17:20.577000: I runner.py:310] Step = 35100 ; steps/s = 0.73, tokens/s = 18503 (18503 target) ; Learning rate = 0.000472 ; Loss = 1.897889\n",
      "2024-12-14 00:19:39.359000: I runner.py:310] Step = 35200 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000471 ; Loss = 1.886975\n",
      "2024-12-14 00:21:58.147000: I runner.py:310] Step = 35300 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000470 ; Loss = 1.926150\n",
      "2024-12-14 00:24:16.978000: I runner.py:310] Step = 35400 ; steps/s = 0.72, tokens/s = 18540 (18540 target) ; Learning rate = 0.000470 ; Loss = 1.948347\n",
      "2024-12-14 00:26:33.593000: I runner.py:310] Step = 35500 ; steps/s = 0.73, tokens/s = 18502 (18502 target) ; Learning rate = 0.000469 ; Loss = 1.881692\n",
      "2024-12-14 00:28:52.359000: I runner.py:310] Step = 35600 ; steps/s = 0.72, tokens/s = 18548 (18548 target) ; Learning rate = 0.000468 ; Loss = 1.903431\n",
      "2024-12-14 00:31:11.150000: I runner.py:310] Step = 35700 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000468 ; Loss = 1.924872\n",
      "2024-12-14 00:33:29.845000: I runner.py:310] Step = 35800 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000467 ; Loss = 1.933971\n",
      "2024-12-14 00:35:46.548000: I runner.py:310] Step = 35900 ; steps/s = 0.73, tokens/s = 18488 (18488 target) ; Learning rate = 0.000466 ; Loss = 1.869956\n",
      "2024-12-14 00:38:05.320000: I runner.py:310] Step = 36000 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000466 ; Loss = 1.916594\n",
      "2024-12-14 00:40:24.120000: I runner.py:310] Step = 36100 ; steps/s = 0.72, tokens/s = 18542 (18542 target) ; Learning rate = 0.000465 ; Loss = 1.921975\n",
      "2024-12-14 00:42:42.940000: I runner.py:310] Step = 36200 ; steps/s = 0.72, tokens/s = 18541 (18541 target) ; Learning rate = 0.000465 ; Loss = 1.933321\n",
      "2024-12-14 00:44:59.598000: I runner.py:310] Step = 36300 ; steps/s = 0.73, tokens/s = 18499 (18499 target) ; Learning rate = 0.000464 ; Loss = 1.928303\n",
      "2024-12-14 00:47:18.341000: I runner.py:310] Step = 36400 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000463 ; Loss = 1.896376\n",
      "2024-12-14 00:49:37.149000: I runner.py:310] Step = 36500 ; steps/s = 0.72, tokens/s = 18540 (18540 target) ; Learning rate = 0.000463 ; Loss = 1.911965\n",
      "2024-12-14 00:51:55.899000: I runner.py:310] Step = 36600 ; steps/s = 0.72, tokens/s = 18553 (18553 target) ; Learning rate = 0.000462 ; Loss = 1.908014\n",
      "2024-12-14 00:54:12.639000: I runner.py:310] Step = 36700 ; steps/s = 0.73, tokens/s = 18486 (18486 target) ; Learning rate = 0.000461 ; Loss = 1.854237\n",
      "2024-12-14 00:56:31.458000: I runner.py:310] Step = 36800 ; steps/s = 0.72, tokens/s = 18541 (18541 target) ; Learning rate = 0.000461 ; Loss = 1.906205\n",
      "2024-12-14 00:58:50.156000: I runner.py:310] Step = 36900 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000460 ; Loss = 1.934631\n",
      "2024-12-14 01:01:08.751000: I runner.py:310] Step = 37000 ; steps/s = 0.72, tokens/s = 18571 (18571 target) ; Learning rate = 0.000460 ; Loss = 1.928222\n",
      "2024-12-14 01:03:25.330000: I runner.py:310] Step = 37100 ; steps/s = 0.73, tokens/s = 18508 (18508 target) ; Learning rate = 0.000459 ; Loss = 1.916126\n",
      "2024-12-14 01:05:44.049000: I runner.py:310] Step = 37200 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000458 ; Loss = 1.904886\n",
      "2024-12-14 01:08:02.788000: I runner.py:310] Step = 37300 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000458 ; Loss = 1.905886\n",
      "2024-12-14 01:10:19.469000: I runner.py:310] Step = 37400 ; steps/s = 0.73, tokens/s = 18495 (18495 target) ; Learning rate = 0.000457 ; Loss = 1.895892\n",
      "2024-12-14 01:12:38.195000: I runner.py:310] Step = 37500 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000456 ; Loss = 1.886822\n",
      "2024-12-14 01:14:56.965000: I runner.py:310] Step = 37600 ; steps/s = 0.72, tokens/s = 18548 (18548 target) ; Learning rate = 0.000456 ; Loss = 1.896542\n",
      "2024-12-14 01:17:15.750000: I runner.py:310] Step = 37700 ; steps/s = 0.72, tokens/s = 18544 (18544 target) ; Learning rate = 0.000455 ; Loss = 1.884228\n",
      "2024-12-14 01:19:32.467000: I runner.py:310] Step = 37800 ; steps/s = 0.73, tokens/s = 18489 (18489 target) ; Learning rate = 0.000455 ; Loss = 1.893734\n",
      "2024-12-14 01:21:51.249000: I runner.py:310] Step = 37900 ; steps/s = 0.72, tokens/s = 18548 (18548 target) ; Learning rate = 0.000454 ; Loss = 1.908587\n",
      "2024-12-14 01:24:10.001000: I runner.py:310] Step = 38000 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000453 ; Loss = 1.878507\n",
      "2024-12-14 01:26:28.724000: I runner.py:310] Step = 38100 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000453 ; Loss = 1.895209\n",
      "2024-12-14 01:28:45.546000: I runner.py:310] Step = 38200 ; steps/s = 0.73, tokens/s = 18471 (18471 target) ; Learning rate = 0.000452 ; Loss = 1.858433\n",
      "2024-12-14 01:31:04.275000: I runner.py:310] Step = 38300 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000452 ; Loss = 1.887732\n",
      "2024-12-14 01:33:23.070000: I runner.py:310] Step = 38400 ; steps/s = 0.72, tokens/s = 18546 (18546 target) ; Learning rate = 0.000451 ; Loss = 1.918662\n",
      "2024-12-14 01:35:41.876000: I runner.py:310] Step = 38500 ; steps/s = 0.72, tokens/s = 18539 (18539 target) ; Learning rate = 0.000450 ; Loss = 1.906902\n",
      "2024-12-14 01:37:58.646000: I runner.py:310] Step = 38600 ; steps/s = 0.73, tokens/s = 18481 (18481 target) ; Learning rate = 0.000450 ; Loss = 1.906584\n",
      "2024-12-14 01:40:17.460000: I runner.py:310] Step = 38700 ; steps/s = 0.72, tokens/s = 18544 (18544 target) ; Learning rate = 0.000449 ; Loss = 1.875238\n",
      "2024-12-14 01:42:36.172000: I runner.py:310] Step = 38800 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000449 ; Loss = 1.892467\n",
      "2024-12-14 01:44:54.901000: I runner.py:310] Step = 38900 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000448 ; Loss = 1.910389\n",
      "2024-12-14 01:47:11.630000: I runner.py:310] Step = 39000 ; steps/s = 0.73, tokens/s = 18488 (18488 target) ; Learning rate = 0.000448 ; Loss = 1.927701\n",
      "2024-12-14 01:49:30.389000: I runner.py:310] Step = 39100 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000447 ; Loss = 1.870749\n",
      "2024-12-14 01:51:49.141000: I runner.py:310] Step = 39200 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000446 ; Loss = 1.891198\n",
      "2024-12-14 01:54:07.665000: I runner.py:310] Step = 39300 ; steps/s = 0.72, tokens/s = 18581 (18581 target) ; Learning rate = 0.000446 ; Loss = 1.898151\n",
      "2024-12-14 01:56:24.146000: I runner.py:310] Step = 39400 ; steps/s = 0.73, tokens/s = 18521 (18521 target) ; Learning rate = 0.000445 ; Loss = 1.836618\n",
      "2024-12-14 01:58:42.949000: I runner.py:310] Step = 39500 ; steps/s = 0.72, tokens/s = 18543 (18543 target) ; Learning rate = 0.000445 ; Loss = 1.887195\n",
      "2024-12-14 02:01:01.736000: I runner.py:310] Step = 39600 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000444 ; Loss = 1.903985\n",
      "2024-12-14 02:03:19.958000: I runner.py:310] Step = 39700 ; steps/s = 0.72, tokens/s = 18533 (18533 target) ; Learning rate = 0.000444 ; Loss = 1.876319\n",
      "2024-12-14 02:05:37.218000: I runner.py:310] Step = 39800 ; steps/s = 0.73, tokens/s = 18507 (18507 target) ; Learning rate = 0.000443 ; Loss = 1.854031\n",
      "2024-12-14 02:07:55.917000: I runner.py:310] Step = 39900 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000442 ; Loss = 1.883589\n",
      "2024-12-14 02:10:14.694000: I runner.py:310] Step = 40000 ; steps/s = 0.72, tokens/s = 18543 (18543 target) ; Learning rate = 0.000442 ; Loss = 1.902107\n",
      "2024-12-14 02:10:16.249000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-40000\n",
      "2024-12-14 02:12:32.818000: I runner.py:310] Step = 40100 ; steps/s = 0.73, tokens/s = 18510 (18510 target) ; Learning rate = 0.000441 ; Loss = 1.871798\n",
      "2024-12-14 02:14:51.612000: I runner.py:310] Step = 40200 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000441 ; Loss = 1.894874\n",
      "2024-12-14 02:17:10.412000: I runner.py:310] Step = 40300 ; steps/s = 0.72, tokens/s = 18542 (18542 target) ; Learning rate = 0.000440 ; Loss = 1.873773\n",
      "2024-12-14 02:19:29.211000: I runner.py:310] Step = 40400 ; steps/s = 0.72, tokens/s = 18541 (18541 target) ; Learning rate = 0.000440 ; Loss = 1.875014\n",
      "2024-12-14 02:21:45.896000: I runner.py:310] Step = 40500 ; steps/s = 0.73, tokens/s = 18494 (18494 target) ; Learning rate = 0.000439 ; Loss = 1.860335\n",
      "2024-12-14 02:24:04.659000: I runner.py:310] Step = 40600 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000439 ; Loss = 1.890096\n",
      "2024-12-14 02:26:23.406000: I runner.py:310] Step = 40700 ; steps/s = 0.72, tokens/s = 18553 (18553 target) ; Learning rate = 0.000438 ; Loss = 1.890882\n",
      "2024-12-14 02:28:42.167000: I runner.py:310] Step = 40800 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000438 ; Loss = 1.910883\n",
      "2024-12-14 02:30:58.869000: I runner.py:310] Step = 40900 ; steps/s = 0.73, tokens/s = 18491 (18491 target) ; Learning rate = 0.000437 ; Loss = 1.881001\n",
      "2024-12-14 02:33:17.611000: I runner.py:310] Step = 41000 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000437 ; Loss = 1.876663\n",
      "2024-12-14 02:35:36.436000: I runner.py:310] Step = 41100 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000436 ; Loss = 1.864897\n",
      "2024-12-14 02:37:55.211000: I runner.py:310] Step = 41200 ; steps/s = 0.72, tokens/s = 18548 (18548 target) ; Learning rate = 0.000435 ; Loss = 1.893606\n",
      "2024-12-14 02:40:11.909000: I runner.py:310] Step = 41300 ; steps/s = 0.73, tokens/s = 18489 (18489 target) ; Learning rate = 0.000435 ; Loss = 1.828273\n",
      "2024-12-14 02:42:30.638000: I runner.py:310] Step = 41400 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000434 ; Loss = 1.874221\n",
      "2024-12-14 02:44:49.458000: I runner.py:310] Step = 41500 ; steps/s = 0.72, tokens/s = 18544 (18544 target) ; Learning rate = 0.000434 ; Loss = 1.888432\n",
      "2024-12-14 02:47:07.957000: I runner.py:310] Step = 41600 ; steps/s = 0.72, tokens/s = 18584 (18584 target) ; Learning rate = 0.000433 ; Loss = 1.898426\n",
      "2024-12-14 02:49:24.415000: I runner.py:310] Step = 41700 ; steps/s = 0.73, tokens/s = 18520 (18520 target) ; Learning rate = 0.000433 ; Loss = 1.890311\n",
      "2024-12-14 02:51:43.160000: I runner.py:310] Step = 41800 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000432 ; Loss = 1.865656\n",
      "2024-12-14 02:54:01.915000: I runner.py:310] Step = 41900 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000432 ; Loss = 1.875606\n",
      "2024-12-14 02:56:20.682000: I runner.py:310] Step = 42000 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000431 ; Loss = 1.880435\n",
      "2024-12-14 02:58:37.386000: I runner.py:310] Step = 42100 ; steps/s = 0.73, tokens/s = 18492 (18492 target) ; Learning rate = 0.000431 ; Loss = 1.894115\n",
      "2024-12-14 03:00:56.193000: I runner.py:310] Step = 42200 ; steps/s = 0.72, tokens/s = 18542 (18542 target) ; Learning rate = 0.000430 ; Loss = 1.868870\n",
      "2024-12-14 03:03:14.904000: I runner.py:310] Step = 42300 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000430 ; Loss = 1.858811\n",
      "2024-12-14 03:05:32.117000: I runner.py:310] Step = 42400 ; steps/s = 0.73, tokens/s = 18498 (18498 target) ; Learning rate = 0.000429 ; Loss = 1.881478\n",
      "2024-12-14 03:07:50.364000: I runner.py:310] Step = 42500 ; steps/s = 0.72, tokens/s = 18542 (18542 target) ; Learning rate = 0.000429 ; Loss = 1.842424\n",
      "2024-12-14 03:10:09.082000: I runner.py:310] Step = 42600 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000428 ; Loss = 1.881676\n",
      "2024-12-14 03:12:27.853000: I runner.py:310] Step = 42700 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000428 ; Loss = 1.882131\n",
      "2024-12-14 03:14:44.619000: I runner.py:310] Step = 42800 ; steps/s = 0.73, tokens/s = 18478 (18478 target) ; Learning rate = 0.000427 ; Loss = 1.845102\n",
      "2024-12-14 03:17:03.265000: I runner.py:310] Step = 42900 ; steps/s = 0.72, tokens/s = 18563 (18563 target) ; Learning rate = 0.000427 ; Loss = 1.860191\n",
      "2024-12-14 03:19:22.065000: I runner.py:310] Step = 43000 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000426 ; Loss = 1.880190\n",
      "2024-12-14 03:21:40.802000: I runner.py:310] Step = 43100 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000426 ; Loss = 1.879458\n",
      "2024-12-14 03:23:57.491000: I runner.py:310] Step = 43200 ; steps/s = 0.73, tokens/s = 18492 (18492 target) ; Learning rate = 0.000425 ; Loss = 1.845421\n",
      "2024-12-14 03:26:16.246000: I runner.py:310] Step = 43300 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000425 ; Loss = 1.863002\n",
      "2024-12-14 03:28:35.066000: I runner.py:310] Step = 43400 ; steps/s = 0.72, tokens/s = 18541 (18541 target) ; Learning rate = 0.000424 ; Loss = 1.873759\n",
      "2024-12-14 03:30:53.823000: I runner.py:310] Step = 43500 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000424 ; Loss = 1.901518\n",
      "2024-12-14 03:33:10.554000: I runner.py:310] Step = 43600 ; steps/s = 0.73, tokens/s = 18488 (18488 target) ; Learning rate = 0.000423 ; Loss = 1.870264\n",
      "2024-12-14 03:35:29.243000: I runner.py:310] Step = 43700 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000423 ; Loss = 1.855578\n",
      "2024-12-14 03:37:47.950000: I runner.py:310] Step = 43800 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000422 ; Loss = 1.849573\n",
      "2024-12-14 03:40:06.485000: I runner.py:310] Step = 43900 ; steps/s = 0.72, tokens/s = 18582 (18582 target) ; Learning rate = 0.000422 ; Loss = 1.869901\n",
      "2024-12-14 03:42:22.879000: I runner.py:310] Step = 44000 ; steps/s = 0.73, tokens/s = 18532 (18532 target) ; Learning rate = 0.000421 ; Loss = 1.874802\n",
      "2024-12-14 03:44:41.615000: I runner.py:310] Step = 44100 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000421 ; Loss = 1.837670\n",
      "2024-12-14 03:47:00.376000: I runner.py:310] Step = 44200 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000420 ; Loss = 1.869879\n",
      "2024-12-14 03:49:19.184000: I runner.py:310] Step = 44300 ; steps/s = 0.72, tokens/s = 18538 (18538 target) ; Learning rate = 0.000420 ; Loss = 1.867032\n",
      "2024-12-14 03:51:35.888000: I runner.py:310] Step = 44400 ; steps/s = 0.73, tokens/s = 18491 (18491 target) ; Learning rate = 0.000419 ; Loss = 1.880925\n",
      "2024-12-14 03:53:54.670000: I runner.py:310] Step = 44500 ; steps/s = 0.72, tokens/s = 18546 (18546 target) ; Learning rate = 0.000419 ; Loss = 1.855261\n",
      "2024-12-14 03:56:13.414000: I runner.py:310] Step = 44600 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000419 ; Loss = 1.859239\n",
      "2024-12-14 03:58:32.206000: I runner.py:310] Step = 44700 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000418 ; Loss = 1.875345\n",
      "2024-12-14 04:00:48.889000: I runner.py:310] Step = 44800 ; steps/s = 0.73, tokens/s = 18492 (18492 target) ; Learning rate = 0.000418 ; Loss = 1.811419\n",
      "2024-12-14 04:03:07.621000: I runner.py:310] Step = 44900 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000417 ; Loss = 1.853557\n",
      "2024-12-14 04:05:26.431000: I runner.py:310] Step = 45000 ; steps/s = 0.72, tokens/s = 18543 (18543 target) ; Learning rate = 0.000417 ; Loss = 1.894734\n",
      "2024-12-14 04:07:43.139000: I runner.py:310] Step = 45100 ; steps/s = 0.73, tokens/s = 18490 (18490 target) ; Learning rate = 0.000416 ; Loss = 1.859464\n",
      "2024-12-14 04:10:01.924000: I runner.py:310] Step = 45200 ; steps/s = 0.72, tokens/s = 18544 (18544 target) ; Learning rate = 0.000416 ; Loss = 1.840677\n",
      "2024-12-14 04:12:20.686000: I runner.py:310] Step = 45300 ; steps/s = 0.72, tokens/s = 18548 (18548 target) ; Learning rate = 0.000415 ; Loss = 1.863273\n",
      "2024-12-14 04:14:39.450000: I runner.py:310] Step = 45400 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000415 ; Loss = 1.884442\n",
      "2024-12-14 04:16:56.148000: I runner.py:310] Step = 45500 ; steps/s = 0.73, tokens/s = 18492 (18492 target) ; Learning rate = 0.000414 ; Loss = 1.855859\n",
      "2024-12-14 04:19:14.882000: I runner.py:310] Step = 45600 ; steps/s = 0.72, tokens/s = 18548 (18548 target) ; Learning rate = 0.000414 ; Loss = 1.851474\n",
      "2024-12-14 04:21:33.617000: I runner.py:310] Step = 45700 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000413 ; Loss = 1.840882\n",
      "2024-12-14 04:23:52.363000: I runner.py:310] Step = 45800 ; steps/s = 0.72, tokens/s = 18553 (18553 target) ; Learning rate = 0.000413 ; Loss = 1.844853\n",
      "2024-12-14 04:26:09.101000: I runner.py:310] Step = 45900 ; steps/s = 0.73, tokens/s = 18489 (18489 target) ; Learning rate = 0.000413 ; Loss = 1.849818\n",
      "2024-12-14 04:28:27.883000: I runner.py:310] Step = 46000 ; steps/s = 0.72, tokens/s = 18546 (18546 target) ; Learning rate = 0.000412 ; Loss = 1.841077\n",
      "2024-12-14 04:30:46.616000: I runner.py:310] Step = 46100 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000412 ; Loss = 1.849640\n",
      "2024-12-14 04:33:05.292000: I runner.py:310] Step = 46200 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000411 ; Loss = 1.845446\n",
      "2024-12-14 04:35:21.644000: I runner.py:310] Step = 46300 ; steps/s = 0.73, tokens/s = 18538 (18538 target) ; Learning rate = 0.000411 ; Loss = 1.858138\n",
      "2024-12-14 04:37:40.397000: I runner.py:310] Step = 46400 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000410 ; Loss = 1.832789\n",
      "2024-12-14 04:39:59.180000: I runner.py:310] Step = 46500 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000410 ; Loss = 1.826854\n",
      "2024-12-14 04:42:17.974000: I runner.py:310] Step = 46600 ; steps/s = 0.72, tokens/s = 18544 (18544 target) ; Learning rate = 0.000409 ; Loss = 1.850858\n",
      "2024-12-14 04:44:34.733000: I runner.py:310] Step = 46700 ; steps/s = 0.73, tokens/s = 18485 (18485 target) ; Learning rate = 0.000409 ; Loss = 1.816788\n",
      "2024-12-14 04:46:53.506000: I runner.py:310] Step = 46800 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000409 ; Loss = 1.845440\n",
      "2024-12-14 04:49:12.248000: I runner.py:310] Step = 46900 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000408 ; Loss = 1.865788\n",
      "2024-12-14 04:51:31.034000: I runner.py:310] Step = 47000 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000408 ; Loss = 1.882544\n",
      "2024-12-14 04:53:47.699000: I runner.py:310] Step = 47100 ; steps/s = 0.73, tokens/s = 18497 (18497 target) ; Learning rate = 0.000407 ; Loss = 1.881651\n",
      "2024-12-14 04:56:06.387000: I runner.py:310] Step = 47200 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000407 ; Loss = 1.837395\n",
      "2024-12-14 04:58:25.216000: I runner.py:310] Step = 47300 ; steps/s = 0.72, tokens/s = 18537 (18537 target) ; Learning rate = 0.000406 ; Loss = 1.839662\n",
      "2024-12-14 05:00:43.970000: I runner.py:310] Step = 47400 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000406 ; Loss = 1.845890\n",
      "2024-12-14 05:03:00.674000: I runner.py:310] Step = 47500 ; steps/s = 0.73, tokens/s = 18488 (18488 target) ; Learning rate = 0.000406 ; Loss = 1.861253\n",
      "2024-12-14 05:05:19.424000: I runner.py:310] Step = 47600 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000405 ; Loss = 1.838057\n",
      "2024-12-14 05:07:38.197000: I runner.py:310] Step = 47700 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000405 ; Loss = 1.844520\n",
      "2024-12-14 05:09:54.921000: I runner.py:310] Step = 47800 ; steps/s = 0.73, tokens/s = 18489 (18489 target) ; Learning rate = 0.000404 ; Loss = 1.834055\n",
      "2024-12-14 05:12:13.691000: I runner.py:310] Step = 47900 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000404 ; Loss = 1.823904\n",
      "2024-12-14 05:14:32.423000: I runner.py:310] Step = 48000 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000403 ; Loss = 1.852324\n",
      "2024-12-14 05:16:51.222000: I runner.py:310] Step = 48100 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000403 ; Loss = 1.846987\n",
      "2024-12-14 05:19:07.892000: I runner.py:310] Step = 48200 ; steps/s = 0.73, tokens/s = 18491 (18491 target) ; Learning rate = 0.000403 ; Loss = 1.843314\n",
      "2024-12-14 05:21:26.586000: I runner.py:310] Step = 48300 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000402 ; Loss = 1.856778\n",
      "2024-12-14 05:23:45.307000: I runner.py:310] Step = 48400 ; steps/s = 0.72, tokens/s = 18560 (18560 target) ; Learning rate = 0.000402 ; Loss = 1.818888\n",
      "2024-12-14 05:26:04.066000: I runner.py:310] Step = 48500 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000401 ; Loss = 1.833049\n",
      "2024-12-14 05:28:20.421000: I runner.py:310] Step = 48600 ; steps/s = 0.73, tokens/s = 18535 (18535 target) ; Learning rate = 0.000401 ; Loss = 1.820057\n",
      "2024-12-14 05:30:39.172000: I runner.py:310] Step = 48700 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000401 ; Loss = 1.836105\n",
      "2024-12-14 05:32:57.920000: I runner.py:310] Step = 48800 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000400 ; Loss = 1.860594\n",
      "2024-12-14 05:35:16.655000: I runner.py:310] Step = 48900 ; steps/s = 0.72, tokens/s = 18553 (18553 target) ; Learning rate = 0.000400 ; Loss = 1.864753\n",
      "2024-12-14 05:37:33.355000: I runner.py:310] Step = 49000 ; steps/s = 0.73, tokens/s = 18492 (18492 target) ; Learning rate = 0.000399 ; Loss = 1.814873\n",
      "2024-12-14 05:39:52.140000: I runner.py:310] Step = 49100 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000399 ; Loss = 1.842222\n",
      "2024-12-14 05:42:10.932000: I runner.py:310] Step = 49200 ; steps/s = 0.72, tokens/s = 18546 (18546 target) ; Learning rate = 0.000398 ; Loss = 1.847743\n",
      "2024-12-14 05:44:29.718000: I runner.py:310] Step = 49300 ; steps/s = 0.72, tokens/s = 18546 (18546 target) ; Learning rate = 0.000398 ; Loss = 1.847989\n",
      "2024-12-14 05:46:46.408000: I runner.py:310] Step = 49400 ; steps/s = 0.73, tokens/s = 18493 (18493 target) ; Learning rate = 0.000398 ; Loss = 1.865913\n",
      "2024-12-14 05:49:05.183000: I runner.py:310] Step = 49500 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000397 ; Loss = 1.829068\n",
      "2024-12-14 05:51:23.893000: I runner.py:310] Step = 49600 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000397 ; Loss = 1.828712\n",
      "2024-12-14 05:53:42.595000: I runner.py:310] Step = 49700 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000396 ; Loss = 1.840506\n",
      "2024-12-14 05:55:59.380000: I runner.py:310] Step = 49800 ; steps/s = 0.73, tokens/s = 18476 (18476 target) ; Learning rate = 0.000396 ; Loss = 1.788934\n",
      "2024-12-14 05:58:18.108000: I runner.py:310] Step = 49900 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000396 ; Loss = 1.842029\n",
      "2024-12-14 06:00:36.862000: I runner.py:310] Step = 50000 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000395 ; Loss = 1.845207\n",
      "2024-12-14 06:00:38.417000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-50000\n",
      "2024-12-14 06:02:57.226000: I runner.py:310] Step = 50100 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000395 ; Loss = 1.865190\n",
      "2024-12-14 06:05:13.929000: I runner.py:310] Step = 50200 ; steps/s = 0.73, tokens/s = 18488 (18488 target) ; Learning rate = 0.000394 ; Loss = 1.861364\n",
      "2024-12-14 06:07:32.584000: I runner.py:310] Step = 50300 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000394 ; Loss = 1.808737\n",
      "2024-12-14 06:09:51.380000: I runner.py:310] Step = 50400 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000394 ; Loss = 1.836620\n",
      "2024-12-14 06:12:08.137000: I runner.py:310] Step = 50500 ; steps/s = 0.73, tokens/s = 18481 (18481 target) ; Learning rate = 0.000393 ; Loss = 1.825619\n",
      "2024-12-14 06:14:26.921000: I runner.py:310] Step = 50600 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000393 ; Loss = 1.823080\n",
      "2024-12-14 06:16:45.730000: I runner.py:310] Step = 50700 ; steps/s = 0.72, tokens/s = 18542 (18542 target) ; Learning rate = 0.000393 ; Loss = 1.848425\n",
      "2024-12-14 06:19:04.417000: I runner.py:310] Step = 50800 ; steps/s = 0.72, tokens/s = 18561 (18561 target) ; Learning rate = 0.000392 ; Loss = 1.837661\n",
      "2024-12-14 06:21:20.728000: I runner.py:310] Step = 50900 ; steps/s = 0.73, tokens/s = 18544 (18544 target) ; Learning rate = 0.000392 ; Loss = 1.838238\n",
      "2024-12-14 06:23:39.438000: I runner.py:310] Step = 51000 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000391 ; Loss = 1.836728\n",
      "2024-12-14 06:25:58.181000: I runner.py:310] Step = 51100 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000391 ; Loss = 1.825852\n",
      "2024-12-14 06:28:16.966000: I runner.py:310] Step = 51200 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000391 ; Loss = 1.826508\n",
      "2024-12-14 06:30:33.610000: I runner.py:310] Step = 51300 ; steps/s = 0.73, tokens/s = 18498 (18498 target) ; Learning rate = 0.000390 ; Loss = 1.818826\n",
      "2024-12-14 06:32:52.383000: I runner.py:310] Step = 51400 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000390 ; Loss = 1.831127\n",
      "2024-12-14 06:35:11.146000: I runner.py:310] Step = 51500 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000389 ; Loss = 1.843888\n",
      "2024-12-14 06:37:29.889000: I runner.py:310] Step = 51600 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000389 ; Loss = 1.845413\n",
      "2024-12-14 06:39:46.629000: I runner.py:310] Step = 51700 ; steps/s = 0.73, tokens/s = 18487 (18487 target) ; Learning rate = 0.000389 ; Loss = 1.798301\n",
      "2024-12-14 06:42:05.430000: I runner.py:310] Step = 51800 ; steps/s = 0.72, tokens/s = 18544 (18544 target) ; Learning rate = 0.000388 ; Loss = 1.831777\n",
      "2024-12-14 06:44:24.224000: I runner.py:310] Step = 51900 ; steps/s = 0.72, tokens/s = 18543 (18543 target) ; Learning rate = 0.000388 ; Loss = 1.849341\n",
      "2024-12-14 06:46:43.020000: I runner.py:310] Step = 52000 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000388 ; Loss = 1.849247\n",
      "2024-12-14 06:48:59.674000: I runner.py:310] Step = 52100 ; steps/s = 0.73, tokens/s = 18497 (18497 target) ; Learning rate = 0.000387 ; Loss = 1.805340\n",
      "2024-12-14 06:51:18.519000: I runner.py:310] Step = 52200 ; steps/s = 0.72, tokens/s = 18537 (18537 target) ; Learning rate = 0.000387 ; Loss = 1.816431\n",
      "2024-12-14 06:53:37.208000: I runner.py:310] Step = 52300 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000386 ; Loss = 1.828008\n",
      "2024-12-14 06:55:55.945000: I runner.py:310] Step = 52400 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000386 ; Loss = 1.845067\n",
      "2024-12-14 06:58:12.635000: I runner.py:310] Step = 52500 ; steps/s = 0.73, tokens/s = 18492 (18492 target) ; Learning rate = 0.000386 ; Loss = 1.783922\n",
      "2024-12-14 07:00:31.344000: I runner.py:310] Step = 52600 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000385 ; Loss = 1.809838\n",
      "2024-12-14 07:02:50.142000: I runner.py:310] Step = 52700 ; steps/s = 0.72, tokens/s = 18543 (18543 target) ; Learning rate = 0.000385 ; Loss = 1.835316\n",
      "2024-12-14 07:05:08.835000: I runner.py:310] Step = 52800 ; steps/s = 0.72, tokens/s = 18563 (18563 target) ; Learning rate = 0.000385 ; Loss = 1.840039\n",
      "2024-12-14 07:07:25.558000: I runner.py:310] Step = 52900 ; steps/s = 0.73, tokens/s = 18490 (18490 target) ; Learning rate = 0.000384 ; Loss = 1.801171\n",
      "2024-12-14 07:09:44.335000: I runner.py:310] Step = 53000 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000384 ; Loss = 1.827342\n",
      "2024-12-14 07:12:03.097000: I runner.py:310] Step = 53100 ; steps/s = 0.72, tokens/s = 18546 (18546 target) ; Learning rate = 0.000384 ; Loss = 1.827011\n",
      "2024-12-14 07:14:19.381000: I runner.py:310] Step = 53200 ; steps/s = 0.73, tokens/s = 18548 (18548 target) ; Learning rate = 0.000383 ; Loss = 1.814864\n",
      "2024-12-14 07:16:38.153000: I runner.py:310] Step = 53300 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000383 ; Loss = 1.828954\n",
      "2024-12-14 07:18:56.908000: I runner.py:310] Step = 53400 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000382 ; Loss = 1.810231\n",
      "2024-12-14 07:21:15.649000: I runner.py:310] Step = 53500 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000382 ; Loss = 1.805543\n",
      "2024-12-14 07:23:32.322000: I runner.py:310] Step = 53600 ; steps/s = 0.73, tokens/s = 18496 (18496 target) ; Learning rate = 0.000382 ; Loss = 1.819639\n",
      "2024-12-14 07:25:51.082000: I runner.py:310] Step = 53700 ; steps/s = 0.72, tokens/s = 18548 (18548 target) ; Learning rate = 0.000381 ; Loss = 1.828791\n",
      "2024-12-14 07:28:09.896000: I runner.py:310] Step = 53800 ; steps/s = 0.72, tokens/s = 18541 (18541 target) ; Learning rate = 0.000381 ; Loss = 1.811565\n",
      "2024-12-14 07:30:28.651000: I runner.py:310] Step = 53900 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000381 ; Loss = 1.826525\n",
      "2024-12-14 07:32:45.314000: I runner.py:310] Step = 54000 ; steps/s = 0.73, tokens/s = 18499 (18499 target) ; Learning rate = 0.000380 ; Loss = 1.825591\n",
      "2024-12-14 07:35:04.124000: I runner.py:310] Step = 54100 ; steps/s = 0.72, tokens/s = 18539 (18539 target) ; Learning rate = 0.000380 ; Loss = 1.816463\n",
      "2024-12-14 07:37:22.850000: I runner.py:310] Step = 54200 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000380 ; Loss = 1.816486\n",
      "2024-12-14 07:39:41.593000: I runner.py:310] Step = 54300 ; steps/s = 0.72, tokens/s = 18553 (18553 target) ; Learning rate = 0.000379 ; Loss = 1.820794\n",
      "2024-12-14 07:41:58.291000: I runner.py:310] Step = 54400 ; steps/s = 0.73, tokens/s = 18491 (18491 target) ; Learning rate = 0.000379 ; Loss = 1.796749\n",
      "2024-12-14 07:44:17.020000: I runner.py:310] Step = 54500 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000379 ; Loss = 1.815864\n",
      "2024-12-14 07:46:35.745000: I runner.py:310] Step = 54600 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000378 ; Loss = 1.825175\n",
      "2024-12-14 07:48:54.502000: I runner.py:310] Step = 54700 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000378 ; Loss = 1.829790\n",
      "2024-12-14 07:51:11.243000: I runner.py:310] Step = 54800 ; steps/s = 0.73, tokens/s = 18486 (18486 target) ; Learning rate = 0.000378 ; Loss = 1.791643\n",
      "2024-12-14 07:53:30.032000: I runner.py:310] Step = 54900 ; steps/s = 0.72, tokens/s = 18544 (18544 target) ; Learning rate = 0.000377 ; Loss = 1.815629\n",
      "2024-12-14 07:55:48.790000: I runner.py:310] Step = 55000 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000377 ; Loss = 1.818360\n",
      "2024-12-14 07:58:07.530000: I runner.py:310] Step = 55100 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000377 ; Loss = 1.841852\n",
      "2024-12-14 08:00:24.154000: I runner.py:310] Step = 55200 ; steps/s = 0.73, tokens/s = 18498 (18498 target) ; Learning rate = 0.000376 ; Loss = 1.842945\n",
      "2024-12-14 08:02:42.989000: I runner.py:310] Step = 55300 ; steps/s = 0.72, tokens/s = 18539 (18539 target) ; Learning rate = 0.000376 ; Loss = 1.797666\n",
      "2024-12-14 08:05:01.687000: I runner.py:310] Step = 55400 ; steps/s = 0.72, tokens/s = 18560 (18560 target) ; Learning rate = 0.000376 ; Loss = 1.811028\n",
      "2024-12-14 08:07:20.145000: I runner.py:310] Step = 55500 ; steps/s = 0.72, tokens/s = 18589 (18589 target) ; Learning rate = 0.000375 ; Loss = 1.818581\n",
      "2024-12-14 08:09:36.724000: I runner.py:310] Step = 55600 ; steps/s = 0.73, tokens/s = 18507 (18507 target) ; Learning rate = 0.000375 ; Loss = 1.829538\n",
      "2024-12-14 08:11:55.495000: I runner.py:310] Step = 55700 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000375 ; Loss = 1.806821\n",
      "2024-12-14 08:14:14.256000: I runner.py:310] Step = 55800 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000374 ; Loss = 1.821921\n",
      "2024-12-14 08:16:30.949000: I runner.py:310] Step = 55900 ; steps/s = 0.73, tokens/s = 18488 (18488 target) ; Learning rate = 0.000374 ; Loss = 1.810577\n",
      "2024-12-14 08:18:49.730000: I runner.py:310] Step = 56000 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000374 ; Loss = 1.806518\n",
      "2024-12-14 08:21:08.452000: I runner.py:310] Step = 56100 ; steps/s = 0.72, tokens/s = 18553 (18553 target) ; Learning rate = 0.000373 ; Loss = 1.814057\n",
      "2024-12-14 08:23:27.246000: I runner.py:310] Step = 56200 ; steps/s = 0.72, tokens/s = 18544 (18544 target) ; Learning rate = 0.000373 ; Loss = 1.818465\n",
      "2024-12-14 08:25:43.949000: I runner.py:310] Step = 56300 ; steps/s = 0.73, tokens/s = 18489 (18489 target) ; Learning rate = 0.000373 ; Loss = 1.798768\n",
      "2024-12-14 08:28:02.697000: I runner.py:310] Step = 56400 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000372 ; Loss = 1.800640\n",
      "2024-12-14 08:30:21.441000: I runner.py:310] Step = 56500 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000372 ; Loss = 1.818929\n",
      "2024-12-14 08:32:40.216000: I runner.py:310] Step = 56600 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000372 ; Loss = 1.826443\n",
      "2024-12-14 08:34:56.871000: I runner.py:310] Step = 56700 ; steps/s = 0.73, tokens/s = 18493 (18493 target) ; Learning rate = 0.000371 ; Loss = 1.787813\n",
      "2024-12-14 08:37:15.636000: I runner.py:310] Step = 56800 ; steps/s = 0.72, tokens/s = 18546 (18546 target) ; Learning rate = 0.000371 ; Loss = 1.800534\n",
      "2024-12-14 08:39:34.401000: I runner.py:310] Step = 56900 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000371 ; Loss = 1.825105\n",
      "2024-12-14 08:41:53.122000: I runner.py:310] Step = 57000 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000370 ; Loss = 1.829935\n",
      "2024-12-14 08:44:09.810000: I runner.py:310] Step = 57100 ; steps/s = 0.73, tokens/s = 18493 (18493 target) ; Learning rate = 0.000370 ; Loss = 1.823314\n",
      "2024-12-14 08:46:28.583000: I runner.py:310] Step = 57200 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000370 ; Loss = 1.790248\n",
      "2024-12-14 08:48:47.354000: I runner.py:310] Step = 57300 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000369 ; Loss = 1.807140\n",
      "2024-12-14 08:51:06.106000: I runner.py:310] Step = 57400 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000369 ; Loss = 1.817513\n",
      "2024-12-14 08:53:22.811000: I runner.py:310] Step = 57500 ; steps/s = 0.73, tokens/s = 18492 (18492 target) ; Learning rate = 0.000369 ; Loss = 1.815885\n",
      "2024-12-14 08:55:41.560000: I runner.py:310] Step = 57600 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000368 ; Loss = 1.794735\n",
      "2024-12-14 08:58:00.402000: I runner.py:310] Step = 57700 ; steps/s = 0.72, tokens/s = 18537 (18537 target) ; Learning rate = 0.000368 ; Loss = 1.810123\n",
      "2024-12-14 09:00:18.860000: I runner.py:310] Step = 57800 ; steps/s = 0.72, tokens/s = 18592 (18592 target) ; Learning rate = 0.000368 ; Loss = 1.806977\n",
      "2024-12-14 09:02:35.482000: I runner.py:310] Step = 57900 ; steps/s = 0.73, tokens/s = 18501 (18501 target) ; Learning rate = 0.000367 ; Loss = 1.784122\n",
      "2024-12-14 09:04:54.266000: I runner.py:310] Step = 58000 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000367 ; Loss = 1.817162\n",
      "2024-12-14 09:07:13.063000: I runner.py:310] Step = 58100 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000367 ; Loss = 1.810889\n",
      "2024-12-14 09:09:31.839000: I runner.py:310] Step = 58200 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000366 ; Loss = 1.847085\n",
      "2024-12-14 09:11:48.568000: I runner.py:310] Step = 58300 ; steps/s = 0.73, tokens/s = 18486 (18486 target) ; Learning rate = 0.000366 ; Loss = 1.780103\n",
      "2024-12-14 09:14:07.257000: I runner.py:310] Step = 58400 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000366 ; Loss = 1.811968\n",
      "2024-12-14 09:16:25.995000: I runner.py:310] Step = 58500 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000365 ; Loss = 1.821607\n",
      "2024-12-14 09:18:42.720000: I runner.py:310] Step = 58600 ; steps/s = 0.73, tokens/s = 18491 (18491 target) ; Learning rate = 0.000365 ; Loss = 1.814841\n",
      "2024-12-14 09:21:01.512000: I runner.py:310] Step = 58700 ; steps/s = 0.72, tokens/s = 18544 (18544 target) ; Learning rate = 0.000365 ; Loss = 1.812703\n",
      "2024-12-14 09:23:20.173000: I runner.py:310] Step = 58800 ; steps/s = 0.72, tokens/s = 18565 (18565 target) ; Learning rate = 0.000365 ; Loss = 1.792205\n",
      "2024-12-14 09:25:39.003000: I runner.py:310] Step = 58900 ; steps/s = 0.72, tokens/s = 18539 (18539 target) ; Learning rate = 0.000364 ; Loss = 1.802938\n",
      "2024-12-14 09:27:55.639000: I runner.py:310] Step = 59000 ; steps/s = 0.73, tokens/s = 18499 (18499 target) ; Learning rate = 0.000364 ; Loss = 1.782406\n",
      "2024-12-14 09:30:14.432000: I runner.py:310] Step = 59100 ; steps/s = 0.72, tokens/s = 18542 (18542 target) ; Learning rate = 0.000364 ; Loss = 1.790842\n",
      "2024-12-14 09:32:33.182000: I runner.py:310] Step = 59200 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000363 ; Loss = 1.819835\n",
      "2024-12-14 09:34:51.847000: I runner.py:310] Step = 59300 ; steps/s = 0.72, tokens/s = 18567 (18567 target) ; Learning rate = 0.000363 ; Loss = 1.831128\n",
      "2024-12-14 09:37:08.572000: I runner.py:310] Step = 59400 ; steps/s = 0.73, tokens/s = 18485 (18485 target) ; Learning rate = 0.000363 ; Loss = 1.787666\n",
      "2024-12-14 09:39:27.328000: I runner.py:310] Step = 59500 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000362 ; Loss = 1.793663\n",
      "2024-12-14 09:41:46.088000: I runner.py:310] Step = 59600 ; steps/s = 0.72, tokens/s = 18548 (18548 target) ; Learning rate = 0.000362 ; Loss = 1.809077\n",
      "2024-12-14 09:44:04.836000: I runner.py:310] Step = 59700 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000362 ; Loss = 1.812430\n",
      "2024-12-14 09:46:21.561000: I runner.py:310] Step = 59800 ; steps/s = 0.73, tokens/s = 18486 (18486 target) ; Learning rate = 0.000361 ; Loss = 1.775794\n",
      "2024-12-14 09:48:40.359000: I runner.py:310] Step = 59900 ; steps/s = 0.72, tokens/s = 18543 (18543 target) ; Learning rate = 0.000361 ; Loss = 1.812241\n",
      "2024-12-14 09:50:59.045000: I runner.py:310] Step = 60000 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000361 ; Loss = 1.810543\n",
      "2024-12-14 09:51:00.548000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-60000\n",
      "2024-12-14 09:53:19.036000: I runner.py:310] Step = 60100 ; steps/s = 0.72, tokens/s = 18587 (18587 target) ; Learning rate = 0.000361 ; Loss = 1.814196\n",
      "2024-12-14 09:55:35.610000: I runner.py:310] Step = 60200 ; steps/s = 0.73, tokens/s = 18506 (18506 target) ; Learning rate = 0.000360 ; Loss = 1.768894\n",
      "2024-12-14 09:57:54.393000: I runner.py:310] Step = 60300 ; steps/s = 0.72, tokens/s = 18549 (18549 target) ; Learning rate = 0.000360 ; Loss = 1.801128\n",
      "2024-12-14 10:00:13.162000: I runner.py:310] Step = 60400 ; steps/s = 0.72, tokens/s = 18546 (18546 target) ; Learning rate = 0.000360 ; Loss = 1.803625\n",
      "2024-12-14 10:02:31.815000: I runner.py:310] Step = 60500 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000359 ; Loss = 1.823850\n",
      "2024-12-14 10:04:48.520000: I runner.py:310] Step = 60600 ; steps/s = 0.73, tokens/s = 18492 (18492 target) ; Learning rate = 0.000359 ; Loss = 1.765318\n",
      "2024-12-14 10:07:07.277000: I runner.py:310] Step = 60700 ; steps/s = 0.72, tokens/s = 18548 (18548 target) ; Learning rate = 0.000359 ; Loss = 1.790181\n",
      "2024-12-14 10:09:26.012000: I runner.py:310] Step = 60800 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000358 ; Loss = 1.814545\n",
      "2024-12-14 10:11:44.522000: I runner.py:310] Step = 60900 ; steps/s = 0.72, tokens/s = 18534 (18534 target) ; Learning rate = 0.000358 ; Loss = 1.816470\n",
      "2024-12-14 10:14:01.576000: I runner.py:310] Step = 61000 ; steps/s = 0.73, tokens/s = 18493 (18493 target) ; Learning rate = 0.000358 ; Loss = 1.775498\n",
      "2024-12-14 10:16:20.378000: I runner.py:310] Step = 61100 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000358 ; Loss = 1.808293\n",
      "2024-12-14 10:18:39.100000: I runner.py:310] Step = 61200 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000357 ; Loss = 1.810027\n",
      "2024-12-14 10:20:55.809000: I runner.py:310] Step = 61300 ; steps/s = 0.73, tokens/s = 18489 (18489 target) ; Learning rate = 0.000357 ; Loss = 1.799972\n",
      "2024-12-14 10:23:14.595000: I runner.py:310] Step = 61400 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000357 ; Loss = 1.775780\n",
      "2024-12-14 10:25:33.386000: I runner.py:310] Step = 61500 ; steps/s = 0.72, tokens/s = 18548 (18548 target) ; Learning rate = 0.000356 ; Loss = 1.793439\n",
      "2024-12-14 10:27:52.126000: I runner.py:310] Step = 61600 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000356 ; Loss = 1.813570\n",
      "2024-12-14 10:30:08.899000: I runner.py:310] Step = 61700 ; steps/s = 0.73, tokens/s = 18480 (18480 target) ; Learning rate = 0.000356 ; Loss = 1.783622\n",
      "2024-12-14 10:32:27.634000: I runner.py:310] Step = 61800 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000356 ; Loss = 1.792342\n",
      "2024-12-14 10:34:46.431000: I runner.py:310] Step = 61900 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000355 ; Loss = 1.792045\n",
      "2024-12-14 10:37:05.101000: I runner.py:310] Step = 62000 ; steps/s = 0.72, tokens/s = 18561 (18561 target) ; Learning rate = 0.000355 ; Loss = 1.807720\n",
      "2024-12-14 10:39:21.812000: I runner.py:310] Step = 62100 ; steps/s = 0.73, tokens/s = 18490 (18490 target) ; Learning rate = 0.000355 ; Loss = 1.789866\n",
      "2024-12-14 10:41:40.587000: I runner.py:310] Step = 62200 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000354 ; Loss = 1.798630\n",
      "2024-12-14 10:43:59.354000: I runner.py:310] Step = 62300 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000354 ; Loss = 1.793999\n",
      "2024-12-14 10:46:17.831000: I runner.py:310] Step = 62400 ; steps/s = 0.72, tokens/s = 18582 (18582 target) ; Learning rate = 0.000354 ; Loss = 1.787711\n",
      "2024-12-14 10:48:34.377000: I runner.py:310] Step = 62500 ; steps/s = 0.73, tokens/s = 18517 (18517 target) ; Learning rate = 0.000354 ; Loss = 1.802206\n",
      "2024-12-14 10:50:53.164000: I runner.py:310] Step = 62600 ; steps/s = 0.72, tokens/s = 18543 (18543 target) ; Learning rate = 0.000353 ; Loss = 1.784138\n",
      "2024-12-14 10:53:11.926000: I runner.py:310] Step = 62700 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000353 ; Loss = 1.788103\n",
      "2024-12-14 10:55:30.703000: I runner.py:310] Step = 62800 ; steps/s = 0.72, tokens/s = 18548 (18548 target) ; Learning rate = 0.000353 ; Loss = 1.787562\n",
      "2024-12-14 10:57:47.604000: I runner.py:310] Step = 62900 ; steps/s = 0.73, tokens/s = 18463 (18463 target) ; Learning rate = 0.000352 ; Loss = 1.760921\n",
      "2024-12-14 11:00:06.314000: I runner.py:310] Step = 63000 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000352 ; Loss = 1.788844\n",
      "2024-12-14 11:02:24.949000: I runner.py:310] Step = 63100 ; steps/s = 0.72, tokens/s = 18568 (18568 target) ; Learning rate = 0.000352 ; Loss = 1.800749\n",
      "2024-12-14 11:04:43.632000: I runner.py:310] Step = 63200 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000352 ; Loss = 1.795564\n",
      "2024-12-14 11:07:00.186000: I runner.py:310] Step = 63300 ; steps/s = 0.73, tokens/s = 18513 (18513 target) ; Learning rate = 0.000351 ; Loss = 1.751617\n",
      "2024-12-14 11:09:18.943000: I runner.py:310] Step = 63400 ; steps/s = 0.72, tokens/s = 18547 (18547 target) ; Learning rate = 0.000351 ; Loss = 1.797641\n",
      "2024-12-14 11:11:37.595000: I runner.py:310] Step = 63500 ; steps/s = 0.72, tokens/s = 18563 (18563 target) ; Learning rate = 0.000351 ; Loss = 1.813583\n",
      "2024-12-14 11:13:54.912000: I runner.py:310] Step = 63600 ; steps/s = 0.73, tokens/s = 18521 (18521 target) ; Learning rate = 0.000350 ; Loss = 1.817993\n",
      "2024-12-14 11:16:12.890000: I runner.py:310] Step = 63700 ; steps/s = 0.72, tokens/s = 18543 (18543 target) ; Learning rate = 0.000350 ; Loss = 1.799841\n",
      "2024-12-14 11:18:31.655000: I runner.py:310] Step = 63800 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000350 ; Loss = 1.783479\n",
      "2024-12-14 11:20:50.296000: I runner.py:310] Step = 63900 ; steps/s = 0.72, tokens/s = 18565 (18565 target) ; Learning rate = 0.000350 ; Loss = 1.793105\n",
      "2024-12-14 11:23:06.951000: I runner.py:310] Step = 64000 ; steps/s = 0.73, tokens/s = 18498 (18498 target) ; Learning rate = 0.000349 ; Loss = 1.781970\n",
      "2024-12-14 11:25:25.653000: I runner.py:310] Step = 64100 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000349 ; Loss = 1.773741\n",
      "2024-12-14 11:27:44.298000: I runner.py:310] Step = 64200 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000349 ; Loss = 1.789560\n",
      "2024-12-14 11:30:02.963000: I runner.py:310] Step = 64300 ; steps/s = 0.72, tokens/s = 18563 (18563 target) ; Learning rate = 0.000349 ; Loss = 1.798715\n",
      "2024-12-14 11:32:19.592000: I runner.py:310] Step = 64400 ; steps/s = 0.73, tokens/s = 18499 (18499 target) ; Learning rate = 0.000348 ; Loss = 1.781607\n",
      "2024-12-14 11:34:38.215000: I runner.py:310] Step = 64500 ; steps/s = 0.72, tokens/s = 18569 (18569 target) ; Learning rate = 0.000348 ; Loss = 1.783844\n",
      "2024-12-14 11:36:56.843000: I runner.py:310] Step = 64600 ; steps/s = 0.72, tokens/s = 18565 (18565 target) ; Learning rate = 0.000348 ; Loss = 1.792043\n",
      "2024-12-14 11:39:15.559000: I runner.py:310] Step = 64700 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000347 ; Loss = 1.798880\n",
      "2024-12-14 11:41:32.100000: I runner.py:310] Step = 64800 ; steps/s = 0.73, tokens/s = 18513 (18513 target) ; Learning rate = 0.000347 ; Loss = 1.800348\n",
      "2024-12-14 11:43:50.802000: I runner.py:310] Step = 64900 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000347 ; Loss = 1.782299\n",
      "2024-12-14 11:46:09.466000: I runner.py:310] Step = 65000 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000347 ; Loss = 1.783925\n",
      "2024-12-14 11:48:28.159000: I runner.py:310] Step = 65100 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000346 ; Loss = 1.793043\n",
      "2024-12-14 11:50:44.683000: I runner.py:310] Step = 65200 ; steps/s = 0.73, tokens/s = 18515 (18515 target) ; Learning rate = 0.000346 ; Loss = 1.754606\n",
      "2024-12-14 11:53:03.392000: I runner.py:310] Step = 65300 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000346 ; Loss = 1.787998\n",
      "2024-12-14 11:55:22.035000: I runner.py:310] Step = 65400 ; steps/s = 0.72, tokens/s = 18563 (18563 target) ; Learning rate = 0.000346 ; Loss = 1.790315\n",
      "2024-12-14 11:57:40.743000: I runner.py:310] Step = 65500 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000345 ; Loss = 1.793946\n",
      "2024-12-14 11:59:57.337000: I runner.py:310] Step = 65600 ; steps/s = 0.73, tokens/s = 18509 (18509 target) ; Learning rate = 0.000345 ; Loss = 1.812673\n",
      "2024-12-14 12:02:16.066000: I runner.py:310] Step = 65700 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000345 ; Loss = 1.774765\n",
      "2024-12-14 12:04:34.674000: I runner.py:310] Step = 65800 ; steps/s = 0.72, tokens/s = 18569 (18569 target) ; Learning rate = 0.000345 ; Loss = 1.777241\n",
      "2024-12-14 12:06:53.420000: I runner.py:310] Step = 65900 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000344 ; Loss = 1.788063\n",
      "2024-12-14 12:09:10.014000: I runner.py:310] Step = 66000 ; steps/s = 0.73, tokens/s = 18508 (18508 target) ; Learning rate = 0.000344 ; Loss = 1.751556\n",
      "2024-12-14 12:11:28.731000: I runner.py:310] Step = 66100 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000344 ; Loss = 1.785471\n",
      "2024-12-14 12:13:47.364000: I runner.py:310] Step = 66200 ; steps/s = 0.72, tokens/s = 18568 (18568 target) ; Learning rate = 0.000344 ; Loss = 1.804132\n",
      "2024-12-14 12:16:04.040000: I runner.py:310] Step = 66300 ; steps/s = 0.73, tokens/s = 18496 (18496 target) ; Learning rate = 0.000343 ; Loss = 1.782857\n",
      "2024-12-14 12:18:22.728000: I runner.py:310] Step = 66400 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000343 ; Loss = 1.768039\n",
      "2024-12-14 12:20:41.380000: I runner.py:310] Step = 66500 ; steps/s = 0.72, tokens/s = 18563 (18563 target) ; Learning rate = 0.000343 ; Loss = 1.794265\n",
      "2024-12-14 12:23:00.026000: I runner.py:310] Step = 66600 ; steps/s = 0.72, tokens/s = 18569 (18569 target) ; Learning rate = 0.000342 ; Loss = 1.813387\n",
      "2024-12-14 12:25:16.606000: I runner.py:310] Step = 66700 ; steps/s = 0.73, tokens/s = 18505 (18505 target) ; Learning rate = 0.000342 ; Loss = 1.775556\n",
      "2024-12-14 12:27:35.351000: I runner.py:310] Step = 66800 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000342 ; Loss = 1.763720\n",
      "2024-12-14 12:29:53.963000: I runner.py:310] Step = 66900 ; steps/s = 0.72, tokens/s = 18569 (18569 target) ; Learning rate = 0.000342 ; Loss = 1.806741\n",
      "2024-12-14 12:32:12.599000: I runner.py:310] Step = 67000 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000341 ; Loss = 1.803317\n",
      "2024-12-14 12:34:29.252000: I runner.py:310] Step = 67100 ; steps/s = 0.73, tokens/s = 18500 (18500 target) ; Learning rate = 0.000341 ; Loss = 1.790248\n",
      "2024-12-14 12:36:47.841000: I runner.py:310] Step = 67200 ; steps/s = 0.72, tokens/s = 18573 (18573 target) ; Learning rate = 0.000341 ; Loss = 1.787733\n",
      "2024-12-14 12:39:06.632000: I runner.py:310] Step = 67300 ; steps/s = 0.72, tokens/s = 18544 (18544 target) ; Learning rate = 0.000341 ; Loss = 1.785866\n",
      "2024-12-14 12:41:25.281000: I runner.py:310] Step = 67400 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000340 ; Loss = 1.786994\n",
      "2024-12-14 12:43:41.915000: I runner.py:310] Step = 67500 ; steps/s = 0.73, tokens/s = 18498 (18498 target) ; Learning rate = 0.000340 ; Loss = 1.751272\n",
      "2024-12-14 12:46:00.591000: I runner.py:310] Step = 67600 ; steps/s = 0.72, tokens/s = 18560 (18560 target) ; Learning rate = 0.000340 ; Loss = 1.777169\n",
      "2024-12-14 12:48:19.291000: I runner.py:310] Step = 67700 ; steps/s = 0.72, tokens/s = 18560 (18560 target) ; Learning rate = 0.000340 ; Loss = 1.800655\n",
      "2024-12-14 12:50:37.975000: I runner.py:310] Step = 67800 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000339 ; Loss = 1.800291\n",
      "2024-12-14 12:52:54.586000: I runner.py:310] Step = 67900 ; steps/s = 0.73, tokens/s = 18507 (18507 target) ; Learning rate = 0.000339 ; Loss = 1.766085\n",
      "2024-12-14 12:55:13.191000: I runner.py:310] Step = 68000 ; steps/s = 0.72, tokens/s = 18569 (18569 target) ; Learning rate = 0.000339 ; Loss = 1.779763\n",
      "2024-12-14 12:57:31.960000: I runner.py:310] Step = 68100 ; steps/s = 0.72, tokens/s = 18548 (18548 target) ; Learning rate = 0.000339 ; Loss = 1.783910\n",
      "2024-12-14 12:59:50.689000: I runner.py:310] Step = 68200 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000338 ; Loss = 1.790339\n",
      "2024-12-14 13:02:07.314000: I runner.py:310] Step = 68300 ; steps/s = 0.73, tokens/s = 18497 (18497 target) ; Learning rate = 0.000338 ; Loss = 1.805647\n",
      "2024-12-14 13:04:26.036000: I runner.py:310] Step = 68400 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000338 ; Loss = 1.768825\n",
      "2024-12-14 13:06:44.690000: I runner.py:310] Step = 68500 ; steps/s = 0.72, tokens/s = 18568 (18568 target) ; Learning rate = 0.000338 ; Loss = 1.763689\n",
      "2024-12-14 13:09:03.342000: I runner.py:310] Step = 68600 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000337 ; Loss = 1.777928\n",
      "2024-12-14 13:11:19.958000: I runner.py:310] Step = 68700 ; steps/s = 0.73, tokens/s = 18504 (18504 target) ; Learning rate = 0.000337 ; Loss = 1.761906\n",
      "2024-12-14 13:13:38.666000: I runner.py:310] Step = 68800 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000337 ; Loss = 1.798883\n",
      "2024-12-14 13:15:57.337000: I runner.py:310] Step = 68900 ; steps/s = 0.72, tokens/s = 18567 (18567 target) ; Learning rate = 0.000337 ; Loss = 1.795266\n",
      "2024-12-14 13:18:13.997000: I runner.py:310] Step = 69000 ; steps/s = 0.73, tokens/s = 18494 (18494 target) ; Learning rate = 0.000336 ; Loss = 1.781223\n",
      "2024-12-14 13:20:32.716000: I runner.py:310] Step = 69100 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000336 ; Loss = 1.802291\n",
      "2024-12-14 13:22:51.420000: I runner.py:310] Step = 69200 ; steps/s = 0.72, tokens/s = 18561 (18561 target) ; Learning rate = 0.000336 ; Loss = 1.756011\n",
      "2024-12-14 13:25:10.106000: I runner.py:310] Step = 69300 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000336 ; Loss = 1.776512\n",
      "2024-12-14 13:27:26.644000: I runner.py:310] Step = 69400 ; steps/s = 0.73, tokens/s = 18508 (18508 target) ; Learning rate = 0.000336 ; Loss = 1.752806\n",
      "2024-12-14 13:29:45.364000: I runner.py:310] Step = 69500 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000335 ; Loss = 1.767585\n",
      "2024-12-14 13:32:04.044000: I runner.py:310] Step = 69600 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000335 ; Loss = 1.785044\n",
      "2024-12-14 13:34:22.798000: I runner.py:310] Step = 69700 ; steps/s = 0.72, tokens/s = 18552 (18552 target) ; Learning rate = 0.000335 ; Loss = 1.796483\n",
      "2024-12-14 13:36:39.356000: I runner.py:310] Step = 69800 ; steps/s = 0.73, tokens/s = 18511 (18511 target) ; Learning rate = 0.000335 ; Loss = 1.749777\n",
      "2024-12-14 13:38:58.051000: I runner.py:310] Step = 69900 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000334 ; Loss = 1.781149\n",
      "2024-12-14 13:41:16.604000: I runner.py:310] Step = 70000 ; steps/s = 0.72, tokens/s = 18580 (18580 target) ; Learning rate = 0.000334 ; Loss = 1.786019\n",
      "2024-12-14 13:41:18.125000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-70000\n",
      "2024-12-14 13:43:35.900000: I runner.py:310] Step = 70100 ; steps/s = 0.73, tokens/s = 18679 (18679 target) ; Learning rate = 0.000334 ; Loss = 1.789197\n",
      "2024-12-14 13:45:51.987000: I runner.py:310] Step = 70200 ; steps/s = 0.73, tokens/s = 18573 (18573 target) ; Learning rate = 0.000334 ; Loss = 1.756170\n",
      "2024-12-14 13:48:10.343000: I runner.py:310] Step = 70300 ; steps/s = 0.72, tokens/s = 18605 (18605 target) ; Learning rate = 0.000333 ; Loss = 1.765824\n",
      "2024-12-14 13:50:28.717000: I runner.py:310] Step = 70400 ; steps/s = 0.72, tokens/s = 18598 (18598 target) ; Learning rate = 0.000333 ; Loss = 1.778766\n",
      "2024-12-14 13:52:47.285000: I runner.py:310] Step = 70500 ; steps/s = 0.72, tokens/s = 18577 (18577 target) ; Learning rate = 0.000333 ; Loss = 1.795442\n",
      "2024-12-14 13:55:03.883000: I runner.py:310] Step = 70600 ; steps/s = 0.73, tokens/s = 18504 (18504 target) ; Learning rate = 0.000333 ; Loss = 1.758467\n",
      "2024-12-14 13:57:22.548000: I runner.py:310] Step = 70700 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000332 ; Loss = 1.779619\n",
      "2024-12-14 13:59:41.222000: I runner.py:310] Step = 70800 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000332 ; Loss = 1.789631\n",
      "2024-12-14 14:01:59.896000: I runner.py:310] Step = 70900 ; steps/s = 0.72, tokens/s = 18560 (18560 target) ; Learning rate = 0.000332 ; Loss = 1.779849\n",
      "2024-12-14 14:04:16.527000: I runner.py:310] Step = 71000 ; steps/s = 0.73, tokens/s = 18502 (18502 target) ; Learning rate = 0.000332 ; Loss = 1.795955\n",
      "2024-12-14 14:06:35.171000: I runner.py:310] Step = 71100 ; steps/s = 0.72, tokens/s = 18567 (18567 target) ; Learning rate = 0.000331 ; Loss = 1.748307\n",
      "2024-12-14 14:08:53.873000: I runner.py:310] Step = 71200 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000331 ; Loss = 1.764825\n",
      "2024-12-14 14:11:12.512000: I runner.py:310] Step = 71300 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000331 ; Loss = 1.771397\n",
      "2024-12-14 14:13:29.114000: I runner.py:310] Step = 71400 ; steps/s = 0.73, tokens/s = 18505 (18505 target) ; Learning rate = 0.000331 ; Loss = 1.749606\n",
      "2024-12-14 14:15:47.755000: I runner.py:310] Step = 71500 ; steps/s = 0.72, tokens/s = 18568 (18568 target) ; Learning rate = 0.000331 ; Loss = 1.784399\n",
      "2024-12-14 14:18:06.466000: I runner.py:310] Step = 71600 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000330 ; Loss = 1.776531\n",
      "2024-12-14 14:20:23.088000: I runner.py:310] Step = 71700 ; steps/s = 0.73, tokens/s = 18503 (18503 target) ; Learning rate = 0.000330 ; Loss = 1.780053\n",
      "2024-12-14 14:22:41.660000: I runner.py:310] Step = 71800 ; steps/s = 0.72, tokens/s = 18575 (18575 target) ; Learning rate = 0.000330 ; Loss = 1.773259\n",
      "2024-12-14 14:25:00.244000: I runner.py:310] Step = 71900 ; steps/s = 0.72, tokens/s = 18573 (18573 target) ; Learning rate = 0.000330 ; Loss = 1.761097\n",
      "2024-12-14 14:27:18.890000: I runner.py:310] Step = 72000 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000329 ; Loss = 1.765645\n",
      "2024-12-14 14:29:35.530000: I runner.py:310] Step = 72100 ; steps/s = 0.73, tokens/s = 18497 (18497 target) ; Learning rate = 0.000329 ; Loss = 1.759976\n",
      "2024-12-14 14:31:54.191000: I runner.py:310] Step = 72200 ; steps/s = 0.72, tokens/s = 18563 (18563 target) ; Learning rate = 0.000329 ; Loss = 1.764961\n",
      "2024-12-14 14:34:12.865000: I runner.py:310] Step = 72300 ; steps/s = 0.72, tokens/s = 18561 (18561 target) ; Learning rate = 0.000329 ; Loss = 1.776170\n",
      "2024-12-14 14:36:31.593000: I runner.py:310] Step = 72400 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000328 ; Loss = 1.790721\n",
      "2024-12-14 14:38:48.104000: I runner.py:310] Step = 72500 ; steps/s = 0.73, tokens/s = 18515 (18515 target) ; Learning rate = 0.000328 ; Loss = 1.787484\n",
      "2024-12-14 14:41:06.741000: I runner.py:310] Step = 72600 ; steps/s = 0.72, tokens/s = 18568 (18568 target) ; Learning rate = 0.000328 ; Loss = 1.773574\n",
      "2024-12-14 14:43:25.130000: I runner.py:310] Step = 72700 ; steps/s = 0.72, tokens/s = 18602 (18602 target) ; Learning rate = 0.000328 ; Loss = 1.765686\n",
      "2024-12-14 14:45:43.469000: I runner.py:310] Step = 72800 ; steps/s = 0.72, tokens/s = 18604 (18604 target) ; Learning rate = 0.000328 ; Loss = 1.762520\n",
      "2024-12-14 14:47:59.758000: I runner.py:310] Step = 72900 ; steps/s = 0.73, tokens/s = 18547 (18547 target) ; Learning rate = 0.000327 ; Loss = 1.776863\n",
      "2024-12-14 14:50:18.222000: I runner.py:310] Step = 73000 ; steps/s = 0.72, tokens/s = 18591 (18591 target) ; Learning rate = 0.000327 ; Loss = 1.759195\n",
      "2024-12-14 14:52:36.842000: I runner.py:310] Step = 73100 ; steps/s = 0.72, tokens/s = 18567 (18567 target) ; Learning rate = 0.000327 ; Loss = 1.762105\n",
      "2024-12-14 14:54:55.499000: I runner.py:310] Step = 73200 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000327 ; Loss = 1.751806\n",
      "2024-12-14 14:57:12.075000: I runner.py:310] Step = 73300 ; steps/s = 0.73, tokens/s = 18508 (18508 target) ; Learning rate = 0.000326 ; Loss = 1.745943\n",
      "2024-12-14 14:59:30.745000: I runner.py:310] Step = 73400 ; steps/s = 0.72, tokens/s = 18562 (18562 target) ; Learning rate = 0.000326 ; Loss = 1.766845\n",
      "2024-12-14 15:01:49.390000: I runner.py:310] Step = 73500 ; steps/s = 0.72, tokens/s = 18565 (18565 target) ; Learning rate = 0.000326 ; Loss = 1.776315\n",
      "2024-12-14 15:04:08.039000: I runner.py:310] Step = 73600 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000326 ; Loss = 1.787559\n",
      "2024-12-14 15:06:24.619000: I runner.py:310] Step = 73700 ; steps/s = 0.73, tokens/s = 18507 (18507 target) ; Learning rate = 0.000326 ; Loss = 1.740564\n",
      "2024-12-14 15:08:43.297000: I runner.py:310] Step = 73800 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000325 ; Loss = 1.774310\n",
      "2024-12-14 15:11:01.970000: I runner.py:310] Step = 73900 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000325 ; Loss = 1.779001\n",
      "2024-12-14 15:13:20.692000: I runner.py:310] Step = 74000 ; steps/s = 0.72, tokens/s = 18560 (18560 target) ; Learning rate = 0.000325 ; Loss = 1.789148\n",
      "2024-12-14 15:15:37.234000: I runner.py:310] Step = 74100 ; steps/s = 0.73, tokens/s = 18514 (18514 target) ; Learning rate = 0.000325 ; Loss = 1.791266\n",
      "2024-12-14 15:17:55.973000: I runner.py:310] Step = 74200 ; steps/s = 0.72, tokens/s = 18553 (18553 target) ; Learning rate = 0.000324 ; Loss = 1.763035\n",
      "2024-12-14 15:20:14.670000: I runner.py:310] Step = 74300 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000324 ; Loss = 1.768764\n",
      "2024-12-14 15:22:31.249000: I runner.py:310] Step = 74400 ; steps/s = 0.73, tokens/s = 18504 (18504 target) ; Learning rate = 0.000324 ; Loss = 1.758110\n",
      "2024-12-14 15:24:49.912000: I runner.py:310] Step = 74500 ; steps/s = 0.72, tokens/s = 18565 (18565 target) ; Learning rate = 0.000324 ; Loss = 1.757175\n",
      "2024-12-14 15:27:08.558000: I runner.py:310] Step = 74600 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000324 ; Loss = 1.776889\n",
      "2024-12-14 15:29:27.217000: I runner.py:310] Step = 74700 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000323 ; Loss = 1.777990\n",
      "2024-12-14 15:31:43.889000: I runner.py:310] Step = 74800 ; steps/s = 0.73, tokens/s = 18494 (18494 target) ; Learning rate = 0.000323 ; Loss = 1.750106\n",
      "2024-12-14 15:34:02.593000: I runner.py:310] Step = 74900 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000323 ; Loss = 1.758597\n",
      "2024-12-14 15:36:21.239000: I runner.py:310] Step = 75000 ; steps/s = 0.72, tokens/s = 18565 (18565 target) ; Learning rate = 0.000323 ; Loss = 1.768743\n",
      "2024-12-14 15:38:39.853000: I runner.py:310] Step = 75100 ; steps/s = 0.72, tokens/s = 18565 (18565 target) ; Learning rate = 0.000323 ; Loss = 1.779700\n",
      "2024-12-14 15:40:56.313000: I runner.py:310] Step = 75200 ; steps/s = 0.73, tokens/s = 18523 (18523 target) ; Learning rate = 0.000322 ; Loss = 1.783121\n",
      "2024-12-14 15:43:14.703000: I runner.py:310] Step = 75300 ; steps/s = 0.72, tokens/s = 18596 (18596 target) ; Learning rate = 0.000322 ; Loss = 1.771880\n",
      "2024-12-14 15:45:33.315000: I runner.py:310] Step = 75400 ; steps/s = 0.72, tokens/s = 18570 (18570 target) ; Learning rate = 0.000322 ; Loss = 1.749464\n",
      "2024-12-14 15:47:51.989000: I runner.py:310] Step = 75500 ; steps/s = 0.72, tokens/s = 18562 (18562 target) ; Learning rate = 0.000322 ; Loss = 1.767846\n",
      "2024-12-14 15:50:08.602000: I runner.py:310] Step = 75600 ; steps/s = 0.73, tokens/s = 18501 (18501 target) ; Learning rate = 0.000321 ; Loss = 1.735397\n",
      "2024-12-14 15:52:27.268000: I runner.py:310] Step = 75700 ; steps/s = 0.72, tokens/s = 18562 (18562 target) ; Learning rate = 0.000321 ; Loss = 1.762847\n",
      "2024-12-14 15:54:45.973000: I runner.py:310] Step = 75800 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000321 ; Loss = 1.775612\n",
      "2024-12-14 15:57:04.641000: I runner.py:310] Step = 75900 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000321 ; Loss = 1.770527\n",
      "2024-12-14 15:59:21.227000: I runner.py:310] Step = 76000 ; steps/s = 0.73, tokens/s = 18508 (18508 target) ; Learning rate = 0.000321 ; Loss = 1.736931\n",
      "2024-12-14 16:01:39.909000: I runner.py:310] Step = 76100 ; steps/s = 0.72, tokens/s = 18563 (18563 target) ; Learning rate = 0.000320 ; Loss = 1.763747\n",
      "2024-12-14 16:03:58.597000: I runner.py:310] Step = 76200 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000320 ; Loss = 1.766984\n",
      "2024-12-14 16:06:17.330000: I runner.py:310] Step = 76300 ; steps/s = 0.72, tokens/s = 18550 (18550 target) ; Learning rate = 0.000320 ; Loss = 1.779556\n",
      "2024-12-14 16:08:33.800000: I runner.py:310] Step = 76400 ; steps/s = 0.73, tokens/s = 18522 (18522 target) ; Learning rate = 0.000320 ; Loss = 1.790842\n",
      "2024-12-14 16:10:52.424000: I runner.py:310] Step = 76500 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000320 ; Loss = 1.753910\n",
      "2024-12-14 16:13:11.127000: I runner.py:310] Step = 76600 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000319 ; Loss = 1.754543\n",
      "2024-12-14 16:15:29.741000: I runner.py:310] Step = 76700 ; steps/s = 0.72, tokens/s = 18570 (18570 target) ; Learning rate = 0.000319 ; Loss = 1.769060\n",
      "2024-12-14 16:17:46.387000: I runner.py:310] Step = 76800 ; steps/s = 0.73, tokens/s = 18499 (18499 target) ; Learning rate = 0.000319 ; Loss = 1.772599\n",
      "2024-12-14 16:20:05.024000: I runner.py:310] Step = 76900 ; steps/s = 0.72, tokens/s = 18565 (18565 target) ; Learning rate = 0.000319 ; Loss = 1.760303\n",
      "2024-12-14 16:22:23.597000: I runner.py:310] Step = 77000 ; steps/s = 0.72, tokens/s = 18574 (18574 target) ; Learning rate = 0.000319 ; Loss = 1.757371\n",
      "2024-12-14 16:24:40.215000: I runner.py:310] Step = 77100 ; steps/s = 0.73, tokens/s = 18503 (18503 target) ; Learning rate = 0.000318 ; Loss = 1.746903\n",
      "2024-12-14 16:26:58.936000: I runner.py:310] Step = 77200 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000318 ; Loss = 1.741418\n",
      "2024-12-14 16:29:17.459000: I runner.py:310] Step = 77300 ; steps/s = 0.72, tokens/s = 18583 (18583 target) ; Learning rate = 0.000318 ; Loss = 1.763308\n",
      "2024-12-14 16:31:36.139000: I runner.py:310] Step = 77400 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000318 ; Loss = 1.773387\n",
      "2024-12-14 16:33:52.784000: I runner.py:310] Step = 77500 ; steps/s = 0.73, tokens/s = 18497 (18497 target) ; Learning rate = 0.000317 ; Loss = 1.744792\n",
      "2024-12-14 16:36:10.996000: I runner.py:310] Step = 77600 ; steps/s = 0.72, tokens/s = 18625 (18625 target) ; Learning rate = 0.000317 ; Loss = 1.759238\n",
      "2024-12-14 16:38:29.640000: I runner.py:310] Step = 77700 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000317 ; Loss = 1.757701\n",
      "2024-12-14 16:40:48.316000: I runner.py:310] Step = 77800 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000317 ; Loss = 1.773648\n",
      "2024-12-14 16:43:04.937000: I runner.py:310] Step = 77900 ; steps/s = 0.73, tokens/s = 18499 (18499 target) ; Learning rate = 0.000317 ; Loss = 1.762226\n",
      "2024-12-14 16:45:23.571000: I runner.py:310] Step = 78000 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000316 ; Loss = 1.756484\n",
      "2024-12-14 16:47:42.195000: I runner.py:310] Step = 78100 ; steps/s = 0.72, tokens/s = 18568 (18568 target) ; Learning rate = 0.000316 ; Loss = 1.746562\n",
      "2024-12-14 16:50:00.863000: I runner.py:310] Step = 78200 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000316 ; Loss = 1.762388\n",
      "2024-12-14 16:52:17.405000: I runner.py:310] Step = 78300 ; steps/s = 0.73, tokens/s = 18508 (18508 target) ; Learning rate = 0.000316 ; Loss = 1.742707\n",
      "2024-12-14 16:54:36.090000: I runner.py:310] Step = 78400 ; steps/s = 0.72, tokens/s = 18561 (18561 target) ; Learning rate = 0.000316 ; Loss = 1.753393\n",
      "2024-12-14 16:56:54.742000: I runner.py:310] Step = 78500 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000315 ; Loss = 1.768725\n",
      "2024-12-14 16:59:13.403000: I runner.py:310] Step = 78600 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000315 ; Loss = 1.763322\n",
      "2024-12-14 17:01:30.004000: I runner.py:310] Step = 78700 ; steps/s = 0.73, tokens/s = 18505 (18505 target) ; Learning rate = 0.000315 ; Loss = 1.725612\n",
      "2024-12-14 17:03:48.699000: I runner.py:310] Step = 78800 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000315 ; Loss = 1.755062\n",
      "2024-12-14 17:06:07.329000: I runner.py:310] Step = 78900 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000315 ; Loss = 1.762981\n",
      "2024-12-14 17:08:25.972000: I runner.py:310] Step = 79000 ; steps/s = 0.72, tokens/s = 18567 (18567 target) ; Learning rate = 0.000314 ; Loss = 1.783157\n",
      "2024-12-14 17:10:42.524000: I runner.py:310] Step = 79100 ; steps/s = 0.73, tokens/s = 18513 (18513 target) ; Learning rate = 0.000314 ; Loss = 1.730700\n",
      "2024-12-14 17:13:01.233000: I runner.py:310] Step = 79200 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000314 ; Loss = 1.742706\n",
      "2024-12-14 17:15:19.877000: I runner.py:310] Step = 79300 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000314 ; Loss = 1.771001\n",
      "2024-12-14 17:17:38.495000: I runner.py:310] Step = 79400 ; steps/s = 0.72, tokens/s = 18570 (18570 target) ; Learning rate = 0.000314 ; Loss = 1.772922\n",
      "2024-12-14 17:19:55.149000: I runner.py:310] Step = 79500 ; steps/s = 0.73, tokens/s = 18500 (18500 target) ; Learning rate = 0.000313 ; Loss = 1.732742\n",
      "2024-12-14 17:22:13.817000: I runner.py:310] Step = 79600 ; steps/s = 0.72, tokens/s = 18561 (18561 target) ; Learning rate = 0.000313 ; Loss = 1.754311\n",
      "2024-12-14 17:24:32.505000: I runner.py:310] Step = 79700 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000313 ; Loss = 1.779940\n",
      "2024-12-14 17:26:49.171000: I runner.py:310] Step = 79800 ; steps/s = 0.73, tokens/s = 18496 (18496 target) ; Learning rate = 0.000313 ; Loss = 1.753162\n",
      "2024-12-14 17:29:07.493000: I runner.py:310] Step = 79900 ; steps/s = 0.72, tokens/s = 18608 (18608 target) ; Learning rate = 0.000313 ; Loss = 1.734522\n",
      "2024-12-14 17:31:26.030000: I runner.py:310] Step = 80000 ; steps/s = 0.72, tokens/s = 18581 (18581 target) ; Learning rate = 0.000312 ; Loss = 1.752250\n",
      "2024-12-14 17:31:27.639000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-80000\n",
      "2024-12-14 17:33:46.260000: I runner.py:310] Step = 80100 ; steps/s = 0.72, tokens/s = 18570 (18570 target) ; Learning rate = 0.000312 ; Loss = 1.769230\n",
      "2024-12-14 17:36:02.863000: I runner.py:310] Step = 80200 ; steps/s = 0.73, tokens/s = 18502 (18502 target) ; Learning rate = 0.000312 ; Loss = 1.760123\n",
      "2024-12-14 17:38:21.499000: I runner.py:310] Step = 80300 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000312 ; Loss = 1.748221\n",
      "2024-12-14 17:40:40.088000: I runner.py:310] Step = 80400 ; steps/s = 0.72, tokens/s = 18571 (18571 target) ; Learning rate = 0.000312 ; Loss = 1.763515\n",
      "2024-12-14 17:42:58.768000: I runner.py:310] Step = 80500 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000312 ; Loss = 1.755354\n",
      "2024-12-14 17:45:15.298000: I runner.py:310] Step = 80600 ; steps/s = 0.73, tokens/s = 18517 (18517 target) ; Learning rate = 0.000311 ; Loss = 1.761023\n",
      "2024-12-14 17:47:33.922000: I runner.py:310] Step = 80700 ; steps/s = 0.72, tokens/s = 18563 (18563 target) ; Learning rate = 0.000311 ; Loss = 1.747375\n",
      "2024-12-14 17:49:52.582000: I runner.py:310] Step = 80800 ; steps/s = 0.72, tokens/s = 18565 (18565 target) ; Learning rate = 0.000311 ; Loss = 1.767751\n",
      "2024-12-14 17:52:11.227000: I runner.py:310] Step = 80900 ; steps/s = 0.72, tokens/s = 18562 (18562 target) ; Learning rate = 0.000311 ; Loss = 1.747522\n",
      "2024-12-14 17:54:27.849000: I runner.py:310] Step = 81000 ; steps/s = 0.73, tokens/s = 18503 (18503 target) ; Learning rate = 0.000311 ; Loss = 1.734628\n",
      "2024-12-14 17:56:46.477000: I runner.py:310] Step = 81100 ; steps/s = 0.72, tokens/s = 18571 (18571 target) ; Learning rate = 0.000310 ; Loss = 1.754652\n",
      "2024-12-14 17:59:05.116000: I runner.py:310] Step = 81200 ; steps/s = 0.72, tokens/s = 18561 (18561 target) ; Learning rate = 0.000310 ; Loss = 1.758953\n",
      "2024-12-14 18:01:23.835000: I runner.py:310] Step = 81300 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000310 ; Loss = 1.777099\n",
      "2024-12-14 18:03:40.355000: I runner.py:310] Step = 81400 ; steps/s = 0.73, tokens/s = 18514 (18514 target) ; Learning rate = 0.000310 ; Loss = 1.725186\n",
      "2024-12-14 18:05:58.979000: I runner.py:310] Step = 81500 ; steps/s = 0.72, tokens/s = 18571 (18571 target) ; Learning rate = 0.000310 ; Loss = 1.758738\n",
      "2024-12-14 18:08:17.632000: I runner.py:310] Step = 81600 ; steps/s = 0.72, tokens/s = 18565 (18565 target) ; Learning rate = 0.000309 ; Loss = 1.763417\n",
      "2024-12-14 18:10:36.366000: I runner.py:310] Step = 81700 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000309 ; Loss = 1.762624\n",
      "2024-12-14 18:12:52.943000: I runner.py:310] Step = 81800 ; steps/s = 0.73, tokens/s = 18509 (18509 target) ; Learning rate = 0.000309 ; Loss = 1.724840\n",
      "2024-12-14 18:15:11.549000: I runner.py:310] Step = 81900 ; steps/s = 0.72, tokens/s = 18569 (18569 target) ; Learning rate = 0.000309 ; Loss = 1.756507\n",
      "2024-12-14 18:17:30.183000: I runner.py:310] Step = 82000 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000309 ; Loss = 1.750675\n",
      "2024-12-14 18:19:48.789000: I runner.py:310] Step = 82100 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000308 ; Loss = 1.759059\n",
      "2024-12-14 18:22:05.256000: I runner.py:310] Step = 82200 ; steps/s = 0.73, tokens/s = 18536 (18536 target) ; Learning rate = 0.000308 ; Loss = 1.755451\n",
      "2024-12-14 18:24:23.704000: I runner.py:310] Step = 82300 ; steps/s = 0.72, tokens/s = 18588 (18588 target) ; Learning rate = 0.000308 ; Loss = 1.742062\n",
      "2024-12-14 18:26:42.387000: I runner.py:310] Step = 82400 ; steps/s = 0.72, tokens/s = 18560 (18560 target) ; Learning rate = 0.000308 ; Loss = 1.741882\n",
      "2024-12-14 18:28:58.995000: I runner.py:310] Step = 82500 ; steps/s = 0.73, tokens/s = 18506 (18506 target) ; Learning rate = 0.000308 ; Loss = 1.752807\n",
      "2024-12-14 18:31:17.674000: I runner.py:310] Step = 82600 ; steps/s = 0.72, tokens/s = 18561 (18561 target) ; Learning rate = 0.000308 ; Loss = 1.746546\n",
      "2024-12-14 18:33:36.343000: I runner.py:310] Step = 82700 ; steps/s = 0.72, tokens/s = 18560 (18560 target) ; Learning rate = 0.000307 ; Loss = 1.755791\n",
      "2024-12-14 18:35:55.007000: I runner.py:310] Step = 82800 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000307 ; Loss = 1.764833\n",
      "2024-12-14 18:38:11.642000: I runner.py:310] Step = 82900 ; steps/s = 0.73, tokens/s = 18499 (18499 target) ; Learning rate = 0.000307 ; Loss = 1.735868\n",
      "2024-12-14 18:40:30.271000: I runner.py:310] Step = 83000 ; steps/s = 0.72, tokens/s = 18569 (18569 target) ; Learning rate = 0.000307 ; Loss = 1.732103\n",
      "2024-12-14 18:42:48.994000: I runner.py:310] Step = 83100 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000307 ; Loss = 1.746723\n",
      "2024-12-14 18:45:07.698000: I runner.py:310] Step = 83200 ; steps/s = 0.72, tokens/s = 18553 (18553 target) ; Learning rate = 0.000306 ; Loss = 1.756341\n",
      "2024-12-14 18:47:24.244000: I runner.py:310] Step = 83300 ; steps/s = 0.73, tokens/s = 18510 (18510 target) ; Learning rate = 0.000306 ; Loss = 1.763724\n",
      "2024-12-14 18:49:42.903000: I runner.py:310] Step = 83400 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000306 ; Loss = 1.732613\n",
      "2024-12-14 18:52:01.545000: I runner.py:310] Step = 83500 ; steps/s = 0.72, tokens/s = 18568 (18568 target) ; Learning rate = 0.000306 ; Loss = 1.737510\n",
      "2024-12-14 18:54:20.223000: I runner.py:310] Step = 83600 ; steps/s = 0.72, tokens/s = 18563 (18563 target) ; Learning rate = 0.000306 ; Loss = 1.746263\n",
      "2024-12-14 18:56:36.909000: I runner.py:310] Step = 83700 ; steps/s = 0.73, tokens/s = 18492 (18492 target) ; Learning rate = 0.000306 ; Loss = 1.725972\n",
      "2024-12-14 18:58:55.562000: I runner.py:310] Step = 83800 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000305 ; Loss = 1.749553\n",
      "2024-12-14 19:01:14.182000: I runner.py:310] Step = 83900 ; steps/s = 0.72, tokens/s = 18569 (18569 target) ; Learning rate = 0.000305 ; Loss = 1.760426\n",
      "2024-12-14 19:03:32.854000: I runner.py:310] Step = 84000 ; steps/s = 0.72, tokens/s = 18561 (18561 target) ; Learning rate = 0.000305 ; Loss = 1.768492\n",
      "2024-12-14 19:05:49.476000: I runner.py:310] Step = 84100 ; steps/s = 0.73, tokens/s = 18496 (18496 target) ; Learning rate = 0.000305 ; Loss = 1.725678\n",
      "2024-12-14 19:08:08.091000: I runner.py:310] Step = 84200 ; steps/s = 0.72, tokens/s = 18567 (18567 target) ; Learning rate = 0.000305 ; Loss = 1.759840\n",
      "2024-12-14 19:10:26.765000: I runner.py:310] Step = 84300 ; steps/s = 0.72, tokens/s = 18563 (18563 target) ; Learning rate = 0.000304 ; Loss = 1.743795\n",
      "2024-12-14 19:12:45.414000: I runner.py:310] Step = 84400 ; steps/s = 0.72, tokens/s = 18568 (18568 target) ; Learning rate = 0.000304 ; Loss = 1.760696\n",
      "2024-12-14 19:15:01.873000: I runner.py:310] Step = 84500 ; steps/s = 0.73, tokens/s = 18524 (18524 target) ; Learning rate = 0.000304 ; Loss = 1.718147\n",
      "2024-12-14 19:17:20.263000: I runner.py:310] Step = 84600 ; steps/s = 0.72, tokens/s = 18603 (18603 target) ; Learning rate = 0.000304 ; Loss = 1.751186\n",
      "2024-12-14 19:19:38.880000: I runner.py:310] Step = 84700 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000304 ; Loss = 1.759015\n",
      "2024-12-14 19:21:56.390000: I runner.py:310] Step = 84800 ; steps/s = 0.73, tokens/s = 18524 (18524 target) ; Learning rate = 0.000304 ; Loss = 1.774820\n",
      "2024-12-14 19:24:14.235000: I runner.py:310] Step = 84900 ; steps/s = 0.73, tokens/s = 18528 (18528 target) ; Learning rate = 0.000303 ; Loss = 1.726718\n",
      "2024-12-14 19:26:32.884000: I runner.py:310] Step = 85000 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000303 ; Loss = 1.748477\n",
      "2024-12-14 19:28:51.595000: I runner.py:310] Step = 85100 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000303 ; Loss = 1.751459\n",
      "2024-12-14 19:31:08.155000: I runner.py:310] Step = 85200 ; steps/s = 0.73, tokens/s = 18508 (18508 target) ; Learning rate = 0.000303 ; Loss = 1.740259\n",
      "2024-12-14 19:33:26.756000: I runner.py:310] Step = 85300 ; steps/s = 0.72, tokens/s = 18572 (18572 target) ; Learning rate = 0.000303 ; Loss = 1.736022\n",
      "2024-12-14 19:35:45.392000: I runner.py:310] Step = 85400 ; steps/s = 0.72, tokens/s = 18565 (18565 target) ; Learning rate = 0.000302 ; Loss = 1.737863\n",
      "2024-12-14 19:38:04.038000: I runner.py:310] Step = 85500 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000302 ; Loss = 1.757402\n",
      "2024-12-14 19:40:20.630000: I runner.py:310] Step = 85600 ; steps/s = 0.73, tokens/s = 18507 (18507 target) ; Learning rate = 0.000302 ; Loss = 1.726730\n",
      "2024-12-14 19:42:39.286000: I runner.py:310] Step = 85700 ; steps/s = 0.72, tokens/s = 18560 (18560 target) ; Learning rate = 0.000302 ; Loss = 1.734964\n",
      "2024-12-14 19:44:57.932000: I runner.py:310] Step = 85800 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000302 ; Loss = 1.756100\n",
      "2024-12-14 19:47:16.631000: I runner.py:310] Step = 85900 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000302 ; Loss = 1.757174\n",
      "2024-12-14 19:49:33.283000: I runner.py:310] Step = 86000 ; steps/s = 0.73, tokens/s = 18497 (18497 target) ; Learning rate = 0.000301 ; Loss = 1.721876\n",
      "2024-12-14 19:51:51.959000: I runner.py:310] Step = 86100 ; steps/s = 0.72, tokens/s = 18562 (18562 target) ; Learning rate = 0.000301 ; Loss = 1.745445\n",
      "2024-12-14 19:54:10.585000: I runner.py:310] Step = 86200 ; steps/s = 0.72, tokens/s = 18568 (18568 target) ; Learning rate = 0.000301 ; Loss = 1.752259\n",
      "2024-12-14 19:56:29.256000: I runner.py:310] Step = 86300 ; steps/s = 0.72, tokens/s = 18562 (18562 target) ; Learning rate = 0.000301 ; Loss = 1.752990\n",
      "2024-12-14 19:58:45.835000: I runner.py:310] Step = 86400 ; steps/s = 0.73, tokens/s = 18509 (18509 target) ; Learning rate = 0.000301 ; Loss = 1.751389\n",
      "2024-12-14 20:01:04.455000: I runner.py:310] Step = 86500 ; steps/s = 0.72, tokens/s = 18569 (18569 target) ; Learning rate = 0.000301 ; Loss = 1.739405\n",
      "2024-12-14 20:03:23.151000: I runner.py:310] Step = 86600 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000300 ; Loss = 1.733466\n",
      "2024-12-14 20:05:41.773000: I runner.py:310] Step = 86700 ; steps/s = 0.72, tokens/s = 18568 (18568 target) ; Learning rate = 0.000300 ; Loss = 1.746380\n",
      "2024-12-14 20:07:58.242000: I runner.py:310] Step = 86800 ; steps/s = 0.73, tokens/s = 18517 (18517 target) ; Learning rate = 0.000300 ; Loss = 1.761749\n",
      "2024-12-14 20:10:16.551000: I runner.py:310] Step = 86900 ; steps/s = 0.72, tokens/s = 18609 (18609 target) ; Learning rate = 0.000300 ; Loss = 1.741286\n",
      "2024-12-14 20:12:35.170000: I runner.py:310] Step = 87000 ; steps/s = 0.72, tokens/s = 18569 (18569 target) ; Learning rate = 0.000300 ; Loss = 1.739059\n",
      "2024-12-14 20:14:53.838000: I runner.py:310] Step = 87100 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000299 ; Loss = 1.754573\n",
      "2024-12-14 20:17:10.465000: I runner.py:310] Step = 87200 ; steps/s = 0.73, tokens/s = 18500 (18500 target) ; Learning rate = 0.000299 ; Loss = 1.726006\n",
      "2024-12-14 20:19:29.137000: I runner.py:310] Step = 87300 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000299 ; Loss = 1.748407\n",
      "2024-12-14 20:21:47.780000: I runner.py:310] Step = 87400 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000299 ; Loss = 1.761642\n",
      "2024-12-14 20:24:04.356000: I runner.py:310] Step = 87500 ; steps/s = 0.73, tokens/s = 18505 (18505 target) ; Learning rate = 0.000299 ; Loss = 1.779316\n",
      "2024-12-14 20:26:23.039000: I runner.py:310] Step = 87600 ; steps/s = 0.72, tokens/s = 18562 (18562 target) ; Learning rate = 0.000299 ; Loss = 1.750588\n",
      "2024-12-14 20:28:41.742000: I runner.py:310] Step = 87700 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000298 ; Loss = 1.747176\n",
      "2024-12-14 20:31:00.441000: I runner.py:310] Step = 87800 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000298 ; Loss = 1.742051\n",
      "2024-12-14 20:33:16.998000: I runner.py:310] Step = 87900 ; steps/s = 0.73, tokens/s = 18508 (18508 target) ; Learning rate = 0.000298 ; Loss = 1.733119\n",
      "2024-12-14 20:35:35.630000: I runner.py:310] Step = 88000 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000298 ; Loss = 1.736447\n",
      "2024-12-14 20:37:54.266000: I runner.py:310] Step = 88100 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000298 ; Loss = 1.745154\n",
      "2024-12-14 20:40:12.927000: I runner.py:310] Step = 88200 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000298 ; Loss = 1.756479\n",
      "2024-12-14 20:42:29.507000: I runner.py:310] Step = 88300 ; steps/s = 0.73, tokens/s = 18510 (18510 target) ; Learning rate = 0.000297 ; Loss = 1.746425\n",
      "2024-12-14 20:44:48.165000: I runner.py:310] Step = 88400 ; steps/s = 0.72, tokens/s = 18563 (18563 target) ; Learning rate = 0.000297 ; Loss = 1.728430\n",
      "2024-12-14 20:47:06.748000: I runner.py:310] Step = 88500 ; steps/s = 0.72, tokens/s = 18575 (18575 target) ; Learning rate = 0.000297 ; Loss = 1.731146\n",
      "2024-12-14 20:49:25.424000: I runner.py:310] Step = 88600 ; steps/s = 0.72, tokens/s = 18560 (18560 target) ; Learning rate = 0.000297 ; Loss = 1.721469\n",
      "2024-12-14 20:51:42.024000: I runner.py:310] Step = 88700 ; steps/s = 0.73, tokens/s = 18504 (18504 target) ; Learning rate = 0.000297 ; Loss = 1.722947\n",
      "2024-12-14 20:54:00.703000: I runner.py:310] Step = 88800 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000297 ; Loss = 1.733608\n",
      "2024-12-14 20:56:19.341000: I runner.py:310] Step = 88900 ; steps/s = 0.72, tokens/s = 18569 (18569 target) ; Learning rate = 0.000296 ; Loss = 1.748708\n",
      "2024-12-14 20:58:37.997000: I runner.py:310] Step = 89000 ; steps/s = 0.72, tokens/s = 18561 (18561 target) ; Learning rate = 0.000296 ; Loss = 1.753110\n",
      "2024-12-14 21:00:54.598000: I runner.py:310] Step = 89100 ; steps/s = 0.73, tokens/s = 18505 (18505 target) ; Learning rate = 0.000296 ; Loss = 1.722393\n",
      "2024-12-14 21:03:12.930000: I runner.py:310] Step = 89200 ; steps/s = 0.72, tokens/s = 18606 (18606 target) ; Learning rate = 0.000296 ; Loss = 1.734579\n",
      "2024-12-14 21:05:31.596000: I runner.py:310] Step = 89300 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000296 ; Loss = 1.752675\n",
      "2024-12-14 21:07:50.270000: I runner.py:310] Step = 89400 ; steps/s = 0.72, tokens/s = 18561 (18561 target) ; Learning rate = 0.000296 ; Loss = 1.766022\n",
      "2024-12-14 21:10:06.831000: I runner.py:310] Step = 89500 ; steps/s = 0.73, tokens/s = 18511 (18511 target) ; Learning rate = 0.000295 ; Loss = 1.714545\n",
      "2024-12-14 21:12:25.483000: I runner.py:310] Step = 89600 ; steps/s = 0.72, tokens/s = 18563 (18563 target) ; Learning rate = 0.000295 ; Loss = 1.746443\n",
      "2024-12-14 21:14:44.117000: I runner.py:310] Step = 89700 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000295 ; Loss = 1.757257\n",
      "2024-12-14 21:17:02.782000: I runner.py:310] Step = 89800 ; steps/s = 0.72, tokens/s = 18561 (18561 target) ; Learning rate = 0.000295 ; Loss = 1.757299\n",
      "2024-12-14 21:19:19.456000: I runner.py:310] Step = 89900 ; steps/s = 0.73, tokens/s = 18495 (18495 target) ; Learning rate = 0.000295 ; Loss = 1.721505\n",
      "2024-12-14 21:21:38.086000: I runner.py:310] Step = 90000 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000295 ; Loss = 1.747492\n",
      "2024-12-14 21:21:39.624000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-90000\n",
      "2024-12-14 21:23:58.300000: I runner.py:310] Step = 90100 ; steps/s = 0.72, tokens/s = 18560 (18560 target) ; Learning rate = 0.000294 ; Loss = 1.741681\n",
      "2024-12-14 21:26:14.838000: I runner.py:310] Step = 90200 ; steps/s = 0.73, tokens/s = 18517 (18517 target) ; Learning rate = 0.000294 ; Loss = 1.738675\n",
      "2024-12-14 21:28:33.518000: I runner.py:310] Step = 90300 ; steps/s = 0.72, tokens/s = 18562 (18562 target) ; Learning rate = 0.000294 ; Loss = 1.756940\n",
      "2024-12-14 21:30:52.115000: I runner.py:310] Step = 90400 ; steps/s = 0.72, tokens/s = 18569 (18569 target) ; Learning rate = 0.000294 ; Loss = 1.744363\n",
      "2024-12-14 21:33:10.724000: I runner.py:310] Step = 90500 ; steps/s = 0.72, tokens/s = 18571 (18571 target) ; Learning rate = 0.000294 ; Loss = 1.738857\n",
      "2024-12-14 21:35:27.363000: I runner.py:310] Step = 90600 ; steps/s = 0.73, tokens/s = 18498 (18498 target) ; Learning rate = 0.000294 ; Loss = 1.748873\n",
      "2024-12-14 21:37:46.040000: I runner.py:310] Step = 90700 ; steps/s = 0.72, tokens/s = 18562 (18562 target) ; Learning rate = 0.000293 ; Loss = 1.748416\n",
      "2024-12-14 21:40:04.662000: I runner.py:310] Step = 90800 ; steps/s = 0.72, tokens/s = 18569 (18569 target) ; Learning rate = 0.000293 ; Loss = 1.737738\n",
      "2024-12-14 21:42:23.303000: I runner.py:310] Step = 90900 ; steps/s = 0.72, tokens/s = 18562 (18562 target) ; Learning rate = 0.000293 ; Loss = 1.738941\n",
      "2024-12-14 21:44:39.922000: I runner.py:310] Step = 91000 ; steps/s = 0.73, tokens/s = 18503 (18503 target) ; Learning rate = 0.000293 ; Loss = 1.727692\n",
      "2024-12-14 21:46:58.562000: I runner.py:310] Step = 91100 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000293 ; Loss = 1.730141\n",
      "2024-12-14 21:49:17.178000: I runner.py:310] Step = 91200 ; steps/s = 0.72, tokens/s = 18570 (18570 target) ; Learning rate = 0.000293 ; Loss = 1.742663\n",
      "2024-12-14 21:51:35.813000: I runner.py:310] Step = 91300 ; steps/s = 0.72, tokens/s = 18567 (18567 target) ; Learning rate = 0.000293 ; Loss = 1.757954\n",
      "2024-12-14 21:53:52.405000: I runner.py:310] Step = 91400 ; steps/s = 0.73, tokens/s = 18503 (18503 target) ; Learning rate = 0.000292 ; Loss = 1.721473\n",
      "2024-12-14 21:56:10.682000: I runner.py:310] Step = 91500 ; steps/s = 0.72, tokens/s = 18616 (18616 target) ; Learning rate = 0.000292 ; Loss = 1.737093\n",
      "2024-12-14 21:58:29.363000: I runner.py:310] Step = 91600 ; steps/s = 0.72, tokens/s = 18560 (18560 target) ; Learning rate = 0.000292 ; Loss = 1.746355\n",
      "2024-12-14 22:00:48.068000: I runner.py:310] Step = 91700 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000292 ; Loss = 1.745455\n",
      "2024-12-14 22:03:04.700000: I runner.py:310] Step = 91800 ; steps/s = 0.73, tokens/s = 18499 (18499 target) ; Learning rate = 0.000292 ; Loss = 1.750870\n",
      "2024-12-14 22:05:23.329000: I runner.py:310] Step = 91900 ; steps/s = 0.72, tokens/s = 18565 (18565 target) ; Learning rate = 0.000292 ; Loss = 1.719348\n",
      "2024-12-14 22:07:42.000000: I runner.py:310] Step = 92000 ; steps/s = 0.72, tokens/s = 18565 (18565 target) ; Learning rate = 0.000291 ; Loss = 1.736536\n",
      "2024-12-14 22:10:00.716000: I runner.py:310] Step = 92100 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000291 ; Loss = 1.726461\n",
      "2024-12-14 22:12:17.332000: I runner.py:310] Step = 92200 ; steps/s = 0.73, tokens/s = 18502 (18502 target) ; Learning rate = 0.000291 ; Loss = 1.710540\n",
      "2024-12-14 22:14:35.900000: I runner.py:310] Step = 92300 ; steps/s = 0.72, tokens/s = 18579 (18579 target) ; Learning rate = 0.000291 ; Loss = 1.751083\n",
      "2024-12-14 22:16:54.577000: I runner.py:310] Step = 92400 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000291 ; Loss = 1.748085\n",
      "2024-12-14 22:19:13.209000: I runner.py:310] Step = 92500 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000291 ; Loss = 1.744145\n",
      "2024-12-14 22:21:29.804000: I runner.py:310] Step = 92600 ; steps/s = 0.73, tokens/s = 18499 (18499 target) ; Learning rate = 0.000290 ; Loss = 1.719662\n",
      "2024-12-14 22:23:48.447000: I runner.py:310] Step = 92700 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000290 ; Loss = 1.734140\n",
      "2024-12-14 22:26:07.091000: I runner.py:310] Step = 92800 ; steps/s = 0.72, tokens/s = 18570 (18570 target) ; Learning rate = 0.000290 ; Loss = 1.758203\n",
      "2024-12-14 22:28:23.738000: I runner.py:310] Step = 92900 ; steps/s = 0.73, tokens/s = 18500 (18500 target) ; Learning rate = 0.000290 ; Loss = 1.736814\n",
      "2024-12-14 22:30:42.423000: I runner.py:310] Step = 93000 ; steps/s = 0.72, tokens/s = 18560 (18560 target) ; Learning rate = 0.000290 ; Loss = 1.717368\n",
      "2024-12-14 22:33:01.083000: I runner.py:310] Step = 93100 ; steps/s = 0.72, tokens/s = 18563 (18563 target) ; Learning rate = 0.000290 ; Loss = 1.733949\n",
      "2024-12-14 22:35:19.708000: I runner.py:310] Step = 93200 ; steps/s = 0.72, tokens/s = 18567 (18567 target) ; Learning rate = 0.000290 ; Loss = 1.745427\n",
      "2024-12-14 22:37:36.315000: I runner.py:310] Step = 93300 ; steps/s = 0.73, tokens/s = 18503 (18503 target) ; Learning rate = 0.000289 ; Loss = 1.750908\n",
      "2024-12-14 22:39:54.938000: I runner.py:310] Step = 93400 ; steps/s = 0.72, tokens/s = 18569 (18569 target) ; Learning rate = 0.000289 ; Loss = 1.745645\n",
      "2024-12-14 22:42:13.565000: I runner.py:310] Step = 93500 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000289 ; Loss = 1.732630\n",
      "2024-12-14 22:44:32.213000: I runner.py:310] Step = 93600 ; steps/s = 0.72, tokens/s = 18568 (18568 target) ; Learning rate = 0.000289 ; Loss = 1.731874\n",
      "2024-12-14 22:46:48.795000: I runner.py:310] Step = 93700 ; steps/s = 0.73, tokens/s = 18505 (18505 target) ; Learning rate = 0.000289 ; Loss = 1.727036\n",
      "2024-12-14 22:49:07.114000: I runner.py:310] Step = 93800 ; steps/s = 0.72, tokens/s = 18609 (18609 target) ; Learning rate = 0.000289 ; Loss = 1.724844\n",
      "2024-12-14 22:51:25.765000: I runner.py:310] Step = 93900 ; steps/s = 0.72, tokens/s = 18565 (18565 target) ; Learning rate = 0.000288 ; Loss = 1.744600\n",
      "2024-12-14 22:53:44.392000: I runner.py:310] Step = 94000 ; steps/s = 0.72, tokens/s = 18568 (18568 target) ; Learning rate = 0.000288 ; Loss = 1.756401\n",
      "2024-12-14 22:56:01.092000: I runner.py:310] Step = 94100 ; steps/s = 0.73, tokens/s = 18490 (18490 target) ; Learning rate = 0.000288 ; Loss = 1.713681\n",
      "2024-12-14 22:58:19.731000: I runner.py:310] Step = 94200 ; steps/s = 0.72, tokens/s = 18565 (18565 target) ; Learning rate = 0.000288 ; Loss = 1.736936\n",
      "2024-12-14 23:00:38.312000: I runner.py:310] Step = 94300 ; steps/s = 0.72, tokens/s = 18576 (18576 target) ; Learning rate = 0.000288 ; Loss = 1.749596\n",
      "2024-12-14 23:02:56.988000: I runner.py:310] Step = 94400 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000288 ; Loss = 1.752348\n",
      "2024-12-14 23:05:13.611000: I runner.py:310] Step = 94500 ; steps/s = 0.73, tokens/s = 18498 (18498 target) ; Learning rate = 0.000288 ; Loss = 1.723684\n",
      "2024-12-14 23:07:32.255000: I runner.py:310] Step = 94600 ; steps/s = 0.72, tokens/s = 18567 (18567 target) ; Learning rate = 0.000287 ; Loss = 1.723762\n",
      "2024-12-14 23:09:50.932000: I runner.py:310] Step = 94700 ; steps/s = 0.72, tokens/s = 18562 (18562 target) ; Learning rate = 0.000287 ; Loss = 1.740913\n",
      "2024-12-14 23:12:09.628000: I runner.py:310] Step = 94800 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000287 ; Loss = 1.755787\n",
      "2024-12-14 23:14:26.224000: I runner.py:310] Step = 94900 ; steps/s = 0.73, tokens/s = 18502 (18502 target) ; Learning rate = 0.000287 ; Loss = 1.753506\n",
      "2024-12-14 23:16:44.901000: I runner.py:310] Step = 95000 ; steps/s = 0.72, tokens/s = 18565 (18565 target) ; Learning rate = 0.000287 ; Loss = 1.727926\n",
      "2024-12-14 23:19:03.609000: I runner.py:310] Step = 95100 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000287 ; Loss = 1.723012\n",
      "2024-12-14 23:21:22.241000: I runner.py:310] Step = 95200 ; steps/s = 0.72, tokens/s = 18567 (18567 target) ; Learning rate = 0.000286 ; Loss = 1.729654\n",
      "2024-12-14 23:23:38.820000: I runner.py:310] Step = 95300 ; steps/s = 0.73, tokens/s = 18505 (18505 target) ; Learning rate = 0.000286 ; Loss = 1.718584\n",
      "2024-12-14 23:25:57.498000: I runner.py:310] Step = 95400 ; steps/s = 0.72, tokens/s = 18563 (18563 target) ; Learning rate = 0.000286 ; Loss = 1.732354\n",
      "2024-12-14 23:28:16.167000: I runner.py:310] Step = 95500 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000286 ; Loss = 1.741104\n",
      "2024-12-14 23:30:32.740000: I runner.py:310] Step = 95600 ; steps/s = 0.73, tokens/s = 18507 (18507 target) ; Learning rate = 0.000286 ; Loss = 1.728525\n",
      "2024-12-14 23:32:51.417000: I runner.py:310] Step = 95700 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000286 ; Loss = 1.719840\n",
      "2024-12-14 23:35:10.094000: I runner.py:310] Step = 95800 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000286 ; Loss = 1.743487\n",
      "2024-12-14 23:37:28.694000: I runner.py:310] Step = 95900 ; steps/s = 0.72, tokens/s = 18571 (18571 target) ; Learning rate = 0.000285 ; Loss = 1.754169\n",
      "2024-12-14 23:39:45.324000: I runner.py:310] Step = 96000 ; steps/s = 0.73, tokens/s = 18500 (18500 target) ; Learning rate = 0.000285 ; Loss = 1.735113\n",
      "2024-12-14 23:42:03.667000: I runner.py:310] Step = 96100 ; steps/s = 0.72, tokens/s = 18603 (18603 target) ; Learning rate = 0.000285 ; Loss = 1.722477\n",
      "2024-12-14 23:44:22.247000: I runner.py:310] Step = 96200 ; steps/s = 0.72, tokens/s = 18576 (18576 target) ; Learning rate = 0.000285 ; Loss = 1.726762\n",
      "2024-12-14 23:46:40.888000: I runner.py:310] Step = 96300 ; steps/s = 0.72, tokens/s = 18560 (18560 target) ; Learning rate = 0.000285 ; Loss = 1.725074\n",
      "2024-12-14 23:48:57.546000: I runner.py:310] Step = 96400 ; steps/s = 0.73, tokens/s = 18501 (18501 target) ; Learning rate = 0.000285 ; Loss = 1.736901\n",
      "2024-12-14 23:51:16.212000: I runner.py:310] Step = 96500 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000285 ; Loss = 1.720071\n",
      "2024-12-14 23:53:34.958000: I runner.py:310] Step = 96600 ; steps/s = 0.72, tokens/s = 18545 (18545 target) ; Learning rate = 0.000284 ; Loss = 1.719364\n",
      "2024-12-14 23:55:53.560000: I runner.py:310] Step = 96700 ; steps/s = 0.72, tokens/s = 18571 (18571 target) ; Learning rate = 0.000284 ; Loss = 1.732571\n",
      "2024-12-14 23:58:10.099000: I runner.py:310] Step = 96800 ; steps/s = 0.73, tokens/s = 18512 (18512 target) ; Learning rate = 0.000284 ; Loss = 1.704701\n",
      "2024-12-15 00:00:28.708000: I runner.py:310] Step = 96900 ; steps/s = 0.72, tokens/s = 18570 (18570 target) ; Learning rate = 0.000284 ; Loss = 1.733832\n",
      "2024-12-15 00:02:47.373000: I runner.py:310] Step = 97000 ; steps/s = 0.72, tokens/s = 18563 (18563 target) ; Learning rate = 0.000284 ; Loss = 1.747647\n",
      "2024-12-15 00:05:06.096000: I runner.py:310] Step = 97100 ; steps/s = 0.72, tokens/s = 18555 (18555 target) ; Learning rate = 0.000284 ; Loss = 1.736891\n",
      "2024-12-15 00:07:22.690000: I runner.py:310] Step = 97200 ; steps/s = 0.73, tokens/s = 18504 (18504 target) ; Learning rate = 0.000284 ; Loss = 1.704646\n",
      "2024-12-15 00:09:41.327000: I runner.py:310] Step = 97300 ; steps/s = 0.72, tokens/s = 18567 (18567 target) ; Learning rate = 0.000283 ; Loss = 1.733529\n",
      "2024-12-15 00:11:59.958000: I runner.py:310] Step = 97400 ; steps/s = 0.72, tokens/s = 18566 (18566 target) ; Learning rate = 0.000283 ; Loss = 1.740027\n",
      "2024-12-15 00:14:18.674000: I runner.py:310] Step = 97500 ; steps/s = 0.72, tokens/s = 18557 (18557 target) ; Learning rate = 0.000283 ; Loss = 1.743832\n",
      "2024-12-15 00:16:35.308000: I runner.py:310] Step = 97600 ; steps/s = 0.73, tokens/s = 18495 (18495 target) ; Learning rate = 0.000283 ; Loss = 1.709324\n",
      "2024-12-15 00:18:53.938000: I runner.py:310] Step = 97700 ; steps/s = 0.72, tokens/s = 18569 (18569 target) ; Learning rate = 0.000283 ; Loss = 1.736217\n",
      "2024-12-15 00:21:12.584000: I runner.py:310] Step = 97800 ; steps/s = 0.72, tokens/s = 18565 (18565 target) ; Learning rate = 0.000283 ; Loss = 1.734747\n",
      "2024-12-15 00:23:31.262000: I runner.py:310] Step = 97900 ; steps/s = 0.72, tokens/s = 18562 (18562 target) ; Learning rate = 0.000282 ; Loss = 1.751309\n",
      "2024-12-15 00:25:47.857000: I runner.py:310] Step = 98000 ; steps/s = 0.73, tokens/s = 18506 (18506 target) ; Learning rate = 0.000282 ; Loss = 1.739956\n",
      "2024-12-15 00:28:06.525000: I runner.py:310] Step = 98100 ; steps/s = 0.72, tokens/s = 18562 (18562 target) ; Learning rate = 0.000282 ; Loss = 1.719138\n",
      "2024-12-15 00:30:25.171000: I runner.py:310] Step = 98200 ; steps/s = 0.72, tokens/s = 18563 (18563 target) ; Learning rate = 0.000282 ; Loss = 1.721659\n",
      "2024-12-15 00:32:41.743000: I runner.py:310] Step = 98300 ; steps/s = 0.73, tokens/s = 18507 (18507 target) ; Learning rate = 0.000282 ; Loss = 1.725827\n",
      "2024-12-15 00:35:00.167000: I runner.py:310] Step = 98400 ; steps/s = 0.72, tokens/s = 18596 (18596 target) ; Learning rate = 0.000282 ; Loss = 1.727919\n",
      "2024-12-15 00:37:18.685000: I runner.py:310] Step = 98500 ; steps/s = 0.72, tokens/s = 18581 (18581 target) ; Learning rate = 0.000282 ; Loss = 1.733924\n",
      "2024-12-15 00:39:37.403000: I runner.py:310] Step = 98600 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000281 ; Loss = 1.746037\n",
      "2024-12-15 00:41:54.038000: I runner.py:310] Step = 98700 ; steps/s = 0.73, tokens/s = 18506 (18506 target) ; Learning rate = 0.000281 ; Loss = 1.717549\n",
      "2024-12-15 00:44:12.683000: I runner.py:310] Step = 98800 ; steps/s = 0.72, tokens/s = 18565 (18565 target) ; Learning rate = 0.000281 ; Loss = 1.711773\n",
      "2024-12-15 00:46:31.283000: I runner.py:310] Step = 98900 ; steps/s = 0.72, tokens/s = 18571 (18571 target) ; Learning rate = 0.000281 ; Loss = 1.730855\n",
      "2024-12-15 00:48:49.958000: I runner.py:310] Step = 99000 ; steps/s = 0.72, tokens/s = 18562 (18562 target) ; Learning rate = 0.000281 ; Loss = 1.739775\n",
      "2024-12-15 00:51:06.634000: I runner.py:310] Step = 99100 ; steps/s = 0.73, tokens/s = 18493 (18493 target) ; Learning rate = 0.000281 ; Loss = 1.716578\n",
      "2024-12-15 00:53:25.241000: I runner.py:310] Step = 99200 ; steps/s = 0.72, tokens/s = 18569 (18569 target) ; Learning rate = 0.000281 ; Loss = 1.732631\n",
      "2024-12-15 00:55:43.948000: I runner.py:310] Step = 99300 ; steps/s = 0.72, tokens/s = 18556 (18556 target) ; Learning rate = 0.000280 ; Loss = 1.730148\n",
      "2024-12-15 00:58:02.604000: I runner.py:310] Step = 99400 ; steps/s = 0.72, tokens/s = 18567 (18567 target) ; Learning rate = 0.000280 ; Loss = 1.744454\n",
      "2024-12-15 01:00:19.209000: I runner.py:310] Step = 99500 ; steps/s = 0.73, tokens/s = 18503 (18503 target) ; Learning rate = 0.000280 ; Loss = 1.743058\n",
      "2024-12-15 01:02:37.941000: I runner.py:310] Step = 99600 ; steps/s = 0.72, tokens/s = 18554 (18554 target) ; Learning rate = 0.000280 ; Loss = 1.710467\n",
      "2024-12-15 01:04:56.606000: I runner.py:310] Step = 99700 ; steps/s = 0.72, tokens/s = 18559 (18559 target) ; Learning rate = 0.000280 ; Loss = 1.727591\n",
      "2024-12-15 01:07:15.259000: I runner.py:310] Step = 99800 ; steps/s = 0.72, tokens/s = 18561 (18561 target) ; Learning rate = 0.000280 ; Loss = 1.729704\n",
      "2024-12-15 01:09:31.929000: I runner.py:310] Step = 99900 ; steps/s = 0.73, tokens/s = 18498 (18498 target) ; Learning rate = 0.000280 ; Loss = 1.741868\n",
      "2024-12-15 01:11:50.625000: I runner.py:310] Step = 100000 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000280 ; Loss = 1.710432\n",
      "2024-12-15 01:11:52.213000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-100000\n",
      "2024-12-15 01:14:10.846000: I runner.py:310] Step = 100100 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000279 ; Loss = 1.724062\n",
      "2024-12-15 01:16:29.494000: I runner.py:310] Step = 100200 ; steps/s = 0.72, tokens/s = 18568 (18568 target) ; Learning rate = 0.000279 ; Loss = 1.725044\n",
      "2024-12-15 01:18:46.032000: I runner.py:310] Step = 100300 ; steps/s = 0.73, tokens/s = 18509 (18509 target) ; Learning rate = 0.000279 ; Loss = 1.733775\n",
      "2024-12-15 01:21:04.739000: I runner.py:310] Step = 100400 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000279 ; Loss = 1.725344\n",
      "2024-12-15 01:23:23.340000: I runner.py:310] Step = 100500 ; steps/s = 0.72, tokens/s = 18569 (18569 target) ; Learning rate = 0.000279 ; Loss = 1.713539\n",
      "2024-12-15 01:25:42.021000: I runner.py:310] Step = 100600 ; steps/s = 0.72, tokens/s = 18564 (18564 target) ; Learning rate = 0.000279 ; Loss = 1.732974\n",
      "2024-12-15 01:27:58.325000: I runner.py:310] Step = 100700 ; steps/s = 0.73, tokens/s = 18547 (18547 target) ; Learning rate = 0.000279 ; Loss = 1.698320\n",
      "2024-12-15 01:30:16.881000: I runner.py:310] Step = 100800 ; steps/s = 0.72, tokens/s = 18578 (18578 target) ; Learning rate = 0.000278 ; Loss = 1.724382\n",
      "2024-12-15 01:32:35.489000: I runner.py:310] Step = 100900 ; steps/s = 0.72, tokens/s = 18571 (18571 target) ; Learning rate = 0.000278 ; Loss = 1.735929\n",
      "2024-12-15 01:34:52.138000: I runner.py:310] Step = 101000 ; steps/s = 0.73, tokens/s = 18493 (18493 target) ; Learning rate = 0.000278 ; Loss = 1.723660\n",
      "2024-12-15 01:37:10.886000: I runner.py:310] Step = 101100 ; steps/s = 0.72, tokens/s = 18551 (18551 target) ; Learning rate = 0.000278 ; Loss = 1.711419\n",
      "2024-12-15 01:39:29.566000: I runner.py:310] Step = 101200 ; steps/s = 0.72, tokens/s = 18558 (18558 target) ; Learning rate = 0.000278 ; Loss = 1.728844\n",
      "2024-12-15 01:41:48.170000: I runner.py:310] Step = 101300 ; steps/s = 0.72, tokens/s = 18572 (18572 target) ; Learning rate = 0.000278 ; Loss = 1.737981\n",
      "2024-12-15 01:44:04.808000: I runner.py:310] Step = 101400 ; steps/s = 0.73, tokens/s = 18500 (18500 target) ; Learning rate = 0.000278 ; Loss = 1.732122\n",
      "2024-12-15 01:46:23.458000: I runner.py:310] Step = 101500 ; steps/s = 0.72, tokens/s = 18567 (18567 target) ; Learning rate = 0.000277 ; Loss = 1.720224\n",
      "2024-12-15 01:48:42.133000: I runner.py:310] Step = 101600 ; steps/s = 0.72, tokens/s = 18563 (18563 target) ; Learning rate = 0.000277 ; Loss = 1.717239\n",
      "2024-12-15 01:51:00.751000: I runner.py:310] Step = 101700 ; steps/s = 0.72, tokens/s = 18567 (18567 target) ; Learning rate = 0.000277 ; Loss = 1.715893\n",
      "2024-12-15 01:53:17.381000: I runner.py:310] Step = 101800 ; steps/s = 0.73, tokens/s = 18499 (18499 target) ; Learning rate = 0.000277 ; Loss = 1.740259\n",
      "2024-12-15 01:55:35.999000: I runner.py:310] Step = 101900 ; steps/s = 0.72, tokens/s = 18572 (18572 target) ; Learning rate = 0.000277 ; Loss = 1.718617\n",
      "2024-12-15 01:57:54.660000: I runner.py:310] Step = 102000 ; steps/s = 0.72, tokens/s = 18561 (18561 target) ; Learning rate = 0.000277 ; Loss = 1.716949\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Tr-En (TED2020)(POS Tags)\n",
    "!onmt-main --model kk-tr-en-modelim.py --config tr-en-pos.yml --auto_config train --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bb7261c-d735-46dc-aa2f-73ae451a37b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-15 16:09:32.942911: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 16:09:33.988874: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-15 16:09:33.988963: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-15 16:09:33.988973: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-15 16:09:35.383000: I main.py:308] Loading model description from POS_TR_EN/model_description.py\n",
      "2024-12-15 16:09:35.589000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-15 16:09:35.589000: I main.py:315] Using model:\n",
      "(model): MultiFeaturesTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): ParallelInputter(\n",
      "      (inputters): ListWrapper(\n",
      "        (0): WordEmbedder()\n",
      "        (1): WordEmbedder()\n",
      "      )\n",
      "      (reducer): ConcatReducer()\n",
      "    )\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): ParallelInputter(\n",
      "        (inputters): ListWrapper(\n",
      "          (0): WordEmbedder()\n",
      "          (1): WordEmbedder()\n",
      "        )\n",
      "        (reducer): ConcatReducer()\n",
      "      )\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-15 16:09:35.594000: I main.py:340] Using parameters:\n",
      "data:\n",
      "  eval_features_file:\n",
      "  - TED2020_tokens_dev\n",
      "  - TED2020_pos_tags_dev.txt\n",
      "  eval_labels_file: TED2020_dev_target_tokens.txt\n",
      "  source_1_vocabulary: tr_vocab.vocab\n",
      "  source_2_vocabulary: Zemberek_TR_unique_pos\n",
      "  target_vocabulary: en_vocab.vocab\n",
      "  train_features_file:\n",
      "  - TED2020_tokens_train\n",
      "  - TED2020_pos_tags_train.txt\n",
      "  train_labels_file: TED2020_train_target_tokens.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: POS_TR_EN\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 205000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-15 16:09:35.778167: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 16:09:36.402396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-15 16:09:36.566000: I inputter.py:316] Initialized source_1 input layer:\n",
      "2024-12-15 16:09:36.566000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-15 16:09:36.566000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-15 16:09:36.570000: I inputter.py:316] Initialized source_2 input layer:\n",
      "2024-12-15 16:09:36.570000: I inputter.py:316]  - vocabulary size: 21\n",
      "2024-12-15 16:09:36.570000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-15 16:09:36.646000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-15 16:09:36.646000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-15 16:09:36.646000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-15 16:09:36.665000: I runner.py:462] Restored checkpoint POS_TR_EN/ckpt-100000\n",
      "2024-12-15 16:09:36.712000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-15 16:09:37.465553: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-15 16:09:37.592000: I runner.py:471] Tracing and optimizing the inference graph...\n",
      "2024-12-15 16:09:50.786026: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-15 16:09:51.643830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-15 16:10:05.590000: I runner.py:471] 1341 predictions are buffered, but waiting for the prediction of queued line 5 to advance the output...\n",
      "2024-12-15 16:10:24.958000: I runner.py:471] 3113 predictions are buffered, but waiting for the prediction of queued line 89 to advance the output...\n",
      "2024-12-15 16:10:35.093000: I runner.py:471] 4105 predictions are buffered, but waiting for the prediction of queued line 89 to advance the output...\n",
      "2024-12-15 16:10:45.469000: I runner.py:471] 5097 predictions are buffered, but waiting for the prediction of queued line 89 to advance the output...\n",
      "2024-12-15 16:10:55.611000: I runner.py:471] 6025 predictions are buffered, but waiting for the prediction of queued line 89 to advance the output...\n",
      "2024-12-15 16:11:05.689000: I runner.py:471] 7017 predictions are buffered, but waiting for the prediction of queued line 89 to advance the output...\n",
      "2024-12-15 16:11:15.899000: I runner.py:471] 7977 predictions are buffered, but waiting for the prediction of queued line 89 to advance the output...\n",
      "2024-12-15 16:11:25.973000: I runner.py:471] 8905 predictions are buffered, but waiting for the prediction of queued line 89 to advance the output...\n",
      "2024-12-15 16:11:36.517000: I runner.py:471] 9772 predictions are buffered, but waiting for the prediction of queued line 89 to advance the output...\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 onmt-main --config tr-en-pos.yml --auto_config --checkpoint_path POS_TR_EN/ckpt-100000 infer --features_file TED2020_tokens_test TED2020_pos_tags_test.txt --predictions_file output_tr_en_pos.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999dec7d-cd42-41b8-b0c6-54441901249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 MT-Preparation/subwording/3-desubword.py en_vocab.model output_tr_en_pos.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12ec7e06-3bea-4b03-a6c5-181e57a87e3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference first sentence: It's quite widespread.\n",
      "Translated first sentence: And this is a really common topic .\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "BLEU2:  BLEU = 23.92 55.9/29.8/17.9/11.0 (BP = 1.000 ratio = 1.049 hyp_len = 207651 ref_len = 197923)\n",
      "CHRF:  chrF2 = 50.51\n"
     ]
    }
   ],
   "source": [
    "# BLEU and chrF scores\n",
    "!python3 compute-bleu.py TED2020.en-tr.en-filtered.en.test output_tr_en_pos.txt.desubword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "066e467f-aca5-4cbf-a1b8-c2605ecefc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama METEOR Puan: 0.5618200915177892\n"
     ]
    }
   ],
   "source": [
    "# Average METEOR score (Ortalama METEOR Puan)\n",
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "def read_and_tokenize_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return [nltk.word_tokenize(line.strip()) for line in lines]\n",
    "\n",
    "def calculate_meteor(reference_file, hypothesis_file):\n",
    "    references = read_and_tokenize_file(reference_file)\n",
    "    hypotheses = read_and_tokenize_file(hypothesis_file)\n",
    "    \n",
    "    if len(references) != len(hypotheses):\n",
    "        raise ValueError(\"Dosyalarn satr saylar elemiyor\")\n",
    "\n",
    "    total_meteor_score = 0.0\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        total_meteor_score += meteor_score([ref], hyp)\n",
    "\n",
    "    average_meteor_score = total_meteor_score / len(references)\n",
    "    return average_meteor_score\n",
    "\n",
    "reference_file = 'TED2020.en-tr.en-filtered.en.test'\n",
    "hypothesis_file = 'output_tr_en_pos.txt.desubword'\n",
    "\n",
    "score = calculate_meteor(reference_file, hypothesis_file)\n",
    "print(f\"Ortalama METEOR Puan: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd1c79-0f00-4119-b98c-dd8d9ac532ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
