{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937e191a-b3f9-41a0-bc19-a2b194f351c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of POS tagging of the Kazakh training set. The test and validation corpora are tagged in the same way.\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Justice0893/xlm-roberta-base-kazakh-pos-tagging\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Justice0893/xlm-roberta-base-kazakh-pos-tagging\")\n",
    "model.to(device)\n",
    "\n",
    "with open(\"kk_train_shuffled.txt-filtered.kk\", 'r', encoding='utf-8') as file:\n",
    "    content = file.readlines()\n",
    "\n",
    "with open(\"RoBERTa_KK_POS_train\", 'w', encoding='utf-8') as output_file:\n",
    "\n",
    "    for line in content:\n",
    "        line = line.strip()\n",
    "\n",
    "        tokens = tokenizer(line, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in tokens.items()}\n",
    "        outputs = model(**inputs).logits\n",
    "\n",
    "        predictions = torch.argmax(outputs, dim=2)[0]\n",
    "        tokens = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])\n",
    "        word_ids = tokenizer(line).word_ids()\n",
    "\n",
    "        previous_word_idx = None\n",
    "        word_pos_dict = {}\n",
    "\n",
    "        for idx, word_idx in enumerate(word_ids):\n",
    "            if word_idx is None:\n",
    "                continue\n",
    "            token = tokens[idx]\n",
    "            if token.startswith(\"##\"):\n",
    "                token = token[2:]\n",
    "                word_pos_dict[word_idx] = (word_pos_dict[word_idx][0] + token, word_pos_dict[word_idx][1])\n",
    "            else:\n",
    "                tag = model.config.id2label[predictions[idx].item()]\n",
    "                word_pos_dict[word_idx] = (token, tag)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        word_pos_tags = [f\"{tag}\" for word, tag in word_pos_dict.values()]\n",
    "        tagged_sentence = \" \".join(word_pos_tags)\n",
    "\n",
    "        output_file.write(tagged_sentence + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d5d45-0445-4efa-9204-a5d70a0e8d7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kk vocab creation\n",
    "!onmt-build-vocab --sentencepiece model_type=bpe --size 32000 --save_vocab kk_vocab kk_train_shuffled.txt-filtered.kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a806597-6d61-4dad-b06d-ed11f4f37246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_words(input_filepath, output_filepath):\n",
    "    unique_words = set() \n",
    "\n",
    "    with open(input_filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "\n",
    "            words = line.split()\n",
    "\n",
    "            unique_words.update(word.lower() for word in words)\n",
    "\n",
    "\n",
    "    with open(output_filepath, 'w', encoding='utf-8') as file:\n",
    "        for word in sorted(unique_words):\n",
    "            file.write(word + '\\n')\n",
    "\n",
    "    print(f\"Total unique words: {len(unique_words)}\")\n",
    "\n",
    "input_file = \"RoBERTa_KK_POS_train\"  \n",
    "output_file = \"RoBERTa_KK_unique_pos\" \n",
    "find_unique_words(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180c40f4-2779-46e2-916c-8869eb3963f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# en vocab creation\n",
    "!onmt-build-vocab --sentencepiece model_type=bpe --size 32000 --save_vocab tgt_1_en_vocab en_train_shuffled.txt-filtered.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "016f1d10-ad93-4989-8ad7-d82daab5629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kazakh subwording and subword units POS tags\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "\n",
    "def preprocess_text(text, pos_tags):\n",
    "\n",
    "    text = re.sub(r\"([.,!?;:()\\\"'])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    new_pos_tags = []\n",
    "    i = 0  # pos_tags index'i\n",
    "    for word in words:\n",
    "        #if re.match(r\"[.,!?;:()\\\"']\", word):\n",
    "            #pass\n",
    "        #else:\n",
    "            if i < len(pos_tags):\n",
    "                new_pos_tags.append(pos_tags[i]) \n",
    "                i += 1\n",
    "\n",
    "    return words, new_pos_tags\n",
    "\n",
    "def tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path):\n",
    "\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(model_path)\n",
    "    \n",
    "    with open(input_text_path, 'r', encoding='utf-8') as text_file, \\\n",
    "         open(input_pos_path, 'r', encoding='utf-8') as pos_file, \\\n",
    "         open(output_token_path, 'w', encoding='utf-8') as token_file, \\\n",
    "         open(output_pos_path, 'w', encoding='utf-8') as pos_file_out:\n",
    "        \n",
    "        for text_line, pos_line in zip(text_file, pos_file):\n",
    "            pos_tags = pos_line.strip().split()\n",
    "            preprocessed_words, adjusted_pos_tags = preprocess_text(text_line, pos_tags)\n",
    "            \n",
    "            tokenized_text = []\n",
    "            tokenized_tags = []\n",
    "            \n",
    "            for word, tag in zip(preprocessed_words, adjusted_pos_tags):\n",
    "                tokens = sp.EncodeAsPieces(word)\n",
    "                tokenized_text.extend(tokens)\n",
    "                tokenized_tags.extend([tag] * len(tokens))\n",
    "            \n",
    "            token_file.write(\" \".join(tokenized_text) + \"\\n\")\n",
    "            pos_file_out.write(\" \".join(tokenized_tags) + \"\\n\")\n",
    "\n",
    "\n",
    "input_text_path = \"kk_train_shuffled.txt-filtered.kk\" \n",
    "input_pos_path = \"RoBERTa_KK_POS_train\" \n",
    "output_token_path = \"tokens_train.txt\" \n",
    "output_pos_path = \"pos_tags_train.txt\" \n",
    "model_path = \"kk_vocab.model\" \n",
    "\n",
    "tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dde31931-52b4-4081-be53-3a8707b4154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_path = \"kk_test_shuffled.txt-filtered.kk\" \n",
    "input_pos_path = \"RoBERTa_KK_POS_test\" \n",
    "output_token_path = \"tokens_test.txt\" \n",
    "output_pos_path = \"pos_tags_test.txt\"\n",
    "model_path = \"kk_vocab.model\" \n",
    "\n",
    "tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path)\n",
    "\n",
    "input_text_path = \"kk_valid_shuffled.txt-filtered.kk\" \n",
    "input_pos_path = \"RoBERTa_KK_POS_valid\" \n",
    "output_token_path = \"tokens_dev.txt\" \n",
    "output_pos_path = \"pos_tags_dev.txt\" \n",
    "model_path = \"kk_vocab.model\" \n",
    "\n",
    "tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ce03d24-5d3a-49d3-ae59-c9e46204e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English subwording\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "\n",
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.Load(\"tgt_1_en_vocab.model\")\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    text = re.sub(r'([.,!?;:()])', r' \\1 ', text)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def tokenize_and_save(input_file, model, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f, open(output_file, 'w', encoding='utf-8') as fo:\n",
    "        for line in f:\n",
    "            \n",
    "            preprocessed_line = preprocess(line.strip())\n",
    "            tokens = model.encode_as_pieces(preprocessed_line)\n",
    "            fo.write(\" \".join(tokens) + \"\\n\")\n",
    "\n",
    "tokenize_and_save(\"en_train_shuffled.txt-filtered.en\", sp_model, \"train_target_tokens.txt\")\n",
    "tokenize_and_save(\"en_valid_shuffled.txt-filtered.en\", sp_model, \"dev_target_tokens.txt\")\n",
    "tokenize_and_save(\"en_test_shuffled.txt-filtered.en\", sp_model, \"test_target_tokens.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "350820cc-2667-4ada-9aff-ab7cd93e99d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-13 15:42:06.953477: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-13 15:42:07.697775: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-13 15:42:07.697844: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-13 15:42:07.697852: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-06-13 15:42:08.639000: I onmt-main:8] Creating model directory RoBERTa_POS-KK-EN\n",
      "2024-06-13 15:42:08.834000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-06-13 15:42:08.834000: I main.py:315] Using model:\n",
      "(model): MultiFeaturesTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): ParallelInputter(\n",
      "      (inputters): ListWrapper(\n",
      "        (0): WordEmbedder()\n",
      "        (1): WordEmbedder()\n",
      "      )\n",
      "      (reducer): ConcatReducer()\n",
      "    )\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): ParallelInputter(\n",
      "        (inputters): ListWrapper(\n",
      "          (0): WordEmbedder()\n",
      "          (1): WordEmbedder()\n",
      "        )\n",
      "        (reducer): ConcatReducer()\n",
      "      )\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-06-13 15:42:08.837096: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-13 15:42:09.861539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-06-13 15:42:09.862282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7538 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-06-13 15:42:09.866000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file:\n",
      "  - tokens_dev.txt\n",
      "  - pos_tags_dev.txt\n",
      "  eval_labels_file: dev_target_tokens.txt\n",
      "  source_1_vocabulary: kk_vocab.vocab\n",
      "  source_2_vocabulary: RoBERTa_KK_unique_pos\n",
      "  target_vocabulary: tgt_1_en_vocab.vocab\n",
      "  train_features_file:\n",
      "  - tokens_train.txt\n",
      "  - pos_tags_train.txt\n",
      "  train_labels_file: train_target_tokens.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: RoBERTa_POS-KK-EN\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 105000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-06-13 15:42:10.187000: I inputter.py:316] Initialized source_1 input layer:\n",
      "2024-06-13 15:42:10.187000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-06-13 15:42:10.188000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-06-13 15:42:10.191000: I inputter.py:316] Initialized source_2 input layer:\n",
      "2024-06-13 15:42:10.191000: I inputter.py:316]  - vocabulary size: 21\n",
      "2024-06-13 15:42:10.191000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-06-13 15:42:10.262000: I inputter.py:316] Initialized target input layer:\n",
      "2024-06-13 15:42:10.262000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-06-13 15:42:10.262000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-06-13 15:42:10.267000: W runner.py:269] No checkpoint to restore in RoBERTa_POS-KK-EN\n",
      "2024-06-13 15:42:10.269000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "2024-06-13 15:42:10.322000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-06-13 15:42:11.311619: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-06-13 15:42:11.432000: I main.py:325] Accumulate gradients of 7 iterations to reach effective batch size of 25000\n",
      "2024-06-13 15:42:11.549000: I mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2024-06-13 15:42:11.793000: I dataset_ops.py:2542] Training on 318032 examples\n",
      "2024-06-13 15:43:15.739324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-06-13 15:43:16.794699: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-06-13 15:43:17.128508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-06-13 15:43:26.035000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-06-13 15:43:26.059000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-06-13 15:43:27.554000: I cross_device_ops.py:897] batch_all_reduce: 261 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-06-13 15:43:31.427000: I cross_device_ops.py:897] batch_all_reduce: 261 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-06-13 15:43:38.035000: I runner.py:310] Number of model parameters: 93357261\n",
      "2024-06-13 15:43:38.039000: I runner.py:310] Number of model weights: 261 (trainable = 261, non trainable = 0)\n",
      "2024-06-13 15:43:38.074000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-06-13 15:43:40.228000: I training.py:176] Saved checkpoint RoBERTa_POS-KK-EN/ckpt-1\n",
      "2024-06-13 15:43:40.822000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-06-13 15:43:40.846000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-06-13 15:43:41.459000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-06-13 15:43:41.480000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-06-13 15:43:42.093000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-06-13 15:43:42.113000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-06-13 15:43:42.691000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-06-13 15:44:40.683000: I runner.py:310] Step = 100 ; steps/s = 1.64, tokens/s = 44646 (44646 target) ; Learning rate = 0.000009 ; Loss = 9.731769\n",
      "2024-06-13 15:45:42.440000: I runner.py:310] Step = 200 ; steps/s = 1.62, tokens/s = 43998 (43998 target) ; Learning rate = 0.000018 ; Loss = 8.901532\n",
      "2024-06-13 15:46:43.496000: I runner.py:310] Step = 300 ; steps/s = 1.64, tokens/s = 44503 (44503 target) ; Learning rate = 0.000027 ; Loss = 7.990983\n",
      "2024-06-13 15:47:44.733000: I runner.py:310] Step = 400 ; steps/s = 1.63, tokens/s = 43519 (43519 target) ; Learning rate = 0.000035 ; Loss = 7.309580\n",
      "2024-06-13 15:48:45.388000: I runner.py:310] Step = 500 ; steps/s = 1.65, tokens/s = 44821 (44821 target) ; Learning rate = 0.000044 ; Loss = 6.961342\n",
      "2024-06-13 15:49:46.032000: I runner.py:310] Step = 600 ; steps/s = 1.65, tokens/s = 44820 (44820 target) ; Learning rate = 0.000053 ; Loss = 6.749004\n",
      "2024-06-13 15:50:46.686000: I runner.py:310] Step = 700 ; steps/s = 1.65, tokens/s = 44790 (44790 target) ; Learning rate = 0.000062 ; Loss = 6.390263\n",
      "2024-06-13 15:51:47.065000: I runner.py:310] Step = 800 ; steps/s = 1.66, tokens/s = 44155 (44155 target) ; Learning rate = 0.000071 ; Loss = 6.151062\n",
      "2024-06-13 15:52:47.727000: I runner.py:310] Step = 900 ; steps/s = 1.65, tokens/s = 44816 (44816 target) ; Learning rate = 0.000080 ; Loss = 5.953564\n",
      "2024-06-13 15:53:48.378000: I runner.py:310] Step = 1000 ; steps/s = 1.65, tokens/s = 44810 (44810 target) ; Learning rate = 0.000088 ; Loss = 5.852128\n",
      "2024-06-13 15:54:49.050000: I runner.py:310] Step = 1100 ; steps/s = 1.65, tokens/s = 44779 (44779 target) ; Learning rate = 0.000097 ; Loss = 5.676125\n",
      "2024-06-13 15:55:49.288000: I runner.py:310] Step = 1200 ; steps/s = 1.66, tokens/s = 44248 (44248 target) ; Learning rate = 0.000106 ; Loss = 5.486677\n",
      "2024-06-13 15:56:49.916000: I runner.py:310] Step = 1300 ; steps/s = 1.65, tokens/s = 44810 (44810 target) ; Learning rate = 0.000115 ; Loss = 5.403056\n",
      "2024-06-13 15:57:50.603000: I runner.py:310] Step = 1400 ; steps/s = 1.65, tokens/s = 44776 (44776 target) ; Learning rate = 0.000124 ; Loss = 5.346852\n",
      "2024-06-13 15:58:51.238000: I runner.py:310] Step = 1500 ; steps/s = 1.65, tokens/s = 44859 (44859 target) ; Learning rate = 0.000133 ; Loss = 5.192470\n",
      "2024-06-13 15:59:51.417000: I runner.py:310] Step = 1600 ; steps/s = 1.66, tokens/s = 44308 (44308 target) ; Learning rate = 0.000142 ; Loss = 5.097085\n",
      "2024-06-13 16:00:52.307000: I runner.py:310] Step = 1700 ; steps/s = 1.64, tokens/s = 44613 (44613 target) ; Learning rate = 0.000150 ; Loss = 4.983065\n",
      "2024-06-13 16:01:52.961000: I runner.py:310] Step = 1800 ; steps/s = 1.65, tokens/s = 44815 (44815 target) ; Learning rate = 0.000159 ; Loss = 4.910740\n",
      "2024-06-13 16:02:53.605000: I runner.py:310] Step = 1900 ; steps/s = 1.65, tokens/s = 44805 (44805 target) ; Learning rate = 0.000168 ; Loss = 4.866720\n",
      "2024-06-13 16:03:53.770000: I runner.py:310] Step = 2000 ; steps/s = 1.66, tokens/s = 44338 (44338 target) ; Learning rate = 0.000177 ; Loss = 4.686041\n",
      "2024-06-13 16:04:54.364000: I runner.py:310] Step = 2100 ; steps/s = 1.65, tokens/s = 44829 (44829 target) ; Learning rate = 0.000186 ; Loss = 4.644146\n",
      "2024-06-13 16:05:55.011000: I runner.py:310] Step = 2200 ; steps/s = 1.65, tokens/s = 44822 (44822 target) ; Learning rate = 0.000195 ; Loss = 4.574093\n",
      "2024-06-13 16:06:55.623000: I runner.py:310] Step = 2300 ; steps/s = 1.65, tokens/s = 44826 (44826 target) ; Learning rate = 0.000203 ; Loss = 4.523789\n",
      "2024-06-13 16:07:55.786000: I runner.py:310] Step = 2400 ; steps/s = 1.66, tokens/s = 44327 (44327 target) ; Learning rate = 0.000212 ; Loss = 4.262405\n",
      "2024-06-13 16:08:56.386000: I runner.py:310] Step = 2500 ; steps/s = 1.65, tokens/s = 44865 (44865 target) ; Learning rate = 0.000221 ; Loss = 4.145658\n",
      "2024-06-13 16:09:57.006000: I runner.py:310] Step = 2600 ; steps/s = 1.65, tokens/s = 44817 (44817 target) ; Learning rate = 0.000230 ; Loss = 3.957705\n",
      "2024-06-13 16:10:57.130000: I runner.py:310] Step = 2700 ; steps/s = 1.66, tokens/s = 44334 (44334 target) ; Learning rate = 0.000239 ; Loss = 3.830564\n",
      "2024-06-13 16:11:57.720000: I runner.py:310] Step = 2800 ; steps/s = 1.65, tokens/s = 44839 (44839 target) ; Learning rate = 0.000248 ; Loss = 3.823290\n",
      "2024-06-13 16:12:58.330000: I runner.py:310] Step = 2900 ; steps/s = 1.65, tokens/s = 44834 (44834 target) ; Learning rate = 0.000256 ; Loss = 3.645505\n",
      "2024-06-13 16:13:59.018000: I runner.py:310] Step = 3000 ; steps/s = 1.65, tokens/s = 44778 (44778 target) ; Learning rate = 0.000265 ; Loss = 3.598913\n",
      "2024-06-13 16:14:59.188000: I runner.py:310] Step = 3100 ; steps/s = 1.66, tokens/s = 44323 (44323 target) ; Learning rate = 0.000274 ; Loss = 3.455204\n",
      "2024-06-13 16:15:59.802000: I runner.py:310] Step = 3200 ; steps/s = 1.65, tokens/s = 44819 (44819 target) ; Learning rate = 0.000283 ; Loss = 3.298180\n",
      "2024-06-13 16:17:00.514000: I runner.py:310] Step = 3300 ; steps/s = 1.65, tokens/s = 44795 (44795 target) ; Learning rate = 0.000292 ; Loss = 3.302286\n",
      "2024-06-13 16:18:01.206000: I runner.py:310] Step = 3400 ; steps/s = 1.65, tokens/s = 44765 (44765 target) ; Learning rate = 0.000301 ; Loss = 3.233102\n",
      "2024-06-13 16:19:01.424000: I runner.py:310] Step = 3500 ; steps/s = 1.66, tokens/s = 44269 (44269 target) ; Learning rate = 0.000309 ; Loss = 3.148311\n",
      "2024-06-13 16:20:02.099000: I runner.py:310] Step = 3600 ; steps/s = 1.65, tokens/s = 44756 (44756 target) ; Learning rate = 0.000318 ; Loss = 3.081223\n",
      "2024-06-13 16:21:02.734000: I runner.py:310] Step = 3700 ; steps/s = 1.65, tokens/s = 44819 (44819 target) ; Learning rate = 0.000327 ; Loss = 3.013120\n",
      "2024-06-13 16:22:03.329000: I runner.py:310] Step = 3800 ; steps/s = 1.65, tokens/s = 44868 (44868 target) ; Learning rate = 0.000336 ; Loss = 2.988201\n",
      "2024-06-13 16:23:03.585000: I runner.py:310] Step = 3900 ; steps/s = 1.66, tokens/s = 44260 (44260 target) ; Learning rate = 0.000345 ; Loss = 2.941116\n",
      "2024-06-13 16:24:04.205000: I runner.py:310] Step = 4000 ; steps/s = 1.65, tokens/s = 44823 (44823 target) ; Learning rate = 0.000354 ; Loss = 2.874516\n",
      "2024-06-13 16:25:04.792000: I runner.py:310] Step = 4100 ; steps/s = 1.65, tokens/s = 44857 (44857 target) ; Learning rate = 0.000362 ; Loss = 2.910150\n",
      "2024-06-13 16:26:05.452000: I runner.py:310] Step = 4200 ; steps/s = 1.65, tokens/s = 44810 (44810 target) ; Learning rate = 0.000371 ; Loss = 2.893197\n",
      "2024-06-13 16:27:05.681000: I runner.py:310] Step = 4300 ; steps/s = 1.66, tokens/s = 44273 (44273 target) ; Learning rate = 0.000380 ; Loss = 2.714411\n",
      "2024-06-13 16:28:06.308000: I runner.py:310] Step = 4400 ; steps/s = 1.65, tokens/s = 44813 (44813 target) ; Learning rate = 0.000389 ; Loss = 2.742282\n",
      "2024-06-13 16:29:06.935000: I runner.py:310] Step = 4500 ; steps/s = 1.65, tokens/s = 44830 (44830 target) ; Learning rate = 0.000398 ; Loss = 2.758815\n",
      "2024-06-13 16:30:07.583000: I runner.py:310] Step = 4600 ; steps/s = 1.65, tokens/s = 44803 (44803 target) ; Learning rate = 0.000407 ; Loss = 2.680378\n",
      "2024-06-13 16:31:07.780000: I runner.py:310] Step = 4700 ; steps/s = 1.66, tokens/s = 44325 (44325 target) ; Learning rate = 0.000416 ; Loss = 2.593995\n",
      "2024-06-13 16:32:08.471000: I runner.py:310] Step = 4800 ; steps/s = 1.65, tokens/s = 44761 (44761 target) ; Learning rate = 0.000424 ; Loss = 2.588862\n",
      "2024-06-13 16:33:09.049000: I runner.py:310] Step = 4900 ; steps/s = 1.65, tokens/s = 44866 (44866 target) ; Learning rate = 0.000433 ; Loss = 2.603767\n",
      "2024-06-13 16:34:09.266000: I runner.py:310] Step = 5000 ; steps/s = 1.66, tokens/s = 44261 (44261 target) ; Learning rate = 0.000442 ; Loss = 2.742513\n",
      "2024-06-13 16:34:09.267000: I training.py:192] Running evaluation for step 5000\n",
      "2024-06-13 16:53:22.594000: I training.py:192] Evaluation result for step 5000: loss = 1.576800 ; perplexity = 4.839447\n",
      "2024-06-13 16:54:23.303000: I runner.py:310] Step = 5100 ; steps/s = 1.65, tokens/s = 44752 (44752 target) ; Learning rate = 0.000451 ; Loss = 2.516266\n",
      "2024-06-13 16:55:24.080000: I runner.py:310] Step = 5200 ; steps/s = 1.65, tokens/s = 44733 (44733 target) ; Learning rate = 0.000460 ; Loss = 2.651194\n",
      "2024-06-13 16:56:24.843000: I runner.py:310] Step = 5300 ; steps/s = 1.65, tokens/s = 44737 (44737 target) ; Learning rate = 0.000469 ; Loss = 2.554707\n",
      "2024-06-13 16:57:25.109000: I runner.py:310] Step = 5400 ; steps/s = 1.66, tokens/s = 44246 (44246 target) ; Learning rate = 0.000477 ; Loss = 2.439762\n",
      "2024-06-13 16:58:25.826000: I runner.py:310] Step = 5500 ; steps/s = 1.65, tokens/s = 44766 (44766 target) ; Learning rate = 0.000486 ; Loss = 2.481815\n",
      "2024-06-13 16:59:26.550000: I runner.py:310] Step = 5600 ; steps/s = 1.65, tokens/s = 44724 (44724 target) ; Learning rate = 0.000495 ; Loss = 2.475596\n",
      "2024-06-13 17:00:27.308000: I runner.py:310] Step = 5700 ; steps/s = 1.65, tokens/s = 44719 (44719 target) ; Learning rate = 0.000504 ; Loss = 2.484212\n",
      "2024-06-13 17:01:27.598000: I runner.py:310] Step = 5800 ; steps/s = 1.66, tokens/s = 44253 (44253 target) ; Learning rate = 0.000513 ; Loss = 2.446994\n",
      "2024-06-13 17:02:28.337000: I runner.py:310] Step = 5900 ; steps/s = 1.65, tokens/s = 44744 (44744 target) ; Learning rate = 0.000522 ; Loss = 2.438273\n",
      "2024-06-13 17:03:29.132000: I runner.py:310] Step = 6000 ; steps/s = 1.65, tokens/s = 44695 (44695 target) ; Learning rate = 0.000530 ; Loss = 2.398786\n",
      "2024-06-13 17:04:29.919000: I runner.py:310] Step = 6100 ; steps/s = 1.65, tokens/s = 44710 (44710 target) ; Learning rate = 0.000539 ; Loss = 2.413075\n",
      "2024-06-13 17:05:30.275000: I runner.py:310] Step = 6200 ; steps/s = 1.66, tokens/s = 44183 (44183 target) ; Learning rate = 0.000548 ; Loss = 2.338847\n",
      "2024-06-13 17:06:31.022000: I runner.py:310] Step = 6300 ; steps/s = 1.65, tokens/s = 44716 (44716 target) ; Learning rate = 0.000557 ; Loss = 2.357291\n",
      "2024-06-13 17:07:31.764000: I runner.py:310] Step = 6400 ; steps/s = 1.65, tokens/s = 44717 (44717 target) ; Learning rate = 0.000566 ; Loss = 2.363923\n",
      "2024-06-13 17:08:32.565000: I runner.py:310] Step = 6500 ; steps/s = 1.64, tokens/s = 44691 (44691 target) ; Learning rate = 0.000575 ; Loss = 2.369475\n",
      "2024-06-13 17:09:32.851000: I runner.py:310] Step = 6600 ; steps/s = 1.66, tokens/s = 44252 (44252 target) ; Learning rate = 0.000583 ; Loss = 2.343002\n",
      "2024-06-13 17:10:33.612000: I runner.py:310] Step = 6700 ; steps/s = 1.65, tokens/s = 44737 (44737 target) ; Learning rate = 0.000592 ; Loss = 2.307311\n",
      "2024-06-13 17:11:34.365000: I runner.py:310] Step = 6800 ; steps/s = 1.65, tokens/s = 44733 (44733 target) ; Learning rate = 0.000601 ; Loss = 2.277131\n",
      "2024-06-13 17:12:35.119000: I runner.py:310] Step = 6900 ; steps/s = 1.65, tokens/s = 44707 (44707 target) ; Learning rate = 0.000610 ; Loss = 2.264884\n",
      "2024-06-13 17:13:35.363000: I runner.py:310] Step = 7000 ; steps/s = 1.66, tokens/s = 44292 (44292 target) ; Learning rate = 0.000619 ; Loss = 2.185325\n",
      "2024-06-13 17:14:36.127000: I runner.py:310] Step = 7100 ; steps/s = 1.65, tokens/s = 44708 (44708 target) ; Learning rate = 0.000628 ; Loss = 2.293891\n",
      "2024-06-13 17:15:36.822000: I runner.py:310] Step = 7200 ; steps/s = 1.65, tokens/s = 44743 (44743 target) ; Learning rate = 0.000636 ; Loss = 2.292772\n",
      "2024-06-13 17:16:37.068000: I runner.py:310] Step = 7300 ; steps/s = 1.66, tokens/s = 44263 (44263 target) ; Learning rate = 0.000645 ; Loss = 2.241667\n",
      "2024-06-13 17:17:37.661000: I runner.py:310] Step = 7400 ; steps/s = 1.65, tokens/s = 44833 (44833 target) ; Learning rate = 0.000654 ; Loss = 2.217574\n",
      "2024-06-13 17:18:38.319000: I runner.py:310] Step = 7500 ; steps/s = 1.65, tokens/s = 44820 (44820 target) ; Learning rate = 0.000663 ; Loss = 2.203386\n",
      "2024-06-13 17:19:38.964000: I runner.py:310] Step = 7600 ; steps/s = 1.65, tokens/s = 44825 (44825 target) ; Learning rate = 0.000672 ; Loss = 2.241512\n",
      "2024-06-13 17:20:39.208000: I runner.py:310] Step = 7700 ; steps/s = 1.66, tokens/s = 44252 (44252 target) ; Learning rate = 0.000681 ; Loss = 2.174582\n",
      "2024-06-13 17:21:39.940000: I runner.py:310] Step = 7800 ; steps/s = 1.65, tokens/s = 44739 (44739 target) ; Learning rate = 0.000690 ; Loss = 2.163547\n",
      "2024-06-13 17:22:40.609000: I runner.py:310] Step = 7900 ; steps/s = 1.65, tokens/s = 44788 (44788 target) ; Learning rate = 0.000698 ; Loss = 2.170358\n",
      "2024-06-13 17:23:41.255000: I runner.py:310] Step = 8000 ; steps/s = 1.65, tokens/s = 44841 (44841 target) ; Learning rate = 0.000707 ; Loss = 2.224681\n",
      "2024-06-13 17:24:41.398000: I runner.py:310] Step = 8100 ; steps/s = 1.66, tokens/s = 44326 (44326 target) ; Learning rate = 0.000716 ; Loss = 2.173925\n",
      "2024-06-13 17:25:42.046000: I runner.py:310] Step = 8200 ; steps/s = 1.65, tokens/s = 44809 (44809 target) ; Learning rate = 0.000725 ; Loss = 2.145205\n",
      "2024-06-13 17:26:42.644000: I runner.py:310] Step = 8300 ; steps/s = 1.65, tokens/s = 44816 (44816 target) ; Learning rate = 0.000734 ; Loss = 2.152805\n",
      "2024-06-13 17:27:43.274000: I runner.py:310] Step = 8400 ; steps/s = 1.65, tokens/s = 44845 (44845 target) ; Learning rate = 0.000743 ; Loss = 2.141093\n",
      "2024-06-13 17:28:43.481000: I runner.py:310] Step = 8500 ; steps/s = 1.66, tokens/s = 44314 (44314 target) ; Learning rate = 0.000751 ; Loss = 2.103072\n",
      "2024-06-13 17:29:44.116000: I runner.py:310] Step = 8600 ; steps/s = 1.65, tokens/s = 44816 (44816 target) ; Learning rate = 0.000760 ; Loss = 2.126334\n",
      "2024-06-13 17:30:44.777000: I runner.py:310] Step = 8700 ; steps/s = 1.65, tokens/s = 44801 (44801 target) ; Learning rate = 0.000769 ; Loss = 2.128111\n",
      "2024-06-13 17:31:45.445000: I runner.py:310] Step = 8800 ; steps/s = 1.65, tokens/s = 44776 (44776 target) ; Learning rate = 0.000778 ; Loss = 2.098024\n",
      "2024-06-13 17:32:45.633000: I runner.py:310] Step = 8900 ; steps/s = 1.66, tokens/s = 44306 (44306 target) ; Learning rate = 0.000787 ; Loss = 2.134939\n",
      "2024-06-13 17:33:46.379000: I runner.py:310] Step = 9000 ; steps/s = 1.65, tokens/s = 44747 (44747 target) ; Learning rate = 0.000796 ; Loss = 2.093984\n",
      "2024-06-13 17:34:47.019000: I runner.py:310] Step = 9100 ; steps/s = 1.65, tokens/s = 44802 (44802 target) ; Learning rate = 0.000804 ; Loss = 2.117148\n",
      "2024-06-13 17:35:47.703000: I runner.py:310] Step = 9200 ; steps/s = 1.65, tokens/s = 44784 (44784 target) ; Learning rate = 0.000813 ; Loss = 2.134113\n",
      "2024-06-13 17:36:47.939000: I runner.py:310] Step = 9300 ; steps/s = 1.66, tokens/s = 44238 (44238 target) ; Learning rate = 0.000822 ; Loss = 2.117177\n",
      "2024-06-13 17:37:48.652000: I runner.py:310] Step = 9400 ; steps/s = 1.65, tokens/s = 44755 (44755 target) ; Learning rate = 0.000831 ; Loss = 2.077172\n",
      "2024-06-13 17:38:49.288000: I runner.py:310] Step = 9500 ; steps/s = 1.65, tokens/s = 44818 (44818 target) ; Learning rate = 0.000840 ; Loss = 2.080434\n",
      "2024-06-13 17:39:49.958000: I runner.py:310] Step = 9600 ; steps/s = 1.65, tokens/s = 44817 (44817 target) ; Learning rate = 0.000849 ; Loss = 2.082644\n",
      "2024-06-13 17:40:50.142000: I runner.py:310] Step = 9700 ; steps/s = 1.66, tokens/s = 44295 (44295 target) ; Learning rate = 0.000857 ; Loss = 2.099536\n",
      "2024-06-13 17:41:50.845000: I runner.py:310] Step = 9800 ; steps/s = 1.65, tokens/s = 44780 (44780 target) ; Learning rate = 0.000866 ; Loss = 2.048578\n",
      "2024-06-13 17:42:51.575000: I runner.py:310] Step = 9900 ; steps/s = 1.65, tokens/s = 44742 (44742 target) ; Learning rate = 0.000875 ; Loss = 2.055730\n",
      "2024-06-13 17:43:51.851000: I runner.py:310] Step = 10000 ; steps/s = 1.66, tokens/s = 44240 (44240 target) ; Learning rate = 0.000884 ; Loss = 2.051516\n",
      "2024-06-13 17:43:53.758000: I training.py:176] Saved checkpoint RoBERTa_POS-KK-EN/ckpt-10000\n",
      "2024-06-13 17:43:53.758000: I training.py:192] Running evaluation for step 10000\n",
      "2024-06-13 17:50:20.334000: I training.py:192] Evaluation result for step 10000: loss = 1.230650 ; perplexity = 3.423455\n",
      "2024-06-13 17:51:20.813000: I runner.py:310] Step = 10100 ; steps/s = 1.65, tokens/s = 44932 (44932 target) ; Learning rate = 0.000879 ; Loss = 2.032466\n",
      "2024-06-13 17:52:21.438000: I runner.py:310] Step = 10200 ; steps/s = 1.65, tokens/s = 44810 (44810 target) ; Learning rate = 0.000875 ; Loss = 2.063849\n",
      "2024-06-13 17:53:22.107000: I runner.py:310] Step = 10300 ; steps/s = 1.65, tokens/s = 44820 (44820 target) ; Learning rate = 0.000871 ; Loss = 2.064472\n",
      "2024-06-13 17:54:22.297000: I runner.py:310] Step = 10400 ; steps/s = 1.66, tokens/s = 44328 (44328 target) ; Learning rate = 0.000867 ; Loss = 1.969247\n",
      "2024-06-13 17:55:22.997000: I runner.py:310] Step = 10500 ; steps/s = 1.65, tokens/s = 44778 (44778 target) ; Learning rate = 0.000863 ; Loss = 2.024433\n",
      "2024-06-13 17:56:23.665000: I runner.py:310] Step = 10600 ; steps/s = 1.65, tokens/s = 44772 (44772 target) ; Learning rate = 0.000858 ; Loss = 2.041902\n",
      "2024-06-13 17:57:24.326000: I runner.py:310] Step = 10700 ; steps/s = 1.65, tokens/s = 44795 (44795 target) ; Learning rate = 0.000854 ; Loss = 2.074844\n",
      "2024-06-13 17:58:24.622000: I runner.py:310] Step = 10800 ; steps/s = 1.66, tokens/s = 44219 (44219 target) ; Learning rate = 0.000850 ; Loss = 1.964375\n",
      "2024-06-13 17:59:25.239000: I runner.py:310] Step = 10900 ; steps/s = 1.65, tokens/s = 44808 (44808 target) ; Learning rate = 0.000847 ; Loss = 1.977976\n",
      "2024-06-13 18:00:25.928000: I runner.py:310] Step = 11000 ; steps/s = 1.65, tokens/s = 44785 (44785 target) ; Learning rate = 0.000843 ; Loss = 2.011317\n",
      "2024-06-13 18:01:26.628000: I runner.py:310] Step = 11100 ; steps/s = 1.65, tokens/s = 44776 (44776 target) ; Learning rate = 0.000839 ; Loss = 2.037669\n",
      "2024-06-13 18:02:26.867000: I runner.py:310] Step = 11200 ; steps/s = 1.66, tokens/s = 44266 (44266 target) ; Learning rate = 0.000835 ; Loss = 2.003464\n",
      "2024-06-13 18:03:27.542000: I runner.py:310] Step = 11300 ; steps/s = 1.65, tokens/s = 44791 (44791 target) ; Learning rate = 0.000831 ; Loss = 1.969151\n",
      "2024-06-13 18:04:28.234000: I runner.py:310] Step = 11400 ; steps/s = 1.65, tokens/s = 44771 (44771 target) ; Learning rate = 0.000828 ; Loss = 1.968199\n",
      "2024-06-13 18:05:28.946000: I runner.py:310] Step = 11500 ; steps/s = 1.65, tokens/s = 44756 (44756 target) ; Learning rate = 0.000824 ; Loss = 1.981233\n",
      "2024-06-13 18:06:29.115000: I runner.py:310] Step = 11600 ; steps/s = 1.66, tokens/s = 44323 (44323 target) ; Learning rate = 0.000821 ; Loss = 1.920956\n",
      "2024-06-13 18:07:29.682000: I runner.py:310] Step = 11700 ; steps/s = 1.65, tokens/s = 44840 (44840 target) ; Learning rate = 0.000817 ; Loss = 1.967247\n",
      "2024-06-13 18:08:30.364000: I runner.py:310] Step = 11800 ; steps/s = 1.65, tokens/s = 44815 (44815 target) ; Learning rate = 0.000814 ; Loss = 1.969407\n",
      "2024-06-13 18:09:30.958000: I runner.py:310] Step = 11900 ; steps/s = 1.65, tokens/s = 44850 (44850 target) ; Learning rate = 0.000810 ; Loss = 1.995840\n",
      "2024-06-13 18:10:31.157000: I runner.py:310] Step = 12000 ; steps/s = 1.66, tokens/s = 44277 (44277 target) ; Learning rate = 0.000807 ; Loss = 1.940661\n",
      "2024-06-13 18:11:31.843000: I runner.py:310] Step = 12100 ; steps/s = 1.65, tokens/s = 44775 (44775 target) ; Learning rate = 0.000803 ; Loss = 1.935105\n",
      "2024-06-13 18:12:32.506000: I runner.py:310] Step = 12200 ; steps/s = 1.65, tokens/s = 44818 (44818 target) ; Learning rate = 0.000800 ; Loss = 1.944791\n",
      "2024-06-13 18:13:32.714000: I runner.py:310] Step = 12300 ; steps/s = 1.66, tokens/s = 44270 (44270 target) ; Learning rate = 0.000797 ; Loss = 1.926327\n",
      "2024-06-13 18:14:33.366000: I runner.py:310] Step = 12400 ; steps/s = 1.65, tokens/s = 44806 (44806 target) ; Learning rate = 0.000794 ; Loss = 1.866776\n",
      "2024-06-13 18:15:34.043000: I runner.py:310] Step = 12500 ; steps/s = 1.65, tokens/s = 44808 (44808 target) ; Learning rate = 0.000791 ; Loss = 1.913181\n",
      "2024-06-13 18:16:34.752000: I runner.py:310] Step = 12600 ; steps/s = 1.65, tokens/s = 44755 (44755 target) ; Learning rate = 0.000787 ; Loss = 1.934723\n",
      "2024-06-13 18:17:34.933000: I runner.py:310] Step = 12700 ; steps/s = 1.66, tokens/s = 44312 (44312 target) ; Learning rate = 0.000784 ; Loss = 1.881118\n",
      "2024-06-13 18:18:35.583000: I runner.py:310] Step = 12800 ; steps/s = 1.65, tokens/s = 44835 (44835 target) ; Learning rate = 0.000781 ; Loss = 1.901652\n",
      "2024-06-13 18:19:36.197000: I runner.py:310] Step = 12900 ; steps/s = 1.65, tokens/s = 44840 (44840 target) ; Learning rate = 0.000778 ; Loss = 1.900526\n",
      "2024-06-13 18:20:36.851000: I runner.py:310] Step = 13000 ; steps/s = 1.65, tokens/s = 44755 (44755 target) ; Learning rate = 0.000775 ; Loss = 1.907298\n",
      "2024-06-13 18:21:36.998000: I runner.py:310] Step = 13100 ; steps/s = 1.66, tokens/s = 44349 (44349 target) ; Learning rate = 0.000772 ; Loss = 1.897195\n",
      "2024-06-13 18:22:37.609000: I runner.py:310] Step = 13200 ; steps/s = 1.65, tokens/s = 44833 (44833 target) ; Learning rate = 0.000769 ; Loss = 1.873579\n",
      "2024-06-13 18:23:38.223000: I runner.py:310] Step = 13300 ; steps/s = 1.65, tokens/s = 44838 (44838 target) ; Learning rate = 0.000766 ; Loss = 1.874257\n",
      "2024-06-13 18:24:38.857000: I runner.py:310] Step = 13400 ; steps/s = 1.65, tokens/s = 44800 (44800 target) ; Learning rate = 0.000764 ; Loss = 1.892561\n",
      "2024-06-13 18:25:39.069000: I runner.py:310] Step = 13500 ; steps/s = 1.66, tokens/s = 44287 (44287 target) ; Learning rate = 0.000761 ; Loss = 1.886837\n",
      "2024-06-13 18:26:39.730000: I runner.py:310] Step = 13600 ; steps/s = 1.65, tokens/s = 44799 (44799 target) ; Learning rate = 0.000758 ; Loss = 1.874171\n",
      "2024-06-13 18:27:40.430000: I runner.py:310] Step = 13700 ; steps/s = 1.65, tokens/s = 44762 (44762 target) ; Learning rate = 0.000755 ; Loss = 1.870012\n",
      "2024-06-13 18:28:41.102000: I runner.py:310] Step = 13800 ; steps/s = 1.65, tokens/s = 44798 (44798 target) ; Learning rate = 0.000752 ; Loss = 1.882316\n",
      "2024-06-13 18:29:41.308000: I runner.py:310] Step = 13900 ; steps/s = 1.66, tokens/s = 44302 (44302 target) ; Learning rate = 0.000750 ; Loss = 1.822352\n",
      "2024-06-13 18:30:41.968000: I runner.py:310] Step = 14000 ; steps/s = 1.65, tokens/s = 44788 (44788 target) ; Learning rate = 0.000747 ; Loss = 1.860456\n",
      "2024-06-13 18:31:42.665000: I runner.py:310] Step = 14100 ; steps/s = 1.65, tokens/s = 44783 (44783 target) ; Learning rate = 0.000744 ; Loss = 1.892029\n",
      "2024-06-13 18:32:43.318000: I runner.py:310] Step = 14200 ; steps/s = 1.65, tokens/s = 44796 (44796 target) ; Learning rate = 0.000742 ; Loss = 1.892691\n",
      "2024-06-13 18:33:43.536000: I runner.py:310] Step = 14300 ; steps/s = 1.66, tokens/s = 44276 (44276 target) ; Learning rate = 0.000739 ; Loss = 1.873559\n",
      "2024-06-13 18:34:44.258000: I runner.py:310] Step = 14400 ; steps/s = 1.65, tokens/s = 44754 (44754 target) ; Learning rate = 0.000737 ; Loss = 1.820125\n",
      "2024-06-13 18:35:44.890000: I runner.py:310] Step = 14500 ; steps/s = 1.65, tokens/s = 44832 (44832 target) ; Learning rate = 0.000734 ; Loss = 1.843718\n",
      "2024-06-13 18:36:45.168000: I runner.py:310] Step = 14600 ; steps/s = 1.66, tokens/s = 44199 (44199 target) ; Learning rate = 0.000731 ; Loss = 1.843437\n",
      "2024-06-13 18:37:45.866000: I runner.py:310] Step = 14700 ; steps/s = 1.65, tokens/s = 44773 (44773 target) ; Learning rate = 0.000729 ; Loss = 1.817707\n",
      "2024-06-13 18:38:46.545000: I runner.py:310] Step = 14800 ; steps/s = 1.65, tokens/s = 44799 (44799 target) ; Learning rate = 0.000727 ; Loss = 1.848665\n",
      "2024-06-13 18:39:47.248000: I runner.py:310] Step = 14900 ; steps/s = 1.65, tokens/s = 44789 (44789 target) ; Learning rate = 0.000724 ; Loss = 1.857574\n",
      "2024-06-13 18:40:47.544000: I runner.py:310] Step = 15000 ; steps/s = 1.66, tokens/s = 44204 (44204 target) ; Learning rate = 0.000722 ; Loss = 1.829363\n",
      "2024-06-13 18:40:47.546000: I training.py:192] Running evaluation for step 15000\n",
      "2024-06-13 18:45:59.007000: I training.py:192] Evaluation result for step 15000: loss = 1.215245 ; perplexity = 3.371119\n",
      "2024-06-13 18:46:59.549000: I runner.py:310] Step = 15100 ; steps/s = 1.65, tokens/s = 44914 (44914 target) ; Learning rate = 0.000719 ; Loss = 1.839813\n",
      "2024-06-13 18:48:00.214000: I runner.py:310] Step = 15200 ; steps/s = 1.65, tokens/s = 44780 (44780 target) ; Learning rate = 0.000717 ; Loss = 1.828032\n",
      "2024-06-13 18:49:00.874000: I runner.py:310] Step = 15300 ; steps/s = 1.65, tokens/s = 44808 (44808 target) ; Learning rate = 0.000715 ; Loss = 1.831047\n",
      "2024-06-13 18:50:01.163000: I runner.py:310] Step = 15400 ; steps/s = 1.66, tokens/s = 44218 (44218 target) ; Learning rate = 0.000712 ; Loss = 1.804378\n",
      "2024-06-13 18:51:01.819000: I runner.py:310] Step = 15500 ; steps/s = 1.65, tokens/s = 44826 (44826 target) ; Learning rate = 0.000710 ; Loss = 1.800272\n",
      "2024-06-13 18:52:02.533000: I runner.py:310] Step = 15600 ; steps/s = 1.65, tokens/s = 44748 (44748 target) ; Learning rate = 0.000708 ; Loss = 1.822397\n",
      "2024-06-13 18:53:03.170000: I runner.py:310] Step = 15700 ; steps/s = 1.65, tokens/s = 44824 (44824 target) ; Learning rate = 0.000705 ; Loss = 1.821367\n",
      "2024-06-13 18:54:03.391000: I runner.py:310] Step = 15800 ; steps/s = 1.66, tokens/s = 44255 (44255 target) ; Learning rate = 0.000703 ; Loss = 1.812141\n",
      "2024-06-13 18:55:04.042000: I runner.py:310] Step = 15900 ; steps/s = 1.65, tokens/s = 44824 (44824 target) ; Learning rate = 0.000701 ; Loss = 1.776747\n",
      "2024-06-13 18:56:04.713000: I runner.py:310] Step = 16000 ; steps/s = 1.65, tokens/s = 44785 (44785 target) ; Learning rate = 0.000699 ; Loss = 1.803379\n",
      "2024-06-13 18:57:05.328000: I runner.py:310] Step = 16100 ; steps/s = 1.65, tokens/s = 44833 (44833 target) ; Learning rate = 0.000697 ; Loss = 1.824605\n",
      "2024-06-13 18:58:05.590000: I runner.py:310] Step = 16200 ; steps/s = 1.66, tokens/s = 44239 (44239 target) ; Learning rate = 0.000694 ; Loss = 1.803467\n",
      "2024-06-13 18:59:06.339000: I runner.py:310] Step = 16300 ; steps/s = 1.65, tokens/s = 44742 (44742 target) ; Learning rate = 0.000692 ; Loss = 1.786715\n",
      "2024-06-13 19:00:07.023000: I runner.py:310] Step = 16400 ; steps/s = 1.65, tokens/s = 44772 (44772 target) ; Learning rate = 0.000690 ; Loss = 1.788070\n",
      "2024-06-13 19:01:07.676000: I runner.py:310] Step = 16500 ; steps/s = 1.65, tokens/s = 44801 (44801 target) ; Learning rate = 0.000688 ; Loss = 1.796679\n",
      "2024-06-13 19:02:07.911000: I runner.py:310] Step = 16600 ; steps/s = 1.66, tokens/s = 44286 (44286 target) ; Learning rate = 0.000686 ; Loss = 1.810304\n",
      "2024-06-13 19:03:08.553000: I runner.py:310] Step = 16700 ; steps/s = 1.65, tokens/s = 44782 (44782 target) ; Learning rate = 0.000684 ; Loss = 1.793179\n",
      "2024-06-13 19:04:09.190000: I runner.py:310] Step = 16800 ; steps/s = 1.65, tokens/s = 44798 (44798 target) ; Learning rate = 0.000682 ; Loss = 1.793156\n",
      "2024-06-13 19:05:09.705000: I runner.py:310] Step = 16900 ; steps/s = 1.65, tokens/s = 44640 (44640 target) ; Learning rate = 0.000680 ; Loss = 1.823999\n",
      "2024-06-13 19:06:10.049000: I runner.py:310] Step = 17000 ; steps/s = 1.66, tokens/s = 44477 (44477 target) ; Learning rate = 0.000678 ; Loss = 1.772361\n",
      "2024-06-13 19:07:10.724000: I runner.py:310] Step = 17100 ; steps/s = 1.65, tokens/s = 44793 (44793 target) ; Learning rate = 0.000676 ; Loss = 1.764964\n",
      "2024-06-13 19:08:11.414000: I runner.py:310] Step = 17200 ; steps/s = 1.65, tokens/s = 44778 (44778 target) ; Learning rate = 0.000674 ; Loss = 1.779199\n",
      "2024-06-13 19:09:11.682000: I runner.py:310] Step = 17300 ; steps/s = 1.66, tokens/s = 44231 (44231 target) ; Learning rate = 0.000672 ; Loss = 1.764295\n",
      "2024-06-13 19:10:12.361000: I runner.py:310] Step = 17400 ; steps/s = 1.65, tokens/s = 44804 (44804 target) ; Learning rate = 0.000670 ; Loss = 1.755442\n",
      "2024-06-13 19:11:13.024000: I runner.py:310] Step = 17500 ; steps/s = 1.65, tokens/s = 44790 (44790 target) ; Learning rate = 0.000668 ; Loss = 1.811616\n",
      "2024-06-13 19:12:13.687000: I runner.py:310] Step = 17600 ; steps/s = 1.65, tokens/s = 44794 (44794 target) ; Learning rate = 0.000666 ; Loss = 1.793840\n",
      "2024-06-13 19:13:13.926000: I runner.py:310] Step = 17700 ; steps/s = 1.66, tokens/s = 44250 (44250 target) ; Learning rate = 0.000664 ; Loss = 1.772860\n",
      "2024-06-13 19:14:14.592000: I runner.py:310] Step = 17800 ; steps/s = 1.65, tokens/s = 44798 (44798 target) ; Learning rate = 0.000662 ; Loss = 1.759254\n",
      "2024-06-13 19:15:15.297000: I runner.py:310] Step = 17900 ; steps/s = 1.65, tokens/s = 44731 (44731 target) ; Learning rate = 0.000661 ; Loss = 1.748928\n",
      "2024-06-13 19:16:15.974000: I runner.py:310] Step = 18000 ; steps/s = 1.65, tokens/s = 44805 (44805 target) ; Learning rate = 0.000659 ; Loss = 1.766127\n",
      "2024-06-13 19:17:16.246000: I runner.py:310] Step = 18100 ; steps/s = 1.66, tokens/s = 44267 (44267 target) ; Learning rate = 0.000657 ; Loss = 1.735271\n",
      "2024-06-13 19:18:16.912000: I runner.py:310] Step = 18200 ; steps/s = 1.65, tokens/s = 44798 (44798 target) ; Learning rate = 0.000655 ; Loss = 1.754691\n",
      "2024-06-13 19:19:17.588000: I runner.py:310] Step = 18300 ; steps/s = 1.65, tokens/s = 44770 (44770 target) ; Learning rate = 0.000653 ; Loss = 1.778700\n",
      "2024-06-13 19:20:18.241000: I runner.py:310] Step = 18400 ; steps/s = 1.65, tokens/s = 44815 (44815 target) ; Learning rate = 0.000652 ; Loss = 1.776514\n",
      "2024-06-13 19:21:18.404000: I runner.py:310] Step = 18500 ; steps/s = 1.66, tokens/s = 44321 (44321 target) ; Learning rate = 0.000650 ; Loss = 1.768443\n",
      "2024-06-13 19:22:19.086000: I runner.py:310] Step = 18600 ; steps/s = 1.65, tokens/s = 44801 (44801 target) ; Learning rate = 0.000648 ; Loss = 1.732564\n",
      "2024-06-13 19:23:19.738000: I runner.py:310] Step = 18700 ; steps/s = 1.65, tokens/s = 44769 (44769 target) ; Learning rate = 0.000646 ; Loss = 1.745277\n",
      "2024-06-13 19:24:20.481000: I runner.py:310] Step = 18800 ; steps/s = 1.65, tokens/s = 44746 (44746 target) ; Learning rate = 0.000645 ; Loss = 1.755822\n",
      "2024-06-13 19:25:20.742000: I runner.py:310] Step = 18900 ; steps/s = 1.66, tokens/s = 44267 (44267 target) ; Learning rate = 0.000643 ; Loss = 1.721694\n",
      "2024-06-13 19:26:21.492000: I runner.py:310] Step = 19000 ; steps/s = 1.65, tokens/s = 44715 (44715 target) ; Learning rate = 0.000641 ; Loss = 1.739950\n",
      "2024-06-13 19:27:22.163000: I runner.py:310] Step = 19100 ; steps/s = 1.65, tokens/s = 44807 (44807 target) ; Learning rate = 0.000640 ; Loss = 1.751201\n",
      "2024-06-13 19:28:22.813000: I runner.py:310] Step = 19200 ; steps/s = 1.65, tokens/s = 44805 (44805 target) ; Learning rate = 0.000638 ; Loss = 1.782164\n",
      "2024-06-13 19:29:23.024000: I runner.py:310] Step = 19300 ; steps/s = 1.66, tokens/s = 44305 (44305 target) ; Learning rate = 0.000636 ; Loss = 1.749157\n",
      "2024-06-13 19:30:23.730000: I runner.py:310] Step = 19400 ; steps/s = 1.65, tokens/s = 44753 (44753 target) ; Learning rate = 0.000635 ; Loss = 1.725746\n",
      "2024-06-13 19:31:24.384000: I runner.py:310] Step = 19500 ; steps/s = 1.65, tokens/s = 44809 (44809 target) ; Learning rate = 0.000633 ; Loss = 1.741992\n",
      "2024-06-13 19:32:24.696000: I runner.py:310] Step = 19600 ; steps/s = 1.66, tokens/s = 44192 (44192 target) ; Learning rate = 0.000631 ; Loss = 1.734619\n",
      "2024-06-13 19:33:25.408000: I runner.py:310] Step = 19700 ; steps/s = 1.65, tokens/s = 44782 (44782 target) ; Learning rate = 0.000630 ; Loss = 1.707938\n",
      "2024-06-13 19:34:26.077000: I runner.py:310] Step = 19800 ; steps/s = 1.65, tokens/s = 44775 (44775 target) ; Learning rate = 0.000628 ; Loss = 1.741921\n",
      "2024-06-13 19:35:26.777000: I runner.py:310] Step = 19900 ; steps/s = 1.65, tokens/s = 44804 (44804 target) ; Learning rate = 0.000627 ; Loss = 1.746400\n",
      "2024-06-13 19:36:26.955000: I runner.py:310] Step = 20000 ; steps/s = 1.66, tokens/s = 44292 (44292 target) ; Learning rate = 0.000625 ; Loss = 1.729575\n",
      "2024-06-13 19:36:28.809000: I training.py:176] Saved checkpoint RoBERTa_POS-KK-EN/ckpt-20000\n",
      "2024-06-13 19:36:28.809000: I training.py:192] Running evaluation for step 20000\n",
      "2024-06-13 19:41:07.882000: I training.py:192] Evaluation result for step 20000: loss = 1.240411 ; perplexity = 3.457033\n",
      "2024-06-13 19:42:08.385000: I runner.py:310] Step = 20100 ; steps/s = 1.65, tokens/s = 44901 (44901 target) ; Learning rate = 0.000623 ; Loss = 1.725192\n",
      "2024-06-13 19:43:09.079000: I runner.py:310] Step = 20200 ; steps/s = 1.65, tokens/s = 44785 (44785 target) ; Learning rate = 0.000622 ; Loss = 1.724123\n",
      "2024-06-13 19:44:09.762000: I runner.py:310] Step = 20300 ; steps/s = 1.65, tokens/s = 44811 (44811 target) ; Learning rate = 0.000620 ; Loss = 1.727954\n",
      "2024-06-13 19:45:10.028000: I runner.py:310] Step = 20400 ; steps/s = 1.66, tokens/s = 44209 (44209 target) ; Learning rate = 0.000619 ; Loss = 1.709123\n",
      "2024-06-13 19:46:10.729000: I runner.py:310] Step = 20500 ; steps/s = 1.65, tokens/s = 44779 (44779 target) ; Learning rate = 0.000617 ; Loss = 1.708281\n",
      "2024-06-13 19:47:11.427000: I runner.py:310] Step = 20600 ; steps/s = 1.65, tokens/s = 44785 (44785 target) ; Learning rate = 0.000616 ; Loss = 1.727243\n",
      "2024-06-13 19:48:12.076000: I runner.py:310] Step = 20700 ; steps/s = 1.65, tokens/s = 44802 (44802 target) ; Learning rate = 0.000614 ; Loss = 1.730504\n",
      "2024-06-13 19:49:12.361000: I runner.py:310] Step = 20800 ; steps/s = 1.66, tokens/s = 44213 (44213 target) ; Learning rate = 0.000613 ; Loss = 1.702425\n",
      "2024-06-13 19:50:13.053000: I runner.py:310] Step = 20900 ; steps/s = 1.65, tokens/s = 44784 (44784 target) ; Learning rate = 0.000611 ; Loss = 1.692354\n",
      "2024-06-13 19:51:13.722000: I runner.py:310] Step = 21000 ; steps/s = 1.65, tokens/s = 44789 (44789 target) ; Learning rate = 0.000610 ; Loss = 1.718062\n",
      "2024-06-13 19:52:14.369000: I runner.py:310] Step = 21100 ; steps/s = 1.65, tokens/s = 44813 (44813 target) ; Learning rate = 0.000608 ; Loss = 1.736691\n",
      "2024-06-13 19:53:14.596000: I runner.py:310] Step = 21200 ; steps/s = 1.66, tokens/s = 44262 (44262 target) ; Learning rate = 0.000607 ; Loss = 1.691150\n",
      "2024-06-13 19:54:15.284000: I runner.py:310] Step = 21300 ; steps/s = 1.65, tokens/s = 44790 (44790 target) ; Learning rate = 0.000606 ; Loss = 1.719628\n",
      "2024-06-13 19:55:16.038000: I runner.py:310] Step = 21400 ; steps/s = 1.65, tokens/s = 44733 (44733 target) ; Learning rate = 0.000604 ; Loss = 1.717401\n",
      "2024-06-13 19:56:16.716000: I runner.py:310] Step = 21500 ; steps/s = 1.65, tokens/s = 44768 (44768 target) ; Learning rate = 0.000603 ; Loss = 1.740221\n",
      "2024-06-13 19:57:16.939000: I runner.py:310] Step = 21600 ; steps/s = 1.66, tokens/s = 44270 (44270 target) ; Learning rate = 0.000601 ; Loss = 1.668635\n",
      "2024-06-13 19:58:17.654000: I runner.py:310] Step = 21700 ; steps/s = 1.65, tokens/s = 44770 (44770 target) ; Learning rate = 0.000600 ; Loss = 1.712758\n",
      "2024-06-13 19:59:18.292000: I runner.py:310] Step = 21800 ; steps/s = 1.65, tokens/s = 44831 (44831 target) ; Learning rate = 0.000599 ; Loss = 1.712858\n",
      "2024-06-13 20:00:18.612000: I runner.py:310] Step = 21900 ; steps/s = 1.66, tokens/s = 44168 (44168 target) ; Learning rate = 0.000597 ; Loss = 1.701830\n",
      "2024-06-13 20:01:19.246000: I runner.py:310] Step = 22000 ; steps/s = 1.65, tokens/s = 44850 (44850 target) ; Learning rate = 0.000596 ; Loss = 1.689390\n",
      "2024-06-13 20:02:19.956000: I runner.py:310] Step = 22100 ; steps/s = 1.65, tokens/s = 44777 (44777 target) ; Learning rate = 0.000595 ; Loss = 1.709800\n",
      "2024-06-13 20:03:20.712000: I runner.py:310] Step = 22200 ; steps/s = 1.65, tokens/s = 44734 (44734 target) ; Learning rate = 0.000593 ; Loss = 1.707336\n",
      "2024-06-13 20:04:21.008000: I runner.py:310] Step = 22300 ; steps/s = 1.66, tokens/s = 44198 (44198 target) ; Learning rate = 0.000592 ; Loss = 1.692701\n",
      "2024-06-13 20:05:21.642000: I runner.py:310] Step = 22400 ; steps/s = 1.65, tokens/s = 44816 (44816 target) ; Learning rate = 0.000591 ; Loss = 1.719566\n",
      "2024-06-13 20:06:22.373000: I runner.py:310] Step = 22500 ; steps/s = 1.65, tokens/s = 44743 (44743 target) ; Learning rate = 0.000589 ; Loss = 1.700469\n",
      "2024-06-13 20:07:23.075000: I runner.py:310] Step = 22600 ; steps/s = 1.65, tokens/s = 44781 (44781 target) ; Learning rate = 0.000588 ; Loss = 1.704653\n",
      "2024-06-13 20:08:23.272000: I runner.py:310] Step = 22700 ; steps/s = 1.66, tokens/s = 44282 (44282 target) ; Learning rate = 0.000587 ; Loss = 1.706835\n",
      "2024-06-13 20:09:24.005000: I runner.py:310] Step = 22800 ; steps/s = 1.65, tokens/s = 44755 (44755 target) ; Learning rate = 0.000585 ; Loss = 1.684643\n",
      "2024-06-13 20:10:24.664000: I runner.py:310] Step = 22900 ; steps/s = 1.65, tokens/s = 44806 (44806 target) ; Learning rate = 0.000584 ; Loss = 1.687540\n",
      "2024-06-13 20:11:25.394000: I runner.py:310] Step = 23000 ; steps/s = 1.65, tokens/s = 44743 (44743 target) ; Learning rate = 0.000583 ; Loss = 1.692194\n",
      "2024-06-13 20:12:25.692000: I runner.py:310] Step = 23100 ; steps/s = 1.66, tokens/s = 44211 (44211 target) ; Learning rate = 0.000582 ; Loss = 1.665390\n",
      "2024-06-13 20:13:26.372000: I runner.py:310] Step = 23200 ; steps/s = 1.65, tokens/s = 44805 (44805 target) ; Learning rate = 0.000580 ; Loss = 1.677154\n",
      "2024-06-13 20:14:27.089000: I runner.py:310] Step = 23300 ; steps/s = 1.65, tokens/s = 44768 (44768 target) ; Learning rate = 0.000579 ; Loss = 1.701335\n",
      "2024-06-13 20:15:27.787000: I runner.py:310] Step = 23400 ; steps/s = 1.65, tokens/s = 44737 (44737 target) ; Learning rate = 0.000578 ; Loss = 1.719155\n",
      "2024-06-13 20:16:28.013000: I runner.py:310] Step = 23500 ; steps/s = 1.66, tokens/s = 44268 (44268 target) ; Learning rate = 0.000577 ; Loss = 1.689852\n",
      "2024-06-13 20:17:28.738000: I runner.py:310] Step = 23600 ; steps/s = 1.65, tokens/s = 44779 (44779 target) ; Learning rate = 0.000575 ; Loss = 1.689049\n",
      "2024-06-13 20:18:29.402000: I runner.py:310] Step = 23700 ; steps/s = 1.65, tokens/s = 44782 (44782 target) ; Learning rate = 0.000574 ; Loss = 1.678287\n",
      "2024-06-13 20:19:30.108000: I runner.py:310] Step = 23800 ; steps/s = 1.65, tokens/s = 44760 (44760 target) ; Learning rate = 0.000573 ; Loss = 1.696568\n",
      "2024-06-13 20:20:30.385000: I runner.py:310] Step = 23900 ; steps/s = 1.66, tokens/s = 44236 (44236 target) ; Learning rate = 0.000572 ; Loss = 1.658222\n",
      "2024-06-13 20:21:31.009000: I runner.py:310] Step = 24000 ; steps/s = 1.65, tokens/s = 44797 (44797 target) ; Learning rate = 0.000571 ; Loss = 1.683117\n",
      "2024-06-13 20:22:31.698000: I runner.py:310] Step = 24100 ; steps/s = 1.65, tokens/s = 44773 (44773 target) ; Learning rate = 0.000569 ; Loss = 1.707276\n",
      "2024-06-13 20:23:32.062000: I runner.py:310] Step = 24200 ; steps/s = 1.66, tokens/s = 44487 (44487 target) ; Learning rate = 0.000568 ; Loss = 1.703227\n",
      "2024-06-13 20:24:32.559000: I runner.py:310] Step = 24300 ; steps/s = 1.65, tokens/s = 44641 (44641 target) ; Learning rate = 0.000567 ; Loss = 1.678108\n",
      "2024-06-13 20:25:33.223000: I runner.py:310] Step = 24400 ; steps/s = 1.65, tokens/s = 44778 (44778 target) ; Learning rate = 0.000566 ; Loss = 1.669954\n",
      "2024-06-13 20:26:33.924000: I runner.py:310] Step = 24500 ; steps/s = 1.65, tokens/s = 44781 (44781 target) ; Learning rate = 0.000565 ; Loss = 1.680964\n",
      "2024-06-13 20:27:34.139000: I runner.py:310] Step = 24600 ; steps/s = 1.66, tokens/s = 44290 (44290 target) ; Learning rate = 0.000564 ; Loss = 1.674723\n",
      "2024-06-13 20:28:34.841000: I runner.py:310] Step = 24700 ; steps/s = 1.65, tokens/s = 44764 (44764 target) ; Learning rate = 0.000562 ; Loss = 1.652549\n",
      "2024-06-13 20:29:35.563000: I runner.py:310] Step = 24800 ; steps/s = 1.65, tokens/s = 44743 (44743 target) ; Learning rate = 0.000561 ; Loss = 1.688872\n",
      "2024-06-13 20:30:36.356000: I runner.py:310] Step = 24900 ; steps/s = 1.65, tokens/s = 44705 (44705 target) ; Learning rate = 0.000560 ; Loss = 1.694455\n",
      "2024-06-13 20:31:36.628000: I runner.py:310] Step = 25000 ; steps/s = 1.66, tokens/s = 44247 (44247 target) ; Learning rate = 0.000559 ; Loss = 1.677895\n",
      "2024-06-13 20:31:36.629000: I training.py:192] Running evaluation for step 25000\n",
      "2024-06-13 20:35:58.228000: I training.py:192] Evaluation result for step 25000: loss = 1.261596 ; perplexity = 3.531052\n",
      "2024-06-13 20:36:58.801000: I runner.py:310] Step = 25100 ; steps/s = 1.65, tokens/s = 44874 (44874 target) ; Learning rate = 0.000558 ; Loss = 1.656556\n",
      "2024-06-13 20:37:59.456000: I runner.py:310] Step = 25200 ; steps/s = 1.65, tokens/s = 44820 (44820 target) ; Learning rate = 0.000557 ; Loss = 1.665148\n",
      "2024-06-13 20:39:00.198000: I runner.py:310] Step = 25300 ; steps/s = 1.65, tokens/s = 44722 (44722 target) ; Learning rate = 0.000556 ; Loss = 1.679268\n",
      "2024-06-13 20:40:00.457000: I runner.py:310] Step = 25400 ; steps/s = 1.66, tokens/s = 44257 (44257 target) ; Learning rate = 0.000555 ; Loss = 1.658650\n",
      "2024-06-13 20:41:01.142000: I runner.py:310] Step = 25500 ; steps/s = 1.65, tokens/s = 44788 (44788 target) ; Learning rate = 0.000553 ; Loss = 1.673352\n",
      "2024-06-13 20:42:01.864000: I runner.py:310] Step = 25600 ; steps/s = 1.65, tokens/s = 44756 (44756 target) ; Learning rate = 0.000552 ; Loss = 1.686031\n",
      "2024-06-13 20:43:02.600000: I runner.py:310] Step = 25700 ; steps/s = 1.65, tokens/s = 44730 (44730 target) ; Learning rate = 0.000551 ; Loss = 1.679811\n",
      "2024-06-13 20:44:02.830000: I runner.py:310] Step = 25800 ; steps/s = 1.66, tokens/s = 44249 (44249 target) ; Learning rate = 0.000550 ; Loss = 1.657016\n",
      "2024-06-13 20:45:03.520000: I runner.py:310] Step = 25900 ; steps/s = 1.65, tokens/s = 44804 (44804 target) ; Learning rate = 0.000549 ; Loss = 1.659976\n",
      "2024-06-13 20:46:04.283000: I runner.py:310] Step = 26000 ; steps/s = 1.65, tokens/s = 44711 (44711 target) ; Learning rate = 0.000548 ; Loss = 1.679200\n",
      "2024-06-13 20:47:04.989000: I runner.py:310] Step = 26100 ; steps/s = 1.65, tokens/s = 44758 (44758 target) ; Learning rate = 0.000547 ; Loss = 1.676227\n",
      "2024-06-13 20:48:05.240000: I runner.py:310] Step = 26200 ; steps/s = 1.66, tokens/s = 44259 (44259 target) ; Learning rate = 0.000546 ; Loss = 1.651984\n",
      "2024-06-13 20:49:05.926000: I runner.py:310] Step = 26300 ; steps/s = 1.65, tokens/s = 44777 (44777 target) ; Learning rate = 0.000545 ; Loss = 1.656650\n",
      "2024-06-13 20:50:06.635000: I runner.py:310] Step = 26400 ; steps/s = 1.65, tokens/s = 44762 (44762 target) ; Learning rate = 0.000544 ; Loss = 1.663192\n",
      "2024-06-13 20:51:07.312000: I runner.py:310] Step = 26500 ; steps/s = 1.65, tokens/s = 44791 (44791 target) ; Learning rate = 0.000543 ; Loss = 1.672654\n",
      "2024-06-13 20:52:07.638000: I runner.py:310] Step = 26600 ; steps/s = 1.66, tokens/s = 44219 (44219 target) ; Learning rate = 0.000542 ; Loss = 1.668497\n",
      "2024-06-13 20:53:08.336000: I runner.py:310] Step = 26700 ; steps/s = 1.65, tokens/s = 44769 (44769 target) ; Learning rate = 0.000541 ; Loss = 1.646024\n",
      "2024-06-13 20:54:09.044000: I runner.py:310] Step = 26800 ; steps/s = 1.65, tokens/s = 44764 (44764 target) ; Learning rate = 0.000540 ; Loss = 1.664045\n",
      "2024-06-13 20:55:09.347000: I runner.py:310] Step = 26900 ; steps/s = 1.66, tokens/s = 44204 (44204 target) ; Learning rate = 0.000539 ; Loss = 1.655121\n",
      "2024-06-13 20:56:10.084000: I runner.py:310] Step = 27000 ; steps/s = 1.65, tokens/s = 44785 (44785 target) ; Learning rate = 0.000538 ; Loss = 1.655541\n",
      "2024-06-13 20:57:10.759000: I runner.py:310] Step = 27100 ; steps/s = 1.65, tokens/s = 44776 (44776 target) ; Learning rate = 0.000537 ; Loss = 1.648511\n",
      "2024-06-13 20:58:11.474000: I runner.py:310] Step = 27200 ; steps/s = 1.65, tokens/s = 44740 (44740 target) ; Learning rate = 0.000536 ; Loss = 1.660064\n",
      "2024-06-13 20:59:11.695000: I runner.py:310] Step = 27300 ; steps/s = 1.66, tokens/s = 44259 (44259 target) ; Learning rate = 0.000535 ; Loss = 1.644043\n",
      "2024-06-13 21:00:12.386000: I runner.py:310] Step = 27400 ; steps/s = 1.65, tokens/s = 44793 (44793 target) ; Learning rate = 0.000534 ; Loss = 1.649891\n",
      "2024-06-13 21:01:13.131000: I runner.py:310] Step = 27500 ; steps/s = 1.65, tokens/s = 44741 (44741 target) ; Learning rate = 0.000533 ; Loss = 1.662021\n",
      "2024-06-13 21:02:13.932000: I runner.py:310] Step = 27600 ; steps/s = 1.64, tokens/s = 44672 (44672 target) ; Learning rate = 0.000532 ; Loss = 1.663143\n",
      "2024-06-13 21:03:14.210000: I runner.py:310] Step = 27700 ; steps/s = 1.66, tokens/s = 44243 (44243 target) ; Learning rate = 0.000531 ; Loss = 1.641339\n",
      "2024-06-13 21:04:14.925000: I runner.py:310] Step = 27800 ; steps/s = 1.65, tokens/s = 44770 (44770 target) ; Learning rate = 0.000530 ; Loss = 1.649560\n",
      "2024-06-13 21:05:15.651000: I runner.py:310] Step = 27900 ; steps/s = 1.65, tokens/s = 44761 (44761 target) ; Learning rate = 0.000529 ; Loss = 1.650622\n",
      "2024-06-13 21:06:16.350000: I runner.py:310] Step = 28000 ; steps/s = 1.65, tokens/s = 44752 (44752 target) ; Learning rate = 0.000528 ; Loss = 1.659825\n",
      "2024-06-13 21:07:16.607000: I runner.py:310] Step = 28100 ; steps/s = 1.66, tokens/s = 44256 (44256 target) ; Learning rate = 0.000527 ; Loss = 1.634589\n",
      "2024-06-13 21:08:17.332000: I runner.py:310] Step = 28200 ; steps/s = 1.65, tokens/s = 44755 (44755 target) ; Learning rate = 0.000526 ; Loss = 1.644936\n",
      "2024-06-13 21:09:18.069000: I runner.py:310] Step = 28300 ; steps/s = 1.65, tokens/s = 44726 (44726 target) ; Learning rate = 0.000525 ; Loss = 1.654771\n",
      "2024-06-13 21:10:18.794000: I runner.py:310] Step = 28400 ; steps/s = 1.65, tokens/s = 44747 (44747 target) ; Learning rate = 0.000524 ; Loss = 1.655365\n",
      "2024-06-13 21:11:19.072000: I runner.py:310] Step = 28500 ; steps/s = 1.66, tokens/s = 44262 (44262 target) ; Learning rate = 0.000524 ; Loss = 1.655762\n",
      "2024-06-13 21:12:19.790000: I runner.py:310] Step = 28600 ; steps/s = 1.65, tokens/s = 44750 (44750 target) ; Learning rate = 0.000523 ; Loss = 1.643632\n",
      "2024-06-13 21:13:20.583000: I runner.py:310] Step = 28700 ; steps/s = 1.65, tokens/s = 44661 (44661 target) ; Learning rate = 0.000522 ; Loss = 1.652067\n",
      "2024-06-13 21:14:21.367000: I runner.py:310] Step = 28800 ; steps/s = 1.65, tokens/s = 44716 (44716 target) ; Learning rate = 0.000521 ; Loss = 1.650958\n",
      "2024-06-13 21:15:21.703000: I runner.py:310] Step = 28900 ; steps/s = 1.66, tokens/s = 44247 (44247 target) ; Learning rate = 0.000520 ; Loss = 1.624011\n",
      "2024-06-13 21:16:22.433000: I runner.py:310] Step = 29000 ; steps/s = 1.65, tokens/s = 44734 (44734 target) ; Learning rate = 0.000519 ; Loss = 1.648737\n",
      "2024-06-13 21:17:23.156000: I runner.py:310] Step = 29100 ; steps/s = 1.65, tokens/s = 44758 (44758 target) ; Learning rate = 0.000518 ; Loss = 1.652932\n",
      "2024-06-13 21:18:23.458000: I runner.py:310] Step = 29200 ; steps/s = 1.66, tokens/s = 44183 (44183 target) ; Learning rate = 0.000517 ; Loss = 1.648174\n",
      "2024-06-13 21:19:24.141000: I runner.py:310] Step = 29300 ; steps/s = 1.65, tokens/s = 44803 (44803 target) ; Learning rate = 0.000516 ; Loss = 1.624673\n",
      "2024-06-13 21:20:24.836000: I runner.py:310] Step = 29400 ; steps/s = 1.65, tokens/s = 44797 (44797 target) ; Learning rate = 0.000515 ; Loss = 1.643308\n",
      "2024-06-13 21:21:25.533000: I runner.py:310] Step = 29500 ; steps/s = 1.65, tokens/s = 44738 (44738 target) ; Learning rate = 0.000515 ; Loss = 1.641947\n",
      "2024-06-13 21:22:25.817000: I runner.py:310] Step = 29600 ; steps/s = 1.66, tokens/s = 44229 (44229 target) ; Learning rate = 0.000514 ; Loss = 1.631441\n",
      "2024-06-13 21:23:26.524000: I runner.py:310] Step = 29700 ; steps/s = 1.65, tokens/s = 44756 (44756 target) ; Learning rate = 0.000513 ; Loss = 1.639375\n",
      "2024-06-13 21:24:27.244000: I runner.py:310] Step = 29800 ; steps/s = 1.65, tokens/s = 44748 (44748 target) ; Learning rate = 0.000512 ; Loss = 1.646353\n",
      "2024-06-13 21:25:27.868000: I runner.py:310] Step = 29900 ; steps/s = 1.65, tokens/s = 44828 (44828 target) ; Learning rate = 0.000511 ; Loss = 1.651858\n",
      "2024-06-13 21:26:28.208000: I runner.py:310] Step = 30000 ; steps/s = 1.66, tokens/s = 44209 (44209 target) ; Learning rate = 0.000510 ; Loss = 1.640917\n",
      "2024-06-13 21:26:30.059000: I training.py:176] Saved checkpoint RoBERTa_POS-KK-EN/ckpt-30000\n",
      "2024-06-13 21:26:30.059000: I training.py:192] Running evaluation for step 30000\n",
      "2024-06-13 21:30:58.520000: I training.py:192] Evaluation result for step 30000: loss = 1.277493 ; perplexity = 3.587635\n",
      "2024-06-13 21:31:59.027000: I runner.py:310] Step = 30100 ; steps/s = 1.65, tokens/s = 44926 (44926 target) ; Learning rate = 0.000509 ; Loss = 1.637762\n",
      "2024-06-13 21:32:59.781000: I runner.py:310] Step = 30200 ; steps/s = 1.65, tokens/s = 44700 (44700 target) ; Learning rate = 0.000509 ; Loss = 1.634477\n",
      "2024-06-13 21:34:00.526000: I runner.py:310] Step = 30300 ; steps/s = 1.65, tokens/s = 44739 (44739 target) ; Learning rate = 0.000508 ; Loss = 1.630201\n",
      "2024-06-13 21:35:00.788000: I runner.py:310] Step = 30400 ; steps/s = 1.66, tokens/s = 44262 (44262 target) ; Learning rate = 0.000507 ; Loss = 1.650421\n",
      "2024-06-13 21:36:01.528000: I runner.py:310] Step = 30500 ; steps/s = 1.65, tokens/s = 44734 (44734 target) ; Learning rate = 0.000506 ; Loss = 1.619605\n",
      "2024-06-13 21:37:02.292000: I runner.py:310] Step = 30600 ; steps/s = 1.65, tokens/s = 44689 (44689 target) ; Learning rate = 0.000505 ; Loss = 1.626339\n",
      "2024-06-13 21:38:03.081000: I runner.py:310] Step = 30700 ; steps/s = 1.65, tokens/s = 44705 (44705 target) ; Learning rate = 0.000504 ; Loss = 1.637725\n",
      "2024-06-13 21:39:03.309000: I runner.py:310] Step = 30800 ; steps/s = 1.66, tokens/s = 44302 (44302 target) ; Learning rate = 0.000504 ; Loss = 1.635821\n",
      "2024-06-13 21:40:04.057000: I runner.py:310] Step = 30900 ; steps/s = 1.65, tokens/s = 44761 (44761 target) ; Learning rate = 0.000503 ; Loss = 1.630669\n",
      "2024-06-13 21:41:04.733000: I runner.py:310] Step = 31000 ; steps/s = 1.65, tokens/s = 44761 (44761 target) ; Learning rate = 0.000502 ; Loss = 1.628376\n",
      "2024-06-13 21:42:05.536000: I runner.py:310] Step = 31100 ; steps/s = 1.64, tokens/s = 44692 (44692 target) ; Learning rate = 0.000501 ; Loss = 1.641736\n",
      "2024-06-13 21:43:05.866000: I runner.py:310] Step = 31200 ; steps/s = 1.66, tokens/s = 44214 (44214 target) ; Learning rate = 0.000500 ; Loss = 1.640615\n",
      "2024-06-13 21:44:06.639000: I runner.py:310] Step = 31300 ; steps/s = 1.65, tokens/s = 44714 (44714 target) ; Learning rate = 0.000500 ; Loss = 1.632467\n",
      "2024-06-13 21:45:07.395000: I runner.py:310] Step = 31400 ; steps/s = 1.65, tokens/s = 44701 (44701 target) ; Learning rate = 0.000499 ; Loss = 1.639963\n",
      "2024-06-13 21:46:07.747000: I runner.py:310] Step = 31500 ; steps/s = 1.66, tokens/s = 44182 (44182 target) ; Learning rate = 0.000498 ; Loss = 1.659353\n",
      "2024-06-13 21:47:08.461000: I runner.py:310] Step = 31600 ; steps/s = 1.65, tokens/s = 44824 (44824 target) ; Learning rate = 0.000497 ; Loss = 1.641962\n",
      "2024-06-13 21:48:09.222000: I runner.py:310] Step = 31700 ; steps/s = 1.65, tokens/s = 44696 (44696 target) ; Learning rate = 0.000496 ; Loss = 1.632222\n",
      "2024-06-13 21:49:09.997000: I runner.py:310] Step = 31800 ; steps/s = 1.65, tokens/s = 44696 (44696 target) ; Learning rate = 0.000496 ; Loss = 1.630113\n",
      "2024-06-13 21:50:10.294000: I runner.py:310] Step = 31900 ; steps/s = 1.66, tokens/s = 44225 (44225 target) ; Learning rate = 0.000495 ; Loss = 1.629477\n",
      "2024-06-13 21:51:11.017000: I runner.py:310] Step = 32000 ; steps/s = 1.65, tokens/s = 44763 (44763 target) ; Learning rate = 0.000494 ; Loss = 1.626264\n",
      "2024-06-13 21:52:11.803000: I runner.py:310] Step = 32100 ; steps/s = 1.65, tokens/s = 44674 (44674 target) ; Learning rate = 0.000493 ; Loss = 1.635117\n",
      "2024-06-13 21:53:12.548000: I runner.py:310] Step = 32200 ; steps/s = 1.65, tokens/s = 44746 (44746 target) ; Learning rate = 0.000493 ; Loss = 1.630017\n",
      "2024-06-13 21:54:12.846000: I runner.py:310] Step = 32300 ; steps/s = 1.66, tokens/s = 44234 (44234 target) ; Learning rate = 0.000492 ; Loss = 1.628531\n",
      "2024-06-13 21:55:13.580000: I runner.py:310] Step = 32400 ; steps/s = 1.65, tokens/s = 44741 (44741 target) ; Learning rate = 0.000491 ; Loss = 1.622830\n",
      "2024-06-13 21:56:14.306000: I runner.py:310] Step = 32500 ; steps/s = 1.65, tokens/s = 44749 (44749 target) ; Learning rate = 0.000490 ; Loss = 1.621715\n",
      "2024-06-13 21:57:15.024000: I runner.py:310] Step = 32600 ; steps/s = 1.65, tokens/s = 44742 (44742 target) ; Learning rate = 0.000490 ; Loss = 1.626186\n",
      "2024-06-13 21:58:15.295000: I runner.py:310] Step = 32700 ; steps/s = 1.66, tokens/s = 44254 (44254 target) ; Learning rate = 0.000489 ; Loss = 1.635952\n",
      "2024-06-13 21:59:16.086000: I runner.py:310] Step = 32800 ; steps/s = 1.65, tokens/s = 44708 (44708 target) ; Learning rate = 0.000488 ; Loss = 1.612400\n",
      "2024-06-13 22:00:16.790000: I runner.py:310] Step = 32900 ; steps/s = 1.65, tokens/s = 44761 (44761 target) ; Learning rate = 0.000487 ; Loss = 1.624246\n",
      "2024-06-13 22:01:17.530000: I runner.py:310] Step = 33000 ; steps/s = 1.65, tokens/s = 44741 (44741 target) ; Learning rate = 0.000487 ; Loss = 1.621663\n",
      "2024-06-13 22:02:17.873000: I runner.py:310] Step = 33100 ; steps/s = 1.66, tokens/s = 44186 (44186 target) ; Learning rate = 0.000486 ; Loss = 1.608692\n",
      "2024-06-13 22:03:18.623000: I runner.py:310] Step = 33200 ; steps/s = 1.65, tokens/s = 44757 (44757 target) ; Learning rate = 0.000485 ; Loss = 1.632106\n",
      "2024-06-13 22:04:19.325000: I runner.py:310] Step = 33300 ; steps/s = 1.65, tokens/s = 44771 (44771 target) ; Learning rate = 0.000484 ; Loss = 1.642581\n",
      "2024-06-13 22:05:20.081000: I runner.py:310] Step = 33400 ; steps/s = 1.65, tokens/s = 44693 (44693 target) ; Learning rate = 0.000484 ; Loss = 1.641012\n",
      "2024-06-13 22:06:20.366000: I runner.py:310] Step = 33500 ; steps/s = 1.66, tokens/s = 44263 (44263 target) ; Learning rate = 0.000483 ; Loss = 1.634602\n",
      "2024-06-13 22:07:21.015000: I runner.py:310] Step = 33600 ; steps/s = 1.65, tokens/s = 44827 (44827 target) ; Learning rate = 0.000482 ; Loss = 1.606600\n",
      "2024-06-13 22:08:21.743000: I runner.py:310] Step = 33700 ; steps/s = 1.65, tokens/s = 44728 (44728 target) ; Learning rate = 0.000481 ; Loss = 1.604325\n",
      "2024-06-13 22:09:22.559000: I runner.py:310] Step = 33800 ; steps/s = 1.64, tokens/s = 44679 (44679 target) ; Learning rate = 0.000481 ; Loss = 1.630374\n",
      "2024-06-13 22:10:22.830000: I runner.py:310] Step = 33900 ; steps/s = 1.66, tokens/s = 44244 (44244 target) ; Learning rate = 0.000480 ; Loss = 1.605280\n",
      "2024-06-13 22:11:23.551000: I runner.py:310] Step = 34000 ; steps/s = 1.65, tokens/s = 44723 (44723 target) ; Learning rate = 0.000479 ; Loss = 1.624096\n",
      "2024-06-13 22:12:24.247000: I runner.py:310] Step = 34100 ; steps/s = 1.65, tokens/s = 44788 (44788 target) ; Learning rate = 0.000479 ; Loss = 1.626961\n",
      "2024-06-13 22:13:24.574000: I runner.py:310] Step = 34200 ; steps/s = 1.66, tokens/s = 44211 (44211 target) ; Learning rate = 0.000478 ; Loss = 1.620003\n",
      "2024-06-13 22:14:25.295000: I runner.py:310] Step = 34300 ; steps/s = 1.65, tokens/s = 44765 (44765 target) ; Learning rate = 0.000477 ; Loss = 1.626566\n",
      "2024-06-13 22:15:26.029000: I runner.py:310] Step = 34400 ; steps/s = 1.65, tokens/s = 44726 (44726 target) ; Learning rate = 0.000477 ; Loss = 1.614470\n",
      "2024-06-13 22:16:26.764000: I runner.py:310] Step = 34500 ; steps/s = 1.65, tokens/s = 44750 (44750 target) ; Learning rate = 0.000476 ; Loss = 1.620652\n",
      "2024-06-13 22:17:27.046000: I runner.py:310] Step = 34600 ; steps/s = 1.66, tokens/s = 44240 (44240 target) ; Learning rate = 0.000475 ; Loss = 1.617561\n",
      "2024-06-13 22:18:27.766000: I runner.py:310] Step = 34700 ; steps/s = 1.65, tokens/s = 44740 (44740 target) ; Learning rate = 0.000474 ; Loss = 1.609542\n",
      "2024-06-13 22:19:28.541000: I runner.py:310] Step = 34800 ; steps/s = 1.65, tokens/s = 44701 (44701 target) ; Learning rate = 0.000474 ; Loss = 1.618754\n",
      "2024-06-13 22:20:29.276000: I runner.py:310] Step = 34900 ; steps/s = 1.65, tokens/s = 44773 (44773 target) ; Learning rate = 0.000473 ; Loss = 1.623642\n",
      "2024-06-13 22:21:29.600000: I runner.py:310] Step = 35000 ; steps/s = 1.66, tokens/s = 44191 (44191 target) ; Learning rate = 0.000472 ; Loss = 1.620372\n",
      "2024-06-13 22:21:29.601000: I training.py:192] Running evaluation for step 35000\n",
      "2024-06-13 22:25:49.984000: I training.py:192] Evaluation result for step 35000: loss = 1.295865 ; perplexity = 3.654155\n",
      "2024-06-13 22:26:50.599000: I runner.py:310] Step = 35100 ; steps/s = 1.65, tokens/s = 44847 (44847 target) ; Learning rate = 0.000472 ; Loss = 1.611789\n",
      "2024-06-13 22:27:51.322000: I runner.py:310] Step = 35200 ; steps/s = 1.65, tokens/s = 44746 (44746 target) ; Learning rate = 0.000471 ; Loss = 1.620452\n",
      "2024-06-13 22:28:52.007000: I runner.py:310] Step = 35300 ; steps/s = 1.65, tokens/s = 44788 (44788 target) ; Learning rate = 0.000470 ; Loss = 1.623855\n",
      "2024-06-13 22:29:52.323000: I runner.py:310] Step = 35400 ; steps/s = 1.66, tokens/s = 44215 (44215 target) ; Learning rate = 0.000470 ; Loss = 1.622795\n",
      "2024-06-13 22:30:53.020000: I runner.py:310] Step = 35500 ; steps/s = 1.65, tokens/s = 44767 (44767 target) ; Learning rate = 0.000469 ; Loss = 1.612895\n",
      "2024-06-13 22:31:53.751000: I runner.py:310] Step = 35600 ; steps/s = 1.65, tokens/s = 44733 (44733 target) ; Learning rate = 0.000468 ; Loss = 1.618971\n",
      "2024-06-13 22:32:54.563000: I runner.py:310] Step = 35700 ; steps/s = 1.64, tokens/s = 44695 (44695 target) ; Learning rate = 0.000468 ; Loss = 1.608238\n",
      "2024-06-13 22:33:54.785000: I runner.py:310] Step = 35800 ; steps/s = 1.66, tokens/s = 44303 (44303 target) ; Learning rate = 0.000467 ; Loss = 1.603005\n",
      "2024-06-13 22:34:55.487000: I runner.py:310] Step = 35900 ; steps/s = 1.65, tokens/s = 44754 (44754 target) ; Learning rate = 0.000466 ; Loss = 1.604099\n",
      "2024-06-13 22:35:56.272000: I runner.py:310] Step = 36000 ; steps/s = 1.65, tokens/s = 44695 (44695 target) ; Learning rate = 0.000466 ; Loss = 1.614113\n",
      "2024-06-13 22:36:56.969000: I runner.py:310] Step = 36100 ; steps/s = 1.65, tokens/s = 44775 (44775 target) ; Learning rate = 0.000465 ; Loss = 1.614919\n",
      "2024-06-13 22:37:57.222000: I runner.py:310] Step = 36200 ; steps/s = 1.66, tokens/s = 44232 (44232 target) ; Learning rate = 0.000465 ; Loss = 1.620651\n",
      "2024-06-13 22:38:57.934000: I runner.py:310] Step = 36300 ; steps/s = 1.65, tokens/s = 44761 (44761 target) ; Learning rate = 0.000464 ; Loss = 1.609369\n",
      "2024-06-13 22:39:58.777000: I runner.py:310] Step = 36400 ; steps/s = 1.64, tokens/s = 44682 (44682 target) ; Learning rate = 0.000463 ; Loss = 1.608894\n",
      "2024-06-13 22:40:59.104000: I runner.py:310] Step = 36500 ; steps/s = 1.66, tokens/s = 44197 (44197 target) ; Learning rate = 0.000463 ; Loss = 1.599141\n",
      "2024-06-13 22:41:59.819000: I runner.py:310] Step = 36600 ; steps/s = 1.65, tokens/s = 44808 (44808 target) ; Learning rate = 0.000462 ; Loss = 1.597412\n",
      "2024-06-13 22:43:00.550000: I runner.py:310] Step = 36700 ; steps/s = 1.65, tokens/s = 44735 (44735 target) ; Learning rate = 0.000461 ; Loss = 1.613332\n",
      "2024-06-13 22:44:01.234000: I runner.py:310] Step = 36800 ; steps/s = 1.65, tokens/s = 44768 (44768 target) ; Learning rate = 0.000461 ; Loss = 1.615128\n",
      "2024-06-13 22:45:01.520000: I runner.py:310] Step = 36900 ; steps/s = 1.66, tokens/s = 44204 (44204 target) ; Learning rate = 0.000460 ; Loss = 1.603397\n",
      "2024-06-13 22:46:02.173000: I runner.py:310] Step = 37000 ; steps/s = 1.65, tokens/s = 44832 (44832 target) ; Learning rate = 0.000460 ; Loss = 1.597836\n",
      "2024-06-13 22:47:02.896000: I runner.py:310] Step = 37100 ; steps/s = 1.65, tokens/s = 44754 (44754 target) ; Learning rate = 0.000459 ; Loss = 1.607656\n",
      "2024-06-13 22:48:03.647000: I runner.py:310] Step = 37200 ; steps/s = 1.65, tokens/s = 44713 (44713 target) ; Learning rate = 0.000458 ; Loss = 1.606451\n",
      "2024-06-13 22:49:03.835000: I runner.py:310] Step = 37300 ; steps/s = 1.66, tokens/s = 44289 (44289 target) ; Learning rate = 0.000458 ; Loss = 1.590465\n",
      "2024-06-13 22:50:04.498000: I runner.py:310] Step = 37400 ; steps/s = 1.65, tokens/s = 44810 (44810 target) ; Learning rate = 0.000457 ; Loss = 1.604443\n",
      "2024-06-13 22:51:05.255000: I runner.py:310] Step = 37500 ; steps/s = 1.65, tokens/s = 44705 (44705 target) ; Learning rate = 0.000456 ; Loss = 1.608333\n",
      "2024-06-13 22:52:05.944000: I runner.py:310] Step = 37600 ; steps/s = 1.65, tokens/s = 44798 (44798 target) ; Learning rate = 0.000456 ; Loss = 1.610800\n",
      "2024-06-13 22:53:06.248000: I runner.py:310] Step = 37700 ; steps/s = 1.66, tokens/s = 44220 (44220 target) ; Learning rate = 0.000455 ; Loss = 1.612566\n",
      "2024-06-13 22:54:06.946000: I runner.py:310] Step = 37800 ; steps/s = 1.65, tokens/s = 44808 (44808 target) ; Learning rate = 0.000455 ; Loss = 1.596781\n",
      "2024-06-13 22:55:07.731000: I runner.py:310] Step = 37900 ; steps/s = 1.65, tokens/s = 44681 (44681 target) ; Learning rate = 0.000454 ; Loss = 1.599470\n",
      "2024-06-13 22:56:08.446000: I runner.py:310] Step = 38000 ; steps/s = 1.65, tokens/s = 44751 (44751 target) ; Learning rate = 0.000453 ; Loss = 1.613363\n",
      "2024-06-13 22:57:08.699000: I runner.py:310] Step = 38100 ; steps/s = 1.66, tokens/s = 44272 (44272 target) ; Learning rate = 0.000453 ; Loss = 1.583732\n",
      "2024-06-13 22:58:09.422000: I runner.py:310] Step = 38200 ; steps/s = 1.65, tokens/s = 44749 (44749 target) ; Learning rate = 0.000452 ; Loss = 1.604778\n",
      "2024-06-13 22:59:10.115000: I runner.py:310] Step = 38300 ; steps/s = 1.65, tokens/s = 44764 (44764 target) ; Learning rate = 0.000452 ; Loss = 1.609298\n",
      "2024-06-13 23:00:10.930000: I runner.py:310] Step = 38400 ; steps/s = 1.64, tokens/s = 44691 (44691 target) ; Learning rate = 0.000451 ; Loss = 1.625386\n",
      "2024-06-13 23:01:11.149000: I runner.py:310] Step = 38500 ; steps/s = 1.66, tokens/s = 44249 (44249 target) ; Learning rate = 0.000450 ; Loss = 1.583326\n",
      "2024-06-13 23:02:11.855000: I runner.py:310] Step = 38600 ; steps/s = 1.65, tokens/s = 44793 (44793 target) ; Learning rate = 0.000450 ; Loss = 1.600624\n",
      "2024-06-13 23:03:12.632000: I runner.py:310] Step = 38700 ; steps/s = 1.65, tokens/s = 44730 (44730 target) ; Learning rate = 0.000449 ; Loss = 1.615046\n",
      "2024-06-13 23:04:12.904000: I runner.py:310] Step = 38800 ; steps/s = 1.66, tokens/s = 44188 (44188 target) ; Learning rate = 0.000449 ; Loss = 1.596205\n",
      "2024-06-13 23:05:13.596000: I runner.py:310] Step = 38900 ; steps/s = 1.65, tokens/s = 44805 (44805 target) ; Learning rate = 0.000448 ; Loss = 1.603829\n",
      "2024-06-13 23:06:14.370000: I runner.py:310] Step = 39000 ; steps/s = 1.65, tokens/s = 44732 (44732 target) ; Learning rate = 0.000448 ; Loss = 1.600491\n",
      "2024-06-13 23:07:15.386000: I runner.py:310] Step = 39100 ; steps/s = 1.64, tokens/s = 44538 (44538 target) ; Learning rate = 0.000447 ; Loss = 1.610330\n",
      "2024-06-13 23:08:15.762000: I runner.py:310] Step = 39200 ; steps/s = 1.66, tokens/s = 44133 (44133 target) ; Learning rate = 0.000446 ; Loss = 1.599700\n",
      "2024-06-13 23:09:16.505000: I runner.py:310] Step = 39300 ; steps/s = 1.65, tokens/s = 44744 (44744 target) ; Learning rate = 0.000446 ; Loss = 1.592518\n",
      "2024-06-13 23:10:17.176000: I runner.py:310] Step = 39400 ; steps/s = 1.65, tokens/s = 44774 (44774 target) ; Learning rate = 0.000445 ; Loss = 1.617209\n",
      "2024-06-13 23:11:17.943000: I runner.py:310] Step = 39500 ; steps/s = 1.65, tokens/s = 44731 (44731 target) ; Learning rate = 0.000445 ; Loss = 1.596124\n",
      "2024-06-13 23:12:18.278000: I runner.py:310] Step = 39600 ; steps/s = 1.66, tokens/s = 44215 (44215 target) ; Learning rate = 0.000444 ; Loss = 1.590948\n",
      "2024-06-13 23:13:19.017000: I runner.py:310] Step = 39700 ; steps/s = 1.65, tokens/s = 44749 (44749 target) ; Learning rate = 0.000444 ; Loss = 1.593365\n",
      "2024-06-13 23:14:19.791000: I runner.py:310] Step = 39800 ; steps/s = 1.65, tokens/s = 44686 (44686 target) ; Learning rate = 0.000443 ; Loss = 1.596522\n",
      "2024-06-13 23:15:20.569000: I runner.py:310] Step = 39900 ; steps/s = 1.65, tokens/s = 44708 (44708 target) ; Learning rate = 0.000442 ; Loss = 1.602116\n",
      "2024-06-13 23:16:20.814000: I runner.py:310] Step = 40000 ; steps/s = 1.66, tokens/s = 44273 (44273 target) ; Learning rate = 0.000442 ; Loss = 1.600255\n",
      "2024-06-13 23:16:22.733000: I training.py:176] Saved checkpoint RoBERTa_POS-KK-EN/ckpt-40000\n",
      "2024-06-13 23:16:22.734000: I training.py:192] Running evaluation for step 40000\n",
      "2024-06-13 23:20:38.408000: I training.py:192] Evaluation result for step 40000: loss = 1.302808 ; perplexity = 3.679615\n",
      "2024-06-13 23:21:39.004000: I runner.py:310] Step = 40100 ; steps/s = 1.65, tokens/s = 44857 (44857 target) ; Learning rate = 0.000441 ; Loss = 1.587610\n",
      "2024-06-13 23:22:39.727000: I runner.py:310] Step = 40200 ; steps/s = 1.65, tokens/s = 44744 (44744 target) ; Learning rate = 0.000441 ; Loss = 1.592126\n",
      "2024-06-13 23:23:40.474000: I runner.py:310] Step = 40300 ; steps/s = 1.65, tokens/s = 44738 (44738 target) ; Learning rate = 0.000440 ; Loss = 1.605213\n",
      "2024-06-13 23:24:40.771000: I runner.py:310] Step = 40400 ; steps/s = 1.66, tokens/s = 44220 (44220 target) ; Learning rate = 0.000440 ; Loss = 1.580425\n",
      "2024-06-13 23:25:41.623000: I runner.py:310] Step = 40500 ; steps/s = 1.64, tokens/s = 44633 (44633 target) ; Learning rate = 0.000439 ; Loss = 1.597485\n",
      "2024-06-13 23:26:42.407000: I runner.py:310] Step = 40600 ; steps/s = 1.65, tokens/s = 44721 (44721 target) ; Learning rate = 0.000439 ; Loss = 1.612854\n",
      "2024-06-13 23:27:43.190000: I runner.py:310] Step = 40700 ; steps/s = 1.65, tokens/s = 44724 (44724 target) ; Learning rate = 0.000438 ; Loss = 1.601825\n",
      "2024-06-13 23:28:43.472000: I runner.py:310] Step = 40800 ; steps/s = 1.66, tokens/s = 44250 (44250 target) ; Learning rate = 0.000438 ; Loss = 1.588019\n",
      "2024-06-13 23:29:44.210000: I runner.py:310] Step = 40900 ; steps/s = 1.65, tokens/s = 44729 (44729 target) ; Learning rate = 0.000437 ; Loss = 1.595960\n",
      "2024-06-13 23:30:44.957000: I runner.py:310] Step = 41000 ; steps/s = 1.65, tokens/s = 44728 (44728 target) ; Learning rate = 0.000437 ; Loss = 1.597078\n",
      "2024-06-13 23:31:45.662000: I runner.py:310] Step = 41100 ; steps/s = 1.65, tokens/s = 44750 (44750 target) ; Learning rate = 0.000436 ; Loss = 1.614987\n",
      "2024-06-13 23:32:45.898000: I runner.py:310] Step = 41200 ; steps/s = 1.66, tokens/s = 44278 (44278 target) ; Learning rate = 0.000435 ; Loss = 1.593535\n",
      "2024-06-13 23:33:46.601000: I runner.py:310] Step = 41300 ; steps/s = 1.65, tokens/s = 44798 (44798 target) ; Learning rate = 0.000435 ; Loss = 1.585717\n",
      "2024-06-13 23:34:47.349000: I runner.py:310] Step = 41400 ; steps/s = 1.65, tokens/s = 44733 (44733 target) ; Learning rate = 0.000434 ; Loss = 1.599997\n",
      "2024-06-13 23:35:47.643000: I runner.py:310] Step = 41500 ; steps/s = 1.66, tokens/s = 44203 (44203 target) ; Learning rate = 0.000434 ; Loss = 1.588339\n",
      "2024-06-13 23:36:48.413000: I runner.py:310] Step = 41600 ; steps/s = 1.65, tokens/s = 44728 (44728 target) ; Learning rate = 0.000433 ; Loss = 1.578905\n",
      "2024-06-13 23:37:49.251000: I runner.py:310] Step = 41700 ; steps/s = 1.64, tokens/s = 44654 (44654 target) ; Learning rate = 0.000433 ; Loss = 1.593351\n",
      "2024-06-13 23:38:50.002000: I runner.py:310] Step = 41800 ; steps/s = 1.65, tokens/s = 44739 (44739 target) ; Learning rate = 0.000432 ; Loss = 1.595405\n",
      "2024-06-13 23:39:50.332000: I runner.py:310] Step = 41900 ; steps/s = 1.66, tokens/s = 44180 (44180 target) ; Learning rate = 0.000432 ; Loss = 1.591015\n",
      "2024-06-13 23:40:51.106000: I runner.py:310] Step = 42000 ; steps/s = 1.65, tokens/s = 44739 (44739 target) ; Learning rate = 0.000431 ; Loss = 1.589672\n",
      "2024-06-13 23:41:51.848000: I runner.py:310] Step = 42100 ; steps/s = 1.65, tokens/s = 44715 (44715 target) ; Learning rate = 0.000431 ; Loss = 1.591512\n",
      "2024-06-13 23:42:52.581000: I runner.py:310] Step = 42200 ; steps/s = 1.65, tokens/s = 44751 (44751 target) ; Learning rate = 0.000430 ; Loss = 1.594471\n",
      "2024-06-13 23:43:52.860000: I runner.py:310] Step = 42300 ; steps/s = 1.66, tokens/s = 44245 (44245 target) ; Learning rate = 0.000430 ; Loss = 1.603246\n",
      "2024-06-13 23:44:53.628000: I runner.py:310] Step = 42400 ; steps/s = 1.65, tokens/s = 44694 (44694 target) ; Learning rate = 0.000429 ; Loss = 1.590057\n",
      "2024-06-13 23:45:54.349000: I runner.py:310] Step = 42500 ; steps/s = 1.65, tokens/s = 44760 (44760 target) ; Learning rate = 0.000429 ; Loss = 1.598492\n",
      "2024-06-13 23:46:55.085000: I runner.py:310] Step = 42600 ; steps/s = 1.65, tokens/s = 44739 (44739 target) ; Learning rate = 0.000428 ; Loss = 1.586780\n",
      "2024-06-13 23:47:55.372000: I runner.py:310] Step = 42700 ; steps/s = 1.66, tokens/s = 44244 (44244 target) ; Learning rate = 0.000428 ; Loss = 1.602578\n",
      "2024-06-13 23:48:56.111000: I runner.py:310] Step = 42800 ; steps/s = 1.65, tokens/s = 44733 (44733 target) ; Learning rate = 0.000427 ; Loss = 1.578767\n",
      "2024-06-13 23:49:56.839000: I runner.py:310] Step = 42900 ; steps/s = 1.65, tokens/s = 44752 (44752 target) ; Learning rate = 0.000427 ; Loss = 1.586727\n",
      "2024-06-13 23:50:57.569000: I runner.py:310] Step = 43000 ; steps/s = 1.65, tokens/s = 44749 (44749 target) ; Learning rate = 0.000426 ; Loss = 1.590732\n",
      "2024-06-13 23:51:57.839000: I runner.py:310] Step = 43100 ; steps/s = 1.66, tokens/s = 44269 (44269 target) ; Learning rate = 0.000426 ; Loss = 1.595787\n",
      "2024-06-13 23:52:58.534000: I runner.py:310] Step = 43200 ; steps/s = 1.65, tokens/s = 44761 (44761 target) ; Learning rate = 0.000425 ; Loss = 1.587940\n",
      "2024-06-13 23:53:59.361000: I runner.py:310] Step = 43300 ; steps/s = 1.64, tokens/s = 44707 (44707 target) ; Learning rate = 0.000425 ; Loss = 1.585185\n",
      "2024-06-13 23:55:00.094000: I runner.py:310] Step = 43400 ; steps/s = 1.65, tokens/s = 44705 (44705 target) ; Learning rate = 0.000424 ; Loss = 1.593032\n",
      "2024-06-13 23:56:00.421000: I runner.py:310] Step = 43500 ; steps/s = 1.66, tokens/s = 44217 (44217 target) ; Learning rate = 0.000424 ; Loss = 1.593385\n",
      "2024-06-13 23:57:01.199000: I runner.py:310] Step = 43600 ; steps/s = 1.65, tokens/s = 44730 (44730 target) ; Learning rate = 0.000423 ; Loss = 1.573837\n",
      "2024-06-13 23:58:01.953000: I runner.py:310] Step = 43700 ; steps/s = 1.65, tokens/s = 44705 (44705 target) ; Learning rate = 0.000423 ; Loss = 1.582108\n",
      "2024-06-13 23:59:02.265000: I runner.py:310] Step = 43800 ; steps/s = 1.66, tokens/s = 44191 (44191 target) ; Learning rate = 0.000422 ; Loss = 1.588336\n",
      "2024-06-14 00:00:03.050000: I runner.py:310] Step = 43900 ; steps/s = 1.65, tokens/s = 44718 (44718 target) ; Learning rate = 0.000422 ; Loss = 1.581519\n",
      "2024-06-14 00:01:03.774000: I runner.py:310] Step = 44000 ; steps/s = 1.65, tokens/s = 44757 (44757 target) ; Learning rate = 0.000421 ; Loss = 1.596389\n",
      "2024-06-14 00:02:04.643000: I runner.py:310] Step = 44100 ; steps/s = 1.64, tokens/s = 44619 (44619 target) ; Learning rate = 0.000421 ; Loss = 1.591080\n",
      "2024-06-14 00:03:04.964000: I runner.py:310] Step = 44200 ; steps/s = 1.66, tokens/s = 44225 (44225 target) ; Learning rate = 0.000420 ; Loss = 1.580171\n",
      "2024-06-14 00:04:05.750000: I runner.py:310] Step = 44300 ; steps/s = 1.65, tokens/s = 44698 (44698 target) ; Learning rate = 0.000420 ; Loss = 1.577583\n",
      "2024-06-14 00:05:06.542000: I runner.py:310] Step = 44400 ; steps/s = 1.65, tokens/s = 44699 (44699 target) ; Learning rate = 0.000419 ; Loss = 1.590061\n",
      "2024-06-14 00:06:07.295000: I runner.py:310] Step = 44500 ; steps/s = 1.65, tokens/s = 44724 (44724 target) ; Learning rate = 0.000419 ; Loss = 1.591695\n",
      "2024-06-14 00:07:07.561000: I runner.py:310] Step = 44600 ; steps/s = 1.66, tokens/s = 44236 (44236 target) ; Learning rate = 0.000419 ; Loss = 1.594056\n",
      "2024-06-14 00:08:08.331000: I runner.py:310] Step = 44700 ; steps/s = 1.65, tokens/s = 44725 (44725 target) ; Learning rate = 0.000418 ; Loss = 1.578373\n",
      "2024-06-14 00:09:09.026000: I runner.py:310] Step = 44800 ; steps/s = 1.65, tokens/s = 44779 (44779 target) ; Learning rate = 0.000418 ; Loss = 1.582920\n",
      "2024-06-14 00:10:09.835000: I runner.py:310] Step = 44900 ; steps/s = 1.64, tokens/s = 44688 (44688 target) ; Learning rate = 0.000417 ; Loss = 1.585264\n",
      "2024-06-14 00:11:10.133000: I runner.py:310] Step = 45000 ; steps/s = 1.66, tokens/s = 44227 (44227 target) ; Learning rate = 0.000417 ; Loss = 1.588842\n",
      "2024-06-14 00:11:10.135000: I training.py:192] Running evaluation for step 45000\n",
      "2024-06-14 00:15:19.136000: I training.py:192] Evaluation result for step 45000: loss = 1.315570 ; perplexity = 3.726876\n",
      "2024-06-14 00:16:19.715000: I runner.py:310] Step = 45100 ; steps/s = 1.65, tokens/s = 44880 (44880 target) ; Learning rate = 0.000416 ; Loss = 1.576186\n",
      "2024-06-14 00:17:20.437000: I runner.py:310] Step = 45200 ; steps/s = 1.65, tokens/s = 44735 (44735 target) ; Learning rate = 0.000416 ; Loss = 1.578315\n",
      "2024-06-14 00:18:21.180000: I runner.py:310] Step = 45300 ; steps/s = 1.65, tokens/s = 44732 (44732 target) ; Learning rate = 0.000415 ; Loss = 1.584087\n",
      "2024-06-14 00:19:21.484000: I runner.py:310] Step = 45400 ; steps/s = 1.66, tokens/s = 44213 (44213 target) ; Learning rate = 0.000415 ; Loss = 1.566564\n",
      "2024-06-14 00:20:22.208000: I runner.py:310] Step = 45500 ; steps/s = 1.65, tokens/s = 44767 (44767 target) ; Learning rate = 0.000414 ; Loss = 1.576076\n",
      "2024-06-14 00:21:22.945000: I runner.py:310] Step = 45600 ; steps/s = 1.65, tokens/s = 44740 (44740 target) ; Learning rate = 0.000414 ; Loss = 1.591009\n",
      "2024-06-14 00:22:23.772000: I runner.py:310] Step = 45700 ; steps/s = 1.64, tokens/s = 44671 (44671 target) ; Learning rate = 0.000413 ; Loss = 1.590274\n",
      "2024-06-14 00:23:24.090000: I runner.py:310] Step = 45800 ; steps/s = 1.66, tokens/s = 44199 (44199 target) ; Learning rate = 0.000413 ; Loss = 1.591417\n",
      "2024-06-14 00:24:24.848000: I runner.py:310] Step = 45900 ; steps/s = 1.65, tokens/s = 44732 (44732 target) ; Learning rate = 0.000413 ; Loss = 1.584372\n",
      "2024-06-14 00:25:25.597000: I runner.py:310] Step = 46000 ; steps/s = 1.65, tokens/s = 44717 (44717 target) ; Learning rate = 0.000412 ; Loss = 1.572296\n",
      "2024-06-14 00:26:25.928000: I runner.py:310] Step = 46100 ; steps/s = 1.66, tokens/s = 44208 (44208 target) ; Learning rate = 0.000412 ; Loss = 1.584105\n",
      "2024-06-14 00:27:26.649000: I runner.py:310] Step = 46200 ; steps/s = 1.65, tokens/s = 44742 (44742 target) ; Learning rate = 0.000411 ; Loss = 1.563293\n",
      "2024-06-14 00:28:27.402000: I runner.py:310] Step = 46300 ; steps/s = 1.65, tokens/s = 44725 (44725 target) ; Learning rate = 0.000411 ; Loss = 1.571809\n",
      "2024-06-14 00:29:28.164000: I runner.py:310] Step = 46400 ; steps/s = 1.65, tokens/s = 44752 (44752 target) ; Learning rate = 0.000410 ; Loss = 1.586768\n",
      "2024-06-14 00:30:28.418000: I runner.py:310] Step = 46500 ; steps/s = 1.66, tokens/s = 44248 (44248 target) ; Learning rate = 0.000410 ; Loss = 1.587332\n",
      "2024-06-14 00:31:29.212000: I runner.py:310] Step = 46600 ; steps/s = 1.65, tokens/s = 44727 (44727 target) ; Learning rate = 0.000409 ; Loss = 1.578218\n",
      "2024-06-14 00:32:29.956000: I runner.py:310] Step = 46700 ; steps/s = 1.65, tokens/s = 44726 (44726 target) ; Learning rate = 0.000409 ; Loss = 1.572408\n",
      "2024-06-14 00:33:30.728000: I runner.py:310] Step = 46800 ; steps/s = 1.65, tokens/s = 44707 (44707 target) ; Learning rate = 0.000409 ; Loss = 1.578883\n",
      "2024-06-14 00:34:31.048000: I runner.py:310] Step = 46900 ; steps/s = 1.66, tokens/s = 44184 (44184 target) ; Learning rate = 0.000408 ; Loss = 1.591925\n",
      "2024-06-14 00:35:31.771000: I runner.py:310] Step = 47000 ; steps/s = 1.65, tokens/s = 44754 (44754 target) ; Learning rate = 0.000408 ; Loss = 1.571783\n",
      "2024-06-14 00:36:32.494000: I runner.py:310] Step = 47100 ; steps/s = 1.65, tokens/s = 44745 (44745 target) ; Learning rate = 0.000407 ; Loss = 1.568556\n",
      "2024-06-14 00:37:33.259000: I runner.py:310] Step = 47200 ; steps/s = 1.65, tokens/s = 44731 (44731 target) ; Learning rate = 0.000407 ; Loss = 1.581052\n",
      "2024-06-14 00:38:33.564000: I runner.py:310] Step = 47300 ; steps/s = 1.66, tokens/s = 44222 (44222 target) ; Learning rate = 0.000406 ; Loss = 1.588217\n",
      "2024-06-14 00:39:34.300000: I runner.py:310] Step = 47400 ; steps/s = 1.65, tokens/s = 44775 (44775 target) ; Learning rate = 0.000406 ; Loss = 1.575730\n",
      "2024-06-14 00:40:35.007000: I runner.py:310] Step = 47500 ; steps/s = 1.65, tokens/s = 44779 (44779 target) ; Learning rate = 0.000406 ; Loss = 1.574186\n",
      "2024-06-14 00:41:35.767000: I runner.py:310] Step = 47600 ; steps/s = 1.65, tokens/s = 44687 (44687 target) ; Learning rate = 0.000405 ; Loss = 1.582228\n",
      "2024-06-14 00:42:36.042000: I runner.py:310] Step = 47700 ; steps/s = 1.66, tokens/s = 44257 (44257 target) ; Learning rate = 0.000405 ; Loss = 1.586798\n",
      "2024-06-14 00:43:36.848000: I runner.py:310] Step = 47800 ; steps/s = 1.64, tokens/s = 44678 (44678 target) ; Learning rate = 0.000404 ; Loss = 1.569930\n",
      "2024-06-14 00:44:37.609000: I runner.py:310] Step = 47900 ; steps/s = 1.65, tokens/s = 44694 (44694 target) ; Learning rate = 0.000404 ; Loss = 1.580790\n",
      "2024-06-14 00:45:38.328000: I runner.py:310] Step = 48000 ; steps/s = 1.65, tokens/s = 44760 (44760 target) ; Learning rate = 0.000403 ; Loss = 1.588251\n",
      "2024-06-14 00:46:38.638000: I runner.py:310] Step = 48100 ; steps/s = 1.66, tokens/s = 44217 (44217 target) ; Learning rate = 0.000403 ; Loss = 1.563155\n",
      "2024-06-14 00:47:39.419000: I runner.py:310] Step = 48200 ; steps/s = 1.65, tokens/s = 44722 (44722 target) ; Learning rate = 0.000403 ; Loss = 1.572783\n",
      "2024-06-14 00:48:40.176000: I runner.py:310] Step = 48300 ; steps/s = 1.65, tokens/s = 44727 (44727 target) ; Learning rate = 0.000402 ; Loss = 1.580797\n",
      "2024-06-14 00:49:40.701000: I runner.py:310] Step = 48400 ; steps/s = 1.65, tokens/s = 44560 (44560 target) ; Learning rate = 0.000402 ; Loss = 1.584582\n",
      "2024-06-14 00:50:41.187000: I runner.py:310] Step = 48500 ; steps/s = 1.65, tokens/s = 44389 (44389 target) ; Learning rate = 0.000401 ; Loss = 1.573222\n",
      "2024-06-14 00:51:41.916000: I runner.py:310] Step = 48600 ; steps/s = 1.65, tokens/s = 44756 (44756 target) ; Learning rate = 0.000401 ; Loss = 1.581991\n",
      "2024-06-14 00:52:42.684000: I runner.py:310] Step = 48700 ; steps/s = 1.65, tokens/s = 44722 (44722 target) ; Learning rate = 0.000401 ; Loss = 1.583185\n",
      "2024-06-14 00:53:42.996000: I runner.py:310] Step = 48800 ; steps/s = 1.66, tokens/s = 44223 (44223 target) ; Learning rate = 0.000400 ; Loss = 1.574415\n",
      "2024-06-14 00:54:43.692000: I runner.py:310] Step = 48900 ; steps/s = 1.65, tokens/s = 44791 (44791 target) ; Learning rate = 0.000400 ; Loss = 1.562041\n",
      "2024-06-14 00:55:44.467000: I runner.py:310] Step = 49000 ; steps/s = 1.65, tokens/s = 44693 (44693 target) ; Learning rate = 0.000399 ; Loss = 1.579794\n",
      "2024-06-14 00:56:45.217000: I runner.py:310] Step = 49100 ; steps/s = 1.65, tokens/s = 44749 (44749 target) ; Learning rate = 0.000399 ; Loss = 1.582352\n",
      "2024-06-14 00:57:45.478000: I runner.py:310] Step = 49200 ; steps/s = 1.66, tokens/s = 44255 (44255 target) ; Learning rate = 0.000398 ; Loss = 1.579584\n",
      "2024-06-14 00:58:46.195000: I runner.py:310] Step = 49300 ; steps/s = 1.65, tokens/s = 44777 (44777 target) ; Learning rate = 0.000398 ; Loss = 1.569193\n",
      "2024-06-14 00:59:46.888000: I runner.py:310] Step = 49400 ; steps/s = 1.65, tokens/s = 44761 (44761 target) ; Learning rate = 0.000398 ; Loss = 1.570242\n",
      "2024-06-14 01:00:47.584000: I runner.py:310] Step = 49500 ; steps/s = 1.65, tokens/s = 44765 (44765 target) ; Learning rate = 0.000397 ; Loss = 1.569605\n",
      "2024-06-14 01:01:47.896000: I runner.py:310] Step = 49600 ; steps/s = 1.66, tokens/s = 44202 (44202 target) ; Learning rate = 0.000397 ; Loss = 1.563177\n",
      "2024-06-14 01:02:48.632000: I runner.py:310] Step = 49700 ; steps/s = 1.65, tokens/s = 44755 (44755 target) ; Learning rate = 0.000396 ; Loss = 1.570883\n",
      "2024-06-14 01:03:49.361000: I runner.py:310] Step = 49800 ; steps/s = 1.65, tokens/s = 44768 (44768 target) ; Learning rate = 0.000396 ; Loss = 1.578721\n",
      "2024-06-14 01:04:50.118000: I runner.py:310] Step = 49900 ; steps/s = 1.65, tokens/s = 44703 (44703 target) ; Learning rate = 0.000396 ; Loss = 1.587417\n",
      "2024-06-14 01:05:50.486000: I runner.py:310] Step = 50000 ; steps/s = 1.66, tokens/s = 44187 (44187 target) ; Learning rate = 0.000395 ; Loss = 1.563642\n",
      "2024-06-14 01:05:52.478000: I training.py:176] Saved checkpoint RoBERTa_POS-KK-EN/ckpt-50000\n",
      "2024-06-14 01:05:52.478000: I training.py:192] Running evaluation for step 50000\n",
      "2024-06-14 01:10:09.234000: I training.py:192] Evaluation result for step 50000: loss = 1.327049 ; perplexity = 3.769904\n",
      "2024-06-14 01:11:09.786000: I runner.py:310] Step = 50100 ; steps/s = 1.65, tokens/s = 44887 (44887 target) ; Learning rate = 0.000395 ; Loss = 1.568851\n",
      "2024-06-14 01:12:10.496000: I runner.py:310] Step = 50200 ; steps/s = 1.65, tokens/s = 44755 (44755 target) ; Learning rate = 0.000394 ; Loss = 1.581703\n",
      "2024-06-14 01:13:11.308000: I runner.py:310] Step = 50300 ; steps/s = 1.64, tokens/s = 44682 (44682 target) ; Learning rate = 0.000394 ; Loss = 1.581671\n",
      "2024-06-14 01:14:11.635000: I runner.py:310] Step = 50400 ; steps/s = 1.66, tokens/s = 44222 (44222 target) ; Learning rate = 0.000394 ; Loss = 1.577989\n",
      "2024-06-14 01:15:12.432000: I runner.py:310] Step = 50500 ; steps/s = 1.64, tokens/s = 44664 (44664 target) ; Learning rate = 0.000393 ; Loss = 1.572074\n",
      "2024-06-14 01:16:13.196000: I runner.py:310] Step = 50600 ; steps/s = 1.65, tokens/s = 44710 (44710 target) ; Learning rate = 0.000393 ; Loss = 1.561293\n",
      "2024-06-14 01:17:13.871000: I runner.py:310] Step = 50700 ; steps/s = 1.65, tokens/s = 44799 (44799 target) ; Learning rate = 0.000393 ; Loss = 1.572996\n",
      "2024-06-14 01:18:14.144000: I runner.py:310] Step = 50800 ; steps/s = 1.66, tokens/s = 44234 (44234 target) ; Learning rate = 0.000392 ; Loss = 1.576638\n",
      "2024-06-14 01:19:14.931000: I runner.py:310] Step = 50900 ; steps/s = 1.65, tokens/s = 44710 (44710 target) ; Learning rate = 0.000392 ; Loss = 1.566086\n",
      "2024-06-14 01:20:15.680000: I runner.py:310] Step = 51000 ; steps/s = 1.65, tokens/s = 44742 (44742 target) ; Learning rate = 0.000391 ; Loss = 1.571747\n",
      "2024-06-14 01:21:15.980000: I runner.py:310] Step = 51100 ; steps/s = 1.66, tokens/s = 44207 (44207 target) ; Learning rate = 0.000391 ; Loss = 1.572360\n",
      "2024-06-14 01:22:16.783000: I runner.py:310] Step = 51200 ; steps/s = 1.64, tokens/s = 44723 (44723 target) ; Learning rate = 0.000391 ; Loss = 1.581626\n",
      "2024-06-14 01:23:17.529000: I runner.py:310] Step = 51300 ; steps/s = 1.65, tokens/s = 44731 (44731 target) ; Learning rate = 0.000390 ; Loss = 1.567323\n",
      "2024-06-14 01:24:18.259000: I runner.py:310] Step = 51400 ; steps/s = 1.65, tokens/s = 44747 (44747 target) ; Learning rate = 0.000390 ; Loss = 1.572494\n",
      "2024-06-14 01:25:18.529000: I runner.py:310] Step = 51500 ; steps/s = 1.66, tokens/s = 44223 (44223 target) ; Learning rate = 0.000389 ; Loss = 1.577061\n",
      "2024-06-14 01:26:19.263000: I runner.py:310] Step = 51600 ; steps/s = 1.65, tokens/s = 44747 (44747 target) ; Learning rate = 0.000389 ; Loss = 1.567235\n",
      "2024-06-14 01:27:19.988000: I runner.py:310] Step = 51700 ; steps/s = 1.65, tokens/s = 44754 (44754 target) ; Learning rate = 0.000389 ; Loss = 1.562239\n",
      "2024-06-14 01:28:20.735000: I runner.py:310] Step = 51800 ; steps/s = 1.65, tokens/s = 44739 (44739 target) ; Learning rate = 0.000388 ; Loss = 1.567327\n",
      "2024-06-14 01:29:20.965000: I runner.py:310] Step = 51900 ; steps/s = 1.66, tokens/s = 44251 (44251 target) ; Learning rate = 0.000388 ; Loss = 1.557269\n",
      "2024-06-14 01:30:21.667000: I runner.py:310] Step = 52000 ; steps/s = 1.65, tokens/s = 44803 (44803 target) ; Learning rate = 0.000388 ; Loss = 1.556725\n",
      "2024-06-14 01:31:22.359000: I runner.py:310] Step = 52100 ; steps/s = 1.65, tokens/s = 44755 (44755 target) ; Learning rate = 0.000387 ; Loss = 1.564312\n",
      "2024-06-14 01:32:23.132000: I runner.py:310] Step = 52200 ; steps/s = 1.65, tokens/s = 44708 (44708 target) ; Learning rate = 0.000387 ; Loss = 1.574150\n",
      "2024-06-14 01:33:23.475000: I runner.py:310] Step = 52300 ; steps/s = 1.66, tokens/s = 44218 (44218 target) ; Learning rate = 0.000386 ; Loss = 1.558003\n",
      "2024-06-14 01:34:24.194000: I runner.py:310] Step = 52400 ; steps/s = 1.65, tokens/s = 44720 (44720 target) ; Learning rate = 0.000386 ; Loss = 1.567743\n",
      "2024-06-14 01:35:24.962000: I runner.py:310] Step = 52500 ; steps/s = 1.65, tokens/s = 44763 (44763 target) ; Learning rate = 0.000386 ; Loss = 1.577326\n",
      "2024-06-14 01:36:25.731000: I runner.py:310] Step = 52600 ; steps/s = 1.65, tokens/s = 44672 (44672 target) ; Learning rate = 0.000385 ; Loss = 1.573393\n",
      "2024-06-14 01:37:26.079000: I runner.py:310] Step = 52700 ; steps/s = 1.66, tokens/s = 44199 (44199 target) ; Learning rate = 0.000385 ; Loss = 1.576047\n",
      "2024-06-14 01:38:26.830000: I runner.py:310] Step = 52800 ; steps/s = 1.65, tokens/s = 44736 (44736 target) ; Learning rate = 0.000385 ; Loss = 1.573034\n",
      "2024-06-14 01:39:27.528000: I runner.py:310] Step = 52900 ; steps/s = 1.65, tokens/s = 44769 (44769 target) ; Learning rate = 0.000384 ; Loss = 1.570651\n",
      "2024-06-14 01:40:28.234000: I runner.py:310] Step = 53000 ; steps/s = 1.65, tokens/s = 44767 (44767 target) ; Learning rate = 0.000384 ; Loss = 1.561264\n",
      "2024-06-14 01:41:28.463000: I runner.py:310] Step = 53100 ; steps/s = 1.66, tokens/s = 44260 (44260 target) ; Learning rate = 0.000384 ; Loss = 1.552487\n",
      "2024-06-14 01:42:29.164000: I runner.py:310] Step = 53200 ; steps/s = 1.65, tokens/s = 44769 (44769 target) ; Learning rate = 0.000383 ; Loss = 1.570695\n",
      "2024-06-14 01:43:29.900000: I runner.py:310] Step = 53300 ; steps/s = 1.65, tokens/s = 44748 (44748 target) ; Learning rate = 0.000383 ; Loss = 1.569308\n",
      "2024-06-14 01:44:30.201000: I runner.py:310] Step = 53400 ; steps/s = 1.66, tokens/s = 44204 (44204 target) ; Learning rate = 0.000382 ; Loss = 1.564710\n",
      "2024-06-14 01:45:30.898000: I runner.py:310] Step = 53500 ; steps/s = 1.65, tokens/s = 44813 (44813 target) ; Learning rate = 0.000382 ; Loss = 1.555955\n",
      "2024-06-14 01:46:31.678000: I runner.py:310] Step = 53600 ; steps/s = 1.65, tokens/s = 44714 (44714 target) ; Learning rate = 0.000382 ; Loss = 1.568794\n",
      "2024-06-14 01:47:32.438000: I runner.py:310] Step = 53700 ; steps/s = 1.65, tokens/s = 44715 (44715 target) ; Learning rate = 0.000381 ; Loss = 1.573024\n",
      "2024-06-14 01:48:32.824000: I runner.py:310] Step = 53800 ; steps/s = 1.66, tokens/s = 44136 (44136 target) ; Learning rate = 0.000381 ; Loss = 1.555462\n",
      "2024-06-14 01:49:33.536000: I runner.py:310] Step = 53900 ; steps/s = 1.65, tokens/s = 44779 (44779 target) ; Learning rate = 0.000381 ; Loss = 1.564046\n",
      "2024-06-14 01:50:34.272000: I runner.py:310] Step = 54000 ; steps/s = 1.65, tokens/s = 44758 (44758 target) ; Learning rate = 0.000380 ; Loss = 1.567786\n",
      "2024-06-14 01:51:35.011000: I runner.py:310] Step = 54100 ; steps/s = 1.65, tokens/s = 44731 (44731 target) ; Learning rate = 0.000380 ; Loss = 1.576576\n",
      "2024-06-14 01:52:35.344000: I runner.py:310] Step = 54200 ; steps/s = 1.66, tokens/s = 44174 (44174 target) ; Learning rate = 0.000380 ; Loss = 1.565930\n",
      "2024-06-14 01:53:36.082000: I runner.py:310] Step = 54300 ; steps/s = 1.65, tokens/s = 44734 (44734 target) ; Learning rate = 0.000379 ; Loss = 1.565546\n",
      "2024-06-14 01:54:36.853000: I runner.py:310] Step = 54400 ; steps/s = 1.65, tokens/s = 44711 (44711 target) ; Learning rate = 0.000379 ; Loss = 1.561377\n",
      "2024-06-14 01:55:37.565000: I runner.py:310] Step = 54500 ; steps/s = 1.65, tokens/s = 44791 (44791 target) ; Learning rate = 0.000379 ; Loss = 1.561492\n",
      "2024-06-14 01:56:37.863000: I runner.py:310] Step = 54600 ; steps/s = 1.66, tokens/s = 44200 (44200 target) ; Learning rate = 0.000378 ; Loss = 1.575841\n",
      "2024-06-14 01:57:38.577000: I runner.py:310] Step = 54700 ; steps/s = 1.65, tokens/s = 44774 (44774 target) ; Learning rate = 0.000378 ; Loss = 1.555210\n",
      "2024-06-14 01:58:39.308000: I runner.py:310] Step = 54800 ; steps/s = 1.65, tokens/s = 44738 (44738 target) ; Learning rate = 0.000378 ; Loss = 1.560967\n",
      "2024-06-14 01:59:40.092000: I runner.py:310] Step = 54900 ; steps/s = 1.65, tokens/s = 44700 (44700 target) ; Learning rate = 0.000377 ; Loss = 1.565917\n",
      "2024-06-14 02:00:40.415000: I runner.py:310] Step = 55000 ; steps/s = 1.66, tokens/s = 44205 (44205 target) ; Learning rate = 0.000377 ; Loss = 1.577106\n",
      "2024-06-14 02:00:40.416000: I training.py:192] Running evaluation for step 55000\n",
      "2024-06-14 02:04:52.454000: I training.py:192] Evaluation result for step 55000: loss = 1.330738 ; perplexity = 3.783836\n",
      "2024-06-14 02:05:53.036000: I runner.py:310] Step = 55100 ; steps/s = 1.65, tokens/s = 44878 (44878 target) ; Learning rate = 0.000377 ; Loss = 1.556140\n",
      "2024-06-14 02:06:53.810000: I runner.py:310] Step = 55200 ; steps/s = 1.65, tokens/s = 44716 (44716 target) ; Learning rate = 0.000376 ; Loss = 1.562331\n",
      "2024-06-14 02:07:54.543000: I runner.py:310] Step = 55300 ; steps/s = 1.65, tokens/s = 44743 (44743 target) ; Learning rate = 0.000376 ; Loss = 1.563429\n",
      "2024-06-14 02:08:54.853000: I runner.py:310] Step = 55400 ; steps/s = 1.66, tokens/s = 44207 (44207 target) ; Learning rate = 0.000376 ; Loss = 1.546508\n",
      "2024-06-14 02:09:55.672000: I runner.py:310] Step = 55500 ; steps/s = 1.64, tokens/s = 44685 (44685 target) ; Learning rate = 0.000375 ; Loss = 1.564581\n",
      "2024-06-14 02:10:56.373000: I runner.py:310] Step = 55600 ; steps/s = 1.65, tokens/s = 44766 (44766 target) ; Learning rate = 0.000375 ; Loss = 1.575243\n",
      "2024-06-14 02:11:56.775000: I runner.py:310] Step = 55700 ; steps/s = 1.66, tokens/s = 44344 (44344 target) ; Learning rate = 0.000375 ; Loss = 1.567215\n",
      "2024-06-14 02:12:57.378000: I runner.py:310] Step = 55800 ; steps/s = 1.65, tokens/s = 44650 (44650 target) ; Learning rate = 0.000374 ; Loss = 1.558504\n",
      "2024-06-14 02:13:58.099000: I runner.py:310] Step = 55900 ; steps/s = 1.65, tokens/s = 44781 (44781 target) ; Learning rate = 0.000374 ; Loss = 1.566400\n",
      "2024-06-14 02:14:58.852000: I runner.py:310] Step = 56000 ; steps/s = 1.65, tokens/s = 44724 (44724 target) ; Learning rate = 0.000374 ; Loss = 1.565237\n",
      "2024-06-14 02:15:59.149000: I runner.py:310] Step = 56100 ; steps/s = 1.66, tokens/s = 44190 (44190 target) ; Learning rate = 0.000373 ; Loss = 1.556698\n",
      "2024-06-14 02:16:59.851000: I runner.py:310] Step = 56200 ; steps/s = 1.65, tokens/s = 44772 (44772 target) ; Learning rate = 0.000373 ; Loss = 1.553162\n",
      "2024-06-14 02:18:00.576000: I runner.py:310] Step = 56300 ; steps/s = 1.65, tokens/s = 44767 (44767 target) ; Learning rate = 0.000373 ; Loss = 1.559074\n",
      "2024-06-14 02:19:01.312000: I runner.py:310] Step = 56400 ; steps/s = 1.65, tokens/s = 44737 (44737 target) ; Learning rate = 0.000372 ; Loss = 1.569975\n",
      "2024-06-14 02:20:01.600000: I runner.py:310] Step = 56500 ; steps/s = 1.66, tokens/s = 44219 (44219 target) ; Learning rate = 0.000372 ; Loss = 1.554799\n",
      "2024-06-14 02:21:02.309000: I runner.py:310] Step = 56600 ; steps/s = 1.65, tokens/s = 44777 (44777 target) ; Learning rate = 0.000372 ; Loss = 1.557060\n",
      "2024-06-14 02:22:03.094000: I runner.py:310] Step = 56700 ; steps/s = 1.65, tokens/s = 44697 (44697 target) ; Learning rate = 0.000371 ; Loss = 1.555253\n",
      "2024-06-14 02:23:03.824000: I runner.py:310] Step = 56800 ; steps/s = 1.65, tokens/s = 44754 (44754 target) ; Learning rate = 0.000371 ; Loss = 1.567410\n",
      "2024-06-14 02:24:04.105000: I runner.py:310] Step = 56900 ; steps/s = 1.66, tokens/s = 44228 (44228 target) ; Learning rate = 0.000371 ; Loss = 1.551495\n",
      "2024-06-14 02:25:04.897000: I runner.py:310] Step = 57000 ; steps/s = 1.65, tokens/s = 44695 (44695 target) ; Learning rate = 0.000370 ; Loss = 1.555229\n",
      "2024-06-14 02:26:05.588000: I runner.py:310] Step = 57100 ; steps/s = 1.65, tokens/s = 44784 (44784 target) ; Learning rate = 0.000370 ; Loss = 1.558834\n",
      "2024-06-14 02:27:06.419000: I runner.py:310] Step = 57200 ; steps/s = 1.64, tokens/s = 44667 (44667 target) ; Learning rate = 0.000370 ; Loss = 1.568454\n",
      "2024-06-14 02:28:06.724000: I runner.py:310] Step = 57300 ; steps/s = 1.66, tokens/s = 44187 (44187 target) ; Learning rate = 0.000369 ; Loss = 1.568210\n",
      "2024-06-14 02:29:07.445000: I runner.py:310] Step = 57400 ; steps/s = 1.65, tokens/s = 44780 (44780 target) ; Learning rate = 0.000369 ; Loss = 1.554650\n",
      "2024-06-14 02:30:08.202000: I runner.py:310] Step = 57500 ; steps/s = 1.65, tokens/s = 44738 (44738 target) ; Learning rate = 0.000369 ; Loss = 1.557532\n",
      "2024-06-14 02:31:08.980000: I runner.py:310] Step = 57600 ; steps/s = 1.65, tokens/s = 44705 (44705 target) ; Learning rate = 0.000368 ; Loss = 1.559505\n",
      "2024-06-14 02:32:09.304000: I runner.py:310] Step = 57700 ; steps/s = 1.66, tokens/s = 44194 (44194 target) ; Learning rate = 0.000368 ; Loss = 1.553186\n",
      "2024-06-14 02:33:10.052000: I runner.py:310] Step = 57800 ; steps/s = 1.65, tokens/s = 44745 (44745 target) ; Learning rate = 0.000368 ; Loss = 1.560242\n",
      "2024-06-14 02:34:10.796000: I runner.py:310] Step = 57900 ; steps/s = 1.65, tokens/s = 44719 (44719 target) ; Learning rate = 0.000367 ; Loss = 1.560902\n",
      "2024-06-14 02:35:11.510000: I runner.py:310] Step = 58000 ; steps/s = 1.65, tokens/s = 44774 (44774 target) ; Learning rate = 0.000367 ; Loss = 1.571372\n",
      "2024-06-14 02:36:11.816000: I runner.py:310] Step = 58100 ; steps/s = 1.66, tokens/s = 44228 (44228 target) ; Learning rate = 0.000367 ; Loss = 1.563723\n",
      "2024-06-14 02:37:12.611000: I runner.py:310] Step = 58200 ; steps/s = 1.65, tokens/s = 44711 (44711 target) ; Learning rate = 0.000366 ; Loss = 1.551675\n",
      "2024-06-14 02:38:13.407000: I runner.py:310] Step = 58300 ; steps/s = 1.65, tokens/s = 44687 (44687 target) ; Learning rate = 0.000366 ; Loss = 1.558471\n",
      "2024-06-14 02:39:13.766000: I runner.py:310] Step = 58400 ; steps/s = 1.66, tokens/s = 44169 (44169 target) ; Learning rate = 0.000366 ; Loss = 1.555805\n",
      "2024-06-14 02:40:14.433000: I runner.py:310] Step = 58500 ; steps/s = 1.65, tokens/s = 44754 (44754 target) ; Learning rate = 0.000365 ; Loss = 1.548641\n",
      "2024-06-14 02:41:15.205000: I runner.py:310] Step = 58600 ; steps/s = 1.65, tokens/s = 44729 (44729 target) ; Learning rate = 0.000365 ; Loss = 1.563467\n",
      "2024-06-14 02:42:15.896000: I runner.py:310] Step = 58700 ; steps/s = 1.65, tokens/s = 44790 (44790 target) ; Learning rate = 0.000365 ; Loss = 1.565697\n",
      "2024-06-14 02:43:16.136000: I runner.py:310] Step = 58800 ; steps/s = 1.66, tokens/s = 44293 (44293 target) ; Learning rate = 0.000365 ; Loss = 1.544801\n",
      "2024-06-14 02:44:16.886000: I runner.py:310] Step = 58900 ; steps/s = 1.65, tokens/s = 44725 (44725 target) ; Learning rate = 0.000364 ; Loss = 1.546288\n",
      "2024-06-14 02:45:17.637000: I runner.py:310] Step = 59000 ; steps/s = 1.65, tokens/s = 44739 (44739 target) ; Learning rate = 0.000364 ; Loss = 1.559549\n",
      "2024-06-14 02:46:18.378000: I runner.py:310] Step = 59100 ; steps/s = 1.65, tokens/s = 44728 (44728 target) ; Learning rate = 0.000364 ; Loss = 1.562986\n",
      "2024-06-14 02:47:18.610000: I runner.py:310] Step = 59200 ; steps/s = 1.66, tokens/s = 44283 (44283 target) ; Learning rate = 0.000363 ; Loss = 1.558542\n",
      "2024-06-14 02:48:19.346000: I runner.py:310] Step = 59300 ; steps/s = 1.65, tokens/s = 44775 (44775 target) ; Learning rate = 0.000363 ; Loss = 1.553464\n",
      "2024-06-14 02:49:20.060000: I runner.py:310] Step = 59400 ; steps/s = 1.65, tokens/s = 44771 (44771 target) ; Learning rate = 0.000363 ; Loss = 1.556412\n",
      "2024-06-14 02:50:20.831000: I runner.py:310] Step = 59500 ; steps/s = 1.65, tokens/s = 44655 (44655 target) ; Learning rate = 0.000362 ; Loss = 1.563575\n",
      "2024-06-14 02:51:21.188000: I runner.py:310] Step = 59600 ; steps/s = 1.66, tokens/s = 44215 (44215 target) ; Learning rate = 0.000362 ; Loss = 1.552271\n",
      "2024-06-14 02:52:21.938000: I runner.py:310] Step = 59700 ; steps/s = 1.65, tokens/s = 44703 (44703 target) ; Learning rate = 0.000362 ; Loss = 1.549912\n",
      "2024-06-14 02:53:22.705000: I runner.py:310] Step = 59800 ; steps/s = 1.65, tokens/s = 44705 (44705 target) ; Learning rate = 0.000361 ; Loss = 1.550647\n",
      "2024-06-14 02:54:23.429000: I runner.py:310] Step = 59900 ; steps/s = 1.65, tokens/s = 44758 (44758 target) ; Learning rate = 0.000361 ; Loss = 1.555272\n",
      "2024-06-14 02:55:23.726000: I runner.py:310] Step = 60000 ; steps/s = 1.66, tokens/s = 44202 (44202 target) ; Learning rate = 0.000361 ; Loss = 1.538703\n",
      "2024-06-14 02:55:25.768000: I training.py:176] Saved checkpoint RoBERTa_POS-KK-EN/ckpt-60000\n",
      "2024-06-14 02:55:25.768000: I training.py:192] Running evaluation for step 60000\n",
      "2024-06-14 02:59:38.135000: I training.py:192] Evaluation result for step 60000: loss = 1.343520 ; perplexity = 3.832510\n",
      "2024-06-14 03:00:38.703000: I runner.py:310] Step = 60100 ; steps/s = 1.65, tokens/s = 44866 (44866 target) ; Learning rate = 0.000361 ; Loss = 1.555619\n",
      "2024-06-14 03:01:39.459000: I runner.py:310] Step = 60200 ; steps/s = 1.65, tokens/s = 44763 (44763 target) ; Learning rate = 0.000360 ; Loss = 1.559043\n",
      "2024-06-14 03:02:40.255000: I runner.py:310] Step = 60300 ; steps/s = 1.65, tokens/s = 44695 (44695 target) ; Learning rate = 0.000360 ; Loss = 1.557073\n",
      "2024-06-14 03:03:40.520000: I runner.py:310] Step = 60400 ; steps/s = 1.66, tokens/s = 44247 (44247 target) ; Learning rate = 0.000360 ; Loss = 1.554400\n",
      "2024-06-14 03:04:41.290000: I runner.py:310] Step = 60500 ; steps/s = 1.65, tokens/s = 44695 (44695 target) ; Learning rate = 0.000359 ; Loss = 1.545453\n",
      "2024-06-14 03:05:42.028000: I runner.py:310] Step = 60600 ; steps/s = 1.65, tokens/s = 44760 (44760 target) ; Learning rate = 0.000359 ; Loss = 1.555456\n",
      "2024-06-14 03:06:42.388000: I runner.py:310] Step = 60700 ; steps/s = 1.66, tokens/s = 44189 (44189 target) ; Learning rate = 0.000359 ; Loss = 1.555803\n",
      "2024-06-14 03:07:43.169000: I runner.py:310] Step = 60800 ; steps/s = 1.65, tokens/s = 44739 (44739 target) ; Learning rate = 0.000358 ; Loss = 1.562618\n",
      "2024-06-14 03:08:43.966000: I runner.py:310] Step = 60900 ; steps/s = 1.64, tokens/s = 44666 (44666 target) ; Learning rate = 0.000358 ; Loss = 1.556026\n",
      "2024-06-14 03:09:44.716000: I runner.py:310] Step = 61000 ; steps/s = 1.65, tokens/s = 44734 (44734 target) ; Learning rate = 0.000358 ; Loss = 1.559538\n",
      "2024-06-14 03:10:45.080000: I runner.py:310] Step = 61100 ; steps/s = 1.66, tokens/s = 44178 (44178 target) ; Learning rate = 0.000358 ; Loss = 1.559239\n",
      "2024-06-14 03:11:45.871000: I runner.py:310] Step = 61200 ; steps/s = 1.65, tokens/s = 44699 (44699 target) ; Learning rate = 0.000357 ; Loss = 1.554928\n",
      "2024-06-14 03:12:46.576000: I runner.py:310] Step = 61300 ; steps/s = 1.65, tokens/s = 44773 (44773 target) ; Learning rate = 0.000357 ; Loss = 1.548271\n",
      "2024-06-14 03:13:47.363000: I runner.py:310] Step = 61400 ; steps/s = 1.65, tokens/s = 44702 (44702 target) ; Learning rate = 0.000357 ; Loss = 1.554963\n",
      "2024-06-14 03:14:47.655000: I runner.py:310] Step = 61500 ; steps/s = 1.66, tokens/s = 44202 (44202 target) ; Learning rate = 0.000356 ; Loss = 1.552673\n",
      "2024-06-14 03:15:48.434000: I runner.py:310] Step = 61600 ; steps/s = 1.65, tokens/s = 44711 (44711 target) ; Learning rate = 0.000356 ; Loss = 1.544587\n",
      "2024-06-14 03:16:49.210000: I runner.py:310] Step = 61700 ; steps/s = 1.65, tokens/s = 44736 (44736 target) ; Learning rate = 0.000356 ; Loss = 1.554210\n",
      "2024-06-14 03:17:49.918000: I runner.py:310] Step = 61800 ; steps/s = 1.65, tokens/s = 44756 (44756 target) ; Learning rate = 0.000356 ; Loss = 1.558600\n",
      "2024-06-14 03:18:50.207000: I runner.py:310] Step = 61900 ; steps/s = 1.66, tokens/s = 44237 (44237 target) ; Learning rate = 0.000355 ; Loss = 1.557668\n",
      "2024-06-14 03:19:50.942000: I runner.py:310] Step = 62000 ; steps/s = 1.65, tokens/s = 44760 (44760 target) ; Learning rate = 0.000355 ; Loss = 1.549280\n",
      "2024-06-14 03:20:51.712000: I runner.py:310] Step = 62100 ; steps/s = 1.65, tokens/s = 44686 (44686 target) ; Learning rate = 0.000355 ; Loss = 1.554827\n",
      "2024-06-14 03:21:52.442000: I runner.py:310] Step = 62200 ; steps/s = 1.65, tokens/s = 44766 (44766 target) ; Learning rate = 0.000354 ; Loss = 1.545605\n",
      "2024-06-14 03:22:52.808000: I runner.py:310] Step = 62300 ; steps/s = 1.66, tokens/s = 44170 (44170 target) ; Learning rate = 0.000354 ; Loss = 1.542373\n",
      "2024-06-14 03:23:53.577000: I runner.py:310] Step = 62400 ; steps/s = 1.65, tokens/s = 44738 (44738 target) ; Learning rate = 0.000354 ; Loss = 1.556648\n",
      "2024-06-14 03:24:54.295000: I runner.py:310] Step = 62500 ; steps/s = 1.65, tokens/s = 44737 (44737 target) ; Learning rate = 0.000354 ; Loss = 1.555349\n",
      "2024-06-14 03:25:55.062000: I runner.py:310] Step = 62600 ; steps/s = 1.65, tokens/s = 44717 (44717 target) ; Learning rate = 0.000353 ; Loss = 1.559983\n",
      "2024-06-14 03:26:55.336000: I runner.py:310] Step = 62700 ; steps/s = 1.66, tokens/s = 44237 (44237 target) ; Learning rate = 0.000353 ; Loss = 1.544219\n",
      "2024-06-14 03:27:56.105000: I runner.py:310] Step = 62800 ; steps/s = 1.65, tokens/s = 44695 (44695 target) ; Learning rate = 0.000353 ; Loss = 1.552223\n",
      "2024-06-14 03:28:56.829000: I runner.py:310] Step = 62900 ; steps/s = 1.65, tokens/s = 44757 (44757 target) ; Learning rate = 0.000352 ; Loss = 1.551975\n",
      "2024-06-14 03:29:57.142000: I runner.py:310] Step = 63000 ; steps/s = 1.66, tokens/s = 44202 (44202 target) ; Learning rate = 0.000352 ; Loss = 1.562398\n",
      "2024-06-14 03:30:57.862000: I runner.py:310] Step = 63100 ; steps/s = 1.65, tokens/s = 44782 (44782 target) ; Learning rate = 0.000352 ; Loss = 1.552408\n",
      "2024-06-14 03:31:58.606000: I runner.py:310] Step = 63200 ; steps/s = 1.65, tokens/s = 44729 (44729 target) ; Learning rate = 0.000352 ; Loss = 1.546775\n",
      "2024-06-14 03:32:59.323000: I runner.py:310] Step = 63300 ; steps/s = 1.65, tokens/s = 44782 (44782 target) ; Learning rate = 0.000351 ; Loss = 1.552486\n",
      "2024-06-14 03:33:59.628000: I runner.py:310] Step = 63400 ; steps/s = 1.66, tokens/s = 44194 (44194 target) ; Learning rate = 0.000351 ; Loss = 1.542152\n",
      "2024-06-14 03:35:00.373000: I runner.py:310] Step = 63500 ; steps/s = 1.65, tokens/s = 44740 (44740 target) ; Learning rate = 0.000351 ; Loss = 1.545535\n",
      "2024-06-14 03:36:01.118000: I runner.py:310] Step = 63600 ; steps/s = 1.65, tokens/s = 44748 (44748 target) ; Learning rate = 0.000350 ; Loss = 1.557526\n",
      "2024-06-14 03:37:01.821000: I runner.py:310] Step = 63700 ; steps/s = 1.65, tokens/s = 44755 (44755 target) ; Learning rate = 0.000350 ; Loss = 1.552860\n",
      "2024-06-14 03:38:02.121000: I runner.py:310] Step = 63800 ; steps/s = 1.66, tokens/s = 44227 (44227 target) ; Learning rate = 0.000350 ; Loss = 1.553495\n",
      "2024-06-14 03:39:02.909000: I runner.py:310] Step = 63900 ; steps/s = 1.65, tokens/s = 44690 (44690 target) ; Learning rate = 0.000350 ; Loss = 1.547377\n",
      "2024-06-14 03:40:03.662000: I runner.py:310] Step = 64000 ; steps/s = 1.65, tokens/s = 44748 (44748 target) ; Learning rate = 0.000349 ; Loss = 1.550763\n",
      "2024-06-14 03:41:04.474000: I runner.py:310] Step = 64100 ; steps/s = 1.64, tokens/s = 44674 (44674 target) ; Learning rate = 0.000349 ; Loss = 1.554568\n",
      "2024-06-14 03:42:04.731000: I runner.py:310] Step = 64200 ; steps/s = 1.66, tokens/s = 44249 (44249 target) ; Learning rate = 0.000349 ; Loss = 1.550933\n",
      "2024-06-14 03:43:05.512000: I runner.py:310] Step = 64300 ; steps/s = 1.65, tokens/s = 44738 (44738 target) ; Learning rate = 0.000349 ; Loss = 1.548974\n",
      "2024-06-14 03:44:06.173000: I runner.py:310] Step = 64400 ; steps/s = 1.65, tokens/s = 44771 (44771 target) ; Learning rate = 0.000348 ; Loss = 1.548910\n",
      "2024-06-14 03:45:06.955000: I runner.py:310] Step = 64500 ; steps/s = 1.65, tokens/s = 44698 (44698 target) ; Learning rate = 0.000348 ; Loss = 1.547776\n",
      "2024-06-14 03:46:07.294000: I runner.py:310] Step = 64600 ; steps/s = 1.66, tokens/s = 44218 (44218 target) ; Learning rate = 0.000348 ; Loss = 1.557784\n",
      "2024-06-14 03:47:08.034000: I runner.py:310] Step = 64700 ; steps/s = 1.65, tokens/s = 44747 (44747 target) ; Learning rate = 0.000347 ; Loss = 1.543952\n",
      "2024-06-14 03:48:08.773000: I runner.py:310] Step = 64800 ; steps/s = 1.65, tokens/s = 44710 (44710 target) ; Learning rate = 0.000347 ; Loss = 1.545842\n",
      "2024-06-14 03:49:09.484000: I runner.py:310] Step = 64900 ; steps/s = 1.65, tokens/s = 44752 (44752 target) ; Learning rate = 0.000347 ; Loss = 1.545151\n",
      "2024-06-14 03:50:09.791000: I runner.py:310] Step = 65000 ; steps/s = 1.66, tokens/s = 44247 (44247 target) ; Learning rate = 0.000347 ; Loss = 1.539104\n",
      "2024-06-14 03:50:09.793000: I training.py:192] Running evaluation for step 65000\n",
      "2024-06-14 03:54:17.838000: I training.py:192] Evaluation result for step 65000: loss = 1.350082 ; perplexity = 3.857740\n",
      "2024-06-14 03:55:18.476000: I runner.py:310] Step = 65100 ; steps/s = 1.65, tokens/s = 44813 (44813 target) ; Learning rate = 0.000346 ; Loss = 1.540795\n",
      "2024-06-14 03:56:19.278000: I runner.py:310] Step = 65200 ; steps/s = 1.64, tokens/s = 44700 (44700 target) ; Learning rate = 0.000346 ; Loss = 1.553693\n",
      "2024-06-14 03:57:20.032000: I runner.py:310] Step = 65300 ; steps/s = 1.65, tokens/s = 44716 (44716 target) ; Learning rate = 0.000346 ; Loss = 1.554081\n",
      "2024-06-14 03:58:20.353000: I runner.py:310] Step = 65400 ; steps/s = 1.66, tokens/s = 44188 (44188 target) ; Learning rate = 0.000346 ; Loss = 1.548314\n",
      "2024-06-14 03:59:21.119000: I runner.py:310] Step = 65500 ; steps/s = 1.65, tokens/s = 44729 (44729 target) ; Learning rate = 0.000345 ; Loss = 1.540045\n",
      "2024-06-14 04:00:21.942000: I runner.py:310] Step = 65600 ; steps/s = 1.64, tokens/s = 44675 (44675 target) ; Learning rate = 0.000345 ; Loss = 1.548942\n",
      "2024-06-14 04:01:22.195000: I runner.py:310] Step = 65700 ; steps/s = 1.66, tokens/s = 44267 (44267 target) ; Learning rate = 0.000345 ; Loss = 1.548224\n",
      "2024-06-14 04:02:22.942000: I runner.py:310] Step = 65800 ; steps/s = 1.65, tokens/s = 44744 (44744 target) ; Learning rate = 0.000345 ; Loss = 1.542247\n",
      "2024-06-14 04:03:23.661000: I runner.py:310] Step = 65900 ; steps/s = 1.65, tokens/s = 44742 (44742 target) ; Learning rate = 0.000344 ; Loss = 1.550998\n",
      "2024-06-14 04:04:24.410000: I runner.py:310] Step = 66000 ; steps/s = 1.65, tokens/s = 44735 (44735 target) ; Learning rate = 0.000344 ; Loss = 1.558760\n",
      "2024-06-14 04:05:24.799000: I runner.py:310] Step = 66100 ; steps/s = 1.66, tokens/s = 44149 (44149 target) ; Learning rate = 0.000344 ; Loss = 1.547335\n",
      "2024-06-14 04:06:25.519000: I runner.py:310] Step = 66200 ; steps/s = 1.65, tokens/s = 44783 (44783 target) ; Learning rate = 0.000344 ; Loss = 1.546398\n",
      "2024-06-14 04:07:26.275000: I runner.py:310] Step = 66300 ; steps/s = 1.65, tokens/s = 44721 (44721 target) ; Learning rate = 0.000343 ; Loss = 1.540450\n",
      "2024-06-14 04:08:27.010000: I runner.py:310] Step = 66400 ; steps/s = 1.65, tokens/s = 44748 (44748 target) ; Learning rate = 0.000343 ; Loss = 1.550069\n",
      "2024-06-14 04:09:27.279000: I runner.py:310] Step = 66500 ; steps/s = 1.66, tokens/s = 44241 (44241 target) ; Learning rate = 0.000343 ; Loss = 1.544514\n",
      "2024-06-14 04:10:28.027000: I runner.py:310] Step = 66600 ; steps/s = 1.65, tokens/s = 44746 (44746 target) ; Learning rate = 0.000342 ; Loss = 1.543420\n",
      "2024-06-14 04:11:28.847000: I runner.py:310] Step = 66700 ; steps/s = 1.64, tokens/s = 44686 (44686 target) ; Learning rate = 0.000342 ; Loss = 1.540935\n",
      "2024-06-14 04:12:29.586000: I runner.py:310] Step = 66800 ; steps/s = 1.65, tokens/s = 44707 (44707 target) ; Learning rate = 0.000342 ; Loss = 1.547292\n",
      "2024-06-14 04:13:29.934000: I runner.py:310] Step = 66900 ; steps/s = 1.66, tokens/s = 44190 (44190 target) ; Learning rate = 0.000342 ; Loss = 1.539861\n",
      "2024-06-14 04:14:30.665000: I runner.py:310] Step = 67000 ; steps/s = 1.65, tokens/s = 44755 (44755 target) ; Learning rate = 0.000341 ; Loss = 1.538104\n",
      "2024-06-14 04:15:31.414000: I runner.py:310] Step = 67100 ; steps/s = 1.65, tokens/s = 44752 (44752 target) ; Learning rate = 0.000341 ; Loss = 1.548351\n",
      "2024-06-14 04:16:32.237000: I runner.py:310] Step = 67200 ; steps/s = 1.64, tokens/s = 44673 (44673 target) ; Learning rate = 0.000341 ; Loss = 1.547246\n",
      "2024-06-14 04:17:32.558000: I runner.py:310] Step = 67300 ; steps/s = 1.66, tokens/s = 44175 (44175 target) ; Learning rate = 0.000341 ; Loss = 1.553645\n",
      "2024-06-14 04:18:33.317000: I runner.py:310] Step = 67400 ; steps/s = 1.65, tokens/s = 44730 (44730 target) ; Learning rate = 0.000340 ; Loss = 1.548634\n",
      "2024-06-14 04:19:34.078000: I runner.py:310] Step = 67500 ; steps/s = 1.65, tokens/s = 44753 (44753 target) ; Learning rate = 0.000340 ; Loss = 1.547756\n",
      "2024-06-14 04:20:34.803000: I runner.py:310] Step = 67600 ; steps/s = 1.65, tokens/s = 44739 (44739 target) ; Learning rate = 0.000340 ; Loss = 1.550939\n",
      "2024-06-14 04:21:35.082000: I runner.py:310] Step = 67700 ; steps/s = 1.66, tokens/s = 44223 (44223 target) ; Learning rate = 0.000340 ; Loss = 1.538216\n",
      "2024-06-14 04:22:35.864000: I runner.py:310] Step = 67800 ; steps/s = 1.65, tokens/s = 44709 (44709 target) ; Learning rate = 0.000339 ; Loss = 1.544515\n",
      "2024-06-14 04:23:36.609000: I runner.py:310] Step = 67900 ; steps/s = 1.65, tokens/s = 44742 (44742 target) ; Learning rate = 0.000339 ; Loss = 1.549997\n",
      "2024-06-14 04:24:36.915000: I runner.py:310] Step = 68000 ; steps/s = 1.66, tokens/s = 44203 (44203 target) ; Learning rate = 0.000339 ; Loss = 1.540506\n",
      "2024-06-14 04:25:37.684000: I runner.py:310] Step = 68100 ; steps/s = 1.65, tokens/s = 44730 (44730 target) ; Learning rate = 0.000339 ; Loss = 1.535797\n",
      "2024-06-14 04:26:38.444000: I runner.py:310] Step = 68200 ; steps/s = 1.65, tokens/s = 44703 (44703 target) ; Learning rate = 0.000338 ; Loss = 1.549688\n",
      "2024-06-14 04:27:39.172000: I runner.py:310] Step = 68300 ; steps/s = 1.65, tokens/s = 44757 (44757 target) ; Learning rate = 0.000338 ; Loss = 1.549206\n",
      "2024-06-14 04:28:39.571000: I runner.py:310] Step = 68400 ; steps/s = 1.66, tokens/s = 44184 (44184 target) ; Learning rate = 0.000338 ; Loss = 1.548322\n",
      "2024-06-14 04:29:40.274000: I runner.py:310] Step = 68500 ; steps/s = 1.65, tokens/s = 44786 (44786 target) ; Learning rate = 0.000338 ; Loss = 1.539814\n",
      "2024-06-14 04:30:41.027000: I runner.py:310] Step = 68600 ; steps/s = 1.65, tokens/s = 44738 (44738 target) ; Learning rate = 0.000337 ; Loss = 1.545851\n",
      "2024-06-14 04:31:41.766000: I runner.py:310] Step = 68700 ; steps/s = 1.65, tokens/s = 44706 (44706 target) ; Learning rate = 0.000337 ; Loss = 1.545395\n",
      "2024-06-14 04:32:42.075000: I runner.py:310] Step = 68800 ; steps/s = 1.66, tokens/s = 44210 (44210 target) ; Learning rate = 0.000337 ; Loss = 1.535419\n",
      "2024-06-14 04:33:42.846000: I runner.py:310] Step = 68900 ; steps/s = 1.65, tokens/s = 44709 (44709 target) ; Learning rate = 0.000337 ; Loss = 1.542072\n",
      "2024-06-14 04:34:43.560000: I runner.py:310] Step = 69000 ; steps/s = 1.65, tokens/s = 44796 (44796 target) ; Learning rate = 0.000336 ; Loss = 1.541401\n",
      "2024-06-14 04:35:44.391000: I runner.py:310] Step = 69100 ; steps/s = 1.64, tokens/s = 44667 (44667 target) ; Learning rate = 0.000336 ; Loss = 1.555491\n",
      "2024-06-14 04:36:44.731000: I runner.py:310] Step = 69200 ; steps/s = 1.66, tokens/s = 44148 (44148 target) ; Learning rate = 0.000336 ; Loss = 1.547180\n",
      "2024-06-14 04:37:45.436000: I runner.py:310] Step = 69300 ; steps/s = 1.65, tokens/s = 44775 (44775 target) ; Learning rate = 0.000336 ; Loss = 1.533827\n",
      "2024-06-14 04:38:46.182000: I runner.py:310] Step = 69400 ; steps/s = 1.65, tokens/s = 44738 (44738 target) ; Learning rate = 0.000336 ; Loss = 1.538652\n",
      "2024-06-14 04:39:46.867000: I runner.py:310] Step = 69500 ; steps/s = 1.65, tokens/s = 44775 (44775 target) ; Learning rate = 0.000335 ; Loss = 1.548040\n",
      "2024-06-14 04:40:47.152000: I runner.py:310] Step = 69600 ; steps/s = 1.66, tokens/s = 44231 (44231 target) ; Learning rate = 0.000335 ; Loss = 1.553015\n",
      "2024-06-14 04:41:47.826000: I runner.py:310] Step = 69700 ; steps/s = 1.65, tokens/s = 44776 (44776 target) ; Learning rate = 0.000335 ; Loss = 1.537683\n",
      "2024-06-14 04:42:48.580000: I runner.py:310] Step = 69800 ; steps/s = 1.65, tokens/s = 44741 (44741 target) ; Learning rate = 0.000335 ; Loss = 1.536363\n",
      "2024-06-14 04:43:49.330000: I runner.py:310] Step = 69900 ; steps/s = 1.65, tokens/s = 44733 (44733 target) ; Learning rate = 0.000334 ; Loss = 1.548444\n",
      "2024-06-14 04:44:49.587000: I runner.py:310] Step = 70000 ; steps/s = 1.66, tokens/s = 44268 (44268 target) ; Learning rate = 0.000334 ; Loss = 1.546120\n",
      "2024-06-14 04:44:52.133000: I training.py:176] Saved checkpoint RoBERTa_POS-KK-EN/ckpt-70000\n",
      "2024-06-14 04:44:52.133000: I training.py:192] Running evaluation for step 70000\n",
      "2024-06-14 04:49:01.016000: I training.py:192] Evaluation result for step 70000: loss = 1.357950 ; perplexity = 3.888213\n",
      "2024-06-14 04:50:01.615000: I runner.py:310] Step = 70100 ; steps/s = 1.65, tokens/s = 44842 (44842 target) ; Learning rate = 0.000334 ; Loss = 1.542441\n",
      "2024-06-14 04:51:02.337000: I runner.py:310] Step = 70200 ; steps/s = 1.65, tokens/s = 44760 (44760 target) ; Learning rate = 0.000334 ; Loss = 1.541024\n",
      "2024-06-14 04:52:02.656000: I runner.py:310] Step = 70300 ; steps/s = 1.66, tokens/s = 44183 (44183 target) ; Learning rate = 0.000333 ; Loss = 1.542792\n",
      "2024-06-14 04:53:03.395000: I runner.py:310] Step = 70400 ; steps/s = 1.65, tokens/s = 44763 (44763 target) ; Learning rate = 0.000333 ; Loss = 1.532774\n",
      "2024-06-14 04:54:04.136000: I runner.py:310] Step = 70500 ; steps/s = 1.65, tokens/s = 44735 (44735 target) ; Learning rate = 0.000333 ; Loss = 1.544594\n",
      "2024-06-14 04:55:04.853000: I runner.py:310] Step = 70600 ; steps/s = 1.65, tokens/s = 44763 (44763 target) ; Learning rate = 0.000333 ; Loss = 1.552719\n",
      "2024-06-14 04:56:05.259000: I runner.py:310] Step = 70700 ; steps/s = 1.66, tokens/s = 44126 (44126 target) ; Learning rate = 0.000332 ; Loss = 1.539027\n",
      "2024-06-14 04:57:06.033000: I runner.py:310] Step = 70800 ; steps/s = 1.65, tokens/s = 44739 (44739 target) ; Learning rate = 0.000332 ; Loss = 1.533293\n",
      "2024-06-14 04:58:06.867000: I runner.py:310] Step = 70900 ; steps/s = 1.64, tokens/s = 44663 (44663 target) ; Learning rate = 0.000332 ; Loss = 1.543464\n",
      "2024-06-14 04:59:07.607000: I runner.py:310] Step = 71000 ; steps/s = 1.65, tokens/s = 44727 (44727 target) ; Learning rate = 0.000332 ; Loss = 1.547724\n",
      "2024-06-14 05:00:07.927000: I runner.py:310] Step = 71100 ; steps/s = 1.66, tokens/s = 44218 (44218 target) ; Learning rate = 0.000331 ; Loss = 1.532962\n",
      "2024-06-14 05:01:08.687000: I runner.py:310] Step = 71200 ; steps/s = 1.65, tokens/s = 44713 (44713 target) ; Learning rate = 0.000331 ; Loss = 1.543621\n",
      "2024-06-14 05:02:09.498000: I runner.py:310] Step = 71300 ; steps/s = 1.64, tokens/s = 44667 (44667 target) ; Learning rate = 0.000331 ; Loss = 1.549847\n",
      "2024-06-14 05:03:10.279000: I runner.py:310] Step = 71400 ; steps/s = 1.65, tokens/s = 44709 (44709 target) ; Learning rate = 0.000331 ; Loss = 1.542240\n",
      "2024-06-14 05:04:10.571000: I runner.py:310] Step = 71500 ; steps/s = 1.66, tokens/s = 44269 (44269 target) ; Learning rate = 0.000331 ; Loss = 1.544633\n",
      "2024-06-14 05:05:11.316000: I runner.py:310] Step = 71600 ; steps/s = 1.65, tokens/s = 44700 (44700 target) ; Learning rate = 0.000330 ; Loss = 1.537892\n",
      "2024-06-14 05:06:12.121000: I runner.py:310] Step = 71700 ; steps/s = 1.64, tokens/s = 44690 (44690 target) ; Learning rate = 0.000330 ; Loss = 1.548372\n",
      "2024-06-14 05:07:12.791000: I runner.py:310] Step = 71800 ; steps/s = 1.65, tokens/s = 44818 (44818 target) ; Learning rate = 0.000330 ; Loss = 1.544763\n",
      "2024-06-14 05:08:13.178000: I runner.py:310] Step = 71900 ; steps/s = 1.66, tokens/s = 44164 (44164 target) ; Learning rate = 0.000330 ; Loss = 1.547101\n",
      "2024-06-14 05:09:13.871000: I runner.py:310] Step = 72000 ; steps/s = 1.65, tokens/s = 44761 (44761 target) ; Learning rate = 0.000329 ; Loss = 1.541715\n",
      "2024-06-14 05:10:14.651000: I runner.py:310] Step = 72100 ; steps/s = 1.65, tokens/s = 44723 (44723 target) ; Learning rate = 0.000329 ; Loss = 1.537364\n",
      "2024-06-14 05:11:15.407000: I runner.py:310] Step = 72200 ; steps/s = 1.65, tokens/s = 44723 (44723 target) ; Learning rate = 0.000329 ; Loss = 1.540110\n",
      "2024-06-14 05:12:15.697000: I runner.py:310] Step = 72300 ; steps/s = 1.66, tokens/s = 44246 (44246 target) ; Learning rate = 0.000329 ; Loss = 1.537941\n",
      "2024-06-14 05:13:16.503000: I runner.py:310] Step = 72400 ; steps/s = 1.64, tokens/s = 44672 (44672 target) ; Learning rate = 0.000328 ; Loss = 1.543198\n",
      "2024-06-14 05:14:17.281000: I runner.py:310] Step = 72500 ; steps/s = 1.65, tokens/s = 44718 (44718 target) ; Learning rate = 0.000328 ; Loss = 1.538131\n",
      "2024-06-14 05:15:17.941000: I runner.py:310] Step = 72600 ; steps/s = 1.65, tokens/s = 44720 (44720 target) ; Learning rate = 0.000328 ; Loss = 1.561211\n",
      "2024-06-14 05:16:18.292000: I runner.py:310] Step = 72700 ; steps/s = 1.66, tokens/s = 44268 (44268 target) ; Learning rate = 0.000328 ; Loss = 1.531048\n",
      "2024-06-14 05:17:19.023000: I runner.py:310] Step = 72800 ; steps/s = 1.65, tokens/s = 44735 (44735 target) ; Learning rate = 0.000328 ; Loss = 1.547093\n",
      "2024-06-14 05:18:19.736000: I runner.py:310] Step = 72900 ; steps/s = 1.65, tokens/s = 44767 (44767 target) ; Learning rate = 0.000327 ; Loss = 1.541854\n",
      "2024-06-14 05:19:20.085000: I runner.py:310] Step = 73000 ; steps/s = 1.66, tokens/s = 44176 (44176 target) ; Learning rate = 0.000327 ; Loss = 1.537164\n",
      "2024-06-14 05:20:20.788000: I runner.py:310] Step = 73100 ; steps/s = 1.65, tokens/s = 44803 (44803 target) ; Learning rate = 0.000327 ; Loss = 1.534536\n",
      "2024-06-14 05:21:21.559000: I runner.py:310] Step = 73200 ; steps/s = 1.65, tokens/s = 44718 (44718 target) ; Learning rate = 0.000327 ; Loss = 1.537197\n",
      "2024-06-14 05:22:22.267000: I runner.py:310] Step = 73300 ; steps/s = 1.65, tokens/s = 44733 (44733 target) ; Learning rate = 0.000326 ; Loss = 1.543462\n",
      "2024-06-14 05:23:22.627000: I runner.py:310] Step = 73400 ; steps/s = 1.66, tokens/s = 44191 (44191 target) ; Learning rate = 0.000326 ; Loss = 1.547143\n",
      "2024-06-14 05:24:23.344000: I runner.py:310] Step = 73500 ; steps/s = 1.65, tokens/s = 44740 (44740 target) ; Learning rate = 0.000326 ; Loss = 1.538615\n",
      "2024-06-14 05:25:24.120000: I runner.py:310] Step = 73600 ; steps/s = 1.65, tokens/s = 44709 (44709 target) ; Learning rate = 0.000326 ; Loss = 1.542338\n",
      "2024-06-14 05:26:24.844000: I runner.py:310] Step = 73700 ; steps/s = 1.65, tokens/s = 44746 (44746 target) ; Learning rate = 0.000326 ; Loss = 1.533986\n",
      "2024-06-14 05:27:25.153000: I runner.py:310] Step = 73800 ; steps/s = 1.66, tokens/s = 44209 (44209 target) ; Learning rate = 0.000325 ; Loss = 1.543251\n",
      "2024-06-14 05:28:25.957000: I runner.py:310] Step = 73900 ; steps/s = 1.64, tokens/s = 44699 (44699 target) ; Learning rate = 0.000325 ; Loss = 1.538372\n",
      "2024-06-14 05:29:26.769000: I runner.py:310] Step = 74000 ; steps/s = 1.64, tokens/s = 44676 (44676 target) ; Learning rate = 0.000325 ; Loss = 1.538052\n",
      "2024-06-14 05:30:27.556000: I runner.py:310] Step = 74100 ; steps/s = 1.65, tokens/s = 44719 (44719 target) ; Learning rate = 0.000325 ; Loss = 1.533155\n",
      "2024-06-14 05:31:27.856000: I runner.py:310] Step = 74200 ; steps/s = 1.66, tokens/s = 44213 (44213 target) ; Learning rate = 0.000324 ; Loss = 1.550881\n",
      "2024-06-14 05:32:28.583000: I runner.py:310] Step = 74300 ; steps/s = 1.65, tokens/s = 44753 (44753 target) ; Learning rate = 0.000324 ; Loss = 1.537867\n",
      "2024-06-14 05:33:29.334000: I runner.py:310] Step = 74400 ; steps/s = 1.65, tokens/s = 44734 (44734 target) ; Learning rate = 0.000324 ; Loss = 1.538946\n",
      "2024-06-14 05:34:30.106000: I runner.py:310] Step = 74500 ; steps/s = 1.65, tokens/s = 44739 (44739 target) ; Learning rate = 0.000324 ; Loss = 1.534207\n",
      "2024-06-14 05:35:30.397000: I runner.py:310] Step = 74600 ; steps/s = 1.66, tokens/s = 44217 (44217 target) ; Learning rate = 0.000324 ; Loss = 1.548725\n",
      "2024-06-14 05:36:31.133000: I runner.py:310] Step = 74700 ; steps/s = 1.65, tokens/s = 44741 (44741 target) ; Learning rate = 0.000323 ; Loss = 1.538095\n",
      "2024-06-14 05:37:31.843000: I runner.py:310] Step = 74800 ; steps/s = 1.65, tokens/s = 44749 (44749 target) ; Learning rate = 0.000323 ; Loss = 1.537213\n",
      "2024-06-14 05:38:32.624000: I runner.py:310] Step = 74900 ; steps/s = 1.65, tokens/s = 44723 (44723 target) ; Learning rate = 0.000323 ; Loss = 1.539611\n",
      "2024-06-14 05:39:32.973000: I runner.py:310] Step = 75000 ; steps/s = 1.66, tokens/s = 44161 (44161 target) ; Learning rate = 0.000323 ; Loss = 1.534738\n",
      "2024-06-14 05:39:32.975000: I training.py:192] Running evaluation for step 75000\n",
      "2024-06-14 05:43:37.627000: I training.py:192] Evaluation result for step 75000: loss = 1.363533 ; perplexity = 3.909985\n",
      "2024-06-14 05:44:38.178000: I runner.py:310] Step = 75100 ; steps/s = 1.65, tokens/s = 44873 (44873 target) ; Learning rate = 0.000323 ; Loss = 1.541294\n",
      "2024-06-14 05:45:38.934000: I runner.py:310] Step = 75200 ; steps/s = 1.65, tokens/s = 44718 (44718 target) ; Learning rate = 0.000322 ; Loss = 1.547149\n",
      "2024-06-14 05:46:39.256000: I runner.py:310] Step = 75300 ; steps/s = 1.66, tokens/s = 44236 (44236 target) ; Learning rate = 0.000322 ; Loss = 1.544275\n",
      "2024-06-14 05:47:40.021000: I runner.py:310] Step = 75400 ; steps/s = 1.65, tokens/s = 44751 (44751 target) ; Learning rate = 0.000322 ; Loss = 1.543709\n",
      "2024-06-14 05:48:40.790000: I runner.py:310] Step = 75500 ; steps/s = 1.65, tokens/s = 44727 (44727 target) ; Learning rate = 0.000322 ; Loss = 1.533917\n",
      "2024-06-14 05:49:41.526000: I runner.py:310] Step = 75600 ; steps/s = 1.65, tokens/s = 44719 (44719 target) ; Learning rate = 0.000321 ; Loss = 1.537758\n",
      "2024-06-14 05:50:41.798000: I runner.py:310] Step = 75700 ; steps/s = 1.66, tokens/s = 44226 (44226 target) ; Learning rate = 0.000321 ; Loss = 1.534991\n",
      "2024-06-14 05:51:42.496000: I runner.py:310] Step = 75800 ; steps/s = 1.65, tokens/s = 44801 (44801 target) ; Learning rate = 0.000321 ; Loss = 1.529576\n",
      "2024-06-14 05:52:43.309000: I runner.py:310] Step = 75900 ; steps/s = 1.64, tokens/s = 44672 (44672 target) ; Learning rate = 0.000321 ; Loss = 1.538282\n",
      "2024-06-14 05:53:44.068000: I runner.py:310] Step = 76000 ; steps/s = 1.65, tokens/s = 44715 (44715 target) ; Learning rate = 0.000321 ; Loss = 1.544703\n",
      "2024-06-14 05:54:44.405000: I runner.py:310] Step = 76100 ; steps/s = 1.66, tokens/s = 44204 (44204 target) ; Learning rate = 0.000320 ; Loss = 1.547536\n",
      "2024-06-14 05:55:45.140000: I runner.py:310] Step = 76200 ; steps/s = 1.65, tokens/s = 44758 (44758 target) ; Learning rate = 0.000320 ; Loss = 1.536107\n",
      "2024-06-14 05:56:45.901000: I runner.py:310] Step = 76300 ; steps/s = 1.65, tokens/s = 44708 (44708 target) ; Learning rate = 0.000320 ; Loss = 1.527864\n",
      "2024-06-14 05:57:46.595000: I runner.py:310] Step = 76400 ; steps/s = 1.65, tokens/s = 44760 (44760 target) ; Learning rate = 0.000320 ; Loss = 1.538464\n",
      "2024-06-14 05:58:46.913000: I runner.py:310] Step = 76500 ; steps/s = 1.66, tokens/s = 44237 (44237 target) ; Learning rate = 0.000320 ; Loss = 1.550644\n",
      "2024-06-14 05:59:47.626000: I runner.py:310] Step = 76600 ; steps/s = 1.65, tokens/s = 44743 (44743 target) ; Learning rate = 0.000319 ; Loss = 1.538670\n",
      "2024-06-14 06:00:48.342000: I runner.py:310] Step = 76700 ; steps/s = 1.65, tokens/s = 44743 (44743 target) ; Learning rate = 0.000319 ; Loss = 1.537813\n",
      "2024-06-14 06:01:49.102000: I runner.py:310] Step = 76800 ; steps/s = 1.65, tokens/s = 44721 (44721 target) ; Learning rate = 0.000319 ; Loss = 1.537909\n",
      "2024-06-14 06:02:49.404000: I runner.py:310] Step = 76900 ; steps/s = 1.66, tokens/s = 44237 (44237 target) ; Learning rate = 0.000319 ; Loss = 1.544339\n",
      "2024-06-14 06:03:50.089000: I runner.py:310] Step = 77000 ; steps/s = 1.65, tokens/s = 44783 (44783 target) ; Learning rate = 0.000319 ; Loss = 1.535160\n",
      "2024-06-14 06:04:50.851000: I runner.py:310] Step = 77100 ; steps/s = 1.65, tokens/s = 44709 (44709 target) ; Learning rate = 0.000318 ; Loss = 1.533523\n",
      "2024-06-14 06:05:51.545000: I runner.py:310] Step = 77200 ; steps/s = 1.65, tokens/s = 44785 (44785 target) ; Learning rate = 0.000318 ; Loss = 1.533011\n",
      "2024-06-14 06:06:51.865000: I runner.py:310] Step = 77300 ; steps/s = 1.66, tokens/s = 44190 (44190 target) ; Learning rate = 0.000318 ; Loss = 1.540713\n",
      "2024-06-14 06:07:52.519000: I runner.py:310] Step = 77400 ; steps/s = 1.65, tokens/s = 44805 (44805 target) ; Learning rate = 0.000318 ; Loss = 1.529366\n",
      "2024-06-14 06:08:53.267000: I runner.py:310] Step = 77500 ; steps/s = 1.65, tokens/s = 44741 (44741 target) ; Learning rate = 0.000317 ; Loss = 1.529512\n",
      "2024-06-14 06:09:53.530000: I runner.py:310] Step = 77600 ; steps/s = 1.66, tokens/s = 44245 (44245 target) ; Learning rate = 0.000317 ; Loss = 1.531788\n",
      "2024-06-14 06:10:54.284000: I runner.py:310] Step = 77700 ; steps/s = 1.65, tokens/s = 44731 (44731 target) ; Learning rate = 0.000317 ; Loss = 1.537057\n",
      "2024-06-14 06:11:55.034000: I runner.py:310] Step = 77800 ; steps/s = 1.65, tokens/s = 44743 (44743 target) ; Learning rate = 0.000317 ; Loss = 1.538995\n",
      "2024-06-14 06:12:55.770000: I runner.py:310] Step = 77900 ; steps/s = 1.65, tokens/s = 44736 (44736 target) ; Learning rate = 0.000317 ; Loss = 1.538028\n",
      "2024-06-14 06:13:56.003000: I runner.py:310] Step = 78000 ; steps/s = 1.66, tokens/s = 44280 (44280 target) ; Learning rate = 0.000316 ; Loss = 1.533291\n",
      "2024-06-14 06:14:56.717000: I runner.py:310] Step = 78100 ; steps/s = 1.65, tokens/s = 44801 (44801 target) ; Learning rate = 0.000316 ; Loss = 1.535122\n",
      "2024-06-14 06:15:57.410000: I runner.py:310] Step = 78200 ; steps/s = 1.65, tokens/s = 44730 (44730 target) ; Learning rate = 0.000316 ; Loss = 1.542998\n",
      "2024-06-14 06:16:58.226000: I runner.py:310] Step = 78300 ; steps/s = 1.64, tokens/s = 44681 (44681 target) ; Learning rate = 0.000316 ; Loss = 1.530491\n",
      "2024-06-14 06:17:58.519000: I runner.py:310] Step = 78400 ; steps/s = 1.66, tokens/s = 44221 (44221 target) ; Learning rate = 0.000316 ; Loss = 1.539496\n",
      "2024-06-14 06:18:59.316000: I runner.py:310] Step = 78500 ; steps/s = 1.64, tokens/s = 44711 (44711 target) ; Learning rate = 0.000315 ; Loss = 1.534685\n",
      "2024-06-14 06:20:00.050000: I runner.py:310] Step = 78600 ; steps/s = 1.65, tokens/s = 44740 (44740 target) ; Learning rate = 0.000315 ; Loss = 1.530970\n",
      "2024-06-14 06:21:00.795000: I runner.py:310] Step = 78700 ; steps/s = 1.65, tokens/s = 44742 (44742 target) ; Learning rate = 0.000315 ; Loss = 1.537251\n",
      "2024-06-14 06:22:01.124000: I runner.py:310] Step = 78800 ; steps/s = 1.66, tokens/s = 44172 (44172 target) ; Learning rate = 0.000315 ; Loss = 1.525398\n",
      "2024-06-14 06:23:01.808000: I runner.py:310] Step = 78900 ; steps/s = 1.65, tokens/s = 44779 (44779 target) ; Learning rate = 0.000315 ; Loss = 1.533524\n",
      "2024-06-14 06:24:02.541000: I runner.py:310] Step = 79000 ; steps/s = 1.65, tokens/s = 44777 (44777 target) ; Learning rate = 0.000314 ; Loss = 1.534006\n",
      "2024-06-14 06:25:03.277000: I runner.py:310] Step = 79100 ; steps/s = 1.65, tokens/s = 44713 (44713 target) ; Learning rate = 0.000314 ; Loss = 1.540200\n",
      "2024-06-14 06:26:03.609000: I runner.py:310] Step = 79200 ; steps/s = 1.66, tokens/s = 44235 (44235 target) ; Learning rate = 0.000314 ; Loss = 1.542396\n",
      "2024-06-14 06:27:04.414000: I runner.py:310] Step = 79300 ; steps/s = 1.64, tokens/s = 44698 (44698 target) ; Learning rate = 0.000314 ; Loss = 1.537646\n",
      "2024-06-14 06:28:05.198000: I runner.py:310] Step = 79400 ; steps/s = 1.65, tokens/s = 44685 (44685 target) ; Learning rate = 0.000314 ; Loss = 1.525907\n",
      "2024-06-14 06:29:05.900000: I runner.py:310] Step = 79500 ; steps/s = 1.65, tokens/s = 44761 (44761 target) ; Learning rate = 0.000313 ; Loss = 1.531878\n",
      "2024-06-14 06:30:06.268000: I runner.py:310] Step = 79600 ; steps/s = 1.66, tokens/s = 44157 (44157 target) ; Learning rate = 0.000313 ; Loss = 1.541648\n",
      "2024-06-14 06:31:07.043000: I runner.py:310] Step = 79700 ; steps/s = 1.65, tokens/s = 44717 (44717 target) ; Learning rate = 0.000313 ; Loss = 1.525065\n",
      "2024-06-14 06:32:07.745000: I runner.py:310] Step = 79800 ; steps/s = 1.65, tokens/s = 44769 (44769 target) ; Learning rate = 0.000313 ; Loss = 1.532968\n",
      "2024-06-14 06:33:08.283000: I runner.py:310] Step = 79900 ; steps/s = 1.65, tokens/s = 44516 (44516 target) ; Learning rate = 0.000313 ; Loss = 1.523715\n",
      "2024-06-14 06:34:08.789000: I runner.py:310] Step = 80000 ; steps/s = 1.65, tokens/s = 44466 (44466 target) ; Learning rate = 0.000312 ; Loss = 1.527069\n",
      "2024-06-14 06:34:10.898000: I training.py:176] Saved checkpoint RoBERTa_POS-KK-EN/ckpt-80000\n",
      "2024-06-14 06:34:10.898000: I training.py:192] Running evaluation for step 80000\n",
      "2024-06-14 06:38:13.437000: I training.py:192] Evaluation result for step 80000: loss = 1.370518 ; perplexity = 3.937389\n",
      "2024-06-14 06:39:13.993000: I runner.py:310] Step = 80100 ; steps/s = 1.65, tokens/s = 44889 (44889 target) ; Learning rate = 0.000312 ; Loss = 1.533547\n",
      "2024-06-14 06:40:14.803000: I runner.py:310] Step = 80200 ; steps/s = 1.64, tokens/s = 44678 (44678 target) ; Learning rate = 0.000312 ; Loss = 1.531842\n",
      "2024-06-14 06:41:15.082000: I runner.py:310] Step = 80300 ; steps/s = 1.66, tokens/s = 44228 (44228 target) ; Learning rate = 0.000312 ; Loss = 1.543996\n",
      "2024-06-14 06:42:15.858000: I runner.py:310] Step = 80400 ; steps/s = 1.65, tokens/s = 44731 (44731 target) ; Learning rate = 0.000312 ; Loss = 1.532806\n",
      "2024-06-14 06:43:16.637000: I runner.py:310] Step = 80500 ; steps/s = 1.65, tokens/s = 44688 (44688 target) ; Learning rate = 0.000312 ; Loss = 1.537395\n",
      "2024-06-14 06:44:17.362000: I runner.py:310] Step = 80600 ; steps/s = 1.65, tokens/s = 44744 (44744 target) ; Learning rate = 0.000311 ; Loss = 1.534025\n",
      "2024-06-14 06:45:17.682000: I runner.py:310] Step = 80700 ; steps/s = 1.66, tokens/s = 44216 (44216 target) ; Learning rate = 0.000311 ; Loss = 1.535733\n",
      "2024-06-14 06:46:18.446000: I runner.py:310] Step = 80800 ; steps/s = 1.65, tokens/s = 44735 (44735 target) ; Learning rate = 0.000311 ; Loss = 1.524161\n",
      "2024-06-14 06:47:19.144000: I runner.py:310] Step = 80900 ; steps/s = 1.65, tokens/s = 44749 (44749 target) ; Learning rate = 0.000311 ; Loss = 1.532731\n",
      "2024-06-14 06:48:19.925000: I runner.py:310] Step = 81000 ; steps/s = 1.65, tokens/s = 44711 (44711 target) ; Learning rate = 0.000311 ; Loss = 1.531592\n",
      "2024-06-14 06:49:20.172000: I runner.py:310] Step = 81100 ; steps/s = 1.66, tokens/s = 44251 (44251 target) ; Learning rate = 0.000310 ; Loss = 1.524522\n",
      "2024-06-14 06:50:20.843000: I runner.py:310] Step = 81200 ; steps/s = 1.65, tokens/s = 44807 (44807 target) ; Learning rate = 0.000310 ; Loss = 1.530193\n",
      "2024-06-14 06:51:21.572000: I runner.py:310] Step = 81300 ; steps/s = 1.65, tokens/s = 44765 (44765 target) ; Learning rate = 0.000310 ; Loss = 1.541944\n",
      "2024-06-14 06:52:22.344000: I runner.py:310] Step = 81400 ; steps/s = 1.65, tokens/s = 44712 (44712 target) ; Learning rate = 0.000310 ; Loss = 1.536832\n",
      "2024-06-14 06:53:22.633000: I runner.py:310] Step = 81500 ; steps/s = 1.66, tokens/s = 44203 (44203 target) ; Learning rate = 0.000310 ; Loss = 1.539732\n",
      "2024-06-14 06:54:23.410000: I runner.py:310] Step = 81600 ; steps/s = 1.65, tokens/s = 44733 (44733 target) ; Learning rate = 0.000309 ; Loss = 1.527822\n",
      "2024-06-14 06:55:24.152000: I runner.py:310] Step = 81700 ; steps/s = 1.65, tokens/s = 44752 (44752 target) ; Learning rate = 0.000309 ; Loss = 1.533894\n",
      "2024-06-14 06:56:24.927000: I runner.py:310] Step = 81800 ; steps/s = 1.65, tokens/s = 44707 (44707 target) ; Learning rate = 0.000309 ; Loss = 1.533102\n",
      "2024-06-14 06:57:25.182000: I runner.py:310] Step = 81900 ; steps/s = 1.66, tokens/s = 44256 (44256 target) ; Learning rate = 0.000309 ; Loss = 1.527394\n",
      "2024-06-14 06:58:26.010000: I runner.py:310] Step = 82000 ; steps/s = 1.64, tokens/s = 44689 (44689 target) ; Learning rate = 0.000309 ; Loss = 1.532149\n",
      "2024-06-14 06:59:26.773000: I runner.py:310] Step = 82100 ; steps/s = 1.65, tokens/s = 44715 (44715 target) ; Learning rate = 0.000308 ; Loss = 1.537275\n",
      "2024-06-14 07:00:27.570000: I runner.py:310] Step = 82200 ; steps/s = 1.64, tokens/s = 44678 (44678 target) ; Learning rate = 0.000308 ; Loss = 1.531887\n",
      "2024-06-14 07:01:27.863000: I runner.py:310] Step = 82300 ; steps/s = 1.66, tokens/s = 44211 (44211 target) ; Learning rate = 0.000308 ; Loss = 1.528649\n",
      "2024-06-14 07:02:28.625000: I runner.py:310] Step = 82400 ; steps/s = 1.65, tokens/s = 44737 (44737 target) ; Learning rate = 0.000308 ; Loss = 1.529141\n",
      "2024-06-14 07:03:29.417000: I runner.py:310] Step = 82500 ; steps/s = 1.65, tokens/s = 44684 (44684 target) ; Learning rate = 0.000308 ; Loss = 1.529017\n",
      "2024-06-14 07:04:29.695000: I runner.py:310] Step = 82600 ; steps/s = 1.66, tokens/s = 44250 (44250 target) ; Learning rate = 0.000308 ; Loss = 1.531437\n",
      "2024-06-14 07:05:30.407000: I runner.py:310] Step = 82700 ; steps/s = 1.65, tokens/s = 44750 (44750 target) ; Learning rate = 0.000307 ; Loss = 1.534303\n",
      "2024-06-14 07:06:31.094000: I runner.py:310] Step = 82800 ; steps/s = 1.65, tokens/s = 44808 (44808 target) ; Learning rate = 0.000307 ; Loss = 1.522642\n",
      "2024-06-14 07:07:31.848000: I runner.py:310] Step = 82900 ; steps/s = 1.65, tokens/s = 44724 (44724 target) ; Learning rate = 0.000307 ; Loss = 1.526818\n",
      "2024-06-14 07:08:32.083000: I runner.py:310] Step = 83000 ; steps/s = 1.66, tokens/s = 44254 (44254 target) ; Learning rate = 0.000307 ; Loss = 1.526328\n",
      "2024-06-14 07:09:32.869000: I runner.py:310] Step = 83100 ; steps/s = 1.65, tokens/s = 44705 (44705 target) ; Learning rate = 0.000307 ; Loss = 1.527790\n",
      "2024-06-14 07:10:33.626000: I runner.py:310] Step = 83200 ; steps/s = 1.65, tokens/s = 44727 (44727 target) ; Learning rate = 0.000306 ; Loss = 1.535393\n",
      "2024-06-14 07:11:34.352000: I runner.py:310] Step = 83300 ; steps/s = 1.65, tokens/s = 44756 (44756 target) ; Learning rate = 0.000306 ; Loss = 1.534511\n",
      "2024-06-14 07:12:34.593000: I runner.py:310] Step = 83400 ; steps/s = 1.66, tokens/s = 44271 (44271 target) ; Learning rate = 0.000306 ; Loss = 1.526876\n",
      "2024-06-14 07:13:35.368000: I runner.py:310] Step = 83500 ; steps/s = 1.65, tokens/s = 44715 (44715 target) ; Learning rate = 0.000306 ; Loss = 1.537742\n",
      "2024-06-14 07:14:36.118000: I runner.py:310] Step = 83600 ; steps/s = 1.65, tokens/s = 44730 (44730 target) ; Learning rate = 0.000306 ; Loss = 1.531356\n",
      "2024-06-14 07:15:36.814000: I runner.py:310] Step = 83700 ; steps/s = 1.65, tokens/s = 44760 (44760 target) ; Learning rate = 0.000306 ; Loss = 1.531667\n",
      "2024-06-14 07:16:37.031000: I runner.py:310] Step = 83800 ; steps/s = 1.66, tokens/s = 44297 (44297 target) ; Learning rate = 0.000305 ; Loss = 1.533844\n",
      "2024-06-14 07:17:37.778000: I runner.py:310] Step = 83900 ; steps/s = 1.65, tokens/s = 44751 (44751 target) ; Learning rate = 0.000305 ; Loss = 1.525534\n",
      "2024-06-14 07:18:38.572000: I runner.py:310] Step = 84000 ; steps/s = 1.65, tokens/s = 44681 (44681 target) ; Learning rate = 0.000305 ; Loss = 1.531913\n",
      "2024-06-14 07:19:39.316000: I runner.py:310] Step = 84100 ; steps/s = 1.65, tokens/s = 44734 (44734 target) ; Learning rate = 0.000305 ; Loss = 1.531785\n",
      "2024-06-14 07:20:39.580000: I runner.py:310] Step = 84200 ; steps/s = 1.66, tokens/s = 44232 (44232 target) ; Learning rate = 0.000305 ; Loss = 1.523925\n",
      "2024-06-14 07:21:40.395000: I runner.py:310] Step = 84300 ; steps/s = 1.64, tokens/s = 44685 (44685 target) ; Learning rate = 0.000304 ; Loss = 1.531629\n",
      "2024-06-14 07:22:41.179000: I runner.py:310] Step = 84400 ; steps/s = 1.65, tokens/s = 44705 (44705 target) ; Learning rate = 0.000304 ; Loss = 1.535680\n",
      "2024-06-14 07:23:41.937000: I runner.py:310] Step = 84500 ; steps/s = 1.65, tokens/s = 44731 (44731 target) ; Learning rate = 0.000304 ; Loss = 1.537999\n",
      "2024-06-14 07:24:42.239000: I runner.py:310] Step = 84600 ; steps/s = 1.66, tokens/s = 44244 (44244 target) ; Learning rate = 0.000304 ; Loss = 1.526509\n",
      "2024-06-14 07:25:42.932000: I runner.py:310] Step = 84700 ; steps/s = 1.65, tokens/s = 44790 (44790 target) ; Learning rate = 0.000304 ; Loss = 1.530071\n",
      "2024-06-14 07:26:43.694000: I runner.py:310] Step = 84800 ; steps/s = 1.65, tokens/s = 44692 (44692 target) ; Learning rate = 0.000304 ; Loss = 1.528067\n",
      "2024-06-14 07:27:44.046000: I runner.py:310] Step = 84900 ; steps/s = 1.66, tokens/s = 44174 (44174 target) ; Learning rate = 0.000303 ; Loss = 1.533022\n",
      "2024-06-14 07:28:44.855000: I runner.py:310] Step = 85000 ; steps/s = 1.64, tokens/s = 44698 (44698 target) ; Learning rate = 0.000303 ; Loss = 1.523858\n",
      "2024-06-14 07:28:44.857000: I training.py:192] Running evaluation for step 85000\n",
      "2024-06-14 07:32:55.033000: I training.py:192] Evaluation result for step 85000: loss = 1.375174 ; perplexity = 3.955766\n",
      "2024-06-14 07:33:55.593000: I runner.py:310] Step = 85100 ; steps/s = 1.65, tokens/s = 44892 (44892 target) ; Learning rate = 0.000303 ; Loss = 1.528814\n",
      "2024-06-14 07:34:56.301000: I runner.py:310] Step = 85200 ; steps/s = 1.65, tokens/s = 44751 (44751 target) ; Learning rate = 0.000303 ; Loss = 1.535851\n",
      "2024-06-14 07:35:56.588000: I runner.py:310] Step = 85300 ; steps/s = 1.66, tokens/s = 44242 (44242 target) ; Learning rate = 0.000303 ; Loss = 1.531778\n",
      "2024-06-14 07:36:57.318000: I runner.py:310] Step = 85400 ; steps/s = 1.65, tokens/s = 44743 (44743 target) ; Learning rate = 0.000302 ; Loss = 1.528411\n",
      "2024-06-14 07:37:58.091000: I runner.py:310] Step = 85500 ; steps/s = 1.65, tokens/s = 44720 (44720 target) ; Learning rate = 0.000302 ; Loss = 1.529294\n",
      "2024-06-14 07:38:58.857000: I runner.py:310] Step = 85600 ; steps/s = 1.65, tokens/s = 44699 (44699 target) ; Learning rate = 0.000302 ; Loss = 1.524302\n",
      "2024-06-14 07:39:59.133000: I runner.py:310] Step = 85700 ; steps/s = 1.66, tokens/s = 44256 (44256 target) ; Learning rate = 0.000302 ; Loss = 1.528065\n",
      "2024-06-14 07:40:59.867000: I runner.py:310] Step = 85800 ; steps/s = 1.65, tokens/s = 44756 (44756 target) ; Learning rate = 0.000302 ; Loss = 1.527921\n",
      "2024-06-14 07:42:00.610000: I runner.py:310] Step = 85900 ; steps/s = 1.65, tokens/s = 44733 (44733 target) ; Learning rate = 0.000302 ; Loss = 1.529230\n",
      "2024-06-14 07:43:01.391000: I runner.py:310] Step = 86000 ; steps/s = 1.65, tokens/s = 44703 (44703 target) ; Learning rate = 0.000301 ; Loss = 1.523776\n",
      "2024-06-14 07:44:01.772000: I runner.py:310] Step = 86100 ; steps/s = 1.66, tokens/s = 44165 (44165 target) ; Learning rate = 0.000301 ; Loss = 1.524269\n",
      "2024-06-14 07:45:02.549000: I runner.py:310] Step = 86200 ; steps/s = 1.65, tokens/s = 44729 (44729 target) ; Learning rate = 0.000301 ; Loss = 1.526697\n",
      "2024-06-14 07:46:03.328000: I runner.py:310] Step = 86300 ; steps/s = 1.65, tokens/s = 44711 (44711 target) ; Learning rate = 0.000301 ; Loss = 1.538585\n",
      "2024-06-14 07:47:04.124000: I runner.py:310] Step = 86400 ; steps/s = 1.65, tokens/s = 44684 (44684 target) ; Learning rate = 0.000301 ; Loss = 1.542553\n",
      "2024-06-14 07:48:04.465000: I runner.py:310] Step = 86500 ; steps/s = 1.66, tokens/s = 44199 (44199 target) ; Learning rate = 0.000301 ; Loss = 1.537907\n",
      "2024-06-14 07:49:05.186000: I runner.py:310] Step = 86600 ; steps/s = 1.65, tokens/s = 44750 (44750 target) ; Learning rate = 0.000300 ; Loss = 1.517093\n",
      "2024-06-14 07:50:05.968000: I runner.py:310] Step = 86700 ; steps/s = 1.65, tokens/s = 44697 (44697 target) ; Learning rate = 0.000300 ; Loss = 1.524924\n",
      "2024-06-14 07:51:06.740000: I runner.py:310] Step = 86800 ; steps/s = 1.65, tokens/s = 44706 (44706 target) ; Learning rate = 0.000300 ; Loss = 1.524331\n",
      "2024-06-14 07:52:07.062000: I runner.py:310] Step = 86900 ; steps/s = 1.66, tokens/s = 44199 (44199 target) ; Learning rate = 0.000300 ; Loss = 1.534480\n",
      "2024-06-14 07:53:07.812000: I runner.py:310] Step = 87000 ; steps/s = 1.65, tokens/s = 44751 (44751 target) ; Learning rate = 0.000300 ; Loss = 1.522597\n",
      "2024-06-14 07:54:08.552000: I runner.py:310] Step = 87100 ; steps/s = 1.65, tokens/s = 44748 (44748 target) ; Learning rate = 0.000299 ; Loss = 1.519829\n",
      "2024-06-14 07:55:08.939000: I runner.py:310] Step = 87200 ; steps/s = 1.66, tokens/s = 44244 (44244 target) ; Learning rate = 0.000299 ; Loss = 1.549904\n",
      "2024-06-14 07:56:09.593000: I runner.py:310] Step = 87300 ; steps/s = 1.65, tokens/s = 44713 (44713 target) ; Learning rate = 0.000299 ; Loss = 1.529513\n",
      "2024-06-14 07:57:10.391000: I runner.py:310] Step = 87400 ; steps/s = 1.64, tokens/s = 44710 (44710 target) ; Learning rate = 0.000299 ; Loss = 1.523391\n",
      "2024-06-14 07:58:11.175000: I runner.py:310] Step = 87500 ; steps/s = 1.65, tokens/s = 44695 (44695 target) ; Learning rate = 0.000299 ; Loss = 1.535052\n",
      "2024-06-14 07:59:11.483000: I runner.py:310] Step = 87600 ; steps/s = 1.66, tokens/s = 44209 (44209 target) ; Learning rate = 0.000299 ; Loss = 1.525416\n",
      "2024-06-14 08:00:12.206000: I runner.py:310] Step = 87700 ; steps/s = 1.65, tokens/s = 44751 (44751 target) ; Learning rate = 0.000298 ; Loss = 1.527098\n",
      "2024-06-14 08:01:12.926000: I runner.py:310] Step = 87800 ; steps/s = 1.65, tokens/s = 44755 (44755 target) ; Learning rate = 0.000298 ; Loss = 1.535416\n",
      "2024-06-14 08:02:13.705000: I runner.py:310] Step = 87900 ; steps/s = 1.65, tokens/s = 44709 (44709 target) ; Learning rate = 0.000298 ; Loss = 1.530657\n",
      "2024-06-14 08:03:14.059000: I runner.py:310] Step = 88000 ; steps/s = 1.66, tokens/s = 44188 (44188 target) ; Learning rate = 0.000298 ; Loss = 1.518237\n",
      "2024-06-14 08:04:14.806000: I runner.py:310] Step = 88100 ; steps/s = 1.65, tokens/s = 44736 (44736 target) ; Learning rate = 0.000298 ; Loss = 1.525608\n",
      "2024-06-14 08:05:15.568000: I runner.py:310] Step = 88200 ; steps/s = 1.65, tokens/s = 44704 (44704 target) ; Learning rate = 0.000298 ; Loss = 1.529509\n",
      "2024-06-14 08:06:16.349000: I runner.py:310] Step = 88300 ; steps/s = 1.65, tokens/s = 44723 (44723 target) ; Learning rate = 0.000297 ; Loss = 1.524373\n",
      "2024-06-14 08:07:16.605000: I runner.py:310] Step = 88400 ; steps/s = 1.66, tokens/s = 44247 (44247 target) ; Learning rate = 0.000297 ; Loss = 1.527871\n",
      "2024-06-14 08:08:17.353000: I runner.py:310] Step = 88500 ; steps/s = 1.65, tokens/s = 44739 (44739 target) ; Learning rate = 0.000297 ; Loss = 1.520739\n",
      "2024-06-14 08:09:18.089000: I runner.py:310] Step = 88600 ; steps/s = 1.65, tokens/s = 44741 (44741 target) ; Learning rate = 0.000297 ; Loss = 1.524657\n",
      "2024-06-14 08:10:18.816000: I runner.py:310] Step = 88700 ; steps/s = 1.65, tokens/s = 44750 (44750 target) ; Learning rate = 0.000297 ; Loss = 1.521168\n",
      "2024-06-14 08:11:19.121000: I runner.py:310] Step = 88800 ; steps/s = 1.66, tokens/s = 44221 (44221 target) ; Learning rate = 0.000297 ; Loss = 1.536927\n",
      "2024-06-14 08:12:19.880000: I runner.py:310] Step = 88900 ; steps/s = 1.65, tokens/s = 44733 (44733 target) ; Learning rate = 0.000296 ; Loss = 1.523786\n",
      "2024-06-14 08:13:20.647000: I runner.py:310] Step = 89000 ; steps/s = 1.65, tokens/s = 44711 (44711 target) ; Learning rate = 0.000296 ; Loss = 1.525127\n",
      "2024-06-14 08:14:21.420000: I runner.py:310] Step = 89100 ; steps/s = 1.65, tokens/s = 44702 (44702 target) ; Learning rate = 0.000296 ; Loss = 1.526926\n",
      "2024-06-14 08:15:21.701000: I runner.py:310] Step = 89200 ; steps/s = 1.66, tokens/s = 44253 (44253 target) ; Learning rate = 0.000296 ; Loss = 1.516392\n",
      "2024-06-14 08:16:22.438000: I runner.py:310] Step = 89300 ; steps/s = 1.65, tokens/s = 44735 (44735 target) ; Learning rate = 0.000296 ; Loss = 1.521499\n",
      "2024-06-14 08:17:23.197000: I runner.py:310] Step = 89400 ; steps/s = 1.65, tokens/s = 44714 (44714 target) ; Learning rate = 0.000296 ; Loss = 1.529940\n",
      "2024-06-14 08:18:24.014000: I runner.py:310] Step = 89500 ; steps/s = 1.64, tokens/s = 44692 (44692 target) ; Learning rate = 0.000295 ; Loss = 1.543955\n",
      "2024-06-14 08:19:24.282000: I runner.py:310] Step = 89600 ; steps/s = 1.66, tokens/s = 44267 (44267 target) ; Learning rate = 0.000295 ; Loss = 1.529556\n",
      "2024-06-14 08:20:24.990000: I runner.py:310] Step = 89700 ; steps/s = 1.65, tokens/s = 44746 (44746 target) ; Learning rate = 0.000295 ; Loss = 1.526155\n",
      "2024-06-14 08:21:25.686000: I runner.py:310] Step = 89800 ; steps/s = 1.65, tokens/s = 44780 (44780 target) ; Learning rate = 0.000295 ; Loss = 1.518305\n",
      "2024-06-14 08:22:26.003000: I runner.py:310] Step = 89900 ; steps/s = 1.66, tokens/s = 44185 (44185 target) ; Learning rate = 0.000295 ; Loss = 1.524146\n",
      "2024-06-14 08:23:26.787000: I runner.py:310] Step = 90000 ; steps/s = 1.65, tokens/s = 44744 (44744 target) ; Learning rate = 0.000295 ; Loss = 1.525165\n",
      "2024-06-14 08:23:29.027000: I training.py:176] Saved checkpoint RoBERTa_POS-KK-EN/ckpt-90000\n",
      "2024-06-14 08:23:29.028000: I training.py:192] Running evaluation for step 90000\n",
      "2024-06-14 08:27:33.142000: I training.py:192] Evaluation result for step 90000: loss = 1.379779 ; perplexity = 3.974024\n",
      "2024-06-14 08:28:33.803000: I runner.py:310] Step = 90100 ; steps/s = 1.65, tokens/s = 44825 (44825 target) ; Learning rate = 0.000294 ; Loss = 1.530951\n",
      "2024-06-14 08:29:34.527000: I runner.py:310] Step = 90200 ; steps/s = 1.65, tokens/s = 44734 (44734 target) ; Learning rate = 0.000294 ; Loss = 1.527361\n",
      "2024-06-14 08:30:34.950000: I runner.py:310] Step = 90300 ; steps/s = 1.66, tokens/s = 44098 (44098 target) ; Learning rate = 0.000294 ; Loss = 1.527479\n",
      "2024-06-14 08:31:35.762000: I runner.py:310] Step = 90400 ; steps/s = 1.64, tokens/s = 44671 (44671 target) ; Learning rate = 0.000294 ; Loss = 1.523847\n",
      "2024-06-14 08:32:36.552000: I runner.py:310] Step = 90500 ; steps/s = 1.65, tokens/s = 44701 (44701 target) ; Learning rate = 0.000294 ; Loss = 1.520611\n",
      "2024-06-14 08:33:37.361000: I runner.py:310] Step = 90600 ; steps/s = 1.64, tokens/s = 44705 (44705 target) ; Learning rate = 0.000294 ; Loss = 1.522258\n",
      "2024-06-14 08:34:37.719000: I runner.py:310] Step = 90700 ; steps/s = 1.66, tokens/s = 44173 (44173 target) ; Learning rate = 0.000293 ; Loss = 1.523004\n",
      "2024-06-14 08:35:38.507000: I runner.py:310] Step = 90800 ; steps/s = 1.65, tokens/s = 44734 (44734 target) ; Learning rate = 0.000293 ; Loss = 1.522388\n",
      "2024-06-14 08:36:39.276000: I runner.py:310] Step = 90900 ; steps/s = 1.65, tokens/s = 44717 (44717 target) ; Learning rate = 0.000293 ; Loss = 1.523776\n",
      "2024-06-14 08:37:39.987000: I runner.py:310] Step = 91000 ; steps/s = 1.65, tokens/s = 44739 (44739 target) ; Learning rate = 0.000293 ; Loss = 1.528568\n",
      "2024-06-14 08:38:40.321000: I runner.py:310] Step = 91100 ; steps/s = 1.66, tokens/s = 44218 (44218 target) ; Learning rate = 0.000293 ; Loss = 1.533759\n",
      "2024-06-14 08:39:41.077000: I runner.py:310] Step = 91200 ; steps/s = 1.65, tokens/s = 44722 (44722 target) ; Learning rate = 0.000293 ; Loss = 1.521479\n",
      "2024-06-14 08:40:41.879000: I runner.py:310] Step = 91300 ; steps/s = 1.64, tokens/s = 44666 (44666 target) ; Learning rate = 0.000293 ; Loss = 1.524201\n",
      "2024-06-14 08:41:42.651000: I runner.py:310] Step = 91400 ; steps/s = 1.65, tokens/s = 44737 (44737 target) ; Learning rate = 0.000292 ; Loss = 1.525908\n",
      "2024-06-14 08:42:42.910000: I runner.py:310] Step = 91500 ; steps/s = 1.66, tokens/s = 44229 (44229 target) ; Learning rate = 0.000292 ; Loss = 1.521088\n",
      "2024-06-14 08:43:43.651000: I runner.py:310] Step = 91600 ; steps/s = 1.65, tokens/s = 44741 (44741 target) ; Learning rate = 0.000292 ; Loss = 1.527178\n",
      "2024-06-14 08:44:44.437000: I runner.py:310] Step = 91700 ; steps/s = 1.65, tokens/s = 44721 (44721 target) ; Learning rate = 0.000292 ; Loss = 1.525733\n",
      "2024-06-14 08:45:45.152000: I runner.py:310] Step = 91800 ; steps/s = 1.65, tokens/s = 44751 (44751 target) ; Learning rate = 0.000292 ; Loss = 1.526070\n",
      "2024-06-14 08:46:45.448000: I runner.py:310] Step = 91900 ; steps/s = 1.66, tokens/s = 44239 (44239 target) ; Learning rate = 0.000292 ; Loss = 1.516082\n",
      "2024-06-14 08:47:46.151000: I runner.py:310] Step = 92000 ; steps/s = 1.65, tokens/s = 44744 (44744 target) ; Learning rate = 0.000291 ; Loss = 1.521633\n",
      "2024-06-14 08:48:46.894000: I runner.py:310] Step = 92100 ; steps/s = 1.65, tokens/s = 44753 (44753 target) ; Learning rate = 0.000291 ; Loss = 1.527949\n",
      "2024-06-14 08:49:47.230000: I runner.py:310] Step = 92200 ; steps/s = 1.66, tokens/s = 44181 (44181 target) ; Learning rate = 0.000291 ; Loss = 1.523905\n",
      "2024-06-14 08:50:47.954000: I runner.py:310] Step = 92300 ; steps/s = 1.65, tokens/s = 44751 (44751 target) ; Learning rate = 0.000291 ; Loss = 1.523772\n",
      "2024-06-14 08:51:48.730000: I runner.py:310] Step = 92400 ; steps/s = 1.65, tokens/s = 44716 (44716 target) ; Learning rate = 0.000291 ; Loss = 1.526057\n",
      "2024-06-14 08:52:49.482000: I runner.py:310] Step = 92500 ; steps/s = 1.65, tokens/s = 44742 (44742 target) ; Learning rate = 0.000291 ; Loss = 1.525807\n",
      "2024-06-14 08:53:49.781000: I runner.py:310] Step = 92600 ; steps/s = 1.66, tokens/s = 44213 (44213 target) ; Learning rate = 0.000290 ; Loss = 1.529636\n",
      "2024-06-14 08:54:50.541000: I runner.py:310] Step = 92700 ; steps/s = 1.65, tokens/s = 44719 (44719 target) ; Learning rate = 0.000290 ; Loss = 1.531704\n",
      "2024-06-14 08:55:51.271000: I runner.py:310] Step = 92800 ; steps/s = 1.65, tokens/s = 44743 (44743 target) ; Learning rate = 0.000290 ; Loss = 1.518669\n",
      "2024-06-14 08:56:52.023000: I runner.py:310] Step = 92900 ; steps/s = 1.65, tokens/s = 44747 (44747 target) ; Learning rate = 0.000290 ; Loss = 1.530188\n",
      "2024-06-14 08:57:52.309000: I runner.py:310] Step = 93000 ; steps/s = 1.66, tokens/s = 44224 (44224 target) ; Learning rate = 0.000290 ; Loss = 1.521886\n",
      "2024-06-14 08:58:53.056000: I runner.py:310] Step = 93100 ; steps/s = 1.65, tokens/s = 44726 (44726 target) ; Learning rate = 0.000290 ; Loss = 1.524591\n",
      "2024-06-14 08:59:53.805000: I runner.py:310] Step = 93200 ; steps/s = 1.65, tokens/s = 44743 (44743 target) ; Learning rate = 0.000290 ; Loss = 1.523772\n",
      "2024-06-14 09:00:54.572000: I runner.py:310] Step = 93300 ; steps/s = 1.65, tokens/s = 44720 (44720 target) ; Learning rate = 0.000289 ; Loss = 1.532196\n",
      "2024-06-14 09:01:54.867000: I runner.py:310] Step = 93400 ; steps/s = 1.66, tokens/s = 44236 (44236 target) ; Learning rate = 0.000289 ; Loss = 1.532304\n",
      "2024-06-14 09:02:55.655000: I runner.py:310] Step = 93500 ; steps/s = 1.65, tokens/s = 44715 (44715 target) ; Learning rate = 0.000289 ; Loss = 1.522051\n",
      "2024-06-14 09:03:56.398000: I runner.py:310] Step = 93600 ; steps/s = 1.65, tokens/s = 44732 (44732 target) ; Learning rate = 0.000289 ; Loss = 1.526351\n",
      "2024-06-14 09:04:57.070000: I runner.py:310] Step = 93700 ; steps/s = 1.65, tokens/s = 44782 (44782 target) ; Learning rate = 0.000289 ; Loss = 1.527073\n",
      "2024-06-14 09:05:57.346000: I runner.py:310] Step = 93800 ; steps/s = 1.66, tokens/s = 44251 (44251 target) ; Learning rate = 0.000289 ; Loss = 1.527401\n",
      "2024-06-14 09:06:58.080000: I runner.py:310] Step = 93900 ; steps/s = 1.65, tokens/s = 44741 (44741 target) ; Learning rate = 0.000288 ; Loss = 1.518184\n",
      "2024-06-14 09:07:58.852000: I runner.py:310] Step = 94000 ; steps/s = 1.65, tokens/s = 44700 (44700 target) ; Learning rate = 0.000288 ; Loss = 1.520088\n",
      "2024-06-14 09:08:59.648000: I runner.py:310] Step = 94100 ; steps/s = 1.64, tokens/s = 44701 (44701 target) ; Learning rate = 0.000288 ; Loss = 1.527004\n",
      "2024-06-14 09:09:59.878000: I runner.py:310] Step = 94200 ; steps/s = 1.66, tokens/s = 44274 (44274 target) ; Learning rate = 0.000288 ; Loss = 1.511268\n",
      "2024-06-14 09:11:00.696000: I runner.py:310] Step = 94300 ; steps/s = 1.64, tokens/s = 44682 (44682 target) ; Learning rate = 0.000288 ; Loss = 1.522125\n",
      "2024-06-14 09:12:01.454000: I runner.py:310] Step = 94400 ; steps/s = 1.65, tokens/s = 44736 (44736 target) ; Learning rate = 0.000288 ; Loss = 1.523392\n",
      "2024-06-14 09:13:01.717000: I runner.py:310] Step = 94500 ; steps/s = 1.66, tokens/s = 44215 (44215 target) ; Learning rate = 0.000288 ; Loss = 1.521084\n",
      "2024-06-14 09:14:02.470000: I runner.py:310] Step = 94600 ; steps/s = 1.65, tokens/s = 44759 (44759 target) ; Learning rate = 0.000287 ; Loss = 1.516658\n",
      "2024-06-14 09:15:03.228000: I runner.py:310] Step = 94700 ; steps/s = 1.65, tokens/s = 44740 (44740 target) ; Learning rate = 0.000287 ; Loss = 1.518647\n",
      "2024-06-14 09:16:03.984000: I runner.py:310] Step = 94800 ; steps/s = 1.65, tokens/s = 44695 (44695 target) ; Learning rate = 0.000287 ; Loss = 1.526126\n",
      "2024-06-14 09:17:04.306000: I runner.py:310] Step = 94900 ; steps/s = 1.66, tokens/s = 44211 (44211 target) ; Learning rate = 0.000287 ; Loss = 1.524011\n",
      "2024-06-14 09:18:05.134000: I runner.py:310] Step = 95000 ; steps/s = 1.64, tokens/s = 44671 (44671 target) ; Learning rate = 0.000287 ; Loss = 1.522723\n",
      "2024-06-14 09:18:05.135000: I training.py:192] Running evaluation for step 95000\n",
      "2024-06-14 09:22:11.263000: I training.py:192] Evaluation result for step 95000: loss = 1.379293 ; perplexity = 3.972091\n",
      "2024-06-14 09:23:11.852000: I runner.py:310] Step = 95100 ; steps/s = 1.65, tokens/s = 44872 (44872 target) ; Learning rate = 0.000287 ; Loss = 1.522870\n",
      "2024-06-14 09:24:12.670000: I runner.py:310] Step = 95200 ; steps/s = 1.64, tokens/s = 44687 (44687 target) ; Learning rate = 0.000286 ; Loss = 1.530951\n",
      "2024-06-14 09:25:12.961000: I runner.py:310] Step = 95300 ; steps/s = 1.66, tokens/s = 44217 (44217 target) ; Learning rate = 0.000286 ; Loss = 1.523386\n",
      "2024-06-14 09:26:13.676000: I runner.py:310] Step = 95400 ; steps/s = 1.65, tokens/s = 44794 (44794 target) ; Learning rate = 0.000286 ; Loss = 1.517048\n",
      "2024-06-14 09:27:14.400000: I runner.py:310] Step = 95500 ; steps/s = 1.65, tokens/s = 44741 (44741 target) ; Learning rate = 0.000286 ; Loss = 1.525239\n",
      "2024-06-14 09:28:15.148000: I runner.py:310] Step = 95600 ; steps/s = 1.65, tokens/s = 44724 (44724 target) ; Learning rate = 0.000286 ; Loss = 1.523620\n",
      "2024-06-14 09:29:15.517000: I runner.py:310] Step = 95700 ; steps/s = 1.66, tokens/s = 44150 (44150 target) ; Learning rate = 0.000286 ; Loss = 1.517641\n",
      "2024-06-14 09:30:16.245000: I runner.py:310] Step = 95800 ; steps/s = 1.65, tokens/s = 44747 (44747 target) ; Learning rate = 0.000286 ; Loss = 1.516000\n",
      "2024-06-14 09:31:17.048000: I runner.py:310] Step = 95900 ; steps/s = 1.64, tokens/s = 44709 (44709 target) ; Learning rate = 0.000285 ; Loss = 1.527053\n",
      "2024-06-14 09:32:17.807000: I runner.py:310] Step = 96000 ; steps/s = 1.65, tokens/s = 44744 (44744 target) ; Learning rate = 0.000285 ; Loss = 1.522196\n",
      "2024-06-14 09:33:18.141000: I runner.py:310] Step = 96100 ; steps/s = 1.66, tokens/s = 44168 (44168 target) ; Learning rate = 0.000285 ; Loss = 1.529716\n",
      "2024-06-14 09:34:18.971000: I runner.py:310] Step = 96200 ; steps/s = 1.64, tokens/s = 44689 (44689 target) ; Learning rate = 0.000285 ; Loss = 1.516550\n",
      "2024-06-14 09:35:19.716000: I runner.py:310] Step = 96300 ; steps/s = 1.65, tokens/s = 44721 (44721 target) ; Learning rate = 0.000285 ; Loss = 1.522206\n",
      "2024-06-14 09:36:20.507000: I runner.py:310] Step = 96400 ; steps/s = 1.65, tokens/s = 44713 (44713 target) ; Learning rate = 0.000285 ; Loss = 1.527203\n",
      "2024-06-14 09:37:20.846000: I runner.py:310] Step = 96500 ; steps/s = 1.66, tokens/s = 44215 (44215 target) ; Learning rate = 0.000285 ; Loss = 1.527213\n",
      "2024-06-14 09:38:21.586000: I runner.py:310] Step = 96600 ; steps/s = 1.65, tokens/s = 44729 (44729 target) ; Learning rate = 0.000284 ; Loss = 1.517536\n",
      "2024-06-14 09:39:22.375000: I runner.py:310] Step = 96700 ; steps/s = 1.65, tokens/s = 44716 (44716 target) ; Learning rate = 0.000284 ; Loss = 1.517555\n",
      "2024-06-14 09:40:23.079000: I runner.py:310] Step = 96800 ; steps/s = 1.65, tokens/s = 44736 (44736 target) ; Learning rate = 0.000284 ; Loss = 1.522084\n",
      "2024-06-14 09:41:23.315000: I runner.py:310] Step = 96900 ; steps/s = 1.66, tokens/s = 44290 (44290 target) ; Learning rate = 0.000284 ; Loss = 1.522949\n",
      "2024-06-14 09:42:24.055000: I runner.py:310] Step = 97000 ; steps/s = 1.65, tokens/s = 44708 (44708 target) ; Learning rate = 0.000284 ; Loss = 1.521073\n",
      "2024-06-14 09:43:24.794000: I runner.py:310] Step = 97100 ; steps/s = 1.65, tokens/s = 44747 (44747 target) ; Learning rate = 0.000284 ; Loss = 1.521212\n",
      "2024-06-14 09:44:25.154000: I runner.py:310] Step = 97200 ; steps/s = 1.66, tokens/s = 44181 (44181 target) ; Learning rate = 0.000284 ; Loss = 1.529065\n",
      "2024-06-14 09:45:25.901000: I runner.py:310] Step = 97300 ; steps/s = 1.65, tokens/s = 44747 (44747 target) ; Learning rate = 0.000283 ; Loss = 1.520501\n",
      "2024-06-14 09:46:26.759000: I runner.py:310] Step = 97400 ; steps/s = 1.64, tokens/s = 44657 (44657 target) ; Learning rate = 0.000283 ; Loss = 1.523196\n",
      "2024-06-14 09:47:27.528000: I runner.py:310] Step = 97500 ; steps/s = 1.65, tokens/s = 44714 (44714 target) ; Learning rate = 0.000283 ; Loss = 1.524527\n",
      "2024-06-14 09:48:27.870000: I runner.py:310] Step = 97600 ; steps/s = 1.66, tokens/s = 44186 (44186 target) ; Learning rate = 0.000283 ; Loss = 1.521938\n",
      "2024-06-14 09:49:28.659000: I runner.py:310] Step = 97700 ; steps/s = 1.65, tokens/s = 44695 (44695 target) ; Learning rate = 0.000283 ; Loss = 1.512350\n",
      "2024-06-14 09:50:29.404000: I runner.py:310] Step = 97800 ; steps/s = 1.65, tokens/s = 44765 (44765 target) ; Learning rate = 0.000283 ; Loss = 1.524523\n",
      "2024-06-14 09:51:30.144000: I runner.py:310] Step = 97900 ; steps/s = 1.65, tokens/s = 44734 (44734 target) ; Learning rate = 0.000282 ; Loss = 1.528147\n",
      "2024-06-14 09:52:30.468000: I runner.py:310] Step = 98000 ; steps/s = 1.66, tokens/s = 44195 (44195 target) ; Learning rate = 0.000282 ; Loss = 1.515283\n",
      "2024-06-14 09:53:31.291000: I runner.py:310] Step = 98100 ; steps/s = 1.64, tokens/s = 44702 (44702 target) ; Learning rate = 0.000282 ; Loss = 1.524756\n",
      "2024-06-14 09:54:32.035000: I runner.py:310] Step = 98200 ; steps/s = 1.65, tokens/s = 44735 (44735 target) ; Learning rate = 0.000282 ; Loss = 1.518344\n",
      "2024-06-14 09:55:32.843000: I runner.py:310] Step = 98300 ; steps/s = 1.64, tokens/s = 44664 (44664 target) ; Learning rate = 0.000282 ; Loss = 1.527463\n",
      "2024-06-14 09:56:33.185000: I runner.py:310] Step = 98400 ; steps/s = 1.66, tokens/s = 44163 (44163 target) ; Learning rate = 0.000282 ; Loss = 1.526301\n",
      "2024-06-14 09:57:33.962000: I runner.py:310] Step = 98500 ; steps/s = 1.65, tokens/s = 44726 (44726 target) ; Learning rate = 0.000282 ; Loss = 1.518754\n",
      "2024-06-14 09:58:34.680000: I runner.py:310] Step = 98600 ; steps/s = 1.65, tokens/s = 44742 (44742 target) ; Learning rate = 0.000281 ; Loss = 1.519728\n",
      "2024-06-14 09:59:35.471000: I runner.py:310] Step = 98700 ; steps/s = 1.65, tokens/s = 44706 (44706 target) ; Learning rate = 0.000281 ; Loss = 1.516584\n",
      "2024-06-14 10:00:35.796000: I runner.py:310] Step = 98800 ; steps/s = 1.66, tokens/s = 44192 (44192 target) ; Learning rate = 0.000281 ; Loss = 1.507577\n",
      "2024-06-14 10:01:36.569000: I runner.py:310] Step = 98900 ; steps/s = 1.65, tokens/s = 44717 (44717 target) ; Learning rate = 0.000281 ; Loss = 1.518233\n",
      "2024-06-14 10:02:37.320000: I runner.py:310] Step = 99000 ; steps/s = 1.65, tokens/s = 44756 (44756 target) ; Learning rate = 0.000281 ; Loss = 1.525951\n",
      "2024-06-14 10:03:38.093000: I runner.py:310] Step = 99100 ; steps/s = 1.65, tokens/s = 44708 (44708 target) ; Learning rate = 0.000281 ; Loss = 1.518720\n",
      "2024-06-14 10:04:38.349000: I runner.py:310] Step = 99200 ; steps/s = 1.66, tokens/s = 44253 (44253 target) ; Learning rate = 0.000281 ; Loss = 1.519939\n",
      "2024-06-14 10:05:39.162000: I runner.py:310] Step = 99300 ; steps/s = 1.64, tokens/s = 44701 (44701 target) ; Learning rate = 0.000280 ; Loss = 1.511327\n",
      "2024-06-14 10:06:39.885000: I runner.py:310] Step = 99400 ; steps/s = 1.65, tokens/s = 44766 (44766 target) ; Learning rate = 0.000280 ; Loss = 1.516620\n",
      "2024-06-14 10:07:40.226000: I runner.py:310] Step = 99500 ; steps/s = 1.66, tokens/s = 44156 (44156 target) ; Learning rate = 0.000280 ; Loss = 1.513550\n",
      "2024-06-14 10:08:41.018000: I runner.py:310] Step = 99600 ; steps/s = 1.65, tokens/s = 44687 (44687 target) ; Learning rate = 0.000280 ; Loss = 1.517883\n",
      "2024-06-14 10:09:41.776000: I runner.py:310] Step = 99700 ; steps/s = 1.65, tokens/s = 44730 (44730 target) ; Learning rate = 0.000280 ; Loss = 1.521293\n",
      "2024-06-14 10:10:42.566000: I runner.py:310] Step = 99800 ; steps/s = 1.65, tokens/s = 44726 (44726 target) ; Learning rate = 0.000280 ; Loss = 1.521051\n",
      "2024-06-14 10:11:42.879000: I runner.py:310] Step = 99900 ; steps/s = 1.66, tokens/s = 44221 (44221 target) ; Learning rate = 0.000280 ; Loss = 1.527828\n",
      "2024-06-14 10:12:43.676000: I runner.py:310] Step = 100000 ; steps/s = 1.64, tokens/s = 44723 (44723 target) ; Learning rate = 0.000280 ; Loss = 1.513145\n",
      "2024-06-14 10:12:45.909000: I training.py:176] Saved checkpoint RoBERTa_POS-KK-EN/ckpt-100000\n",
      "2024-06-14 10:12:45.909000: I training.py:192] Running evaluation for step 100000\n",
      "2024-06-14 10:16:49.808000: I training.py:192] Evaluation result for step 100000: loss = 1.386400 ; perplexity = 4.000422\n",
      "2024-06-14 10:17:50.332000: I runner.py:310] Step = 100100 ; steps/s = 1.65, tokens/s = 44899 (44899 target) ; Learning rate = 0.000279 ; Loss = 1.518251\n",
      "2024-06-14 10:18:51.052000: I runner.py:310] Step = 100200 ; steps/s = 1.65, tokens/s = 44721 (44721 target) ; Learning rate = 0.000279 ; Loss = 1.522934\n",
      "2024-06-14 10:19:51.395000: I runner.py:310] Step = 100300 ; steps/s = 1.66, tokens/s = 44189 (44189 target) ; Learning rate = 0.000279 ; Loss = 1.513192\n",
      "2024-06-14 10:20:52.117000: I runner.py:310] Step = 100400 ; steps/s = 1.65, tokens/s = 44770 (44770 target) ; Learning rate = 0.000279 ; Loss = 1.521663\n",
      "2024-06-14 10:21:52.936000: I runner.py:310] Step = 100500 ; steps/s = 1.64, tokens/s = 44710 (44710 target) ; Learning rate = 0.000279 ; Loss = 1.526043\n",
      "2024-06-14 10:22:53.711000: I runner.py:310] Step = 100600 ; steps/s = 1.65, tokens/s = 44694 (44694 target) ; Learning rate = 0.000279 ; Loss = 1.520504\n",
      "2024-06-14 10:23:54.102000: I runner.py:310] Step = 100700 ; steps/s = 1.66, tokens/s = 44143 (44143 target) ; Learning rate = 0.000279 ; Loss = 1.526433\n",
      "2024-06-14 10:24:54.857000: I runner.py:310] Step = 100800 ; steps/s = 1.65, tokens/s = 44763 (44763 target) ; Learning rate = 0.000278 ; Loss = 1.512553\n",
      "2024-06-14 10:25:55.626000: I runner.py:310] Step = 100900 ; steps/s = 1.65, tokens/s = 44705 (44705 target) ; Learning rate = 0.000278 ; Loss = 1.517005\n",
      "2024-06-14 10:26:56.424000: I runner.py:310] Step = 101000 ; steps/s = 1.64, tokens/s = 44676 (44676 target) ; Learning rate = 0.000278 ; Loss = 1.513281\n",
      "2024-06-14 10:27:56.732000: I runner.py:310] Step = 101100 ; steps/s = 1.66, tokens/s = 44219 (44219 target) ; Learning rate = 0.000278 ; Loss = 1.522906\n",
      "2024-06-14 10:28:57.562000: I runner.py:310] Step = 101200 ; steps/s = 1.64, tokens/s = 44684 (44684 target) ; Learning rate = 0.000278 ; Loss = 1.514373\n",
      "2024-06-14 10:29:58.262000: I runner.py:310] Step = 101300 ; steps/s = 1.65, tokens/s = 44759 (44759 target) ; Learning rate = 0.000278 ; Loss = 1.519103\n",
      "2024-06-14 10:30:59.087000: I runner.py:310] Step = 101400 ; steps/s = 1.64, tokens/s = 44663 (44663 target) ; Learning rate = 0.000278 ; Loss = 1.515566\n",
      "2024-06-14 10:31:59.400000: I runner.py:310] Step = 101500 ; steps/s = 1.66, tokens/s = 44206 (44206 target) ; Learning rate = 0.000277 ; Loss = 1.513288\n",
      "2024-06-14 10:33:00.112000: I runner.py:310] Step = 101600 ; steps/s = 1.65, tokens/s = 44774 (44774 target) ; Learning rate = 0.000277 ; Loss = 1.516498\n",
      "2024-06-14 10:34:00.855000: I runner.py:310] Step = 101700 ; steps/s = 1.65, tokens/s = 44751 (44751 target) ; Learning rate = 0.000277 ; Loss = 1.520923\n",
      "2024-06-14 10:35:01.159000: I runner.py:310] Step = 101800 ; steps/s = 1.66, tokens/s = 44175 (44175 target) ; Learning rate = 0.000277 ; Loss = 1.516953\n",
      "2024-06-14 10:36:01.905000: I runner.py:310] Step = 101900 ; steps/s = 1.65, tokens/s = 44761 (44761 target) ; Learning rate = 0.000277 ; Loss = 1.520396\n",
      "2024-06-14 10:37:02.672000: I runner.py:310] Step = 102000 ; steps/s = 1.65, tokens/s = 44708 (44708 target) ; Learning rate = 0.000277 ; Loss = 1.521425\n",
      "2024-06-14 10:38:03.431000: I runner.py:310] Step = 102100 ; steps/s = 1.65, tokens/s = 44713 (44713 target) ; Learning rate = 0.000277 ; Loss = 1.519421\n",
      "2024-06-14 10:39:03.700000: I runner.py:310] Step = 102200 ; steps/s = 1.66, tokens/s = 44253 (44253 target) ; Learning rate = 0.000276 ; Loss = 1.523267\n",
      "2024-06-14 10:40:04.389000: I runner.py:310] Step = 102300 ; steps/s = 1.65, tokens/s = 44762 (44762 target) ; Learning rate = 0.000276 ; Loss = 1.517748\n",
      "2024-06-14 10:41:05.141000: I runner.py:310] Step = 102400 ; steps/s = 1.65, tokens/s = 44750 (44750 target) ; Learning rate = 0.000276 ; Loss = 1.518457\n",
      "2024-06-14 10:42:05.885000: I runner.py:310] Step = 102500 ; steps/s = 1.65, tokens/s = 44759 (44759 target) ; Learning rate = 0.000276 ; Loss = 1.518913\n",
      "2024-06-14 10:43:06.245000: I runner.py:310] Step = 102600 ; steps/s = 1.66, tokens/s = 44166 (44166 target) ; Learning rate = 0.000276 ; Loss = 1.516846\n",
      "2024-06-14 10:44:06.982000: I runner.py:310] Step = 102700 ; steps/s = 1.65, tokens/s = 44737 (44737 target) ; Learning rate = 0.000276 ; Loss = 1.515113\n",
      "2024-06-14 10:45:07.742000: I runner.py:310] Step = 102800 ; steps/s = 1.65, tokens/s = 44748 (44748 target) ; Learning rate = 0.000276 ; Loss = 1.513533\n",
      "2024-06-14 10:46:08.556000: I runner.py:310] Step = 102900 ; steps/s = 1.64, tokens/s = 44665 (44665 target) ; Learning rate = 0.000276 ; Loss = 1.514025\n",
      "2024-06-14 10:47:08.814000: I runner.py:310] Step = 103000 ; steps/s = 1.66, tokens/s = 44269 (44269 target) ; Learning rate = 0.000275 ; Loss = 1.529111\n",
      "2024-06-14 10:48:09.552000: I runner.py:310] Step = 103100 ; steps/s = 1.65, tokens/s = 44723 (44723 target) ; Learning rate = 0.000275 ; Loss = 1.516068\n",
      "2024-06-14 10:49:10.306000: I runner.py:310] Step = 103200 ; steps/s = 1.65, tokens/s = 44715 (44715 target) ; Learning rate = 0.000275 ; Loss = 1.511628\n",
      "2024-06-14 10:50:11.050000: I runner.py:310] Step = 103300 ; steps/s = 1.65, tokens/s = 44748 (44748 target) ; Learning rate = 0.000275 ; Loss = 1.519500\n",
      "2024-06-14 10:51:11.329000: I runner.py:310] Step = 103400 ; steps/s = 1.66, tokens/s = 44236 (44236 target) ; Learning rate = 0.000275 ; Loss = 1.513799\n",
      "2024-06-14 10:52:12.109000: I runner.py:310] Step = 103500 ; steps/s = 1.65, tokens/s = 44693 (44693 target) ; Learning rate = 0.000275 ; Loss = 1.514200\n",
      "2024-06-14 10:53:12.897000: I runner.py:310] Step = 103600 ; steps/s = 1.65, tokens/s = 44711 (44711 target) ; Learning rate = 0.000275 ; Loss = 1.525360\n",
      "2024-06-14 10:54:13.713000: I runner.py:310] Step = 103700 ; steps/s = 1.64, tokens/s = 44689 (44689 target) ; Learning rate = 0.000274 ; Loss = 1.523193\n",
      "2024-06-14 10:55:14.082000: I runner.py:310] Step = 103800 ; steps/s = 1.66, tokens/s = 44191 (44191 target) ; Learning rate = 0.000274 ; Loss = 1.508915\n",
      "2024-06-14 10:56:14.906000: I runner.py:310] Step = 103900 ; steps/s = 1.64, tokens/s = 44680 (44680 target) ; Learning rate = 0.000274 ; Loss = 1.518731\n",
      "2024-06-14 10:57:15.642000: I runner.py:310] Step = 104000 ; steps/s = 1.65, tokens/s = 44739 (44739 target) ; Learning rate = 0.000274 ; Loss = 1.518318\n",
      "2024-06-14 10:58:16.305000: I runner.py:310] Step = 104100 ; steps/s = 1.65, tokens/s = 44607 (44607 target) ; Learning rate = 0.000274 ; Loss = 1.541563\n",
      "2024-06-14 10:59:16.666000: I runner.py:310] Step = 104200 ; steps/s = 1.66, tokens/s = 44375 (44375 target) ; Learning rate = 0.000274 ; Loss = 1.518292\n",
      "2024-06-14 11:00:17.405000: I runner.py:310] Step = 104300 ; steps/s = 1.65, tokens/s = 44728 (44728 target) ; Learning rate = 0.000274 ; Loss = 1.510528\n",
      "2024-06-14 11:01:18.160000: I runner.py:310] Step = 104400 ; steps/s = 1.65, tokens/s = 44730 (44730 target) ; Learning rate = 0.000274 ; Loss = 1.519968\n",
      "2024-06-14 11:02:18.478000: I runner.py:310] Step = 104500 ; steps/s = 1.66, tokens/s = 44195 (44195 target) ; Learning rate = 0.000273 ; Loss = 1.516686\n",
      "2024-06-14 11:03:19.206000: I runner.py:310] Step = 104600 ; steps/s = 1.65, tokens/s = 44783 (44783 target) ; Learning rate = 0.000273 ; Loss = 1.512577\n",
      "2024-06-14 11:04:19.962000: I runner.py:310] Step = 104700 ; steps/s = 1.65, tokens/s = 44685 (44685 target) ; Learning rate = 0.000273 ; Loss = 1.509784\n",
      "2024-06-14 11:05:20.696000: I runner.py:310] Step = 104800 ; steps/s = 1.65, tokens/s = 44745 (44745 target) ; Learning rate = 0.000273 ; Loss = 1.520792\n",
      "2024-06-14 11:06:21.065000: I runner.py:310] Step = 104900 ; steps/s = 1.66, tokens/s = 44177 (44177 target) ; Learning rate = 0.000273 ; Loss = 1.520536\n",
      "2024-06-14 11:07:21.808000: I runner.py:310] Step = 105000 ; steps/s = 1.65, tokens/s = 44761 (44761 target) ; Learning rate = 0.000273 ; Loss = 1.517768\n",
      "2024-06-14 11:07:21.810000: I training.py:192] Running evaluation for step 105000\n",
      "2024-06-14 11:11:29.271000: I training.py:192] Evaluation result for step 105000: loss = 1.390125 ; perplexity = 4.015354\n",
      "2024-06-14 11:11:30.990000: W runner.py:310] 0.225% of target tokens are out of vocabulary (6138132 out of 2727613120 tokens)\n",
      "2024-06-14 11:11:30.993000: I runner.py:310] The 10 most frequent out of vocabulary target tokens are: Q (8.4%); $ (8.3%); ұ (6.4%); + (6.3%); ? (6.1%);  (3.7%); ° (3.4%); ! (3.3%); _ (2.2%);  (2.0%)\n",
      "2024-06-14 11:11:32.970000: I training.py:176] Saved checkpoint RoBERTa_POS-KK-EN/ckpt-105000\n",
      "2024-06-14 11:11:32.973000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file:\n",
      "  - tokens_dev.txt\n",
      "  - pos_tags_dev.txt\n",
      "  eval_labels_file: dev_target_tokens.txt\n",
      "  source_1_vocabulary: kk_vocab.vocab\n",
      "  source_2_vocabulary: RoBERTa_KK_unique_pos\n",
      "  target_vocabulary: tgt_1_en_vocab.vocab\n",
      "  train_features_file:\n",
      "  - tokens_train.txt\n",
      "  - pos_tags_train.txt\n",
      "  train_labels_file: train_target_tokens.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: RoBERTa_POS-KK-EN\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    scale: 2.0\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 105000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-06-14 11:11:33.164000: I inputter.py:316] Initialized source_1 input layer:\n",
      "2024-06-14 11:11:33.164000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-06-14 11:11:33.164000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-06-14 11:11:33.165000: I inputter.py:316] Initialized source_2 input layer:\n",
      "2024-06-14 11:11:33.165000: I inputter.py:316]  - vocabulary size: 21\n",
      "2024-06-14 11:11:33.165000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-06-14 11:11:33.231000: I inputter.py:316] Initialized target input layer:\n",
      "2024-06-14 11:11:33.231000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-06-14 11:11:33.231000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-06-14 11:11:33.255000: I runner.py:383] Restored checkpoint RoBERTa_POS-KK-EN/ckpt-105000\n",
      "2024-06-14 11:11:34.986000: I runner.py:386] Averaging 8 checkpoints...\n",
      "2024-06-14 11:11:34.986000: I runner.py:386] Reading checkpoint RoBERTa_POS-KK-EN/ckpt-40000...\n",
      "2024-06-14 11:11:36.090000: I runner.py:386] Reading checkpoint RoBERTa_POS-KK-EN/ckpt-50000...\n",
      "2024-06-14 11:11:36.424000: I runner.py:386] Reading checkpoint RoBERTa_POS-KK-EN/ckpt-60000...\n",
      "2024-06-14 11:11:36.744000: I runner.py:386] Reading checkpoint RoBERTa_POS-KK-EN/ckpt-70000...\n",
      "2024-06-14 11:11:37.060000: I runner.py:386] Reading checkpoint RoBERTa_POS-KK-EN/ckpt-80000...\n",
      "2024-06-14 11:11:37.377000: I runner.py:386] Reading checkpoint RoBERTa_POS-KK-EN/ckpt-90000...\n",
      "2024-06-14 11:11:37.695000: I runner.py:386] Reading checkpoint RoBERTa_POS-KK-EN/ckpt-100000...\n",
      "2024-06-14 11:11:38.014000: I runner.py:386] Reading checkpoint RoBERTa_POS-KK-EN/ckpt-105000...\n",
      "2024-06-14 11:11:40.340000: I runner.py:386] Saved averaged checkpoint to RoBERTa_POS-KK-EN/avg/ckpt-105000\n"
     ]
    }
   ],
   "source": [
    "# Kk-En (POS Tags)\n",
    "!onmt-main --model modelim.py --config data.yml --auto_config train --with_eval --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1de49b0-2eaf-4a62-a670-0e571bfeb1f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-14 13:36:54.811532: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-14 13:36:55.674074: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-14 13:36:55.674180: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-14 13:36:55.674188: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-06-14 13:36:56.749000: I main.py:308] Loading model description from RoBERTa_POS-KK-EN/model_description.py\n",
      "2024-06-14 13:36:56.945000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-06-14 13:36:56.945000: I main.py:315] Using model:\n",
      "(model): MultiFeaturesTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): ParallelInputter(\n",
      "      (inputters): ListWrapper(\n",
      "        (0): WordEmbedder()\n",
      "        (1): WordEmbedder()\n",
      "      )\n",
      "      (reducer): ConcatReducer()\n",
      "    )\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): ParallelInputter(\n",
      "        (inputters): ListWrapper(\n",
      "          (0): WordEmbedder()\n",
      "          (1): WordEmbedder()\n",
      "        )\n",
      "        (reducer): ConcatReducer()\n",
      "      )\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-06-14 13:36:56.950000: I main.py:340] Using parameters:\n",
      "data:\n",
      "  eval_features_file:\n",
      "  - tokens_dev.txt\n",
      "  - pos_tags_dev.txt\n",
      "  eval_labels_file: dev_target_tokens.txt\n",
      "  source_1_vocabulary: kk_vocab.vocab\n",
      "  source_2_vocabulary: RoBERTa_KK_unique_pos\n",
      "  target_vocabulary: tgt_1_en_vocab.vocab\n",
      "  train_features_file:\n",
      "  - tokens_train.txt\n",
      "  - pos_tags_train.txt\n",
      "  train_labels_file: train_target_tokens.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: RoBERTa_POS-KK-EN\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 105000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-06-14 13:36:57.127205: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-14 13:36:57.716481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-06-14 13:36:57.867000: I inputter.py:316] Initialized source_1 input layer:\n",
      "2024-06-14 13:36:57.867000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-06-14 13:36:57.867000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-06-14 13:36:57.870000: I inputter.py:316] Initialized source_2 input layer:\n",
      "2024-06-14 13:36:57.870000: I inputter.py:316]  - vocabulary size: 21\n",
      "2024-06-14 13:36:57.870000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-06-14 13:36:57.944000: I inputter.py:316] Initialized target input layer:\n",
      "2024-06-14 13:36:57.944000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-06-14 13:36:57.944000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-06-14 13:36:57.964000: I runner.py:462] Restored checkpoint RoBERTa_POS-KK-EN/ckpt-100000\n",
      "2024-06-14 13:36:58.007000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-06-14 13:36:58.792899: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-06-14 13:36:58.919000: I runner.py:471] Tracing and optimizing the inference graph...\n",
      "2024-06-14 13:37:12.098697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-06-14 13:37:12.957019: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-06-14 13:37:23.605000: I runner.py:471] 959 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-14 13:37:33.742000: I runner.py:471] 1791 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-14 13:37:43.769000: I runner.py:471] 2687 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-14 13:37:54.092000: I runner.py:471] 3519 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-14 13:38:04.194000: I runner.py:471] 4415 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-14 13:38:14.443000: I runner.py:471] 5215 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-14 13:38:24.568000: I runner.py:471] 6047 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-14 13:38:34.751000: I runner.py:471] 6815 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-14 13:38:44.982000: I runner.py:471] 7679 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-14 13:38:55.190000: I runner.py:471] 8639 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-14 13:39:05.551000: I runner.py:471] 9599 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-14 13:39:15.587000: I runner.py:471] 10367 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-14 13:39:25.784000: I runner.py:471] 11167 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-14 13:39:35.813000: I runner.py:471] 11999 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-14 13:39:46.146000: I runner.py:471] 12799 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-14 13:40:07.433000: I runner.py:471] 14510 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-06-14 13:40:17.868000: I runner.py:471] 15310 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-06-14 13:40:28.527000: I runner.py:471] 16270 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-06-14 13:40:39.004000: I runner.py:471] 17134 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-06-14 13:40:49.396000: I runner.py:471] 17867 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-06-14 13:41:08.057000: I runner.py:471] 17805 predictions are buffered, but waiting for the prediction of queued line 281 to advance the output...\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 onmt-main --config data.yml --auto_config --checkpoint_path RoBERTa_POS-KK-EN/ckpt-100000 infer --features_file tokens_test.txt pos_tags_test.txt --predictions_file output_kk_en.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d15513-da1e-4140-8f33-4ae730b43906",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 MT-Preparation/subwording/3-desubword.py tgt_1_en_vocab.model output_kk_en.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d29554c-f3b4-45c3-aad5-83d99635ab53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference first sentence: In the developed world, this figure is 35 25%\n",
      "Translated first sentence: In the developed countries of the world , this figure is 35 25%\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "BLEU:  BLEU = 50.02 71.5/54.3/44.0/36.6 (BP = 1.000 ratio = 1.093 hyp_len = 452895 ref_len = 414303)\n",
      "CHRF:  chrF2 = 73.89\n"
     ]
    }
   ],
   "source": [
    "# BLEU and chrF scores\n",
    "!python3 compute-bleu.py en_test_shuffled.txt-filtered.en output_kk_en.txt.desubword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72d9ddb0-0a60-42cb-89ff-2dba0e86ff86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama METEOR Puanı: 0.7377232733926027\n"
     ]
    }
   ],
   "source": [
    "# Average METEOR score (Ortalama METEOR Puanı)\n",
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "def read_and_tokenize_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return [nltk.word_tokenize(line.strip()) for line in lines]\n",
    "\n",
    "def calculate_meteor(reference_file, hypothesis_file):\n",
    "    references = read_and_tokenize_file(reference_file)\n",
    "    hypotheses = read_and_tokenize_file(hypothesis_file)\n",
    "    \n",
    "    if len(references) != len(hypotheses):\n",
    "        raise ValueError(\"Dosyaların satır sayıları eşleşmiyor\")\n",
    "\n",
    "    total_meteor_score = 0.0\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        total_meteor_score += meteor_score([ref], hyp)\n",
    "\n",
    "    average_meteor_score = total_meteor_score / len(references)\n",
    "    return average_meteor_score\n",
    "\n",
    "reference_file = 'en_test_shuffled.txt-filtered.en'\n",
    "hypothesis_file = 'output_kk_en.txt.desubword'\n",
    "\n",
    "score = calculate_meteor(reference_file, hypothesis_file)\n",
    "print(f\"Ortalama METEOR Puanı: {score}\") #Average METEOR score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "359311c8-7fdb-491d-ba06-60931469107c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-16 11:50:11.725490: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-16 11:50:12.480913: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-16 11:50:12.480996: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-16 11:50:12.481005: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-06-16 11:50:13.428000: I onmt-main:8] Creating model directory KK-EN-Standard-Transformer\n",
      "2024-06-16 11:50:13.622000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-06-16 11:50:13.622000: I main.py:315] Using model:\n",
      "(model): MyCustomTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): WordEmbedder()\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): WordEmbedder()\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-06-16 11:50:13.625311: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-16 11:50:14.704771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-06-16 11:50:14.705422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7643 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-06-16 11:50:14.709000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file: tokens_dev.txt\n",
      "  eval_labels_file: dev_target_tokens.txt\n",
      "  source_vocabulary: kk_vocab.vocab\n",
      "  target_vocabulary: tgt_1_en_vocab.vocab\n",
      "  train_features_file: tokens_train.txt\n",
      "  train_labels_file: train_target_tokens.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: KK-EN-Standard-Transformer\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 105000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-06-16 11:50:15.032000: I inputter.py:316] Initialized source input layer:\n",
      "2024-06-16 11:50:15.032000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-06-16 11:50:15.033000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-06-16 11:50:15.105000: I inputter.py:316] Initialized target input layer:\n",
      "2024-06-16 11:50:15.105000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-06-16 11:50:15.105000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-06-16 11:50:15.109000: W runner.py:269] No checkpoint to restore in KK-EN-Standard-Transformer\n",
      "2024-06-16 11:50:15.112000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "2024-06-16 11:50:15.155000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-06-16 11:50:16.043431: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-06-16 11:50:16.158000: I main.py:325] Accumulate gradients of 7 iterations to reach effective batch size of 25000\n",
      "2024-06-16 11:50:16.284000: I mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2024-06-16 11:50:16.478000: I dataset_ops.py:2542] Training on 318032 examples\n",
      "2024-06-16 11:51:18.368624: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-06-16 11:51:19.451998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-06-16 11:51:19.722544: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-06-16 11:51:28.644000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-06-16 11:51:28.668000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-06-16 11:51:30.121000: I cross_device_ops.py:897] batch_all_reduce: 260 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-06-16 11:51:34.954000: I cross_device_ops.py:897] batch_all_reduce: 260 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-06-16 11:51:40.495000: I runner.py:310] Number of model parameters: 93326081\n",
      "2024-06-16 11:51:40.499000: I runner.py:310] Number of model weights: 260 (trainable = 260, non trainable = 0)\n",
      "2024-06-16 11:51:40.531000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-06-16 11:51:40.538000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-06-16 11:51:42.648000: I training.py:176] Saved checkpoint KK-EN-Standard-Transformer/ckpt-1\n",
      "2024-06-16 11:51:43.228000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-06-16 11:51:43.251000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-06-16 11:51:43.868000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-06-16 11:51:43.891000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-06-16 11:51:44.508000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-06-16 11:51:44.527000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-06-16 11:52:42.686000: I runner.py:310] Step = 100 ; steps/s = 1.65, tokens/s = 81284 (36370 source, 44914 target) ; Learning rate = 0.000009 ; Loss = 9.716975\n",
      "2024-06-16 11:53:43.196000: I runner.py:310] Step = 200 ; steps/s = 1.65, tokens/s = 81251 (36351 source, 44900 target) ; Learning rate = 0.000018 ; Loss = 8.928395\n",
      "2024-06-16 11:54:43.594000: I runner.py:310] Step = 300 ; steps/s = 1.66, tokens/s = 81495 (36471 source, 45024 target) ; Learning rate = 0.000027 ; Loss = 8.009831\n",
      "2024-06-16 11:55:43.851000: I runner.py:310] Step = 400 ; steps/s = 1.66, tokens/s = 80110 (35861 source, 44249 target) ; Learning rate = 0.000035 ; Loss = 7.359418\n",
      "2024-06-16 11:56:44.250000: I runner.py:310] Step = 500 ; steps/s = 1.66, tokens/s = 81453 (36431 source, 45022 target) ; Learning rate = 0.000044 ; Loss = 6.967281\n",
      "2024-06-16 11:57:44.737000: I runner.py:310] Step = 600 ; steps/s = 1.65, tokens/s = 81334 (36410 source, 44924 target) ; Learning rate = 0.000053 ; Loss = 6.684498\n",
      "2024-06-16 11:58:45.162000: I runner.py:310] Step = 700 ; steps/s = 1.66, tokens/s = 81398 (36444 source, 44954 target) ; Learning rate = 0.000062 ; Loss = 6.545239\n",
      "2024-06-16 11:59:45.038000: I runner.py:310] Step = 800 ; steps/s = 1.67, tokens/s = 80601 (36063 source, 44538 target) ; Learning rate = 0.000071 ; Loss = 6.234168\n",
      "2024-06-16 12:00:45.464000: I runner.py:310] Step = 900 ; steps/s = 1.66, tokens/s = 81427 (36443 source, 44984 target) ; Learning rate = 0.000080 ; Loss = 6.035399\n",
      "2024-06-16 12:01:45.872000: I runner.py:310] Step = 1000 ; steps/s = 1.66, tokens/s = 81406 (36406 source, 45000 target) ; Learning rate = 0.000088 ; Loss = 5.838676\n",
      "2024-06-16 12:02:46.354000: I runner.py:310] Step = 1100 ; steps/s = 1.65, tokens/s = 81348 (36422 source, 44926 target) ; Learning rate = 0.000097 ; Loss = 5.610527\n",
      "2024-06-16 12:03:46.319000: I runner.py:310] Step = 1200 ; steps/s = 1.67, tokens/s = 80507 (36062 source, 44445 target) ; Learning rate = 0.000106 ; Loss = 5.547122\n",
      "2024-06-16 12:04:46.751000: I runner.py:310] Step = 1300 ; steps/s = 1.65, tokens/s = 81388 (36431 source, 44957 target) ; Learning rate = 0.000115 ; Loss = 5.386599\n",
      "2024-06-16 12:05:47.182000: I runner.py:310] Step = 1400 ; steps/s = 1.66, tokens/s = 81407 (36430 source, 44977 target) ; Learning rate = 0.000124 ; Loss = 5.268390\n",
      "2024-06-16 12:06:47.605000: I runner.py:310] Step = 1500 ; steps/s = 1.66, tokens/s = 81409 (36429 source, 44980 target) ; Learning rate = 0.000133 ; Loss = 5.117297\n",
      "2024-06-16 12:07:47.810000: I runner.py:310] Step = 1600 ; steps/s = 1.66, tokens/s = 80161 (35865 source, 44296 target) ; Learning rate = 0.000142 ; Loss = 4.927552\n",
      "2024-06-16 12:08:48.266000: I runner.py:310] Step = 1700 ; steps/s = 1.65, tokens/s = 81372 (36428 source, 44944 target) ; Learning rate = 0.000150 ; Loss = 4.695496\n",
      "2024-06-16 12:09:48.705000: I runner.py:310] Step = 1800 ; steps/s = 1.65, tokens/s = 81409 (36450 source, 44959 target) ; Learning rate = 0.000159 ; Loss = 4.612584\n",
      "2024-06-16 12:10:49.069000: I runner.py:310] Step = 1900 ; steps/s = 1.66, tokens/s = 81461 (36443 source, 45018 target) ; Learning rate = 0.000168 ; Loss = 4.395267\n",
      "2024-06-16 12:11:49.010000: I runner.py:310] Step = 2000 ; steps/s = 1.67, tokens/s = 80506 (35997 source, 44509 target) ; Learning rate = 0.000177 ; Loss = 4.327210\n",
      "2024-06-16 12:12:49.436000: I runner.py:310] Step = 2100 ; steps/s = 1.66, tokens/s = 81419 (36437 source, 44982 target) ; Learning rate = 0.000186 ; Loss = 4.182874\n",
      "2024-06-16 12:13:49.838000: I runner.py:310] Step = 2200 ; steps/s = 1.66, tokens/s = 81426 (36451 source, 44975 target) ; Learning rate = 0.000195 ; Loss = 4.084275\n",
      "2024-06-16 12:14:50.317000: I runner.py:310] Step = 2300 ; steps/s = 1.65, tokens/s = 81357 (36420 source, 44937 target) ; Learning rate = 0.000203 ; Loss = 3.933593\n",
      "2024-06-16 12:15:50.297000: I runner.py:310] Step = 2400 ; steps/s = 1.67, tokens/s = 80471 (36008 source, 44463 target) ; Learning rate = 0.000212 ; Loss = 3.717402\n",
      "2024-06-16 12:16:50.680000: I runner.py:310] Step = 2500 ; steps/s = 1.66, tokens/s = 81478 (36465 source, 45013 target) ; Learning rate = 0.000221 ; Loss = 3.593108\n",
      "2024-06-16 12:17:51.118000: I runner.py:310] Step = 2600 ; steps/s = 1.65, tokens/s = 81386 (36433 source, 44953 target) ; Learning rate = 0.000230 ; Loss = 3.479917\n",
      "2024-06-16 12:18:51.107000: I runner.py:310] Step = 2700 ; steps/s = 1.67, tokens/s = 80434 (36000 source, 44434 target) ; Learning rate = 0.000239 ; Loss = 3.469125\n",
      "2024-06-16 12:19:51.560000: I runner.py:310] Step = 2800 ; steps/s = 1.65, tokens/s = 81370 (36412 source, 44958 target) ; Learning rate = 0.000248 ; Loss = 3.297229\n",
      "2024-06-16 12:20:52.041000: I runner.py:310] Step = 2900 ; steps/s = 1.65, tokens/s = 81362 (36401 source, 44961 target) ; Learning rate = 0.000256 ; Loss = 3.268570\n",
      "2024-06-16 12:21:52.463000: I runner.py:310] Step = 3000 ; steps/s = 1.66, tokens/s = 81410 (36422 source, 44988 target) ; Learning rate = 0.000265 ; Loss = 3.261239\n",
      "2024-06-16 12:22:52.563000: I runner.py:310] Step = 3100 ; steps/s = 1.66, tokens/s = 80282 (35950 source, 44332 target) ; Learning rate = 0.000274 ; Loss = 3.180655\n",
      "2024-06-16 12:23:53.003000: I runner.py:310] Step = 3200 ; steps/s = 1.65, tokens/s = 81367 (36369 source, 44998 target) ; Learning rate = 0.000283 ; Loss = 3.106328\n",
      "2024-06-16 12:24:53.466000: I runner.py:310] Step = 3300 ; steps/s = 1.65, tokens/s = 81367 (36443 source, 44924 target) ; Learning rate = 0.000292 ; Loss = 3.030639\n",
      "2024-06-16 12:25:53.887000: I runner.py:310] Step = 3400 ; steps/s = 1.66, tokens/s = 81416 (36430 source, 44986 target) ; Learning rate = 0.000301 ; Loss = 3.002868\n",
      "2024-06-16 12:26:53.965000: I runner.py:310] Step = 3500 ; steps/s = 1.66, tokens/s = 80319 (35972 source, 44347 target) ; Learning rate = 0.000309 ; Loss = 3.040898\n",
      "2024-06-16 12:27:54.351000: I runner.py:310] Step = 3600 ; steps/s = 1.66, tokens/s = 81491 (36487 source, 45004 target) ; Learning rate = 0.000318 ; Loss = 2.910630\n",
      "2024-06-16 12:28:54.805000: I runner.py:310] Step = 3700 ; steps/s = 1.65, tokens/s = 81392 (36421 source, 44971 target) ; Learning rate = 0.000327 ; Loss = 2.881419\n",
      "2024-06-16 12:29:55.266000: I runner.py:310] Step = 3800 ; steps/s = 1.65, tokens/s = 81360 (36398 source, 44962 target) ; Learning rate = 0.000336 ; Loss = 2.833770\n",
      "2024-06-16 12:30:55.255000: I runner.py:310] Step = 3900 ; steps/s = 1.67, tokens/s = 80452 (36014 source, 44438 target) ; Learning rate = 0.000345 ; Loss = 2.775971\n",
      "2024-06-16 12:31:55.735000: I runner.py:310] Step = 4000 ; steps/s = 1.65, tokens/s = 81327 (36402 source, 44925 target) ; Learning rate = 0.000354 ; Loss = 2.758108\n",
      "2024-06-16 12:32:56.176000: I runner.py:310] Step = 4100 ; steps/s = 1.65, tokens/s = 81370 (36407 source, 44963 target) ; Learning rate = 0.000362 ; Loss = 2.806335\n",
      "2024-06-16 12:33:56.599000: I runner.py:310] Step = 4200 ; steps/s = 1.66, tokens/s = 81415 (36425 source, 44990 target) ; Learning rate = 0.000371 ; Loss = 2.717254\n",
      "2024-06-16 12:34:56.558000: I runner.py:310] Step = 4300 ; steps/s = 1.67, tokens/s = 80518 (36052 source, 44466 target) ; Learning rate = 0.000380 ; Loss = 2.628564\n",
      "2024-06-16 12:35:57.008000: I runner.py:310] Step = 4400 ; steps/s = 1.65, tokens/s = 81420 (36433 source, 44987 target) ; Learning rate = 0.000389 ; Loss = 2.626786\n",
      "2024-06-16 12:36:57.505000: I runner.py:310] Step = 4500 ; steps/s = 1.65, tokens/s = 81289 (36408 source, 44881 target) ; Learning rate = 0.000398 ; Loss = 2.685122\n",
      "2024-06-16 12:37:57.953000: I runner.py:310] Step = 4600 ; steps/s = 1.65, tokens/s = 81325 (36383 source, 44942 target) ; Learning rate = 0.000407 ; Loss = 2.613432\n",
      "2024-06-16 12:38:57.896000: I runner.py:310] Step = 4700 ; steps/s = 1.67, tokens/s = 80548 (36051 source, 44497 target) ; Learning rate = 0.000416 ; Loss = 2.574295\n",
      "2024-06-16 12:39:58.320000: I runner.py:310] Step = 4800 ; steps/s = 1.66, tokens/s = 81402 (36399 source, 45003 target) ; Learning rate = 0.000424 ; Loss = 2.555419\n",
      "2024-06-16 12:40:58.669000: I runner.py:310] Step = 4900 ; steps/s = 1.66, tokens/s = 81487 (36468 source, 45019 target) ; Learning rate = 0.000433 ; Loss = 2.503959\n",
      "2024-06-16 12:41:58.606000: I runner.py:310] Step = 5000 ; steps/s = 1.67, tokens/s = 80523 (36060 source, 44463 target) ; Learning rate = 0.000442 ; Loss = 2.487269\n",
      "2024-06-16 12:41:58.608000: I training.py:192] Running evaluation for step 5000\n",
      "2024-06-16 12:50:33.477000: I training.py:192] Evaluation result for step 5000: loss = 1.437090 ; perplexity = 4.208432\n",
      "2024-06-16 12:51:33.887000: I runner.py:310] Step = 5100 ; steps/s = 1.66, tokens/s = 81439 (36441 source, 44998 target) ; Learning rate = 0.000451 ; Loss = 2.504269\n",
      "2024-06-16 12:52:34.468000: I runner.py:310] Step = 5200 ; steps/s = 1.65, tokens/s = 81198 (36320 source, 44878 target) ; Learning rate = 0.000460 ; Loss = 2.444065\n",
      "2024-06-16 12:53:35.062000: I runner.py:310] Step = 5300 ; steps/s = 1.65, tokens/s = 81159 (36339 source, 44820 target) ; Learning rate = 0.000469 ; Loss = 2.456298\n",
      "2024-06-16 12:54:35.196000: I runner.py:310] Step = 5400 ; steps/s = 1.66, tokens/s = 80287 (35942 source, 44345 target) ; Learning rate = 0.000477 ; Loss = 2.444095\n",
      "2024-06-16 12:55:35.795000: I runner.py:310] Step = 5500 ; steps/s = 1.65, tokens/s = 81182 (36322 source, 44860 target) ; Learning rate = 0.000486 ; Loss = 2.451485\n",
      "2024-06-16 12:56:36.386000: I runner.py:310] Step = 5600 ; steps/s = 1.65, tokens/s = 81206 (36368 source, 44838 target) ; Learning rate = 0.000495 ; Loss = 2.421510\n",
      "2024-06-16 12:57:36.939000: I runner.py:310] Step = 5700 ; steps/s = 1.65, tokens/s = 81233 (36331 source, 44902 target) ; Learning rate = 0.000504 ; Loss = 2.453771\n",
      "2024-06-16 12:58:37.124000: I runner.py:310] Step = 5800 ; steps/s = 1.66, tokens/s = 80154 (35886 source, 44268 target) ; Learning rate = 0.000513 ; Loss = 2.326321\n",
      "2024-06-16 12:59:37.712000: I runner.py:310] Step = 5900 ; steps/s = 1.65, tokens/s = 81193 (36332 source, 44861 target) ; Learning rate = 0.000522 ; Loss = 2.306188\n",
      "2024-06-16 13:00:38.268000: I runner.py:310] Step = 6000 ; steps/s = 1.65, tokens/s = 81226 (36368 source, 44858 target) ; Learning rate = 0.000530 ; Loss = 2.300496\n",
      "2024-06-16 13:01:38.863000: I runner.py:310] Step = 6100 ; steps/s = 1.65, tokens/s = 81184 (36314 source, 44870 target) ; Learning rate = 0.000539 ; Loss = 2.333053\n",
      "2024-06-16 13:02:39.032000: I runner.py:310] Step = 6200 ; steps/s = 1.66, tokens/s = 80206 (35890 source, 44316 target) ; Learning rate = 0.000548 ; Loss = 2.299043\n",
      "2024-06-16 13:03:39.610000: I runner.py:310] Step = 6300 ; steps/s = 1.65, tokens/s = 81251 (36368 source, 44883 target) ; Learning rate = 0.000557 ; Loss = 2.289339\n",
      "2024-06-16 13:04:40.174000: I runner.py:310] Step = 6400 ; steps/s = 1.65, tokens/s = 81223 (36386 source, 44837 target) ; Learning rate = 0.000566 ; Loss = 2.271440\n",
      "2024-06-16 13:05:40.728000: I runner.py:310] Step = 6500 ; steps/s = 1.65, tokens/s = 81222 (36338 source, 44884 target) ; Learning rate = 0.000575 ; Loss = 2.280250\n",
      "2024-06-16 13:06:40.876000: I runner.py:310] Step = 6600 ; steps/s = 1.66, tokens/s = 80230 (35891 source, 44339 target) ; Learning rate = 0.000583 ; Loss = 2.269446\n",
      "2024-06-16 13:07:41.435000: I runner.py:310] Step = 6700 ; steps/s = 1.65, tokens/s = 81237 (36364 source, 44873 target) ; Learning rate = 0.000592 ; Loss = 2.276175\n",
      "2024-06-16 13:08:41.963000: I runner.py:310] Step = 6800 ; steps/s = 1.65, tokens/s = 81258 (36361 source, 44897 target) ; Learning rate = 0.000601 ; Loss = 2.233207\n",
      "2024-06-16 13:09:42.500000: I runner.py:310] Step = 6900 ; steps/s = 1.65, tokens/s = 81258 (36375 source, 44883 target) ; Learning rate = 0.000610 ; Loss = 2.270217\n",
      "2024-06-16 13:10:42.545000: I runner.py:310] Step = 7000 ; steps/s = 1.67, tokens/s = 80398 (35950 source, 44448 target) ; Learning rate = 0.000619 ; Loss = 2.209992\n",
      "2024-06-16 13:11:43.151000: I runner.py:310] Step = 7100 ; steps/s = 1.65, tokens/s = 81173 (36349 source, 44824 target) ; Learning rate = 0.000628 ; Loss = 2.163889\n",
      "2024-06-16 13:12:43.711000: I runner.py:310] Step = 7200 ; steps/s = 1.65, tokens/s = 81212 (36356 source, 44856 target) ; Learning rate = 0.000636 ; Loss = 2.220538\n",
      "2024-06-16 13:13:43.826000: I runner.py:310] Step = 7300 ; steps/s = 1.66, tokens/s = 80276 (35926 source, 44350 target) ; Learning rate = 0.000645 ; Loss = 2.150230\n",
      "2024-06-16 13:14:44.277000: I runner.py:310] Step = 7400 ; steps/s = 1.65, tokens/s = 81400 (36427 source, 44973 target) ; Learning rate = 0.000654 ; Loss = 2.113571\n",
      "2024-06-16 13:15:44.736000: I runner.py:310] Step = 7500 ; steps/s = 1.65, tokens/s = 81342 (36394 source, 44948 target) ; Learning rate = 0.000663 ; Loss = 2.188874\n",
      "2024-06-16 13:16:45.240000: I runner.py:310] Step = 7600 ; steps/s = 1.65, tokens/s = 81293 (36384 source, 44909 target) ; Learning rate = 0.000672 ; Loss = 2.146091\n",
      "2024-06-16 13:17:45.220000: I runner.py:310] Step = 7700 ; steps/s = 1.67, tokens/s = 80471 (36011 source, 44460 target) ; Learning rate = 0.000681 ; Loss = 2.098219\n",
      "2024-06-16 13:18:45.735000: I runner.py:310] Step = 7800 ; steps/s = 1.65, tokens/s = 81274 (36358 source, 44916 target) ; Learning rate = 0.000690 ; Loss = 2.105082\n",
      "2024-06-16 13:19:46.254000: I runner.py:310] Step = 7900 ; steps/s = 1.65, tokens/s = 81234 (36331 source, 44903 target) ; Learning rate = 0.000698 ; Loss = 2.142473\n",
      "2024-06-16 13:20:46.687000: I runner.py:310] Step = 8000 ; steps/s = 1.65, tokens/s = 81432 (36456 source, 44976 target) ; Learning rate = 0.000707 ; Loss = 2.149143\n",
      "2024-06-16 13:21:46.725000: I runner.py:310] Step = 8100 ; steps/s = 1.67, tokens/s = 80434 (36011 source, 44423 target) ; Learning rate = 0.000716 ; Loss = 2.135333\n",
      "2024-06-16 13:22:47.234000: I runner.py:310] Step = 8200 ; steps/s = 1.65, tokens/s = 81273 (36362 source, 44911 target) ; Learning rate = 0.000725 ; Loss = 2.094565\n",
      "2024-06-16 13:23:47.754000: I runner.py:310] Step = 8300 ; steps/s = 1.65, tokens/s = 81292 (36395 source, 44897 target) ; Learning rate = 0.000734 ; Loss = 2.107926\n",
      "2024-06-16 13:24:48.250000: I runner.py:310] Step = 8400 ; steps/s = 1.65, tokens/s = 81330 (36391 source, 44939 target) ; Learning rate = 0.000743 ; Loss = 2.121514\n",
      "2024-06-16 13:25:48.305000: I runner.py:310] Step = 8500 ; steps/s = 1.67, tokens/s = 80343 (35957 source, 44386 target) ; Learning rate = 0.000751 ; Loss = 2.053366\n",
      "2024-06-16 13:26:48.826000: I runner.py:310] Step = 8600 ; steps/s = 1.65, tokens/s = 81297 (36389 source, 44908 target) ; Learning rate = 0.000760 ; Loss = 2.083387\n",
      "2024-06-16 13:27:49.315000: I runner.py:310] Step = 8700 ; steps/s = 1.65, tokens/s = 81301 (36356 source, 44945 target) ; Learning rate = 0.000769 ; Loss = 2.082472\n",
      "2024-06-16 13:28:49.812000: I runner.py:310] Step = 8800 ; steps/s = 1.65, tokens/s = 81335 (36432 source, 44903 target) ; Learning rate = 0.000778 ; Loss = 2.119826\n",
      "2024-06-16 13:29:49.790000: I runner.py:310] Step = 8900 ; steps/s = 1.67, tokens/s = 80491 (36046 source, 44445 target) ; Learning rate = 0.000787 ; Loss = 2.015820\n",
      "2024-06-16 13:30:50.267000: I runner.py:310] Step = 9000 ; steps/s = 1.65, tokens/s = 81321 (36366 source, 44955 target) ; Learning rate = 0.000796 ; Loss = 2.031685\n",
      "2024-06-16 13:31:50.796000: I runner.py:310] Step = 9100 ; steps/s = 1.65, tokens/s = 81275 (36375 source, 44900 target) ; Learning rate = 0.000804 ; Loss = 2.055485\n",
      "2024-06-16 13:32:51.276000: I runner.py:310] Step = 9200 ; steps/s = 1.65, tokens/s = 81313 (36403 source, 44910 target) ; Learning rate = 0.000813 ; Loss = 2.099861\n",
      "2024-06-16 13:33:51.298000: I runner.py:310] Step = 9300 ; steps/s = 1.67, tokens/s = 80405 (35977 source, 44428 target) ; Learning rate = 0.000822 ; Loss = 2.023409\n",
      "2024-06-16 13:34:51.742000: I runner.py:310] Step = 9400 ; steps/s = 1.65, tokens/s = 81352 (36406 source, 44946 target) ; Learning rate = 0.000831 ; Loss = 2.029924\n",
      "2024-06-16 13:35:52.170000: I runner.py:310] Step = 9500 ; steps/s = 1.66, tokens/s = 81426 (36459 source, 44967 target) ; Learning rate = 0.000840 ; Loss = 2.023261\n",
      "2024-06-16 13:36:52.688000: I runner.py:310] Step = 9600 ; steps/s = 1.65, tokens/s = 81298 (36396 source, 44902 target) ; Learning rate = 0.000849 ; Loss = 2.046093\n",
      "2024-06-16 13:37:52.676000: I runner.py:310] Step = 9700 ; steps/s = 1.67, tokens/s = 80465 (35987 source, 44478 target) ; Learning rate = 0.000857 ; Loss = 2.014820\n",
      "2024-06-16 13:38:53.188000: I runner.py:310] Step = 9800 ; steps/s = 1.65, tokens/s = 81288 (36378 source, 44910 target) ; Learning rate = 0.000866 ; Loss = 2.019459\n",
      "2024-06-16 13:39:53.681000: I runner.py:310] Step = 9900 ; steps/s = 1.65, tokens/s = 81326 (36407 source, 44919 target) ; Learning rate = 0.000875 ; Loss = 2.051908\n",
      "2024-06-16 13:40:53.667000: I runner.py:310] Step = 10000 ; steps/s = 1.67, tokens/s = 80435 (36010 source, 44425 target) ; Learning rate = 0.000884 ; Loss = 1.989776\n",
      "2024-06-16 13:40:55.551000: I training.py:176] Saved checkpoint KK-EN-Standard-Transformer/ckpt-10000\n",
      "2024-06-16 13:40:55.551000: I training.py:192] Running evaluation for step 10000\n",
      "2024-06-16 13:48:09.711000: I training.py:192] Evaluation result for step 10000: loss = 1.243869 ; perplexity = 3.469010\n",
      "2024-06-16 13:49:10.014000: I runner.py:310] Step = 10100 ; steps/s = 1.66, tokens/s = 81577 (36502 source, 45075 target) ; Learning rate = 0.000879 ; Loss = 1.969388\n",
      "2024-06-16 13:50:10.461000: I runner.py:310] Step = 10200 ; steps/s = 1.65, tokens/s = 81364 (36399 source, 44965 target) ; Learning rate = 0.000875 ; Loss = 1.970573\n",
      "2024-06-16 13:51:10.902000: I runner.py:310] Step = 10300 ; steps/s = 1.65, tokens/s = 81378 (36411 source, 44967 target) ; Learning rate = 0.000871 ; Loss = 1.970368\n",
      "2024-06-16 13:52:10.965000: I runner.py:310] Step = 10400 ; steps/s = 1.67, tokens/s = 80386 (35995 source, 44391 target) ; Learning rate = 0.000867 ; Loss = 1.936026\n",
      "2024-06-16 13:53:11.453000: I runner.py:310] Step = 10500 ; steps/s = 1.65, tokens/s = 81325 (36394 source, 44931 target) ; Learning rate = 0.000863 ; Loss = 1.948237\n",
      "2024-06-16 13:54:11.987000: I runner.py:310] Step = 10600 ; steps/s = 1.65, tokens/s = 81256 (36360 source, 44896 target) ; Learning rate = 0.000858 ; Loss = 1.986107\n",
      "2024-06-16 13:55:12.455000: I runner.py:310] Step = 10700 ; steps/s = 1.65, tokens/s = 81325 (36379 source, 44946 target) ; Learning rate = 0.000854 ; Loss = 2.003726\n",
      "2024-06-16 13:56:12.545000: I runner.py:310] Step = 10800 ; steps/s = 1.66, tokens/s = 80324 (35976 source, 44348 target) ; Learning rate = 0.000850 ; Loss = 1.920172\n",
      "2024-06-16 13:57:13.082000: I runner.py:310] Step = 10900 ; steps/s = 1.65, tokens/s = 81289 (36377 source, 44912 target) ; Learning rate = 0.000847 ; Loss = 1.933081\n",
      "2024-06-16 13:58:13.586000: I runner.py:310] Step = 11000 ; steps/s = 1.65, tokens/s = 81318 (36411 source, 44907 target) ; Learning rate = 0.000843 ; Loss = 1.923260\n",
      "2024-06-16 13:59:14.056000: I runner.py:310] Step = 11100 ; steps/s = 1.65, tokens/s = 81347 (36391 source, 44956 target) ; Learning rate = 0.000839 ; Loss = 1.968800\n",
      "2024-06-16 14:00:14.144000: I runner.py:310] Step = 11200 ; steps/s = 1.66, tokens/s = 80293 (35912 source, 44381 target) ; Learning rate = 0.000835 ; Loss = 1.873735\n",
      "2024-06-16 14:01:14.665000: I runner.py:310] Step = 11300 ; steps/s = 1.65, tokens/s = 81324 (36425 source, 44899 target) ; Learning rate = 0.000831 ; Loss = 1.903431\n",
      "2024-06-16 14:02:15.210000: I runner.py:310] Step = 11400 ; steps/s = 1.65, tokens/s = 81234 (36342 source, 44892 target) ; Learning rate = 0.000828 ; Loss = 1.943070\n",
      "2024-06-16 14:03:15.789000: I runner.py:310] Step = 11500 ; steps/s = 1.65, tokens/s = 81195 (36348 source, 44847 target) ; Learning rate = 0.000824 ; Loss = 1.949282\n",
      "2024-06-16 14:04:15.859000: I runner.py:310] Step = 11600 ; steps/s = 1.66, tokens/s = 80329 (35951 source, 44378 target) ; Learning rate = 0.000821 ; Loss = 1.903911\n",
      "2024-06-16 14:05:16.425000: I runner.py:310] Step = 11700 ; steps/s = 1.65, tokens/s = 81262 (36372 source, 44890 target) ; Learning rate = 0.000817 ; Loss = 1.881248\n",
      "2024-06-16 14:06:16.901000: I runner.py:310] Step = 11800 ; steps/s = 1.65, tokens/s = 81344 (36392 source, 44952 target) ; Learning rate = 0.000814 ; Loss = 1.903054\n",
      "2024-06-16 14:07:17.398000: I runner.py:310] Step = 11900 ; steps/s = 1.65, tokens/s = 81272 (36384 source, 44888 target) ; Learning rate = 0.000810 ; Loss = 1.894537\n",
      "2024-06-16 14:08:17.445000: I runner.py:310] Step = 12000 ; steps/s = 1.67, tokens/s = 80379 (35964 source, 44415 target) ; Learning rate = 0.000807 ; Loss = 1.871065\n",
      "2024-06-16 14:09:17.924000: I runner.py:310] Step = 12100 ; steps/s = 1.65, tokens/s = 81326 (36388 source, 44938 target) ; Learning rate = 0.000803 ; Loss = 1.863512\n",
      "2024-06-16 14:10:18.446000: I runner.py:310] Step = 12200 ; steps/s = 1.65, tokens/s = 81278 (36394 source, 44884 target) ; Learning rate = 0.000800 ; Loss = 1.855314\n",
      "2024-06-16 14:11:18.495000: I runner.py:310] Step = 12300 ; steps/s = 1.67, tokens/s = 80366 (35964 source, 44402 target) ; Learning rate = 0.000797 ; Loss = 1.859897\n",
      "2024-06-16 14:12:18.883000: I runner.py:310] Step = 12400 ; steps/s = 1.66, tokens/s = 81469 (36454 source, 45015 target) ; Learning rate = 0.000794 ; Loss = 1.853334\n",
      "2024-06-16 14:13:19.338000: I runner.py:310] Step = 12500 ; steps/s = 1.65, tokens/s = 81380 (36436 source, 44944 target) ; Learning rate = 0.000791 ; Loss = 1.863058\n",
      "2024-06-16 14:14:19.790000: I runner.py:310] Step = 12600 ; steps/s = 1.65, tokens/s = 81343 (36387 source, 44956 target) ; Learning rate = 0.000787 ; Loss = 1.848405\n",
      "2024-06-16 14:15:19.837000: I runner.py:310] Step = 12700 ; steps/s = 1.67, tokens/s = 80393 (35982 source, 44411 target) ; Learning rate = 0.000784 ; Loss = 1.830827\n",
      "2024-06-16 14:16:20.361000: I runner.py:310] Step = 12800 ; steps/s = 1.65, tokens/s = 81283 (36382 source, 44901 target) ; Learning rate = 0.000781 ; Loss = 1.815836\n",
      "2024-06-16 14:17:20.894000: I runner.py:310] Step = 12900 ; steps/s = 1.65, tokens/s = 81250 (36354 source, 44896 target) ; Learning rate = 0.000778 ; Loss = 1.833556\n",
      "2024-06-16 14:18:21.359000: I runner.py:310] Step = 13000 ; steps/s = 1.65, tokens/s = 81324 (36409 source, 44915 target) ; Learning rate = 0.000775 ; Loss = 1.851980\n",
      "2024-06-16 14:19:21.418000: I runner.py:310] Step = 13100 ; steps/s = 1.67, tokens/s = 80356 (35932 source, 44424 target) ; Learning rate = 0.000772 ; Loss = 1.793612\n",
      "2024-06-16 14:20:21.948000: I runner.py:310] Step = 13200 ; steps/s = 1.65, tokens/s = 81268 (36383 source, 44885 target) ; Learning rate = 0.000769 ; Loss = 1.796840\n",
      "2024-06-16 14:21:22.453000: I runner.py:310] Step = 13300 ; steps/s = 1.65, tokens/s = 81288 (36368 source, 44920 target) ; Learning rate = 0.000766 ; Loss = 1.825986\n",
      "2024-06-16 14:22:22.908000: I runner.py:310] Step = 13400 ; steps/s = 1.65, tokens/s = 81387 (36440 source, 44947 target) ; Learning rate = 0.000764 ; Loss = 1.845020\n",
      "2024-06-16 14:23:23.029000: I runner.py:310] Step = 13500 ; steps/s = 1.66, tokens/s = 80299 (35955 source, 44344 target) ; Learning rate = 0.000761 ; Loss = 1.808000\n",
      "2024-06-16 14:24:23.518000: I runner.py:310] Step = 13600 ; steps/s = 1.65, tokens/s = 81343 (36406 source, 44937 target) ; Learning rate = 0.000758 ; Loss = 1.797658\n",
      "2024-06-16 14:25:23.996000: I runner.py:310] Step = 13700 ; steps/s = 1.65, tokens/s = 81358 (36440 source, 44918 target) ; Learning rate = 0.000755 ; Loss = 1.796967\n",
      "2024-06-16 14:26:24.475000: I runner.py:310] Step = 13800 ; steps/s = 1.65, tokens/s = 81294 (36363 source, 44931 target) ; Learning rate = 0.000752 ; Loss = 1.819358\n",
      "2024-06-16 14:27:24.534000: I runner.py:310] Step = 13900 ; steps/s = 1.67, tokens/s = 80375 (35965 source, 44410 target) ; Learning rate = 0.000750 ; Loss = 1.759976\n",
      "2024-06-16 14:28:25.147000: I runner.py:310] Step = 14000 ; steps/s = 1.65, tokens/s = 81149 (36338 source, 44811 target) ; Learning rate = 0.000747 ; Loss = 1.799375\n",
      "2024-06-16 14:29:25.676000: I runner.py:310] Step = 14100 ; steps/s = 1.65, tokens/s = 81263 (36346 source, 44917 target) ; Learning rate = 0.000744 ; Loss = 1.797839\n",
      "2024-06-16 14:30:26.193000: I runner.py:310] Step = 14200 ; steps/s = 1.65, tokens/s = 81276 (36378 source, 44898 target) ; Learning rate = 0.000742 ; Loss = 1.815591\n",
      "2024-06-16 14:31:26.200000: I runner.py:310] Step = 14300 ; steps/s = 1.67, tokens/s = 80422 (35978 source, 44444 target) ; Learning rate = 0.000739 ; Loss = 1.754915\n",
      "2024-06-16 14:32:26.758000: I runner.py:310] Step = 14400 ; steps/s = 1.65, tokens/s = 81208 (36320 source, 44888 target) ; Learning rate = 0.000737 ; Loss = 1.784574\n",
      "2024-06-16 14:33:27.271000: I runner.py:310] Step = 14500 ; steps/s = 1.65, tokens/s = 81292 (36376 source, 44916 target) ; Learning rate = 0.000734 ; Loss = 1.803296\n",
      "2024-06-16 14:34:27.345000: I runner.py:310] Step = 14600 ; steps/s = 1.66, tokens/s = 80354 (36009 source, 44345 target) ; Learning rate = 0.000731 ; Loss = 1.790583\n",
      "2024-06-16 14:35:27.786000: I runner.py:310] Step = 14700 ; steps/s = 1.65, tokens/s = 81427 (36429 source, 44998 target) ; Learning rate = 0.000729 ; Loss = 1.745998\n",
      "2024-06-16 14:36:28.269000: I runner.py:310] Step = 14800 ; steps/s = 1.65, tokens/s = 81321 (36399 source, 44922 target) ; Learning rate = 0.000727 ; Loss = 1.780962\n",
      "2024-06-16 14:37:28.802000: I runner.py:310] Step = 14900 ; steps/s = 1.65, tokens/s = 81245 (36347 source, 44898 target) ; Learning rate = 0.000724 ; Loss = 1.791031\n",
      "2024-06-16 14:38:28.793000: I runner.py:310] Step = 15000 ; steps/s = 1.67, tokens/s = 80451 (36009 source, 44442 target) ; Learning rate = 0.000722 ; Loss = 1.752822\n",
      "2024-06-16 14:38:28.794000: I training.py:192] Running evaluation for step 15000\n",
      "2024-06-16 14:43:46.173000: I training.py:192] Evaluation result for step 15000: loss = 1.252727 ; perplexity = 3.499874\n",
      "2024-06-16 14:44:46.522000: I runner.py:310] Step = 15100 ; steps/s = 1.66, tokens/s = 81479 (36452 source, 45027 target) ; Learning rate = 0.000719 ; Loss = 1.759871\n",
      "2024-06-16 14:45:47.054000: I runner.py:310] Step = 15200 ; steps/s = 1.65, tokens/s = 81299 (36373 source, 44926 target) ; Learning rate = 0.000717 ; Loss = 1.740372\n",
      "2024-06-16 14:46:47.594000: I runner.py:310] Step = 15300 ; steps/s = 1.65, tokens/s = 81248 (36375 source, 44873 target) ; Learning rate = 0.000715 ; Loss = 1.767864\n",
      "2024-06-16 14:47:47.666000: I runner.py:310] Step = 15400 ; steps/s = 1.66, tokens/s = 80344 (35969 source, 44375 target) ; Learning rate = 0.000712 ; Loss = 1.755202\n",
      "2024-06-16 14:48:48.228000: I runner.py:310] Step = 15500 ; steps/s = 1.65, tokens/s = 81204 (36333 source, 44871 target) ; Learning rate = 0.000710 ; Loss = 1.730067\n",
      "2024-06-16 14:49:48.747000: I runner.py:310] Step = 15600 ; steps/s = 1.65, tokens/s = 81309 (36418 source, 44891 target) ; Learning rate = 0.000708 ; Loss = 1.745999\n",
      "2024-06-16 14:50:49.236000: I runner.py:310] Step = 15700 ; steps/s = 1.65, tokens/s = 81325 (36395 source, 44930 target) ; Learning rate = 0.000705 ; Loss = 1.744876\n",
      "2024-06-16 14:51:49.232000: I runner.py:310] Step = 15800 ; steps/s = 1.67, tokens/s = 80451 (35983 source, 44468 target) ; Learning rate = 0.000703 ; Loss = 1.708856\n",
      "2024-06-16 14:52:49.717000: I runner.py:310] Step = 15900 ; steps/s = 1.65, tokens/s = 81305 (36407 source, 44898 target) ; Learning rate = 0.000701 ; Loss = 1.729684\n",
      "2024-06-16 14:53:50.158000: I runner.py:310] Step = 16000 ; steps/s = 1.65, tokens/s = 81407 (36423 source, 44984 target) ; Learning rate = 0.000699 ; Loss = 1.749497\n",
      "2024-06-16 14:54:50.636000: I runner.py:310] Step = 16100 ; steps/s = 1.65, tokens/s = 81315 (36394 source, 44921 target) ; Learning rate = 0.000697 ; Loss = 1.760685\n",
      "2024-06-16 14:55:50.676000: I runner.py:310] Step = 16200 ; steps/s = 1.67, tokens/s = 80401 (35990 source, 44411 target) ; Learning rate = 0.000694 ; Loss = 1.714937\n",
      "2024-06-16 14:56:51.178000: I runner.py:310] Step = 16300 ; steps/s = 1.65, tokens/s = 81319 (36405 source, 44914 target) ; Learning rate = 0.000692 ; Loss = 1.745729\n",
      "2024-06-16 14:57:51.665000: I runner.py:310] Step = 16400 ; steps/s = 1.65, tokens/s = 81346 (36404 source, 44942 target) ; Learning rate = 0.000690 ; Loss = 1.740984\n",
      "2024-06-16 14:58:52.167000: I runner.py:310] Step = 16500 ; steps/s = 1.65, tokens/s = 81286 (36369 source, 44917 target) ; Learning rate = 0.000688 ; Loss = 1.755552\n",
      "2024-06-16 14:59:52.241000: I runner.py:310] Step = 16600 ; steps/s = 1.66, tokens/s = 80378 (35977 source, 44401 target) ; Learning rate = 0.000686 ; Loss = 1.692996\n",
      "2024-06-16 15:00:52.705000: I runner.py:310] Step = 16700 ; steps/s = 1.65, tokens/s = 81346 (36413 source, 44933 target) ; Learning rate = 0.000684 ; Loss = 1.724820\n",
      "2024-06-16 15:01:53.220000: I runner.py:310] Step = 16800 ; steps/s = 1.65, tokens/s = 81250 (36343 source, 44907 target) ; Learning rate = 0.000682 ; Loss = 1.742535\n",
      "2024-06-16 15:02:53.594000: I runner.py:310] Step = 16900 ; steps/s = 1.66, tokens/s = 80975 (36246 source, 44729 target) ; Learning rate = 0.000680 ; Loss = 1.775646\n",
      "2024-06-16 15:03:53.716000: I runner.py:310] Step = 17000 ; steps/s = 1.66, tokens/s = 80808 (36158 source, 44650 target) ; Learning rate = 0.000678 ; Loss = 1.713229\n",
      "2024-06-16 15:04:54.212000: I runner.py:310] Step = 17100 ; steps/s = 1.65, tokens/s = 81314 (36406 source, 44908 target) ; Learning rate = 0.000676 ; Loss = 1.703542\n",
      "2024-06-16 15:05:54.686000: I runner.py:310] Step = 17200 ; steps/s = 1.65, tokens/s = 81326 (36386 source, 44940 target) ; Learning rate = 0.000674 ; Loss = 1.712175\n",
      "2024-06-16 15:06:54.720000: I runner.py:310] Step = 17300 ; steps/s = 1.67, tokens/s = 80390 (35969 source, 44421 target) ; Learning rate = 0.000672 ; Loss = 1.719680\n",
      "2024-06-16 15:07:55.236000: I runner.py:310] Step = 17400 ; steps/s = 1.65, tokens/s = 81304 (36407 source, 44897 target) ; Learning rate = 0.000670 ; Loss = 1.687299\n",
      "2024-06-16 15:08:55.733000: I runner.py:310] Step = 17500 ; steps/s = 1.65, tokens/s = 81324 (36396 source, 44928 target) ; Learning rate = 0.000668 ; Loss = 1.712303\n",
      "2024-06-16 15:09:56.209000: I runner.py:310] Step = 17600 ; steps/s = 1.65, tokens/s = 81329 (36400 source, 44929 target) ; Learning rate = 0.000666 ; Loss = 1.723075\n",
      "2024-06-16 15:10:56.275000: I runner.py:310] Step = 17700 ; steps/s = 1.67, tokens/s = 80334 (35943 source, 44391 target) ; Learning rate = 0.000664 ; Loss = 1.679662\n",
      "2024-06-16 15:11:56.803000: I runner.py:310] Step = 17800 ; steps/s = 1.65, tokens/s = 81266 (36361 source, 44905 target) ; Learning rate = 0.000662 ; Loss = 1.671207\n",
      "2024-06-16 15:12:57.261000: I runner.py:310] Step = 17900 ; steps/s = 1.65, tokens/s = 81346 (36393 source, 44953 target) ; Learning rate = 0.000661 ; Loss = 1.707268\n",
      "2024-06-16 15:13:57.748000: I runner.py:310] Step = 18000 ; steps/s = 1.65, tokens/s = 81324 (36412 source, 44912 target) ; Learning rate = 0.000659 ; Loss = 1.715805\n",
      "2024-06-16 15:14:57.793000: I runner.py:310] Step = 18100 ; steps/s = 1.67, tokens/s = 80399 (35971 source, 44428 target) ; Learning rate = 0.000657 ; Loss = 1.702263\n",
      "2024-06-16 15:15:58.282000: I runner.py:310] Step = 18200 ; steps/s = 1.65, tokens/s = 81355 (36438 source, 44917 target) ; Learning rate = 0.000655 ; Loss = 1.682875\n",
      "2024-06-16 15:16:58.835000: I runner.py:310] Step = 18300 ; steps/s = 1.65, tokens/s = 81238 (36333 source, 44905 target) ; Learning rate = 0.000653 ; Loss = 1.687632\n",
      "2024-06-16 15:17:59.240000: I runner.py:310] Step = 18400 ; steps/s = 1.66, tokens/s = 81426 (36452 source, 44974 target) ; Learning rate = 0.000652 ; Loss = 1.702829\n",
      "2024-06-16 15:18:59.298000: I runner.py:310] Step = 18500 ; steps/s = 1.67, tokens/s = 80362 (35967 source, 44395 target) ; Learning rate = 0.000650 ; Loss = 1.693687\n",
      "2024-06-16 15:19:59.791000: I runner.py:310] Step = 18600 ; steps/s = 1.65, tokens/s = 81338 (36386 source, 44952 target) ; Learning rate = 0.000648 ; Loss = 1.680008\n",
      "2024-06-16 15:21:00.353000: I runner.py:310] Step = 18700 ; steps/s = 1.65, tokens/s = 81182 (36342 source, 44840 target) ; Learning rate = 0.000646 ; Loss = 1.693630\n",
      "2024-06-16 15:22:00.855000: I runner.py:310] Step = 18800 ; steps/s = 1.65, tokens/s = 81304 (36379 source, 44925 target) ; Learning rate = 0.000645 ; Loss = 1.689884\n",
      "2024-06-16 15:23:00.865000: I runner.py:310] Step = 18900 ; steps/s = 1.67, tokens/s = 80446 (36012 source, 44434 target) ; Learning rate = 0.000643 ; Loss = 1.686975\n",
      "2024-06-16 15:24:01.342000: I runner.py:310] Step = 19000 ; steps/s = 1.65, tokens/s = 81341 (36421 source, 44920 target) ; Learning rate = 0.000641 ; Loss = 1.672273\n",
      "2024-06-16 15:25:01.877000: I runner.py:310] Step = 19100 ; steps/s = 1.65, tokens/s = 81251 (36361 source, 44890 target) ; Learning rate = 0.000640 ; Loss = 1.687759\n",
      "2024-06-16 15:26:02.299000: I runner.py:310] Step = 19200 ; steps/s = 1.66, tokens/s = 81412 (36421 source, 44991 target) ; Learning rate = 0.000638 ; Loss = 1.685318\n",
      "2024-06-16 15:27:02.411000: I runner.py:310] Step = 19300 ; steps/s = 1.66, tokens/s = 80309 (35933 source, 44376 target) ; Learning rate = 0.000636 ; Loss = 1.657894\n",
      "2024-06-16 15:28:02.888000: I runner.py:310] Step = 19400 ; steps/s = 1.65, tokens/s = 81301 (36375 source, 44926 target) ; Learning rate = 0.000635 ; Loss = 1.678018\n",
      "2024-06-16 15:29:03.417000: I runner.py:310] Step = 19500 ; steps/s = 1.65, tokens/s = 81296 (36406 source, 44890 target) ; Learning rate = 0.000633 ; Loss = 1.682846\n",
      "2024-06-16 15:30:03.397000: I runner.py:310] Step = 19600 ; steps/s = 1.67, tokens/s = 80465 (36010 source, 44455 target) ; Learning rate = 0.000631 ; Loss = 1.669868\n",
      "2024-06-16 15:31:03.941000: I runner.py:310] Step = 19700 ; steps/s = 1.65, tokens/s = 81250 (36334 source, 44916 target) ; Learning rate = 0.000630 ; Loss = 1.674447\n",
      "2024-06-16 15:32:04.468000: I runner.py:310] Step = 19800 ; steps/s = 1.65, tokens/s = 81262 (36331 source, 44931 target) ; Learning rate = 0.000628 ; Loss = 1.675332\n",
      "2024-06-16 15:33:04.950000: I runner.py:310] Step = 19900 ; steps/s = 1.65, tokens/s = 81324 (36428 source, 44896 target) ; Learning rate = 0.000627 ; Loss = 1.669353\n",
      "2024-06-16 15:34:04.978000: I runner.py:310] Step = 20000 ; steps/s = 1.67, tokens/s = 80381 (35986 source, 44395 target) ; Learning rate = 0.000625 ; Loss = 1.666277\n",
      "2024-06-16 15:34:06.843000: I training.py:176] Saved checkpoint KK-EN-Standard-Transformer/ckpt-20000\n",
      "2024-06-16 15:34:06.843000: I training.py:192] Running evaluation for step 20000\n",
      "2024-06-16 15:38:49.646000: I training.py:192] Evaluation result for step 20000: loss = 1.296132 ; perplexity = 3.655131\n",
      "2024-06-16 15:39:49.963000: I runner.py:310] Step = 20100 ; steps/s = 1.66, tokens/s = 81591 (36529 source, 45062 target) ; Learning rate = 0.000623 ; Loss = 1.660902\n",
      "2024-06-16 15:40:50.490000: I runner.py:310] Step = 20200 ; steps/s = 1.65, tokens/s = 81276 (36377 source, 44899 target) ; Learning rate = 0.000622 ; Loss = 1.661262\n",
      "2024-06-16 15:41:51.009000: I runner.py:310] Step = 20300 ; steps/s = 1.65, tokens/s = 81255 (36377 source, 44878 target) ; Learning rate = 0.000620 ; Loss = 1.662309\n",
      "2024-06-16 15:42:51.056000: I runner.py:310] Step = 20400 ; steps/s = 1.67, tokens/s = 80393 (35987 source, 44406 target) ; Learning rate = 0.000619 ; Loss = 1.645536\n",
      "2024-06-16 15:43:51.520000: I runner.py:310] Step = 20500 ; steps/s = 1.65, tokens/s = 81335 (36404 source, 44931 target) ; Learning rate = 0.000617 ; Loss = 1.654583\n",
      "2024-06-16 15:44:52.047000: I runner.py:310] Step = 20600 ; steps/s = 1.65, tokens/s = 81297 (36380 source, 44917 target) ; Learning rate = 0.000616 ; Loss = 1.666573\n",
      "2024-06-16 15:45:52.526000: I runner.py:310] Step = 20700 ; steps/s = 1.65, tokens/s = 81343 (36402 source, 44941 target) ; Learning rate = 0.000614 ; Loss = 1.677994\n",
      "2024-06-16 15:46:52.533000: I runner.py:310] Step = 20800 ; steps/s = 1.67, tokens/s = 80432 (35989 source, 44443 target) ; Learning rate = 0.000613 ; Loss = 1.629504\n",
      "2024-06-16 15:47:53.025000: I runner.py:310] Step = 20900 ; steps/s = 1.65, tokens/s = 81336 (36414 source, 44922 target) ; Learning rate = 0.000611 ; Loss = 1.651551\n",
      "2024-06-16 15:48:53.558000: I runner.py:310] Step = 21000 ; steps/s = 1.65, tokens/s = 81283 (36367 source, 44916 target) ; Learning rate = 0.000610 ; Loss = 1.661919\n",
      "2024-06-16 15:49:54.072000: I runner.py:310] Step = 21100 ; steps/s = 1.65, tokens/s = 81270 (36372 source, 44898 target) ; Learning rate = 0.000608 ; Loss = 1.666279\n",
      "2024-06-16 15:50:54.160000: I runner.py:310] Step = 21200 ; steps/s = 1.66, tokens/s = 80324 (35943 source, 44381 target) ; Learning rate = 0.000607 ; Loss = 1.639183\n",
      "2024-06-16 15:51:54.714000: I runner.py:310] Step = 21300 ; steps/s = 1.65, tokens/s = 81234 (36363 source, 44871 target) ; Learning rate = 0.000606 ; Loss = 1.652266\n",
      "2024-06-16 15:52:55.213000: I runner.py:310] Step = 21400 ; steps/s = 1.65, tokens/s = 81315 (36380 source, 44935 target) ; Learning rate = 0.000604 ; Loss = 1.654004\n",
      "2024-06-16 15:53:55.706000: I runner.py:310] Step = 21500 ; steps/s = 1.65, tokens/s = 81305 (36390 source, 44915 target) ; Learning rate = 0.000603 ; Loss = 1.658770\n",
      "2024-06-16 15:54:55.751000: I runner.py:310] Step = 21600 ; steps/s = 1.67, tokens/s = 80344 (35938 source, 44406 target) ; Learning rate = 0.000601 ; Loss = 1.627082\n",
      "2024-06-16 15:55:56.261000: I runner.py:310] Step = 21700 ; steps/s = 1.65, tokens/s = 81329 (36391 source, 44938 target) ; Learning rate = 0.000600 ; Loss = 1.649209\n",
      "2024-06-16 15:56:56.727000: I runner.py:310] Step = 21800 ; steps/s = 1.65, tokens/s = 81363 (36446 source, 44917 target) ; Learning rate = 0.000599 ; Loss = 1.667422\n",
      "2024-06-16 15:57:56.868000: I runner.py:310] Step = 21900 ; steps/s = 1.66, tokens/s = 80230 (35909 source, 44321 target) ; Learning rate = 0.000597 ; Loss = 1.639289\n",
      "2024-06-16 15:58:57.355000: I runner.py:310] Step = 22000 ; steps/s = 1.65, tokens/s = 81352 (36432 source, 44920 target) ; Learning rate = 0.000596 ; Loss = 1.626034\n",
      "2024-06-16 15:59:57.895000: I runner.py:310] Step = 22100 ; steps/s = 1.65, tokens/s = 81273 (36347 source, 44926 target) ; Learning rate = 0.000595 ; Loss = 1.641705\n",
      "2024-06-16 16:00:58.388000: I runner.py:310] Step = 22200 ; steps/s = 1.65, tokens/s = 81281 (36371 source, 44910 target) ; Learning rate = 0.000593 ; Loss = 1.650023\n",
      "2024-06-16 16:01:58.413000: I runner.py:310] Step = 22300 ; steps/s = 1.67, tokens/s = 80419 (35996 source, 44423 target) ; Learning rate = 0.000592 ; Loss = 1.629049\n",
      "2024-06-16 16:02:58.951000: I runner.py:310] Step = 22400 ; steps/s = 1.65, tokens/s = 81249 (36354 source, 44895 target) ; Learning rate = 0.000591 ; Loss = 1.635507\n",
      "2024-06-16 16:03:59.463000: I runner.py:310] Step = 22500 ; steps/s = 1.65, tokens/s = 81281 (36377 source, 44904 target) ; Learning rate = 0.000589 ; Loss = 1.640514\n",
      "2024-06-16 16:04:59.991000: I runner.py:310] Step = 22600 ; steps/s = 1.65, tokens/s = 81268 (36388 source, 44880 target) ; Learning rate = 0.000588 ; Loss = 1.649089\n",
      "2024-06-16 16:06:00.055000: I runner.py:310] Step = 22700 ; steps/s = 1.67, tokens/s = 80364 (35952 source, 44412 target) ; Learning rate = 0.000587 ; Loss = 1.649026\n",
      "2024-06-16 16:07:00.561000: I runner.py:310] Step = 22800 ; steps/s = 1.65, tokens/s = 81297 (36355 source, 44942 target) ; Learning rate = 0.000585 ; Loss = 1.636388\n",
      "2024-06-16 16:08:01.002000: I runner.py:310] Step = 22900 ; steps/s = 1.65, tokens/s = 81391 (36415 source, 44976 target) ; Learning rate = 0.000584 ; Loss = 1.636264\n",
      "2024-06-16 16:09:01.540000: I runner.py:310] Step = 23000 ; steps/s = 1.65, tokens/s = 81236 (36396 source, 44840 target) ; Learning rate = 0.000583 ; Loss = 1.646582\n",
      "2024-06-16 16:10:01.545000: I runner.py:310] Step = 23100 ; steps/s = 1.67, tokens/s = 80439 (35986 source, 44453 target) ; Learning rate = 0.000582 ; Loss = 1.617865\n",
      "2024-06-16 16:11:02.013000: I runner.py:310] Step = 23200 ; steps/s = 1.65, tokens/s = 81317 (36397 source, 44920 target) ; Learning rate = 0.000580 ; Loss = 1.632511\n",
      "2024-06-16 16:12:02.528000: I runner.py:310] Step = 23300 ; steps/s = 1.65, tokens/s = 81301 (36395 source, 44906 target) ; Learning rate = 0.000579 ; Loss = 1.633288\n",
      "2024-06-16 16:13:03.044000: I runner.py:310] Step = 23400 ; steps/s = 1.65, tokens/s = 81305 (36376 source, 44929 target) ; Learning rate = 0.000578 ; Loss = 1.654003\n",
      "2024-06-16 16:14:03.056000: I runner.py:310] Step = 23500 ; steps/s = 1.67, tokens/s = 80431 (35993 source, 44438 target) ; Learning rate = 0.000577 ; Loss = 1.642799\n",
      "2024-06-16 16:15:03.542000: I runner.py:310] Step = 23600 ; steps/s = 1.65, tokens/s = 81338 (36421 source, 44917 target) ; Learning rate = 0.000575 ; Loss = 1.620155\n",
      "2024-06-16 16:16:04.068000: I runner.py:310] Step = 23700 ; steps/s = 1.65, tokens/s = 81277 (36361 source, 44916 target) ; Learning rate = 0.000574 ; Loss = 1.637484\n",
      "2024-06-16 16:17:04.606000: I runner.py:310] Step = 23800 ; steps/s = 1.65, tokens/s = 81229 (36369 source, 44860 target) ; Learning rate = 0.000573 ; Loss = 1.632588\n",
      "2024-06-16 16:18:04.636000: I runner.py:310] Step = 23900 ; steps/s = 1.67, tokens/s = 80409 (35967 source, 44442 target) ; Learning rate = 0.000572 ; Loss = 1.631337\n",
      "2024-06-16 16:19:05.152000: I runner.py:310] Step = 24000 ; steps/s = 1.65, tokens/s = 81290 (36383 source, 44907 target) ; Learning rate = 0.000571 ; Loss = 1.616097\n",
      "2024-06-16 16:20:05.586000: I runner.py:310] Step = 24100 ; steps/s = 1.65, tokens/s = 81390 (36416 source, 44974 target) ; Learning rate = 0.000569 ; Loss = 1.630100\n",
      "2024-06-16 16:21:05.909000: I runner.py:310] Step = 24200 ; steps/s = 1.66, tokens/s = 80523 (36061 source, 44462 target) ; Learning rate = 0.000568 ; Loss = 1.630617\n",
      "2024-06-16 16:22:06.190000: I runner.py:310] Step = 24300 ; steps/s = 1.66, tokens/s = 81109 (36292 source, 44817 target) ; Learning rate = 0.000567 ; Loss = 1.619577\n",
      "2024-06-16 16:23:06.734000: I runner.py:310] Step = 24400 ; steps/s = 1.65, tokens/s = 81230 (36352 source, 44878 target) ; Learning rate = 0.000566 ; Loss = 1.605632\n",
      "2024-06-16 16:24:07.278000: I runner.py:310] Step = 24500 ; steps/s = 1.65, tokens/s = 81211 (36353 source, 44858 target) ; Learning rate = 0.000565 ; Loss = 1.622962\n",
      "2024-06-16 16:25:07.312000: I runner.py:310] Step = 24600 ; steps/s = 1.67, tokens/s = 80404 (35991 source, 44413 target) ; Learning rate = 0.000564 ; Loss = 1.612740\n",
      "2024-06-16 16:26:07.810000: I runner.py:310] Step = 24700 ; steps/s = 1.65, tokens/s = 81299 (36371 source, 44928 target) ; Learning rate = 0.000562 ; Loss = 1.608121\n",
      "2024-06-16 16:27:08.360000: I runner.py:310] Step = 24800 ; steps/s = 1.65, tokens/s = 81238 (36350 source, 44888 target) ; Learning rate = 0.000561 ; Loss = 1.630339\n",
      "2024-06-16 16:28:08.822000: I runner.py:310] Step = 24900 ; steps/s = 1.65, tokens/s = 81379 (36436 source, 44943 target) ; Learning rate = 0.000560 ; Loss = 1.641187\n",
      "2024-06-16 16:29:08.880000: I runner.py:310] Step = 25000 ; steps/s = 1.67, tokens/s = 80367 (35964 source, 44403 target) ; Learning rate = 0.000559 ; Loss = 1.612076\n",
      "2024-06-16 16:29:08.882000: I training.py:192] Running evaluation for step 25000\n",
      "2024-06-16 16:33:34.991000: I training.py:192] Evaluation result for step 25000: loss = 1.321822 ; perplexity = 3.750247\n",
      "2024-06-16 16:34:35.319000: I runner.py:310] Step = 25100 ; steps/s = 1.66, tokens/s = 81583 (36509 source, 45074 target) ; Learning rate = 0.000558 ; Loss = 1.609928\n",
      "2024-06-16 16:35:35.835000: I runner.py:310] Step = 25200 ; steps/s = 1.65, tokens/s = 81258 (36345 source, 44913 target) ; Learning rate = 0.000557 ; Loss = 1.613164\n",
      "2024-06-16 16:36:36.341000: I runner.py:310] Step = 25300 ; steps/s = 1.65, tokens/s = 81288 (36412 source, 44876 target) ; Learning rate = 0.000556 ; Loss = 1.614111\n",
      "2024-06-16 16:37:36.381000: I runner.py:310] Step = 25400 ; steps/s = 1.67, tokens/s = 80420 (35980 source, 44440 target) ; Learning rate = 0.000555 ; Loss = 1.616224\n",
      "2024-06-16 16:38:36.937000: I runner.py:310] Step = 25500 ; steps/s = 1.65, tokens/s = 81230 (36352 source, 44878 target) ; Learning rate = 0.000553 ; Loss = 1.604581\n",
      "2024-06-16 16:39:37.492000: I runner.py:310] Step = 25600 ; steps/s = 1.65, tokens/s = 81221 (36332 source, 44889 target) ; Learning rate = 0.000552 ; Loss = 1.609414\n",
      "2024-06-16 16:40:38.008000: I runner.py:310] Step = 25700 ; steps/s = 1.65, tokens/s = 81275 (36395 source, 44880 target) ; Learning rate = 0.000551 ; Loss = 1.623483\n",
      "2024-06-16 16:41:38.067000: I runner.py:310] Step = 25800 ; steps/s = 1.67, tokens/s = 80351 (35959 source, 44392 target) ; Learning rate = 0.000550 ; Loss = 1.626976\n",
      "2024-06-16 16:42:38.549000: I runner.py:310] Step = 25900 ; steps/s = 1.65, tokens/s = 81324 (36414 source, 44910 target) ; Learning rate = 0.000549 ; Loss = 1.602319\n",
      "2024-06-16 16:43:39.082000: I runner.py:310] Step = 26000 ; steps/s = 1.65, tokens/s = 81251 (36352 source, 44899 target) ; Learning rate = 0.000548 ; Loss = 1.607955\n",
      "2024-06-16 16:44:39.590000: I runner.py:310] Step = 26100 ; steps/s = 1.65, tokens/s = 81320 (36386 source, 44934 target) ; Learning rate = 0.000547 ; Loss = 1.615706\n",
      "2024-06-16 16:45:39.586000: I runner.py:310] Step = 26200 ; steps/s = 1.67, tokens/s = 80438 (35996 source, 44442 target) ; Learning rate = 0.000546 ; Loss = 1.585600\n",
      "2024-06-16 16:46:40.099000: I runner.py:310] Step = 26300 ; steps/s = 1.65, tokens/s = 81266 (36357 source, 44909 target) ; Learning rate = 0.000545 ; Loss = 1.614671\n",
      "2024-06-16 16:47:40.596000: I runner.py:310] Step = 26400 ; steps/s = 1.65, tokens/s = 81362 (36434 source, 44928 target) ; Learning rate = 0.000544 ; Loss = 1.609884\n",
      "2024-06-16 16:48:41.107000: I runner.py:310] Step = 26500 ; steps/s = 1.65, tokens/s = 81279 (36393 source, 44886 target) ; Learning rate = 0.000543 ; Loss = 1.619282\n",
      "2024-06-16 16:49:41.100000: I runner.py:310] Step = 26600 ; steps/s = 1.67, tokens/s = 80441 (35967 source, 44474 target) ; Learning rate = 0.000542 ; Loss = 1.599677\n",
      "2024-06-16 16:50:41.607000: I runner.py:310] Step = 26700 ; steps/s = 1.65, tokens/s = 81311 (36393 source, 44918 target) ; Learning rate = 0.000541 ; Loss = 1.607528\n",
      "2024-06-16 16:51:42.062000: I runner.py:310] Step = 26800 ; steps/s = 1.65, tokens/s = 81360 (36432 source, 44928 target) ; Learning rate = 0.000540 ; Loss = 1.611270\n",
      "2024-06-16 16:52:42.158000: I runner.py:310] Step = 26900 ; steps/s = 1.66, tokens/s = 80319 (35947 source, 44372 target) ; Learning rate = 0.000539 ; Loss = 1.598074\n",
      "2024-06-16 16:53:42.653000: I runner.py:310] Step = 27000 ; steps/s = 1.65, tokens/s = 81327 (36401 source, 44926 target) ; Learning rate = 0.000538 ; Loss = 1.589944\n",
      "2024-06-16 16:54:43.245000: I runner.py:310] Step = 27100 ; steps/s = 1.65, tokens/s = 81148 (36303 source, 44845 target) ; Learning rate = 0.000537 ; Loss = 1.618063\n",
      "2024-06-16 16:55:43.672000: I runner.py:310] Step = 27200 ; steps/s = 1.66, tokens/s = 81436 (36450 source, 44986 target) ; Learning rate = 0.000536 ; Loss = 1.615652\n",
      "2024-06-16 16:56:43.758000: I runner.py:310] Step = 27300 ; steps/s = 1.66, tokens/s = 80317 (35943 source, 44374 target) ; Learning rate = 0.000535 ; Loss = 1.589742\n",
      "2024-06-16 16:57:44.270000: I runner.py:310] Step = 27400 ; steps/s = 1.65, tokens/s = 81291 (36383 source, 44908 target) ; Learning rate = 0.000534 ; Loss = 1.590158\n",
      "2024-06-16 16:58:44.820000: I runner.py:310] Step = 27500 ; steps/s = 1.65, tokens/s = 81231 (36354 source, 44877 target) ; Learning rate = 0.000533 ; Loss = 1.604859\n",
      "2024-06-16 16:59:45.369000: I runner.py:310] Step = 27600 ; steps/s = 1.65, tokens/s = 81252 (36374 source, 44878 target) ; Learning rate = 0.000532 ; Loss = 1.606296\n",
      "2024-06-16 17:00:45.391000: I runner.py:310] Step = 27700 ; steps/s = 1.67, tokens/s = 80406 (35982 source, 44424 target) ; Learning rate = 0.000531 ; Loss = 1.595226\n",
      "2024-06-16 17:01:45.862000: I runner.py:310] Step = 27800 ; steps/s = 1.65, tokens/s = 81351 (36410 source, 44941 target) ; Learning rate = 0.000530 ; Loss = 1.590338\n",
      "2024-06-16 17:02:46.355000: I runner.py:310] Step = 27900 ; steps/s = 1.65, tokens/s = 81309 (36385 source, 44924 target) ; Learning rate = 0.000529 ; Loss = 1.605696\n",
      "2024-06-16 17:03:46.856000: I runner.py:310] Step = 28000 ; steps/s = 1.65, tokens/s = 81315 (36398 source, 44917 target) ; Learning rate = 0.000528 ; Loss = 1.596565\n",
      "2024-06-16 17:04:46.885000: I runner.py:310] Step = 28100 ; steps/s = 1.67, tokens/s = 80390 (35966 source, 44424 target) ; Learning rate = 0.000527 ; Loss = 1.608771\n",
      "2024-06-16 17:05:47.383000: I runner.py:310] Step = 28200 ; steps/s = 1.65, tokens/s = 81303 (36372 source, 44931 target) ; Learning rate = 0.000526 ; Loss = 1.596504\n",
      "2024-06-16 17:06:47.865000: I runner.py:310] Step = 28300 ; steps/s = 1.65, tokens/s = 81349 (36416 source, 44933 target) ; Learning rate = 0.000525 ; Loss = 1.592218\n",
      "2024-06-16 17:07:48.312000: I runner.py:310] Step = 28400 ; steps/s = 1.65, tokens/s = 81365 (36425 source, 44940 target) ; Learning rate = 0.000524 ; Loss = 1.600953\n",
      "2024-06-16 17:08:48.403000: I runner.py:310] Step = 28500 ; steps/s = 1.66, tokens/s = 80363 (35960 source, 44403 target) ; Learning rate = 0.000524 ; Loss = 1.595770\n",
      "2024-06-16 17:09:48.913000: I runner.py:310] Step = 28600 ; steps/s = 1.65, tokens/s = 81283 (36363 source, 44920 target) ; Learning rate = 0.000523 ; Loss = 1.582336\n",
      "2024-06-16 17:10:49.476000: I runner.py:310] Step = 28700 ; steps/s = 1.65, tokens/s = 81217 (36358 source, 44859 target) ; Learning rate = 0.000522 ; Loss = 1.591551\n",
      "2024-06-16 17:11:49.977000: I runner.py:310] Step = 28800 ; steps/s = 1.65, tokens/s = 81299 (36398 source, 44901 target) ; Learning rate = 0.000521 ; Loss = 1.589950\n",
      "2024-06-16 17:12:50.085000: I runner.py:310] Step = 28900 ; steps/s = 1.66, tokens/s = 80318 (35957 source, 44361 target) ; Learning rate = 0.000520 ; Loss = 1.585801\n",
      "2024-06-16 17:13:50.611000: I runner.py:310] Step = 29000 ; steps/s = 1.65, tokens/s = 81254 (36349 source, 44905 target) ; Learning rate = 0.000519 ; Loss = 1.581831\n",
      "2024-06-16 17:14:51.120000: I runner.py:310] Step = 29100 ; steps/s = 1.65, tokens/s = 81281 (36368 source, 44913 target) ; Learning rate = 0.000518 ; Loss = 1.596202\n",
      "2024-06-16 17:15:51.134000: I runner.py:310] Step = 29200 ; steps/s = 1.67, tokens/s = 80440 (36016 source, 44424 target) ; Learning rate = 0.000517 ; Loss = 1.592509\n",
      "2024-06-16 17:16:51.595000: I runner.py:310] Step = 29300 ; steps/s = 1.65, tokens/s = 81377 (36409 source, 44968 target) ; Learning rate = 0.000516 ; Loss = 1.577324\n",
      "2024-06-16 17:17:52.131000: I runner.py:310] Step = 29400 ; steps/s = 1.65, tokens/s = 81251 (36372 source, 44879 target) ; Learning rate = 0.000515 ; Loss = 1.589469\n",
      "2024-06-16 17:18:52.602000: I runner.py:310] Step = 29500 ; steps/s = 1.65, tokens/s = 81343 (36412 source, 44931 target) ; Learning rate = 0.000515 ; Loss = 1.612969\n",
      "2024-06-16 17:19:52.701000: I runner.py:310] Step = 29600 ; steps/s = 1.66, tokens/s = 80308 (35926 source, 44382 target) ; Learning rate = 0.000514 ; Loss = 1.584469\n",
      "2024-06-16 17:20:53.105000: I runner.py:310] Step = 29700 ; steps/s = 1.66, tokens/s = 81433 (36439 source, 44994 target) ; Learning rate = 0.000513 ; Loss = 1.581089\n",
      "2024-06-16 17:21:53.557000: I runner.py:310] Step = 29800 ; steps/s = 1.65, tokens/s = 81362 (36395 source, 44967 target) ; Learning rate = 0.000512 ; Loss = 1.587433\n",
      "2024-06-16 17:22:54.089000: I runner.py:310] Step = 29900 ; steps/s = 1.65, tokens/s = 81274 (36402 source, 44872 target) ; Learning rate = 0.000511 ; Loss = 1.598366\n",
      "2024-06-16 17:23:54.140000: I runner.py:310] Step = 30000 ; steps/s = 1.67, tokens/s = 80369 (35960 source, 44409 target) ; Learning rate = 0.000510 ; Loss = 1.581486\n",
      "2024-06-16 17:23:56.077000: I training.py:176] Saved checkpoint KK-EN-Standard-Transformer/ckpt-30000\n",
      "2024-06-16 17:23:56.078000: I training.py:192] Running evaluation for step 30000\n",
      "2024-06-16 17:28:30.014000: I training.py:192] Evaluation result for step 30000: loss = 1.346301 ; perplexity = 3.843185\n",
      "2024-06-16 17:29:30.361000: I runner.py:310] Step = 30100 ; steps/s = 1.66, tokens/s = 81564 (36502 source, 45062 target) ; Learning rate = 0.000509 ; Loss = 1.578601\n",
      "2024-06-16 17:30:30.854000: I runner.py:310] Step = 30200 ; steps/s = 1.65, tokens/s = 81335 (36400 source, 44935 target) ; Learning rate = 0.000509 ; Loss = 1.584336\n",
      "2024-06-16 17:31:31.354000: I runner.py:310] Step = 30300 ; steps/s = 1.65, tokens/s = 81282 (36389 source, 44893 target) ; Learning rate = 0.000508 ; Loss = 1.593307\n",
      "2024-06-16 17:32:31.393000: I runner.py:310] Step = 30400 ; steps/s = 1.67, tokens/s = 80392 (35976 source, 44416 target) ; Learning rate = 0.000507 ; Loss = 1.591770\n",
      "2024-06-16 17:33:31.908000: I runner.py:310] Step = 30500 ; steps/s = 1.65, tokens/s = 81262 (36376 source, 44886 target) ; Learning rate = 0.000506 ; Loss = 1.586142\n",
      "2024-06-16 17:34:32.397000: I runner.py:310] Step = 30600 ; steps/s = 1.65, tokens/s = 81315 (36380 source, 44935 target) ; Learning rate = 0.000505 ; Loss = 1.587307\n",
      "2024-06-16 17:35:32.947000: I runner.py:310] Step = 30700 ; steps/s = 1.65, tokens/s = 81265 (36386 source, 44879 target) ; Learning rate = 0.000504 ; Loss = 1.582071\n",
      "2024-06-16 17:36:33.071000: I runner.py:310] Step = 30800 ; steps/s = 1.66, tokens/s = 80272 (35926 source, 44346 target) ; Learning rate = 0.000504 ; Loss = 1.588024\n",
      "2024-06-16 17:37:33.539000: I runner.py:310] Step = 30900 ; steps/s = 1.65, tokens/s = 81329 (36377 source, 44952 target) ; Learning rate = 0.000503 ; Loss = 1.574770\n",
      "2024-06-16 17:38:34.023000: I runner.py:310] Step = 31000 ; steps/s = 1.65, tokens/s = 81349 (36409 source, 44940 target) ; Learning rate = 0.000502 ; Loss = 1.587899\n",
      "2024-06-16 17:39:34.588000: I runner.py:310] Step = 31100 ; steps/s = 1.65, tokens/s = 81212 (36351 source, 44861 target) ; Learning rate = 0.000501 ; Loss = 1.578398\n",
      "2024-06-16 17:40:34.675000: I runner.py:310] Step = 31200 ; steps/s = 1.66, tokens/s = 80336 (35964 source, 44372 target) ; Learning rate = 0.000500 ; Loss = 1.569407\n",
      "2024-06-16 17:41:35.158000: I runner.py:310] Step = 31300 ; steps/s = 1.65, tokens/s = 81336 (36399 source, 44937 target) ; Learning rate = 0.000500 ; Loss = 1.582106\n",
      "2024-06-16 17:42:35.638000: I runner.py:310] Step = 31400 ; steps/s = 1.65, tokens/s = 81316 (36373 source, 44943 target) ; Learning rate = 0.000499 ; Loss = 1.597799\n",
      "2024-06-16 17:43:35.697000: I runner.py:310] Step = 31500 ; steps/s = 1.67, tokens/s = 80351 (35984 source, 44367 target) ; Learning rate = 0.000498 ; Loss = 1.590292\n",
      "2024-06-16 17:44:36.207000: I runner.py:310] Step = 31600 ; steps/s = 1.65, tokens/s = 81299 (36356 source, 44943 target) ; Learning rate = 0.000497 ; Loss = 1.580151\n",
      "2024-06-16 17:45:36.707000: I runner.py:310] Step = 31700 ; steps/s = 1.65, tokens/s = 81301 (36395 source, 44906 target) ; Learning rate = 0.000496 ; Loss = 1.569760\n",
      "2024-06-16 17:46:37.213000: I runner.py:310] Step = 31800 ; steps/s = 1.65, tokens/s = 81282 (36390 source, 44892 target) ; Learning rate = 0.000496 ; Loss = 1.583656\n",
      "2024-06-16 17:47:37.250000: I runner.py:310] Step = 31900 ; steps/s = 1.67, tokens/s = 80422 (35991 source, 44431 target) ; Learning rate = 0.000495 ; Loss = 1.573277\n",
      "2024-06-16 17:48:37.799000: I runner.py:310] Step = 32000 ; steps/s = 1.65, tokens/s = 81208 (36352 source, 44856 target) ; Learning rate = 0.000494 ; Loss = 1.574242\n",
      "2024-06-16 17:49:38.323000: I runner.py:310] Step = 32100 ; steps/s = 1.65, tokens/s = 81247 (36344 source, 44903 target) ; Learning rate = 0.000493 ; Loss = 1.578254\n",
      "2024-06-16 17:50:38.862000: I runner.py:310] Step = 32200 ; steps/s = 1.65, tokens/s = 81273 (36386 source, 44887 target) ; Learning rate = 0.000493 ; Loss = 1.574088\n",
      "2024-06-16 17:51:39.019000: I runner.py:310] Step = 32300 ; steps/s = 1.66, tokens/s = 80266 (35920 source, 44346 target) ; Learning rate = 0.000492 ; Loss = 1.579626\n",
      "2024-06-16 17:52:39.444000: I runner.py:310] Step = 32400 ; steps/s = 1.66, tokens/s = 81415 (36459 source, 44956 target) ; Learning rate = 0.000491 ; Loss = 1.572754\n",
      "2024-06-16 17:53:39.985000: I runner.py:310] Step = 32500 ; steps/s = 1.65, tokens/s = 81217 (36346 source, 44871 target) ; Learning rate = 0.000490 ; Loss = 1.570622\n",
      "2024-06-16 17:54:40.486000: I runner.py:310] Step = 32600 ; steps/s = 1.65, tokens/s = 81336 (36419 source, 44917 target) ; Learning rate = 0.000490 ; Loss = 1.580855\n",
      "2024-06-16 17:55:40.530000: I runner.py:310] Step = 32700 ; steps/s = 1.67, tokens/s = 80360 (35908 source, 44452 target) ; Learning rate = 0.000489 ; Loss = 1.565244\n",
      "2024-06-16 17:56:41.034000: I runner.py:310] Step = 32800 ; steps/s = 1.65, tokens/s = 81349 (36425 source, 44924 target) ; Learning rate = 0.000488 ; Loss = 1.571258\n",
      "2024-06-16 17:57:41.490000: I runner.py:310] Step = 32900 ; steps/s = 1.65, tokens/s = 81344 (36403 source, 44941 target) ; Learning rate = 0.000487 ; Loss = 1.572634\n",
      "2024-06-16 17:58:42.012000: I runner.py:310] Step = 33000 ; steps/s = 1.65, tokens/s = 81278 (36385 source, 44893 target) ; Learning rate = 0.000487 ; Loss = 1.581713\n",
      "2024-06-16 17:59:42.021000: I runner.py:310] Step = 33100 ; steps/s = 1.67, tokens/s = 80420 (35971 source, 44449 target) ; Learning rate = 0.000486 ; Loss = 1.572834\n",
      "2024-06-16 18:00:42.553000: I runner.py:310] Step = 33200 ; steps/s = 1.65, tokens/s = 81289 (36363 source, 44926 target) ; Learning rate = 0.000485 ; Loss = 1.571199\n",
      "2024-06-16 18:01:43.029000: I runner.py:310] Step = 33300 ; steps/s = 1.65, tokens/s = 81316 (36405 source, 44911 target) ; Learning rate = 0.000484 ; Loss = 1.562929\n",
      "2024-06-16 18:02:43.522000: I runner.py:310] Step = 33400 ; steps/s = 1.65, tokens/s = 81311 (36401 source, 44910 target) ; Learning rate = 0.000484 ; Loss = 1.576138\n",
      "2024-06-16 18:03:43.552000: I runner.py:310] Step = 33500 ; steps/s = 1.67, tokens/s = 80377 (35956 source, 44421 target) ; Learning rate = 0.000483 ; Loss = 1.559416\n",
      "2024-06-16 18:04:44.061000: I runner.py:310] Step = 33600 ; steps/s = 1.65, tokens/s = 81360 (36405 source, 44955 target) ; Learning rate = 0.000482 ; Loss = 1.579073\n",
      "2024-06-16 18:05:44.566000: I runner.py:310] Step = 33700 ; steps/s = 1.65, tokens/s = 81253 (36369 source, 44884 target) ; Learning rate = 0.000481 ; Loss = 1.584315\n",
      "2024-06-16 18:06:45.115000: I runner.py:310] Step = 33800 ; steps/s = 1.65, tokens/s = 81240 (36387 source, 44853 target) ; Learning rate = 0.000481 ; Loss = 1.589362\n",
      "2024-06-16 18:07:45.129000: I runner.py:310] Step = 33900 ; steps/s = 1.67, tokens/s = 80416 (35971 source, 44445 target) ; Learning rate = 0.000480 ; Loss = 1.554232\n",
      "2024-06-16 18:08:45.574000: I runner.py:310] Step = 34000 ; steps/s = 1.65, tokens/s = 81381 (36421 source, 44960 target) ; Learning rate = 0.000479 ; Loss = 1.572763\n",
      "2024-06-16 18:09:46.135000: I runner.py:310] Step = 34100 ; steps/s = 1.65, tokens/s = 81218 (36350 source, 44868 target) ; Learning rate = 0.000479 ; Loss = 1.575805\n",
      "2024-06-16 18:10:46.178000: I runner.py:310] Step = 34200 ; steps/s = 1.67, tokens/s = 80411 (36009 source, 44402 target) ; Learning rate = 0.000478 ; Loss = 1.565967\n",
      "2024-06-16 18:11:46.714000: I runner.py:310] Step = 34300 ; steps/s = 1.65, tokens/s = 81272 (36354 source, 44918 target) ; Learning rate = 0.000477 ; Loss = 1.571782\n",
      "2024-06-16 18:12:47.253000: I runner.py:310] Step = 34400 ; steps/s = 1.65, tokens/s = 81256 (36371 source, 44885 target) ; Learning rate = 0.000477 ; Loss = 1.561522\n",
      "2024-06-16 18:13:47.801000: I runner.py:310] Step = 34500 ; steps/s = 1.65, tokens/s = 81257 (36364 source, 44893 target) ; Learning rate = 0.000476 ; Loss = 1.560981\n",
      "2024-06-16 18:14:47.899000: I runner.py:310] Step = 34600 ; steps/s = 1.66, tokens/s = 80263 (35924 source, 44339 target) ; Learning rate = 0.000475 ; Loss = 1.556514\n",
      "2024-06-16 18:15:48.361000: I runner.py:310] Step = 34700 ; steps/s = 1.65, tokens/s = 81327 (36387 source, 44940 target) ; Learning rate = 0.000474 ; Loss = 1.560386\n",
      "2024-06-16 18:16:48.877000: I runner.py:310] Step = 34800 ; steps/s = 1.65, tokens/s = 81306 (36388 source, 44918 target) ; Learning rate = 0.000474 ; Loss = 1.568160\n",
      "2024-06-16 18:17:49.367000: I runner.py:310] Step = 34900 ; steps/s = 1.65, tokens/s = 81340 (36399 source, 44941 target) ; Learning rate = 0.000473 ; Loss = 1.570995\n",
      "2024-06-16 18:18:49.443000: I runner.py:310] Step = 35000 ; steps/s = 1.66, tokens/s = 80348 (35981 source, 44367 target) ; Learning rate = 0.000472 ; Loss = 1.557938\n",
      "2024-06-16 18:18:49.444000: I training.py:192] Running evaluation for step 35000\n",
      "2024-06-16 18:23:05.969000: I training.py:192] Evaluation result for step 35000: loss = 1.363505 ; perplexity = 3.909873\n",
      "2024-06-16 18:24:06.233000: I runner.py:310] Step = 35100 ; steps/s = 1.66, tokens/s = 81632 (36534 source, 45098 target) ; Learning rate = 0.000472 ; Loss = 1.562152\n",
      "2024-06-16 18:25:06.727000: I runner.py:310] Step = 35200 ; steps/s = 1.65, tokens/s = 81314 (36374 source, 44940 target) ; Learning rate = 0.000471 ; Loss = 1.564151\n",
      "2024-06-16 18:26:07.255000: I runner.py:310] Step = 35300 ; steps/s = 1.65, tokens/s = 81274 (36394 source, 44880 target) ; Learning rate = 0.000470 ; Loss = 1.570552\n",
      "2024-06-16 18:27:07.323000: I runner.py:310] Step = 35400 ; steps/s = 1.67, tokens/s = 80358 (35932 source, 44426 target) ; Learning rate = 0.000470 ; Loss = 1.557145\n",
      "2024-06-16 18:28:07.892000: I runner.py:310] Step = 35500 ; steps/s = 1.65, tokens/s = 81257 (36388 source, 44869 target) ; Learning rate = 0.000469 ; Loss = 1.563525\n",
      "2024-06-16 18:29:08.411000: I runner.py:310] Step = 35600 ; steps/s = 1.65, tokens/s = 81266 (36374 source, 44892 target) ; Learning rate = 0.000468 ; Loss = 1.564561\n",
      "2024-06-16 18:30:08.882000: I runner.py:310] Step = 35700 ; steps/s = 1.65, tokens/s = 81311 (36392 source, 44919 target) ; Learning rate = 0.000468 ; Loss = 1.570032\n",
      "2024-06-16 18:31:08.947000: I runner.py:310] Step = 35800 ; steps/s = 1.67, tokens/s = 80357 (35936 source, 44421 target) ; Learning rate = 0.000467 ; Loss = 1.552852\n",
      "2024-06-16 18:32:09.500000: I runner.py:310] Step = 35900 ; steps/s = 1.65, tokens/s = 81236 (36359 source, 44877 target) ; Learning rate = 0.000466 ; Loss = 1.558127\n",
      "2024-06-16 18:33:09.946000: I runner.py:310] Step = 36000 ; steps/s = 1.65, tokens/s = 81363 (36409 source, 44954 target) ; Learning rate = 0.000466 ; Loss = 1.573665\n",
      "2024-06-16 18:34:10.471000: I runner.py:310] Step = 36100 ; steps/s = 1.65, tokens/s = 81290 (36400 source, 44890 target) ; Learning rate = 0.000465 ; Loss = 1.569547\n",
      "2024-06-16 18:35:10.528000: I runner.py:310] Step = 36200 ; steps/s = 1.67, tokens/s = 80376 (35970 source, 44406 target) ; Learning rate = 0.000465 ; Loss = 1.561711\n",
      "2024-06-16 18:36:11.055000: I runner.py:310] Step = 36300 ; steps/s = 1.65, tokens/s = 81245 (36358 source, 44887 target) ; Learning rate = 0.000464 ; Loss = 1.562591\n",
      "2024-06-16 18:37:11.559000: I runner.py:310] Step = 36400 ; steps/s = 1.65, tokens/s = 81299 (36382 source, 44917 target) ; Learning rate = 0.000463 ; Loss = 1.559112\n",
      "2024-06-16 18:38:11.711000: I runner.py:310] Step = 36500 ; steps/s = 1.66, tokens/s = 80248 (35932 source, 44316 target) ; Learning rate = 0.000463 ; Loss = 1.565440\n",
      "2024-06-16 18:39:12.207000: I runner.py:310] Step = 36600 ; steps/s = 1.65, tokens/s = 81303 (36360 source, 44943 target) ; Learning rate = 0.000462 ; Loss = 1.561558\n",
      "2024-06-16 18:40:12.721000: I runner.py:310] Step = 36700 ; steps/s = 1.65, tokens/s = 81300 (36369 source, 44931 target) ; Learning rate = 0.000461 ; Loss = 1.558919\n",
      "2024-06-16 18:41:13.225000: I runner.py:310] Step = 36800 ; steps/s = 1.65, tokens/s = 81259 (36380 source, 44879 target) ; Learning rate = 0.000461 ; Loss = 1.567401\n",
      "2024-06-16 18:42:13.255000: I runner.py:310] Step = 36900 ; steps/s = 1.67, tokens/s = 80428 (36020 source, 44408 target) ; Learning rate = 0.000460 ; Loss = 1.559788\n",
      "2024-06-16 18:43:13.756000: I runner.py:310] Step = 37000 ; steps/s = 1.65, tokens/s = 81289 (36381 source, 44908 target) ; Learning rate = 0.000460 ; Loss = 1.561138\n",
      "2024-06-16 18:44:14.270000: I runner.py:310] Step = 37100 ; steps/s = 1.65, tokens/s = 81314 (36378 source, 44936 target) ; Learning rate = 0.000459 ; Loss = 1.555801\n",
      "2024-06-16 18:45:14.788000: I runner.py:310] Step = 37200 ; steps/s = 1.65, tokens/s = 81273 (36396 source, 44877 target) ; Learning rate = 0.000458 ; Loss = 1.551338\n",
      "2024-06-16 18:46:14.813000: I runner.py:310] Step = 37300 ; steps/s = 1.67, tokens/s = 80419 (35988 source, 44431 target) ; Learning rate = 0.000458 ; Loss = 1.550296\n",
      "2024-06-16 18:47:15.328000: I runner.py:310] Step = 37400 ; steps/s = 1.65, tokens/s = 81304 (36404 source, 44900 target) ; Learning rate = 0.000457 ; Loss = 1.554060\n",
      "2024-06-16 18:48:15.832000: I runner.py:310] Step = 37500 ; steps/s = 1.65, tokens/s = 81288 (36352 source, 44936 target) ; Learning rate = 0.000456 ; Loss = 1.562519\n",
      "2024-06-16 18:49:16.394000: I runner.py:310] Step = 37600 ; steps/s = 1.65, tokens/s = 81216 (36366 source, 44850 target) ; Learning rate = 0.000456 ; Loss = 1.572549\n",
      "2024-06-16 18:50:16.426000: I runner.py:310] Step = 37700 ; steps/s = 1.67, tokens/s = 80397 (35960 source, 44437 target) ; Learning rate = 0.000455 ; Loss = 1.566597\n",
      "2024-06-16 18:51:16.896000: I runner.py:310] Step = 37800 ; steps/s = 1.65, tokens/s = 81343 (36377 source, 44966 target) ; Learning rate = 0.000455 ; Loss = 1.554036\n",
      "2024-06-16 18:52:17.371000: I runner.py:310] Step = 37900 ; steps/s = 1.65, tokens/s = 81345 (36393 source, 44952 target) ; Learning rate = 0.000454 ; Loss = 1.557859\n",
      "2024-06-16 18:53:17.856000: I runner.py:310] Step = 38000 ; steps/s = 1.65, tokens/s = 81337 (36427 source, 44910 target) ; Learning rate = 0.000453 ; Loss = 1.565707\n",
      "2024-06-16 18:54:17.998000: I runner.py:310] Step = 38100 ; steps/s = 1.66, tokens/s = 80230 (35926 source, 44304 target) ; Learning rate = 0.000453 ; Loss = 1.542302\n",
      "2024-06-16 18:55:18.489000: I runner.py:310] Step = 38200 ; steps/s = 1.65, tokens/s = 81324 (36381 source, 44943 target) ; Learning rate = 0.000452 ; Loss = 1.552580\n",
      "2024-06-16 18:56:18.945000: I runner.py:310] Step = 38300 ; steps/s = 1.65, tokens/s = 81348 (36422 source, 44926 target) ; Learning rate = 0.000452 ; Loss = 1.560875\n",
      "2024-06-16 18:57:19.423000: I runner.py:310] Step = 38400 ; steps/s = 1.65, tokens/s = 81367 (36427 source, 44940 target) ; Learning rate = 0.000451 ; Loss = 1.567759\n",
      "2024-06-16 18:58:19.410000: I runner.py:310] Step = 38500 ; steps/s = 1.67, tokens/s = 80469 (35994 source, 44475 target) ; Learning rate = 0.000450 ; Loss = 1.541438\n",
      "2024-06-16 18:59:19.918000: I runner.py:310] Step = 38600 ; steps/s = 1.65, tokens/s = 81313 (36358 source, 44955 target) ; Learning rate = 0.000450 ; Loss = 1.558995\n",
      "2024-06-16 19:00:20.397000: I runner.py:310] Step = 38700 ; steps/s = 1.65, tokens/s = 81325 (36414 source, 44911 target) ; Learning rate = 0.000449 ; Loss = 1.558687\n",
      "2024-06-16 19:01:20.510000: I runner.py:310] Step = 38800 ; steps/s = 1.66, tokens/s = 80273 (35958 source, 44315 target) ; Learning rate = 0.000449 ; Loss = 1.554008\n",
      "2024-06-16 19:02:20.990000: I runner.py:310] Step = 38900 ; steps/s = 1.65, tokens/s = 81354 (36412 source, 44942 target) ; Learning rate = 0.000448 ; Loss = 1.546090\n",
      "2024-06-16 19:03:21.545000: I runner.py:310] Step = 39000 ; steps/s = 1.65, tokens/s = 81239 (36364 source, 44875 target) ; Learning rate = 0.000448 ; Loss = 1.546518\n",
      "2024-06-16 19:04:22.027000: I runner.py:310] Step = 39100 ; steps/s = 1.65, tokens/s = 81320 (36383 source, 44937 target) ; Learning rate = 0.000447 ; Loss = 1.556314\n",
      "2024-06-16 19:05:22.017000: I runner.py:310] Step = 39200 ; steps/s = 1.67, tokens/s = 80452 (35989 source, 44463 target) ; Learning rate = 0.000446 ; Loss = 1.546600\n",
      "2024-06-16 19:06:22.546000: I runner.py:310] Step = 39300 ; steps/s = 1.65, tokens/s = 81257 (36372 source, 44885 target) ; Learning rate = 0.000446 ; Loss = 1.535781\n",
      "2024-06-16 19:07:22.998000: I runner.py:310] Step = 39400 ; steps/s = 1.65, tokens/s = 81389 (36433 source, 44956 target) ; Learning rate = 0.000445 ; Loss = 1.555400\n",
      "2024-06-16 19:08:23.542000: I runner.py:310] Step = 39500 ; steps/s = 1.65, tokens/s = 81254 (36361 source, 44893 target) ; Learning rate = 0.000445 ; Loss = 1.563002\n",
      "2024-06-16 19:09:23.645000: I runner.py:310] Step = 39600 ; steps/s = 1.66, tokens/s = 80307 (35934 source, 44373 target) ; Learning rate = 0.000444 ; Loss = 1.546804\n",
      "2024-06-16 19:10:24.087000: I runner.py:310] Step = 39700 ; steps/s = 1.65, tokens/s = 81400 (36428 source, 44972 target) ; Learning rate = 0.000444 ; Loss = 1.543518\n",
      "2024-06-16 19:11:24.593000: I runner.py:310] Step = 39800 ; steps/s = 1.65, tokens/s = 81310 (36387 source, 44923 target) ; Learning rate = 0.000443 ; Loss = 1.549379\n",
      "2024-06-16 19:12:25.067000: I runner.py:310] Step = 39900 ; steps/s = 1.65, tokens/s = 81335 (36413 source, 44922 target) ; Learning rate = 0.000442 ; Loss = 1.559958\n",
      "2024-06-16 19:13:25.130000: I runner.py:310] Step = 40000 ; steps/s = 1.67, tokens/s = 80331 (35964 source, 44367 target) ; Learning rate = 0.000442 ; Loss = 1.543334\n",
      "2024-06-16 19:13:27.083000: I training.py:176] Saved checkpoint KK-EN-Standard-Transformer/ckpt-40000\n",
      "2024-06-16 19:13:27.083000: I training.py:192] Running evaluation for step 40000\n",
      "2024-06-16 19:17:40.471000: I training.py:192] Evaluation result for step 40000: loss = 1.379615 ; perplexity = 3.973370\n",
      "2024-06-16 19:18:40.763000: I runner.py:310] Step = 40100 ; steps/s = 1.66, tokens/s = 81593 (36514 source, 45079 target) ; Learning rate = 0.000441 ; Loss = 1.545112\n",
      "2024-06-16 19:19:41.337000: I runner.py:310] Step = 40200 ; steps/s = 1.65, tokens/s = 81186 (36333 source, 44853 target) ; Learning rate = 0.000441 ; Loss = 1.557778\n",
      "2024-06-16 19:20:41.861000: I runner.py:310] Step = 40300 ; steps/s = 1.65, tokens/s = 81307 (36378 source, 44929 target) ; Learning rate = 0.000440 ; Loss = 1.561617\n",
      "2024-06-16 19:21:42.010000: I runner.py:310] Step = 40400 ; steps/s = 1.66, tokens/s = 80227 (35882 source, 44345 target) ; Learning rate = 0.000440 ; Loss = 1.551959\n",
      "2024-06-16 19:22:42.616000: I runner.py:310] Step = 40500 ; steps/s = 1.65, tokens/s = 81178 (36309 source, 44869 target) ; Learning rate = 0.000439 ; Loss = 1.547247\n",
      "2024-06-16 19:23:43.198000: I runner.py:310] Step = 40600 ; steps/s = 1.65, tokens/s = 81197 (36340 source, 44857 target) ; Learning rate = 0.000439 ; Loss = 1.553994\n",
      "2024-06-16 19:24:43.809000: I runner.py:310] Step = 40700 ; steps/s = 1.65, tokens/s = 81163 (36344 source, 44819 target) ; Learning rate = 0.000438 ; Loss = 1.556216\n",
      "2024-06-16 19:25:43.931000: I runner.py:310] Step = 40800 ; steps/s = 1.66, tokens/s = 80247 (35912 source, 44335 target) ; Learning rate = 0.000438 ; Loss = 1.547024\n",
      "2024-06-16 19:26:44.410000: I runner.py:310] Step = 40900 ; steps/s = 1.65, tokens/s = 81377 (36443 source, 44934 target) ; Learning rate = 0.000437 ; Loss = 1.548944\n",
      "2024-06-16 19:27:44.958000: I runner.py:310] Step = 41000 ; steps/s = 1.65, tokens/s = 81219 (36345 source, 44874 target) ; Learning rate = 0.000437 ; Loss = 1.547624\n",
      "2024-06-16 19:28:45.505000: I runner.py:310] Step = 41100 ; steps/s = 1.65, tokens/s = 81218 (36352 source, 44866 target) ; Learning rate = 0.000436 ; Loss = 1.562598\n",
      "2024-06-16 19:29:45.545000: I runner.py:310] Step = 41200 ; steps/s = 1.67, tokens/s = 80410 (35976 source, 44434 target) ; Learning rate = 0.000435 ; Loss = 1.547998\n",
      "2024-06-16 19:30:46.021000: I runner.py:310] Step = 41300 ; steps/s = 1.65, tokens/s = 81349 (36409 source, 44940 target) ; Learning rate = 0.000435 ; Loss = 1.545711\n",
      "2024-06-16 19:31:46.608000: I runner.py:310] Step = 41400 ; steps/s = 1.65, tokens/s = 81170 (36338 source, 44832 target) ; Learning rate = 0.000434 ; Loss = 1.546750\n",
      "2024-06-16 19:32:46.663000: I runner.py:310] Step = 41500 ; steps/s = 1.67, tokens/s = 80378 (35970 source, 44408 target) ; Learning rate = 0.000434 ; Loss = 1.544847\n",
      "2024-06-16 19:33:47.192000: I runner.py:310] Step = 41600 ; steps/s = 1.65, tokens/s = 81264 (36369 source, 44895 target) ; Learning rate = 0.000433 ; Loss = 1.550164\n",
      "2024-06-16 19:34:47.744000: I runner.py:310] Step = 41700 ; steps/s = 1.65, tokens/s = 81192 (36338 source, 44854 target) ; Learning rate = 0.000433 ; Loss = 1.551018\n",
      "2024-06-16 19:35:48.238000: I runner.py:310] Step = 41800 ; steps/s = 1.65, tokens/s = 81333 (36379 source, 44954 target) ; Learning rate = 0.000432 ; Loss = 1.547926\n",
      "2024-06-16 19:36:48.334000: I runner.py:310] Step = 41900 ; steps/s = 1.66, tokens/s = 80348 (35972 source, 44376 target) ; Learning rate = 0.000432 ; Loss = 1.544843\n",
      "2024-06-16 19:37:48.797000: I runner.py:310] Step = 42000 ; steps/s = 1.65, tokens/s = 81353 (36406 source, 44947 target) ; Learning rate = 0.000431 ; Loss = 1.542756\n",
      "2024-06-16 19:38:49.317000: I runner.py:310] Step = 42100 ; steps/s = 1.65, tokens/s = 81261 (36359 source, 44902 target) ; Learning rate = 0.000431 ; Loss = 1.540154\n",
      "2024-06-16 19:39:49.819000: I runner.py:310] Step = 42200 ; steps/s = 1.65, tokens/s = 81318 (36428 source, 44890 target) ; Learning rate = 0.000430 ; Loss = 1.542752\n",
      "2024-06-16 19:40:49.904000: I runner.py:310] Step = 42300 ; steps/s = 1.66, tokens/s = 80322 (35931 source, 44391 target) ; Learning rate = 0.000430 ; Loss = 1.540171\n",
      "2024-06-16 19:41:50.474000: I runner.py:310] Step = 42400 ; steps/s = 1.65, tokens/s = 81224 (36345 source, 44879 target) ; Learning rate = 0.000429 ; Loss = 1.544434\n",
      "2024-06-16 19:42:50.970000: I runner.py:310] Step = 42500 ; steps/s = 1.65, tokens/s = 81310 (36391 source, 44919 target) ; Learning rate = 0.000429 ; Loss = 1.545385\n",
      "2024-06-16 19:43:51.535000: I runner.py:310] Step = 42600 ; steps/s = 1.65, tokens/s = 81233 (36351 source, 44882 target) ; Learning rate = 0.000428 ; Loss = 1.550634\n",
      "2024-06-16 19:44:51.529000: I runner.py:310] Step = 42700 ; steps/s = 1.67, tokens/s = 80452 (35997 source, 44455 target) ; Learning rate = 0.000428 ; Loss = 1.542261\n",
      "2024-06-16 19:45:52.061000: I runner.py:310] Step = 42800 ; steps/s = 1.65, tokens/s = 81266 (36401 source, 44865 target) ; Learning rate = 0.000427 ; Loss = 1.540515\n",
      "2024-06-16 19:46:52.514000: I runner.py:310] Step = 42900 ; steps/s = 1.65, tokens/s = 81370 (36408 source, 44962 target) ; Learning rate = 0.000427 ; Loss = 1.548004\n",
      "2024-06-16 19:47:53.059000: I runner.py:310] Step = 43000 ; steps/s = 1.65, tokens/s = 81228 (36353 source, 44875 target) ; Learning rate = 0.000426 ; Loss = 1.551247\n",
      "2024-06-16 19:48:53.170000: I runner.py:310] Step = 43100 ; steps/s = 1.66, tokens/s = 80310 (35949 source, 44361 target) ; Learning rate = 0.000426 ; Loss = 1.533021\n",
      "2024-06-16 19:49:53.729000: I runner.py:310] Step = 43200 ; steps/s = 1.65, tokens/s = 81226 (36370 source, 44856 target) ; Learning rate = 0.000425 ; Loss = 1.550786\n",
      "2024-06-16 19:50:54.234000: I runner.py:310] Step = 43300 ; steps/s = 1.65, tokens/s = 81265 (36330 source, 44935 target) ; Learning rate = 0.000425 ; Loss = 1.547637\n",
      "2024-06-16 19:51:54.750000: I runner.py:310] Step = 43400 ; steps/s = 1.65, tokens/s = 81320 (36405 source, 44915 target) ; Learning rate = 0.000424 ; Loss = 1.550384\n",
      "2024-06-16 19:52:54.813000: I runner.py:310] Step = 43500 ; steps/s = 1.67, tokens/s = 80337 (35948 source, 44389 target) ; Learning rate = 0.000424 ; Loss = 1.543171\n",
      "2024-06-16 19:53:55.317000: I runner.py:310] Step = 43600 ; steps/s = 1.65, tokens/s = 81300 (36393 source, 44907 target) ; Learning rate = 0.000423 ; Loss = 1.532193\n",
      "2024-06-16 19:54:55.869000: I runner.py:310] Step = 43700 ; steps/s = 1.65, tokens/s = 81244 (36369 source, 44875 target) ; Learning rate = 0.000423 ; Loss = 1.540564\n",
      "2024-06-16 19:55:55.960000: I runner.py:310] Step = 43800 ; steps/s = 1.66, tokens/s = 80329 (35945 source, 44384 target) ; Learning rate = 0.000422 ; Loss = 1.540259\n",
      "2024-06-16 19:56:56.441000: I runner.py:310] Step = 43900 ; steps/s = 1.65, tokens/s = 81343 (36400 source, 44943 target) ; Learning rate = 0.000422 ; Loss = 1.531392\n",
      "2024-06-16 19:57:56.937000: I runner.py:310] Step = 44000 ; steps/s = 1.65, tokens/s = 81289 (36403 source, 44886 target) ; Learning rate = 0.000421 ; Loss = 1.542587\n",
      "2024-06-16 19:58:57.469000: I runner.py:310] Step = 44100 ; steps/s = 1.65, tokens/s = 81267 (36368 source, 44899 target) ; Learning rate = 0.000421 ; Loss = 1.552749\n",
      "2024-06-16 19:59:57.575000: I runner.py:310] Step = 44200 ; steps/s = 1.66, tokens/s = 80319 (35925 source, 44394 target) ; Learning rate = 0.000420 ; Loss = 1.534967\n",
      "2024-06-16 20:00:58.071000: I runner.py:310] Step = 44300 ; steps/s = 1.65, tokens/s = 81330 (36398 source, 44932 target) ; Learning rate = 0.000420 ; Loss = 1.540753\n",
      "2024-06-16 20:01:58.613000: I runner.py:310] Step = 44400 ; steps/s = 1.65, tokens/s = 81217 (36342 source, 44875 target) ; Learning rate = 0.000419 ; Loss = 1.532831\n",
      "2024-06-16 20:02:59.088000: I runner.py:310] Step = 44500 ; steps/s = 1.65, tokens/s = 81321 (36400 source, 44921 target) ; Learning rate = 0.000419 ; Loss = 1.538057\n",
      "2024-06-16 20:03:59.188000: I runner.py:310] Step = 44600 ; steps/s = 1.66, tokens/s = 80355 (35953 source, 44402 target) ; Learning rate = 0.000419 ; Loss = 1.536547\n",
      "2024-06-16 20:04:59.654000: I runner.py:310] Step = 44700 ; steps/s = 1.65, tokens/s = 81348 (36402 source, 44946 target) ; Learning rate = 0.000418 ; Loss = 1.529382\n",
      "2024-06-16 20:06:00.225000: I runner.py:310] Step = 44800 ; steps/s = 1.65, tokens/s = 81236 (36385 source, 44851 target) ; Learning rate = 0.000418 ; Loss = 1.533439\n",
      "2024-06-16 20:07:00.759000: I runner.py:310] Step = 44900 ; steps/s = 1.65, tokens/s = 81225 (36328 source, 44897 target) ; Learning rate = 0.000417 ; Loss = 1.537822\n",
      "2024-06-16 20:08:00.803000: I runner.py:310] Step = 45000 ; steps/s = 1.67, tokens/s = 80386 (35973 source, 44413 target) ; Learning rate = 0.000417 ; Loss = 1.531991\n",
      "2024-06-16 20:08:00.805000: I training.py:192] Running evaluation for step 45000\n",
      "2024-06-16 20:12:19.646000: I training.py:192] Evaluation result for step 45000: loss = 1.397160 ; perplexity = 4.043701\n",
      "2024-06-16 20:13:19.976000: I runner.py:310] Step = 45100 ; steps/s = 1.66, tokens/s = 81551 (36515 source, 45036 target) ; Learning rate = 0.000416 ; Loss = 1.534360\n",
      "2024-06-16 20:14:20.447000: I runner.py:310] Step = 45200 ; steps/s = 1.65, tokens/s = 81363 (36397 source, 44966 target) ; Learning rate = 0.000416 ; Loss = 1.538599\n",
      "2024-06-16 20:15:20.961000: I runner.py:310] Step = 45300 ; steps/s = 1.65, tokens/s = 81278 (36393 source, 44885 target) ; Learning rate = 0.000415 ; Loss = 1.541553\n",
      "2024-06-16 20:16:21.067000: I runner.py:310] Step = 45400 ; steps/s = 1.66, tokens/s = 80260 (35913 source, 44347 target) ; Learning rate = 0.000415 ; Loss = 1.543426\n",
      "2024-06-16 20:17:21.605000: I runner.py:310] Step = 45500 ; steps/s = 1.65, tokens/s = 81263 (36360 source, 44903 target) ; Learning rate = 0.000414 ; Loss = 1.535372\n",
      "2024-06-16 20:18:22.069000: I runner.py:310] Step = 45600 ; steps/s = 1.65, tokens/s = 81335 (36393 source, 44942 target) ; Learning rate = 0.000414 ; Loss = 1.541181\n",
      "2024-06-16 20:19:22.597000: I runner.py:310] Step = 45700 ; steps/s = 1.65, tokens/s = 81291 (36406 source, 44885 target) ; Learning rate = 0.000413 ; Loss = 1.540735\n",
      "2024-06-16 20:20:22.686000: I runner.py:310] Step = 45800 ; steps/s = 1.66, tokens/s = 80320 (35928 source, 44392 target) ; Learning rate = 0.000413 ; Loss = 1.538711\n",
      "2024-06-16 20:21:23.209000: I runner.py:310] Step = 45900 ; steps/s = 1.65, tokens/s = 81253 (36350 source, 44903 target) ; Learning rate = 0.000413 ; Loss = 1.532302\n",
      "2024-06-16 20:22:23.704000: I runner.py:310] Step = 46000 ; steps/s = 1.65, tokens/s = 81322 (36387 source, 44935 target) ; Learning rate = 0.000412 ; Loss = 1.535795\n",
      "2024-06-16 20:23:23.785000: I runner.py:310] Step = 46100 ; steps/s = 1.66, tokens/s = 80351 (36002 source, 44349 target) ; Learning rate = 0.000412 ; Loss = 1.538525\n",
      "2024-06-16 20:24:24.320000: I runner.py:310] Step = 46200 ; steps/s = 1.65, tokens/s = 81272 (36364 source, 44908 target) ; Learning rate = 0.000411 ; Loss = 1.533880\n",
      "2024-06-16 20:25:24.861000: I runner.py:310] Step = 46300 ; steps/s = 1.65, tokens/s = 81251 (36372 source, 44879 target) ; Learning rate = 0.000411 ; Loss = 1.532103\n",
      "2024-06-16 20:26:25.417000: I runner.py:310] Step = 46400 ; steps/s = 1.65, tokens/s = 81234 (36342 source, 44892 target) ; Learning rate = 0.000410 ; Loss = 1.534149\n",
      "2024-06-16 20:27:25.473000: I runner.py:310] Step = 46500 ; steps/s = 1.67, tokens/s = 80359 (35972 source, 44387 target) ; Learning rate = 0.000410 ; Loss = 1.526061\n",
      "2024-06-16 20:28:25.991000: I runner.py:310] Step = 46600 ; steps/s = 1.65, tokens/s = 81307 (36383 source, 44924 target) ; Learning rate = 0.000409 ; Loss = 1.530405\n",
      "2024-06-16 20:29:26.514000: I runner.py:310] Step = 46700 ; steps/s = 1.65, tokens/s = 81238 (36374 source, 44864 target) ; Learning rate = 0.000409 ; Loss = 1.536467\n",
      "2024-06-16 20:30:27.017000: I runner.py:310] Step = 46800 ; steps/s = 1.65, tokens/s = 81334 (36412 source, 44922 target) ; Learning rate = 0.000409 ; Loss = 1.539296\n",
      "2024-06-16 20:31:27.167000: I runner.py:310] Step = 46900 ; steps/s = 1.66, tokens/s = 80227 (35885 source, 44342 target) ; Learning rate = 0.000408 ; Loss = 1.532060\n",
      "2024-06-16 20:32:27.658000: I runner.py:310] Step = 47000 ; steps/s = 1.65, tokens/s = 81303 (36374 source, 44929 target) ; Learning rate = 0.000408 ; Loss = 1.528488\n",
      "2024-06-16 20:33:28.203000: I runner.py:310] Step = 47100 ; steps/s = 1.65, tokens/s = 81244 (36364 source, 44880 target) ; Learning rate = 0.000407 ; Loss = 1.531499\n",
      "2024-06-16 20:34:28.695000: I runner.py:310] Step = 47200 ; steps/s = 1.65, tokens/s = 81327 (36409 source, 44918 target) ; Learning rate = 0.000407 ; Loss = 1.538454\n",
      "2024-06-16 20:35:28.752000: I runner.py:310] Step = 47300 ; steps/s = 1.67, tokens/s = 80376 (35960 source, 44416 target) ; Learning rate = 0.000406 ; Loss = 1.535141\n",
      "2024-06-16 20:36:29.283000: I runner.py:310] Step = 47400 ; steps/s = 1.65, tokens/s = 81251 (36350 source, 44901 target) ; Learning rate = 0.000406 ; Loss = 1.530942\n",
      "2024-06-16 20:37:29.754000: I runner.py:310] Step = 47500 ; steps/s = 1.65, tokens/s = 81366 (36415 source, 44951 target) ; Learning rate = 0.000406 ; Loss = 1.539553\n",
      "2024-06-16 20:38:30.285000: I runner.py:310] Step = 47600 ; steps/s = 1.65, tokens/s = 81239 (36356 source, 44883 target) ; Learning rate = 0.000405 ; Loss = 1.547497\n",
      "2024-06-16 20:39:30.333000: I runner.py:310] Step = 47700 ; steps/s = 1.67, tokens/s = 80432 (36022 source, 44410 target) ; Learning rate = 0.000405 ; Loss = 1.538362\n",
      "2024-06-16 20:40:30.789000: I runner.py:310] Step = 47800 ; steps/s = 1.65, tokens/s = 81371 (36396 source, 44975 target) ; Learning rate = 0.000404 ; Loss = 1.531580\n",
      "2024-06-16 20:41:31.385000: I runner.py:310] Step = 47900 ; steps/s = 1.65, tokens/s = 81169 (36319 source, 44850 target) ; Learning rate = 0.000404 ; Loss = 1.528013\n",
      "2024-06-16 20:42:31.900000: I runner.py:310] Step = 48000 ; steps/s = 1.65, tokens/s = 81253 (36379 source, 44874 target) ; Learning rate = 0.000403 ; Loss = 1.545612\n",
      "2024-06-16 20:43:31.954000: I runner.py:310] Step = 48100 ; steps/s = 1.67, tokens/s = 80389 (36008 source, 44381 target) ; Learning rate = 0.000403 ; Loss = 1.513790\n",
      "2024-06-16 20:44:32.464000: I runner.py:310] Step = 48200 ; steps/s = 1.65, tokens/s = 81312 (36402 source, 44910 target) ; Learning rate = 0.000403 ; Loss = 1.535483\n",
      "2024-06-16 20:45:32.989000: I runner.py:310] Step = 48300 ; steps/s = 1.65, tokens/s = 81298 (36372 source, 44926 target) ; Learning rate = 0.000402 ; Loss = 1.530260\n",
      "2024-06-16 20:46:33.378000: I runner.py:310] Step = 48400 ; steps/s = 1.66, tokens/s = 80808 (36145 source, 44663 target) ; Learning rate = 0.000402 ; Loss = 1.534271\n",
      "2024-06-16 20:47:33.528000: I runner.py:310] Step = 48500 ; steps/s = 1.66, tokens/s = 80802 (36132 source, 44670 target) ; Learning rate = 0.000401 ; Loss = 1.531640\n",
      "2024-06-16 20:48:34.003000: I runner.py:310] Step = 48600 ; steps/s = 1.65, tokens/s = 81324 (36395 source, 44929 target) ; Learning rate = 0.000401 ; Loss = 1.532418\n",
      "2024-06-16 20:49:34.477000: I runner.py:310] Step = 48700 ; steps/s = 1.65, tokens/s = 81355 (36430 source, 44925 target) ; Learning rate = 0.000401 ; Loss = 1.531819\n",
      "2024-06-16 20:50:34.528000: I runner.py:310] Step = 48800 ; steps/s = 1.67, tokens/s = 80411 (36004 source, 44407 target) ; Learning rate = 0.000400 ; Loss = 1.531451\n",
      "2024-06-16 20:51:35.044000: I runner.py:310] Step = 48900 ; steps/s = 1.65, tokens/s = 81275 (36372 source, 44903 target) ; Learning rate = 0.000400 ; Loss = 1.527382\n",
      "2024-06-16 20:52:35.537000: I runner.py:310] Step = 49000 ; steps/s = 1.65, tokens/s = 81333 (36380 source, 44953 target) ; Learning rate = 0.000399 ; Loss = 1.534693\n",
      "2024-06-16 20:53:36.077000: I runner.py:310] Step = 49100 ; steps/s = 1.65, tokens/s = 81241 (36359 source, 44882 target) ; Learning rate = 0.000399 ; Loss = 1.538565\n",
      "2024-06-16 20:54:36.111000: I runner.py:310] Step = 49200 ; steps/s = 1.67, tokens/s = 80395 (35995 source, 44400 target) ; Learning rate = 0.000398 ; Loss = 1.526636\n",
      "2024-06-16 20:55:36.691000: I runner.py:310] Step = 49300 ; steps/s = 1.65, tokens/s = 81178 (36310 source, 44868 target) ; Learning rate = 0.000398 ; Loss = 1.531598\n",
      "2024-06-16 20:56:37.257000: I runner.py:310] Step = 49400 ; steps/s = 1.65, tokens/s = 81232 (36359 source, 44873 target) ; Learning rate = 0.000398 ; Loss = 1.532496\n",
      "2024-06-16 20:57:37.747000: I runner.py:310] Step = 49500 ; steps/s = 1.65, tokens/s = 81322 (36386 source, 44936 target) ; Learning rate = 0.000397 ; Loss = 1.531381\n",
      "2024-06-16 20:58:37.822000: I runner.py:310] Step = 49600 ; steps/s = 1.66, tokens/s = 80352 (35974 source, 44378 target) ; Learning rate = 0.000397 ; Loss = 1.529185\n",
      "2024-06-16 20:59:38.378000: I runner.py:310] Step = 49700 ; steps/s = 1.65, tokens/s = 81215 (36311 source, 44904 target) ; Learning rate = 0.000396 ; Loss = 1.516733\n",
      "2024-06-16 21:00:38.897000: I runner.py:310] Step = 49800 ; steps/s = 1.65, tokens/s = 81285 (36382 source, 44903 target) ; Learning rate = 0.000396 ; Loss = 1.539562\n",
      "2024-06-16 21:01:39.414000: I runner.py:310] Step = 49900 ; steps/s = 1.65, tokens/s = 81312 (36444 source, 44868 target) ; Learning rate = 0.000396 ; Loss = 1.530839\n",
      "2024-06-16 21:02:39.450000: I runner.py:310] Step = 50000 ; steps/s = 1.67, tokens/s = 80389 (35973 source, 44416 target) ; Learning rate = 0.000395 ; Loss = 1.523854\n",
      "2024-06-16 21:02:41.468000: I training.py:176] Saved checkpoint KK-EN-Standard-Transformer/ckpt-50000\n",
      "2024-06-16 21:02:41.468000: I training.py:192] Running evaluation for step 50000\n",
      "2024-06-16 21:06:54.228000: I training.py:192] Evaluation result for step 50000: loss = 1.406140 ; perplexity = 4.080176\n",
      "2024-06-16 21:07:54.552000: I runner.py:310] Step = 50100 ; steps/s = 1.66, tokens/s = 81564 (36500 source, 45064 target) ; Learning rate = 0.000395 ; Loss = 1.531356\n",
      "2024-06-16 21:08:55.089000: I runner.py:310] Step = 50200 ; steps/s = 1.65, tokens/s = 81269 (36384 source, 44885 target) ; Learning rate = 0.000394 ; Loss = 1.540221\n",
      "2024-06-16 21:09:55.621000: I runner.py:310] Step = 50300 ; steps/s = 1.65, tokens/s = 81249 (36341 source, 44908 target) ; Learning rate = 0.000394 ; Loss = 1.528689\n",
      "2024-06-16 21:10:55.743000: I runner.py:310] Step = 50400 ; steps/s = 1.66, tokens/s = 80283 (35941 source, 44342 target) ; Learning rate = 0.000394 ; Loss = 1.539542\n",
      "2024-06-16 21:11:56.271000: I runner.py:310] Step = 50500 ; steps/s = 1.65, tokens/s = 81268 (36363 source, 44905 target) ; Learning rate = 0.000393 ; Loss = 1.525373\n",
      "2024-06-16 21:12:56.713000: I runner.py:310] Step = 50600 ; steps/s = 1.65, tokens/s = 81407 (36439 source, 44968 target) ; Learning rate = 0.000393 ; Loss = 1.533082\n",
      "2024-06-16 21:13:57.236000: I runner.py:310] Step = 50700 ; steps/s = 1.65, tokens/s = 81243 (36355 source, 44888 target) ; Learning rate = 0.000393 ; Loss = 1.529944\n",
      "2024-06-16 21:14:57.232000: I runner.py:310] Step = 50800 ; steps/s = 1.67, tokens/s = 80438 (35986 source, 44452 target) ; Learning rate = 0.000392 ; Loss = 1.537236\n",
      "2024-06-16 21:15:57.681000: I runner.py:310] Step = 50900 ; steps/s = 1.65, tokens/s = 81422 (36456 source, 44966 target) ; Learning rate = 0.000392 ; Loss = 1.531836\n",
      "2024-06-16 21:16:58.201000: I runner.py:310] Step = 51000 ; steps/s = 1.65, tokens/s = 81253 (36379 source, 44874 target) ; Learning rate = 0.000391 ; Loss = 1.528645\n",
      "2024-06-16 21:17:58.186000: I runner.py:310] Step = 51100 ; steps/s = 1.67, tokens/s = 80447 (35990 source, 44457 target) ; Learning rate = 0.000391 ; Loss = 1.526167\n",
      "2024-06-16 21:18:58.675000: I runner.py:310] Step = 51200 ; steps/s = 1.65, tokens/s = 81312 (36375 source, 44937 target) ; Learning rate = 0.000391 ; Loss = 1.520945\n",
      "2024-06-16 21:19:59.174000: I runner.py:310] Step = 51300 ; steps/s = 1.65, tokens/s = 81341 (36428 source, 44913 target) ; Learning rate = 0.000390 ; Loss = 1.527644\n",
      "2024-06-16 21:20:59.710000: I runner.py:310] Step = 51400 ; steps/s = 1.65, tokens/s = 81230 (36339 source, 44891 target) ; Learning rate = 0.000390 ; Loss = 1.534549\n",
      "2024-06-16 21:21:59.773000: I runner.py:310] Step = 51500 ; steps/s = 1.67, tokens/s = 80372 (35991 source, 44381 target) ; Learning rate = 0.000389 ; Loss = 1.527720\n",
      "2024-06-16 21:23:00.313000: I runner.py:310] Step = 51600 ; steps/s = 1.65, tokens/s = 81255 (36367 source, 44888 target) ; Learning rate = 0.000389 ; Loss = 1.523473\n",
      "2024-06-16 21:24:00.829000: I runner.py:310] Step = 51700 ; steps/s = 1.65, tokens/s = 81326 (36388 source, 44938 target) ; Learning rate = 0.000389 ; Loss = 1.522642\n",
      "2024-06-16 21:25:01.292000: I runner.py:310] Step = 51800 ; steps/s = 1.65, tokens/s = 81331 (36401 source, 44930 target) ; Learning rate = 0.000388 ; Loss = 1.530306\n",
      "2024-06-16 21:26:01.348000: I runner.py:310] Step = 51900 ; steps/s = 1.67, tokens/s = 80341 (35944 source, 44397 target) ; Learning rate = 0.000388 ; Loss = 1.530913\n",
      "2024-06-16 21:27:01.878000: I runner.py:310] Step = 52000 ; steps/s = 1.65, tokens/s = 81254 (36363 source, 44891 target) ; Learning rate = 0.000388 ; Loss = 1.527420\n",
      "2024-06-16 21:28:02.347000: I runner.py:310] Step = 52100 ; steps/s = 1.65, tokens/s = 81355 (36406 source, 44949 target) ; Learning rate = 0.000387 ; Loss = 1.518682\n",
      "2024-06-16 21:29:02.848000: I runner.py:310] Step = 52200 ; steps/s = 1.65, tokens/s = 81344 (36412 source, 44932 target) ; Learning rate = 0.000387 ; Loss = 1.524330\n",
      "2024-06-16 21:30:02.938000: I runner.py:310] Step = 52300 ; steps/s = 1.66, tokens/s = 80302 (35932 source, 44370 target) ; Learning rate = 0.000386 ; Loss = 1.515491\n",
      "2024-06-16 21:31:03.450000: I runner.py:310] Step = 52400 ; steps/s = 1.65, tokens/s = 81296 (36374 source, 44922 target) ; Learning rate = 0.000386 ; Loss = 1.527173\n",
      "2024-06-16 21:32:03.974000: I runner.py:310] Step = 52500 ; steps/s = 1.65, tokens/s = 81292 (36375 source, 44917 target) ; Learning rate = 0.000386 ; Loss = 1.527864\n",
      "2024-06-16 21:33:04.535000: I runner.py:310] Step = 52600 ; steps/s = 1.65, tokens/s = 81219 (36376 source, 44843 target) ; Learning rate = 0.000385 ; Loss = 1.529525\n",
      "2024-06-16 21:34:04.571000: I runner.py:310] Step = 52700 ; steps/s = 1.67, tokens/s = 80386 (35968 source, 44418 target) ; Learning rate = 0.000385 ; Loss = 1.518241\n",
      "2024-06-16 21:35:05.135000: I runner.py:310] Step = 52800 ; steps/s = 1.65, tokens/s = 81238 (36377 source, 44861 target) ; Learning rate = 0.000385 ; Loss = 1.526576\n",
      "2024-06-16 21:36:05.662000: I runner.py:310] Step = 52900 ; steps/s = 1.65, tokens/s = 81272 (36373 source, 44899 target) ; Learning rate = 0.000384 ; Loss = 1.525607\n",
      "2024-06-16 21:37:06.245000: I runner.py:310] Step = 53000 ; steps/s = 1.65, tokens/s = 81178 (36321 source, 44857 target) ; Learning rate = 0.000384 ; Loss = 1.534450\n",
      "2024-06-16 21:38:06.251000: I runner.py:310] Step = 53100 ; steps/s = 1.67, tokens/s = 80402 (35970 source, 44432 target) ; Learning rate = 0.000384 ; Loss = 1.514659\n",
      "2024-06-16 21:39:06.750000: I runner.py:310] Step = 53200 ; steps/s = 1.65, tokens/s = 81298 (36387 source, 44911 target) ; Learning rate = 0.000383 ; Loss = 1.523316\n",
      "2024-06-16 21:40:07.308000: I runner.py:310] Step = 53300 ; steps/s = 1.65, tokens/s = 81253 (36370 source, 44883 target) ; Learning rate = 0.000383 ; Loss = 1.526369\n",
      "2024-06-16 21:41:07.400000: I runner.py:310] Step = 53400 ; steps/s = 1.66, tokens/s = 80322 (35955 source, 44367 target) ; Learning rate = 0.000382 ; Loss = 1.524517\n",
      "2024-06-16 21:42:07.841000: I runner.py:310] Step = 53500 ; steps/s = 1.65, tokens/s = 81392 (36404 source, 44988 target) ; Learning rate = 0.000382 ; Loss = 1.526023\n",
      "2024-06-16 21:43:08.353000: I runner.py:310] Step = 53600 ; steps/s = 1.65, tokens/s = 81295 (36391 source, 44904 target) ; Learning rate = 0.000382 ; Loss = 1.516771\n",
      "2024-06-16 21:44:08.843000: I runner.py:310] Step = 53700 ; steps/s = 1.65, tokens/s = 81307 (36373 source, 44934 target) ; Learning rate = 0.000381 ; Loss = 1.516268\n",
      "2024-06-16 21:45:08.897000: I runner.py:310] Step = 53800 ; steps/s = 1.67, tokens/s = 80356 (35985 source, 44371 target) ; Learning rate = 0.000381 ; Loss = 1.523353\n",
      "2024-06-16 21:46:09.412000: I runner.py:310] Step = 53900 ; steps/s = 1.65, tokens/s = 81301 (36397 source, 44904 target) ; Learning rate = 0.000381 ; Loss = 1.515998\n",
      "2024-06-16 21:47:09.940000: I runner.py:310] Step = 54000 ; steps/s = 1.65, tokens/s = 81247 (36358 source, 44889 target) ; Learning rate = 0.000380 ; Loss = 1.522725\n",
      "2024-06-16 21:48:10.427000: I runner.py:310] Step = 54100 ; steps/s = 1.65, tokens/s = 81333 (36409 source, 44924 target) ; Learning rate = 0.000380 ; Loss = 1.526060\n",
      "2024-06-16 21:49:10.482000: I runner.py:310] Step = 54200 ; steps/s = 1.67, tokens/s = 80381 (35970 source, 44411 target) ; Learning rate = 0.000380 ; Loss = 1.516632\n",
      "2024-06-16 21:50:11.013000: I runner.py:310] Step = 54300 ; steps/s = 1.65, tokens/s = 81222 (36329 source, 44893 target) ; Learning rate = 0.000379 ; Loss = 1.527879\n",
      "2024-06-16 21:51:11.504000: I runner.py:310] Step = 54400 ; steps/s = 1.65, tokens/s = 81346 (36434 source, 44912 target) ; Learning rate = 0.000379 ; Loss = 1.528408\n",
      "2024-06-16 21:52:12.004000: I runner.py:310] Step = 54500 ; steps/s = 1.65, tokens/s = 81320 (36384 source, 44936 target) ; Learning rate = 0.000379 ; Loss = 1.523396\n",
      "2024-06-16 21:53:12.092000: I runner.py:310] Step = 54600 ; steps/s = 1.66, tokens/s = 80362 (35955 source, 44407 target) ; Learning rate = 0.000378 ; Loss = 1.515644\n",
      "2024-06-16 21:54:12.602000: I runner.py:310] Step = 54700 ; steps/s = 1.65, tokens/s = 81254 (36360 source, 44894 target) ; Learning rate = 0.000378 ; Loss = 1.522042\n",
      "2024-06-16 21:55:13.098000: I runner.py:310] Step = 54800 ; steps/s = 1.65, tokens/s = 81326 (36416 source, 44910 target) ; Learning rate = 0.000378 ; Loss = 1.526569\n",
      "2024-06-16 21:56:13.583000: I runner.py:310] Step = 54900 ; steps/s = 1.65, tokens/s = 81330 (36394 source, 44936 target) ; Learning rate = 0.000377 ; Loss = 1.538275\n",
      "2024-06-16 21:57:13.662000: I runner.py:310] Step = 55000 ; steps/s = 1.66, tokens/s = 80320 (35909 source, 44411 target) ; Learning rate = 0.000377 ; Loss = 1.519990\n",
      "2024-06-16 21:57:13.663000: I training.py:192] Running evaluation for step 55000\n",
      "2024-06-16 22:01:28.660000: I training.py:192] Evaluation result for step 55000: loss = 1.412107 ; perplexity = 4.104596\n",
      "2024-06-16 22:02:28.967000: I runner.py:310] Step = 55100 ; steps/s = 1.66, tokens/s = 81595 (36517 source, 45078 target) ; Learning rate = 0.000377 ; Loss = 1.515313\n",
      "2024-06-16 22:03:29.490000: I runner.py:310] Step = 55200 ; steps/s = 1.65, tokens/s = 81260 (36386 source, 44874 target) ; Learning rate = 0.000376 ; Loss = 1.519562\n",
      "2024-06-16 22:04:29.942000: I runner.py:310] Step = 55300 ; steps/s = 1.65, tokens/s = 81373 (36439 source, 44934 target) ; Learning rate = 0.000376 ; Loss = 1.522146\n",
      "2024-06-16 22:05:30.014000: I runner.py:310] Step = 55400 ; steps/s = 1.66, tokens/s = 80353 (35942 source, 44411 target) ; Learning rate = 0.000376 ; Loss = 1.508525\n",
      "2024-06-16 22:06:30.469000: I runner.py:310] Step = 55500 ; steps/s = 1.65, tokens/s = 81326 (36368 source, 44958 target) ; Learning rate = 0.000375 ; Loss = 1.517640\n",
      "2024-06-16 22:07:31.058000: I runner.py:310] Step = 55600 ; steps/s = 1.65, tokens/s = 81220 (36350 source, 44870 target) ; Learning rate = 0.000375 ; Loss = 1.521785\n",
      "2024-06-16 22:08:31.298000: I runner.py:310] Step = 55700 ; steps/s = 1.66, tokens/s = 80512 (36077 source, 44435 target) ; Learning rate = 0.000375 ; Loss = 1.529103\n",
      "2024-06-16 22:09:31.667000: I runner.py:310] Step = 55800 ; steps/s = 1.66, tokens/s = 81113 (36297 source, 44816 target) ; Learning rate = 0.000374 ; Loss = 1.517729\n",
      "2024-06-16 22:10:32.194000: I runner.py:310] Step = 55900 ; steps/s = 1.65, tokens/s = 81263 (36362 source, 44901 target) ; Learning rate = 0.000374 ; Loss = 1.517647\n",
      "2024-06-16 22:11:32.792000: I runner.py:310] Step = 56000 ; steps/s = 1.65, tokens/s = 81178 (36359 source, 44819 target) ; Learning rate = 0.000374 ; Loss = 1.518536\n",
      "2024-06-16 22:12:32.837000: I runner.py:310] Step = 56100 ; steps/s = 1.67, tokens/s = 80357 (35937 source, 44420 target) ; Learning rate = 0.000373 ; Loss = 1.522654\n",
      "2024-06-16 22:13:33.352000: I runner.py:310] Step = 56200 ; steps/s = 1.65, tokens/s = 81305 (36401 source, 44904 target) ; Learning rate = 0.000373 ; Loss = 1.519159\n",
      "2024-06-16 22:14:33.878000: I runner.py:310] Step = 56300 ; steps/s = 1.65, tokens/s = 81287 (36379 source, 44908 target) ; Learning rate = 0.000373 ; Loss = 1.528399\n",
      "2024-06-16 22:15:34.413000: I runner.py:310] Step = 56400 ; steps/s = 1.65, tokens/s = 81244 (36355 source, 44889 target) ; Learning rate = 0.000372 ; Loss = 1.532111\n",
      "2024-06-16 22:16:34.487000: I runner.py:310] Step = 56500 ; steps/s = 1.66, tokens/s = 80338 (35935 source, 44403 target) ; Learning rate = 0.000372 ; Loss = 1.518895\n",
      "2024-06-16 22:17:35.040000: I runner.py:310] Step = 56600 ; steps/s = 1.65, tokens/s = 81257 (36384 source, 44873 target) ; Learning rate = 0.000372 ; Loss = 1.516182\n",
      "2024-06-16 22:18:35.562000: I runner.py:310] Step = 56700 ; steps/s = 1.65, tokens/s = 81265 (36364 source, 44901 target) ; Learning rate = 0.000371 ; Loss = 1.521637\n",
      "2024-06-16 22:19:36.058000: I runner.py:310] Step = 56800 ; steps/s = 1.65, tokens/s = 81309 (36403 source, 44906 target) ; Learning rate = 0.000371 ; Loss = 1.525079\n",
      "2024-06-16 22:20:36.130000: I runner.py:310] Step = 56900 ; steps/s = 1.66, tokens/s = 80347 (35965 source, 44382 target) ; Learning rate = 0.000371 ; Loss = 1.512274\n",
      "2024-06-16 22:21:36.643000: I runner.py:310] Step = 57000 ; steps/s = 1.65, tokens/s = 81311 (36387 source, 44924 target) ; Learning rate = 0.000370 ; Loss = 1.517111\n",
      "2024-06-16 22:22:37.187000: I runner.py:310] Step = 57100 ; steps/s = 1.65, tokens/s = 81233 (36335 source, 44898 target) ; Learning rate = 0.000370 ; Loss = 1.524508\n",
      "2024-06-16 22:23:37.699000: I runner.py:310] Step = 57200 ; steps/s = 1.65, tokens/s = 81278 (36387 source, 44891 target) ; Learning rate = 0.000370 ; Loss = 1.527986\n",
      "2024-06-16 22:24:37.766000: I runner.py:310] Step = 57300 ; steps/s = 1.67, tokens/s = 80352 (35957 source, 44395 target) ; Learning rate = 0.000369 ; Loss = 1.511420\n",
      "2024-06-16 22:25:38.274000: I runner.py:310] Step = 57400 ; steps/s = 1.65, tokens/s = 81290 (36363 source, 44927 target) ; Learning rate = 0.000369 ; Loss = 1.519022\n",
      "2024-06-16 22:26:38.802000: I runner.py:310] Step = 57500 ; steps/s = 1.65, tokens/s = 81285 (36375 source, 44910 target) ; Learning rate = 0.000369 ; Loss = 1.528151\n",
      "2024-06-16 22:27:39.360000: I runner.py:310] Step = 57600 ; steps/s = 1.65, tokens/s = 81216 (36376 source, 44840 target) ; Learning rate = 0.000368 ; Loss = 1.519622\n",
      "2024-06-16 22:28:39.409000: I runner.py:310] Step = 57700 ; steps/s = 1.67, tokens/s = 80341 (35947 source, 44394 target) ; Learning rate = 0.000368 ; Loss = 1.512669\n",
      "2024-06-16 22:29:39.969000: I runner.py:310] Step = 57800 ; steps/s = 1.65, tokens/s = 81272 (36384 source, 44888 target) ; Learning rate = 0.000368 ; Loss = 1.517570\n",
      "2024-06-16 22:30:40.482000: I runner.py:310] Step = 57900 ; steps/s = 1.65, tokens/s = 81282 (36356 source, 44926 target) ; Learning rate = 0.000367 ; Loss = 1.520514\n",
      "2024-06-16 22:31:40.996000: I runner.py:310] Step = 58000 ; steps/s = 1.65, tokens/s = 81284 (36394 source, 44890 target) ; Learning rate = 0.000367 ; Loss = 1.523704\n",
      "2024-06-16 22:32:41.002000: I runner.py:310] Step = 58100 ; steps/s = 1.67, tokens/s = 80446 (36019 source, 44427 target) ; Learning rate = 0.000367 ; Loss = 1.519912\n",
      "2024-06-16 22:33:41.497000: I runner.py:310] Step = 58200 ; steps/s = 1.65, tokens/s = 81323 (36398 source, 44925 target) ; Learning rate = 0.000366 ; Loss = 1.512695\n",
      "2024-06-16 22:34:42.013000: I runner.py:310] Step = 58300 ; steps/s = 1.65, tokens/s = 81283 (36347 source, 44936 target) ; Learning rate = 0.000366 ; Loss = 1.517065\n",
      "2024-06-16 22:35:42.084000: I runner.py:310] Step = 58400 ; steps/s = 1.66, tokens/s = 80323 (35959 source, 44364 target) ; Learning rate = 0.000366 ; Loss = 1.519950\n",
      "2024-06-16 22:36:42.556000: I runner.py:310] Step = 58500 ; steps/s = 1.65, tokens/s = 81350 (36384 source, 44966 target) ; Learning rate = 0.000365 ; Loss = 1.531372\n",
      "2024-06-16 22:37:43.103000: I runner.py:310] Step = 58600 ; steps/s = 1.65, tokens/s = 81257 (36379 source, 44878 target) ; Learning rate = 0.000365 ; Loss = 1.516985\n",
      "2024-06-16 22:38:43.681000: I runner.py:310] Step = 58700 ; steps/s = 1.65, tokens/s = 81178 (36328 source, 44850 target) ; Learning rate = 0.000365 ; Loss = 1.512092\n",
      "2024-06-16 22:39:43.770000: I runner.py:310] Step = 58800 ; steps/s = 1.66, tokens/s = 80321 (35955 source, 44366 target) ; Learning rate = 0.000365 ; Loss = 1.516459\n",
      "2024-06-16 22:40:44.248000: I runner.py:310] Step = 58900 ; steps/s = 1.65, tokens/s = 81340 (36414 source, 44926 target) ; Learning rate = 0.000364 ; Loss = 1.512339\n",
      "2024-06-16 22:41:44.762000: I runner.py:310] Step = 59000 ; steps/s = 1.65, tokens/s = 81305 (36398 source, 44907 target) ; Learning rate = 0.000364 ; Loss = 1.513708\n",
      "2024-06-16 22:42:45.277000: I runner.py:310] Step = 59100 ; steps/s = 1.65, tokens/s = 81285 (36362 source, 44923 target) ; Learning rate = 0.000364 ; Loss = 1.519218\n",
      "2024-06-16 22:43:45.424000: I runner.py:310] Step = 59200 ; steps/s = 1.66, tokens/s = 80244 (35899 source, 44345 target) ; Learning rate = 0.000363 ; Loss = 1.519872\n",
      "2024-06-16 22:44:45.976000: I runner.py:310] Step = 59300 ; steps/s = 1.65, tokens/s = 81264 (36390 source, 44874 target) ; Learning rate = 0.000363 ; Loss = 1.513372\n",
      "2024-06-16 22:45:46.540000: I runner.py:310] Step = 59400 ; steps/s = 1.65, tokens/s = 81182 (36357 source, 44825 target) ; Learning rate = 0.000363 ; Loss = 1.515423\n",
      "2024-06-16 22:46:47.078000: I runner.py:310] Step = 59500 ; steps/s = 1.65, tokens/s = 81265 (36350 source, 44915 target) ; Learning rate = 0.000362 ; Loss = 1.516006\n",
      "2024-06-16 22:47:47.152000: I runner.py:310] Step = 59600 ; steps/s = 1.66, tokens/s = 80351 (35941 source, 44410 target) ; Learning rate = 0.000362 ; Loss = 1.514603\n",
      "2024-06-16 22:48:47.643000: I runner.py:310] Step = 59700 ; steps/s = 1.65, tokens/s = 81290 (36389 source, 44901 target) ; Learning rate = 0.000362 ; Loss = 1.508158\n",
      "2024-06-16 22:49:48.098000: I runner.py:310] Step = 59800 ; steps/s = 1.65, tokens/s = 81364 (36410 source, 44954 target) ; Learning rate = 0.000361 ; Loss = 1.517356\n",
      "2024-06-16 22:50:48.550000: I runner.py:310] Step = 59900 ; steps/s = 1.65, tokens/s = 81376 (36420 source, 44956 target) ; Learning rate = 0.000361 ; Loss = 1.521528\n",
      "2024-06-16 22:51:48.599000: I runner.py:310] Step = 60000 ; steps/s = 1.67, tokens/s = 80407 (35963 source, 44444 target) ; Learning rate = 0.000361 ; Loss = 1.507079\n",
      "2024-06-16 22:51:50.604000: I training.py:176] Saved checkpoint KK-EN-Standard-Transformer/ckpt-60000\n",
      "2024-06-16 22:51:50.604000: I training.py:192] Running evaluation for step 60000\n",
      "2024-06-16 22:56:01.534000: I training.py:192] Evaluation result for step 60000: loss = 1.425623 ; perplexity = 4.160450\n",
      "2024-06-16 22:57:01.957000: I runner.py:310] Step = 60100 ; steps/s = 1.66, tokens/s = 81496 (36499 source, 44997 target) ; Learning rate = 0.000361 ; Loss = 1.517293\n",
      "2024-06-16 22:58:02.475000: I runner.py:310] Step = 60200 ; steps/s = 1.65, tokens/s = 81260 (36371 source, 44889 target) ; Learning rate = 0.000360 ; Loss = 1.516879\n",
      "2024-06-16 22:59:02.935000: I runner.py:310] Step = 60300 ; steps/s = 1.65, tokens/s = 81346 (36405 source, 44941 target) ; Learning rate = 0.000360 ; Loss = 1.517274\n",
      "2024-06-16 23:00:02.961000: I runner.py:310] Step = 60400 ; steps/s = 1.67, tokens/s = 80431 (36009 source, 44422 target) ; Learning rate = 0.000360 ; Loss = 1.503999\n",
      "2024-06-16 23:01:03.514000: I runner.py:310] Step = 60500 ; steps/s = 1.65, tokens/s = 81244 (36364 source, 44880 target) ; Learning rate = 0.000359 ; Loss = 1.511798\n",
      "2024-06-16 23:02:03.977000: I runner.py:310] Step = 60600 ; steps/s = 1.65, tokens/s = 81383 (36421 source, 44962 target) ; Learning rate = 0.000359 ; Loss = 1.517105\n",
      "2024-06-16 23:03:04.012000: I runner.py:310] Step = 60700 ; steps/s = 1.67, tokens/s = 80333 (35936 source, 44397 target) ; Learning rate = 0.000359 ; Loss = 1.506943\n",
      "2024-06-16 23:04:04.523000: I runner.py:310] Step = 60800 ; steps/s = 1.65, tokens/s = 81263 (36368 source, 44895 target) ; Learning rate = 0.000358 ; Loss = 1.515114\n",
      "2024-06-16 23:05:05.027000: I runner.py:310] Step = 60900 ; steps/s = 1.65, tokens/s = 81322 (36413 source, 44909 target) ; Learning rate = 0.000358 ; Loss = 1.515010\n",
      "2024-06-16 23:06:05.597000: I runner.py:310] Step = 61000 ; steps/s = 1.65, tokens/s = 81226 (36332 source, 44894 target) ; Learning rate = 0.000358 ; Loss = 1.517118\n",
      "2024-06-16 23:07:05.740000: I runner.py:310] Step = 61100 ; steps/s = 1.66, tokens/s = 80270 (35913 source, 44357 target) ; Learning rate = 0.000358 ; Loss = 1.516143\n",
      "2024-06-16 23:08:06.320000: I runner.py:310] Step = 61200 ; steps/s = 1.65, tokens/s = 81179 (36328 source, 44851 target) ; Learning rate = 0.000357 ; Loss = 1.518168\n",
      "2024-06-16 23:09:06.823000: I runner.py:310] Step = 61300 ; steps/s = 1.65, tokens/s = 81269 (36390 source, 44879 target) ; Learning rate = 0.000357 ; Loss = 1.515320\n",
      "2024-06-16 23:10:07.352000: I runner.py:310] Step = 61400 ; steps/s = 1.65, tokens/s = 81281 (36379 source, 44902 target) ; Learning rate = 0.000357 ; Loss = 1.514557\n",
      "2024-06-16 23:11:07.427000: I runner.py:310] Step = 61500 ; steps/s = 1.66, tokens/s = 80388 (35958 source, 44430 target) ; Learning rate = 0.000356 ; Loss = 1.502514\n",
      "2024-06-16 23:12:07.982000: I runner.py:310] Step = 61600 ; steps/s = 1.65, tokens/s = 81226 (36338 source, 44888 target) ; Learning rate = 0.000356 ; Loss = 1.506664\n",
      "2024-06-16 23:13:08.434000: I runner.py:310] Step = 61700 ; steps/s = 1.65, tokens/s = 81423 (36451 source, 44972 target) ; Learning rate = 0.000356 ; Loss = 1.507944\n",
      "2024-06-16 23:14:08.927000: I runner.py:310] Step = 61800 ; steps/s = 1.65, tokens/s = 81276 (36374 source, 44902 target) ; Learning rate = 0.000356 ; Loss = 1.518423\n",
      "2024-06-16 23:15:09.041000: I runner.py:310] Step = 61900 ; steps/s = 1.66, tokens/s = 80255 (35926 source, 44329 target) ; Learning rate = 0.000355 ; Loss = 1.505236\n",
      "2024-06-16 23:16:09.524000: I runner.py:310] Step = 62000 ; steps/s = 1.65, tokens/s = 81338 (36396 source, 44942 target) ; Learning rate = 0.000355 ; Loss = 1.510850\n",
      "2024-06-16 23:17:10.056000: I runner.py:310] Step = 62100 ; steps/s = 1.65, tokens/s = 81271 (36369 source, 44902 target) ; Learning rate = 0.000355 ; Loss = 1.517379\n",
      "2024-06-16 23:18:10.628000: I runner.py:310] Step = 62200 ; steps/s = 1.65, tokens/s = 81198 (36338 source, 44860 target) ; Learning rate = 0.000354 ; Loss = 1.517558\n",
      "2024-06-16 23:19:10.717000: I runner.py:310] Step = 62300 ; steps/s = 1.66, tokens/s = 80324 (35969 source, 44355 target) ; Learning rate = 0.000354 ; Loss = 1.507442\n",
      "2024-06-16 23:20:11.263000: I runner.py:310] Step = 62400 ; steps/s = 1.65, tokens/s = 81210 (36319 source, 44891 target) ; Learning rate = 0.000354 ; Loss = 1.515243\n",
      "2024-06-16 23:21:11.781000: I runner.py:310] Step = 62500 ; steps/s = 1.65, tokens/s = 81282 (36410 source, 44872 target) ; Learning rate = 0.000354 ; Loss = 1.514978\n",
      "2024-06-16 23:22:12.373000: I runner.py:310] Step = 62600 ; steps/s = 1.65, tokens/s = 81219 (36343 source, 44876 target) ; Learning rate = 0.000353 ; Loss = 1.515744\n",
      "2024-06-16 23:23:12.425000: I runner.py:310] Step = 62700 ; steps/s = 1.67, tokens/s = 80382 (35975 source, 44407 target) ; Learning rate = 0.000353 ; Loss = 1.522461\n",
      "2024-06-16 23:24:12.950000: I runner.py:310] Step = 62800 ; steps/s = 1.65, tokens/s = 81253 (36347 source, 44906 target) ; Learning rate = 0.000353 ; Loss = 1.515391\n",
      "2024-06-16 23:25:13.532000: I runner.py:310] Step = 62900 ; steps/s = 1.65, tokens/s = 81225 (36376 source, 44849 target) ; Learning rate = 0.000352 ; Loss = 1.511954\n",
      "2024-06-16 23:26:13.531000: I runner.py:310] Step = 63000 ; steps/s = 1.67, tokens/s = 80430 (35991 source, 44439 target) ; Learning rate = 0.000352 ; Loss = 1.517157\n",
      "2024-06-16 23:27:14.055000: I runner.py:310] Step = 63100 ; steps/s = 1.65, tokens/s = 81307 (36372 source, 44935 target) ; Learning rate = 0.000352 ; Loss = 1.511706\n",
      "2024-06-16 23:28:14.565000: I runner.py:310] Step = 63200 ; steps/s = 1.65, tokens/s = 81283 (36382 source, 44901 target) ; Learning rate = 0.000352 ; Loss = 1.505766\n",
      "2024-06-16 23:29:15.105000: I runner.py:310] Step = 63300 ; steps/s = 1.65, tokens/s = 81257 (36369 source, 44888 target) ; Learning rate = 0.000351 ; Loss = 1.515472\n",
      "2024-06-16 23:30:15.079000: I runner.py:310] Step = 63400 ; steps/s = 1.67, tokens/s = 80470 (36019 source, 44451 target) ; Learning rate = 0.000351 ; Loss = 1.514305\n",
      "2024-06-16 23:31:15.587000: I runner.py:310] Step = 63500 ; steps/s = 1.65, tokens/s = 81285 (36363 source, 44922 target) ; Learning rate = 0.000351 ; Loss = 1.518594\n",
      "2024-06-16 23:32:16.100000: I runner.py:310] Step = 63600 ; steps/s = 1.65, tokens/s = 81289 (36370 source, 44919 target) ; Learning rate = 0.000350 ; Loss = 1.507527\n",
      "2024-06-16 23:33:16.540000: I runner.py:310] Step = 63700 ; steps/s = 1.65, tokens/s = 81409 (36454 source, 44955 target) ; Learning rate = 0.000350 ; Loss = 1.512285\n",
      "2024-06-16 23:34:16.574000: I runner.py:310] Step = 63800 ; steps/s = 1.67, tokens/s = 80379 (35965 source, 44414 target) ; Learning rate = 0.000350 ; Loss = 1.512407\n",
      "2024-06-16 23:35:17.155000: I runner.py:310] Step = 63900 ; steps/s = 1.65, tokens/s = 81229 (36363 source, 44866 target) ; Learning rate = 0.000350 ; Loss = 1.512576\n",
      "2024-06-16 23:36:17.710000: I runner.py:310] Step = 64000 ; steps/s = 1.65, tokens/s = 81192 (36336 source, 44856 target) ; Learning rate = 0.000349 ; Loss = 1.511347\n",
      "2024-06-16 23:37:18.261000: I runner.py:310] Step = 64100 ; steps/s = 1.65, tokens/s = 81258 (36354 source, 44904 target) ; Learning rate = 0.000349 ; Loss = 1.507912\n",
      "2024-06-16 23:38:18.334000: I runner.py:310] Step = 64200 ; steps/s = 1.66, tokens/s = 80372 (35974 source, 44398 target) ; Learning rate = 0.000349 ; Loss = 1.510107\n",
      "2024-06-16 23:39:18.849000: I runner.py:310] Step = 64300 ; steps/s = 1.65, tokens/s = 81263 (36366 source, 44897 target) ; Learning rate = 0.000349 ; Loss = 1.512819\n",
      "2024-06-16 23:40:19.373000: I runner.py:310] Step = 64400 ; steps/s = 1.65, tokens/s = 81254 (36376 source, 44878 target) ; Learning rate = 0.000348 ; Loss = 1.510682\n",
      "2024-06-16 23:41:19.895000: I runner.py:310] Step = 64500 ; steps/s = 1.65, tokens/s = 81313 (36384 source, 44929 target) ; Learning rate = 0.000348 ; Loss = 1.514157\n",
      "2024-06-16 23:42:19.929000: I runner.py:310] Step = 64600 ; steps/s = 1.67, tokens/s = 80419 (35991 source, 44428 target) ; Learning rate = 0.000348 ; Loss = 1.514274\n",
      "2024-06-16 23:43:20.345000: I runner.py:310] Step = 64700 ; steps/s = 1.66, tokens/s = 81396 (36403 source, 44993 target) ; Learning rate = 0.000347 ; Loss = 1.501406\n",
      "2024-06-16 23:44:20.803000: I runner.py:310] Step = 64800 ; steps/s = 1.65, tokens/s = 81360 (36430 source, 44930 target) ; Learning rate = 0.000347 ; Loss = 1.513200\n",
      "2024-06-16 23:45:21.276000: I runner.py:310] Step = 64900 ; steps/s = 1.65, tokens/s = 81351 (36425 source, 44926 target) ; Learning rate = 0.000347 ; Loss = 1.509737\n",
      "2024-06-16 23:46:21.357000: I runner.py:310] Step = 65000 ; steps/s = 1.66, tokens/s = 80355 (35956 source, 44399 target) ; Learning rate = 0.000347 ; Loss = 1.515866\n",
      "2024-06-16 23:46:21.358000: I training.py:192] Running evaluation for step 65000\n",
      "2024-06-16 23:50:37.989000: I training.py:192] Evaluation result for step 65000: loss = 1.429760 ; perplexity = 4.177696\n",
      "2024-06-16 23:51:38.342000: I runner.py:310] Step = 65100 ; steps/s = 1.66, tokens/s = 81524 (36491 source, 45033 target) ; Learning rate = 0.000346 ; Loss = 1.503724\n",
      "2024-06-16 23:52:38.945000: I runner.py:310] Step = 65200 ; steps/s = 1.65, tokens/s = 81140 (36314 source, 44826 target) ; Learning rate = 0.000346 ; Loss = 1.515041\n",
      "2024-06-16 23:53:39.438000: I runner.py:310] Step = 65300 ; steps/s = 1.65, tokens/s = 81306 (36384 source, 44922 target) ; Learning rate = 0.000346 ; Loss = 1.512774\n",
      "2024-06-16 23:54:39.577000: I runner.py:310] Step = 65400 ; steps/s = 1.66, tokens/s = 80242 (35889 source, 44353 target) ; Learning rate = 0.000346 ; Loss = 1.501232\n",
      "2024-06-16 23:55:40.076000: I runner.py:310] Step = 65500 ; steps/s = 1.65, tokens/s = 81340 (36449 source, 44891 target) ; Learning rate = 0.000345 ; Loss = 1.513671\n",
      "2024-06-16 23:56:40.561000: I runner.py:310] Step = 65600 ; steps/s = 1.65, tokens/s = 81332 (36392 source, 44940 target) ; Learning rate = 0.000345 ; Loss = 1.510376\n",
      "2024-06-16 23:57:40.626000: I runner.py:310] Step = 65700 ; steps/s = 1.67, tokens/s = 80328 (35943 source, 44385 target) ; Learning rate = 0.000345 ; Loss = 1.515767\n",
      "2024-06-16 23:58:41.177000: I runner.py:310] Step = 65800 ; steps/s = 1.65, tokens/s = 81260 (36352 source, 44908 target) ; Learning rate = 0.000345 ; Loss = 1.510501\n",
      "2024-06-16 23:59:41.652000: I runner.py:310] Step = 65900 ; steps/s = 1.65, tokens/s = 81337 (36397 source, 44940 target) ; Learning rate = 0.000344 ; Loss = 1.505268\n",
      "2024-06-17 00:00:42.190000: I runner.py:310] Step = 66000 ; steps/s = 1.65, tokens/s = 81229 (36346 source, 44883 target) ; Learning rate = 0.000344 ; Loss = 1.507490\n",
      "2024-06-17 00:01:42.224000: I runner.py:310] Step = 66100 ; steps/s = 1.67, tokens/s = 80399 (35983 source, 44416 target) ; Learning rate = 0.000344 ; Loss = 1.504731\n",
      "2024-06-17 00:02:42.761000: I runner.py:310] Step = 66200 ; steps/s = 1.65, tokens/s = 81245 (36376 source, 44869 target) ; Learning rate = 0.000344 ; Loss = 1.508231\n",
      "2024-06-17 00:03:43.277000: I runner.py:310] Step = 66300 ; steps/s = 1.65, tokens/s = 81297 (36402 source, 44895 target) ; Learning rate = 0.000343 ; Loss = 1.515368\n",
      "2024-06-17 00:04:43.801000: I runner.py:310] Step = 66400 ; steps/s = 1.65, tokens/s = 81271 (36341 source, 44930 target) ; Learning rate = 0.000343 ; Loss = 1.510684\n",
      "2024-06-17 00:05:43.819000: I runner.py:310] Step = 66500 ; steps/s = 1.67, tokens/s = 80426 (35991 source, 44435 target) ; Learning rate = 0.000343 ; Loss = 1.509452\n",
      "2024-06-17 00:06:44.304000: I runner.py:310] Step = 66600 ; steps/s = 1.65, tokens/s = 81340 (36409 source, 44931 target) ; Learning rate = 0.000342 ; Loss = 1.495664\n",
      "2024-06-17 00:07:44.807000: I runner.py:310] Step = 66700 ; steps/s = 1.65, tokens/s = 81278 (36398 source, 44880 target) ; Learning rate = 0.000342 ; Loss = 1.509665\n",
      "2024-06-17 00:08:45.371000: I runner.py:310] Step = 66800 ; steps/s = 1.65, tokens/s = 81243 (36355 source, 44888 target) ; Learning rate = 0.000342 ; Loss = 1.501586\n",
      "2024-06-17 00:09:45.474000: I runner.py:310] Step = 66900 ; steps/s = 1.66, tokens/s = 80269 (35890 source, 44379 target) ; Learning rate = 0.000342 ; Loss = 1.510267\n",
      "2024-06-17 00:10:45.978000: I runner.py:310] Step = 67000 ; steps/s = 1.65, tokens/s = 81281 (36368 source, 44913 target) ; Learning rate = 0.000341 ; Loss = 1.508935\n",
      "2024-06-17 00:11:46.470000: I runner.py:310] Step = 67100 ; steps/s = 1.65, tokens/s = 81332 (36432 source, 44900 target) ; Learning rate = 0.000341 ; Loss = 1.515891\n",
      "2024-06-17 00:12:47.063000: I runner.py:310] Step = 67200 ; steps/s = 1.65, tokens/s = 81210 (36336 source, 44874 target) ; Learning rate = 0.000341 ; Loss = 1.509711\n",
      "2024-06-17 00:13:47.181000: I runner.py:310] Step = 67300 ; steps/s = 1.66, tokens/s = 80275 (35933 source, 44342 target) ; Learning rate = 0.000341 ; Loss = 1.501595\n",
      "2024-06-17 00:14:47.722000: I runner.py:310] Step = 67400 ; steps/s = 1.65, tokens/s = 81278 (36365 source, 44913 target) ; Learning rate = 0.000340 ; Loss = 1.508079\n",
      "2024-06-17 00:15:48.279000: I runner.py:310] Step = 67500 ; steps/s = 1.65, tokens/s = 81238 (36371 source, 44867 target) ; Learning rate = 0.000340 ; Loss = 1.510437\n",
      "2024-06-17 00:16:48.842000: I runner.py:310] Step = 67600 ; steps/s = 1.65, tokens/s = 81218 (36351 source, 44867 target) ; Learning rate = 0.000340 ; Loss = 1.518116\n",
      "2024-06-17 00:17:48.935000: I runner.py:310] Step = 67700 ; steps/s = 1.66, tokens/s = 80299 (35921 source, 44378 target) ; Learning rate = 0.000340 ; Loss = 1.498937\n",
      "2024-06-17 00:18:49.457000: I runner.py:310] Step = 67800 ; steps/s = 1.65, tokens/s = 81254 (36356 source, 44898 target) ; Learning rate = 0.000339 ; Loss = 1.509879\n",
      "2024-06-17 00:19:49.987000: I runner.py:310] Step = 67900 ; steps/s = 1.65, tokens/s = 81288 (36405 source, 44883 target) ; Learning rate = 0.000339 ; Loss = 1.506392\n",
      "2024-06-17 00:20:49.972000: I runner.py:310] Step = 68000 ; steps/s = 1.67, tokens/s = 80471 (36018 source, 44453 target) ; Learning rate = 0.000339 ; Loss = 1.509986\n",
      "2024-06-17 00:21:50.537000: I runner.py:310] Step = 68100 ; steps/s = 1.65, tokens/s = 81230 (36359 source, 44871 target) ; Learning rate = 0.000339 ; Loss = 1.499819\n",
      "2024-06-17 00:22:51.017000: I runner.py:310] Step = 68200 ; steps/s = 1.65, tokens/s = 81336 (36410 source, 44926 target) ; Learning rate = 0.000338 ; Loss = 1.504707\n",
      "2024-06-17 00:23:51.547000: I runner.py:310] Step = 68300 ; steps/s = 1.65, tokens/s = 81250 (36361 source, 44889 target) ; Learning rate = 0.000338 ; Loss = 1.512146\n",
      "2024-06-17 00:24:51.553000: I runner.py:310] Step = 68400 ; steps/s = 1.67, tokens/s = 80448 (35988 source, 44460 target) ; Learning rate = 0.000338 ; Loss = 1.504793\n",
      "2024-06-17 00:25:52.001000: I runner.py:310] Step = 68500 ; steps/s = 1.65, tokens/s = 81371 (36392 source, 44979 target) ; Learning rate = 0.000338 ; Loss = 1.507773\n",
      "2024-06-17 00:26:52.548000: I runner.py:310] Step = 68600 ; steps/s = 1.65, tokens/s = 81284 (36394 source, 44890 target) ; Learning rate = 0.000337 ; Loss = 1.502991\n",
      "2024-06-17 00:27:53.077000: I runner.py:310] Step = 68700 ; steps/s = 1.65, tokens/s = 81235 (36352 source, 44883 target) ; Learning rate = 0.000337 ; Loss = 1.502561\n",
      "2024-06-17 00:28:53.117000: I runner.py:310] Step = 68800 ; steps/s = 1.67, tokens/s = 80372 (35984 source, 44388 target) ; Learning rate = 0.000337 ; Loss = 1.507506\n",
      "2024-06-17 00:29:53.623000: I runner.py:310] Step = 68900 ; steps/s = 1.65, tokens/s = 81294 (36363 source, 44931 target) ; Learning rate = 0.000337 ; Loss = 1.506207\n",
      "2024-06-17 00:30:54.119000: I runner.py:310] Step = 69000 ; steps/s = 1.65, tokens/s = 81269 (36376 source, 44893 target) ; Learning rate = 0.000336 ; Loss = 1.504245\n",
      "2024-06-17 00:31:54.672000: I runner.py:310] Step = 69100 ; steps/s = 1.65, tokens/s = 81255 (36373 source, 44882 target) ; Learning rate = 0.000336 ; Loss = 1.512005\n",
      "2024-06-17 00:32:54.786000: I runner.py:310] Step = 69200 ; steps/s = 1.66, tokens/s = 80325 (35944 source, 44381 target) ; Learning rate = 0.000336 ; Loss = 1.499293\n",
      "2024-06-17 00:33:55.279000: I runner.py:310] Step = 69300 ; steps/s = 1.65, tokens/s = 81324 (36398 source, 44926 target) ; Learning rate = 0.000336 ; Loss = 1.501171\n",
      "2024-06-17 00:34:55.830000: I runner.py:310] Step = 69400 ; steps/s = 1.65, tokens/s = 81239 (36373 source, 44866 target) ; Learning rate = 0.000336 ; Loss = 1.508370\n",
      "2024-06-17 00:35:56.333000: I runner.py:310] Step = 69500 ; steps/s = 1.65, tokens/s = 81272 (36361 source, 44911 target) ; Learning rate = 0.000335 ; Loss = 1.513418\n",
      "2024-06-17 00:36:56.320000: I runner.py:310] Step = 69600 ; steps/s = 1.67, tokens/s = 80478 (36033 source, 44445 target) ; Learning rate = 0.000335 ; Loss = 1.504020\n",
      "2024-06-17 00:37:56.826000: I runner.py:310] Step = 69700 ; steps/s = 1.65, tokens/s = 81292 (36354 source, 44938 target) ; Learning rate = 0.000335 ; Loss = 1.503979\n",
      "2024-06-17 00:38:57.317000: I runner.py:310] Step = 69800 ; steps/s = 1.65, tokens/s = 81324 (36425 source, 44899 target) ; Learning rate = 0.000335 ; Loss = 1.504982\n",
      "2024-06-17 00:39:57.748000: I runner.py:310] Step = 69900 ; steps/s = 1.65, tokens/s = 81389 (36412 source, 44977 target) ; Learning rate = 0.000334 ; Loss = 1.510871\n",
      "2024-06-17 00:40:57.867000: I runner.py:310] Step = 70000 ; steps/s = 1.66, tokens/s = 80288 (35922 source, 44366 target) ; Learning rate = 0.000334 ; Loss = 1.508240\n",
      "2024-06-17 00:40:59.940000: I training.py:176] Saved checkpoint KK-EN-Standard-Transformer/ckpt-70000\n",
      "2024-06-17 00:40:59.940000: I training.py:192] Running evaluation for step 70000\n",
      "2024-06-17 00:45:10.236000: I training.py:192] Evaluation result for step 70000: loss = 1.436058 ; perplexity = 4.204092\n",
      "2024-06-17 00:46:10.502000: I runner.py:310] Step = 70100 ; steps/s = 1.66, tokens/s = 81647 (36538 source, 45109 target) ; Learning rate = 0.000334 ; Loss = 1.504537\n",
      "2024-06-17 00:47:11.045000: I runner.py:310] Step = 70200 ; steps/s = 1.65, tokens/s = 81267 (36377 source, 44890 target) ; Learning rate = 0.000334 ; Loss = 1.507737\n",
      "2024-06-17 00:48:11.186000: I runner.py:310] Step = 70300 ; steps/s = 1.66, tokens/s = 80211 (35915 source, 44296 target) ; Learning rate = 0.000333 ; Loss = 1.497707\n",
      "2024-06-17 00:49:11.680000: I runner.py:310] Step = 70400 ; steps/s = 1.65, tokens/s = 81317 (36373 source, 44944 target) ; Learning rate = 0.000333 ; Loss = 1.505778\n",
      "2024-06-17 00:50:12.235000: I runner.py:310] Step = 70500 ; steps/s = 1.65, tokens/s = 81269 (36392 source, 44877 target) ; Learning rate = 0.000333 ; Loss = 1.497525\n",
      "2024-06-17 00:51:12.722000: I runner.py:310] Step = 70600 ; steps/s = 1.65, tokens/s = 81307 (36396 source, 44911 target) ; Learning rate = 0.000333 ; Loss = 1.501335\n",
      "2024-06-17 00:52:12.781000: I runner.py:310] Step = 70700 ; steps/s = 1.67, tokens/s = 80363 (35947 source, 44416 target) ; Learning rate = 0.000332 ; Loss = 1.506633\n",
      "2024-06-17 00:53:13.316000: I runner.py:310] Step = 70800 ; steps/s = 1.65, tokens/s = 81300 (36383 source, 44917 target) ; Learning rate = 0.000332 ; Loss = 1.498466\n",
      "2024-06-17 00:54:13.868000: I runner.py:310] Step = 70900 ; steps/s = 1.65, tokens/s = 81217 (36350 source, 44867 target) ; Learning rate = 0.000332 ; Loss = 1.501019\n",
      "2024-06-17 00:55:14.400000: I runner.py:310] Step = 71000 ; steps/s = 1.65, tokens/s = 81261 (36359 source, 44902 target) ; Learning rate = 0.000332 ; Loss = 1.506036\n",
      "2024-06-17 00:56:14.512000: I runner.py:310] Step = 71100 ; steps/s = 1.66, tokens/s = 80259 (35921 source, 44338 target) ; Learning rate = 0.000331 ; Loss = 1.508347\n",
      "2024-06-17 00:57:15.067000: I runner.py:310] Step = 71200 ; steps/s = 1.65, tokens/s = 81234 (36339 source, 44895 target) ; Learning rate = 0.000331 ; Loss = 1.505048\n",
      "2024-06-17 00:58:15.563000: I runner.py:310] Step = 71300 ; steps/s = 1.65, tokens/s = 81336 (36412 source, 44924 target) ; Learning rate = 0.000331 ; Loss = 1.504037\n",
      "2024-06-17 00:59:16.015000: I runner.py:310] Step = 71400 ; steps/s = 1.65, tokens/s = 81363 (36415 source, 44948 target) ; Learning rate = 0.000331 ; Loss = 1.508169\n",
      "2024-06-17 01:00:16.061000: I runner.py:310] Step = 71500 ; steps/s = 1.67, tokens/s = 80384 (35969 source, 44415 target) ; Learning rate = 0.000331 ; Loss = 1.499065\n",
      "2024-06-17 01:01:16.539000: I runner.py:310] Step = 71600 ; steps/s = 1.65, tokens/s = 81315 (36397 source, 44918 target) ; Learning rate = 0.000330 ; Loss = 1.506382\n",
      "2024-06-17 01:02:17.078000: I runner.py:310] Step = 71700 ; steps/s = 1.65, tokens/s = 81263 (36380 source, 44883 target) ; Learning rate = 0.000330 ; Loss = 1.517423\n",
      "2024-06-17 01:03:17.577000: I runner.py:310] Step = 71800 ; steps/s = 1.65, tokens/s = 81316 (36391 source, 44925 target) ; Learning rate = 0.000330 ; Loss = 1.504945\n",
      "2024-06-17 01:04:17.621000: I runner.py:310] Step = 71900 ; steps/s = 1.67, tokens/s = 80364 (35931 source, 44433 target) ; Learning rate = 0.000330 ; Loss = 1.501134\n",
      "2024-06-17 01:05:18.083000: I runner.py:310] Step = 72000 ; steps/s = 1.65, tokens/s = 81365 (36439 source, 44926 target) ; Learning rate = 0.000329 ; Loss = 1.505019\n",
      "2024-06-17 01:06:18.609000: I runner.py:310] Step = 72100 ; steps/s = 1.65, tokens/s = 81266 (36383 source, 44883 target) ; Learning rate = 0.000329 ; Loss = 1.506872\n",
      "2024-06-17 01:07:19.136000: I runner.py:310] Step = 72200 ; steps/s = 1.65, tokens/s = 81285 (36373 source, 44912 target) ; Learning rate = 0.000329 ; Loss = 1.511511\n",
      "2024-06-17 01:08:19.218000: I runner.py:310] Step = 72300 ; steps/s = 1.66, tokens/s = 80322 (35948 source, 44374 target) ; Learning rate = 0.000329 ; Loss = 1.503498\n",
      "2024-06-17 01:09:19.702000: I runner.py:310] Step = 72400 ; steps/s = 1.65, tokens/s = 81317 (36372 source, 44945 target) ; Learning rate = 0.000328 ; Loss = 1.495878\n",
      "2024-06-17 01:10:20.235000: I runner.py:310] Step = 72500 ; steps/s = 1.65, tokens/s = 81238 (36353 source, 44885 target) ; Learning rate = 0.000328 ; Loss = 1.508162\n",
      "2024-06-17 01:11:20.711000: I runner.py:310] Step = 72600 ; steps/s = 1.65, tokens/s = 81259 (36396 source, 44863 target) ; Learning rate = 0.000328 ; Loss = 1.526670\n",
      "2024-06-17 01:12:20.822000: I runner.py:310] Step = 72700 ; steps/s = 1.66, tokens/s = 80406 (35966 source, 44440 target) ; Learning rate = 0.000328 ; Loss = 1.502871\n",
      "2024-06-17 01:13:21.379000: I runner.py:310] Step = 72800 ; steps/s = 1.65, tokens/s = 81219 (36326 source, 44893 target) ; Learning rate = 0.000328 ; Loss = 1.499532\n",
      "2024-06-17 01:14:21.863000: I runner.py:310] Step = 72900 ; steps/s = 1.65, tokens/s = 81347 (36423 source, 44924 target) ; Learning rate = 0.000327 ; Loss = 1.504173\n",
      "2024-06-17 01:15:21.919000: I runner.py:310] Step = 73000 ; steps/s = 1.67, tokens/s = 80349 (35987 source, 44362 target) ; Learning rate = 0.000327 ; Loss = 1.507042\n",
      "2024-06-17 01:16:22.462000: I runner.py:310] Step = 73100 ; steps/s = 1.65, tokens/s = 81256 (36356 source, 44900 target) ; Learning rate = 0.000327 ; Loss = 1.505842\n",
      "2024-06-17 01:17:22.936000: I runner.py:310] Step = 73200 ; steps/s = 1.65, tokens/s = 81341 (36394 source, 44947 target) ; Learning rate = 0.000327 ; Loss = 1.498268\n",
      "2024-06-17 01:18:23.393000: I runner.py:310] Step = 73300 ; steps/s = 1.65, tokens/s = 81381 (36452 source, 44929 target) ; Learning rate = 0.000326 ; Loss = 1.501214\n",
      "2024-06-17 01:19:23.484000: I runner.py:310] Step = 73400 ; steps/s = 1.66, tokens/s = 80304 (35929 source, 44375 target) ; Learning rate = 0.000326 ; Loss = 1.504082\n",
      "2024-06-17 01:20:23.977000: I runner.py:310] Step = 73500 ; steps/s = 1.65, tokens/s = 81325 (36393 source, 44932 target) ; Learning rate = 0.000326 ; Loss = 1.504221\n",
      "2024-06-17 01:21:24.468000: I runner.py:310] Step = 73600 ; steps/s = 1.65, tokens/s = 81297 (36356 source, 44941 target) ; Learning rate = 0.000326 ; Loss = 1.501521\n",
      "2024-06-17 01:22:25.000000: I runner.py:310] Step = 73700 ; steps/s = 1.65, tokens/s = 81280 (36383 source, 44897 target) ; Learning rate = 0.000326 ; Loss = 1.506944\n",
      "2024-06-17 01:23:25.034000: I runner.py:310] Step = 73800 ; steps/s = 1.67, tokens/s = 80373 (35980 source, 44393 target) ; Learning rate = 0.000325 ; Loss = 1.499980\n",
      "2024-06-17 01:24:25.519000: I runner.py:310] Step = 73900 ; steps/s = 1.65, tokens/s = 81338 (36385 source, 44953 target) ; Learning rate = 0.000325 ; Loss = 1.501589\n",
      "2024-06-17 01:25:25.986000: I runner.py:310] Step = 74000 ; steps/s = 1.65, tokens/s = 81354 (36419 source, 44935 target) ; Learning rate = 0.000325 ; Loss = 1.498384\n",
      "2024-06-17 01:26:26.500000: I runner.py:310] Step = 74100 ; steps/s = 1.65, tokens/s = 81272 (36379 source, 44893 target) ; Learning rate = 0.000325 ; Loss = 1.506787\n",
      "2024-06-17 01:27:26.653000: I runner.py:310] Step = 74200 ; steps/s = 1.66, tokens/s = 80239 (35917 source, 44322 target) ; Learning rate = 0.000324 ; Loss = 1.512789\n",
      "2024-06-17 01:28:27.168000: I runner.py:310] Step = 74300 ; steps/s = 1.65, tokens/s = 81302 (36352 source, 44950 target) ; Learning rate = 0.000324 ; Loss = 1.491450\n",
      "2024-06-17 01:29:27.672000: I runner.py:310] Step = 74400 ; steps/s = 1.65, tokens/s = 81271 (36395 source, 44876 target) ; Learning rate = 0.000324 ; Loss = 1.496857\n",
      "2024-06-17 01:30:28.198000: I runner.py:310] Step = 74500 ; steps/s = 1.65, tokens/s = 81318 (36406 source, 44912 target) ; Learning rate = 0.000324 ; Loss = 1.502108\n",
      "2024-06-17 01:31:28.270000: I runner.py:310] Step = 74600 ; steps/s = 1.66, tokens/s = 80311 (35926 source, 44385 target) ; Learning rate = 0.000324 ; Loss = 1.498747\n",
      "2024-06-17 01:32:28.764000: I runner.py:310] Step = 74700 ; steps/s = 1.65, tokens/s = 81319 (36383 source, 44936 target) ; Learning rate = 0.000323 ; Loss = 1.504256\n",
      "2024-06-17 01:33:29.313000: I runner.py:310] Step = 74800 ; steps/s = 1.65, tokens/s = 81230 (36356 source, 44874 target) ; Learning rate = 0.000323 ; Loss = 1.501380\n",
      "2024-06-17 01:34:29.810000: I runner.py:310] Step = 74900 ; steps/s = 1.65, tokens/s = 81341 (36435 source, 44906 target) ; Learning rate = 0.000323 ; Loss = 1.505748\n",
      "2024-06-17 01:35:29.804000: I runner.py:310] Step = 75000 ; steps/s = 1.67, tokens/s = 80453 (36003 source, 44450 target) ; Learning rate = 0.000323 ; Loss = 1.500787\n",
      "2024-06-17 01:35:29.805000: I training.py:192] Running evaluation for step 75000\n",
      "2024-06-17 01:39:36.277000: I training.py:192] Evaluation result for step 75000: loss = 1.445526 ; perplexity = 4.244085\n",
      "2024-06-17 01:40:36.583000: I runner.py:310] Step = 75100 ; steps/s = 1.66, tokens/s = 81587 (36534 source, 45053 target) ; Learning rate = 0.000323 ; Loss = 1.503208\n",
      "2024-06-17 01:41:37.132000: I runner.py:310] Step = 75200 ; steps/s = 1.65, tokens/s = 81240 (36351 source, 44889 target) ; Learning rate = 0.000322 ; Loss = 1.507243\n",
      "2024-06-17 01:42:37.184000: I runner.py:310] Step = 75300 ; steps/s = 1.67, tokens/s = 80350 (35953 source, 44397 target) ; Learning rate = 0.000322 ; Loss = 1.498572\n",
      "2024-06-17 01:43:37.698000: I runner.py:310] Step = 75400 ; steps/s = 1.65, tokens/s = 81297 (36377 source, 44920 target) ; Learning rate = 0.000322 ; Loss = 1.502891\n",
      "2024-06-17 01:44:38.156000: I runner.py:310] Step = 75500 ; steps/s = 1.65, tokens/s = 81389 (36440 source, 44949 target) ; Learning rate = 0.000322 ; Loss = 1.498948\n",
      "2024-06-17 01:45:38.685000: I runner.py:310] Step = 75600 ; steps/s = 1.65, tokens/s = 81246 (36355 source, 44891 target) ; Learning rate = 0.000321 ; Loss = 1.502920\n",
      "2024-06-17 01:46:38.832000: I runner.py:310] Step = 75700 ; steps/s = 1.66, tokens/s = 80237 (35919 source, 44318 target) ; Learning rate = 0.000321 ; Loss = 1.493081\n",
      "2024-06-17 01:47:39.360000: I runner.py:310] Step = 75800 ; steps/s = 1.65, tokens/s = 81287 (36381 source, 44906 target) ; Learning rate = 0.000321 ; Loss = 1.498488\n",
      "2024-06-17 01:48:39.825000: I runner.py:310] Step = 75900 ; steps/s = 1.65, tokens/s = 81325 (36395 source, 44930 target) ; Learning rate = 0.000321 ; Loss = 1.498588\n",
      "2024-06-17 01:49:40.346000: I runner.py:310] Step = 76000 ; steps/s = 1.65, tokens/s = 81297 (36373 source, 44924 target) ; Learning rate = 0.000321 ; Loss = 1.503984\n",
      "2024-06-17 01:50:40.352000: I runner.py:310] Step = 76100 ; steps/s = 1.67, tokens/s = 80388 (35963 source, 44425 target) ; Learning rate = 0.000320 ; Loss = 1.503599\n",
      "2024-06-17 01:51:40.832000: I runner.py:310] Step = 76200 ; steps/s = 1.65, tokens/s = 81367 (36440 source, 44927 target) ; Learning rate = 0.000320 ; Loss = 1.509683\n",
      "2024-06-17 01:52:41.379000: I runner.py:310] Step = 76300 ; steps/s = 1.65, tokens/s = 81257 (36383 source, 44874 target) ; Learning rate = 0.000320 ; Loss = 1.496357\n",
      "2024-06-17 01:53:41.913000: I runner.py:310] Step = 76400 ; steps/s = 1.65, tokens/s = 81263 (36344 source, 44919 target) ; Learning rate = 0.000320 ; Loss = 1.496725\n",
      "2024-06-17 01:54:41.930000: I runner.py:310] Step = 76500 ; steps/s = 1.67, tokens/s = 80433 (35997 source, 44436 target) ; Learning rate = 0.000320 ; Loss = 1.500440\n",
      "2024-06-17 01:55:42.459000: I runner.py:310] Step = 76600 ; steps/s = 1.65, tokens/s = 81266 (36365 source, 44901 target) ; Learning rate = 0.000319 ; Loss = 1.495260\n",
      "2024-06-17 01:56:42.950000: I runner.py:310] Step = 76700 ; steps/s = 1.65, tokens/s = 81295 (36388 source, 44907 target) ; Learning rate = 0.000319 ; Loss = 1.500980\n",
      "2024-06-17 01:57:43.476000: I runner.py:310] Step = 76800 ; steps/s = 1.65, tokens/s = 81262 (36356 source, 44906 target) ; Learning rate = 0.000319 ; Loss = 1.499673\n",
      "2024-06-17 01:58:43.538000: I runner.py:310] Step = 76900 ; steps/s = 1.67, tokens/s = 80391 (36006 source, 44385 target) ; Learning rate = 0.000319 ; Loss = 1.492389\n",
      "2024-06-17 01:59:44.074000: I runner.py:310] Step = 77000 ; steps/s = 1.65, tokens/s = 81264 (36368 source, 44896 target) ; Learning rate = 0.000319 ; Loss = 1.496542\n",
      "2024-06-17 02:00:44.617000: I runner.py:310] Step = 77100 ; steps/s = 1.65, tokens/s = 81249 (36352 source, 44897 target) ; Learning rate = 0.000318 ; Loss = 1.500178\n",
      "2024-06-17 02:01:45.144000: I runner.py:310] Step = 77200 ; steps/s = 1.65, tokens/s = 81246 (36353 source, 44893 target) ; Learning rate = 0.000318 ; Loss = 1.511185\n",
      "2024-06-17 02:02:45.196000: I runner.py:310] Step = 77300 ; steps/s = 1.67, tokens/s = 80372 (35946 source, 44426 target) ; Learning rate = 0.000318 ; Loss = 1.508207\n",
      "2024-06-17 02:03:45.743000: I runner.py:310] Step = 77400 ; steps/s = 1.65, tokens/s = 81249 (36388 source, 44861 target) ; Learning rate = 0.000318 ; Loss = 1.494833\n",
      "2024-06-17 02:04:46.261000: I runner.py:310] Step = 77500 ; steps/s = 1.65, tokens/s = 81278 (36383 source, 44895 target) ; Learning rate = 0.000317 ; Loss = 1.496624\n",
      "2024-06-17 02:05:46.318000: I runner.py:310] Step = 77600 ; steps/s = 1.67, tokens/s = 80345 (35957 source, 44388 target) ; Learning rate = 0.000317 ; Loss = 1.500391\n",
      "2024-06-17 02:06:46.768000: I runner.py:310] Step = 77700 ; steps/s = 1.65, tokens/s = 81371 (36393 source, 44978 target) ; Learning rate = 0.000317 ; Loss = 1.499486\n",
      "2024-06-17 02:07:47.279000: I runner.py:310] Step = 77800 ; steps/s = 1.65, tokens/s = 81332 (36395 source, 44937 target) ; Learning rate = 0.000317 ; Loss = 1.498359\n",
      "2024-06-17 02:08:47.828000: I runner.py:310] Step = 77900 ; steps/s = 1.65, tokens/s = 81225 (36371 source, 44854 target) ; Learning rate = 0.000317 ; Loss = 1.493649\n",
      "2024-06-17 02:09:47.948000: I runner.py:310] Step = 78000 ; steps/s = 1.66, tokens/s = 80278 (35937 source, 44341 target) ; Learning rate = 0.000316 ; Loss = 1.501979\n",
      "2024-06-17 02:10:48.376000: I runner.py:310] Step = 78100 ; steps/s = 1.66, tokens/s = 81399 (36410 source, 44989 target) ; Learning rate = 0.000316 ; Loss = 1.492681\n",
      "2024-06-17 02:11:48.885000: I runner.py:310] Step = 78200 ; steps/s = 1.65, tokens/s = 81285 (36392 source, 44893 target) ; Learning rate = 0.000316 ; Loss = 1.496907\n",
      "2024-06-17 02:12:49.408000: I runner.py:310] Step = 78300 ; steps/s = 1.65, tokens/s = 81291 (36381 source, 44910 target) ; Learning rate = 0.000316 ; Loss = 1.501879\n",
      "2024-06-17 02:13:49.491000: I runner.py:310] Step = 78400 ; steps/s = 1.66, tokens/s = 80283 (35907 source, 44376 target) ; Learning rate = 0.000316 ; Loss = 1.504250\n",
      "2024-06-17 02:14:49.985000: I runner.py:310] Step = 78500 ; steps/s = 1.65, tokens/s = 81303 (36381 source, 44922 target) ; Learning rate = 0.000315 ; Loss = 1.493680\n",
      "2024-06-17 02:15:50.488000: I runner.py:310] Step = 78600 ; steps/s = 1.65, tokens/s = 81316 (36407 source, 44909 target) ; Learning rate = 0.000315 ; Loss = 1.496974\n",
      "2024-06-17 02:16:51.010000: I runner.py:310] Step = 78700 ; steps/s = 1.65, tokens/s = 81292 (36384 source, 44908 target) ; Learning rate = 0.000315 ; Loss = 1.505858\n",
      "2024-06-17 02:17:51.064000: I runner.py:310] Step = 78800 ; steps/s = 1.67, tokens/s = 80385 (35970 source, 44415 target) ; Learning rate = 0.000315 ; Loss = 1.505110\n",
      "2024-06-17 02:18:51.503000: I runner.py:310] Step = 78900 ; steps/s = 1.65, tokens/s = 81409 (36426 source, 44983 target) ; Learning rate = 0.000315 ; Loss = 1.492519\n",
      "2024-06-17 02:19:51.967000: I runner.py:310] Step = 79000 ; steps/s = 1.65, tokens/s = 81347 (36416 source, 44931 target) ; Learning rate = 0.000314 ; Loss = 1.500354\n",
      "2024-06-17 02:20:52.499000: I runner.py:310] Step = 79100 ; steps/s = 1.65, tokens/s = 81256 (36368 source, 44888 target) ; Learning rate = 0.000314 ; Loss = 1.498759\n",
      "2024-06-17 02:21:52.529000: I runner.py:310] Step = 79200 ; steps/s = 1.67, tokens/s = 80394 (36004 source, 44390 target) ; Learning rate = 0.000314 ; Loss = 1.495086\n",
      "2024-06-17 02:22:53.006000: I runner.py:310] Step = 79300 ; steps/s = 1.65, tokens/s = 81325 (36346 source, 44979 target) ; Learning rate = 0.000314 ; Loss = 1.499260\n",
      "2024-06-17 02:23:53.486000: I runner.py:310] Step = 79400 ; steps/s = 1.65, tokens/s = 81350 (36425 source, 44925 target) ; Learning rate = 0.000314 ; Loss = 1.497126\n",
      "2024-06-17 02:24:53.986000: I runner.py:310] Step = 79500 ; steps/s = 1.65, tokens/s = 81308 (36403 source, 44905 target) ; Learning rate = 0.000313 ; Loss = 1.498792\n",
      "2024-06-17 02:25:54.003000: I runner.py:310] Step = 79600 ; steps/s = 1.67, tokens/s = 80397 (35963 source, 44434 target) ; Learning rate = 0.000313 ; Loss = 1.499437\n",
      "2024-06-17 02:26:54.512000: I runner.py:310] Step = 79700 ; steps/s = 1.65, tokens/s = 81342 (36417 source, 44925 target) ; Learning rate = 0.000313 ; Loss = 1.494954\n",
      "2024-06-17 02:27:55.049000: I runner.py:310] Step = 79800 ; steps/s = 1.65, tokens/s = 81224 (36337 source, 44887 target) ; Learning rate = 0.000313 ; Loss = 1.503022\n",
      "2024-06-17 02:28:55.399000: I runner.py:310] Step = 79900 ; steps/s = 1.66, tokens/s = 80836 (36200 source, 44636 target) ; Learning rate = 0.000313 ; Loss = 1.496904\n",
      "2024-06-17 02:29:55.573000: I runner.py:310] Step = 80000 ; steps/s = 1.66, tokens/s = 80894 (36213 source, 44681 target) ; Learning rate = 0.000312 ; Loss = 1.498122\n",
      "2024-06-17 02:29:57.851000: I training.py:176] Saved checkpoint KK-EN-Standard-Transformer/ckpt-80000\n",
      "2024-06-17 02:29:57.851000: I training.py:192] Running evaluation for step 80000\n",
      "2024-06-17 02:34:08.120000: I training.py:192] Evaluation result for step 80000: loss = 1.447394 ; perplexity = 4.252018\n",
      "2024-06-17 02:35:08.475000: I runner.py:310] Step = 80100 ; steps/s = 1.66, tokens/s = 81490 (36476 source, 45014 target) ; Learning rate = 0.000312 ; Loss = 1.494172\n",
      "2024-06-17 02:36:08.984000: I runner.py:310] Step = 80200 ; steps/s = 1.65, tokens/s = 81297 (36399 source, 44898 target) ; Learning rate = 0.000312 ; Loss = 1.497714\n",
      "2024-06-17 02:37:09.072000: I runner.py:310] Step = 80300 ; steps/s = 1.66, tokens/s = 80344 (35928 source, 44416 target) ; Learning rate = 0.000312 ; Loss = 1.503985\n",
      "2024-06-17 02:38:09.654000: I runner.py:310] Step = 80400 ; steps/s = 1.65, tokens/s = 81215 (36348 source, 44867 target) ; Learning rate = 0.000312 ; Loss = 1.495669\n",
      "2024-06-17 02:39:10.174000: I runner.py:310] Step = 80500 ; steps/s = 1.65, tokens/s = 81279 (36383 source, 44896 target) ; Learning rate = 0.000312 ; Loss = 1.495615\n",
      "2024-06-17 02:40:10.675000: I runner.py:310] Step = 80600 ; steps/s = 1.65, tokens/s = 81293 (36375 source, 44918 target) ; Learning rate = 0.000311 ; Loss = 1.497676\n",
      "2024-06-17 02:41:10.831000: I runner.py:310] Step = 80700 ; steps/s = 1.66, tokens/s = 80224 (35904 source, 44320 target) ; Learning rate = 0.000311 ; Loss = 1.500842\n",
      "2024-06-17 02:42:11.294000: I runner.py:310] Step = 80800 ; steps/s = 1.65, tokens/s = 81352 (36404 source, 44948 target) ; Learning rate = 0.000311 ; Loss = 1.488125\n",
      "2024-06-17 02:43:11.794000: I runner.py:310] Step = 80900 ; steps/s = 1.65, tokens/s = 81298 (36377 source, 44921 target) ; Learning rate = 0.000311 ; Loss = 1.498135\n",
      "2024-06-17 02:44:12.299000: I runner.py:310] Step = 81000 ; steps/s = 1.65, tokens/s = 81286 (36374 source, 44912 target) ; Learning rate = 0.000311 ; Loss = 1.493666\n",
      "2024-06-17 02:45:12.396000: I runner.py:310] Step = 81100 ; steps/s = 1.66, tokens/s = 80360 (35980 source, 44380 target) ; Learning rate = 0.000310 ; Loss = 1.507061\n",
      "2024-06-17 02:46:12.964000: I runner.py:310] Step = 81200 ; steps/s = 1.65, tokens/s = 81245 (36349 source, 44896 target) ; Learning rate = 0.000310 ; Loss = 1.491051\n",
      "2024-06-17 02:47:13.472000: I runner.py:310] Step = 81300 ; steps/s = 1.65, tokens/s = 81305 (36415 source, 44890 target) ; Learning rate = 0.000310 ; Loss = 1.493952\n",
      "2024-06-17 02:48:14.006000: I runner.py:310] Step = 81400 ; steps/s = 1.65, tokens/s = 81213 (36334 source, 44879 target) ; Learning rate = 0.000310 ; Loss = 1.492064\n",
      "2024-06-17 02:49:14.097000: I runner.py:310] Step = 81500 ; steps/s = 1.66, tokens/s = 80283 (35919 source, 44364 target) ; Learning rate = 0.000310 ; Loss = 1.497073\n",
      "2024-06-17 02:50:14.643000: I runner.py:310] Step = 81600 ; steps/s = 1.65, tokens/s = 81241 (36354 source, 44887 target) ; Learning rate = 0.000309 ; Loss = 1.493271\n",
      "2024-06-17 02:51:15.178000: I runner.py:310] Step = 81700 ; steps/s = 1.65, tokens/s = 81246 (36355 source, 44891 target) ; Learning rate = 0.000309 ; Loss = 1.497747\n",
      "2024-06-17 02:52:15.708000: I runner.py:310] Step = 81800 ; steps/s = 1.65, tokens/s = 81309 (36403 source, 44906 target) ; Learning rate = 0.000309 ; Loss = 1.494431\n",
      "2024-06-17 02:53:15.741000: I runner.py:310] Step = 81900 ; steps/s = 1.67, tokens/s = 80376 (35953 source, 44423 target) ; Learning rate = 0.000309 ; Loss = 1.492741\n",
      "2024-06-17 02:54:16.194000: I runner.py:310] Step = 82000 ; steps/s = 1.65, tokens/s = 81354 (36432 source, 44922 target) ; Learning rate = 0.000309 ; Loss = 1.501680\n",
      "2024-06-17 02:55:16.708000: I runner.py:310] Step = 82100 ; steps/s = 1.65, tokens/s = 81302 (36377 source, 44925 target) ; Learning rate = 0.000308 ; Loss = 1.496508\n",
      "2024-06-17 02:56:17.244000: I runner.py:310] Step = 82200 ; steps/s = 1.65, tokens/s = 81270 (36381 source, 44889 target) ; Learning rate = 0.000308 ; Loss = 1.495630\n",
      "2024-06-17 02:57:17.289000: I runner.py:310] Step = 82300 ; steps/s = 1.67, tokens/s = 80381 (35956 source, 44425 target) ; Learning rate = 0.000308 ; Loss = 1.500110\n",
      "2024-06-17 02:58:17.763000: I runner.py:310] Step = 82400 ; steps/s = 1.65, tokens/s = 81329 (36424 source, 44905 target) ; Learning rate = 0.000308 ; Loss = 1.496045\n",
      "2024-06-17 02:59:18.332000: I runner.py:310] Step = 82500 ; steps/s = 1.65, tokens/s = 81203 (36317 source, 44886 target) ; Learning rate = 0.000308 ; Loss = 1.499459\n",
      "2024-06-17 03:00:18.382000: I runner.py:310] Step = 82600 ; steps/s = 1.67, tokens/s = 80391 (35993 source, 44398 target) ; Learning rate = 0.000308 ; Loss = 1.492723\n",
      "2024-06-17 03:01:18.880000: I runner.py:310] Step = 82700 ; steps/s = 1.65, tokens/s = 81303 (36391 source, 44912 target) ; Learning rate = 0.000307 ; Loss = 1.497986\n",
      "2024-06-17 03:02:19.419000: I runner.py:310] Step = 82800 ; steps/s = 1.65, tokens/s = 81266 (36349 source, 44917 target) ; Learning rate = 0.000307 ; Loss = 1.487398\n",
      "2024-06-17 03:03:19.955000: I runner.py:310] Step = 82900 ; steps/s = 1.65, tokens/s = 81257 (36395 source, 44862 target) ; Learning rate = 0.000307 ; Loss = 1.494393\n",
      "2024-06-17 03:04:20.057000: I runner.py:310] Step = 83000 ; steps/s = 1.66, tokens/s = 80311 (35927 source, 44384 target) ; Learning rate = 0.000307 ; Loss = 1.491761\n",
      "2024-06-17 03:05:20.535000: I runner.py:310] Step = 83100 ; steps/s = 1.65, tokens/s = 81322 (36389 source, 44933 target) ; Learning rate = 0.000307 ; Loss = 1.490115\n",
      "2024-06-17 03:06:21.015000: I runner.py:310] Step = 83200 ; steps/s = 1.65, tokens/s = 81356 (36438 source, 44918 target) ; Learning rate = 0.000306 ; Loss = 1.499581\n",
      "2024-06-17 03:07:21.585000: I runner.py:310] Step = 83300 ; steps/s = 1.65, tokens/s = 81198 (36313 source, 44885 target) ; Learning rate = 0.000306 ; Loss = 1.492133\n",
      "2024-06-17 03:08:21.679000: I runner.py:310] Step = 83400 ; steps/s = 1.66, tokens/s = 80310 (35942 source, 44368 target) ; Learning rate = 0.000306 ; Loss = 1.490492\n",
      "2024-06-17 03:09:22.217000: I runner.py:310] Step = 83500 ; steps/s = 1.65, tokens/s = 81259 (36361 source, 44898 target) ; Learning rate = 0.000306 ; Loss = 1.486358\n",
      "2024-06-17 03:10:22.768000: I runner.py:310] Step = 83600 ; steps/s = 1.65, tokens/s = 81270 (36367 source, 44903 target) ; Learning rate = 0.000306 ; Loss = 1.498864\n",
      "2024-06-17 03:11:23.256000: I runner.py:310] Step = 83700 ; steps/s = 1.65, tokens/s = 81307 (36392 source, 44915 target) ; Learning rate = 0.000306 ; Loss = 1.503638\n",
      "2024-06-17 03:12:23.437000: I runner.py:310] Step = 83800 ; steps/s = 1.66, tokens/s = 80176 (35900 source, 44276 target) ; Learning rate = 0.000305 ; Loss = 1.494361\n",
      "2024-06-17 03:13:23.962000: I runner.py:310] Step = 83900 ; steps/s = 1.65, tokens/s = 81300 (36388 source, 44912 target) ; Learning rate = 0.000305 ; Loss = 1.490685\n",
      "2024-06-17 03:14:24.525000: I runner.py:310] Step = 84000 ; steps/s = 1.65, tokens/s = 81201 (36347 source, 44854 target) ; Learning rate = 0.000305 ; Loss = 1.499616\n",
      "2024-06-17 03:15:24.988000: I runner.py:310] Step = 84100 ; steps/s = 1.65, tokens/s = 81353 (36401 source, 44952 target) ; Learning rate = 0.000305 ; Loss = 1.497719\n",
      "2024-06-17 03:16:25.061000: I runner.py:310] Step = 84200 ; steps/s = 1.66, tokens/s = 80365 (35986 source, 44379 target) ; Learning rate = 0.000305 ; Loss = 1.500504\n",
      "2024-06-17 03:17:25.533000: I runner.py:310] Step = 84300 ; steps/s = 1.65, tokens/s = 81343 (36392 source, 44951 target) ; Learning rate = 0.000304 ; Loss = 1.497389\n",
      "2024-06-17 03:18:26.045000: I runner.py:310] Step = 84400 ; steps/s = 1.65, tokens/s = 81280 (36361 source, 44919 target) ; Learning rate = 0.000304 ; Loss = 1.489752\n",
      "2024-06-17 03:19:26.576000: I runner.py:310] Step = 84500 ; steps/s = 1.65, tokens/s = 81267 (36374 source, 44893 target) ; Learning rate = 0.000304 ; Loss = 1.493899\n",
      "2024-06-17 03:20:26.581000: I runner.py:310] Step = 84600 ; steps/s = 1.67, tokens/s = 80397 (35933 source, 44464 target) ; Learning rate = 0.000304 ; Loss = 1.490687\n",
      "2024-06-17 03:21:27.092000: I runner.py:310] Step = 84700 ; steps/s = 1.65, tokens/s = 81310 (36396 source, 44914 target) ; Learning rate = 0.000304 ; Loss = 1.496442\n",
      "2024-06-17 03:22:27.603000: I runner.py:310] Step = 84800 ; steps/s = 1.65, tokens/s = 81311 (36394 source, 44917 target) ; Learning rate = 0.000304 ; Loss = 1.493140\n",
      "2024-06-17 03:23:27.674000: I runner.py:310] Step = 84900 ; steps/s = 1.66, tokens/s = 80346 (36005 source, 44341 target) ; Learning rate = 0.000303 ; Loss = 1.493809\n",
      "2024-06-17 03:24:28.198000: I runner.py:310] Step = 85000 ; steps/s = 1.65, tokens/s = 81285 (36374 source, 44911 target) ; Learning rate = 0.000303 ; Loss = 1.490635\n",
      "2024-06-17 03:24:28.200000: I training.py:192] Running evaluation for step 85000\n",
      "2024-06-17 03:28:38.681000: I training.py:192] Evaluation result for step 85000: loss = 1.455794 ; perplexity = 4.287888\n",
      "2024-06-17 03:29:39.025000: I runner.py:310] Step = 85100 ; steps/s = 1.66, tokens/s = 81579 (36522 source, 45057 target) ; Learning rate = 0.000303 ; Loss = 1.493573\n",
      "2024-06-17 03:30:39.580000: I runner.py:310] Step = 85200 ; steps/s = 1.65, tokens/s = 81194 (36339 source, 44855 target) ; Learning rate = 0.000303 ; Loss = 1.492573\n",
      "2024-06-17 03:31:39.643000: I runner.py:310] Step = 85300 ; steps/s = 1.67, tokens/s = 80334 (35923 source, 44411 target) ; Learning rate = 0.000303 ; Loss = 1.490925\n",
      "2024-06-17 03:32:40.214000: I runner.py:310] Step = 85400 ; steps/s = 1.65, tokens/s = 81206 (36358 source, 44848 target) ; Learning rate = 0.000302 ; Loss = 1.493132\n",
      "2024-06-17 03:33:40.823000: I runner.py:310] Step = 85500 ; steps/s = 1.65, tokens/s = 81133 (36293 source, 44840 target) ; Learning rate = 0.000302 ; Loss = 1.493159\n",
      "2024-06-17 03:34:41.369000: I runner.py:310] Step = 85600 ; steps/s = 1.65, tokens/s = 81257 (36368 source, 44889 target) ; Learning rate = 0.000302 ; Loss = 1.496233\n",
      "2024-06-17 03:35:41.506000: I runner.py:310] Step = 85700 ; steps/s = 1.66, tokens/s = 80302 (35957 source, 44345 target) ; Learning rate = 0.000302 ; Loss = 1.500201\n",
      "2024-06-17 03:36:41.986000: I runner.py:310] Step = 85800 ; steps/s = 1.65, tokens/s = 81318 (36410 source, 44908 target) ; Learning rate = 0.000302 ; Loss = 1.487629\n",
      "2024-06-17 03:37:42.510000: I runner.py:310] Step = 85900 ; steps/s = 1.65, tokens/s = 81250 (36343 source, 44907 target) ; Learning rate = 0.000302 ; Loss = 1.493031\n",
      "2024-06-17 03:38:43.009000: I runner.py:310] Step = 86000 ; steps/s = 1.65, tokens/s = 81321 (36389 source, 44932 target) ; Learning rate = 0.000301 ; Loss = 1.490387\n",
      "2024-06-17 03:39:43.071000: I runner.py:310] Step = 86100 ; steps/s = 1.67, tokens/s = 80335 (35940 source, 44395 target) ; Learning rate = 0.000301 ; Loss = 1.495506\n",
      "2024-06-17 03:40:43.589000: I runner.py:310] Step = 86200 ; steps/s = 1.65, tokens/s = 81311 (36387 source, 44924 target) ; Learning rate = 0.000301 ; Loss = 1.490653\n",
      "2024-06-17 03:41:44.124000: I runner.py:310] Step = 86300 ; steps/s = 1.65, tokens/s = 81270 (36367 source, 44903 target) ; Learning rate = 0.000301 ; Loss = 1.492318\n",
      "2024-06-17 03:42:44.642000: I runner.py:310] Step = 86400 ; steps/s = 1.65, tokens/s = 81271 (36386 source, 44885 target) ; Learning rate = 0.000301 ; Loss = 1.501844\n",
      "2024-06-17 03:43:44.731000: I runner.py:310] Step = 86500 ; steps/s = 1.66, tokens/s = 80315 (35936 source, 44379 target) ; Learning rate = 0.000301 ; Loss = 1.486019\n",
      "2024-06-17 03:44:45.254000: I runner.py:310] Step = 86600 ; steps/s = 1.65, tokens/s = 81285 (36370 source, 44915 target) ; Learning rate = 0.000300 ; Loss = 1.494067\n",
      "2024-06-17 03:45:45.794000: I runner.py:310] Step = 86700 ; steps/s = 1.65, tokens/s = 81261 (36400 source, 44861 target) ; Learning rate = 0.000300 ; Loss = 1.494782\n",
      "2024-06-17 03:46:46.353000: I runner.py:310] Step = 86800 ; steps/s = 1.65, tokens/s = 81217 (36340 source, 44877 target) ; Learning rate = 0.000300 ; Loss = 1.497122\n",
      "2024-06-17 03:47:46.395000: I runner.py:310] Step = 86900 ; steps/s = 1.67, tokens/s = 80384 (35976 source, 44408 target) ; Learning rate = 0.000300 ; Loss = 1.501449\n",
      "2024-06-17 03:48:46.917000: I runner.py:310] Step = 87000 ; steps/s = 1.65, tokens/s = 81275 (36371 source, 44904 target) ; Learning rate = 0.000300 ; Loss = 1.494751\n",
      "2024-06-17 03:49:47.495000: I runner.py:310] Step = 87100 ; steps/s = 1.65, tokens/s = 81190 (36324 source, 44866 target) ; Learning rate = 0.000299 ; Loss = 1.497904\n",
      "2024-06-17 03:50:47.623000: I runner.py:310] Step = 87200 ; steps/s = 1.66, tokens/s = 80494 (36050 source, 44444 target) ; Learning rate = 0.000299 ; Loss = 1.494032\n",
      "2024-06-17 03:51:48.071000: I runner.py:310] Step = 87300 ; steps/s = 1.65, tokens/s = 81166 (36302 source, 44864 target) ; Learning rate = 0.000299 ; Loss = 1.493666\n",
      "2024-06-17 03:52:48.506000: I runner.py:310] Step = 87400 ; steps/s = 1.65, tokens/s = 81357 (36416 source, 44941 target) ; Learning rate = 0.000299 ; Loss = 1.492983\n",
      "2024-06-17 03:53:49.043000: I runner.py:310] Step = 87500 ; steps/s = 1.65, tokens/s = 81289 (36381 source, 44908 target) ; Learning rate = 0.000299 ; Loss = 1.492113\n",
      "2024-06-17 03:54:49.096000: I runner.py:310] Step = 87600 ; steps/s = 1.67, tokens/s = 80386 (35986 source, 44400 target) ; Learning rate = 0.000299 ; Loss = 1.497150\n",
      "2024-06-17 03:55:49.625000: I runner.py:310] Step = 87700 ; steps/s = 1.65, tokens/s = 81255 (36360 source, 44895 target) ; Learning rate = 0.000298 ; Loss = 1.494939\n",
      "2024-06-17 03:56:50.128000: I runner.py:310] Step = 87800 ; steps/s = 1.65, tokens/s = 81297 (36402 source, 44895 target) ; Learning rate = 0.000298 ; Loss = 1.491296\n",
      "2024-06-17 03:57:50.698000: I runner.py:310] Step = 87900 ; steps/s = 1.65, tokens/s = 81233 (36347 source, 44886 target) ; Learning rate = 0.000298 ; Loss = 1.490278\n",
      "2024-06-17 03:58:50.739000: I runner.py:310] Step = 88000 ; steps/s = 1.67, tokens/s = 80389 (35977 source, 44412 target) ; Learning rate = 0.000298 ; Loss = 1.492226\n",
      "2024-06-17 03:59:51.278000: I runner.py:310] Step = 88100 ; steps/s = 1.65, tokens/s = 81286 (36377 source, 44909 target) ; Learning rate = 0.000298 ; Loss = 1.490456\n",
      "2024-06-17 04:00:51.811000: I runner.py:310] Step = 88200 ; steps/s = 1.65, tokens/s = 81241 (36356 source, 44885 target) ; Learning rate = 0.000298 ; Loss = 1.497544\n",
      "2024-06-17 04:01:52.321000: I runner.py:310] Step = 88300 ; steps/s = 1.65, tokens/s = 81262 (36364 source, 44898 target) ; Learning rate = 0.000297 ; Loss = 1.500569\n",
      "2024-06-17 04:02:52.396000: I runner.py:310] Step = 88400 ; steps/s = 1.66, tokens/s = 80363 (35960 source, 44403 target) ; Learning rate = 0.000297 ; Loss = 1.488700\n",
      "2024-06-17 04:03:52.861000: I runner.py:310] Step = 88500 ; steps/s = 1.65, tokens/s = 81362 (36390 source, 44972 target) ; Learning rate = 0.000297 ; Loss = 1.493341\n",
      "2024-06-17 04:04:53.445000: I runner.py:310] Step = 88600 ; steps/s = 1.65, tokens/s = 81169 (36343 source, 44826 target) ; Learning rate = 0.000297 ; Loss = 1.492696\n",
      "2024-06-17 04:05:53.934000: I runner.py:310] Step = 88700 ; steps/s = 1.65, tokens/s = 81360 (36424 source, 44936 target) ; Learning rate = 0.000297 ; Loss = 1.492674\n",
      "2024-06-17 04:06:54.001000: I runner.py:310] Step = 88800 ; steps/s = 1.66, tokens/s = 80353 (35980 source, 44373 target) ; Learning rate = 0.000297 ; Loss = 1.492866\n",
      "2024-06-17 04:07:54.448000: I runner.py:310] Step = 88900 ; steps/s = 1.65, tokens/s = 81372 (36421 source, 44951 target) ; Learning rate = 0.000296 ; Loss = 1.487278\n",
      "2024-06-17 04:08:54.980000: I runner.py:310] Step = 89000 ; steps/s = 1.65, tokens/s = 81260 (36380 source, 44880 target) ; Learning rate = 0.000296 ; Loss = 1.490914\n",
      "2024-06-17 04:09:55.521000: I runner.py:310] Step = 89100 ; steps/s = 1.65, tokens/s = 81239 (36319 source, 44920 target) ; Learning rate = 0.000296 ; Loss = 1.491918\n",
      "2024-06-17 04:10:55.650000: I runner.py:310] Step = 89200 ; steps/s = 1.66, tokens/s = 80282 (35954 source, 44328 target) ; Learning rate = 0.000296 ; Loss = 1.498432\n",
      "2024-06-17 04:11:56.145000: I runner.py:310] Step = 89300 ; steps/s = 1.65, tokens/s = 81312 (36393 source, 44919 target) ; Learning rate = 0.000296 ; Loss = 1.483789\n",
      "2024-06-17 04:12:56.674000: I runner.py:310] Step = 89400 ; steps/s = 1.65, tokens/s = 81234 (36327 source, 44907 target) ; Learning rate = 0.000296 ; Loss = 1.483563\n",
      "2024-06-17 04:13:57.102000: I runner.py:310] Step = 89500 ; steps/s = 1.66, tokens/s = 81419 (36452 source, 44967 target) ; Learning rate = 0.000295 ; Loss = 1.493789\n",
      "2024-06-17 04:14:57.094000: I runner.py:310] Step = 89600 ; steps/s = 1.67, tokens/s = 80457 (36003 source, 44454 target) ; Learning rate = 0.000295 ; Loss = 1.491486\n",
      "2024-06-17 04:15:57.594000: I runner.py:310] Step = 89700 ; steps/s = 1.65, tokens/s = 81312 (36388 source, 44924 target) ; Learning rate = 0.000295 ; Loss = 1.490496\n",
      "2024-06-17 04:16:58.078000: I runner.py:310] Step = 89800 ; steps/s = 1.65, tokens/s = 81308 (36381 source, 44927 target) ; Learning rate = 0.000295 ; Loss = 1.494855\n",
      "2024-06-17 04:17:58.109000: I runner.py:310] Step = 89900 ; steps/s = 1.67, tokens/s = 80401 (35994 source, 44407 target) ; Learning rate = 0.000295 ; Loss = 1.491637\n",
      "2024-06-17 04:18:58.556000: I runner.py:310] Step = 90000 ; steps/s = 1.65, tokens/s = 81385 (36428 source, 44957 target) ; Learning rate = 0.000295 ; Loss = 1.487651\n",
      "2024-06-17 04:19:01.321000: I training.py:176] Saved checkpoint KK-EN-Standard-Transformer/ckpt-90000\n",
      "2024-06-17 04:19:01.321000: I training.py:192] Running evaluation for step 90000\n",
      "2024-06-17 04:23:06.274000: I training.py:192] Evaluation result for step 90000: loss = 1.459749 ; perplexity = 4.304877\n",
      "2024-06-17 04:24:06.637000: I runner.py:310] Step = 90100 ; steps/s = 1.66, tokens/s = 81510 (36476 source, 45034 target) ; Learning rate = 0.000294 ; Loss = 1.489415\n",
      "2024-06-17 04:25:07.163000: I runner.py:310] Step = 90200 ; steps/s = 1.65, tokens/s = 81261 (36370 source, 44891 target) ; Learning rate = 0.000294 ; Loss = 1.496840\n",
      "2024-06-17 04:26:07.239000: I runner.py:310] Step = 90300 ; steps/s = 1.66, tokens/s = 80348 (35951 source, 44397 target) ; Learning rate = 0.000294 ; Loss = 1.486958\n",
      "2024-06-17 04:27:07.770000: I runner.py:310] Step = 90400 ; steps/s = 1.65, tokens/s = 81231 (36345 source, 44886 target) ; Learning rate = 0.000294 ; Loss = 1.490657\n",
      "2024-06-17 04:28:08.266000: I runner.py:310] Step = 90500 ; steps/s = 1.65, tokens/s = 81316 (36389 source, 44927 target) ; Learning rate = 0.000294 ; Loss = 1.488358\n",
      "2024-06-17 04:29:08.822000: I runner.py:310] Step = 90600 ; steps/s = 1.65, tokens/s = 81261 (36387 source, 44874 target) ; Learning rate = 0.000294 ; Loss = 1.489261\n",
      "2024-06-17 04:30:08.948000: I runner.py:310] Step = 90700 ; steps/s = 1.66, tokens/s = 80279 (35919 source, 44360 target) ; Learning rate = 0.000293 ; Loss = 1.493868\n",
      "2024-06-17 04:31:09.468000: I runner.py:310] Step = 90800 ; steps/s = 1.65, tokens/s = 81279 (36407 source, 44872 target) ; Learning rate = 0.000293 ; Loss = 1.487465\n",
      "2024-06-17 04:32:09.975000: I runner.py:310] Step = 90900 ; steps/s = 1.65, tokens/s = 81297 (36387 source, 44910 target) ; Learning rate = 0.000293 ; Loss = 1.489975\n",
      "2024-06-17 04:33:10.479000: I runner.py:310] Step = 91000 ; steps/s = 1.65, tokens/s = 81306 (36377 source, 44929 target) ; Learning rate = 0.000293 ; Loss = 1.490408\n",
      "2024-06-17 04:34:10.539000: I runner.py:310] Step = 91100 ; steps/s = 1.67, tokens/s = 80348 (35945 source, 44403 target) ; Learning rate = 0.000293 ; Loss = 1.494451\n",
      "2024-06-17 04:35:11.034000: I runner.py:310] Step = 91200 ; steps/s = 1.65, tokens/s = 81321 (36404 source, 44917 target) ; Learning rate = 0.000293 ; Loss = 1.488425\n",
      "2024-06-17 04:36:11.582000: I runner.py:310] Step = 91300 ; steps/s = 1.65, tokens/s = 81207 (36338 source, 44869 target) ; Learning rate = 0.000293 ; Loss = 1.495871\n",
      "2024-06-17 04:37:12.103000: I runner.py:310] Step = 91400 ; steps/s = 1.65, tokens/s = 81315 (36391 source, 44924 target) ; Learning rate = 0.000292 ; Loss = 1.487848\n",
      "2024-06-17 04:38:12.136000: I runner.py:310] Step = 91500 ; steps/s = 1.67, tokens/s = 80386 (35962 source, 44424 target) ; Learning rate = 0.000292 ; Loss = 1.483319\n",
      "2024-06-17 04:39:12.652000: I runner.py:310] Step = 91600 ; steps/s = 1.65, tokens/s = 81288 (36386 source, 44902 target) ; Learning rate = 0.000292 ; Loss = 1.488563\n",
      "2024-06-17 04:40:13.172000: I runner.py:310] Step = 91700 ; steps/s = 1.65, tokens/s = 81246 (36359 source, 44887 target) ; Learning rate = 0.000292 ; Loss = 1.499849\n",
      "2024-06-17 04:41:13.677000: I runner.py:310] Step = 91800 ; steps/s = 1.65, tokens/s = 81342 (36412 source, 44930 target) ; Learning rate = 0.000292 ; Loss = 1.494676\n",
      "2024-06-17 04:42:13.756000: I runner.py:310] Step = 91900 ; steps/s = 1.66, tokens/s = 80299 (35926 source, 44373 target) ; Learning rate = 0.000292 ; Loss = 1.492412\n",
      "2024-06-17 04:43:14.280000: I runner.py:310] Step = 92000 ; steps/s = 1.65, tokens/s = 81266 (36363 source, 44903 target) ; Learning rate = 0.000291 ; Loss = 1.483446\n",
      "2024-06-17 04:44:14.864000: I runner.py:310] Step = 92100 ; steps/s = 1.65, tokens/s = 81231 (36375 source, 44856 target) ; Learning rate = 0.000291 ; Loss = 1.493835\n",
      "2024-06-17 04:45:14.948000: I runner.py:310] Step = 92200 ; steps/s = 1.66, tokens/s = 80340 (35958 source, 44382 target) ; Learning rate = 0.000291 ; Loss = 1.490927\n",
      "2024-06-17 04:46:15.497000: I runner.py:310] Step = 92300 ; steps/s = 1.65, tokens/s = 81236 (36332 source, 44904 target) ; Learning rate = 0.000291 ; Loss = 1.495041\n",
      "2024-06-17 04:47:15.991000: I runner.py:310] Step = 92400 ; steps/s = 1.65, tokens/s = 81333 (36420 source, 44913 target) ; Learning rate = 0.000291 ; Loss = 1.486353\n",
      "2024-06-17 04:48:16.486000: I runner.py:310] Step = 92500 ; steps/s = 1.65, tokens/s = 81297 (36354 source, 44943 target) ; Learning rate = 0.000291 ; Loss = 1.487238\n",
      "2024-06-17 04:49:16.567000: I runner.py:310] Step = 92600 ; steps/s = 1.66, tokens/s = 80316 (35972 source, 44344 target) ; Learning rate = 0.000290 ; Loss = 1.484938\n",
      "2024-06-17 04:50:17.056000: I runner.py:310] Step = 92700 ; steps/s = 1.65, tokens/s = 81292 (36373 source, 44919 target) ; Learning rate = 0.000290 ; Loss = 1.489686\n",
      "2024-06-17 04:51:17.556000: I runner.py:310] Step = 92800 ; steps/s = 1.65, tokens/s = 81336 (36412 source, 44924 target) ; Learning rate = 0.000290 ; Loss = 1.494558\n",
      "2024-06-17 04:52:18.086000: I runner.py:310] Step = 92900 ; steps/s = 1.65, tokens/s = 81288 (36383 source, 44905 target) ; Learning rate = 0.000290 ; Loss = 1.488590\n",
      "2024-06-17 04:53:18.139000: I runner.py:310] Step = 93000 ; steps/s = 1.67, tokens/s = 80364 (35941 source, 44423 target) ; Learning rate = 0.000290 ; Loss = 1.482786\n",
      "2024-06-17 04:54:18.642000: I runner.py:310] Step = 93100 ; steps/s = 1.65, tokens/s = 81261 (36365 source, 44896 target) ; Learning rate = 0.000290 ; Loss = 1.496290\n",
      "2024-06-17 04:55:19.134000: I runner.py:310] Step = 93200 ; steps/s = 1.65, tokens/s = 81325 (36402 source, 44923 target) ; Learning rate = 0.000290 ; Loss = 1.489042\n",
      "2024-06-17 04:56:19.642000: I runner.py:310] Step = 93300 ; steps/s = 1.65, tokens/s = 81317 (36418 source, 44899 target) ; Learning rate = 0.000289 ; Loss = 1.492783\n",
      "2024-06-17 04:57:19.725000: I runner.py:310] Step = 93400 ; steps/s = 1.66, tokens/s = 80345 (35958 source, 44387 target) ; Learning rate = 0.000289 ; Loss = 1.485240\n",
      "2024-06-17 04:58:20.218000: I runner.py:310] Step = 93500 ; steps/s = 1.65, tokens/s = 81331 (36377 source, 44954 target) ; Learning rate = 0.000289 ; Loss = 1.491846\n",
      "2024-06-17 04:59:20.706000: I runner.py:310] Step = 93600 ; steps/s = 1.65, tokens/s = 81283 (36372 source, 44911 target) ; Learning rate = 0.000289 ; Loss = 1.493669\n",
      "2024-06-17 05:00:21.192000: I runner.py:310] Step = 93700 ; steps/s = 1.65, tokens/s = 81338 (36423 source, 44915 target) ; Learning rate = 0.000289 ; Loss = 1.492193\n",
      "2024-06-17 05:01:21.214000: I runner.py:310] Step = 93800 ; steps/s = 1.67, tokens/s = 80414 (35990 source, 44424 target) ; Learning rate = 0.000289 ; Loss = 1.494170\n",
      "2024-06-17 05:02:21.714000: I runner.py:310] Step = 93900 ; steps/s = 1.65, tokens/s = 81326 (36414 source, 44912 target) ; Learning rate = 0.000288 ; Loss = 1.484188\n",
      "2024-06-17 05:03:22.298000: I runner.py:310] Step = 94000 ; steps/s = 1.65, tokens/s = 81202 (36318 source, 44884 target) ; Learning rate = 0.000288 ; Loss = 1.488550\n",
      "2024-06-17 05:04:22.860000: I runner.py:310] Step = 94100 ; steps/s = 1.65, tokens/s = 81202 (36354 source, 44848 target) ; Learning rate = 0.000288 ; Loss = 1.498293\n",
      "2024-06-17 05:05:22.905000: I runner.py:310] Step = 94200 ; steps/s = 1.67, tokens/s = 80369 (35939 source, 44430 target) ; Learning rate = 0.000288 ; Loss = 1.487426\n",
      "2024-06-17 05:06:23.408000: I runner.py:310] Step = 94300 ; steps/s = 1.65, tokens/s = 81294 (36379 source, 44915 target) ; Learning rate = 0.000288 ; Loss = 1.488434\n",
      "2024-06-17 05:07:23.916000: I runner.py:310] Step = 94400 ; steps/s = 1.65, tokens/s = 81299 (36389 source, 44910 target) ; Learning rate = 0.000288 ; Loss = 1.493057\n",
      "2024-06-17 05:08:24.018000: I runner.py:310] Step = 94500 ; steps/s = 1.66, tokens/s = 80316 (35973 source, 44343 target) ; Learning rate = 0.000288 ; Loss = 1.485044\n",
      "2024-06-17 05:09:24.501000: I runner.py:310] Step = 94600 ; steps/s = 1.65, tokens/s = 81338 (36383 source, 44955 target) ; Learning rate = 0.000287 ; Loss = 1.488986\n",
      "2024-06-17 05:10:24.940000: I runner.py:310] Step = 94700 ; steps/s = 1.65, tokens/s = 81417 (36464 source, 44953 target) ; Learning rate = 0.000287 ; Loss = 1.484561\n",
      "2024-06-17 05:11:25.495000: I runner.py:310] Step = 94800 ; steps/s = 1.65, tokens/s = 81224 (36335 source, 44889 target) ; Learning rate = 0.000287 ; Loss = 1.488811\n",
      "2024-06-17 05:12:25.552000: I runner.py:310] Step = 94900 ; steps/s = 1.67, tokens/s = 80335 (35948 source, 44387 target) ; Learning rate = 0.000287 ; Loss = 1.488271\n",
      "2024-06-17 05:13:26.110000: I runner.py:310] Step = 95000 ; steps/s = 1.65, tokens/s = 81241 (36360 source, 44881 target) ; Learning rate = 0.000287 ; Loss = 1.483761\n",
      "2024-06-17 05:13:26.111000: I training.py:192] Running evaluation for step 95000\n",
      "2024-06-17 05:17:32.715000: I training.py:192] Evaluation result for step 95000: loss = 1.463113 ; perplexity = 4.319384\n",
      "2024-06-17 05:18:33.108000: I runner.py:310] Step = 95100 ; steps/s = 1.66, tokens/s = 81476 (36474 source, 45002 target) ; Learning rate = 0.000287 ; Loss = 1.493387\n",
      "2024-06-17 05:19:33.641000: I runner.py:310] Step = 95200 ; steps/s = 1.65, tokens/s = 81246 (36341 source, 44905 target) ; Learning rate = 0.000286 ; Loss = 1.486777\n",
      "2024-06-17 05:20:33.680000: I runner.py:310] Step = 95300 ; steps/s = 1.67, tokens/s = 80386 (35987 source, 44399 target) ; Learning rate = 0.000286 ; Loss = 1.484125\n",
      "2024-06-17 05:21:34.200000: I runner.py:310] Step = 95400 ; steps/s = 1.65, tokens/s = 81263 (36349 source, 44914 target) ; Learning rate = 0.000286 ; Loss = 1.487383\n",
      "2024-06-17 05:22:34.781000: I runner.py:310] Step = 95500 ; steps/s = 1.65, tokens/s = 81190 (36355 source, 44835 target) ; Learning rate = 0.000286 ; Loss = 1.485202\n",
      "2024-06-17 05:23:35.338000: I runner.py:310] Step = 95600 ; steps/s = 1.65, tokens/s = 81251 (36370 source, 44881 target) ; Learning rate = 0.000286 ; Loss = 1.488034\n",
      "2024-06-17 05:24:35.473000: I runner.py:310] Step = 95700 ; steps/s = 1.66, tokens/s = 80279 (35904 source, 44375 target) ; Learning rate = 0.000286 ; Loss = 1.495177\n",
      "2024-06-17 05:25:36.029000: I runner.py:310] Step = 95800 ; steps/s = 1.65, tokens/s = 81233 (36349 source, 44884 target) ; Learning rate = 0.000286 ; Loss = 1.481204\n",
      "2024-06-17 05:26:36.469000: I runner.py:310] Step = 95900 ; steps/s = 1.65, tokens/s = 81386 (36449 source, 44937 target) ; Learning rate = 0.000285 ; Loss = 1.478386\n",
      "2024-06-17 05:27:37.010000: I runner.py:310] Step = 96000 ; steps/s = 1.65, tokens/s = 81278 (36391 source, 44887 target) ; Learning rate = 0.000285 ; Loss = 1.487384\n",
      "2024-06-17 05:28:37.055000: I runner.py:310] Step = 96100 ; steps/s = 1.67, tokens/s = 80366 (35951 source, 44415 target) ; Learning rate = 0.000285 ; Loss = 1.492680\n",
      "2024-06-17 05:29:37.535000: I runner.py:310] Step = 96200 ; steps/s = 1.65, tokens/s = 81363 (36422 source, 44941 target) ; Learning rate = 0.000285 ; Loss = 1.481929\n",
      "2024-06-17 05:30:38.046000: I runner.py:310] Step = 96300 ; steps/s = 1.65, tokens/s = 81283 (36346 source, 44937 target) ; Learning rate = 0.000285 ; Loss = 1.484748\n",
      "2024-06-17 05:31:38.584000: I runner.py:310] Step = 96400 ; steps/s = 1.65, tokens/s = 81226 (36374 source, 44852 target) ; Learning rate = 0.000285 ; Loss = 1.482879\n",
      "2024-06-17 05:32:38.624000: I runner.py:310] Step = 96500 ; steps/s = 1.67, tokens/s = 80369 (35963 source, 44406 target) ; Learning rate = 0.000285 ; Loss = 1.475801\n",
      "2024-06-17 05:33:39.186000: I runner.py:310] Step = 96600 ; steps/s = 1.65, tokens/s = 81211 (36327 source, 44884 target) ; Learning rate = 0.000284 ; Loss = 1.484886\n",
      "2024-06-17 05:34:39.685000: I runner.py:310] Step = 96700 ; steps/s = 1.65, tokens/s = 81339 (36428 source, 44911 target) ; Learning rate = 0.000284 ; Loss = 1.488736\n",
      "2024-06-17 05:35:40.231000: I runner.py:310] Step = 96800 ; steps/s = 1.65, tokens/s = 81246 (36366 source, 44880 target) ; Learning rate = 0.000284 ; Loss = 1.494595\n",
      "2024-06-17 05:36:40.324000: I runner.py:310] Step = 96900 ; steps/s = 1.66, tokens/s = 80322 (35940 source, 44382 target) ; Learning rate = 0.000284 ; Loss = 1.487877\n",
      "2024-06-17 05:37:40.849000: I runner.py:310] Step = 97000 ; steps/s = 1.65, tokens/s = 81255 (36367 source, 44888 target) ; Learning rate = 0.000284 ; Loss = 1.488864\n",
      "2024-06-17 05:38:41.413000: I runner.py:310] Step = 97100 ; steps/s = 1.65, tokens/s = 81220 (36344 source, 44876 target) ; Learning rate = 0.000284 ; Loss = 1.489430\n",
      "2024-06-17 05:39:41.515000: I runner.py:310] Step = 97200 ; steps/s = 1.66, tokens/s = 80307 (35945 source, 44362 target) ; Learning rate = 0.000284 ; Loss = 1.486335\n",
      "2024-06-17 05:40:42.059000: I runner.py:310] Step = 97300 ; steps/s = 1.65, tokens/s = 81255 (36388 source, 44867 target) ; Learning rate = 0.000283 ; Loss = 1.482896\n",
      "2024-06-17 05:41:42.588000: I runner.py:310] Step = 97400 ; steps/s = 1.65, tokens/s = 81261 (36369 source, 44892 target) ; Learning rate = 0.000283 ; Loss = 1.492997\n",
      "2024-06-17 05:42:43.104000: I runner.py:310] Step = 97500 ; steps/s = 1.65, tokens/s = 81292 (36355 source, 44937 target) ; Learning rate = 0.000283 ; Loss = 1.486198\n",
      "2024-06-17 05:43:43.211000: I runner.py:310] Step = 97600 ; steps/s = 1.66, tokens/s = 80302 (35950 source, 44352 target) ; Learning rate = 0.000283 ; Loss = 1.490715\n",
      "2024-06-17 05:44:43.742000: I runner.py:310] Step = 97700 ; steps/s = 1.65, tokens/s = 81259 (36378 source, 44881 target) ; Learning rate = 0.000283 ; Loss = 1.487910\n",
      "2024-06-17 05:45:44.213000: I runner.py:310] Step = 97800 ; steps/s = 1.65, tokens/s = 81342 (36393 source, 44949 target) ; Learning rate = 0.000283 ; Loss = 1.486539\n",
      "2024-06-17 05:46:44.690000: I runner.py:310] Step = 97900 ; steps/s = 1.65, tokens/s = 81362 (36430 source, 44932 target) ; Learning rate = 0.000282 ; Loss = 1.485027\n",
      "2024-06-17 05:47:44.777000: I runner.py:310] Step = 98000 ; steps/s = 1.66, tokens/s = 80319 (35918 source, 44401 target) ; Learning rate = 0.000282 ; Loss = 1.486572\n",
      "2024-06-17 05:48:45.269000: I runner.py:310] Step = 98100 ; steps/s = 1.65, tokens/s = 81289 (36408 source, 44881 target) ; Learning rate = 0.000282 ; Loss = 1.480416\n",
      "2024-06-17 05:49:45.809000: I runner.py:310] Step = 98200 ; steps/s = 1.65, tokens/s = 81255 (36341 source, 44914 target) ; Learning rate = 0.000282 ; Loss = 1.488417\n",
      "2024-06-17 05:50:46.328000: I runner.py:310] Step = 98300 ; steps/s = 1.65, tokens/s = 81288 (36380 source, 44908 target) ; Learning rate = 0.000282 ; Loss = 1.488162\n",
      "2024-06-17 05:51:46.324000: I runner.py:310] Step = 98400 ; steps/s = 1.67, tokens/s = 80436 (36004 source, 44432 target) ; Learning rate = 0.000282 ; Loss = 1.485660\n",
      "2024-06-17 05:52:46.892000: I runner.py:310] Step = 98500 ; steps/s = 1.65, tokens/s = 81241 (36349 source, 44892 target) ; Learning rate = 0.000282 ; Loss = 1.486632\n",
      "2024-06-17 05:53:47.477000: I runner.py:310] Step = 98600 ; steps/s = 1.65, tokens/s = 81192 (36357 source, 44835 target) ; Learning rate = 0.000281 ; Loss = 1.487334\n",
      "2024-06-17 05:54:47.919000: I runner.py:310] Step = 98700 ; steps/s = 1.65, tokens/s = 81377 (36425 source, 44952 target) ; Learning rate = 0.000281 ; Loss = 1.493538\n",
      "2024-06-17 05:55:48.037000: I runner.py:310] Step = 98800 ; steps/s = 1.66, tokens/s = 80298 (35924 source, 44374 target) ; Learning rate = 0.000281 ; Loss = 1.481702\n",
      "2024-06-17 05:56:48.548000: I runner.py:310] Step = 98900 ; steps/s = 1.65, tokens/s = 81308 (36395 source, 44913 target) ; Learning rate = 0.000281 ; Loss = 1.481724\n",
      "2024-06-17 05:57:49.035000: I runner.py:310] Step = 99000 ; steps/s = 1.65, tokens/s = 81325 (36375 source, 44950 target) ; Learning rate = 0.000281 ; Loss = 1.490036\n",
      "2024-06-17 05:58:49.537000: I runner.py:310] Step = 99100 ; steps/s = 1.65, tokens/s = 81286 (36394 source, 44892 target) ; Learning rate = 0.000281 ; Loss = 1.492293\n",
      "2024-06-17 05:59:49.591000: I runner.py:310] Step = 99200 ; steps/s = 1.67, tokens/s = 80358 (35941 source, 44417 target) ; Learning rate = 0.000281 ; Loss = 1.478521\n",
      "2024-06-17 06:00:50.110000: I runner.py:310] Step = 99300 ; steps/s = 1.65, tokens/s = 81277 (36392 source, 44885 target) ; Learning rate = 0.000280 ; Loss = 1.491412\n",
      "2024-06-17 06:01:50.608000: I runner.py:310] Step = 99400 ; steps/s = 1.65, tokens/s = 81296 (36394 source, 44902 target) ; Learning rate = 0.000280 ; Loss = 1.489463\n",
      "2024-06-17 06:02:50.663000: I runner.py:310] Step = 99500 ; steps/s = 1.67, tokens/s = 80379 (35959 source, 44420 target) ; Learning rate = 0.000280 ; Loss = 1.489536\n",
      "2024-06-17 06:03:51.177000: I runner.py:310] Step = 99600 ; steps/s = 1.65, tokens/s = 81285 (36359 source, 44926 target) ; Learning rate = 0.000280 ; Loss = 1.483617\n",
      "2024-06-17 06:04:51.744000: I runner.py:310] Step = 99700 ; steps/s = 1.65, tokens/s = 81241 (36372 source, 44869 target) ; Learning rate = 0.000280 ; Loss = 1.484246\n",
      "2024-06-17 06:05:52.226000: I runner.py:310] Step = 99800 ; steps/s = 1.65, tokens/s = 81301 (36395 source, 44906 target) ; Learning rate = 0.000280 ; Loss = 1.491893\n",
      "2024-06-17 06:06:52.307000: I runner.py:310] Step = 99900 ; steps/s = 1.66, tokens/s = 80356 (35965 source, 44391 target) ; Learning rate = 0.000280 ; Loss = 1.484456\n",
      "2024-06-17 06:07:52.825000: I runner.py:310] Step = 100000 ; steps/s = 1.65, tokens/s = 81272 (36354 source, 44918 target) ; Learning rate = 0.000280 ; Loss = 1.483427\n",
      "2024-06-17 06:07:55.167000: I training.py:176] Saved checkpoint KK-EN-Standard-Transformer/ckpt-100000\n",
      "2024-06-17 06:07:55.167000: I training.py:192] Running evaluation for step 100000\n",
      "2024-06-17 06:12:04.707000: I training.py:192] Evaluation result for step 100000: loss = 1.467076 ; perplexity = 4.336538\n",
      "2024-06-17 06:13:05.077000: I runner.py:310] Step = 100100 ; steps/s = 1.66, tokens/s = 81512 (36491 source, 45021 target) ; Learning rate = 0.000279 ; Loss = 1.481631\n",
      "2024-06-17 06:14:05.580000: I runner.py:310] Step = 100200 ; steps/s = 1.65, tokens/s = 81293 (36387 source, 44906 target) ; Learning rate = 0.000279 ; Loss = 1.491746\n",
      "2024-06-17 06:15:05.688000: I runner.py:310] Step = 100300 ; steps/s = 1.66, tokens/s = 80295 (35906 source, 44389 target) ; Learning rate = 0.000279 ; Loss = 1.491707\n",
      "2024-06-17 06:16:06.240000: I runner.py:310] Step = 100400 ; steps/s = 1.65, tokens/s = 81278 (36437 source, 44841 target) ; Learning rate = 0.000279 ; Loss = 1.491733\n",
      "2024-06-17 06:17:06.822000: I runner.py:310] Step = 100500 ; steps/s = 1.65, tokens/s = 81201 (36332 source, 44869 target) ; Learning rate = 0.000279 ; Loss = 1.480380\n",
      "2024-06-17 06:18:07.296000: I runner.py:310] Step = 100600 ; steps/s = 1.65, tokens/s = 81293 (36368 source, 44925 target) ; Learning rate = 0.000279 ; Loss = 1.484388\n",
      "2024-06-17 06:19:07.375000: I runner.py:310] Step = 100700 ; steps/s = 1.66, tokens/s = 80333 (35960 source, 44373 target) ; Learning rate = 0.000279 ; Loss = 1.493645\n",
      "2024-06-17 06:20:07.828000: I runner.py:310] Step = 100800 ; steps/s = 1.65, tokens/s = 81382 (36417 source, 44965 target) ; Learning rate = 0.000278 ; Loss = 1.481316\n",
      "2024-06-17 06:21:08.348000: I runner.py:310] Step = 100900 ; steps/s = 1.65, tokens/s = 81309 (36395 source, 44914 target) ; Learning rate = 0.000278 ; Loss = 1.485829\n",
      "2024-06-17 06:22:08.902000: I runner.py:310] Step = 101000 ; steps/s = 1.65, tokens/s = 81192 (36329 source, 44863 target) ; Learning rate = 0.000278 ; Loss = 1.482985\n",
      "2024-06-17 06:23:08.954000: I runner.py:310] Step = 101100 ; steps/s = 1.67, tokens/s = 80360 (35947 source, 44413 target) ; Learning rate = 0.000278 ; Loss = 1.482394\n",
      "2024-06-17 06:24:09.500000: I runner.py:310] Step = 101200 ; steps/s = 1.65, tokens/s = 81222 (36339 source, 44883 target) ; Learning rate = 0.000278 ; Loss = 1.487389\n",
      "2024-06-17 06:25:10.000000: I runner.py:310] Step = 101300 ; steps/s = 1.65, tokens/s = 81330 (36411 source, 44919 target) ; Learning rate = 0.000278 ; Loss = 1.482181\n",
      "2024-06-17 06:26:10.464000: I runner.py:310] Step = 101400 ; steps/s = 1.65, tokens/s = 81353 (36421 source, 44932 target) ; Learning rate = 0.000278 ; Loss = 1.487296\n",
      "2024-06-17 06:27:10.462000: I runner.py:310] Step = 101500 ; steps/s = 1.67, tokens/s = 80493 (36043 source, 44450 target) ; Learning rate = 0.000277 ; Loss = 1.478907\n",
      "2024-06-17 06:28:10.974000: I runner.py:310] Step = 101600 ; steps/s = 1.65, tokens/s = 81261 (36371 source, 44890 target) ; Learning rate = 0.000277 ; Loss = 1.491255\n",
      "2024-06-17 06:29:11.537000: I runner.py:310] Step = 101700 ; steps/s = 1.65, tokens/s = 81220 (36331 source, 44889 target) ; Learning rate = 0.000277 ; Loss = 1.491462\n",
      "2024-06-17 06:30:11.641000: I runner.py:310] Step = 101800 ; steps/s = 1.66, tokens/s = 80292 (35933 source, 44359 target) ; Learning rate = 0.000277 ; Loss = 1.482801\n",
      "2024-06-17 06:31:12.144000: I runner.py:310] Step = 101900 ; steps/s = 1.65, tokens/s = 81341 (36413 source, 44928 target) ; Learning rate = 0.000277 ; Loss = 1.481457\n",
      "2024-06-17 06:32:12.621000: I runner.py:310] Step = 102000 ; steps/s = 1.65, tokens/s = 81324 (36388 source, 44936 target) ; Learning rate = 0.000277 ; Loss = 1.484705\n",
      "2024-06-17 06:33:13.150000: I runner.py:310] Step = 102100 ; steps/s = 1.65, tokens/s = 81265 (36359 source, 44906 target) ; Learning rate = 0.000277 ; Loss = 1.488255\n",
      "2024-06-17 06:34:13.297000: I runner.py:310] Step = 102200 ; steps/s = 1.66, tokens/s = 80227 (35900 source, 44327 target) ; Learning rate = 0.000276 ; Loss = 1.487443\n",
      "2024-06-17 06:35:13.805000: I runner.py:310] Step = 102300 ; steps/s = 1.65, tokens/s = 81277 (36355 source, 44922 target) ; Learning rate = 0.000276 ; Loss = 1.488422\n",
      "2024-06-17 06:36:14.296000: I runner.py:310] Step = 102400 ; steps/s = 1.65, tokens/s = 81356 (36410 source, 44946 target) ; Learning rate = 0.000276 ; Loss = 1.488263\n",
      "2024-06-17 06:37:14.799000: I runner.py:310] Step = 102500 ; steps/s = 1.65, tokens/s = 81315 (36394 source, 44921 target) ; Learning rate = 0.000276 ; Loss = 1.485681\n",
      "2024-06-17 06:38:14.880000: I runner.py:310] Step = 102600 ; steps/s = 1.66, tokens/s = 80313 (35967 source, 44346 target) ; Learning rate = 0.000276 ; Loss = 1.488639\n",
      "2024-06-17 06:39:15.344000: I runner.py:310] Step = 102700 ; steps/s = 1.65, tokens/s = 81365 (36421 source, 44944 target) ; Learning rate = 0.000276 ; Loss = 1.482307\n",
      "2024-06-17 06:40:15.911000: I runner.py:310] Step = 102800 ; steps/s = 1.65, tokens/s = 81219 (36325 source, 44894 target) ; Learning rate = 0.000276 ; Loss = 1.480662\n",
      "2024-06-17 06:41:16.427000: I runner.py:310] Step = 102900 ; steps/s = 1.65, tokens/s = 81269 (36370 source, 44899 target) ; Learning rate = 0.000276 ; Loss = 1.483306\n",
      "2024-06-17 06:42:16.521000: I runner.py:310] Step = 103000 ; steps/s = 1.66, tokens/s = 80330 (35980 source, 44350 target) ; Learning rate = 0.000275 ; Loss = 1.487252\n",
      "2024-06-17 06:43:16.997000: I runner.py:310] Step = 103100 ; steps/s = 1.65, tokens/s = 81318 (36391 source, 44927 target) ; Learning rate = 0.000275 ; Loss = 1.485920\n",
      "2024-06-17 06:44:17.499000: I runner.py:310] Step = 103200 ; steps/s = 1.65, tokens/s = 81311 (36383 source, 44928 target) ; Learning rate = 0.000275 ; Loss = 1.486424\n",
      "2024-06-17 06:45:18.016000: I runner.py:310] Step = 103300 ; steps/s = 1.65, tokens/s = 81298 (36381 source, 44917 target) ; Learning rate = 0.000275 ; Loss = 1.481074\n",
      "2024-06-17 06:46:18.036000: I runner.py:310] Step = 103400 ; steps/s = 1.67, tokens/s = 80397 (35965 source, 44432 target) ; Learning rate = 0.000275 ; Loss = 1.478688\n",
      "2024-06-17 06:47:18.543000: I runner.py:310] Step = 103500 ; steps/s = 1.65, tokens/s = 81276 (36385 source, 44891 target) ; Learning rate = 0.000275 ; Loss = 1.482931\n",
      "2024-06-17 06:48:19.056000: I runner.py:310] Step = 103600 ; steps/s = 1.65, tokens/s = 81310 (36398 source, 44912 target) ; Learning rate = 0.000275 ; Loss = 1.488422\n",
      "2024-06-17 06:49:19.485000: I runner.py:310] Step = 103700 ; steps/s = 1.66, tokens/s = 81419 (36450 source, 44969 target) ; Learning rate = 0.000274 ; Loss = 1.486662\n",
      "2024-06-17 06:50:19.543000: I runner.py:310] Step = 103800 ; steps/s = 1.67, tokens/s = 80372 (35950 source, 44422 target) ; Learning rate = 0.000274 ; Loss = 1.477300\n",
      "2024-06-17 06:51:20.128000: I runner.py:310] Step = 103900 ; steps/s = 1.65, tokens/s = 81151 (36308 source, 44843 target) ; Learning rate = 0.000274 ; Loss = 1.487650\n",
      "2024-06-17 06:52:20.756000: I runner.py:310] Step = 104000 ; steps/s = 1.65, tokens/s = 81152 (36332 source, 44820 target) ; Learning rate = 0.000274 ; Loss = 1.481080\n",
      "2024-06-17 06:53:21.198000: I runner.py:310] Step = 104100 ; steps/s = 1.65, tokens/s = 81093 (36305 source, 44788 target) ; Learning rate = 0.000274 ; Loss = 1.508150\n",
      "2024-06-17 06:54:21.376000: I runner.py:310] Step = 104200 ; steps/s = 1.66, tokens/s = 80475 (36021 source, 44454 target) ; Learning rate = 0.000274 ; Loss = 1.485601\n",
      "2024-06-17 06:55:21.900000: I runner.py:310] Step = 104300 ; steps/s = 1.65, tokens/s = 81277 (36358 source, 44919 target) ; Learning rate = 0.000274 ; Loss = 1.480750\n",
      "2024-06-17 06:56:22.387000: I runner.py:310] Step = 104400 ; steps/s = 1.65, tokens/s = 81352 (36403 source, 44949 target) ; Learning rate = 0.000274 ; Loss = 1.481147\n",
      "2024-06-17 06:57:22.437000: I runner.py:310] Step = 104500 ; steps/s = 1.67, tokens/s = 80378 (35999 source, 44379 target) ; Learning rate = 0.000273 ; Loss = 1.488977\n",
      "2024-06-17 06:58:22.976000: I runner.py:310] Step = 104600 ; steps/s = 1.65, tokens/s = 81256 (36380 source, 44876 target) ; Learning rate = 0.000273 ; Loss = 1.480365\n",
      "2024-06-17 06:59:23.459000: I runner.py:310] Step = 104700 ; steps/s = 1.65, tokens/s = 81347 (36385 source, 44962 target) ; Learning rate = 0.000273 ; Loss = 1.484398\n",
      "2024-06-17 07:00:24.021000: I runner.py:310] Step = 104800 ; steps/s = 1.65, tokens/s = 81233 (36357 source, 44876 target) ; Learning rate = 0.000273 ; Loss = 1.486333\n",
      "2024-06-17 07:01:24.139000: I runner.py:310] Step = 104900 ; steps/s = 1.66, tokens/s = 80275 (35899 source, 44376 target) ; Learning rate = 0.000273 ; Loss = 1.487517\n",
      "2024-06-17 07:02:24.658000: I runner.py:310] Step = 105000 ; steps/s = 1.65, tokens/s = 81259 (36372 source, 44887 target) ; Learning rate = 0.000273 ; Loss = 1.482243\n",
      "2024-06-17 07:02:24.660000: I training.py:192] Running evaluation for step 105000\n",
      "2024-06-17 07:06:29.684000: I training.py:192] Evaluation result for step 105000: loss = 1.472488 ; perplexity = 4.360069\n",
      "2024-06-17 07:06:31.239000: W runner.py:310] 0.262% of source tokens are out of vocabulary (6030382 out of 2299988516 tokens)\n",
      "2024-06-17 07:06:31.243000: I runner.py:310] The 10 most frequent out of vocabulary source tokens are: Я (4.8%); Q (3.9%); Ц (3.8%); Ң (3.5%); ӛ (3.2%); щ (2.9%); ° (2.9%); J (2.7%); Z (2.7%); Й (2.4%)\n",
      "2024-06-17 07:06:31.243000: W runner.py:310] 0.225% of target tokens are out of vocabulary (6138260 out of 2727612469 tokens)\n",
      "2024-06-17 07:06:31.244000: I runner.py:310] The 10 most frequent out of vocabulary target tokens are: Q (8.4%); $ (8.3%); ұ (6.4%); + (6.3%); ? (6.1%);  (3.7%); ° (3.4%); ! (3.3%); _ (2.2%);  (2.0%)\n",
      "2024-06-17 07:06:33.233000: I training.py:176] Saved checkpoint KK-EN-Standard-Transformer/ckpt-105000\n",
      "2024-06-17 07:06:33.237000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file: tokens_dev.txt\n",
      "  eval_labels_file: dev_target_tokens.txt\n",
      "  source_vocabulary: kk_vocab.vocab\n",
      "  target_vocabulary: tgt_1_en_vocab.vocab\n",
      "  train_features_file: tokens_train.txt\n",
      "  train_labels_file: train_target_tokens.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: KK-EN-Standard-Transformer\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    scale: 2.0\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 105000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-06-17 07:06:33.411000: I inputter.py:316] Initialized source input layer:\n",
      "2024-06-17 07:06:33.411000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-06-17 07:06:33.411000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-06-17 07:06:33.473000: I inputter.py:316] Initialized target input layer:\n",
      "2024-06-17 07:06:33.473000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-06-17 07:06:33.473000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-06-17 07:06:33.496000: I runner.py:383] Restored checkpoint KK-EN-Standard-Transformer/ckpt-105000\n",
      "2024-06-17 07:06:35.129000: I runner.py:386] Averaging 8 checkpoints...\n",
      "2024-06-17 07:06:35.129000: I runner.py:386] Reading checkpoint KK-EN-Standard-Transformer/ckpt-40000...\n",
      "2024-06-17 07:06:35.454000: I runner.py:386] Reading checkpoint KK-EN-Standard-Transformer/ckpt-50000...\n",
      "2024-06-17 07:06:35.763000: I runner.py:386] Reading checkpoint KK-EN-Standard-Transformer/ckpt-60000...\n",
      "2024-06-17 07:06:36.066000: I runner.py:386] Reading checkpoint KK-EN-Standard-Transformer/ckpt-70000...\n",
      "2024-06-17 07:06:36.385000: I runner.py:386] Reading checkpoint KK-EN-Standard-Transformer/ckpt-80000...\n",
      "2024-06-17 07:06:36.695000: I runner.py:386] Reading checkpoint KK-EN-Standard-Transformer/ckpt-90000...\n",
      "2024-06-17 07:06:36.998000: I runner.py:386] Reading checkpoint KK-EN-Standard-Transformer/ckpt-100000...\n",
      "2024-06-17 07:06:37.307000: I runner.py:386] Reading checkpoint KK-EN-Standard-Transformer/ckpt-105000...\n",
      "2024-06-17 07:06:39.450000: I runner.py:386] Saved averaged checkpoint to KK-EN-Standard-Transformer/avg/ckpt-105000\n"
     ]
    }
   ],
   "source": [
    "# Kk-En (base model)\n",
    "!onmt-main --model kk-standart-modelim.py --config data-transformer.yml --auto_config train --with_eval --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4a99098-fcac-47d6-a34d-9b0eb51b73fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-17 13:04:24.111462: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-17 13:04:24.877466: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-17 13:04:24.877548: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-17 13:04:24.877557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-06-17 13:04:25.836000: I main.py:308] Loading model description from KK-EN-Standard-Transformer/model_description.py\n",
      "2024-06-17 13:04:26.018000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-06-17 13:04:26.018000: I main.py:315] Using model:\n",
      "(model): MyCustomTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): WordEmbedder()\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): WordEmbedder()\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-06-17 13:04:26.023000: I main.py:340] Using parameters:\n",
      "data:\n",
      "  eval_features_file: tokens_dev.txt\n",
      "  eval_labels_file: dev_target_tokens.txt\n",
      "  source_vocabulary: kk_vocab.vocab\n",
      "  target_vocabulary: tgt_1_en_vocab.vocab\n",
      "  train_features_file: tokens_train.txt\n",
      "  train_labels_file: train_target_tokens.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: KK-EN-Standard-Transformer\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 105000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-06-17 13:04:26.200499: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-17 13:04:26.809945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-06-17 13:04:26.952000: I inputter.py:316] Initialized source input layer:\n",
      "2024-06-17 13:04:26.952000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-06-17 13:04:26.952000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-06-17 13:04:27.034000: I inputter.py:316] Initialized target input layer:\n",
      "2024-06-17 13:04:27.034000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-06-17 13:04:27.034000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-06-17 13:04:27.051000: I runner.py:462] Restored checkpoint KK-EN-Standard-Transformer/ckpt-100000\n",
      "2024-06-17 13:04:27.087000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-06-17 13:04:27.565138: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-06-17 13:04:27.680000: I runner.py:471] Tracing and optimizing the inference graph...\n",
      "2024-06-17 13:04:40.916574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-06-17 13:04:41.792980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-06-17 13:04:52.447000: I runner.py:471] 959 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-17 13:05:02.613000: I runner.py:471] 1855 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-17 13:05:13.260000: I runner.py:471] 2751 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-17 13:05:23.624000: I runner.py:471] 3583 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-17 13:05:33.693000: I runner.py:471] 4447 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-17 13:05:44.016000: I runner.py:471] 5375 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-17 13:05:54.061000: I runner.py:471] 6111 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-17 13:06:04.145000: I runner.py:471] 6879 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-17 13:06:14.265000: I runner.py:471] 7711 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-17 13:06:24.521000: I runner.py:471] 8703 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-17 13:06:34.905000: I runner.py:471] 9631 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-17 13:06:45.240000: I runner.py:471] 10495 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-17 13:06:55.478000: I runner.py:471] 11327 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-17 13:07:05.823000: I runner.py:471] 12223 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-17 13:07:15.860000: I runner.py:471] 12927 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-06-17 13:07:35.820000: I runner.py:471] 14510 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-06-17 13:07:46.162000: I runner.py:471] 15310 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-06-17 13:07:56.193000: I runner.py:471] 16238 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-06-17 13:08:06.392000: I runner.py:471] 17102 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-06-17 13:08:16.893000: I runner.py:471] 17845 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-06-17 13:08:36.046000: I runner.py:471] 17809 predictions are buffered, but waiting for the prediction of queued line 281 to advance the output...\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 onmt-main --config data-transformer.yml --auto_config --checkpoint_path KK-EN-Standard-Transformer/ckpt-100000 infer --features_file tokens_test.txt --predictions_file output_kk_en_standart.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5ba17a-e217-41f8-a589-4eabd8102bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 MT-Preparation/subwording/3-desubword.py tgt_1_en_vocab.model output_kk_en_standart.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "020a4f51-ca89-4835-9e6f-af4618ed1d25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference first sentence: In the developed world, this figure is 35 25%\n",
      "Translated first sentence: In developed countries of the world , this figure is 35%\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "BLEU:  BLEU = 49.30 70.9/53.6/43.3/35.9 (BP = 1.000 ratio = 1.098 hyp_len = 454860 ref_len = 414303)\n",
      "CHRF:  chrF2 = 73.67\n"
     ]
    }
   ],
   "source": [
    "# BLEU and chrF scores\n",
    "!python3 compute-bleu.py en_test_shuffled.txt-filtered.en output_kk_en_standart.txt.desubword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d66de3d3-47dc-4101-81c1-3ac8aca3222a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama METEOR Puanı: 0.7329952068579672\n"
     ]
    }
   ],
   "source": [
    "# Average METEOR score (Ortalama METEOR Puanı)\n",
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "def read_and_tokenize_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return [nltk.word_tokenize(line.strip()) for line in lines]\n",
    "\n",
    "def calculate_meteor(reference_file, hypothesis_file):\n",
    "    references = read_and_tokenize_file(reference_file)\n",
    "    hypotheses = read_and_tokenize_file(hypothesis_file)\n",
    "    \n",
    "    if len(references) != len(hypotheses):\n",
    "        raise ValueError(\"Dosyaların satır sayıları eşleşmiyor\")\n",
    "\n",
    "    total_meteor_score = 0.0\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        total_meteor_score += meteor_score([ref], hyp)\n",
    "\n",
    "    average_meteor_score = total_meteor_score / len(references)\n",
    "    return average_meteor_score\n",
    "\n",
    "reference_file = 'en_test_shuffled.txt-filtered.en'\n",
    "hypothesis_file = 'output_kk_en_standart.txt.desubword'\n",
    "\n",
    "score = calculate_meteor(reference_file, hypothesis_file)\n",
    "print(f\"Ortalama METEOR Puanı: {score}\") #Average METEOR score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f6fe27-9069-471c-8313-cc0249e113d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
