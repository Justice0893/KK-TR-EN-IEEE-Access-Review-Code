model_dir: RoBERTa_POS-KK-EN

data:
  # (required for train run type).
  train_features_file:
      - tokens_train.txt
      - pos_tags_train.txt
  train_labels_file: train_target_tokens.txt

  # (required for train_end_eval and eval run types).
  eval_features_file:
      - tokens_dev.txt
      - pos_tags_dev.txt
  eval_labels_file: dev_target_tokens.txt

  # (optional) Models may require additional resource files (e.g. vocabularies).
  source_1_vocabulary: kk_vocab.vocab
  source_2_vocabulary: RoBERTa_KK_unique_pos
  target_vocabulary: tgt_1_en_vocab.vocab

params:
  # The optimizer class name in tf.keras.optimizers or tfa.optimizers.
  optimizer: Adam
  # (optional) Additional optimizer parameters as defined in their documentation.
  # If weight_decay is set, the optimizer will be extended with decoupled weight decay.
  optimizer_params:
    beta_1: 0.8
    beta_2: 0.998
  learning_rate: 2.0

  dropout: 0.1

  decay_type: NoamDecay
  # (optional unless decay_type is set) Decay parameters.
  decay_params:
    model_dim: 512
    warmup_steps: 10000

  beam_width: 8

  length_penalty: 0.2

  coverage_penalty: 0.2

train:
  # (optional) Training batch size. If set to 0, the training will search the largest
  # possible batch size.
  batch_size: 2048
   
  max_step: 105000

  scorers: bleu

  sample_buffer_size: 250000

  save_checkpoints_steps: 10000
  # (optional) How many checkpoints to keep on disk.
  keep_checkpoint_max: 2
  