{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4ea674-59df-4889-b63b-04acb56d9015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing\n",
    "!python3 MT-Preparation/filtering/filter.py Tatoeba.en-tr.tr Tatoeba.en-tr.en tr en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9020d4-f37b-4390-91e4-70c08f468306",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 MT-Preparation/train_dev_split/train_dev_test_split.py 10000 10000 Tatoeba.en-tr.tr-filtered.tr Tatoeba.en-tr.en-filtered.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210c7a6b-1630-4f24-acdf-b5ae9ab626a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"Data-train-src.txt\", \"w\")\n",
    "with open(\"Tatoeba.en-tr.tr-filtered.tr.train\", 'r') as file:\n",
    "    for i in range(300000):\n",
    "        fp.write(file.readline())\n",
    "\n",
    "fp.close()\n",
    "\n",
    "fp = open(\"Data-train-src.txt\", \"a\")\n",
    "with open(\"kk_train_shuffled.txt-filtered.kk\", 'r') as file:\n",
    "    for i in range(300000):\n",
    "        fp.write(file.readline())\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1275778-89d8-4127-8a01-3ef83249d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"Data-train-tgt.txt\", \"w\")\n",
    "with open(\"Tatoeba.en-tr.en-filtered.en.train\", 'r') as file:\n",
    "    for i in range(300000):\n",
    "        fp.write(file.readline())\n",
    "\n",
    "fp.close()\n",
    "\n",
    "fp = open(\"Data-train-tgt.txt\", \"a\")\n",
    "with open(\"en_train_shuffled.txt-filtered.en\", 'r') as file:\n",
    "    for i in range(300000):\n",
    "        fp.write(file.readline())\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d5d45-0445-4efa-9204-a5d70a0e8d7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Kazakh and Turkish shared vocabulary\n",
    "!onmt-build-vocab --sentencepiece model_type=bpe --size 32000 --save_vocab kk_tr_shared_vocab Data-train-src.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180c40f4-2779-46e2-916c-8869eb3963f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#shared vocabulary English side\n",
    "!onmt-build-vocab --sentencepiece model_type=bpe --size 32000 --save_vocab en_shared_vocab Data-train-tgt.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "350820cc-2667-4ada-9aff-ab7cd93e99d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-01 15:51:49.425203: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-01 15:51:50.214137: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-01 15:51:50.214204: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-01 15:51:50.214212: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-01 15:51:51.170000: I onmt-main:8] Creating model directory KK-TR-EN-Shared-vocab\n",
      "2024-12-01 15:51:51.376000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-01 15:51:51.376000: I main.py:315] Using model:\n",
      "(model): MyCustomTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): WordEmbedder()\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): WordEmbedder()\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-01 15:51:51.379292: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-01 15:51:52.938155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-01 15:51:52.938689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7704 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-12-01 15:51:52.939080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 6099 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "2024-12-01 15:51:52.942000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file: KK_tokens_valid_shared\n",
      "  eval_labels_file: KK_valid_target_tokens_shared.txt\n",
      "  source_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file: KK_tokens_train_shared\n",
      "  train_labels_file: KK_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: KK-TR-EN-Shared-vocab\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 250000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-01 15:51:53.274000: I inputter.py:316] Initialized source input layer:\n",
      "2024-12-01 15:51:53.274000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-01 15:51:53.274000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-01 15:51:53.347000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-01 15:51:53.347000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-01 15:51:53.347000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-01 15:51:53.352000: W runner.py:269] No checkpoint to restore in KK-TR-EN-Shared-vocab\n",
      "2024-12-01 15:51:53.354000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "2024-12-01 15:51:53.398000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-01 15:51:54.298379: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-01 15:51:54.416000: I main.py:325] Accumulate gradients of 7 iterations to reach effective batch size of 25000\n",
      "2024-12-01 15:51:54.537000: I mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2024-12-01 15:51:54.743000: I dataset_ops.py:2542] Training on 318032 examples\n",
      "2024-12-01 15:52:58.397280: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-01 15:52:59.419661: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-01 15:52:59.701471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-01 15:53:08.730000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-01 15:53:08.750000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-01 15:53:10.280000: I cross_device_ops.py:897] batch_all_reduce: 260 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-01 15:53:14.182000: I cross_device_ops.py:897] batch_all_reduce: 260 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-01 15:53:20.812000: I runner.py:310] Number of model parameters: 93326081\n",
      "2024-12-01 15:53:20.816000: I runner.py:310] Number of model weights: 260 (trainable = 260, non trainable = 0)\n",
      "2024-12-01 15:53:20.850000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-01 15:53:20.857000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-01 15:53:22.933000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-1\n",
      "2024-12-01 15:53:23.534000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-01 15:53:23.555000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-01 15:53:24.169000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-01 15:53:24.192000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-01 15:53:24.781000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-01 15:53:24.805000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-01 15:54:24.375000: I runner.py:310] Step = 100 ; steps/s = 1.62, tokens/s = 82082 (38917 source, 43165 target) ; Learning rate = 0.000009 ; Loss = 9.749280\n",
      "2024-12-01 15:55:25.872000: I runner.py:310] Step = 200 ; steps/s = 1.63, tokens/s = 82603 (39151 source, 43452 target) ; Learning rate = 0.000018 ; Loss = 8.970151\n",
      "2024-12-01 15:56:27.474000: I runner.py:310] Step = 300 ; steps/s = 1.62, tokens/s = 82446 (39108 source, 43338 target) ; Learning rate = 0.000027 ; Loss = 8.001841\n",
      "2024-12-01 15:57:28.787000: I runner.py:310] Step = 400 ; steps/s = 1.63, tokens/s = 81298 (38521 source, 42777 target) ; Learning rate = 0.000035 ; Loss = 7.317937\n",
      "2024-12-01 15:58:30.292000: I runner.py:310] Step = 500 ; steps/s = 1.63, tokens/s = 82602 (39167 source, 43435 target) ; Learning rate = 0.000044 ; Loss = 7.003286\n",
      "2024-12-01 15:59:31.819000: I runner.py:310] Step = 600 ; steps/s = 1.63, tokens/s = 82585 (39174 source, 43411 target) ; Learning rate = 0.000053 ; Loss = 6.675618\n",
      "2024-12-01 16:00:33.361000: I runner.py:310] Step = 700 ; steps/s = 1.63, tokens/s = 82533 (39110 source, 43423 target) ; Learning rate = 0.000062 ; Loss = 6.455898\n",
      "2024-12-01 16:01:34.346000: I runner.py:310] Step = 800 ; steps/s = 1.64, tokens/s = 81713 (38738 source, 42975 target) ; Learning rate = 0.000071 ; Loss = 6.211778\n",
      "2024-12-01 16:02:35.915000: I runner.py:310] Step = 900 ; steps/s = 1.62, tokens/s = 82526 (39115 source, 43411 target) ; Learning rate = 0.000080 ; Loss = 6.009434\n",
      "2024-12-01 16:03:37.479000: I runner.py:310] Step = 1000 ; steps/s = 1.62, tokens/s = 82546 (39154 source, 43392 target) ; Learning rate = 0.000088 ; Loss = 5.804181\n",
      "2024-12-01 16:04:39.072000: I runner.py:310] Step = 1100 ; steps/s = 1.62, tokens/s = 82471 (39108 source, 43363 target) ; Learning rate = 0.000097 ; Loss = 5.700804\n",
      "2024-12-01 16:05:40.064000: I runner.py:310] Step = 1200 ; steps/s = 1.64, tokens/s = 81697 (38708 source, 42989 target) ; Learning rate = 0.000106 ; Loss = 5.486543\n",
      "2024-12-01 16:06:41.558000: I runner.py:310] Step = 1300 ; steps/s = 1.63, tokens/s = 82584 (39158 source, 43426 target) ; Learning rate = 0.000115 ; Loss = 5.343123\n",
      "2024-12-01 16:07:43.001000: I runner.py:310] Step = 1400 ; steps/s = 1.63, tokens/s = 82713 (39198 source, 43515 target) ; Learning rate = 0.000124 ; Loss = 5.216481\n",
      "2024-12-01 16:08:44.489000: I runner.py:310] Step = 1500 ; steps/s = 1.63, tokens/s = 82623 (39180 source, 43443 target) ; Learning rate = 0.000133 ; Loss = 5.029794\n",
      "2024-12-01 16:09:45.403000: I runner.py:310] Step = 1600 ; steps/s = 1.64, tokens/s = 81833 (38785 source, 43048 target) ; Learning rate = 0.000142 ; Loss = 4.765778\n",
      "2024-12-01 16:10:46.925000: I runner.py:310] Step = 1700 ; steps/s = 1.63, tokens/s = 82607 (39160 source, 43447 target) ; Learning rate = 0.000150 ; Loss = 4.636253\n",
      "2024-12-01 16:11:48.411000: I runner.py:310] Step = 1800 ; steps/s = 1.63, tokens/s = 82638 (39187 source, 43451 target) ; Learning rate = 0.000159 ; Loss = 4.474829\n",
      "2024-12-01 16:12:49.946000: I runner.py:310] Step = 1900 ; steps/s = 1.63, tokens/s = 82520 (39118 source, 43402 target) ; Learning rate = 0.000168 ; Loss = 4.406997\n",
      "2024-12-01 16:13:50.927000: I runner.py:310] Step = 2000 ; steps/s = 1.64, tokens/s = 81688 (38743 source, 42945 target) ; Learning rate = 0.000177 ; Loss = 4.190769\n",
      "2024-12-01 16:14:52.394000: I runner.py:310] Step = 2100 ; steps/s = 1.63, tokens/s = 82663 (39172 source, 43491 target) ; Learning rate = 0.000186 ; Loss = 4.145429\n",
      "2024-12-01 16:15:53.950000: I runner.py:310] Step = 2200 ; steps/s = 1.62, tokens/s = 82522 (39130 source, 43392 target) ; Learning rate = 0.000195 ; Loss = 3.829814\n",
      "2024-12-01 16:16:55.518000: I runner.py:310] Step = 2300 ; steps/s = 1.62, tokens/s = 82506 (39131 source, 43375 target) ; Learning rate = 0.000203 ; Loss = 3.806648\n",
      "2024-12-01 16:17:56.524000: I runner.py:310] Step = 2400 ; steps/s = 1.64, tokens/s = 81691 (38721 source, 42970 target) ; Learning rate = 0.000212 ; Loss = 3.645451\n",
      "2024-12-01 16:18:58.077000: I runner.py:310] Step = 2500 ; steps/s = 1.62, tokens/s = 82547 (39122 source, 43425 target) ; Learning rate = 0.000221 ; Loss = 3.585573\n",
      "2024-12-01 16:19:59.626000: I runner.py:310] Step = 2600 ; steps/s = 1.62, tokens/s = 82546 (39123 source, 43423 target) ; Learning rate = 0.000230 ; Loss = 3.444509\n",
      "2024-12-01 16:21:01.213000: I runner.py:310] Step = 2700 ; steps/s = 1.62, tokens/s = 82471 (39115 source, 43356 target) ; Learning rate = 0.000239 ; Loss = 3.335416\n",
      "2024-12-01 16:22:02.223000: I runner.py:310] Step = 2800 ; steps/s = 1.64, tokens/s = 81682 (38747 source, 42935 target) ; Learning rate = 0.000248 ; Loss = 3.310345\n",
      "2024-12-01 16:23:03.720000: I runner.py:310] Step = 2900 ; steps/s = 1.63, tokens/s = 82571 (39149 source, 43422 target) ; Learning rate = 0.000256 ; Loss = 3.227197\n",
      "2024-12-01 16:24:05.245000: I runner.py:310] Step = 3000 ; steps/s = 1.63, tokens/s = 82598 (39161 source, 43437 target) ; Learning rate = 0.000265 ; Loss = 3.084160\n",
      "2024-12-01 16:25:06.785000: I runner.py:310] Step = 3100 ; steps/s = 1.63, tokens/s = 82548 (39125 source, 43423 target) ; Learning rate = 0.000274 ; Loss = 3.029574\n",
      "2024-12-01 16:26:07.835000: I runner.py:310] Step = 3200 ; steps/s = 1.64, tokens/s = 81643 (38688 source, 42955 target) ; Learning rate = 0.000283 ; Loss = 3.012242\n",
      "2024-12-01 16:27:09.340000: I runner.py:310] Step = 3300 ; steps/s = 1.63, tokens/s = 82587 (39147 source, 43440 target) ; Learning rate = 0.000292 ; Loss = 2.917101\n",
      "2024-12-01 16:28:10.905000: I runner.py:310] Step = 3400 ; steps/s = 1.62, tokens/s = 82525 (39148 source, 43377 target) ; Learning rate = 0.000301 ; Loss = 3.019344\n",
      "2024-12-01 16:29:12.412000: I runner.py:310] Step = 3500 ; steps/s = 1.63, tokens/s = 82609 (39166 source, 43443 target) ; Learning rate = 0.000309 ; Loss = 2.869958\n",
      "2024-12-01 16:30:13.296000: I runner.py:310] Step = 3600 ; steps/s = 1.64, tokens/s = 81847 (38820 source, 43027 target) ; Learning rate = 0.000318 ; Loss = 2.742088\n",
      "2024-12-01 16:31:14.886000: I runner.py:310] Step = 3700 ; steps/s = 1.62, tokens/s = 82472 (39074 source, 43398 target) ; Learning rate = 0.000327 ; Loss = 2.759110\n",
      "2024-12-01 16:32:16.424000: I runner.py:310] Step = 3800 ; steps/s = 1.63, tokens/s = 82610 (39188 source, 43422 target) ; Learning rate = 0.000336 ; Loss = 2.754234\n",
      "2024-12-01 16:33:17.982000: I runner.py:310] Step = 3900 ; steps/s = 1.62, tokens/s = 82502 (39099 source, 43403 target) ; Learning rate = 0.000345 ; Loss = 2.758230\n",
      "2024-12-01 16:34:18.929000: I runner.py:310] Step = 4000 ; steps/s = 1.64, tokens/s = 81764 (38755 source, 43009 target) ; Learning rate = 0.000354 ; Loss = 2.673042\n",
      "2024-12-01 16:35:20.409000: I runner.py:310] Step = 4100 ; steps/s = 1.63, tokens/s = 82643 (39189 source, 43454 target) ; Learning rate = 0.000362 ; Loss = 2.618351\n",
      "2024-12-01 16:36:21.880000: I runner.py:310] Step = 4200 ; steps/s = 1.63, tokens/s = 82660 (39195 source, 43465 target) ; Learning rate = 0.000371 ; Loss = 2.608297\n",
      "2024-12-01 16:37:22.970000: I runner.py:310] Step = 4300 ; steps/s = 1.64, tokens/s = 82069 (38906 source, 43163 target) ; Learning rate = 0.000380 ; Loss = 2.592569\n",
      "2024-12-01 16:38:24.334000: I runner.py:310] Step = 4400 ; steps/s = 1.63, tokens/s = 82302 (39006 source, 43296 target) ; Learning rate = 0.000389 ; Loss = 2.554784\n",
      "2024-12-01 16:39:25.866000: I runner.py:310] Step = 4500 ; steps/s = 1.63, tokens/s = 82555 (39153 source, 43402 target) ; Learning rate = 0.000398 ; Loss = 2.544574\n",
      "2024-12-01 16:40:27.355000: I runner.py:310] Step = 4600 ; steps/s = 1.63, tokens/s = 82603 (39166 source, 43437 target) ; Learning rate = 0.000407 ; Loss = 2.491748\n",
      "2024-12-01 16:41:28.321000: I runner.py:310] Step = 4700 ; steps/s = 1.64, tokens/s = 81732 (38744 source, 42988 target) ; Learning rate = 0.000416 ; Loss = 2.634591\n",
      "2024-12-01 16:42:29.887000: I runner.py:310] Step = 4800 ; steps/s = 1.62, tokens/s = 82500 (39090 source, 43410 target) ; Learning rate = 0.000424 ; Loss = 2.386446\n",
      "2024-12-01 16:43:31.384000: I runner.py:310] Step = 4900 ; steps/s = 1.63, tokens/s = 82623 (39204 source, 43419 target) ; Learning rate = 0.000433 ; Loss = 2.463292\n",
      "2024-12-01 16:44:32.924000: I runner.py:310] Step = 5000 ; steps/s = 1.63, tokens/s = 82546 (39121 source, 43425 target) ; Learning rate = 0.000442 ; Loss = 2.423284\n",
      "2024-12-01 16:44:32.925000: I training.py:192] Running evaluation for step 5000\n",
      "2024-12-01 16:52:00.929000: I training.py:192] Evaluation result for step 5000: loss = 1.256158 ; perplexity = 3.511901\n",
      "2024-12-01 16:53:01.835000: I runner.py:310] Step = 5100 ; steps/s = 1.64, tokens/s = 81837 (38789 source, 43048 target) ; Learning rate = 0.000451 ; Loss = 2.364668\n",
      "2024-12-01 16:54:03.436000: I runner.py:310] Step = 5200 ; steps/s = 1.62, tokens/s = 82438 (39090 source, 43348 target) ; Learning rate = 0.000460 ; Loss = 2.331618\n",
      "2024-12-01 16:55:05.090000: I runner.py:310] Step = 5300 ; steps/s = 1.62, tokens/s = 82398 (39062 source, 43336 target) ; Learning rate = 0.000469 ; Loss = 2.390818\n",
      "2024-12-01 16:56:06.700000: I runner.py:310] Step = 5400 ; steps/s = 1.62, tokens/s = 82486 (39107 source, 43379 target) ; Learning rate = 0.000477 ; Loss = 2.346128\n",
      "2024-12-01 16:57:07.852000: I runner.py:310] Step = 5500 ; steps/s = 1.64, tokens/s = 81523 (38641 source, 42882 target) ; Learning rate = 0.000486 ; Loss = 2.275053\n",
      "2024-12-01 16:58:09.414000: I runner.py:310] Step = 5600 ; steps/s = 1.62, tokens/s = 82505 (39126 source, 43379 target) ; Learning rate = 0.000495 ; Loss = 2.270329\n",
      "2024-12-01 16:59:11.057000: I runner.py:310] Step = 5700 ; steps/s = 1.62, tokens/s = 82419 (39063 source, 43356 target) ; Learning rate = 0.000504 ; Loss = 2.260457\n",
      "2024-12-01 17:00:12.642000: I runner.py:310] Step = 5800 ; steps/s = 1.62, tokens/s = 82491 (39091 source, 43400 target) ; Learning rate = 0.000513 ; Loss = 2.292606\n",
      "2024-12-01 17:01:13.587000: I runner.py:310] Step = 5900 ; steps/s = 1.64, tokens/s = 81772 (38787 source, 42985 target) ; Learning rate = 0.000522 ; Loss = 2.289727\n",
      "2024-12-01 17:02:15.166000: I runner.py:310] Step = 6000 ; steps/s = 1.62, tokens/s = 82501 (39113 source, 43388 target) ; Learning rate = 0.000530 ; Loss = 2.222753\n",
      "2024-12-01 17:03:16.827000: I runner.py:310] Step = 6100 ; steps/s = 1.62, tokens/s = 82374 (39059 source, 43315 target) ; Learning rate = 0.000539 ; Loss = 2.256320\n",
      "2024-12-01 17:04:18.418000: I runner.py:310] Step = 6200 ; steps/s = 1.62, tokens/s = 82484 (39115 source, 43369 target) ; Learning rate = 0.000548 ; Loss = 2.232637\n",
      "2024-12-01 17:05:19.437000: I runner.py:310] Step = 6300 ; steps/s = 1.64, tokens/s = 81680 (38706 source, 42974 target) ; Learning rate = 0.000557 ; Loss = 2.127200\n",
      "2024-12-01 17:06:21.057000: I runner.py:310] Step = 6400 ; steps/s = 1.62, tokens/s = 82417 (39058 source, 43359 target) ; Learning rate = 0.000566 ; Loss = 2.172724\n",
      "2024-12-01 17:07:22.665000: I runner.py:310] Step = 6500 ; steps/s = 1.62, tokens/s = 82484 (39106 source, 43378 target) ; Learning rate = 0.000575 ; Loss = 2.189261\n",
      "2024-12-01 17:08:24.285000: I runner.py:310] Step = 6600 ; steps/s = 1.62, tokens/s = 82453 (39108 source, 43345 target) ; Learning rate = 0.000583 ; Loss = 2.201686\n",
      "2024-12-01 17:09:25.296000: I runner.py:310] Step = 6700 ; steps/s = 1.64, tokens/s = 81664 (38728 source, 42936 target) ; Learning rate = 0.000592 ; Loss = 2.168091\n",
      "2024-12-01 17:10:26.882000: I runner.py:310] Step = 6800 ; steps/s = 1.62, tokens/s = 82500 (39103 source, 43397 target) ; Learning rate = 0.000601 ; Loss = 2.165978\n",
      "2024-12-01 17:11:28.428000: I runner.py:310] Step = 6900 ; steps/s = 1.62, tokens/s = 82531 (39151 source, 43380 target) ; Learning rate = 0.000610 ; Loss = 2.112283\n",
      "2024-12-01 17:12:29.998000: I runner.py:310] Step = 7000 ; steps/s = 1.62, tokens/s = 82527 (39127 source, 43400 target) ; Learning rate = 0.000619 ; Loss = 2.133057\n",
      "2024-12-01 17:13:30.898000: I runner.py:310] Step = 7100 ; steps/s = 1.64, tokens/s = 81844 (38793 source, 43051 target) ; Learning rate = 0.000628 ; Loss = 2.109241\n",
      "2024-12-01 17:14:32.486000: I runner.py:310] Step = 7200 ; steps/s = 1.62, tokens/s = 82450 (39080 source, 43370 target) ; Learning rate = 0.000636 ; Loss = 2.133360\n",
      "2024-12-01 17:15:34.017000: I runner.py:310] Step = 7300 ; steps/s = 1.63, tokens/s = 82584 (39177 source, 43407 target) ; Learning rate = 0.000645 ; Loss = 2.108189\n",
      "2024-12-01 17:16:35.531000: I runner.py:310] Step = 7400 ; steps/s = 1.63, tokens/s = 82593 (39138 source, 43455 target) ; Learning rate = 0.000654 ; Loss = 2.087561\n",
      "2024-12-01 17:17:36.490000: I runner.py:310] Step = 7500 ; steps/s = 1.64, tokens/s = 81746 (38746 source, 43000 target) ; Learning rate = 0.000663 ; Loss = 2.087336\n",
      "2024-12-01 17:18:38.031000: I runner.py:310] Step = 7600 ; steps/s = 1.63, tokens/s = 82527 (39123 source, 43404 target) ; Learning rate = 0.000672 ; Loss = 2.041871\n",
      "2024-12-01 17:19:39.593000: I runner.py:310] Step = 7700 ; steps/s = 1.62, tokens/s = 82531 (39142 source, 43389 target) ; Learning rate = 0.000681 ; Loss = 2.071119\n",
      "2024-12-01 17:20:41.120000: I runner.py:310] Step = 7800 ; steps/s = 1.63, tokens/s = 82593 (39148 source, 43445 target) ; Learning rate = 0.000690 ; Loss = 2.082692\n",
      "2024-12-01 17:21:42.092000: I runner.py:310] Step = 7900 ; steps/s = 1.64, tokens/s = 81722 (38748 source, 42974 target) ; Learning rate = 0.000698 ; Loss = 1.994521\n",
      "2024-12-01 17:22:43.621000: I runner.py:310] Step = 8000 ; steps/s = 1.63, tokens/s = 82548 (39158 source, 43390 target) ; Learning rate = 0.000707 ; Loss = 2.051642\n",
      "2024-12-01 17:23:45.202000: I runner.py:310] Step = 8100 ; steps/s = 1.62, tokens/s = 82510 (39099 source, 43411 target) ; Learning rate = 0.000716 ; Loss = 2.040190\n",
      "2024-12-01 17:24:46.735000: I runner.py:310] Step = 8200 ; steps/s = 1.63, tokens/s = 82593 (39150 source, 43443 target) ; Learning rate = 0.000725 ; Loss = 2.077532\n",
      "2024-12-01 17:25:47.658000: I runner.py:310] Step = 8300 ; steps/s = 1.64, tokens/s = 81795 (38760 source, 43035 target) ; Learning rate = 0.000734 ; Loss = 1.975334\n",
      "2024-12-01 17:26:49.221000: I runner.py:310] Step = 8400 ; steps/s = 1.62, tokens/s = 82565 (39153 source, 43412 target) ; Learning rate = 0.000743 ; Loss = 2.014287\n",
      "2024-12-01 17:27:50.685000: I runner.py:310] Step = 8500 ; steps/s = 1.63, tokens/s = 82604 (39173 source, 43431 target) ; Learning rate = 0.000751 ; Loss = 2.027897\n",
      "2024-12-01 17:28:52.005000: I runner.py:310] Step = 8600 ; steps/s = 1.63, tokens/s = 82225 (38980 source, 43245 target) ; Learning rate = 0.000760 ; Loss = 2.034459\n",
      "2024-12-01 17:29:53.157000: I runner.py:310] Step = 8700 ; steps/s = 1.64, tokens/s = 82094 (38920 source, 43174 target) ; Learning rate = 0.000769 ; Loss = 1.957233\n",
      "2024-12-01 17:30:54.729000: I runner.py:310] Step = 8800 ; steps/s = 1.62, tokens/s = 82527 (39124 source, 43403 target) ; Learning rate = 0.000778 ; Loss = 1.988760\n",
      "2024-12-01 17:31:56.200000: I runner.py:310] Step = 8900 ; steps/s = 1.63, tokens/s = 82635 (39178 source, 43457 target) ; Learning rate = 0.000787 ; Loss = 1.996539\n",
      "2024-12-01 17:32:57.251000: I runner.py:310] Step = 9000 ; steps/s = 1.64, tokens/s = 81631 (38705 source, 42926 target) ; Learning rate = 0.000796 ; Loss = 1.988385\n",
      "2024-12-01 17:33:58.781000: I runner.py:310] Step = 9100 ; steps/s = 1.63, tokens/s = 82606 (39144 source, 43462 target) ; Learning rate = 0.000804 ; Loss = 1.960278\n",
      "2024-12-01 17:35:00.296000: I runner.py:310] Step = 9200 ; steps/s = 1.63, tokens/s = 82570 (39177 source, 43393 target) ; Learning rate = 0.000813 ; Loss = 1.971171\n",
      "2024-12-01 17:36:01.825000: I runner.py:310] Step = 9300 ; steps/s = 1.63, tokens/s = 82567 (39146 source, 43421 target) ; Learning rate = 0.000822 ; Loss = 1.967017\n",
      "2024-12-01 17:37:02.777000: I runner.py:310] Step = 9400 ; steps/s = 1.64, tokens/s = 81736 (38736 source, 43000 target) ; Learning rate = 0.000831 ; Loss = 1.963369\n",
      "2024-12-01 17:38:04.305000: I runner.py:310] Step = 9500 ; steps/s = 1.63, tokens/s = 82599 (39153 source, 43446 target) ; Learning rate = 0.000840 ; Loss = 1.914961\n",
      "2024-12-01 17:39:05.804000: I runner.py:310] Step = 9600 ; steps/s = 1.63, tokens/s = 82612 (39162 source, 43450 target) ; Learning rate = 0.000849 ; Loss = 1.943317\n",
      "2024-12-01 17:40:07.389000: I runner.py:310] Step = 9700 ; steps/s = 1.62, tokens/s = 82470 (39105 source, 43365 target) ; Learning rate = 0.000857 ; Loss = 1.960823\n",
      "2024-12-01 17:41:08.326000: I runner.py:310] Step = 9800 ; steps/s = 1.64, tokens/s = 81772 (38767 source, 43005 target) ; Learning rate = 0.000866 ; Loss = 1.894834\n",
      "2024-12-01 17:42:09.864000: I runner.py:310] Step = 9900 ; steps/s = 1.63, tokens/s = 82546 (39117 source, 43429 target) ; Learning rate = 0.000875 ; Loss = 1.915113\n",
      "2024-12-01 17:43:11.481000: I runner.py:310] Step = 10000 ; steps/s = 1.62, tokens/s = 82454 (39126 source, 43328 target) ; Learning rate = 0.000884 ; Loss = 1.944677\n",
      "2024-12-01 17:43:13.252000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-10000\n",
      "2024-12-01 17:43:13.252000: I training.py:192] Running evaluation for step 10000\n",
      "2024-12-01 17:49:31.905000: I training.py:192] Evaluation result for step 10000: loss = 1.082318 ; perplexity = 2.951513\n",
      "2024-12-01 17:50:33.292000: I runner.py:310] Step = 10100 ; steps/s = 1.63, tokens/s = 82797 (39251 source, 43546 target) ; Learning rate = 0.000879 ; Loss = 1.946798\n",
      "2024-12-01 17:51:34.166000: I runner.py:310] Step = 10200 ; steps/s = 1.64, tokens/s = 81853 (38785 source, 43068 target) ; Learning rate = 0.000875 ; Loss = 1.939823\n",
      "2024-12-01 17:52:35.689000: I runner.py:310] Step = 10300 ; steps/s = 1.63, tokens/s = 82550 (39132 source, 43418 target) ; Learning rate = 0.000871 ; Loss = 1.906951\n",
      "2024-12-01 17:53:37.238000: I runner.py:310] Step = 10400 ; steps/s = 1.62, tokens/s = 82570 (39169 source, 43401 target) ; Learning rate = 0.000867 ; Loss = 1.912247\n",
      "2024-12-01 17:54:38.737000: I runner.py:310] Step = 10500 ; steps/s = 1.63, tokens/s = 82598 (39167 source, 43431 target) ; Learning rate = 0.000863 ; Loss = 1.914960\n",
      "2024-12-01 17:55:39.676000: I runner.py:310] Step = 10600 ; steps/s = 1.64, tokens/s = 81784 (38754 source, 43030 target) ; Learning rate = 0.000858 ; Loss = 1.880953\n",
      "2024-12-01 17:56:41.218000: I runner.py:310] Step = 10700 ; steps/s = 1.63, tokens/s = 82523 (39110 source, 43413 target) ; Learning rate = 0.000854 ; Loss = 1.860226\n",
      "2024-12-01 17:57:42.760000: I runner.py:310] Step = 10800 ; steps/s = 1.63, tokens/s = 82553 (39171 source, 43382 target) ; Learning rate = 0.000850 ; Loss = 1.873386\n",
      "2024-12-01 17:58:44.268000: I runner.py:310] Step = 10900 ; steps/s = 1.63, tokens/s = 82623 (39169 source, 43454 target) ; Learning rate = 0.000847 ; Loss = 1.881452\n",
      "2024-12-01 17:59:45.221000: I runner.py:310] Step = 11000 ; steps/s = 1.64, tokens/s = 81744 (38730 source, 43014 target) ; Learning rate = 0.000843 ; Loss = 1.829165\n",
      "2024-12-01 18:00:46.725000: I runner.py:310] Step = 11100 ; steps/s = 1.63, tokens/s = 82599 (39159 source, 43440 target) ; Learning rate = 0.000839 ; Loss = 1.901997\n",
      "2024-12-01 18:01:48.174000: I runner.py:310] Step = 11200 ; steps/s = 1.63, tokens/s = 82659 (39198 source, 43461 target) ; Learning rate = 0.000835 ; Loss = 1.875189\n",
      "2024-12-01 18:02:49.682000: I runner.py:310] Step = 11300 ; steps/s = 1.63, tokens/s = 82609 (39170 source, 43439 target) ; Learning rate = 0.000831 ; Loss = 1.901414\n",
      "2024-12-01 18:03:50.550000: I runner.py:310] Step = 11400 ; steps/s = 1.64, tokens/s = 81891 (38857 source, 43034 target) ; Learning rate = 0.000828 ; Loss = 1.845187\n",
      "2024-12-01 18:04:52.026000: I runner.py:310] Step = 11500 ; steps/s = 1.63, tokens/s = 82634 (39179 source, 43455 target) ; Learning rate = 0.000824 ; Loss = 1.831967\n",
      "2024-12-01 18:05:53.527000: I runner.py:310] Step = 11600 ; steps/s = 1.63, tokens/s = 82596 (39145 source, 43451 target) ; Learning rate = 0.000821 ; Loss = 1.834370\n",
      "2024-12-01 18:06:55.070000: I runner.py:310] Step = 11700 ; steps/s = 1.63, tokens/s = 82555 (39110 source, 43445 target) ; Learning rate = 0.000817 ; Loss = 1.846722\n",
      "2024-12-01 18:07:56.021000: I runner.py:310] Step = 11800 ; steps/s = 1.64, tokens/s = 81715 (38742 source, 42973 target) ; Learning rate = 0.000814 ; Loss = 1.798346\n",
      "2024-12-01 18:08:57.577000: I runner.py:310] Step = 11900 ; steps/s = 1.62, tokens/s = 82550 (39125 source, 43425 target) ; Learning rate = 0.000810 ; Loss = 1.826163\n",
      "2024-12-01 18:09:59.028000: I runner.py:310] Step = 12000 ; steps/s = 1.63, tokens/s = 82685 (39216 source, 43469 target) ; Learning rate = 0.000807 ; Loss = 1.853099\n",
      "2024-12-01 18:11:00.563000: I runner.py:310] Step = 12100 ; steps/s = 1.63, tokens/s = 82552 (39139 source, 43413 target) ; Learning rate = 0.000803 ; Loss = 1.877245\n",
      "2024-12-01 18:12:01.483000: I runner.py:310] Step = 12200 ; steps/s = 1.64, tokens/s = 81848 (38803 source, 43045 target) ; Learning rate = 0.000800 ; Loss = 1.773738\n",
      "2024-12-01 18:13:02.929000: I runner.py:310] Step = 12300 ; steps/s = 1.63, tokens/s = 82721 (39212 source, 43509 target) ; Learning rate = 0.000797 ; Loss = 1.778515\n",
      "2024-12-01 18:14:04.334000: I runner.py:310] Step = 12400 ; steps/s = 1.63, tokens/s = 82666 (39211 source, 43455 target) ; Learning rate = 0.000794 ; Loss = 1.809659\n",
      "2024-12-01 18:15:05.795000: I runner.py:310] Step = 12500 ; steps/s = 1.63, tokens/s = 82652 (39175 source, 43477 target) ; Learning rate = 0.000791 ; Loss = 1.841671\n",
      "2024-12-01 18:16:06.708000: I runner.py:310] Step = 12600 ; steps/s = 1.64, tokens/s = 81789 (38769 source, 43020 target) ; Learning rate = 0.000787 ; Loss = 1.757426\n",
      "2024-12-01 18:17:08.237000: I runner.py:310] Step = 12700 ; steps/s = 1.63, tokens/s = 82565 (39144 source, 43421 target) ; Learning rate = 0.000784 ; Loss = 1.811486\n",
      "2024-12-01 18:18:09.684000: I runner.py:310] Step = 12800 ; steps/s = 1.63, tokens/s = 82693 (39190 source, 43503 target) ; Learning rate = 0.000781 ; Loss = 1.789250\n",
      "2024-12-01 18:19:11.115000: I runner.py:310] Step = 12900 ; steps/s = 1.63, tokens/s = 82487 (39120 source, 43367 target) ; Learning rate = 0.000778 ; Loss = 1.843585\n",
      "2024-12-01 18:20:12.141000: I runner.py:310] Step = 13000 ; steps/s = 1.64, tokens/s = 81882 (38814 source, 43068 target) ; Learning rate = 0.000775 ; Loss = 1.748915\n",
      "2024-12-01 18:21:13.671000: I runner.py:310] Step = 13100 ; steps/s = 1.63, tokens/s = 82577 (39143 source, 43434 target) ; Learning rate = 0.000772 ; Loss = 1.786527\n",
      "2024-12-01 18:22:15.119000: I runner.py:310] Step = 13200 ; steps/s = 1.63, tokens/s = 82685 (39186 source, 43499 target) ; Learning rate = 0.000769 ; Loss = 1.817143\n",
      "2024-12-01 18:23:16.067000: I runner.py:310] Step = 13300 ; steps/s = 1.64, tokens/s = 81718 (38765 source, 42953 target) ; Learning rate = 0.000766 ; Loss = 1.772104\n",
      "2024-12-01 18:24:17.554000: I runner.py:310] Step = 13400 ; steps/s = 1.63, tokens/s = 82634 (39159 source, 43475 target) ; Learning rate = 0.000764 ; Loss = 1.744040\n",
      "2024-12-01 18:25:19.049000: I runner.py:310] Step = 13500 ; steps/s = 1.63, tokens/s = 82598 (39174 source, 43424 target) ; Learning rate = 0.000761 ; Loss = 1.761645\n",
      "2024-12-01 18:26:20.558000: I runner.py:310] Step = 13600 ; steps/s = 1.63, tokens/s = 82622 (39186 source, 43436 target) ; Learning rate = 0.000758 ; Loss = 1.776361\n",
      "2024-12-01 18:27:21.545000: I runner.py:310] Step = 13700 ; steps/s = 1.64, tokens/s = 81702 (38725 source, 42977 target) ; Learning rate = 0.000755 ; Loss = 1.746019\n",
      "2024-12-01 18:28:22.984000: I runner.py:310] Step = 13800 ; steps/s = 1.63, tokens/s = 82696 (39196 source, 43500 target) ; Learning rate = 0.000752 ; Loss = 1.765324\n",
      "2024-12-01 18:29:24.505000: I runner.py:310] Step = 13900 ; steps/s = 1.63, tokens/s = 82578 (39173 source, 43405 target) ; Learning rate = 0.000750 ; Loss = 1.749147\n",
      "2024-12-01 18:30:26.028000: I runner.py:310] Step = 14000 ; steps/s = 1.63, tokens/s = 82597 (39144 source, 43453 target) ; Learning rate = 0.000747 ; Loss = 1.744593\n",
      "2024-12-01 18:31:26.939000: I runner.py:310] Step = 14100 ; steps/s = 1.64, tokens/s = 81794 (38788 source, 43006 target) ; Learning rate = 0.000744 ; Loss = 1.729346\n",
      "2024-12-01 18:32:28.433000: I runner.py:310] Step = 14200 ; steps/s = 1.63, tokens/s = 82600 (39144 source, 43456 target) ; Learning rate = 0.000742 ; Loss = 1.742190\n",
      "2024-12-01 18:33:29.934000: I runner.py:310] Step = 14300 ; steps/s = 1.63, tokens/s = 82628 (39203 source, 43425 target) ; Learning rate = 0.000739 ; Loss = 1.730923\n",
      "2024-12-01 18:34:31.408000: I runner.py:310] Step = 14400 ; steps/s = 1.63, tokens/s = 82618 (39149 source, 43469 target) ; Learning rate = 0.000737 ; Loss = 1.758710\n",
      "2024-12-01 18:35:32.370000: I runner.py:310] Step = 14500 ; steps/s = 1.64, tokens/s = 81749 (38751 source, 42998 target) ; Learning rate = 0.000734 ; Loss = 1.736723\n",
      "2024-12-01 18:36:33.842000: I runner.py:310] Step = 14600 ; steps/s = 1.63, tokens/s = 82626 (39152 source, 43474 target) ; Learning rate = 0.000731 ; Loss = 1.714151\n",
      "2024-12-01 18:37:35.344000: I runner.py:310] Step = 14700 ; steps/s = 1.63, tokens/s = 82587 (39184 source, 43403 target) ; Learning rate = 0.000729 ; Loss = 1.721584\n",
      "2024-12-01 18:38:36.869000: I runner.py:310] Step = 14800 ; steps/s = 1.63, tokens/s = 82606 (39156 source, 43450 target) ; Learning rate = 0.000727 ; Loss = 1.726518\n",
      "2024-12-01 18:39:37.833000: I runner.py:310] Step = 14900 ; steps/s = 1.64, tokens/s = 81775 (38762 source, 43013 target) ; Learning rate = 0.000724 ; Loss = 1.689905\n",
      "2024-12-01 18:40:39.290000: I runner.py:310] Step = 15000 ; steps/s = 1.63, tokens/s = 82663 (39187 source, 43476 target) ; Learning rate = 0.000722 ; Loss = 1.725431\n",
      "2024-12-01 18:40:39.291000: I training.py:192] Running evaluation for step 15000\n",
      "2024-12-01 18:45:57.879000: I training.py:192] Evaluation result for step 15000: loss = 1.091082 ; perplexity = 2.977494\n",
      "2024-12-01 18:46:59.259000: I runner.py:310] Step = 15100 ; steps/s = 1.63, tokens/s = 82791 (39281 source, 43510 target) ; Learning rate = 0.000719 ; Loss = 1.731347\n",
      "2024-12-01 18:48:00.691000: I runner.py:310] Step = 15200 ; steps/s = 1.63, tokens/s = 82687 (39179 source, 43508 target) ; Learning rate = 0.000717 ; Loss = 1.732220\n",
      "2024-12-01 18:49:01.671000: I runner.py:310] Step = 15300 ; steps/s = 1.64, tokens/s = 81725 (38758 source, 42967 target) ; Learning rate = 0.000715 ; Loss = 1.716709\n",
      "2024-12-01 18:50:03.207000: I runner.py:310] Step = 15400 ; steps/s = 1.63, tokens/s = 82580 (39120 source, 43460 target) ; Learning rate = 0.000712 ; Loss = 1.694662\n",
      "2024-12-01 18:51:04.731000: I runner.py:310] Step = 15500 ; steps/s = 1.63, tokens/s = 82599 (39161 source, 43438 target) ; Learning rate = 0.000710 ; Loss = 1.706576\n",
      "2024-12-01 18:52:06.260000: I runner.py:310] Step = 15600 ; steps/s = 1.63, tokens/s = 82532 (39145 source, 43387 target) ; Learning rate = 0.000708 ; Loss = 1.707893\n",
      "2024-12-01 18:53:07.180000: I runner.py:310] Step = 15700 ; steps/s = 1.64, tokens/s = 81756 (38747 source, 43009 target) ; Learning rate = 0.000705 ; Loss = 1.697698\n",
      "2024-12-01 18:54:08.679000: I runner.py:310] Step = 15800 ; steps/s = 1.63, tokens/s = 82586 (39148 source, 43438 target) ; Learning rate = 0.000703 ; Loss = 1.683480\n",
      "2024-12-01 18:55:10.178000: I runner.py:310] Step = 15900 ; steps/s = 1.63, tokens/s = 82656 (39190 source, 43466 target) ; Learning rate = 0.000701 ; Loss = 1.705863\n",
      "2024-12-01 18:56:11.740000: I runner.py:310] Step = 16000 ; steps/s = 1.62, tokens/s = 82522 (39131 source, 43391 target) ; Learning rate = 0.000699 ; Loss = 1.713802\n",
      "2024-12-01 18:57:12.639000: I runner.py:310] Step = 16100 ; steps/s = 1.64, tokens/s = 81804 (38789 source, 43015 target) ; Learning rate = 0.000697 ; Loss = 1.657082\n",
      "2024-12-01 18:58:14.110000: I runner.py:310] Step = 16200 ; steps/s = 1.63, tokens/s = 82628 (39188 source, 43440 target) ; Learning rate = 0.000694 ; Loss = 1.701678\n",
      "2024-12-01 18:59:15.677000: I runner.py:310] Step = 16300 ; steps/s = 1.62, tokens/s = 82542 (39121 source, 43421 target) ; Learning rate = 0.000692 ; Loss = 1.717876\n",
      "2024-12-01 19:00:17.160000: I runner.py:310] Step = 16400 ; steps/s = 1.63, tokens/s = 82633 (39189 source, 43444 target) ; Learning rate = 0.000690 ; Loss = 1.711246\n",
      "2024-12-01 19:01:18.079000: I runner.py:310] Step = 16500 ; steps/s = 1.64, tokens/s = 81829 (38778 source, 43051 target) ; Learning rate = 0.000688 ; Loss = 1.712606\n",
      "2024-12-01 19:02:19.600000: I runner.py:310] Step = 16600 ; steps/s = 1.63, tokens/s = 82570 (39143 source, 43427 target) ; Learning rate = 0.000686 ; Loss = 1.679586\n",
      "2024-12-01 19:03:21.085000: I runner.py:310] Step = 16700 ; steps/s = 1.63, tokens/s = 82626 (39179 source, 43447 target) ; Learning rate = 0.000684 ; Loss = 1.688846\n",
      "2024-12-01 19:04:22.609000: I runner.py:310] Step = 16800 ; steps/s = 1.63, tokens/s = 82559 (39150 source, 43409 target) ; Learning rate = 0.000682 ; Loss = 1.677796\n",
      "2024-12-01 19:05:23.506000: I runner.py:310] Step = 16900 ; steps/s = 1.64, tokens/s = 81832 (38776 source, 43056 target) ; Learning rate = 0.000680 ; Loss = 1.649592\n",
      "2024-12-01 19:06:25.020000: I runner.py:310] Step = 17000 ; steps/s = 1.63, tokens/s = 82571 (39150 source, 43421 target) ; Learning rate = 0.000678 ; Loss = 1.688465\n",
      "2024-12-01 19:07:26.568000: I runner.py:310] Step = 17100 ; steps/s = 1.62, tokens/s = 82566 (39151 source, 43415 target) ; Learning rate = 0.000676 ; Loss = 1.682775\n",
      "2024-12-01 19:08:28.013000: I runner.py:310] Step = 17200 ; steps/s = 1.63, tokens/s = 82644 (39187 source, 43457 target) ; Learning rate = 0.000674 ; Loss = 1.696237\n",
      "2024-12-01 19:09:28.986000: I runner.py:310] Step = 17300 ; steps/s = 1.64, tokens/s = 81732 (38727 source, 43005 target) ; Learning rate = 0.000672 ; Loss = 1.680128\n",
      "2024-12-01 19:10:30.439000: I runner.py:310] Step = 17400 ; steps/s = 1.63, tokens/s = 82673 (39214 source, 43459 target) ; Learning rate = 0.000670 ; Loss = 1.666025\n",
      "2024-12-01 19:11:31.979000: I runner.py:310] Step = 17500 ; steps/s = 1.63, tokens/s = 82563 (39139 source, 43424 target) ; Learning rate = 0.000668 ; Loss = 1.672640\n",
      "2024-12-01 19:12:32.918000: I runner.py:310] Step = 17600 ; steps/s = 1.64, tokens/s = 81749 (38759 source, 42990 target) ; Learning rate = 0.000666 ; Loss = 1.659797\n",
      "2024-12-01 19:13:34.424000: I runner.py:310] Step = 17700 ; steps/s = 1.63, tokens/s = 82632 (39166 source, 43466 target) ; Learning rate = 0.000664 ; Loss = 1.637398\n",
      "2024-12-01 19:14:35.967000: I runner.py:310] Step = 17800 ; steps/s = 1.63, tokens/s = 82529 (39138 source, 43391 target) ; Learning rate = 0.000662 ; Loss = 1.668229\n",
      "2024-12-01 19:15:37.488000: I runner.py:310] Step = 17900 ; steps/s = 1.63, tokens/s = 82593 (39154 source, 43439 target) ; Learning rate = 0.000661 ; Loss = 1.672043\n",
      "2024-12-01 19:16:38.426000: I runner.py:310] Step = 18000 ; steps/s = 1.64, tokens/s = 81751 (38756 source, 42995 target) ; Learning rate = 0.000659 ; Loss = 1.662627\n",
      "2024-12-01 19:17:39.933000: I runner.py:310] Step = 18100 ; steps/s = 1.63, tokens/s = 82620 (39169 source, 43451 target) ; Learning rate = 0.000657 ; Loss = 1.659316\n",
      "2024-12-01 19:18:41.472000: I runner.py:310] Step = 18200 ; steps/s = 1.63, tokens/s = 82559 (39147 source, 43412 target) ; Learning rate = 0.000655 ; Loss = 1.659383\n",
      "2024-12-01 19:19:42.962000: I runner.py:310] Step = 18300 ; steps/s = 1.63, tokens/s = 82584 (39151 source, 43433 target) ; Learning rate = 0.000653 ; Loss = 1.665234\n",
      "2024-12-01 19:20:43.957000: I runner.py:310] Step = 18400 ; steps/s = 1.64, tokens/s = 81724 (38744 source, 42980 target) ; Learning rate = 0.000652 ; Loss = 1.649727\n",
      "2024-12-01 19:21:45.456000: I runner.py:310] Step = 18500 ; steps/s = 1.63, tokens/s = 82614 (39179 source, 43435 target) ; Learning rate = 0.000650 ; Loss = 1.657679\n",
      "2024-12-01 19:22:47.012000: I runner.py:310] Step = 18600 ; steps/s = 1.62, tokens/s = 82533 (39122 source, 43411 target) ; Learning rate = 0.000648 ; Loss = 1.649273\n",
      "2024-12-01 19:23:48.490000: I runner.py:310] Step = 18700 ; steps/s = 1.63, tokens/s = 82639 (39159 source, 43480 target) ; Learning rate = 0.000646 ; Loss = 1.654091\n",
      "2024-12-01 19:24:49.529000: I runner.py:310] Step = 18800 ; steps/s = 1.64, tokens/s = 81616 (38706 source, 42910 target) ; Learning rate = 0.000645 ; Loss = 1.648219\n",
      "2024-12-01 19:25:51.064000: I runner.py:310] Step = 18900 ; steps/s = 1.63, tokens/s = 82550 (39130 source, 43420 target) ; Learning rate = 0.000643 ; Loss = 1.657181\n",
      "2024-12-01 19:26:52.623000: I runner.py:310] Step = 19000 ; steps/s = 1.62, tokens/s = 82486 (39130 source, 43356 target) ; Learning rate = 0.000641 ; Loss = 1.636661\n",
      "2024-12-01 19:27:54.146000: I runner.py:310] Step = 19100 ; steps/s = 1.63, tokens/s = 82610 (39166 source, 43444 target) ; Learning rate = 0.000640 ; Loss = 1.661347\n",
      "2024-12-01 19:28:55.084000: I runner.py:310] Step = 19200 ; steps/s = 1.64, tokens/s = 81786 (38746 source, 43040 target) ; Learning rate = 0.000638 ; Loss = 1.622837\n",
      "2024-12-01 19:29:56.549000: I runner.py:310] Step = 19300 ; steps/s = 1.63, tokens/s = 82671 (39208 source, 43463 target) ; Learning rate = 0.000636 ; Loss = 1.650143\n",
      "2024-12-01 19:30:58.080000: I runner.py:310] Step = 19400 ; steps/s = 1.63, tokens/s = 82574 (39134 source, 43440 target) ; Learning rate = 0.000635 ; Loss = 1.670113\n",
      "2024-12-01 19:31:59.617000: I runner.py:310] Step = 19500 ; steps/s = 1.63, tokens/s = 82546 (39152 source, 43394 target) ; Learning rate = 0.000633 ; Loss = 1.673150\n",
      "2024-12-01 19:33:00.633000: I runner.py:310] Step = 19600 ; steps/s = 1.64, tokens/s = 81673 (38735 source, 42938 target) ; Learning rate = 0.000631 ; Loss = 1.625797\n",
      "2024-12-01 19:34:02.129000: I runner.py:310] Step = 19700 ; steps/s = 1.63, tokens/s = 82619 (39134 source, 43485 target) ; Learning rate = 0.000630 ; Loss = 1.637853\n",
      "2024-12-01 19:35:03.728000: I runner.py:310] Step = 19800 ; steps/s = 1.62, tokens/s = 82475 (39077 source, 43398 target) ; Learning rate = 0.000628 ; Loss = 1.651301\n",
      "2024-12-01 19:36:05.244000: I runner.py:310] Step = 19900 ; steps/s = 1.63, tokens/s = 82573 (39192 source, 43381 target) ; Learning rate = 0.000627 ; Loss = 1.650638\n",
      "2024-12-01 19:37:06.256000: I runner.py:310] Step = 20000 ; steps/s = 1.64, tokens/s = 81651 (38702 source, 42949 target) ; Learning rate = 0.000625 ; Loss = 1.611062\n",
      "2024-12-01 19:37:07.985000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-20000\n",
      "2024-12-01 19:37:07.985000: I training.py:192] Running evaluation for step 20000\n",
      "2024-12-01 19:42:25.774000: I training.py:192] Evaluation result for step 20000: loss = 1.120690 ; perplexity = 3.066970\n",
      "2024-12-01 19:43:27.190000: I runner.py:310] Step = 20100 ; steps/s = 1.63, tokens/s = 82720 (39243 source, 43477 target) ; Learning rate = 0.000623 ; Loss = 1.644687\n",
      "2024-12-01 19:44:28.692000: I runner.py:310] Step = 20200 ; steps/s = 1.63, tokens/s = 82625 (39164 source, 43461 target) ; Learning rate = 0.000622 ; Loss = 1.652540\n",
      "2024-12-01 19:45:30.224000: I runner.py:310] Step = 20300 ; steps/s = 1.63, tokens/s = 82574 (39149 source, 43425 target) ; Learning rate = 0.000620 ; Loss = 1.645621\n",
      "2024-12-01 19:46:31.210000: I runner.py:310] Step = 20400 ; steps/s = 1.64, tokens/s = 81722 (38726 source, 42996 target) ; Learning rate = 0.000619 ; Loss = 1.637140\n",
      "2024-12-01 19:47:32.788000: I runner.py:310] Step = 20500 ; steps/s = 1.62, tokens/s = 82507 (39113 source, 43394 target) ; Learning rate = 0.000617 ; Loss = 1.619422\n",
      "2024-12-01 19:48:34.306000: I runner.py:310] Step = 20600 ; steps/s = 1.63, tokens/s = 82585 (39128 source, 43457 target) ; Learning rate = 0.000616 ; Loss = 1.635227\n",
      "2024-12-01 19:49:35.836000: I runner.py:310] Step = 20700 ; steps/s = 1.63, tokens/s = 82558 (39170 source, 43388 target) ; Learning rate = 0.000614 ; Loss = 1.644607\n",
      "2024-12-01 19:50:36.812000: I runner.py:310] Step = 20800 ; steps/s = 1.64, tokens/s = 81719 (38747 source, 42972 target) ; Learning rate = 0.000613 ; Loss = 1.639281\n",
      "2024-12-01 19:51:38.367000: I runner.py:310] Step = 20900 ; steps/s = 1.62, tokens/s = 82527 (39137 source, 43390 target) ; Learning rate = 0.000611 ; Loss = 1.628204\n",
      "2024-12-01 19:52:39.935000: I runner.py:310] Step = 21000 ; steps/s = 1.62, tokens/s = 82489 (39099 source, 43390 target) ; Learning rate = 0.000610 ; Loss = 1.625701\n",
      "2024-12-01 19:53:41.480000: I runner.py:310] Step = 21100 ; steps/s = 1.62, tokens/s = 82567 (39151 source, 43416 target) ; Learning rate = 0.000608 ; Loss = 1.648066\n",
      "2024-12-01 19:54:42.534000: I runner.py:310] Step = 21200 ; steps/s = 1.64, tokens/s = 81625 (38672 source, 42953 target) ; Learning rate = 0.000607 ; Loss = 1.622569\n",
      "2024-12-01 19:55:44.033000: I runner.py:310] Step = 21300 ; steps/s = 1.63, tokens/s = 82603 (39174 source, 43429 target) ; Learning rate = 0.000606 ; Loss = 1.619262\n",
      "2024-12-01 19:56:45.596000: I runner.py:310] Step = 21400 ; steps/s = 1.62, tokens/s = 82502 (39123 source, 43379 target) ; Learning rate = 0.000604 ; Loss = 1.617338\n",
      "2024-12-01 19:57:47.132000: I runner.py:310] Step = 21500 ; steps/s = 1.63, tokens/s = 82570 (39144 source, 43426 target) ; Learning rate = 0.000603 ; Loss = 1.618172\n",
      "2024-12-01 19:58:48.097000: I runner.py:310] Step = 21600 ; steps/s = 1.64, tokens/s = 81748 (38771 source, 42977 target) ; Learning rate = 0.000601 ; Loss = 1.621280\n",
      "2024-12-01 19:59:49.643000: I runner.py:310] Step = 21700 ; steps/s = 1.63, tokens/s = 82555 (39138 source, 43417 target) ; Learning rate = 0.000600 ; Loss = 1.614347\n",
      "2024-12-01 20:00:51.207000: I runner.py:310] Step = 21800 ; steps/s = 1.62, tokens/s = 82530 (39112 source, 43418 target) ; Learning rate = 0.000599 ; Loss = 1.615016\n",
      "2024-12-01 20:01:52.194000: I runner.py:310] Step = 21900 ; steps/s = 1.64, tokens/s = 81693 (38730 source, 42963 target) ; Learning rate = 0.000597 ; Loss = 1.611839\n",
      "2024-12-01 20:02:53.716000: I runner.py:310] Step = 22000 ; steps/s = 1.63, tokens/s = 82566 (39152 source, 43414 target) ; Learning rate = 0.000596 ; Loss = 1.598988\n",
      "2024-12-01 20:03:55.273000: I runner.py:310] Step = 22100 ; steps/s = 1.62, tokens/s = 82530 (39113 source, 43417 target) ; Learning rate = 0.000595 ; Loss = 1.620514\n",
      "2024-12-01 20:04:56.833000: I runner.py:310] Step = 22200 ; steps/s = 1.62, tokens/s = 82537 (39136 source, 43401 target) ; Learning rate = 0.000593 ; Loss = 1.626529\n",
      "2024-12-01 20:05:57.762000: I runner.py:310] Step = 22300 ; steps/s = 1.64, tokens/s = 81787 (38761 source, 43026 target) ; Learning rate = 0.000592 ; Loss = 1.607193\n",
      "2024-12-01 20:06:59.313000: I runner.py:310] Step = 22400 ; steps/s = 1.62, tokens/s = 82521 (39122 source, 43399 target) ; Learning rate = 0.000591 ; Loss = 1.598485\n",
      "2024-12-01 20:08:00.874000: I runner.py:310] Step = 22500 ; steps/s = 1.62, tokens/s = 82509 (39112 source, 43397 target) ; Learning rate = 0.000589 ; Loss = 1.613180\n",
      "2024-12-01 20:09:02.462000: I runner.py:310] Step = 22600 ; steps/s = 1.62, tokens/s = 82492 (39107 source, 43385 target) ; Learning rate = 0.000588 ; Loss = 1.624669\n",
      "2024-12-01 20:10:03.371000: I runner.py:310] Step = 22700 ; steps/s = 1.64, tokens/s = 81855 (38814 source, 43041 target) ; Learning rate = 0.000587 ; Loss = 1.609352\n",
      "2024-12-01 20:11:04.892000: I runner.py:310] Step = 22800 ; steps/s = 1.63, tokens/s = 82632 (39179 source, 43453 target) ; Learning rate = 0.000585 ; Loss = 1.605668\n",
      "2024-12-01 20:12:06.424000: I runner.py:310] Step = 22900 ; steps/s = 1.63, tokens/s = 82523 (39123 source, 43400 target) ; Learning rate = 0.000584 ; Loss = 1.604209\n",
      "2024-12-01 20:13:08.006000: I runner.py:310] Step = 23000 ; steps/s = 1.62, tokens/s = 82471 (39114 source, 43357 target) ; Learning rate = 0.000583 ; Loss = 1.609005\n",
      "2024-12-01 20:14:08.937000: I runner.py:310] Step = 23100 ; steps/s = 1.64, tokens/s = 81779 (38747 source, 43032 target) ; Learning rate = 0.000582 ; Loss = 1.609825\n",
      "2024-12-01 20:15:10.457000: I runner.py:310] Step = 23200 ; steps/s = 1.63, tokens/s = 82571 (39159 source, 43412 target) ; Learning rate = 0.000580 ; Loss = 1.596846\n",
      "2024-12-01 20:16:11.985000: I runner.py:310] Step = 23300 ; steps/s = 1.63, tokens/s = 82530 (39140 source, 43390 target) ; Learning rate = 0.000579 ; Loss = 1.617897\n",
      "2024-12-01 20:17:13.462000: I runner.py:310] Step = 23400 ; steps/s = 1.63, tokens/s = 82664 (39179 source, 43485 target) ; Learning rate = 0.000578 ; Loss = 1.618563\n",
      "2024-12-01 20:18:14.391000: I runner.py:310] Step = 23500 ; steps/s = 1.64, tokens/s = 81808 (38781 source, 43027 target) ; Learning rate = 0.000577 ; Loss = 1.584543\n",
      "2024-12-01 20:19:15.918000: I runner.py:310] Step = 23600 ; steps/s = 1.63, tokens/s = 82559 (39148 source, 43411 target) ; Learning rate = 0.000575 ; Loss = 1.599765\n",
      "2024-12-01 20:20:17.447000: I runner.py:310] Step = 23700 ; steps/s = 1.63, tokens/s = 82577 (39161 source, 43416 target) ; Learning rate = 0.000574 ; Loss = 1.611987\n",
      "2024-12-01 20:21:18.968000: I runner.py:310] Step = 23800 ; steps/s = 1.63, tokens/s = 82570 (39143 source, 43427 target) ; Learning rate = 0.000573 ; Loss = 1.611059\n",
      "2024-12-01 20:22:19.927000: I runner.py:310] Step = 23900 ; steps/s = 1.64, tokens/s = 81766 (38736 source, 43030 target) ; Learning rate = 0.000572 ; Loss = 1.609523\n",
      "2024-12-01 20:23:21.448000: I runner.py:310] Step = 24000 ; steps/s = 1.63, tokens/s = 82596 (39197 source, 43399 target) ; Learning rate = 0.000571 ; Loss = 1.590261\n",
      "2024-12-01 20:24:22.951000: I runner.py:310] Step = 24100 ; steps/s = 1.63, tokens/s = 82603 (39151 source, 43452 target) ; Learning rate = 0.000569 ; Loss = 1.608353\n",
      "2024-12-01 20:25:24.477000: I runner.py:310] Step = 24200 ; steps/s = 1.63, tokens/s = 82556 (39118 source, 43438 target) ; Learning rate = 0.000568 ; Loss = 1.602925\n",
      "2024-12-01 20:26:25.525000: I runner.py:310] Step = 24300 ; steps/s = 1.64, tokens/s = 81599 (38665 source, 42934 target) ; Learning rate = 0.000567 ; Loss = 1.592909\n",
      "2024-12-01 20:27:27.006000: I runner.py:310] Step = 24400 ; steps/s = 1.63, tokens/s = 82603 (39174 source, 43429 target) ; Learning rate = 0.000566 ; Loss = 1.591663\n",
      "2024-12-01 20:28:28.583000: I runner.py:310] Step = 24500 ; steps/s = 1.62, tokens/s = 82541 (39149 source, 43392 target) ; Learning rate = 0.000565 ; Loss = 1.591917\n",
      "2024-12-01 20:29:30.158000: I runner.py:310] Step = 24600 ; steps/s = 1.62, tokens/s = 82500 (39123 source, 43377 target) ; Learning rate = 0.000564 ; Loss = 1.607180\n",
      "2024-12-01 20:30:31.137000: I runner.py:310] Step = 24700 ; steps/s = 1.64, tokens/s = 81728 (38744 source, 42984 target) ; Learning rate = 0.000562 ; Loss = 1.574000\n",
      "2024-12-01 20:31:32.656000: I runner.py:310] Step = 24800 ; steps/s = 1.63, tokens/s = 82546 (39167 source, 43379 target) ; Learning rate = 0.000561 ; Loss = 1.596232\n",
      "2024-12-01 20:32:34.200000: I runner.py:310] Step = 24900 ; steps/s = 1.63, tokens/s = 82540 (39116 source, 43424 target) ; Learning rate = 0.000560 ; Loss = 1.593477\n",
      "2024-12-01 20:33:35.722000: I runner.py:310] Step = 25000 ; steps/s = 1.63, tokens/s = 82632 (39146 source, 43486 target) ; Learning rate = 0.000559 ; Loss = 1.598746\n",
      "2024-12-01 20:33:35.724000: I training.py:192] Running evaluation for step 25000\n",
      "2024-12-01 20:38:07.661000: I training.py:192] Evaluation result for step 25000: loss = 1.136739 ; perplexity = 3.116590\n",
      "2024-12-01 20:39:08.523000: I runner.py:310] Step = 25100 ; steps/s = 1.64, tokens/s = 81910 (38858 source, 43052 target) ; Learning rate = 0.000558 ; Loss = 1.576781\n",
      "2024-12-01 20:40:10.090000: I runner.py:310] Step = 25200 ; steps/s = 1.62, tokens/s = 82494 (39110 source, 43384 target) ; Learning rate = 0.000557 ; Loss = 1.594521\n",
      "2024-12-01 20:41:11.642000: I runner.py:310] Step = 25300 ; steps/s = 1.62, tokens/s = 82524 (39128 source, 43396 target) ; Learning rate = 0.000556 ; Loss = 1.600537\n",
      "2024-12-01 20:42:13.307000: I runner.py:310] Step = 25400 ; steps/s = 1.62, tokens/s = 82406 (39069 source, 43337 target) ; Learning rate = 0.000555 ; Loss = 1.594584\n",
      "2024-12-01 20:43:14.339000: I runner.py:310] Step = 25500 ; steps/s = 1.64, tokens/s = 81636 (38684 source, 42952 target) ; Learning rate = 0.000553 ; Loss = 1.594582\n",
      "2024-12-01 20:44:15.940000: I runner.py:310] Step = 25600 ; steps/s = 1.62, tokens/s = 82446 (39060 source, 43386 target) ; Learning rate = 0.000552 ; Loss = 1.595134\n",
      "2024-12-01 20:45:17.551000: I runner.py:310] Step = 25700 ; steps/s = 1.62, tokens/s = 82456 (39093 source, 43363 target) ; Learning rate = 0.000551 ; Loss = 1.594878\n",
      "2024-12-01 20:46:19.134000: I runner.py:310] Step = 25800 ; steps/s = 1.62, tokens/s = 82517 (39149 source, 43368 target) ; Learning rate = 0.000550 ; Loss = 1.599635\n",
      "2024-12-01 20:47:20.163000: I runner.py:310] Step = 25900 ; steps/s = 1.64, tokens/s = 81643 (38702 source, 42941 target) ; Learning rate = 0.000549 ; Loss = 1.575313\n",
      "2024-12-01 20:48:21.675000: I runner.py:310] Step = 26000 ; steps/s = 1.63, tokens/s = 82599 (39161 source, 43438 target) ; Learning rate = 0.000548 ; Loss = 1.598857\n",
      "2024-12-01 20:49:23.238000: I runner.py:310] Step = 26100 ; steps/s = 1.62, tokens/s = 82528 (39126 source, 43402 target) ; Learning rate = 0.000547 ; Loss = 1.591570\n",
      "2024-12-01 20:50:24.249000: I runner.py:310] Step = 26200 ; steps/s = 1.64, tokens/s = 81675 (38733 source, 42942 target) ; Learning rate = 0.000546 ; Loss = 1.581261\n",
      "2024-12-01 20:51:25.737000: I runner.py:310] Step = 26300 ; steps/s = 1.63, tokens/s = 82629 (39153 source, 43476 target) ; Learning rate = 0.000545 ; Loss = 1.577935\n",
      "2024-12-01 20:52:27.252000: I runner.py:310] Step = 26400 ; steps/s = 1.63, tokens/s = 82577 (39177 source, 43400 target) ; Learning rate = 0.000544 ; Loss = 1.590198\n",
      "2024-12-01 20:53:28.845000: I runner.py:310] Step = 26500 ; steps/s = 1.62, tokens/s = 82509 (39093 source, 43416 target) ; Learning rate = 0.000543 ; Loss = 1.586257\n",
      "2024-12-01 20:54:29.855000: I runner.py:310] Step = 26600 ; steps/s = 1.64, tokens/s = 81652 (38716 source, 42936 target) ; Learning rate = 0.000542 ; Loss = 1.580879\n",
      "2024-12-01 20:55:31.438000: I runner.py:310] Step = 26700 ; steps/s = 1.62, tokens/s = 82529 (39116 source, 43413 target) ; Learning rate = 0.000541 ; Loss = 1.593246\n",
      "2024-12-01 20:56:33.037000: I runner.py:310] Step = 26800 ; steps/s = 1.62, tokens/s = 82488 (39109 source, 43379 target) ; Learning rate = 0.000540 ; Loss = 1.580037\n",
      "2024-12-01 20:57:34.600000: I runner.py:310] Step = 26900 ; steps/s = 1.62, tokens/s = 82502 (39092 source, 43410 target) ; Learning rate = 0.000539 ; Loss = 1.588720\n",
      "2024-12-01 20:58:35.648000: I runner.py:310] Step = 27000 ; steps/s = 1.64, tokens/s = 81606 (38718 source, 42888 target) ; Learning rate = 0.000538 ; Loss = 1.584732\n",
      "2024-12-01 20:59:37.127000: I runner.py:310] Step = 27100 ; steps/s = 1.63, tokens/s = 82630 (39158 source, 43472 target) ; Learning rate = 0.000537 ; Loss = 1.576800\n",
      "2024-12-01 21:00:38.726000: I runner.py:310] Step = 27200 ; steps/s = 1.62, tokens/s = 82501 (39141 source, 43360 target) ; Learning rate = 0.000536 ; Loss = 1.585369\n",
      "2024-12-01 21:01:40.257000: I runner.py:310] Step = 27300 ; steps/s = 1.63, tokens/s = 82528 (39105 source, 43423 target) ; Learning rate = 0.000535 ; Loss = 1.584542\n",
      "2024-12-01 21:02:41.283000: I runner.py:310] Step = 27400 ; steps/s = 1.64, tokens/s = 81687 (38739 source, 42948 target) ; Learning rate = 0.000534 ; Loss = 1.567754\n",
      "2024-12-01 21:03:42.818000: I runner.py:310] Step = 27500 ; steps/s = 1.63, tokens/s = 82594 (39155 source, 43439 target) ; Learning rate = 0.000533 ; Loss = 1.584442\n",
      "2024-12-01 21:04:44.450000: I runner.py:310] Step = 27600 ; steps/s = 1.62, tokens/s = 82391 (39062 source, 43329 target) ; Learning rate = 0.000532 ; Loss = 1.586437\n",
      "2024-12-01 21:05:45.959000: I runner.py:310] Step = 27700 ; steps/s = 1.63, tokens/s = 82605 (39142 source, 43463 target) ; Learning rate = 0.000531 ; Loss = 1.584426\n",
      "2024-12-01 21:06:46.918000: I runner.py:310] Step = 27800 ; steps/s = 1.64, tokens/s = 81739 (38786 source, 42953 target) ; Learning rate = 0.000530 ; Loss = 1.571482\n",
      "2024-12-01 21:07:48.460000: I runner.py:310] Step = 27900 ; steps/s = 1.63, tokens/s = 82550 (39128 source, 43422 target) ; Learning rate = 0.000529 ; Loss = 1.569202\n",
      "2024-12-01 21:08:50.014000: I runner.py:310] Step = 28000 ; steps/s = 1.62, tokens/s = 82514 (39119 source, 43395 target) ; Learning rate = 0.000528 ; Loss = 1.587856\n",
      "2024-12-01 21:09:51.678000: I runner.py:310] Step = 28100 ; steps/s = 1.62, tokens/s = 82382 (39064 source, 43318 target) ; Learning rate = 0.000527 ; Loss = 1.589959\n",
      "2024-12-01 21:10:52.704000: I runner.py:310] Step = 28200 ; steps/s = 1.64, tokens/s = 81673 (38730 source, 42943 target) ; Learning rate = 0.000526 ; Loss = 1.574328\n",
      "2024-12-01 21:11:54.269000: I runner.py:310] Step = 28300 ; steps/s = 1.62, tokens/s = 82480 (39099 source, 43381 target) ; Learning rate = 0.000525 ; Loss = 1.579812\n",
      "2024-12-01 21:12:55.834000: I runner.py:310] Step = 28400 ; steps/s = 1.62, tokens/s = 82535 (39131 source, 43404 target) ; Learning rate = 0.000524 ; Loss = 1.570288\n",
      "2024-12-01 21:13:57.347000: I runner.py:310] Step = 28500 ; steps/s = 1.63, tokens/s = 82616 (39167 source, 43449 target) ; Learning rate = 0.000524 ; Loss = 1.574588\n",
      "2024-12-01 21:14:58.334000: I runner.py:310] Step = 28600 ; steps/s = 1.64, tokens/s = 81700 (38704 source, 42996 target) ; Learning rate = 0.000523 ; Loss = 1.575220\n",
      "2024-12-01 21:15:59.879000: I runner.py:310] Step = 28700 ; steps/s = 1.62, tokens/s = 82554 (39138 source, 43416 target) ; Learning rate = 0.000522 ; Loss = 1.574666\n",
      "2024-12-01 21:17:01.480000: I runner.py:310] Step = 28800 ; steps/s = 1.62, tokens/s = 82459 (39107 source, 43352 target) ; Learning rate = 0.000521 ; Loss = 1.574299\n",
      "2024-12-01 21:18:03.055000: I runner.py:310] Step = 28900 ; steps/s = 1.62, tokens/s = 82508 (39126 source, 43382 target) ; Learning rate = 0.000520 ; Loss = 1.576683\n",
      "2024-12-01 21:19:04.101000: I runner.py:310] Step = 29000 ; steps/s = 1.64, tokens/s = 81632 (38687 source, 42945 target) ; Learning rate = 0.000519 ; Loss = 1.554318\n",
      "2024-12-01 21:20:05.718000: I runner.py:310] Step = 29100 ; steps/s = 1.62, tokens/s = 82453 (39109 source, 43344 target) ; Learning rate = 0.000518 ; Loss = 1.571713\n",
      "2024-12-01 21:21:07.305000: I runner.py:310] Step = 29200 ; steps/s = 1.62, tokens/s = 82462 (39103 source, 43359 target) ; Learning rate = 0.000517 ; Loss = 1.580477\n",
      "2024-12-01 21:22:08.922000: I runner.py:310] Step = 29300 ; steps/s = 1.62, tokens/s = 82450 (39079 source, 43371 target) ; Learning rate = 0.000516 ; Loss = 1.576584\n",
      "2024-12-01 21:23:09.897000: I runner.py:310] Step = 29400 ; steps/s = 1.64, tokens/s = 81723 (38740 source, 42983 target) ; Learning rate = 0.000515 ; Loss = 1.551852\n",
      "2024-12-01 21:24:11.500000: I runner.py:310] Step = 29500 ; steps/s = 1.62, tokens/s = 82474 (39102 source, 43372 target) ; Learning rate = 0.000515 ; Loss = 1.574302\n",
      "2024-12-01 21:25:13.088000: I runner.py:310] Step = 29600 ; steps/s = 1.62, tokens/s = 82483 (39099 source, 43384 target) ; Learning rate = 0.000514 ; Loss = 1.576351\n",
      "2024-12-01 21:26:14.621000: I runner.py:310] Step = 29700 ; steps/s = 1.63, tokens/s = 82590 (39152 source, 43438 target) ; Learning rate = 0.000513 ; Loss = 1.581388\n",
      "2024-12-01 21:27:15.637000: I runner.py:310] Step = 29800 ; steps/s = 1.64, tokens/s = 81658 (38707 source, 42951 target) ; Learning rate = 0.000512 ; Loss = 1.565247\n",
      "2024-12-01 21:28:17.135000: I runner.py:310] Step = 29900 ; steps/s = 1.63, tokens/s = 82592 (39161 source, 43431 target) ; Learning rate = 0.000511 ; Loss = 1.560689\n",
      "2024-12-01 21:29:18.642000: I runner.py:310] Step = 30000 ; steps/s = 1.63, tokens/s = 82618 (39170 source, 43448 target) ; Learning rate = 0.000510 ; Loss = 1.568165\n",
      "2024-12-01 21:29:20.476000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-30000\n",
      "2024-12-01 21:29:20.476000: I training.py:192] Running evaluation for step 30000\n",
      "2024-12-01 21:33:40.362000: I training.py:192] Evaluation result for step 30000: loss = 1.160972 ; perplexity = 3.193036\n",
      "2024-12-01 21:34:41.726000: I runner.py:310] Step = 30100 ; steps/s = 1.63, tokens/s = 82811 (39265 source, 43546 target) ; Learning rate = 0.000509 ; Loss = 1.585475\n",
      "2024-12-01 21:35:42.715000: I runner.py:310] Step = 30200 ; steps/s = 1.64, tokens/s = 81691 (38700 source, 42991 target) ; Learning rate = 0.000509 ; Loss = 1.560395\n",
      "2024-12-01 21:36:44.328000: I runner.py:310] Step = 30300 ; steps/s = 1.62, tokens/s = 82488 (39119 source, 43369 target) ; Learning rate = 0.000508 ; Loss = 1.562890\n",
      "2024-12-01 21:37:45.929000: I runner.py:310] Step = 30400 ; steps/s = 1.62, tokens/s = 82491 (39116 source, 43375 target) ; Learning rate = 0.000507 ; Loss = 1.560481\n",
      "2024-12-01 21:38:47.029000: I runner.py:310] Step = 30500 ; steps/s = 1.64, tokens/s = 81504 (38655 source, 42849 target) ; Learning rate = 0.000506 ; Loss = 1.566628\n",
      "2024-12-01 21:39:48.640000: I runner.py:310] Step = 30600 ; steps/s = 1.62, tokens/s = 82482 (39090 source, 43392 target) ; Learning rate = 0.000505 ; Loss = 1.564353\n",
      "2024-12-01 21:40:50.262000: I runner.py:310] Step = 30700 ; steps/s = 1.62, tokens/s = 82446 (39110 source, 43336 target) ; Learning rate = 0.000504 ; Loss = 1.551240\n",
      "2024-12-01 21:41:51.865000: I runner.py:310] Step = 30800 ; steps/s = 1.62, tokens/s = 82477 (39091 source, 43386 target) ; Learning rate = 0.000504 ; Loss = 1.556198\n",
      "2024-12-01 21:42:52.873000: I runner.py:310] Step = 30900 ; steps/s = 1.64, tokens/s = 81669 (38716 source, 42953 target) ; Learning rate = 0.000503 ; Loss = 1.561223\n",
      "2024-12-01 21:43:54.418000: I runner.py:310] Step = 31000 ; steps/s = 1.63, tokens/s = 82540 (39140 source, 43400 target) ; Learning rate = 0.000502 ; Loss = 1.562087\n",
      "2024-12-01 21:44:56.043000: I runner.py:310] Step = 31100 ; steps/s = 1.62, tokens/s = 82450 (39084 source, 43366 target) ; Learning rate = 0.000501 ; Loss = 1.560861\n",
      "2024-12-01 21:45:57.597000: I runner.py:310] Step = 31200 ; steps/s = 1.62, tokens/s = 82525 (39120 source, 43405 target) ; Learning rate = 0.000500 ; Loss = 1.557873\n",
      "2024-12-01 21:46:58.649000: I runner.py:310] Step = 31300 ; steps/s = 1.64, tokens/s = 81616 (38686 source, 42930 target) ; Learning rate = 0.000500 ; Loss = 1.556729\n",
      "2024-12-01 21:48:00.199000: I runner.py:310] Step = 31400 ; steps/s = 1.62, tokens/s = 82535 (39135 source, 43400 target) ; Learning rate = 0.000499 ; Loss = 1.570016\n",
      "2024-12-01 21:49:01.880000: I runner.py:310] Step = 31500 ; steps/s = 1.62, tokens/s = 82365 (39040 source, 43325 target) ; Learning rate = 0.000498 ; Loss = 1.549430\n",
      "2024-12-01 21:50:03.431000: I runner.py:310] Step = 31600 ; steps/s = 1.62, tokens/s = 82521 (39145 source, 43376 target) ; Learning rate = 0.000497 ; Loss = 1.557408\n",
      "2024-12-01 21:51:04.437000: I runner.py:310] Step = 31700 ; steps/s = 1.64, tokens/s = 81698 (38721 source, 42977 target) ; Learning rate = 0.000496 ; Loss = 1.557404\n",
      "2024-12-01 21:52:06.032000: I runner.py:310] Step = 31800 ; steps/s = 1.62, tokens/s = 82460 (39098 source, 43362 target) ; Learning rate = 0.000496 ; Loss = 1.551744\n",
      "2024-12-01 21:53:07.601000: I runner.py:310] Step = 31900 ; steps/s = 1.62, tokens/s = 82527 (39115 source, 43412 target) ; Learning rate = 0.000495 ; Loss = 1.558393\n",
      "2024-12-01 21:54:09.248000: I runner.py:310] Step = 32000 ; steps/s = 1.62, tokens/s = 82405 (39065 source, 43340 target) ; Learning rate = 0.000494 ; Loss = 1.557685\n",
      "2024-12-01 21:55:10.277000: I runner.py:310] Step = 32100 ; steps/s = 1.64, tokens/s = 81657 (38742 source, 42915 target) ; Learning rate = 0.000493 ; Loss = 1.571290\n",
      "2024-12-01 21:56:11.875000: I runner.py:310] Step = 32200 ; steps/s = 1.62, tokens/s = 82465 (39068 source, 43397 target) ; Learning rate = 0.000493 ; Loss = 1.551818\n",
      "2024-12-01 21:57:13.452000: I runner.py:310] Step = 32300 ; steps/s = 1.62, tokens/s = 82519 (39142 source, 43377 target) ; Learning rate = 0.000492 ; Loss = 1.555223\n",
      "2024-12-01 21:58:15.046000: I runner.py:310] Step = 32400 ; steps/s = 1.62, tokens/s = 82490 (39097 source, 43393 target) ; Learning rate = 0.000491 ; Loss = 1.559977\n",
      "2024-12-01 21:59:16.134000: I runner.py:310] Step = 32500 ; steps/s = 1.64, tokens/s = 81555 (38690 source, 42865 target) ; Learning rate = 0.000490 ; Loss = 1.547647\n",
      "2024-12-01 22:00:17.656000: I runner.py:310] Step = 32600 ; steps/s = 1.63, tokens/s = 82583 (39149 source, 43434 target) ; Learning rate = 0.000490 ; Loss = 1.554238\n",
      "2024-12-01 22:01:19.233000: I runner.py:310] Step = 32700 ; steps/s = 1.62, tokens/s = 82525 (39144 source, 43381 target) ; Learning rate = 0.000489 ; Loss = 1.564830\n",
      "2024-12-01 22:02:20.796000: I runner.py:310] Step = 32800 ; steps/s = 1.62, tokens/s = 82518 (39099 source, 43419 target) ; Learning rate = 0.000488 ; Loss = 1.561691\n",
      "2024-12-01 22:03:21.759000: I runner.py:310] Step = 32900 ; steps/s = 1.64, tokens/s = 81738 (38726 source, 43012 target) ; Learning rate = 0.000487 ; Loss = 1.543243\n",
      "2024-12-01 22:04:23.311000: I runner.py:310] Step = 33000 ; steps/s = 1.62, tokens/s = 82512 (39126 source, 43386 target) ; Learning rate = 0.000487 ; Loss = 1.552351\n",
      "2024-12-01 22:05:24.882000: I runner.py:310] Step = 33100 ; steps/s = 1.62, tokens/s = 82554 (39153 source, 43401 target) ; Learning rate = 0.000486 ; Loss = 1.553996\n",
      "2024-12-01 22:06:26.531000: I runner.py:310] Step = 33200 ; steps/s = 1.62, tokens/s = 82386 (39054 source, 43332 target) ; Learning rate = 0.000485 ; Loss = 1.558948\n",
      "2024-12-01 22:07:27.596000: I runner.py:310] Step = 33300 ; steps/s = 1.64, tokens/s = 81613 (38678 source, 42935 target) ; Learning rate = 0.000484 ; Loss = 1.544051\n",
      "2024-12-01 22:08:29.227000: I runner.py:310] Step = 33400 ; steps/s = 1.62, tokens/s = 82440 (39097 source, 43343 target) ; Learning rate = 0.000484 ; Loss = 1.551280\n",
      "2024-12-01 22:09:30.805000: I runner.py:310] Step = 33500 ; steps/s = 1.62, tokens/s = 82522 (39139 source, 43383 target) ; Learning rate = 0.000483 ; Loss = 1.551431\n",
      "2024-12-01 22:10:32.355000: I runner.py:310] Step = 33600 ; steps/s = 1.62, tokens/s = 82524 (39105 source, 43419 target) ; Learning rate = 0.000482 ; Loss = 1.560913\n",
      "2024-12-01 22:11:33.372000: I runner.py:310] Step = 33700 ; steps/s = 1.64, tokens/s = 81650 (38701 source, 42949 target) ; Learning rate = 0.000481 ; Loss = 1.539178\n",
      "2024-12-01 22:12:34.951000: I runner.py:310] Step = 33800 ; steps/s = 1.62, tokens/s = 82500 (39099 source, 43401 target) ; Learning rate = 0.000481 ; Loss = 1.555746\n",
      "2024-12-01 22:13:36.578000: I runner.py:310] Step = 33900 ; steps/s = 1.62, tokens/s = 82430 (39059 source, 43371 target) ; Learning rate = 0.000480 ; Loss = 1.570166\n",
      "2024-12-01 22:14:38.192000: I runner.py:310] Step = 34000 ; steps/s = 1.62, tokens/s = 82470 (39123 source, 43347 target) ; Learning rate = 0.000479 ; Loss = 1.559714\n",
      "2024-12-01 22:15:39.183000: I runner.py:310] Step = 34100 ; steps/s = 1.64, tokens/s = 81692 (38759 source, 42933 target) ; Learning rate = 0.000479 ; Loss = 1.557073\n",
      "2024-12-01 22:16:40.734000: I runner.py:310] Step = 34200 ; steps/s = 1.62, tokens/s = 82550 (39136 source, 43414 target) ; Learning rate = 0.000478 ; Loss = 1.560956\n",
      "2024-12-01 22:17:42.314000: I runner.py:310] Step = 34300 ; steps/s = 1.62, tokens/s = 82488 (39110 source, 43378 target) ; Learning rate = 0.000477 ; Loss = 1.547302\n",
      "2024-12-01 22:18:43.959000: I runner.py:310] Step = 34400 ; steps/s = 1.62, tokens/s = 82427 (39069 source, 43358 target) ; Learning rate = 0.000477 ; Loss = 1.543512\n",
      "2024-12-01 22:19:44.948000: I runner.py:310] Step = 34500 ; steps/s = 1.64, tokens/s = 81721 (38730 source, 42991 target) ; Learning rate = 0.000476 ; Loss = 1.539161\n",
      "2024-12-01 22:20:46.615000: I runner.py:310] Step = 34600 ; steps/s = 1.62, tokens/s = 82401 (39072 source, 43329 target) ; Learning rate = 0.000475 ; Loss = 1.550346\n",
      "2024-12-01 22:21:48.224000: I runner.py:310] Step = 34700 ; steps/s = 1.62, tokens/s = 82447 (39087 source, 43360 target) ; Learning rate = 0.000474 ; Loss = 1.555447\n",
      "2024-12-01 22:22:49.243000: I runner.py:310] Step = 34800 ; steps/s = 1.64, tokens/s = 81636 (38716 source, 42920 target) ; Learning rate = 0.000474 ; Loss = 1.541241\n",
      "2024-12-01 22:23:50.811000: I runner.py:310] Step = 34900 ; steps/s = 1.62, tokens/s = 82567 (39122 source, 43445 target) ; Learning rate = 0.000473 ; Loss = 1.547703\n",
      "2024-12-01 22:24:52.381000: I runner.py:310] Step = 35000 ; steps/s = 1.62, tokens/s = 82503 (39133 source, 43370 target) ; Learning rate = 0.000472 ; Loss = 1.544595\n",
      "2024-12-01 22:24:52.382000: I training.py:192] Running evaluation for step 35000\n",
      "2024-12-01 22:29:09.396000: I training.py:192] Evaluation result for step 35000: loss = 1.176578 ; perplexity = 3.243258\n",
      "2024-12-01 22:30:10.690000: I runner.py:310] Step = 35100 ; steps/s = 1.63, tokens/s = 82903 (39307 source, 43596 target) ; Learning rate = 0.000472 ; Loss = 1.543188\n",
      "2024-12-01 22:31:11.610000: I runner.py:310] Step = 35200 ; steps/s = 1.64, tokens/s = 81765 (38752 source, 43013 target) ; Learning rate = 0.000471 ; Loss = 1.542404\n",
      "2024-12-01 22:32:13.128000: I runner.py:310] Step = 35300 ; steps/s = 1.63, tokens/s = 82620 (39163 source, 43457 target) ; Learning rate = 0.000470 ; Loss = 1.550839\n",
      "2024-12-01 22:33:14.742000: I runner.py:310] Step = 35400 ; steps/s = 1.62, tokens/s = 82484 (39114 source, 43370 target) ; Learning rate = 0.000470 ; Loss = 1.536847\n",
      "2024-12-01 22:34:16.274000: I runner.py:310] Step = 35500 ; steps/s = 1.63, tokens/s = 82518 (39133 source, 43385 target) ; Learning rate = 0.000469 ; Loss = 1.544243\n",
      "2024-12-01 22:35:17.295000: I runner.py:310] Step = 35600 ; steps/s = 1.64, tokens/s = 81662 (38715 source, 42947 target) ; Learning rate = 0.000468 ; Loss = 1.537227\n",
      "2024-12-01 22:36:18.877000: I runner.py:310] Step = 35700 ; steps/s = 1.62, tokens/s = 82440 (39062 source, 43378 target) ; Learning rate = 0.000468 ; Loss = 1.539323\n",
      "2024-12-01 22:37:20.469000: I runner.py:310] Step = 35800 ; steps/s = 1.62, tokens/s = 82485 (39120 source, 43365 target) ; Learning rate = 0.000467 ; Loss = 1.551533\n",
      "2024-12-01 22:38:21.993000: I runner.py:310] Step = 35900 ; steps/s = 1.63, tokens/s = 82600 (39156 source, 43444 target) ; Learning rate = 0.000466 ; Loss = 1.557690\n",
      "2024-12-01 22:39:22.940000: I runner.py:310] Step = 36000 ; steps/s = 1.64, tokens/s = 81779 (38778 source, 43001 target) ; Learning rate = 0.000466 ; Loss = 1.539667\n",
      "2024-12-01 22:40:24.500000: I runner.py:310] Step = 36100 ; steps/s = 1.62, tokens/s = 82551 (39129 source, 43422 target) ; Learning rate = 0.000465 ; Loss = 1.539671\n",
      "2024-12-01 22:41:26.010000: I runner.py:310] Step = 36200 ; steps/s = 1.63, tokens/s = 82585 (39162 source, 43423 target) ; Learning rate = 0.000465 ; Loss = 1.552532\n",
      "2024-12-01 22:42:27.549000: I runner.py:310] Step = 36300 ; steps/s = 1.63, tokens/s = 82549 (39132 source, 43417 target) ; Learning rate = 0.000464 ; Loss = 1.548587\n",
      "2024-12-01 22:43:28.487000: I runner.py:310] Step = 36400 ; steps/s = 1.64, tokens/s = 81772 (38768 source, 43004 target) ; Learning rate = 0.000463 ; Loss = 1.542975\n",
      "2024-12-01 22:44:29.976000: I runner.py:310] Step = 36500 ; steps/s = 1.63, tokens/s = 82586 (39142 source, 43444 target) ; Learning rate = 0.000463 ; Loss = 1.536794\n",
      "2024-12-01 22:45:31.452000: I runner.py:310] Step = 36600 ; steps/s = 1.63, tokens/s = 82685 (39189 source, 43496 target) ; Learning rate = 0.000462 ; Loss = 1.542031\n",
      "2024-12-01 22:46:32.934000: I runner.py:310] Step = 36700 ; steps/s = 1.63, tokens/s = 82629 (39188 source, 43441 target) ; Learning rate = 0.000461 ; Loss = 1.558136\n",
      "2024-12-01 22:47:33.942000: I runner.py:310] Step = 36800 ; steps/s = 1.64, tokens/s = 81682 (38733 source, 42949 target) ; Learning rate = 0.000461 ; Loss = 1.548970\n",
      "2024-12-01 22:48:35.485000: I runner.py:310] Step = 36900 ; steps/s = 1.63, tokens/s = 82570 (39148 source, 43422 target) ; Learning rate = 0.000460 ; Loss = 1.544394\n",
      "2024-12-01 22:49:37.051000: I runner.py:310] Step = 37000 ; steps/s = 1.62, tokens/s = 82518 (39128 source, 43390 target) ; Learning rate = 0.000460 ; Loss = 1.546592\n",
      "2024-12-01 22:50:38.594000: I runner.py:310] Step = 37100 ; steps/s = 1.63, tokens/s = 82522 (39128 source, 43394 target) ; Learning rate = 0.000459 ; Loss = 1.549699\n",
      "2024-12-01 22:51:39.545000: I runner.py:310] Step = 37200 ; steps/s = 1.64, tokens/s = 81768 (38764 source, 43004 target) ; Learning rate = 0.000458 ; Loss = 1.531182\n",
      "2024-12-01 22:52:41.082000: I runner.py:310] Step = 37300 ; steps/s = 1.63, tokens/s = 82509 (39087 source, 43422 target) ; Learning rate = 0.000458 ; Loss = 1.546067\n",
      "2024-12-01 22:53:42.637000: I runner.py:310] Step = 37400 ; steps/s = 1.62, tokens/s = 82549 (39129 source, 43420 target) ; Learning rate = 0.000457 ; Loss = 1.549566\n",
      "2024-12-01 22:54:44.217000: I runner.py:310] Step = 37500 ; steps/s = 1.62, tokens/s = 82493 (39129 source, 43364 target) ; Learning rate = 0.000456 ; Loss = 1.538727\n",
      "2024-12-01 22:55:45.157000: I runner.py:310] Step = 37600 ; steps/s = 1.64, tokens/s = 81801 (38792 source, 43009 target) ; Learning rate = 0.000456 ; Loss = 1.539503\n",
      "2024-12-01 22:56:46.675000: I runner.py:310] Step = 37700 ; steps/s = 1.63, tokens/s = 82571 (39142 source, 43429 target) ; Learning rate = 0.000455 ; Loss = 1.540281\n",
      "2024-12-01 22:57:48.168000: I runner.py:310] Step = 37800 ; steps/s = 1.63, tokens/s = 82624 (39178 source, 43446 target) ; Learning rate = 0.000455 ; Loss = 1.548196\n",
      "2024-12-01 22:58:49.703000: I runner.py:310] Step = 37900 ; steps/s = 1.63, tokens/s = 82554 (39132 source, 43422 target) ; Learning rate = 0.000454 ; Loss = 1.540443\n",
      "2024-12-01 22:59:50.610000: I runner.py:310] Step = 38000 ; steps/s = 1.64, tokens/s = 81793 (38766 source, 43027 target) ; Learning rate = 0.000453 ; Loss = 1.541112\n",
      "2024-12-01 23:00:52.093000: I runner.py:310] Step = 38100 ; steps/s = 1.63, tokens/s = 82637 (39178 source, 43459 target) ; Learning rate = 0.000453 ; Loss = 1.539955\n",
      "2024-12-01 23:01:53.595000: I runner.py:310] Step = 38200 ; steps/s = 1.63, tokens/s = 82575 (39162 source, 43413 target) ; Learning rate = 0.000452 ; Loss = 1.546858\n",
      "2024-12-01 23:02:55.101000: I runner.py:310] Step = 38300 ; steps/s = 1.63, tokens/s = 82616 (39172 source, 43444 target) ; Learning rate = 0.000452 ; Loss = 1.545263\n",
      "2024-12-01 23:03:56.047000: I runner.py:310] Step = 38400 ; steps/s = 1.64, tokens/s = 81779 (38750 source, 43029 target) ; Learning rate = 0.000451 ; Loss = 1.543635\n",
      "2024-12-01 23:04:57.554000: I runner.py:310] Step = 38500 ; steps/s = 1.63, tokens/s = 82623 (39186 source, 43437 target) ; Learning rate = 0.000450 ; Loss = 1.535636\n",
      "2024-12-01 23:05:59.098000: I runner.py:310] Step = 38600 ; steps/s = 1.63, tokens/s = 82530 (39136 source, 43394 target) ; Learning rate = 0.000450 ; Loss = 1.538028\n",
      "2024-12-01 23:07:00.645000: I runner.py:310] Step = 38700 ; steps/s = 1.62, tokens/s = 82547 (39132 source, 43415 target) ; Learning rate = 0.000449 ; Loss = 1.536826\n",
      "2024-12-01 23:08:01.639000: I runner.py:310] Step = 38800 ; steps/s = 1.64, tokens/s = 81663 (38692 source, 42971 target) ; Learning rate = 0.000449 ; Loss = 1.525764\n",
      "2024-12-01 23:09:03.166000: I runner.py:310] Step = 38900 ; steps/s = 1.63, tokens/s = 82569 (39157 source, 43412 target) ; Learning rate = 0.000448 ; Loss = 1.542615\n",
      "2024-12-01 23:10:04.670000: I runner.py:310] Step = 39000 ; steps/s = 1.63, tokens/s = 82616 (39202 source, 43414 target) ; Learning rate = 0.000448 ; Loss = 1.550745\n",
      "2024-12-01 23:11:05.686000: I runner.py:310] Step = 39100 ; steps/s = 1.64, tokens/s = 81661 (38700 source, 42961 target) ; Learning rate = 0.000447 ; Loss = 1.535458\n",
      "2024-12-01 23:12:07.266000: I runner.py:310] Step = 39200 ; steps/s = 1.62, tokens/s = 82511 (39117 source, 43394 target) ; Learning rate = 0.000446 ; Loss = 1.539751\n",
      "2024-12-01 23:13:08.860000: I runner.py:310] Step = 39300 ; steps/s = 1.62, tokens/s = 82487 (39126 source, 43361 target) ; Learning rate = 0.000446 ; Loss = 1.526206\n",
      "2024-12-01 23:14:10.349000: I runner.py:310] Step = 39400 ; steps/s = 1.63, tokens/s = 82635 (39160 source, 43475 target) ; Learning rate = 0.000445 ; Loss = 1.539813\n",
      "2024-12-01 23:15:11.329000: I runner.py:310] Step = 39500 ; steps/s = 1.64, tokens/s = 81700 (38720 source, 42980 target) ; Learning rate = 0.000445 ; Loss = 1.538734\n",
      "2024-12-01 23:16:12.878000: I runner.py:310] Step = 39600 ; steps/s = 1.62, tokens/s = 82561 (39126 source, 43435 target) ; Learning rate = 0.000444 ; Loss = 1.541175\n",
      "2024-12-01 23:17:14.337000: I runner.py:310] Step = 39700 ; steps/s = 1.63, tokens/s = 82661 (39183 source, 43478 target) ; Learning rate = 0.000444 ; Loss = 1.538623\n",
      "2024-12-01 23:18:15.884000: I runner.py:310] Step = 39800 ; steps/s = 1.62, tokens/s = 82525 (39131 source, 43394 target) ; Learning rate = 0.000443 ; Loss = 1.528605\n",
      "2024-12-01 23:19:16.878000: I runner.py:310] Step = 39900 ; steps/s = 1.64, tokens/s = 81698 (38752 source, 42946 target) ; Learning rate = 0.000442 ; Loss = 1.537179\n",
      "2024-12-01 23:20:18.415000: I runner.py:310] Step = 40000 ; steps/s = 1.63, tokens/s = 82545 (39107 source, 43438 target) ; Learning rate = 0.000442 ; Loss = 1.532673\n",
      "2024-12-01 23:20:20.073000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-40000\n",
      "2024-12-01 23:20:20.073000: I training.py:192] Running evaluation for step 40000\n",
      "2024-12-01 23:24:30.132000: I training.py:192] Evaluation result for step 40000: loss = 1.184828 ; perplexity = 3.270124\n",
      "2024-12-01 23:25:31.547000: I runner.py:310] Step = 40100 ; steps/s = 1.63, tokens/s = 82763 (39243 source, 43520 target) ; Learning rate = 0.000441 ; Loss = 1.528785\n",
      "2024-12-01 23:26:33.073000: I runner.py:310] Step = 40200 ; steps/s = 1.63, tokens/s = 82549 (39141 source, 43408 target) ; Learning rate = 0.000441 ; Loss = 1.534007\n",
      "2024-12-01 23:27:34.059000: I runner.py:310] Step = 40300 ; steps/s = 1.64, tokens/s = 81721 (38736 source, 42985 target) ; Learning rate = 0.000440 ; Loss = 1.537115\n",
      "2024-12-01 23:28:35.680000: I runner.py:310] Step = 40400 ; steps/s = 1.62, tokens/s = 82455 (39120 source, 43335 target) ; Learning rate = 0.000440 ; Loss = 1.526838\n",
      "2024-12-01 23:29:37.176000: I runner.py:310] Step = 40500 ; steps/s = 1.63, tokens/s = 82562 (39133 source, 43429 target) ; Learning rate = 0.000439 ; Loss = 1.536011\n",
      "2024-12-01 23:30:38.694000: I runner.py:310] Step = 40600 ; steps/s = 1.63, tokens/s = 82601 (39169 source, 43432 target) ; Learning rate = 0.000439 ; Loss = 1.531533\n",
      "2024-12-01 23:31:39.698000: I runner.py:310] Step = 40700 ; steps/s = 1.64, tokens/s = 81708 (38726 source, 42982 target) ; Learning rate = 0.000438 ; Loss = 1.542581\n",
      "2024-12-01 23:32:41.212000: I runner.py:310] Step = 40800 ; steps/s = 1.63, tokens/s = 82573 (39173 source, 43400 target) ; Learning rate = 0.000438 ; Loss = 1.527014\n",
      "2024-12-01 23:33:42.711000: I runner.py:310] Step = 40900 ; steps/s = 1.63, tokens/s = 82604 (39147 source, 43457 target) ; Learning rate = 0.000437 ; Loss = 1.529908\n",
      "2024-12-01 23:34:44.265000: I runner.py:310] Step = 41000 ; steps/s = 1.62, tokens/s = 82553 (39118 source, 43435 target) ; Learning rate = 0.000437 ; Loss = 1.537683\n",
      "2024-12-01 23:35:45.193000: I runner.py:310] Step = 41100 ; steps/s = 1.64, tokens/s = 81814 (38799 source, 43015 target) ; Learning rate = 0.000436 ; Loss = 1.532496\n",
      "2024-12-01 23:36:46.680000: I runner.py:310] Step = 41200 ; steps/s = 1.63, tokens/s = 82607 (39162 source, 43445 target) ; Learning rate = 0.000435 ; Loss = 1.530951\n",
      "2024-12-01 23:37:48.217000: I runner.py:310] Step = 41300 ; steps/s = 1.63, tokens/s = 82541 (39132 source, 43409 target) ; Learning rate = 0.000435 ; Loss = 1.527833\n",
      "2024-12-01 23:38:49.781000: I runner.py:310] Step = 41400 ; steps/s = 1.62, tokens/s = 82514 (39127 source, 43387 target) ; Learning rate = 0.000434 ; Loss = 1.538679\n",
      "2024-12-01 23:39:50.751000: I runner.py:310] Step = 41500 ; steps/s = 1.64, tokens/s = 81723 (38713 source, 43010 target) ; Learning rate = 0.000434 ; Loss = 1.531963\n",
      "2024-12-01 23:40:52.263000: I runner.py:310] Step = 41600 ; steps/s = 1.63, tokens/s = 82600 (39162 source, 43438 target) ; Learning rate = 0.000433 ; Loss = 1.534870\n",
      "2024-12-01 23:41:53.808000: I runner.py:310] Step = 41700 ; steps/s = 1.63, tokens/s = 82554 (39137 source, 43417 target) ; Learning rate = 0.000433 ; Loss = 1.527673\n",
      "2024-12-01 23:42:55.297000: I runner.py:310] Step = 41800 ; steps/s = 1.63, tokens/s = 82607 (39179 source, 43428 target) ; Learning rate = 0.000432 ; Loss = 1.531853\n",
      "2024-12-01 23:43:56.226000: I runner.py:310] Step = 41900 ; steps/s = 1.64, tokens/s = 81802 (38785 source, 43017 target) ; Learning rate = 0.000432 ; Loss = 1.519136\n",
      "2024-12-01 23:44:57.852000: I runner.py:310] Step = 42000 ; steps/s = 1.62, tokens/s = 82428 (39112 source, 43316 target) ; Learning rate = 0.000431 ; Loss = 1.527175\n",
      "2024-12-01 23:45:59.426000: I runner.py:310] Step = 42100 ; steps/s = 1.62, tokens/s = 82541 (39146 source, 43395 target) ; Learning rate = 0.000431 ; Loss = 1.533008\n",
      "2024-12-01 23:47:00.965000: I runner.py:310] Step = 42200 ; steps/s = 1.63, tokens/s = 82509 (39079 source, 43430 target) ; Learning rate = 0.000430 ; Loss = 1.535404\n",
      "2024-12-01 23:48:02.007000: I runner.py:310] Step = 42300 ; steps/s = 1.64, tokens/s = 81664 (38707 source, 42957 target) ; Learning rate = 0.000430 ; Loss = 1.530842\n",
      "2024-12-01 23:49:03.561000: I runner.py:310] Step = 42400 ; steps/s = 1.62, tokens/s = 82546 (39136 source, 43410 target) ; Learning rate = 0.000429 ; Loss = 1.527321\n",
      "2024-12-01 23:50:05.094000: I runner.py:310] Step = 42500 ; steps/s = 1.63, tokens/s = 82570 (39180 source, 43390 target) ; Learning rate = 0.000429 ; Loss = 1.527802\n",
      "2024-12-01 23:51:06.654000: I runner.py:310] Step = 42600 ; steps/s = 1.62, tokens/s = 82495 (39086 source, 43409 target) ; Learning rate = 0.000428 ; Loss = 1.526397\n",
      "2024-12-01 23:52:07.579000: I runner.py:310] Step = 42700 ; steps/s = 1.64, tokens/s = 81762 (38774 source, 42988 target) ; Learning rate = 0.000428 ; Loss = 1.531003\n",
      "2024-12-01 23:53:09.099000: I runner.py:310] Step = 42800 ; steps/s = 1.63, tokens/s = 82603 (39177 source, 43426 target) ; Learning rate = 0.000427 ; Loss = 1.527666\n",
      "2024-12-01 23:54:10.616000: I runner.py:310] Step = 42900 ; steps/s = 1.63, tokens/s = 82605 (39135 source, 43470 target) ; Learning rate = 0.000427 ; Loss = 1.526595\n",
      "2024-12-01 23:55:12.121000: I runner.py:310] Step = 43000 ; steps/s = 1.63, tokens/s = 82584 (39160 source, 43424 target) ; Learning rate = 0.000426 ; Loss = 1.529626\n",
      "2024-12-01 23:56:13.062000: I runner.py:310] Step = 43100 ; steps/s = 1.64, tokens/s = 81772 (38736 source, 43036 target) ; Learning rate = 0.000426 ; Loss = 1.524461\n",
      "2024-12-01 23:57:14.545000: I runner.py:310] Step = 43200 ; steps/s = 1.63, tokens/s = 82623 (39170 source, 43453 target) ; Learning rate = 0.000425 ; Loss = 1.523017\n",
      "2024-12-01 23:58:16.066000: I runner.py:310] Step = 43300 ; steps/s = 1.63, tokens/s = 82601 (39158 source, 43443 target) ; Learning rate = 0.000425 ; Loss = 1.527741\n",
      "2024-12-01 23:59:17.195000: I runner.py:310] Step = 43400 ; steps/s = 1.64, tokens/s = 82033 (38925 source, 43108 target) ; Learning rate = 0.000424 ; Loss = 1.527025\n",
      "2024-12-02 00:00:18.463000: I runner.py:310] Step = 43500 ; steps/s = 1.63, tokens/s = 82397 (39057 source, 43340 target) ; Learning rate = 0.000424 ; Loss = 1.525348\n",
      "2024-12-02 00:01:20.033000: I runner.py:310] Step = 43600 ; steps/s = 1.62, tokens/s = 82502 (39111 source, 43391 target) ; Learning rate = 0.000423 ; Loss = 1.511569\n",
      "2024-12-02 00:02:21.480000: I runner.py:310] Step = 43700 ; steps/s = 1.63, tokens/s = 82710 (39215 source, 43495 target) ; Learning rate = 0.000423 ; Loss = 1.519159\n",
      "2024-12-02 00:03:22.448000: I runner.py:310] Step = 43800 ; steps/s = 1.64, tokens/s = 81705 (38742 source, 42963 target) ; Learning rate = 0.000422 ; Loss = 1.526394\n",
      "2024-12-02 00:04:23.962000: I runner.py:310] Step = 43900 ; steps/s = 1.63, tokens/s = 82599 (39131 source, 43468 target) ; Learning rate = 0.000422 ; Loss = 1.519536\n",
      "2024-12-02 00:05:25.466000: I runner.py:310] Step = 44000 ; steps/s = 1.63, tokens/s = 82593 (39165 source, 43428 target) ; Learning rate = 0.000421 ; Loss = 1.523947\n",
      "2024-12-02 00:06:26.929000: I runner.py:310] Step = 44100 ; steps/s = 1.63, tokens/s = 82648 (39191 source, 43457 target) ; Learning rate = 0.000421 ; Loss = 1.543440\n",
      "2024-12-02 00:07:27.878000: I runner.py:310] Step = 44200 ; steps/s = 1.64, tokens/s = 81768 (38766 source, 43002 target) ; Learning rate = 0.000420 ; Loss = 1.523899\n",
      "2024-12-02 00:08:29.473000: I runner.py:310] Step = 44300 ; steps/s = 1.62, tokens/s = 82438 (39064 source, 43374 target) ; Learning rate = 0.000420 ; Loss = 1.514740\n",
      "2024-12-02 00:09:31.054000: I runner.py:310] Step = 44400 ; steps/s = 1.62, tokens/s = 82531 (39139 source, 43392 target) ; Learning rate = 0.000419 ; Loss = 1.522671\n",
      "2024-12-02 00:10:32.587000: I runner.py:310] Step = 44500 ; steps/s = 1.63, tokens/s = 82579 (39155 source, 43424 target) ; Learning rate = 0.000419 ; Loss = 1.526681\n",
      "2024-12-02 00:11:33.477000: I runner.py:310] Step = 44600 ; steps/s = 1.64, tokens/s = 81835 (38823 source, 43012 target) ; Learning rate = 0.000419 ; Loss = 1.514192\n",
      "2024-12-02 00:12:35.003000: I runner.py:310] Step = 44700 ; steps/s = 1.63, tokens/s = 82573 (39155 source, 43418 target) ; Learning rate = 0.000418 ; Loss = 1.520357\n",
      "2024-12-02 00:13:36.590000: I runner.py:310] Step = 44800 ; steps/s = 1.62, tokens/s = 82503 (39117 source, 43386 target) ; Learning rate = 0.000418 ; Loss = 1.526205\n",
      "2024-12-02 00:14:38.162000: I runner.py:310] Step = 44900 ; steps/s = 1.62, tokens/s = 82494 (39098 source, 43396 target) ; Learning rate = 0.000417 ; Loss = 1.526332\n",
      "2024-12-02 00:15:39.078000: I runner.py:310] Step = 45000 ; steps/s = 1.64, tokens/s = 81808 (38764 source, 43044 target) ; Learning rate = 0.000417 ; Loss = 1.522732\n",
      "2024-12-02 00:15:39.080000: I training.py:192] Running evaluation for step 45000\n",
      "2024-12-02 00:19:53.521000: I training.py:192] Evaluation result for step 45000: loss = 1.199163 ; perplexity = 3.317339\n",
      "2024-12-02 00:20:54.915000: I runner.py:310] Step = 45100 ; steps/s = 1.63, tokens/s = 82800 (39247 source, 43553 target) ; Learning rate = 0.000416 ; Loss = 1.522047\n",
      "2024-12-02 00:21:56.537000: I runner.py:310] Step = 45200 ; steps/s = 1.62, tokens/s = 82429 (39087 source, 43342 target) ; Learning rate = 0.000416 ; Loss = 1.527105\n",
      "2024-12-02 00:22:58.110000: I runner.py:310] Step = 45300 ; steps/s = 1.62, tokens/s = 82513 (39126 source, 43387 target) ; Learning rate = 0.000415 ; Loss = 1.528578\n",
      "2024-12-02 00:23:59.101000: I runner.py:310] Step = 45400 ; steps/s = 1.64, tokens/s = 81673 (38705 source, 42968 target) ; Learning rate = 0.000415 ; Loss = 1.521100\n",
      "2024-12-02 00:25:00.670000: I runner.py:310] Step = 45500 ; steps/s = 1.62, tokens/s = 82535 (39124 source, 43411 target) ; Learning rate = 0.000414 ; Loss = 1.520754\n",
      "2024-12-02 00:26:02.258000: I runner.py:310] Step = 45600 ; steps/s = 1.62, tokens/s = 82488 (39122 source, 43366 target) ; Learning rate = 0.000414 ; Loss = 1.521875\n",
      "2024-12-02 00:27:03.773000: I runner.py:310] Step = 45700 ; steps/s = 1.63, tokens/s = 82583 (39142 source, 43441 target) ; Learning rate = 0.000413 ; Loss = 1.520148\n",
      "2024-12-02 00:28:04.797000: I runner.py:310] Step = 45800 ; steps/s = 1.64, tokens/s = 81651 (38736 source, 42915 target) ; Learning rate = 0.000413 ; Loss = 1.531186\n",
      "2024-12-02 00:29:06.376000: I runner.py:310] Step = 45900 ; steps/s = 1.62, tokens/s = 82532 (39122 source, 43410 target) ; Learning rate = 0.000413 ; Loss = 1.516122\n",
      "2024-12-02 00:30:07.900000: I runner.py:310] Step = 46000 ; steps/s = 1.63, tokens/s = 82527 (39119 source, 43408 target) ; Learning rate = 0.000412 ; Loss = 1.523164\n",
      "2024-12-02 00:31:09.422000: I runner.py:310] Step = 46100 ; steps/s = 1.63, tokens/s = 82604 (39169 source, 43435 target) ; Learning rate = 0.000412 ; Loss = 1.522943\n",
      "2024-12-02 00:32:10.446000: I runner.py:310] Step = 46200 ; steps/s = 1.64, tokens/s = 81629 (38710 source, 42919 target) ; Learning rate = 0.000411 ; Loss = 1.513282\n",
      "2024-12-02 00:33:11.951000: I runner.py:310] Step = 46300 ; steps/s = 1.63, tokens/s = 82597 (39147 source, 43450 target) ; Learning rate = 0.000411 ; Loss = 1.520043\n",
      "2024-12-02 00:34:13.403000: I runner.py:310] Step = 46400 ; steps/s = 1.63, tokens/s = 82692 (39228 source, 43464 target) ; Learning rate = 0.000410 ; Loss = 1.520717\n",
      "2024-12-02 00:35:14.915000: I runner.py:310] Step = 46500 ; steps/s = 1.63, tokens/s = 82569 (39133 source, 43436 target) ; Learning rate = 0.000410 ; Loss = 1.524709\n",
      "2024-12-02 00:36:15.851000: I runner.py:310] Step = 46600 ; steps/s = 1.64, tokens/s = 81784 (38735 source, 43049 target) ; Learning rate = 0.000409 ; Loss = 1.518027\n",
      "2024-12-02 00:37:17.402000: I runner.py:310] Step = 46700 ; steps/s = 1.62, tokens/s = 82542 (39121 source, 43421 target) ; Learning rate = 0.000409 ; Loss = 1.520180\n",
      "2024-12-02 00:38:18.906000: I runner.py:310] Step = 46800 ; steps/s = 1.63, tokens/s = 82607 (39197 source, 43410 target) ; Learning rate = 0.000409 ; Loss = 1.518152\n",
      "2024-12-02 00:39:20.420000: I runner.py:310] Step = 46900 ; steps/s = 1.63, tokens/s = 82585 (39165 source, 43420 target) ; Learning rate = 0.000408 ; Loss = 1.528534\n",
      "2024-12-02 00:40:21.361000: I runner.py:310] Step = 47000 ; steps/s = 1.64, tokens/s = 81778 (38759 source, 43019 target) ; Learning rate = 0.000408 ; Loss = 1.520251\n",
      "2024-12-02 00:41:22.869000: I runner.py:310] Step = 47100 ; steps/s = 1.63, tokens/s = 82598 (39191 source, 43407 target) ; Learning rate = 0.000407 ; Loss = 1.520374\n",
      "2024-12-02 00:42:24.436000: I runner.py:310] Step = 47200 ; steps/s = 1.62, tokens/s = 82506 (39087 source, 43419 target) ; Learning rate = 0.000407 ; Loss = 1.520199\n",
      "2024-12-02 00:43:25.973000: I runner.py:310] Step = 47300 ; steps/s = 1.63, tokens/s = 82543 (39135 source, 43408 target) ; Learning rate = 0.000406 ; Loss = 1.525672\n",
      "2024-12-02 00:44:26.949000: I runner.py:310] Step = 47400 ; steps/s = 1.64, tokens/s = 81730 (38737 source, 42993 target) ; Learning rate = 0.000406 ; Loss = 1.509967\n",
      "2024-12-02 00:45:28.540000: I runner.py:310] Step = 47500 ; steps/s = 1.62, tokens/s = 82505 (39116 source, 43389 target) ; Learning rate = 0.000406 ; Loss = 1.521729\n",
      "2024-12-02 00:46:30.086000: I runner.py:310] Step = 47600 ; steps/s = 1.63, tokens/s = 82542 (39141 source, 43401 target) ; Learning rate = 0.000405 ; Loss = 1.521619\n",
      "2024-12-02 00:47:31.450000: I runner.py:310] Step = 47700 ; steps/s = 1.63, tokens/s = 82168 (38958 source, 43210 target) ; Learning rate = 0.000405 ; Loss = 1.525197\n",
      "2024-12-02 00:48:32.563000: I runner.py:310] Step = 47800 ; steps/s = 1.64, tokens/s = 82189 (38962 source, 43227 target) ; Learning rate = 0.000404 ; Loss = 1.519543\n",
      "2024-12-02 00:49:34.082000: I runner.py:310] Step = 47900 ; steps/s = 1.63, tokens/s = 82561 (39140 source, 43421 target) ; Learning rate = 0.000404 ; Loss = 1.517232\n",
      "2024-12-02 00:50:35.618000: I runner.py:310] Step = 48000 ; steps/s = 1.63, tokens/s = 82574 (39149 source, 43425 target) ; Learning rate = 0.000403 ; Loss = 1.514557\n",
      "2024-12-02 00:51:36.581000: I runner.py:310] Step = 48100 ; steps/s = 1.64, tokens/s = 81710 (38745 source, 42965 target) ; Learning rate = 0.000403 ; Loss = 1.515941\n",
      "2024-12-02 00:52:38.107000: I runner.py:310] Step = 48200 ; steps/s = 1.63, tokens/s = 82591 (39151 source, 43440 target) ; Learning rate = 0.000403 ; Loss = 1.512499\n",
      "2024-12-02 00:53:39.623000: I runner.py:310] Step = 48300 ; steps/s = 1.63, tokens/s = 82529 (39122 source, 43407 target) ; Learning rate = 0.000402 ; Loss = 1.518411\n",
      "2024-12-02 00:54:41.093000: I runner.py:310] Step = 48400 ; steps/s = 1.63, tokens/s = 82658 (39192 source, 43466 target) ; Learning rate = 0.000402 ; Loss = 1.512251\n",
      "2024-12-02 00:55:42.051000: I runner.py:310] Step = 48500 ; steps/s = 1.64, tokens/s = 81755 (38764 source, 42991 target) ; Learning rate = 0.000401 ; Loss = 1.517100\n",
      "2024-12-02 00:56:43.671000: I runner.py:310] Step = 48600 ; steps/s = 1.62, tokens/s = 82471 (39070 source, 43401 target) ; Learning rate = 0.000401 ; Loss = 1.514615\n",
      "2024-12-02 00:57:45.207000: I runner.py:310] Step = 48700 ; steps/s = 1.63, tokens/s = 82600 (39168 source, 43432 target) ; Learning rate = 0.000401 ; Loss = 1.514723\n",
      "2024-12-02 00:58:46.728000: I runner.py:310] Step = 48800 ; steps/s = 1.63, tokens/s = 82564 (39160 source, 43404 target) ; Learning rate = 0.000400 ; Loss = 1.516602\n",
      "2024-12-02 00:59:47.781000: I runner.py:310] Step = 48900 ; steps/s = 1.64, tokens/s = 81585 (38677 source, 42908 target) ; Learning rate = 0.000400 ; Loss = 1.512088\n",
      "2024-12-02 01:00:49.386000: I runner.py:310] Step = 49000 ; steps/s = 1.62, tokens/s = 82402 (39067 source, 43335 target) ; Learning rate = 0.000399 ; Loss = 1.510680\n",
      "2024-12-02 01:01:50.841000: I runner.py:310] Step = 49100 ; steps/s = 1.63, tokens/s = 82729 (39211 source, 43518 target) ; Learning rate = 0.000399 ; Loss = 1.523113\n",
      "2024-12-02 01:02:52.296000: I runner.py:310] Step = 49200 ; steps/s = 1.63, tokens/s = 82650 (39188 source, 43462 target) ; Learning rate = 0.000398 ; Loss = 1.517717\n",
      "2024-12-02 01:03:53.236000: I runner.py:310] Step = 49300 ; steps/s = 1.64, tokens/s = 81781 (38779 source, 43002 target) ; Learning rate = 0.000398 ; Loss = 1.510139\n",
      "2024-12-02 01:04:54.788000: I runner.py:310] Step = 49400 ; steps/s = 1.62, tokens/s = 82544 (39146 source, 43398 target) ; Learning rate = 0.000398 ; Loss = 1.510285\n",
      "2024-12-02 01:05:56.337000: I runner.py:310] Step = 49500 ; steps/s = 1.63, tokens/s = 82538 (39126 source, 43412 target) ; Learning rate = 0.000397 ; Loss = 1.523895\n",
      "2024-12-02 01:06:57.896000: I runner.py:310] Step = 49600 ; steps/s = 1.62, tokens/s = 82538 (39144 source, 43394 target) ; Learning rate = 0.000397 ; Loss = 1.519038\n",
      "2024-12-02 01:07:58.822000: I runner.py:310] Step = 49700 ; steps/s = 1.64, tokens/s = 81805 (38761 source, 43044 target) ; Learning rate = 0.000396 ; Loss = 1.517929\n",
      "2024-12-02 01:09:00.373000: I runner.py:310] Step = 49800 ; steps/s = 1.62, tokens/s = 82510 (39124 source, 43386 target) ; Learning rate = 0.000396 ; Loss = 1.509654\n",
      "2024-12-02 01:10:01.822000: I runner.py:310] Step = 49900 ; steps/s = 1.63, tokens/s = 82693 (39199 source, 43494 target) ; Learning rate = 0.000396 ; Loss = 1.506975\n",
      "2024-12-02 01:11:03.362000: I runner.py:310] Step = 50000 ; steps/s = 1.63, tokens/s = 82541 (39139 source, 43402 target) ; Learning rate = 0.000395 ; Loss = 1.520868\n",
      "2024-12-02 01:11:04.972000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-50000\n",
      "2024-12-02 01:11:04.972000: I training.py:192] Running evaluation for step 50000\n",
      "2024-12-02 01:15:20.944000: I training.py:192] Evaluation result for step 50000: loss = 1.205271 ; perplexity = 3.337664\n",
      "2024-12-02 01:16:21.782000: I runner.py:310] Step = 50100 ; steps/s = 1.64, tokens/s = 81914 (38817 source, 43097 target) ; Learning rate = 0.000395 ; Loss = 1.516897\n",
      "2024-12-02 01:17:23.326000: I runner.py:310] Step = 50200 ; steps/s = 1.63, tokens/s = 82559 (39131 source, 43428 target) ; Learning rate = 0.000394 ; Loss = 1.506918\n",
      "2024-12-02 01:18:24.939000: I runner.py:310] Step = 50300 ; steps/s = 1.62, tokens/s = 82479 (39129 source, 43350 target) ; Learning rate = 0.000394 ; Loss = 1.507940\n",
      "2024-12-02 01:19:26.579000: I runner.py:310] Step = 50400 ; steps/s = 1.62, tokens/s = 82407 (39056 source, 43351 target) ; Learning rate = 0.000394 ; Loss = 1.509623\n",
      "2024-12-02 01:20:27.593000: I runner.py:310] Step = 50500 ; steps/s = 1.64, tokens/s = 81665 (38718 source, 42947 target) ; Learning rate = 0.000393 ; Loss = 1.517373\n",
      "2024-12-02 01:21:29.189000: I runner.py:310] Step = 50600 ; steps/s = 1.62, tokens/s = 82477 (39098 source, 43379 target) ; Learning rate = 0.000393 ; Loss = 1.510612\n",
      "2024-12-02 01:22:30.767000: I runner.py:310] Step = 50700 ; steps/s = 1.62, tokens/s = 82509 (39134 source, 43375 target) ; Learning rate = 0.000393 ; Loss = 1.505946\n",
      "2024-12-02 01:23:32.304000: I runner.py:310] Step = 50800 ; steps/s = 1.63, tokens/s = 82564 (39140 source, 43424 target) ; Learning rate = 0.000392 ; Loss = 1.514973\n",
      "2024-12-02 01:24:33.322000: I runner.py:310] Step = 50900 ; steps/s = 1.64, tokens/s = 81664 (38705 source, 42959 target) ; Learning rate = 0.000392 ; Loss = 1.517856\n",
      "2024-12-02 01:25:34.964000: I runner.py:310] Step = 51000 ; steps/s = 1.62, tokens/s = 82439 (39069 source, 43370 target) ; Learning rate = 0.000391 ; Loss = 1.509830\n",
      "2024-12-02 01:26:36.504000: I runner.py:310] Step = 51100 ; steps/s = 1.63, tokens/s = 82543 (39139 source, 43404 target) ; Learning rate = 0.000391 ; Loss = 1.519303\n",
      "2024-12-02 01:27:38.034000: I runner.py:310] Step = 51200 ; steps/s = 1.63, tokens/s = 82534 (39148 source, 43386 target) ; Learning rate = 0.000391 ; Loss = 1.515939\n",
      "2024-12-02 01:28:39.033000: I runner.py:310] Step = 51300 ; steps/s = 1.64, tokens/s = 81717 (38734 source, 42983 target) ; Learning rate = 0.000390 ; Loss = 1.520490\n",
      "2024-12-02 01:29:40.563000: I runner.py:310] Step = 51400 ; steps/s = 1.63, tokens/s = 82559 (39135 source, 43424 target) ; Learning rate = 0.000390 ; Loss = 1.508076\n",
      "2024-12-02 01:30:42.132000: I runner.py:310] Step = 51500 ; steps/s = 1.62, tokens/s = 82502 (39136 source, 43366 target) ; Learning rate = 0.000389 ; Loss = 1.507359\n",
      "2024-12-02 01:31:43.723000: I runner.py:310] Step = 51600 ; steps/s = 1.62, tokens/s = 82493 (39114 source, 43379 target) ; Learning rate = 0.000389 ; Loss = 1.514022\n",
      "2024-12-02 01:32:44.624000: I runner.py:310] Step = 51700 ; steps/s = 1.64, tokens/s = 81817 (38765 source, 43052 target) ; Learning rate = 0.000389 ; Loss = 1.514141\n",
      "2024-12-02 01:33:46.151000: I runner.py:310] Step = 51800 ; steps/s = 1.63, tokens/s = 82599 (39170 source, 43429 target) ; Learning rate = 0.000388 ; Loss = 1.505403\n",
      "2024-12-02 01:34:47.674000: I runner.py:310] Step = 51900 ; steps/s = 1.63, tokens/s = 82555 (39126 source, 43429 target) ; Learning rate = 0.000388 ; Loss = 1.510651\n",
      "2024-12-02 01:35:49.139000: I runner.py:310] Step = 52000 ; steps/s = 1.63, tokens/s = 82423 (39096 source, 43327 target) ; Learning rate = 0.000388 ; Loss = 1.528560\n",
      "2024-12-02 01:36:50.139000: I runner.py:310] Step = 52100 ; steps/s = 1.64, tokens/s = 81895 (38816 source, 43079 target) ; Learning rate = 0.000387 ; Loss = 1.516829\n",
      "2024-12-02 01:37:51.642000: I runner.py:310] Step = 52200 ; steps/s = 1.63, tokens/s = 82623 (39192 source, 43431 target) ; Learning rate = 0.000387 ; Loss = 1.508339\n",
      "2024-12-02 01:38:53.257000: I runner.py:310] Step = 52300 ; steps/s = 1.62, tokens/s = 82482 (39093 source, 43389 target) ; Learning rate = 0.000386 ; Loss = 1.510817\n",
      "2024-12-02 01:39:54.254000: I runner.py:310] Step = 52400 ; steps/s = 1.64, tokens/s = 81674 (38724 source, 42950 target) ; Learning rate = 0.000386 ; Loss = 1.507004\n",
      "2024-12-02 01:40:55.775000: I runner.py:310] Step = 52500 ; steps/s = 1.63, tokens/s = 82627 (39164 source, 43463 target) ; Learning rate = 0.000386 ; Loss = 1.507986\n",
      "2024-12-02 01:41:57.333000: I runner.py:310] Step = 52600 ; steps/s = 1.62, tokens/s = 82498 (39110 source, 43388 target) ; Learning rate = 0.000385 ; Loss = 1.515686\n",
      "2024-12-02 01:42:58.929000: I runner.py:310] Step = 52700 ; steps/s = 1.62, tokens/s = 82463 (39084 source, 43379 target) ; Learning rate = 0.000385 ; Loss = 1.509933\n",
      "2024-12-02 01:43:59.859000: I runner.py:310] Step = 52800 ; steps/s = 1.64, tokens/s = 81794 (38794 source, 43000 target) ; Learning rate = 0.000385 ; Loss = 1.514052\n",
      "2024-12-02 01:45:01.356000: I runner.py:310] Step = 52900 ; steps/s = 1.63, tokens/s = 82614 (39178 source, 43436 target) ; Learning rate = 0.000384 ; Loss = 1.516298\n",
      "2024-12-02 01:46:02.910000: I runner.py:310] Step = 53000 ; steps/s = 1.62, tokens/s = 82556 (39154 source, 43402 target) ; Learning rate = 0.000384 ; Loss = 1.514438\n",
      "2024-12-02 01:47:04.515000: I runner.py:310] Step = 53100 ; steps/s = 1.62, tokens/s = 82472 (39082 source, 43390 target) ; Learning rate = 0.000384 ; Loss = 1.511701\n",
      "2024-12-02 01:48:05.550000: I runner.py:310] Step = 53200 ; steps/s = 1.64, tokens/s = 81624 (38685 source, 42939 target) ; Learning rate = 0.000383 ; Loss = 1.509787\n",
      "2024-12-02 01:49:07.087000: I runner.py:310] Step = 53300 ; steps/s = 1.63, tokens/s = 82548 (39136 source, 43412 target) ; Learning rate = 0.000383 ; Loss = 1.512553\n",
      "2024-12-02 01:50:08.571000: I runner.py:310] Step = 53400 ; steps/s = 1.63, tokens/s = 82644 (39173 source, 43471 target) ; Learning rate = 0.000382 ; Loss = 1.507967\n",
      "2024-12-02 01:51:10.151000: I runner.py:310] Step = 53500 ; steps/s = 1.62, tokens/s = 82503 (39118 source, 43385 target) ; Learning rate = 0.000382 ; Loss = 1.505867\n",
      "2024-12-02 01:52:11.159000: I runner.py:310] Step = 53600 ; steps/s = 1.64, tokens/s = 81680 (38734 source, 42946 target) ; Learning rate = 0.000382 ; Loss = 1.510938\n",
      "2024-12-02 01:53:12.730000: I runner.py:310] Step = 53700 ; steps/s = 1.62, tokens/s = 82484 (39119 source, 43365 target) ; Learning rate = 0.000381 ; Loss = 1.511736\n",
      "2024-12-02 01:54:14.218000: I runner.py:310] Step = 53800 ; steps/s = 1.63, tokens/s = 82653 (39174 source, 43479 target) ; Learning rate = 0.000381 ; Loss = 1.512049\n",
      "2024-12-02 01:55:15.794000: I runner.py:310] Step = 53900 ; steps/s = 1.62, tokens/s = 82481 (39112 source, 43369 target) ; Learning rate = 0.000381 ; Loss = 1.507751\n",
      "2024-12-02 01:56:16.818000: I runner.py:310] Step = 54000 ; steps/s = 1.64, tokens/s = 81690 (38713 source, 42977 target) ; Learning rate = 0.000380 ; Loss = 1.498396\n",
      "2024-12-02 01:57:18.373000: I runner.py:310] Step = 54100 ; steps/s = 1.62, tokens/s = 82514 (39105 source, 43409 target) ; Learning rate = 0.000380 ; Loss = 1.507552\n",
      "2024-12-02 01:58:19.930000: I runner.py:310] Step = 54200 ; steps/s = 1.62, tokens/s = 82545 (39148 source, 43397 target) ; Learning rate = 0.000380 ; Loss = 1.508952\n",
      "2024-12-02 01:59:21.552000: I runner.py:310] Step = 54300 ; steps/s = 1.62, tokens/s = 82427 (39068 source, 43359 target) ; Learning rate = 0.000379 ; Loss = 1.514466\n",
      "2024-12-02 02:00:22.539000: I runner.py:310] Step = 54400 ; steps/s = 1.64, tokens/s = 81690 (38730 source, 42960 target) ; Learning rate = 0.000379 ; Loss = 1.515780\n",
      "2024-12-02 02:01:24.062000: I runner.py:310] Step = 54500 ; steps/s = 1.63, tokens/s = 82608 (39176 source, 43432 target) ; Learning rate = 0.000379 ; Loss = 1.500337\n",
      "2024-12-02 02:02:25.595000: I runner.py:310] Step = 54600 ; steps/s = 1.63, tokens/s = 82533 (39138 source, 43395 target) ; Learning rate = 0.000378 ; Loss = 1.505954\n",
      "2024-12-02 02:03:27.118000: I runner.py:310] Step = 54700 ; steps/s = 1.63, tokens/s = 82589 (39153 source, 43436 target) ; Learning rate = 0.000378 ; Loss = 1.506972\n",
      "2024-12-02 02:04:28.134000: I runner.py:310] Step = 54800 ; steps/s = 1.64, tokens/s = 81674 (38714 source, 42960 target) ; Learning rate = 0.000378 ; Loss = 1.500833\n",
      "2024-12-02 02:05:29.615000: I runner.py:310] Step = 54900 ; steps/s = 1.63, tokens/s = 82638 (39195 source, 43443 target) ; Learning rate = 0.000377 ; Loss = 1.503792\n",
      "2024-12-02 02:06:31.164000: I runner.py:310] Step = 55000 ; steps/s = 1.62, tokens/s = 82545 (39148 source, 43397 target) ; Learning rate = 0.000377 ; Loss = 1.511694\n",
      "2024-12-02 02:06:31.165000: I training.py:192] Running evaluation for step 55000\n",
      "2024-12-02 02:10:38.155000: I training.py:192] Evaluation result for step 55000: loss = 1.215824 ; perplexity = 3.373074\n",
      "2024-12-02 02:11:39.548000: I runner.py:310] Step = 55100 ; steps/s = 1.63, tokens/s = 82764 (39217 source, 43547 target) ; Learning rate = 0.000377 ; Loss = 1.503885\n",
      "2024-12-02 02:12:40.438000: I runner.py:310] Step = 55200 ; steps/s = 1.64, tokens/s = 81856 (38816 source, 43040 target) ; Learning rate = 0.000376 ; Loss = 1.495024\n",
      "2024-12-02 02:13:42.052000: I runner.py:310] Step = 55300 ; steps/s = 1.62, tokens/s = 82451 (39100 source, 43351 target) ; Learning rate = 0.000376 ; Loss = 1.502652\n",
      "2024-12-02 02:14:43.597000: I runner.py:310] Step = 55400 ; steps/s = 1.63, tokens/s = 82547 (39130 source, 43417 target) ; Learning rate = 0.000376 ; Loss = 1.520032\n",
      "2024-12-02 02:15:45.145000: I runner.py:310] Step = 55500 ; steps/s = 1.62, tokens/s = 82542 (39119 source, 43423 target) ; Learning rate = 0.000375 ; Loss = 1.516448\n",
      "2024-12-02 02:16:46.132000: I runner.py:310] Step = 55600 ; steps/s = 1.64, tokens/s = 81719 (38741 source, 42978 target) ; Learning rate = 0.000375 ; Loss = 1.500637\n",
      "2024-12-02 02:17:47.664000: I runner.py:310] Step = 55700 ; steps/s = 1.63, tokens/s = 82556 (39142 source, 43414 target) ; Learning rate = 0.000375 ; Loss = 1.517564\n",
      "2024-12-02 02:18:49.193000: I runner.py:310] Step = 55800 ; steps/s = 1.63, tokens/s = 82570 (39133 source, 43437 target) ; Learning rate = 0.000374 ; Loss = 1.509901\n",
      "2024-12-02 02:19:50.761000: I runner.py:310] Step = 55900 ; steps/s = 1.62, tokens/s = 82501 (39112 source, 43389 target) ; Learning rate = 0.000374 ; Loss = 1.510578\n",
      "2024-12-02 02:20:51.746000: I runner.py:310] Step = 56000 ; steps/s = 1.64, tokens/s = 81701 (38715 source, 42986 target) ; Learning rate = 0.000374 ; Loss = 1.494043\n",
      "2024-12-02 02:21:53.277000: I runner.py:310] Step = 56100 ; steps/s = 1.63, tokens/s = 82568 (39168 source, 43400 target) ; Learning rate = 0.000373 ; Loss = 1.506297\n",
      "2024-12-02 02:22:54.828000: I runner.py:310] Step = 56200 ; steps/s = 1.62, tokens/s = 82544 (39134 source, 43410 target) ; Learning rate = 0.000373 ; Loss = 1.509885\n",
      "2024-12-02 02:23:56.359000: I runner.py:310] Step = 56300 ; steps/s = 1.63, tokens/s = 82556 (39144 source, 43412 target) ; Learning rate = 0.000373 ; Loss = 1.506654\n",
      "2024-12-02 02:24:57.327000: I runner.py:310] Step = 56400 ; steps/s = 1.64, tokens/s = 81751 (38738 source, 43013 target) ; Learning rate = 0.000372 ; Loss = 1.504549\n",
      "2024-12-02 02:25:58.855000: I runner.py:310] Step = 56500 ; steps/s = 1.63, tokens/s = 82554 (39134 source, 43420 target) ; Learning rate = 0.000372 ; Loss = 1.503620\n",
      "2024-12-02 02:27:00.396000: I runner.py:310] Step = 56600 ; steps/s = 1.63, tokens/s = 82564 (39137 source, 43427 target) ; Learning rate = 0.000372 ; Loss = 1.504795\n",
      "2024-12-02 02:28:01.429000: I runner.py:310] Step = 56700 ; steps/s = 1.64, tokens/s = 81628 (38729 source, 42899 target) ; Learning rate = 0.000371 ; Loss = 1.502171\n",
      "2024-12-02 02:29:02.980000: I runner.py:310] Step = 56800 ; steps/s = 1.62, tokens/s = 82533 (39105 source, 43428 target) ; Learning rate = 0.000371 ; Loss = 1.510848\n",
      "2024-12-02 02:30:04.592000: I runner.py:310] Step = 56900 ; steps/s = 1.62, tokens/s = 82468 (39105 source, 43363 target) ; Learning rate = 0.000371 ; Loss = 1.508910\n",
      "2024-12-02 02:31:06.140000: I runner.py:310] Step = 57000 ; steps/s = 1.63, tokens/s = 82514 (39109 source, 43405 target) ; Learning rate = 0.000370 ; Loss = 1.508162\n",
      "2024-12-02 02:32:07.123000: I runner.py:310] Step = 57100 ; steps/s = 1.64, tokens/s = 81737 (38780 source, 42957 target) ; Learning rate = 0.000370 ; Loss = 1.493931\n",
      "2024-12-02 02:33:08.653000: I runner.py:310] Step = 57200 ; steps/s = 1.63, tokens/s = 82576 (39139 source, 43437 target) ; Learning rate = 0.000370 ; Loss = 1.498282\n",
      "2024-12-02 02:34:10.222000: I runner.py:310] Step = 57300 ; steps/s = 1.62, tokens/s = 82536 (39134 source, 43402 target) ; Learning rate = 0.000369 ; Loss = 1.510254\n",
      "2024-12-02 02:35:11.834000: I runner.py:310] Step = 57400 ; steps/s = 1.62, tokens/s = 82439 (39091 source, 43348 target) ; Learning rate = 0.000369 ; Loss = 1.506162\n",
      "2024-12-02 02:36:12.811000: I runner.py:310] Step = 57500 ; steps/s = 1.64, tokens/s = 81713 (38732 source, 42981 target) ; Learning rate = 0.000369 ; Loss = 1.505012\n",
      "2024-12-02 02:37:14.363000: I runner.py:310] Step = 57600 ; steps/s = 1.62, tokens/s = 82527 (39147 source, 43380 target) ; Learning rate = 0.000368 ; Loss = 1.496018\n",
      "2024-12-02 02:38:15.912000: I runner.py:310] Step = 57700 ; steps/s = 1.62, tokens/s = 82544 (39139 source, 43405 target) ; Learning rate = 0.000368 ; Loss = 1.513762\n",
      "2024-12-02 02:39:17.454000: I runner.py:310] Step = 57800 ; steps/s = 1.63, tokens/s = 82551 (39125 source, 43426 target) ; Learning rate = 0.000368 ; Loss = 1.505050\n",
      "2024-12-02 02:40:18.432000: I runner.py:310] Step = 57900 ; steps/s = 1.64, tokens/s = 81719 (38719 source, 43000 target) ; Learning rate = 0.000367 ; Loss = 1.507465\n",
      "2024-12-02 02:41:19.980000: I runner.py:310] Step = 58000 ; steps/s = 1.62, tokens/s = 82529 (39120 source, 43409 target) ; Learning rate = 0.000367 ; Loss = 1.501878\n",
      "2024-12-02 02:42:21.483000: I runner.py:310] Step = 58100 ; steps/s = 1.63, tokens/s = 82604 (39159 source, 43445 target) ; Learning rate = 0.000367 ; Loss = 1.507608\n",
      "2024-12-02 02:43:22.975000: I runner.py:310] Step = 58200 ; steps/s = 1.63, tokens/s = 82634 (39199 source, 43435 target) ; Learning rate = 0.000366 ; Loss = 1.498329\n",
      "2024-12-02 02:44:23.953000: I runner.py:310] Step = 58300 ; steps/s = 1.64, tokens/s = 81738 (38750 source, 42988 target) ; Learning rate = 0.000366 ; Loss = 1.501104\n",
      "2024-12-02 02:45:25.448000: I runner.py:310] Step = 58400 ; steps/s = 1.63, tokens/s = 82604 (39156 source, 43448 target) ; Learning rate = 0.000366 ; Loss = 1.501647\n",
      "2024-12-02 02:46:26.943000: I runner.py:310] Step = 58500 ; steps/s = 1.63, tokens/s = 82598 (39181 source, 43417 target) ; Learning rate = 0.000365 ; Loss = 1.504791\n",
      "2024-12-02 02:47:28.524000: I runner.py:310] Step = 58600 ; steps/s = 1.62, tokens/s = 82509 (39099 source, 43410 target) ; Learning rate = 0.000365 ; Loss = 1.504162\n",
      "2024-12-02 02:48:29.505000: I runner.py:310] Step = 58700 ; steps/s = 1.64, tokens/s = 81699 (38730 source, 42969 target) ; Learning rate = 0.000365 ; Loss = 1.500862\n",
      "2024-12-02 02:49:31.049000: I runner.py:310] Step = 58800 ; steps/s = 1.63, tokens/s = 82528 (39125 source, 43403 target) ; Learning rate = 0.000365 ; Loss = 1.499807\n",
      "2024-12-02 02:50:32.562000: I runner.py:310] Step = 58900 ; steps/s = 1.63, tokens/s = 82589 (39169 source, 43420 target) ; Learning rate = 0.000364 ; Loss = 1.497657\n",
      "2024-12-02 02:51:34.109000: I runner.py:310] Step = 59000 ; steps/s = 1.63, tokens/s = 82568 (39127 source, 43441 target) ; Learning rate = 0.000364 ; Loss = 1.512576\n",
      "2024-12-02 02:52:35.017000: I runner.py:310] Step = 59100 ; steps/s = 1.64, tokens/s = 81813 (38812 source, 43001 target) ; Learning rate = 0.000364 ; Loss = 1.505902\n",
      "2024-12-02 02:53:36.587000: I runner.py:310] Step = 59200 ; steps/s = 1.62, tokens/s = 82542 (39141 source, 43401 target) ; Learning rate = 0.000363 ; Loss = 1.499645\n",
      "2024-12-02 02:54:38.136000: I runner.py:310] Step = 59300 ; steps/s = 1.62, tokens/s = 82576 (39146 source, 43430 target) ; Learning rate = 0.000363 ; Loss = 1.499561\n",
      "2024-12-02 02:55:39.658000: I runner.py:310] Step = 59400 ; steps/s = 1.63, tokens/s = 82554 (39129 source, 43425 target) ; Learning rate = 0.000363 ; Loss = 1.498882\n",
      "2024-12-02 02:56:40.632000: I runner.py:310] Step = 59500 ; steps/s = 1.64, tokens/s = 81749 (38746 source, 43003 target) ; Learning rate = 0.000362 ; Loss = 1.499423\n",
      "2024-12-02 02:57:42.233000: I runner.py:310] Step = 59600 ; steps/s = 1.62, tokens/s = 82465 (39086 source, 43379 target) ; Learning rate = 0.000362 ; Loss = 1.502637\n",
      "2024-12-02 02:58:43.743000: I runner.py:310] Step = 59700 ; steps/s = 1.63, tokens/s = 82591 (39156 source, 43435 target) ; Learning rate = 0.000362 ; Loss = 1.504119\n",
      "2024-12-02 02:59:45.264000: I runner.py:310] Step = 59800 ; steps/s = 1.63, tokens/s = 82569 (39161 source, 43408 target) ; Learning rate = 0.000361 ; Loss = 1.508091\n",
      "2024-12-02 03:00:46.300000: I runner.py:310] Step = 59900 ; steps/s = 1.64, tokens/s = 81634 (38705 source, 42929 target) ; Learning rate = 0.000361 ; Loss = 1.488709\n",
      "2024-12-02 03:01:47.853000: I runner.py:310] Step = 60000 ; steps/s = 1.62, tokens/s = 82491 (39131 source, 43360 target) ; Learning rate = 0.000361 ; Loss = 1.507973\n",
      "2024-12-02 03:01:49.592000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-60000\n",
      "2024-12-02 03:01:49.592000: I training.py:192] Running evaluation for step 60000\n",
      "2024-12-02 03:05:59.678000: I training.py:192] Evaluation result for step 60000: loss = 1.222335 ; perplexity = 3.395108\n",
      "2024-12-02 03:07:01.049000: I runner.py:310] Step = 60100 ; steps/s = 1.63, tokens/s = 82815 (39253 source, 43562 target) ; Learning rate = 0.000361 ; Loss = 1.499195\n",
      "2024-12-02 03:08:02.610000: I runner.py:310] Step = 60200 ; steps/s = 1.62, tokens/s = 82549 (39124 source, 43425 target) ; Learning rate = 0.000360 ; Loss = 1.508413\n",
      "2024-12-02 03:09:03.489000: I runner.py:310] Step = 60300 ; steps/s = 1.64, tokens/s = 81843 (38788 source, 43055 target) ; Learning rate = 0.000360 ; Loss = 1.494178\n",
      "2024-12-02 03:10:04.994000: I runner.py:310] Step = 60400 ; steps/s = 1.63, tokens/s = 82622 (39171 source, 43451 target) ; Learning rate = 0.000360 ; Loss = 1.495844\n",
      "2024-12-02 03:11:06.515000: I runner.py:310] Step = 60500 ; steps/s = 1.63, tokens/s = 82554 (39127 source, 43427 target) ; Learning rate = 0.000359 ; Loss = 1.507678\n",
      "2024-12-02 03:12:07.970000: I runner.py:310] Step = 60600 ; steps/s = 1.63, tokens/s = 82670 (39212 source, 43458 target) ; Learning rate = 0.000359 ; Loss = 1.511766\n",
      "2024-12-02 03:13:08.878000: I runner.py:310] Step = 60700 ; steps/s = 1.64, tokens/s = 81831 (38792 source, 43039 target) ; Learning rate = 0.000359 ; Loss = 1.503227\n",
      "2024-12-02 03:14:10.383000: I runner.py:310] Step = 60800 ; steps/s = 1.63, tokens/s = 82580 (39157 source, 43423 target) ; Learning rate = 0.000358 ; Loss = 1.504525\n",
      "2024-12-02 03:15:11.892000: I runner.py:310] Step = 60900 ; steps/s = 1.63, tokens/s = 82596 (39162 source, 43434 target) ; Learning rate = 0.000358 ; Loss = 1.498937\n",
      "2024-12-02 03:16:12.793000: I runner.py:310] Step = 61000 ; steps/s = 1.64, tokens/s = 81813 (38779 source, 43034 target) ; Learning rate = 0.000358 ; Loss = 1.501064\n",
      "2024-12-02 03:17:14.296000: I runner.py:310] Step = 61100 ; steps/s = 1.63, tokens/s = 82584 (39149 source, 43435 target) ; Learning rate = 0.000358 ; Loss = 1.494315\n",
      "2024-12-02 03:18:15.800000: I runner.py:310] Step = 61200 ; steps/s = 1.63, tokens/s = 82622 (39163 source, 43459 target) ; Learning rate = 0.000357 ; Loss = 1.499514\n",
      "2024-12-02 03:19:17.284000: I runner.py:310] Step = 61300 ; steps/s = 1.63, tokens/s = 82618 (39158 source, 43460 target) ; Learning rate = 0.000357 ; Loss = 1.513253\n",
      "2024-12-02 03:20:18.282000: I runner.py:310] Step = 61400 ; steps/s = 1.64, tokens/s = 81708 (38755 source, 42953 target) ; Learning rate = 0.000357 ; Loss = 1.502061\n",
      "2024-12-02 03:21:19.753000: I runner.py:310] Step = 61500 ; steps/s = 1.63, tokens/s = 82661 (39194 source, 43467 target) ; Learning rate = 0.000356 ; Loss = 1.504248\n",
      "2024-12-02 03:22:21.177000: I runner.py:310] Step = 61600 ; steps/s = 1.63, tokens/s = 82701 (39199 source, 43502 target) ; Learning rate = 0.000356 ; Loss = 1.498163\n",
      "2024-12-02 03:23:22.736000: I runner.py:310] Step = 61700 ; steps/s = 1.62, tokens/s = 82522 (39125 source, 43397 target) ; Learning rate = 0.000356 ; Loss = 1.495217\n",
      "2024-12-02 03:24:23.725000: I runner.py:310] Step = 61800 ; steps/s = 1.64, tokens/s = 81702 (38726 source, 42976 target) ; Learning rate = 0.000356 ; Loss = 1.498491\n",
      "2024-12-02 03:25:25.161000: I runner.py:310] Step = 61900 ; steps/s = 1.63, tokens/s = 82691 (39177 source, 43514 target) ; Learning rate = 0.000355 ; Loss = 1.490218\n",
      "2024-12-02 03:26:26.659000: I runner.py:310] Step = 62000 ; steps/s = 1.63, tokens/s = 82611 (39205 source, 43406 target) ; Learning rate = 0.000355 ; Loss = 1.499426\n",
      "2024-12-02 03:27:28.133000: I runner.py:310] Step = 62100 ; steps/s = 1.63, tokens/s = 82665 (39188 source, 43477 target) ; Learning rate = 0.000355 ; Loss = 1.501779\n",
      "2024-12-02 03:28:29.055000: I runner.py:310] Step = 62200 ; steps/s = 1.64, tokens/s = 81779 (38764 source, 43015 target) ; Learning rate = 0.000354 ; Loss = 1.501704\n",
      "2024-12-02 03:29:30.542000: I runner.py:310] Step = 62300 ; steps/s = 1.63, tokens/s = 82607 (39153 source, 43454 target) ; Learning rate = 0.000354 ; Loss = 1.494994\n",
      "2024-12-02 03:30:32.021000: I runner.py:310] Step = 62400 ; steps/s = 1.63, tokens/s = 82694 (39209 source, 43485 target) ; Learning rate = 0.000354 ; Loss = 1.497086\n",
      "2024-12-02 03:31:33.480000: I runner.py:310] Step = 62500 ; steps/s = 1.63, tokens/s = 82651 (39197 source, 43454 target) ; Learning rate = 0.000354 ; Loss = 1.497151\n",
      "2024-12-02 03:32:34.382000: I runner.py:310] Step = 62600 ; steps/s = 1.64, tokens/s = 81835 (38778 source, 43057 target) ; Learning rate = 0.000353 ; Loss = 1.502483\n",
      "2024-12-02 03:33:35.809000: I runner.py:310] Step = 62700 ; steps/s = 1.63, tokens/s = 82686 (39228 source, 43458 target) ; Learning rate = 0.000353 ; Loss = 1.495971\n",
      "2024-12-02 03:34:37.259000: I runner.py:310] Step = 62800 ; steps/s = 1.63, tokens/s = 82654 (39190 source, 43464 target) ; Learning rate = 0.000353 ; Loss = 1.498638\n",
      "2024-12-02 03:35:38.747000: I runner.py:310] Step = 62900 ; steps/s = 1.63, tokens/s = 82619 (39151 source, 43468 target) ; Learning rate = 0.000352 ; Loss = 1.497486\n",
      "2024-12-02 03:36:39.663000: I runner.py:310] Step = 63000 ; steps/s = 1.64, tokens/s = 81827 (38803 source, 43024 target) ; Learning rate = 0.000352 ; Loss = 1.492409\n",
      "2024-12-02 03:37:41.147000: I runner.py:310] Step = 63100 ; steps/s = 1.63, tokens/s = 82643 (39184 source, 43459 target) ; Learning rate = 0.000352 ; Loss = 1.493909\n",
      "2024-12-02 03:38:42.640000: I runner.py:310] Step = 63200 ; steps/s = 1.63, tokens/s = 82619 (39199 source, 43420 target) ; Learning rate = 0.000352 ; Loss = 1.502169\n",
      "2024-12-02 03:39:44.120000: I runner.py:310] Step = 63300 ; steps/s = 1.63, tokens/s = 82591 (39123 source, 43468 target) ; Learning rate = 0.000351 ; Loss = 1.503870\n",
      "2024-12-02 03:40:45.042000: I runner.py:310] Step = 63400 ; steps/s = 1.64, tokens/s = 81851 (38791 source, 43060 target) ; Learning rate = 0.000351 ; Loss = 1.508758\n",
      "2024-12-02 03:41:46.520000: I runner.py:310] Step = 63500 ; steps/s = 1.63, tokens/s = 82651 (39197 source, 43454 target) ; Learning rate = 0.000351 ; Loss = 1.496059\n",
      "2024-12-02 03:42:48.048000: I runner.py:310] Step = 63600 ; steps/s = 1.63, tokens/s = 82538 (39138 source, 43400 target) ; Learning rate = 0.000350 ; Loss = 1.497487\n",
      "2024-12-02 03:43:49.506000: I runner.py:310] Step = 63700 ; steps/s = 1.63, tokens/s = 82614 (39148 source, 43466 target) ; Learning rate = 0.000350 ; Loss = 1.499847\n",
      "2024-12-02 03:44:50.363000: I runner.py:310] Step = 63800 ; steps/s = 1.64, tokens/s = 81907 (38855 source, 43052 target) ; Learning rate = 0.000350 ; Loss = 1.500524\n",
      "2024-12-02 03:45:51.947000: I runner.py:310] Step = 63900 ; steps/s = 1.62, tokens/s = 82508 (39112 source, 43396 target) ; Learning rate = 0.000350 ; Loss = 1.495030\n",
      "2024-12-02 03:46:53.400000: I runner.py:310] Step = 64000 ; steps/s = 1.63, tokens/s = 82676 (39200 source, 43476 target) ; Learning rate = 0.000349 ; Loss = 1.493276\n",
      "2024-12-02 03:47:54.844000: I runner.py:310] Step = 64100 ; steps/s = 1.63, tokens/s = 82665 (39188 source, 43477 target) ; Learning rate = 0.000349 ; Loss = 1.498550\n",
      "2024-12-02 03:48:55.778000: I runner.py:310] Step = 64200 ; steps/s = 1.64, tokens/s = 81740 (38725 source, 43015 target) ; Learning rate = 0.000349 ; Loss = 1.491810\n",
      "2024-12-02 03:49:57.294000: I runner.py:310] Step = 64300 ; steps/s = 1.63, tokens/s = 82614 (39190 source, 43424 target) ; Learning rate = 0.000349 ; Loss = 1.498932\n",
      "2024-12-02 03:50:58.755000: I runner.py:310] Step = 64400 ; steps/s = 1.63, tokens/s = 82678 (39204 source, 43474 target) ; Learning rate = 0.000348 ; Loss = 1.493100\n",
      "2024-12-02 03:52:00.240000: I runner.py:310] Step = 64500 ; steps/s = 1.63, tokens/s = 82610 (39164 source, 43446 target) ; Learning rate = 0.000348 ; Loss = 1.498896\n",
      "2024-12-02 03:53:01.161000: I runner.py:310] Step = 64600 ; steps/s = 1.64, tokens/s = 81803 (38775 source, 43028 target) ; Learning rate = 0.000348 ; Loss = 1.491803\n",
      "2024-12-02 03:54:02.634000: I runner.py:310] Step = 64700 ; steps/s = 1.63, tokens/s = 82662 (39217 source, 43445 target) ; Learning rate = 0.000347 ; Loss = 1.492693\n",
      "2024-12-02 03:55:04.128000: I runner.py:310] Step = 64800 ; steps/s = 1.63, tokens/s = 82594 (39122 source, 43472 target) ; Learning rate = 0.000347 ; Loss = 1.499935\n",
      "2024-12-02 03:56:05.671000: I runner.py:310] Step = 64900 ; steps/s = 1.63, tokens/s = 82550 (39149 source, 43401 target) ; Learning rate = 0.000347 ; Loss = 1.508912\n",
      "2024-12-02 03:57:06.636000: I runner.py:310] Step = 65000 ; steps/s = 1.64, tokens/s = 81747 (38769 source, 42978 target) ; Learning rate = 0.000347 ; Loss = 1.492763\n",
      "2024-12-02 03:57:06.637000: I training.py:192] Running evaluation for step 65000\n",
      "2024-12-02 04:01:19.402000: I training.py:192] Evaluation result for step 65000: loss = 1.229033 ; perplexity = 3.417922\n",
      "2024-12-02 04:02:20.820000: I runner.py:310] Step = 65100 ; steps/s = 1.63, tokens/s = 82755 (39251 source, 43504 target) ; Learning rate = 0.000346 ; Loss = 1.487516\n",
      "2024-12-02 04:03:22.410000: I runner.py:310] Step = 65200 ; steps/s = 1.62, tokens/s = 82454 (39083 source, 43371 target) ; Learning rate = 0.000346 ; Loss = 1.491306\n",
      "2024-12-02 04:04:23.449000: I runner.py:310] Step = 65300 ; steps/s = 1.64, tokens/s = 81641 (38689 source, 42952 target) ; Learning rate = 0.000346 ; Loss = 1.499596\n",
      "2024-12-02 04:05:25.066000: I runner.py:310] Step = 65400 ; steps/s = 1.62, tokens/s = 82470 (39101 source, 43369 target) ; Learning rate = 0.000346 ; Loss = 1.495130\n",
      "2024-12-02 04:06:26.599000: I runner.py:310] Step = 65500 ; steps/s = 1.63, tokens/s = 82564 (39136 source, 43428 target) ; Learning rate = 0.000345 ; Loss = 1.496503\n",
      "2024-12-02 04:07:28.190000: I runner.py:310] Step = 65600 ; steps/s = 1.62, tokens/s = 82485 (39118 source, 43367 target) ; Learning rate = 0.000345 ; Loss = 1.493532\n",
      "2024-12-02 04:08:29.200000: I runner.py:310] Step = 65700 ; steps/s = 1.64, tokens/s = 81656 (38696 source, 42960 target) ; Learning rate = 0.000345 ; Loss = 1.496796\n",
      "2024-12-02 04:09:30.797000: I runner.py:310] Step = 65800 ; steps/s = 1.62, tokens/s = 82485 (39096 source, 43389 target) ; Learning rate = 0.000345 ; Loss = 1.491724\n",
      "2024-12-02 04:10:32.350000: I runner.py:310] Step = 65900 ; steps/s = 1.62, tokens/s = 82507 (39133 source, 43374 target) ; Learning rate = 0.000344 ; Loss = 1.496291\n",
      "2024-12-02 04:11:33.955000: I runner.py:310] Step = 66000 ; steps/s = 1.62, tokens/s = 82427 (39087 source, 43340 target) ; Learning rate = 0.000344 ; Loss = 1.498512\n",
      "2024-12-02 04:12:34.981000: I runner.py:310] Step = 66100 ; steps/s = 1.64, tokens/s = 81711 (38732 source, 42979 target) ; Learning rate = 0.000344 ; Loss = 1.491614\n",
      "2024-12-02 04:13:36.504000: I runner.py:310] Step = 66200 ; steps/s = 1.63, tokens/s = 82606 (39153 source, 43453 target) ; Learning rate = 0.000344 ; Loss = 1.491713\n",
      "2024-12-02 04:14:37.958000: I runner.py:310] Step = 66300 ; steps/s = 1.63, tokens/s = 82675 (39205 source, 43470 target) ; Learning rate = 0.000343 ; Loss = 1.495186\n",
      "2024-12-02 04:15:39.508000: I runner.py:310] Step = 66400 ; steps/s = 1.62, tokens/s = 82541 (39132 source, 43409 target) ; Learning rate = 0.000343 ; Loss = 1.500079\n",
      "2024-12-02 04:16:40.469000: I runner.py:310] Step = 66500 ; steps/s = 1.64, tokens/s = 81717 (38735 source, 42982 target) ; Learning rate = 0.000343 ; Loss = 1.495060\n",
      "2024-12-02 04:17:42.063000: I runner.py:310] Step = 66600 ; steps/s = 1.62, tokens/s = 82462 (39099 source, 43363 target) ; Learning rate = 0.000342 ; Loss = 1.493339\n",
      "2024-12-02 04:18:43.613000: I runner.py:310] Step = 66700 ; steps/s = 1.62, tokens/s = 82563 (39158 source, 43405 target) ; Learning rate = 0.000342 ; Loss = 1.492110\n",
      "2024-12-02 04:19:45.080000: I runner.py:310] Step = 66800 ; steps/s = 1.63, tokens/s = 82646 (39165 source, 43481 target) ; Learning rate = 0.000342 ; Loss = 1.504035\n",
      "2024-12-02 04:20:46.117000: I runner.py:310] Step = 66900 ; steps/s = 1.64, tokens/s = 81642 (38689 source, 42953 target) ; Learning rate = 0.000342 ; Loss = 1.491299\n",
      "2024-12-02 04:21:47.712000: I runner.py:310] Step = 67000 ; steps/s = 1.62, tokens/s = 82508 (39120 source, 43388 target) ; Learning rate = 0.000341 ; Loss = 1.493652\n",
      "2024-12-02 04:22:49.214000: I runner.py:310] Step = 67100 ; steps/s = 1.63, tokens/s = 82594 (39178 source, 43416 target) ; Learning rate = 0.000341 ; Loss = 1.497253\n",
      "2024-12-02 04:23:50.731000: I runner.py:310] Step = 67200 ; steps/s = 1.63, tokens/s = 82577 (39145 source, 43432 target) ; Learning rate = 0.000341 ; Loss = 1.505627\n",
      "2024-12-02 04:24:51.654000: I runner.py:310] Step = 67300 ; steps/s = 1.64, tokens/s = 81764 (38749 source, 43015 target) ; Learning rate = 0.000341 ; Loss = 1.492359\n",
      "2024-12-02 04:25:53.214000: I runner.py:310] Step = 67400 ; steps/s = 1.62, tokens/s = 82513 (39129 source, 43384 target) ; Learning rate = 0.000340 ; Loss = 1.494503\n",
      "2024-12-02 04:26:54.774000: I runner.py:310] Step = 67500 ; steps/s = 1.62, tokens/s = 82541 (39143 source, 43398 target) ; Learning rate = 0.000340 ; Loss = 1.491207\n",
      "2024-12-02 04:27:56.379000: I runner.py:310] Step = 67600 ; steps/s = 1.62, tokens/s = 82495 (39129 source, 43366 target) ; Learning rate = 0.000340 ; Loss = 1.496083\n",
      "2024-12-02 04:28:57.348000: I runner.py:310] Step = 67700 ; steps/s = 1.64, tokens/s = 81727 (38706 source, 43021 target) ; Learning rate = 0.000340 ; Loss = 1.493036\n",
      "2024-12-02 04:29:58.954000: I runner.py:310] Step = 67800 ; steps/s = 1.62, tokens/s = 82465 (39101 source, 43364 target) ; Learning rate = 0.000339 ; Loss = 1.491948\n",
      "2024-12-02 04:31:00.551000: I runner.py:310] Step = 67900 ; steps/s = 1.62, tokens/s = 82467 (39123 source, 43344 target) ; Learning rate = 0.000339 ; Loss = 1.494997\n",
      "2024-12-02 04:32:02.175000: I runner.py:310] Step = 68000 ; steps/s = 1.62, tokens/s = 82445 (39070 source, 43375 target) ; Learning rate = 0.000339 ; Loss = 1.497090\n",
      "2024-12-02 04:33:03.132000: I runner.py:310] Step = 68100 ; steps/s = 1.64, tokens/s = 81753 (38738 source, 43015 target) ; Learning rate = 0.000339 ; Loss = 1.487463\n",
      "2024-12-02 04:34:04.695000: I runner.py:310] Step = 68200 ; steps/s = 1.62, tokens/s = 82482 (39081 source, 43401 target) ; Learning rate = 0.000338 ; Loss = 1.489975\n",
      "2024-12-02 04:35:06.249000: I runner.py:310] Step = 68300 ; steps/s = 1.62, tokens/s = 82564 (39172 source, 43392 target) ; Learning rate = 0.000338 ; Loss = 1.495387\n",
      "2024-12-02 04:36:07.848000: I runner.py:310] Step = 68400 ; steps/s = 1.62, tokens/s = 82491 (39127 source, 43364 target) ; Learning rate = 0.000338 ; Loss = 1.495498\n",
      "2024-12-02 04:37:08.880000: I runner.py:310] Step = 68500 ; steps/s = 1.64, tokens/s = 81661 (38699 source, 42962 target) ; Learning rate = 0.000338 ; Loss = 1.497549\n",
      "2024-12-02 04:38:10.424000: I runner.py:310] Step = 68600 ; steps/s = 1.63, tokens/s = 82574 (39154 source, 43420 target) ; Learning rate = 0.000337 ; Loss = 1.488962\n",
      "2024-12-02 04:39:11.998000: I runner.py:310] Step = 68700 ; steps/s = 1.62, tokens/s = 82481 (39107 source, 43374 target) ; Learning rate = 0.000337 ; Loss = 1.498600\n",
      "2024-12-02 04:40:13.597000: I runner.py:310] Step = 68800 ; steps/s = 1.62, tokens/s = 82444 (39080 source, 43364 target) ; Learning rate = 0.000337 ; Loss = 1.494072\n",
      "2024-12-02 04:41:14.516000: I runner.py:310] Step = 68900 ; steps/s = 1.64, tokens/s = 81823 (38788 source, 43035 target) ; Learning rate = 0.000337 ; Loss = 1.485768\n",
      "2024-12-02 04:42:16.091000: I runner.py:310] Step = 69000 ; steps/s = 1.62, tokens/s = 82463 (39091 source, 43372 target) ; Learning rate = 0.000336 ; Loss = 1.496478\n",
      "2024-12-02 04:43:17.618000: I runner.py:310] Step = 69100 ; steps/s = 1.63, tokens/s = 82574 (39153 source, 43421 target) ; Learning rate = 0.000336 ; Loss = 1.498052\n",
      "2024-12-02 04:44:19.136000: I runner.py:310] Step = 69200 ; steps/s = 1.63, tokens/s = 82594 (39179 source, 43415 target) ; Learning rate = 0.000336 ; Loss = 1.504420\n",
      "2024-12-02 04:45:20.176000: I runner.py:310] Step = 69300 ; steps/s = 1.64, tokens/s = 81636 (38672 source, 42964 target) ; Learning rate = 0.000336 ; Loss = 1.485373\n",
      "2024-12-02 04:46:21.692000: I runner.py:310] Step = 69400 ; steps/s = 1.63, tokens/s = 82594 (39149 source, 43445 target) ; Learning rate = 0.000336 ; Loss = 1.491740\n",
      "2024-12-02 04:47:23.270000: I runner.py:310] Step = 69500 ; steps/s = 1.62, tokens/s = 82513 (39115 source, 43398 target) ; Learning rate = 0.000335 ; Loss = 1.494602\n",
      "2024-12-02 04:48:24.209000: I runner.py:310] Step = 69600 ; steps/s = 1.64, tokens/s = 81749 (38795 source, 42954 target) ; Learning rate = 0.000335 ; Loss = 1.489201\n",
      "2024-12-02 04:49:25.790000: I runner.py:310] Step = 69700 ; steps/s = 1.62, tokens/s = 82511 (39097 source, 43414 target) ; Learning rate = 0.000335 ; Loss = 1.483855\n",
      "2024-12-02 04:50:27.399000: I runner.py:310] Step = 69800 ; steps/s = 1.62, tokens/s = 82456 (39119 source, 43337 target) ; Learning rate = 0.000335 ; Loss = 1.498879\n",
      "2024-12-02 04:51:28.924000: I runner.py:310] Step = 69900 ; steps/s = 1.63, tokens/s = 82575 (39143 source, 43432 target) ; Learning rate = 0.000334 ; Loss = 1.492995\n",
      "2024-12-02 04:52:29.870000: I runner.py:310] Step = 70000 ; steps/s = 1.64, tokens/s = 81754 (38750 source, 43004 target) ; Learning rate = 0.000334 ; Loss = 1.487315\n",
      "2024-12-02 04:52:31.209000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-70000\n",
      "2024-12-02 04:52:31.209000: I training.py:192] Running evaluation for step 70000\n",
      "2024-12-02 04:56:33.401000: I training.py:192] Evaluation result for step 70000: loss = 1.237564 ; perplexity = 3.447207\n",
      "2024-12-02 04:57:34.807000: I runner.py:310] Step = 70100 ; steps/s = 1.63, tokens/s = 82755 (39227 source, 43528 target) ; Learning rate = 0.000334 ; Loss = 1.494354\n",
      "2024-12-02 04:58:36.427000: I runner.py:310] Step = 70200 ; steps/s = 1.62, tokens/s = 82444 (39104 source, 43340 target) ; Learning rate = 0.000334 ; Loss = 1.492934\n",
      "2024-12-02 04:59:37.947000: I runner.py:310] Step = 70300 ; steps/s = 1.63, tokens/s = 82568 (39131 source, 43437 target) ; Learning rate = 0.000333 ; Loss = 1.490095\n",
      "2024-12-02 05:00:39.007000: I runner.py:310] Step = 70400 ; steps/s = 1.64, tokens/s = 81614 (38692 source, 42922 target) ; Learning rate = 0.000333 ; Loss = 1.484127\n",
      "2024-12-02 05:01:40.564000: I runner.py:310] Step = 70500 ; steps/s = 1.62, tokens/s = 82540 (39140 source, 43400 target) ; Learning rate = 0.000333 ; Loss = 1.484855\n",
      "2024-12-02 05:02:42.091000: I runner.py:310] Step = 70600 ; steps/s = 1.63, tokens/s = 82583 (39163 source, 43420 target) ; Learning rate = 0.000333 ; Loss = 1.495510\n",
      "2024-12-02 05:03:43.658000: I runner.py:310] Step = 70700 ; steps/s = 1.62, tokens/s = 82526 (39100 source, 43426 target) ; Learning rate = 0.000332 ; Loss = 1.495525\n",
      "2024-12-02 05:04:44.741000: I runner.py:310] Step = 70800 ; steps/s = 1.64, tokens/s = 81570 (38687 source, 42883 target) ; Learning rate = 0.000332 ; Loss = 1.485777\n",
      "2024-12-02 05:05:46.255000: I runner.py:310] Step = 70900 ; steps/s = 1.63, tokens/s = 82582 (39151 source, 43431 target) ; Learning rate = 0.000332 ; Loss = 1.489817\n",
      "2024-12-02 05:06:47.845000: I runner.py:310] Step = 71000 ; steps/s = 1.62, tokens/s = 82498 (39130 source, 43368 target) ; Learning rate = 0.000332 ; Loss = 1.488362\n",
      "2024-12-02 05:07:49.401000: I runner.py:310] Step = 71100 ; steps/s = 1.62, tokens/s = 82511 (39102 source, 43409 target) ; Learning rate = 0.000331 ; Loss = 1.498232\n",
      "2024-12-02 05:08:50.335000: I runner.py:310] Step = 71200 ; steps/s = 1.64, tokens/s = 81787 (38765 source, 43022 target) ; Learning rate = 0.000331 ; Loss = 1.490903\n",
      "2024-12-02 05:09:51.904000: I runner.py:310] Step = 71300 ; steps/s = 1.62, tokens/s = 82513 (39108 source, 43405 target) ; Learning rate = 0.000331 ; Loss = 1.488919\n",
      "2024-12-02 05:10:53.444000: I runner.py:310] Step = 71400 ; steps/s = 1.63, tokens/s = 82581 (39175 source, 43406 target) ; Learning rate = 0.000331 ; Loss = 1.494387\n",
      "2024-12-02 05:11:55.094000: I runner.py:310] Step = 71500 ; steps/s = 1.62, tokens/s = 82398 (39057 source, 43341 target) ; Learning rate = 0.000331 ; Loss = 1.493793\n",
      "2024-12-02 05:12:56.095000: I runner.py:310] Step = 71600 ; steps/s = 1.64, tokens/s = 81657 (38684 source, 42973 target) ; Learning rate = 0.000330 ; Loss = 1.487493\n",
      "2024-12-02 05:13:57.702000: I runner.py:310] Step = 71700 ; steps/s = 1.62, tokens/s = 82455 (39108 source, 43347 target) ; Learning rate = 0.000330 ; Loss = 1.499992\n",
      "2024-12-02 05:14:59.241000: I runner.py:310] Step = 71800 ; steps/s = 1.63, tokens/s = 82539 (39144 source, 43395 target) ; Learning rate = 0.000330 ; Loss = 1.487927\n",
      "2024-12-02 05:16:00.811000: I runner.py:310] Step = 71900 ; steps/s = 1.62, tokens/s = 82517 (39124 source, 43393 target) ; Learning rate = 0.000330 ; Loss = 1.497311\n",
      "2024-12-02 05:17:01.798000: I runner.py:310] Step = 72000 ; steps/s = 1.64, tokens/s = 81711 (38740 source, 42971 target) ; Learning rate = 0.000329 ; Loss = 1.483003\n",
      "2024-12-02 05:18:03.295000: I runner.py:310] Step = 72100 ; steps/s = 1.63, tokens/s = 82615 (39167 source, 43448 target) ; Learning rate = 0.000329 ; Loss = 1.486042\n",
      "2024-12-02 05:19:04.845000: I runner.py:310] Step = 72200 ; steps/s = 1.62, tokens/s = 82541 (39133 source, 43408 target) ; Learning rate = 0.000329 ; Loss = 1.493487\n",
      "2024-12-02 05:20:06.459000: I runner.py:310] Step = 72300 ; steps/s = 1.62, tokens/s = 82465 (39088 source, 43377 target) ; Learning rate = 0.000329 ; Loss = 1.503206\n",
      "2024-12-02 05:21:07.450000: I runner.py:310] Step = 72400 ; steps/s = 1.64, tokens/s = 81705 (38726 source, 42979 target) ; Learning rate = 0.000328 ; Loss = 1.495318\n",
      "2024-12-02 05:22:09.019000: I runner.py:310] Step = 72500 ; steps/s = 1.62, tokens/s = 82529 (39124 source, 43405 target) ; Learning rate = 0.000328 ; Loss = 1.486321\n",
      "2024-12-02 05:23:10.630000: I runner.py:310] Step = 72600 ; steps/s = 1.62, tokens/s = 82459 (39111 source, 43348 target) ; Learning rate = 0.000328 ; Loss = 1.486714\n",
      "2024-12-02 05:24:12.173000: I runner.py:310] Step = 72700 ; steps/s = 1.63, tokens/s = 82527 (39116 source, 43411 target) ; Learning rate = 0.000328 ; Loss = 1.493282\n",
      "2024-12-02 05:25:13.213000: I runner.py:310] Step = 72800 ; steps/s = 1.64, tokens/s = 81648 (38718 source, 42930 target) ; Learning rate = 0.000328 ; Loss = 1.487553\n",
      "2024-12-02 05:26:14.772000: I runner.py:310] Step = 72900 ; steps/s = 1.62, tokens/s = 82547 (39146 source, 43401 target) ; Learning rate = 0.000327 ; Loss = 1.492941\n",
      "2024-12-02 05:27:16.381000: I runner.py:310] Step = 73000 ; steps/s = 1.62, tokens/s = 82454 (39085 source, 43369 target) ; Learning rate = 0.000327 ; Loss = 1.490461\n",
      "2024-12-02 05:28:17.979000: I runner.py:310] Step = 73100 ; steps/s = 1.62, tokens/s = 82466 (39092 source, 43374 target) ; Learning rate = 0.000327 ; Loss = 1.493480\n",
      "2024-12-02 05:29:18.985000: I runner.py:310] Step = 73200 ; steps/s = 1.64, tokens/s = 81695 (38723 source, 42972 target) ; Learning rate = 0.000327 ; Loss = 1.479253\n",
      "2024-12-02 05:30:20.560000: I runner.py:310] Step = 73300 ; steps/s = 1.62, tokens/s = 82501 (39152 source, 43349 target) ; Learning rate = 0.000326 ; Loss = 1.491143\n",
      "2024-12-02 05:31:22.103000: I runner.py:310] Step = 73400 ; steps/s = 1.63, tokens/s = 82540 (39132 source, 43408 target) ; Learning rate = 0.000326 ; Loss = 1.492025\n",
      "2024-12-02 05:32:23.632000: I runner.py:310] Step = 73500 ; steps/s = 1.63, tokens/s = 82569 (39122 source, 43447 target) ; Learning rate = 0.000326 ; Loss = 1.491217\n",
      "2024-12-02 05:33:24.565000: I runner.py:310] Step = 73600 ; steps/s = 1.64, tokens/s = 81795 (38766 source, 43029 target) ; Learning rate = 0.000326 ; Loss = 1.489189\n",
      "2024-12-02 05:34:26.086000: I runner.py:310] Step = 73700 ; steps/s = 1.63, tokens/s = 82570 (39151 source, 43419 target) ; Learning rate = 0.000326 ; Loss = 1.485573\n",
      "2024-12-02 05:35:27.721000: I runner.py:310] Step = 73800 ; steps/s = 1.62, tokens/s = 82427 (39072 source, 43355 target) ; Learning rate = 0.000325 ; Loss = 1.485565\n",
      "2024-12-02 05:36:28.766000: I runner.py:310] Step = 73900 ; steps/s = 1.64, tokens/s = 81609 (38709 source, 42900 target) ; Learning rate = 0.000325 ; Loss = 1.488610\n",
      "2024-12-02 05:37:30.294000: I runner.py:310] Step = 74000 ; steps/s = 1.63, tokens/s = 82582 (39121 source, 43461 target) ; Learning rate = 0.000325 ; Loss = 1.488244\n",
      "2024-12-02 05:38:31.893000: I runner.py:310] Step = 74100 ; steps/s = 1.62, tokens/s = 82454 (39104 source, 43350 target) ; Learning rate = 0.000325 ; Loss = 1.480913\n",
      "2024-12-02 05:39:33.394000: I runner.py:310] Step = 74200 ; steps/s = 1.63, tokens/s = 82608 (39162 source, 43446 target) ; Learning rate = 0.000324 ; Loss = 1.487857\n",
      "2024-12-02 05:40:34.378000: I runner.py:310] Step = 74300 ; steps/s = 1.64, tokens/s = 81725 (38750 source, 42975 target) ; Learning rate = 0.000324 ; Loss = 1.485057\n",
      "2024-12-02 05:41:35.908000: I runner.py:310] Step = 74400 ; steps/s = 1.63, tokens/s = 82572 (39143 source, 43429 target) ; Learning rate = 0.000324 ; Loss = 1.484720\n",
      "2024-12-02 05:42:37.440000: I runner.py:310] Step = 74500 ; steps/s = 1.63, tokens/s = 82575 (39160 source, 43415 target) ; Learning rate = 0.000324 ; Loss = 1.487376\n",
      "2024-12-02 05:43:39.010000: I runner.py:310] Step = 74600 ; steps/s = 1.62, tokens/s = 82470 (39094 source, 43376 target) ; Learning rate = 0.000324 ; Loss = 1.489963\n",
      "2024-12-02 05:44:39.994000: I runner.py:310] Step = 74700 ; steps/s = 1.64, tokens/s = 81737 (38753 source, 42984 target) ; Learning rate = 0.000323 ; Loss = 1.497357\n",
      "2024-12-02 05:45:41.541000: I runner.py:310] Step = 74800 ; steps/s = 1.62, tokens/s = 82550 (39134 source, 43416 target) ; Learning rate = 0.000323 ; Loss = 1.487235\n",
      "2024-12-02 05:46:43.037000: I runner.py:310] Step = 74900 ; steps/s = 1.63, tokens/s = 82638 (39192 source, 43446 target) ; Learning rate = 0.000323 ; Loss = 1.489256\n",
      "2024-12-02 05:47:44.623000: I runner.py:310] Step = 75000 ; steps/s = 1.62, tokens/s = 82463 (39096 source, 43367 target) ; Learning rate = 0.000323 ; Loss = 1.490172\n",
      "2024-12-02 05:47:44.625000: I training.py:192] Running evaluation for step 75000\n",
      "2024-12-02 05:51:54.529000: I training.py:192] Evaluation result for step 75000: loss = 1.242598 ; perplexity = 3.464602\n",
      "2024-12-02 05:52:55.324000: I runner.py:310] Step = 75100 ; steps/s = 1.65, tokens/s = 81999 (38861 source, 43138 target) ; Learning rate = 0.000323 ; Loss = 1.493960\n",
      "2024-12-02 05:53:56.812000: I runner.py:310] Step = 75200 ; steps/s = 1.63, tokens/s = 82584 (39157 source, 43427 target) ; Learning rate = 0.000322 ; Loss = 1.487051\n",
      "2024-12-02 05:54:58.407000: I runner.py:310] Step = 75300 ; steps/s = 1.62, tokens/s = 82493 (39123 source, 43370 target) ; Learning rate = 0.000322 ; Loss = 1.487582\n",
      "2024-12-02 05:55:59.925000: I runner.py:310] Step = 75400 ; steps/s = 1.63, tokens/s = 82585 (39137 source, 43448 target) ; Learning rate = 0.000322 ; Loss = 1.488050\n",
      "2024-12-02 05:57:00.976000: I runner.py:310] Step = 75500 ; steps/s = 1.64, tokens/s = 81621 (38694 source, 42927 target) ; Learning rate = 0.000322 ; Loss = 1.486909\n",
      "2024-12-02 05:58:02.519000: I runner.py:310] Step = 75600 ; steps/s = 1.63, tokens/s = 82588 (39149 source, 43439 target) ; Learning rate = 0.000321 ; Loss = 1.487095\n",
      "2024-12-02 05:59:04.110000: I runner.py:310] Step = 75700 ; steps/s = 1.62, tokens/s = 82458 (39097 source, 43361 target) ; Learning rate = 0.000321 ; Loss = 1.492473\n",
      "2024-12-02 06:00:05.583000: I runner.py:310] Step = 75800 ; steps/s = 1.63, tokens/s = 82638 (39175 source, 43463 target) ; Learning rate = 0.000321 ; Loss = 1.491613\n",
      "2024-12-02 06:01:06.601000: I runner.py:310] Step = 75900 ; steps/s = 1.64, tokens/s = 81658 (38706 source, 42952 target) ; Learning rate = 0.000321 ; Loss = 1.485407\n",
      "2024-12-02 06:02:08.234000: I runner.py:310] Step = 76000 ; steps/s = 1.62, tokens/s = 82448 (39059 source, 43389 target) ; Learning rate = 0.000321 ; Loss = 1.481005\n",
      "2024-12-02 06:03:09.708000: I runner.py:310] Step = 76100 ; steps/s = 1.63, tokens/s = 82608 (39171 source, 43437 target) ; Learning rate = 0.000320 ; Loss = 1.492367\n",
      "2024-12-02 06:04:11.191000: I runner.py:310] Step = 76200 ; steps/s = 1.63, tokens/s = 82653 (39224 source, 43429 target) ; Learning rate = 0.000320 ; Loss = 1.485549\n",
      "2024-12-02 06:05:12.228000: I runner.py:310] Step = 76300 ; steps/s = 1.64, tokens/s = 81628 (38697 source, 42931 target) ; Learning rate = 0.000320 ; Loss = 1.480719\n",
      "2024-12-02 06:06:13.861000: I runner.py:310] Step = 76400 ; steps/s = 1.62, tokens/s = 82406 (39089 source, 43317 target) ; Learning rate = 0.000320 ; Loss = 1.486007\n",
      "2024-12-02 06:07:15.441000: I runner.py:310] Step = 76500 ; steps/s = 1.62, tokens/s = 82514 (39117 source, 43397 target) ; Learning rate = 0.000320 ; Loss = 1.484101\n",
      "2024-12-02 06:08:17.017000: I runner.py:310] Step = 76600 ; steps/s = 1.62, tokens/s = 82536 (39105 source, 43431 target) ; Learning rate = 0.000319 ; Loss = 1.488631\n",
      "2024-12-02 06:09:18.033000: I runner.py:310] Step = 76700 ; steps/s = 1.64, tokens/s = 81690 (38721 source, 42969 target) ; Learning rate = 0.000319 ; Loss = 1.499151\n",
      "2024-12-02 06:10:19.604000: I runner.py:310] Step = 76800 ; steps/s = 1.62, tokens/s = 82485 (39092 source, 43393 target) ; Learning rate = 0.000319 ; Loss = 1.487003\n",
      "2024-12-02 06:11:21.222000: I runner.py:310] Step = 76900 ; steps/s = 1.62, tokens/s = 82460 (39110 source, 43350 target) ; Learning rate = 0.000319 ; Loss = 1.483432\n",
      "2024-12-02 06:12:22.853000: I runner.py:310] Step = 77000 ; steps/s = 1.62, tokens/s = 82441 (39090 source, 43351 target) ; Learning rate = 0.000319 ; Loss = 1.487869\n",
      "2024-12-02 06:13:23.811000: I runner.py:310] Step = 77100 ; steps/s = 1.64, tokens/s = 81754 (38770 source, 42984 target) ; Learning rate = 0.000318 ; Loss = 1.488751\n",
      "2024-12-02 06:14:25.329000: I runner.py:310] Step = 77200 ; steps/s = 1.63, tokens/s = 82571 (39153 source, 43418 target) ; Learning rate = 0.000318 ; Loss = 1.482278\n",
      "2024-12-02 06:15:26.917000: I runner.py:310] Step = 77300 ; steps/s = 1.62, tokens/s = 82481 (39103 source, 43378 target) ; Learning rate = 0.000318 ; Loss = 1.496090\n",
      "2024-12-02 06:16:28.543000: I runner.py:310] Step = 77400 ; steps/s = 1.62, tokens/s = 82437 (39068 source, 43369 target) ; Learning rate = 0.000318 ; Loss = 1.487794\n",
      "2024-12-02 06:17:29.543000: I runner.py:310] Step = 77500 ; steps/s = 1.64, tokens/s = 81717 (38738 source, 42979 target) ; Learning rate = 0.000317 ; Loss = 1.480700\n",
      "2024-12-02 06:18:31.105000: I runner.py:310] Step = 77600 ; steps/s = 1.62, tokens/s = 82532 (39105 source, 43427 target) ; Learning rate = 0.000317 ; Loss = 1.485097\n",
      "2024-12-02 06:19:32.656000: I runner.py:310] Step = 77700 ; steps/s = 1.62, tokens/s = 82491 (39141 source, 43350 target) ; Learning rate = 0.000317 ; Loss = 1.490284\n",
      "2024-12-02 06:20:34.279000: I runner.py:310] Step = 77800 ; steps/s = 1.62, tokens/s = 82455 (39098 source, 43357 target) ; Learning rate = 0.000317 ; Loss = 1.487880\n",
      "2024-12-02 06:21:35.334000: I runner.py:310] Step = 77900 ; steps/s = 1.64, tokens/s = 81628 (38687 source, 42941 target) ; Learning rate = 0.000317 ; Loss = 1.477985\n",
      "2024-12-02 06:22:36.896000: I runner.py:310] Step = 78000 ; steps/s = 1.62, tokens/s = 82492 (39128 source, 43364 target) ; Learning rate = 0.000316 ; Loss = 1.480834\n",
      "2024-12-02 06:23:38.411000: I runner.py:310] Step = 78100 ; steps/s = 1.63, tokens/s = 82596 (39140 source, 43456 target) ; Learning rate = 0.000316 ; Loss = 1.494185\n",
      "2024-12-02 06:24:39.409000: I runner.py:310] Step = 78200 ; steps/s = 1.64, tokens/s = 81704 (38757 source, 42947 target) ; Learning rate = 0.000316 ; Loss = 1.507314\n",
      "2024-12-02 06:25:40.969000: I runner.py:310] Step = 78300 ; steps/s = 1.62, tokens/s = 82549 (39120 source, 43429 target) ; Learning rate = 0.000316 ; Loss = 1.482821\n",
      "2024-12-02 06:26:42.570000: I runner.py:310] Step = 78400 ; steps/s = 1.62, tokens/s = 82449 (39075 source, 43374 target) ; Learning rate = 0.000316 ; Loss = 1.486445\n",
      "2024-12-02 06:27:44.222000: I runner.py:310] Step = 78500 ; steps/s = 1.62, tokens/s = 82439 (39074 source, 43365 target) ; Learning rate = 0.000315 ; Loss = 1.483644\n",
      "2024-12-02 06:28:45.263000: I runner.py:310] Step = 78600 ; steps/s = 1.64, tokens/s = 81600 (38708 source, 42892 target) ; Learning rate = 0.000315 ; Loss = 1.490862\n",
      "2024-12-02 06:29:46.814000: I runner.py:310] Step = 78700 ; steps/s = 1.62, tokens/s = 82552 (39132 source, 43420 target) ; Learning rate = 0.000315 ; Loss = 1.486294\n",
      "2024-12-02 06:30:48.402000: I runner.py:310] Step = 78800 ; steps/s = 1.62, tokens/s = 82451 (39094 source, 43357 target) ; Learning rate = 0.000315 ; Loss = 1.484026\n",
      "2024-12-02 06:31:50.036000: I runner.py:310] Step = 78900 ; steps/s = 1.62, tokens/s = 82439 (39102 source, 43337 target) ; Learning rate = 0.000315 ; Loss = 1.488784\n",
      "2024-12-02 06:32:50.964000: I runner.py:310] Step = 79000 ; steps/s = 1.64, tokens/s = 81804 (38767 source, 43037 target) ; Learning rate = 0.000314 ; Loss = 1.483583\n",
      "2024-12-02 06:33:52.520000: I runner.py:310] Step = 79100 ; steps/s = 1.62, tokens/s = 82510 (39111 source, 43399 target) ; Learning rate = 0.000314 ; Loss = 1.482905\n",
      "2024-12-02 06:34:54.098000: I runner.py:310] Step = 79200 ; steps/s = 1.62, tokens/s = 82514 (39142 source, 43372 target) ; Learning rate = 0.000314 ; Loss = 1.482250\n",
      "2024-12-02 06:35:55.702000: I runner.py:310] Step = 79300 ; steps/s = 1.62, tokens/s = 82477 (39082 source, 43395 target) ; Learning rate = 0.000314 ; Loss = 1.485655\n",
      "2024-12-02 06:36:56.798000: I runner.py:310] Step = 79400 ; steps/s = 1.64, tokens/s = 81545 (38665 source, 42880 target) ; Learning rate = 0.000314 ; Loss = 1.488157\n",
      "2024-12-02 06:37:58.426000: I runner.py:310] Step = 79500 ; steps/s = 1.62, tokens/s = 82467 (39091 source, 43376 target) ; Learning rate = 0.000313 ; Loss = 1.487028\n",
      "2024-12-02 06:38:59.978000: I runner.py:310] Step = 79600 ; steps/s = 1.62, tokens/s = 82531 (39106 source, 43425 target) ; Learning rate = 0.000313 ; Loss = 1.478489\n",
      "2024-12-02 06:40:01.571000: I runner.py:310] Step = 79700 ; steps/s = 1.62, tokens/s = 82469 (39110 source, 43359 target) ; Learning rate = 0.000313 ; Loss = 1.488390\n",
      "2024-12-02 06:41:02.646000: I runner.py:310] Step = 79800 ; steps/s = 1.64, tokens/s = 81586 (38682 source, 42904 target) ; Learning rate = 0.000313 ; Loss = 1.484190\n",
      "2024-12-02 06:42:04.222000: I runner.py:310] Step = 79900 ; steps/s = 1.62, tokens/s = 82500 (39114 source, 43386 target) ; Learning rate = 0.000313 ; Loss = 1.487129\n",
      "2024-12-02 06:43:05.779000: I runner.py:310] Step = 80000 ; steps/s = 1.62, tokens/s = 82530 (39150 source, 43380 target) ; Learning rate = 0.000312 ; Loss = 1.494680\n",
      "2024-12-02 06:43:07.414000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-80000\n",
      "2024-12-02 06:43:07.414000: I training.py:192] Running evaluation for step 80000\n",
      "2024-12-02 06:47:15.616000: I training.py:192] Evaluation result for step 80000: loss = 1.244751 ; perplexity = 3.472069\n",
      "2024-12-02 06:48:16.981000: I runner.py:310] Step = 80100 ; steps/s = 1.63, tokens/s = 82805 (39239 source, 43566 target) ; Learning rate = 0.000312 ; Loss = 1.490759\n",
      "2024-12-02 06:49:17.964000: I runner.py:310] Step = 80200 ; steps/s = 1.64, tokens/s = 81739 (38749 source, 42990 target) ; Learning rate = 0.000312 ; Loss = 1.482474\n",
      "2024-12-02 06:50:19.530000: I runner.py:310] Step = 80300 ; steps/s = 1.62, tokens/s = 82504 (39099 source, 43405 target) ; Learning rate = 0.000312 ; Loss = 1.481871\n",
      "2024-12-02 06:51:21.157000: I runner.py:310] Step = 80400 ; steps/s = 1.62, tokens/s = 82428 (39094 source, 43334 target) ; Learning rate = 0.000312 ; Loss = 1.487891\n",
      "2024-12-02 06:52:22.810000: I runner.py:310] Step = 80500 ; steps/s = 1.62, tokens/s = 82397 (39067 source, 43330 target) ; Learning rate = 0.000312 ; Loss = 1.487687\n",
      "2024-12-02 06:53:23.837000: I runner.py:310] Step = 80600 ; steps/s = 1.64, tokens/s = 81679 (38735 source, 42944 target) ; Learning rate = 0.000311 ; Loss = 1.491997\n",
      "2024-12-02 06:54:25.411000: I runner.py:310] Step = 80700 ; steps/s = 1.62, tokens/s = 82515 (39128 source, 43387 target) ; Learning rate = 0.000311 ; Loss = 1.477680\n",
      "2024-12-02 06:55:27.119000: I runner.py:310] Step = 80800 ; steps/s = 1.62, tokens/s = 82290 (39011 source, 43279 target) ; Learning rate = 0.000311 ; Loss = 1.483311\n",
      "2024-12-02 06:56:28.726000: I runner.py:310] Step = 80900 ; steps/s = 1.62, tokens/s = 82492 (39105 source, 43387 target) ; Learning rate = 0.000311 ; Loss = 1.485874\n",
      "2024-12-02 06:57:29.738000: I runner.py:310] Step = 81000 ; steps/s = 1.64, tokens/s = 81682 (38709 source, 42973 target) ; Learning rate = 0.000311 ; Loss = 1.489428\n",
      "2024-12-02 06:58:31.315000: I runner.py:310] Step = 81100 ; steps/s = 1.62, tokens/s = 82524 (39125 source, 43399 target) ; Learning rate = 0.000310 ; Loss = 1.480072\n",
      "2024-12-02 06:59:32.954000: I runner.py:310] Step = 81200 ; steps/s = 1.62, tokens/s = 82413 (39070 source, 43343 target) ; Learning rate = 0.000310 ; Loss = 1.491928\n",
      "2024-12-02 07:00:34.510000: I runner.py:310] Step = 81300 ; steps/s = 1.62, tokens/s = 82501 (39125 source, 43376 target) ; Learning rate = 0.000310 ; Loss = 1.483843\n",
      "2024-12-02 07:01:35.540000: I runner.py:310] Step = 81400 ; steps/s = 1.64, tokens/s = 81648 (38707 source, 42941 target) ; Learning rate = 0.000310 ; Loss = 1.484358\n",
      "2024-12-02 07:02:37.061000: I runner.py:310] Step = 81500 ; steps/s = 1.63, tokens/s = 82579 (39148 source, 43431 target) ; Learning rate = 0.000310 ; Loss = 1.480827\n",
      "2024-12-02 07:03:38.610000: I runner.py:310] Step = 81600 ; steps/s = 1.62, tokens/s = 82526 (39116 source, 43410 target) ; Learning rate = 0.000309 ; Loss = 1.478244\n",
      "2024-12-02 07:04:40.214000: I runner.py:310] Step = 81700 ; steps/s = 1.62, tokens/s = 82479 (39116 source, 43363 target) ; Learning rate = 0.000309 ; Loss = 1.481717\n",
      "2024-12-02 07:05:41.240000: I runner.py:310] Step = 81800 ; steps/s = 1.64, tokens/s = 81683 (38733 source, 42950 target) ; Learning rate = 0.000309 ; Loss = 1.486586\n",
      "2024-12-02 07:06:42.787000: I runner.py:310] Step = 81900 ; steps/s = 1.62, tokens/s = 82559 (39123 source, 43436 target) ; Learning rate = 0.000309 ; Loss = 1.477241\n",
      "2024-12-02 07:07:44.408000: I runner.py:310] Step = 82000 ; steps/s = 1.62, tokens/s = 82414 (39080 source, 43334 target) ; Learning rate = 0.000309 ; Loss = 1.482266\n",
      "2024-12-02 07:08:46.040000: I runner.py:310] Step = 82100 ; steps/s = 1.62, tokens/s = 82432 (39095 source, 43337 target) ; Learning rate = 0.000308 ; Loss = 1.479044\n",
      "2024-12-02 07:09:47.087000: I runner.py:310] Step = 82200 ; steps/s = 1.64, tokens/s = 81615 (38682 source, 42933 target) ; Learning rate = 0.000308 ; Loss = 1.475259\n",
      "2024-12-02 07:10:48.667000: I runner.py:310] Step = 82300 ; steps/s = 1.62, tokens/s = 82535 (39138 source, 43397 target) ; Learning rate = 0.000308 ; Loss = 1.486418\n",
      "2024-12-02 07:11:50.285000: I runner.py:310] Step = 82400 ; steps/s = 1.62, tokens/s = 82421 (39070 source, 43351 target) ; Learning rate = 0.000308 ; Loss = 1.486978\n",
      "2024-12-02 07:12:51.447000: I runner.py:310] Step = 82500 ; steps/s = 1.64, tokens/s = 82010 (38885 source, 43125 target) ; Learning rate = 0.000308 ; Loss = 1.487180\n",
      "2024-12-02 07:13:52.768000: I runner.py:310] Step = 82600 ; steps/s = 1.63, tokens/s = 82342 (39050 source, 43292 target) ; Learning rate = 0.000308 ; Loss = 1.484965\n",
      "2024-12-02 07:14:54.241000: I runner.py:310] Step = 82700 ; steps/s = 1.63, tokens/s = 82633 (39164 source, 43469 target) ; Learning rate = 0.000307 ; Loss = 1.475767\n",
      "2024-12-02 07:15:55.772000: I runner.py:310] Step = 82800 ; steps/s = 1.63, tokens/s = 82546 (39129 source, 43417 target) ; Learning rate = 0.000307 ; Loss = 1.487072\n",
      "2024-12-02 07:16:56.852000: I runner.py:310] Step = 82900 ; steps/s = 1.64, tokens/s = 81587 (38678 source, 42909 target) ; Learning rate = 0.000307 ; Loss = 1.474559\n",
      "2024-12-02 07:17:58.443000: I runner.py:310] Step = 83000 ; steps/s = 1.62, tokens/s = 82489 (39092 source, 43397 target) ; Learning rate = 0.000307 ; Loss = 1.483137\n",
      "2024-12-02 07:19:00.001000: I runner.py:310] Step = 83100 ; steps/s = 1.62, tokens/s = 82513 (39151 source, 43362 target) ; Learning rate = 0.000307 ; Loss = 1.478125\n",
      "2024-12-02 07:20:01.574000: I runner.py:310] Step = 83200 ; steps/s = 1.62, tokens/s = 82504 (39106 source, 43398 target) ; Learning rate = 0.000306 ; Loss = 1.487596\n",
      "2024-12-02 07:21:02.600000: I runner.py:310] Step = 83300 ; steps/s = 1.64, tokens/s = 81685 (38720 source, 42965 target) ; Learning rate = 0.000306 ; Loss = 1.480855\n",
      "2024-12-02 07:22:04.206000: I runner.py:310] Step = 83400 ; steps/s = 1.62, tokens/s = 82448 (39094 source, 43354 target) ; Learning rate = 0.000306 ; Loss = 1.477739\n",
      "2024-12-02 07:23:05.802000: I runner.py:310] Step = 83500 ; steps/s = 1.62, tokens/s = 82496 (39110 source, 43386 target) ; Learning rate = 0.000306 ; Loss = 1.481441\n",
      "2024-12-02 07:24:07.271000: I runner.py:310] Step = 83600 ; steps/s = 1.63, tokens/s = 82637 (39185 source, 43452 target) ; Learning rate = 0.000306 ; Loss = 1.479853\n",
      "2024-12-02 07:25:08.356000: I runner.py:310] Step = 83700 ; steps/s = 1.64, tokens/s = 81564 (38664 source, 42900 target) ; Learning rate = 0.000306 ; Loss = 1.488469\n",
      "2024-12-02 07:26:09.932000: I runner.py:310] Step = 83800 ; steps/s = 1.62, tokens/s = 82492 (39114 source, 43378 target) ; Learning rate = 0.000305 ; Loss = 1.483270\n",
      "2024-12-02 07:27:11.537000: I runner.py:310] Step = 83900 ; steps/s = 1.62, tokens/s = 82490 (39121 source, 43369 target) ; Learning rate = 0.000305 ; Loss = 1.481396\n",
      "2024-12-02 07:28:13.187000: I runner.py:310] Step = 84000 ; steps/s = 1.62, tokens/s = 82384 (39042 source, 43342 target) ; Learning rate = 0.000305 ; Loss = 1.479429\n",
      "2024-12-02 07:29:14.207000: I runner.py:310] Step = 84100 ; steps/s = 1.64, tokens/s = 81694 (38726 source, 42968 target) ; Learning rate = 0.000305 ; Loss = 1.475372\n",
      "2024-12-02 07:30:15.806000: I runner.py:310] Step = 84200 ; steps/s = 1.62, tokens/s = 82466 (39106 source, 43360 target) ; Learning rate = 0.000305 ; Loss = 1.480497\n",
      "2024-12-02 07:31:17.389000: I runner.py:310] Step = 84300 ; steps/s = 1.62, tokens/s = 82493 (39110 source, 43383 target) ; Learning rate = 0.000304 ; Loss = 1.483985\n",
      "2024-12-02 07:32:18.952000: I runner.py:310] Step = 84400 ; steps/s = 1.62, tokens/s = 82523 (39112 source, 43411 target) ; Learning rate = 0.000304 ; Loss = 1.486290\n",
      "2024-12-02 07:33:20.037000: I runner.py:310] Step = 84500 ; steps/s = 1.64, tokens/s = 81544 (38654 source, 42890 target) ; Learning rate = 0.000304 ; Loss = 1.478330\n",
      "2024-12-02 07:34:21.575000: I runner.py:310] Step = 84600 ; steps/s = 1.63, tokens/s = 82557 (39140 source, 43417 target) ; Learning rate = 0.000304 ; Loss = 1.481321\n",
      "2024-12-02 07:35:23.158000: I runner.py:310] Step = 84700 ; steps/s = 1.62, tokens/s = 82508 (39118 source, 43390 target) ; Learning rate = 0.000304 ; Loss = 1.484758\n",
      "2024-12-02 07:36:24.742000: I runner.py:310] Step = 84800 ; steps/s = 1.62, tokens/s = 82518 (39120 source, 43398 target) ; Learning rate = 0.000304 ; Loss = 1.491994\n",
      "2024-12-02 07:37:25.743000: I runner.py:310] Step = 84900 ; steps/s = 1.64, tokens/s = 81691 (38726 source, 42965 target) ; Learning rate = 0.000303 ; Loss = 1.475738\n",
      "2024-12-02 07:38:27.330000: I runner.py:310] Step = 85000 ; steps/s = 1.62, tokens/s = 82464 (39093 source, 43371 target) ; Learning rate = 0.000303 ; Loss = 1.489575\n",
      "2024-12-02 07:38:27.333000: I training.py:192] Running evaluation for step 85000\n",
      "2024-12-02 07:42:35.473000: I training.py:192] Evaluation result for step 85000: loss = 1.252641 ; perplexity = 3.499573\n",
      "2024-12-02 07:43:36.818000: I runner.py:310] Step = 85100 ; steps/s = 1.63, tokens/s = 82828 (39276 source, 43552 target) ; Learning rate = 0.000303 ; Loss = 1.481152\n",
      "2024-12-02 07:44:38.419000: I runner.py:310] Step = 85200 ; steps/s = 1.62, tokens/s = 82477 (39117 source, 43360 target) ; Learning rate = 0.000303 ; Loss = 1.487128\n",
      "2024-12-02 07:45:39.442000: I runner.py:310] Step = 85300 ; steps/s = 1.64, tokens/s = 81698 (38720 source, 42978 target) ; Learning rate = 0.000303 ; Loss = 1.486027\n",
      "2024-12-02 07:46:41.039000: I runner.py:310] Step = 85400 ; steps/s = 1.62, tokens/s = 82469 (39060 source, 43409 target) ; Learning rate = 0.000302 ; Loss = 1.477834\n",
      "2024-12-02 07:47:42.634000: I runner.py:310] Step = 85500 ; steps/s = 1.62, tokens/s = 82485 (39133 source, 43352 target) ; Learning rate = 0.000302 ; Loss = 1.480836\n",
      "2024-12-02 07:48:44.239000: I runner.py:310] Step = 85600 ; steps/s = 1.62, tokens/s = 82466 (39109 source, 43357 target) ; Learning rate = 0.000302 ; Loss = 1.479992\n",
      "2024-12-02 07:49:45.307000: I runner.py:310] Step = 85700 ; steps/s = 1.64, tokens/s = 81580 (38685 source, 42895 target) ; Learning rate = 0.000302 ; Loss = 1.480161\n",
      "2024-12-02 07:50:46.818000: I runner.py:310] Step = 85800 ; steps/s = 1.63, tokens/s = 82628 (39197 source, 43431 target) ; Learning rate = 0.000302 ; Loss = 1.477572\n",
      "2024-12-02 07:51:48.437000: I runner.py:310] Step = 85900 ; steps/s = 1.62, tokens/s = 82421 (39070 source, 43351 target) ; Learning rate = 0.000302 ; Loss = 1.488570\n",
      "2024-12-02 07:52:50.035000: I runner.py:310] Step = 86000 ; steps/s = 1.62, tokens/s = 82491 (39081 source, 43410 target) ; Learning rate = 0.000301 ; Loss = 1.485116\n",
      "2024-12-02 07:53:51.025000: I runner.py:310] Step = 86100 ; steps/s = 1.64, tokens/s = 81695 (38712 source, 42983 target) ; Learning rate = 0.000301 ; Loss = 1.471413\n",
      "2024-12-02 07:54:52.614000: I runner.py:310] Step = 86200 ; steps/s = 1.62, tokens/s = 82493 (39096 source, 43397 target) ; Learning rate = 0.000301 ; Loss = 1.479837\n",
      "2024-12-02 07:55:54.178000: I runner.py:310] Step = 86300 ; steps/s = 1.62, tokens/s = 82499 (39128 source, 43371 target) ; Learning rate = 0.000301 ; Loss = 1.482807\n",
      "2024-12-02 07:56:55.697000: I runner.py:310] Step = 86400 ; steps/s = 1.63, tokens/s = 82596 (39174 source, 43422 target) ; Learning rate = 0.000301 ; Loss = 1.481243\n",
      "2024-12-02 07:57:56.725000: I runner.py:310] Step = 86500 ; steps/s = 1.64, tokens/s = 81625 (38689 source, 42936 target) ; Learning rate = 0.000301 ; Loss = 1.471821\n",
      "2024-12-02 07:58:58.363000: I runner.py:310] Step = 86600 ; steps/s = 1.62, tokens/s = 82426 (39088 source, 43338 target) ; Learning rate = 0.000300 ; Loss = 1.483003\n",
      "2024-12-02 07:59:59.954000: I runner.py:310] Step = 86700 ; steps/s = 1.62, tokens/s = 82506 (39110 source, 43396 target) ; Learning rate = 0.000300 ; Loss = 1.483954\n",
      "2024-12-02 08:01:01.296000: I runner.py:310] Step = 86800 ; steps/s = 1.63, tokens/s = 82211 (38981 source, 43230 target) ; Learning rate = 0.000300 ; Loss = 1.483128\n",
      "2024-12-02 08:02:02.527000: I runner.py:310] Step = 86900 ; steps/s = 1.63, tokens/s = 81997 (38887 source, 43110 target) ; Learning rate = 0.000300 ; Loss = 1.483594\n",
      "2024-12-02 08:03:04.104000: I runner.py:310] Step = 87000 ; steps/s = 1.62, tokens/s = 82493 (39096 source, 43397 target) ; Learning rate = 0.000300 ; Loss = 1.477902\n",
      "2024-12-02 08:04:05.652000: I runner.py:310] Step = 87100 ; steps/s = 1.63, tokens/s = 82533 (39127 source, 43406 target) ; Learning rate = 0.000299 ; Loss = 1.477060\n",
      "2024-12-02 08:05:06.658000: I runner.py:310] Step = 87200 ; steps/s = 1.64, tokens/s = 81708 (38741 source, 42967 target) ; Learning rate = 0.000299 ; Loss = 1.477411\n",
      "2024-12-02 08:06:08.211000: I runner.py:310] Step = 87300 ; steps/s = 1.62, tokens/s = 82528 (39130 source, 43398 target) ; Learning rate = 0.000299 ; Loss = 1.474121\n",
      "2024-12-02 08:07:09.859000: I runner.py:310] Step = 87400 ; steps/s = 1.62, tokens/s = 82412 (39109 source, 43303 target) ; Learning rate = 0.000299 ; Loss = 1.481337\n",
      "2024-12-02 08:08:11.422000: I runner.py:310] Step = 87500 ; steps/s = 1.62, tokens/s = 82542 (39108 source, 43434 target) ; Learning rate = 0.000299 ; Loss = 1.477413\n",
      "2024-12-02 08:09:12.515000: I runner.py:310] Step = 87600 ; steps/s = 1.64, tokens/s = 81555 (38646 source, 42909 target) ; Learning rate = 0.000299 ; Loss = 1.477982\n",
      "2024-12-02 08:10:14.048000: I runner.py:310] Step = 87700 ; steps/s = 1.63, tokens/s = 82563 (39145 source, 43418 target) ; Learning rate = 0.000298 ; Loss = 1.478729\n",
      "2024-12-02 08:11:15.595000: I runner.py:310] Step = 87800 ; steps/s = 1.62, tokens/s = 82530 (39106 source, 43424 target) ; Learning rate = 0.000298 ; Loss = 1.480728\n",
      "2024-12-02 08:12:17.115000: I runner.py:310] Step = 87900 ; steps/s = 1.63, tokens/s = 82561 (39154 source, 43407 target) ; Learning rate = 0.000298 ; Loss = 1.481172\n",
      "2024-12-02 08:13:18.077000: I runner.py:310] Step = 88000 ; steps/s = 1.64, tokens/s = 81797 (38801 source, 42996 target) ; Learning rate = 0.000298 ; Loss = 1.475220\n",
      "2024-12-02 08:14:19.596000: I runner.py:310] Step = 88100 ; steps/s = 1.63, tokens/s = 82560 (39111 source, 43449 target) ; Learning rate = 0.000298 ; Loss = 1.479094\n",
      "2024-12-02 08:15:21.157000: I runner.py:310] Step = 88200 ; steps/s = 1.62, tokens/s = 82531 (39115 source, 43416 target) ; Learning rate = 0.000298 ; Loss = 1.479744\n",
      "2024-12-02 08:16:22.712000: I runner.py:310] Step = 88300 ; steps/s = 1.62, tokens/s = 82534 (39157 source, 43377 target) ; Learning rate = 0.000297 ; Loss = 1.477792\n",
      "2024-12-02 08:17:23.717000: I runner.py:310] Step = 88400 ; steps/s = 1.64, tokens/s = 81700 (38724 source, 42976 target) ; Learning rate = 0.000297 ; Loss = 1.477750\n",
      "2024-12-02 08:18:25.319000: I runner.py:310] Step = 88500 ; steps/s = 1.62, tokens/s = 82485 (39124 source, 43361 target) ; Learning rate = 0.000297 ; Loss = 1.478940\n",
      "2024-12-02 08:19:26.913000: I runner.py:310] Step = 88600 ; steps/s = 1.62, tokens/s = 82437 (39065 source, 43372 target) ; Learning rate = 0.000297 ; Loss = 1.477473\n",
      "2024-12-02 08:20:28.503000: I runner.py:310] Step = 88700 ; steps/s = 1.62, tokens/s = 82496 (39109 source, 43387 target) ; Learning rate = 0.000297 ; Loss = 1.478669\n",
      "2024-12-02 08:21:29.520000: I runner.py:310] Step = 88800 ; steps/s = 1.64, tokens/s = 81671 (38725 source, 42946 target) ; Learning rate = 0.000297 ; Loss = 1.475434\n",
      "2024-12-02 08:22:31.094000: I runner.py:310] Step = 88900 ; steps/s = 1.62, tokens/s = 82524 (39106 source, 43418 target) ; Learning rate = 0.000296 ; Loss = 1.472156\n",
      "2024-12-02 08:23:32.763000: I runner.py:310] Step = 89000 ; steps/s = 1.62, tokens/s = 82369 (39052 source, 43317 target) ; Learning rate = 0.000296 ; Loss = 1.479114\n",
      "2024-12-02 08:24:34.289000: I runner.py:310] Step = 89100 ; steps/s = 1.63, tokens/s = 82575 (39163 source, 43412 target) ; Learning rate = 0.000296 ; Loss = 1.484116\n",
      "2024-12-02 08:25:35.331000: I runner.py:310] Step = 89200 ; steps/s = 1.64, tokens/s = 81619 (38698 source, 42921 target) ; Learning rate = 0.000296 ; Loss = 1.476279\n",
      "2024-12-02 08:26:36.836000: I runner.py:310] Step = 89300 ; steps/s = 1.63, tokens/s = 82591 (39169 source, 43422 target) ; Learning rate = 0.000296 ; Loss = 1.479556\n",
      "2024-12-02 08:27:38.415000: I runner.py:310] Step = 89400 ; steps/s = 1.62, tokens/s = 82518 (39107 source, 43411 target) ; Learning rate = 0.000296 ; Loss = 1.480026\n",
      "2024-12-02 08:28:40.029000: I runner.py:310] Step = 89500 ; steps/s = 1.62, tokens/s = 82436 (39091 source, 43345 target) ; Learning rate = 0.000295 ; Loss = 1.485172\n",
      "2024-12-02 08:29:41.022000: I runner.py:310] Step = 89600 ; steps/s = 1.64, tokens/s = 81739 (38748 source, 42991 target) ; Learning rate = 0.000295 ; Loss = 1.483369\n",
      "2024-12-02 08:30:42.530000: I runner.py:310] Step = 89700 ; steps/s = 1.63, tokens/s = 82597 (39185 source, 43412 target) ; Learning rate = 0.000295 ; Loss = 1.481122\n",
      "2024-12-02 08:31:44.129000: I runner.py:310] Step = 89800 ; steps/s = 1.62, tokens/s = 82465 (39091 source, 43374 target) ; Learning rate = 0.000295 ; Loss = 1.477516\n",
      "2024-12-02 08:32:45.711000: I runner.py:310] Step = 89900 ; steps/s = 1.62, tokens/s = 82477 (39089 source, 43388 target) ; Learning rate = 0.000295 ; Loss = 1.478067\n",
      "2024-12-02 08:33:46.711000: I runner.py:310] Step = 90000 ; steps/s = 1.64, tokens/s = 81705 (38739 source, 42966 target) ; Learning rate = 0.000295 ; Loss = 1.486201\n",
      "2024-12-02 08:33:48.743000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-90000\n",
      "2024-12-02 08:33:48.743000: I training.py:192] Running evaluation for step 90000\n",
      "2024-12-02 08:37:53.911000: I training.py:192] Evaluation result for step 90000: loss = 1.253109 ; perplexity = 3.501212\n",
      "2024-12-02 08:38:55.309000: I runner.py:310] Step = 90100 ; steps/s = 1.63, tokens/s = 82743 (39244 source, 43499 target) ; Learning rate = 0.000294 ; Loss = 1.473702\n",
      "2024-12-02 08:39:56.889000: I runner.py:310] Step = 90200 ; steps/s = 1.62, tokens/s = 82513 (39096 source, 43417 target) ; Learning rate = 0.000294 ; Loss = 1.481759\n",
      "2024-12-02 08:40:58.485000: I runner.py:310] Step = 90300 ; steps/s = 1.62, tokens/s = 82484 (39103 source, 43381 target) ; Learning rate = 0.000294 ; Loss = 1.480515\n",
      "2024-12-02 08:41:59.468000: I runner.py:310] Step = 90400 ; steps/s = 1.64, tokens/s = 81695 (38713 source, 42982 target) ; Learning rate = 0.000294 ; Loss = 1.480600\n",
      "2024-12-02 08:43:01.020000: I runner.py:310] Step = 90500 ; steps/s = 1.62, tokens/s = 82566 (39164 source, 43402 target) ; Learning rate = 0.000294 ; Loss = 1.474804\n",
      "2024-12-02 08:44:02.598000: I runner.py:310] Step = 90600 ; steps/s = 1.62, tokens/s = 82502 (39104 source, 43398 target) ; Learning rate = 0.000294 ; Loss = 1.475104\n",
      "2024-12-02 08:45:04.220000: I runner.py:310] Step = 90700 ; steps/s = 1.62, tokens/s = 82426 (39092 source, 43334 target) ; Learning rate = 0.000293 ; Loss = 1.482661\n",
      "2024-12-02 08:46:05.256000: I runner.py:310] Step = 90800 ; steps/s = 1.64, tokens/s = 81630 (38706 source, 42924 target) ; Learning rate = 0.000293 ; Loss = 1.479530\n",
      "2024-12-02 08:47:06.825000: I runner.py:310] Step = 90900 ; steps/s = 1.62, tokens/s = 82512 (39109 source, 43403 target) ; Learning rate = 0.000293 ; Loss = 1.479977\n",
      "2024-12-02 08:48:08.445000: I runner.py:310] Step = 91000 ; steps/s = 1.62, tokens/s = 82444 (39104 source, 43340 target) ; Learning rate = 0.000293 ; Loss = 1.483804\n",
      "2024-12-02 08:49:09.943000: I runner.py:310] Step = 91100 ; steps/s = 1.63, tokens/s = 82396 (39050 source, 43346 target) ; Learning rate = 0.000293 ; Loss = 1.489768\n",
      "2024-12-02 08:50:10.981000: I runner.py:310] Step = 91200 ; steps/s = 1.64, tokens/s = 81866 (38806 source, 43060 target) ; Learning rate = 0.000293 ; Loss = 1.473410\n",
      "2024-12-02 08:51:12.561000: I runner.py:310] Step = 91300 ; steps/s = 1.62, tokens/s = 82485 (39114 source, 43371 target) ; Learning rate = 0.000293 ; Loss = 1.480135\n",
      "2024-12-02 08:52:14.087000: I runner.py:310] Step = 91400 ; steps/s = 1.63, tokens/s = 82563 (39135 source, 43428 target) ; Learning rate = 0.000292 ; Loss = 1.482665\n",
      "2024-12-02 08:53:15.159000: I runner.py:310] Step = 91500 ; steps/s = 1.64, tokens/s = 81625 (38703 source, 42922 target) ; Learning rate = 0.000292 ; Loss = 1.481835\n",
      "2024-12-02 08:54:16.812000: I runner.py:310] Step = 91600 ; steps/s = 1.62, tokens/s = 82414 (39064 source, 43350 target) ; Learning rate = 0.000292 ; Loss = 1.475660\n",
      "2024-12-02 08:55:18.388000: I runner.py:310] Step = 91700 ; steps/s = 1.62, tokens/s = 82470 (39085 source, 43385 target) ; Learning rate = 0.000292 ; Loss = 1.481332\n",
      "2024-12-02 08:56:19.987000: I runner.py:310] Step = 91800 ; steps/s = 1.62, tokens/s = 82503 (39113 source, 43390 target) ; Learning rate = 0.000292 ; Loss = 1.484894\n",
      "2024-12-02 08:57:20.995000: I runner.py:310] Step = 91900 ; steps/s = 1.64, tokens/s = 81666 (38722 source, 42944 target) ; Learning rate = 0.000292 ; Loss = 1.483755\n",
      "2024-12-02 08:58:22.562000: I runner.py:310] Step = 92000 ; steps/s = 1.62, tokens/s = 82522 (39126 source, 43396 target) ; Learning rate = 0.000291 ; Loss = 1.487718\n",
      "2024-12-02 08:59:24.164000: I runner.py:310] Step = 92100 ; steps/s = 1.62, tokens/s = 82484 (39146 source, 43338 target) ; Learning rate = 0.000291 ; Loss = 1.475595\n",
      "2024-12-02 09:00:25.717000: I runner.py:310] Step = 92200 ; steps/s = 1.62, tokens/s = 82505 (39098 source, 43407 target) ; Learning rate = 0.000291 ; Loss = 1.478860\n",
      "2024-12-02 09:01:26.728000: I runner.py:310] Step = 92300 ; steps/s = 1.64, tokens/s = 81696 (38722 source, 42974 target) ; Learning rate = 0.000291 ; Loss = 1.471908\n",
      "2024-12-02 09:02:28.362000: I runner.py:310] Step = 92400 ; steps/s = 1.62, tokens/s = 82435 (39077 source, 43358 target) ; Learning rate = 0.000291 ; Loss = 1.476543\n",
      "2024-12-02 09:03:29.926000: I runner.py:310] Step = 92500 ; steps/s = 1.62, tokens/s = 82536 (39111 source, 43425 target) ; Learning rate = 0.000291 ; Loss = 1.476473\n",
      "2024-12-02 09:04:31.500000: I runner.py:310] Step = 92600 ; steps/s = 1.62, tokens/s = 82495 (39120 source, 43375 target) ; Learning rate = 0.000290 ; Loss = 1.477879\n",
      "2024-12-02 09:05:32.522000: I runner.py:310] Step = 92700 ; steps/s = 1.64, tokens/s = 81664 (38732 source, 42932 target) ; Learning rate = 0.000290 ; Loss = 1.474646\n",
      "2024-12-02 09:06:34.073000: I runner.py:310] Step = 92800 ; steps/s = 1.62, tokens/s = 82519 (39097 source, 43422 target) ; Learning rate = 0.000290 ; Loss = 1.478252\n",
      "2024-12-02 09:07:35.612000: I runner.py:310] Step = 92900 ; steps/s = 1.63, tokens/s = 82527 (39123 source, 43404 target) ; Learning rate = 0.000290 ; Loss = 1.482907\n",
      "2024-12-02 09:08:37.145000: I runner.py:310] Step = 93000 ; steps/s = 1.63, tokens/s = 82592 (39198 source, 43394 target) ; Learning rate = 0.000290 ; Loss = 1.484672\n",
      "2024-12-02 09:09:38.129000: I runner.py:310] Step = 93100 ; steps/s = 1.64, tokens/s = 81740 (38722 source, 43018 target) ; Learning rate = 0.000290 ; Loss = 1.486768\n",
      "2024-12-02 09:10:39.749000: I runner.py:310] Step = 93200 ; steps/s = 1.62, tokens/s = 82419 (39088 source, 43331 target) ; Learning rate = 0.000290 ; Loss = 1.473759\n",
      "2024-12-02 09:11:41.426000: I runner.py:310] Step = 93300 ; steps/s = 1.62, tokens/s = 82354 (39050 source, 43304 target) ; Learning rate = 0.000289 ; Loss = 1.478384\n",
      "2024-12-02 09:12:43.009000: I runner.py:310] Step = 93400 ; steps/s = 1.62, tokens/s = 82516 (39125 source, 43391 target) ; Learning rate = 0.000289 ; Loss = 1.474059\n",
      "2024-12-02 09:13:44.067000: I runner.py:310] Step = 93500 ; steps/s = 1.64, tokens/s = 81618 (38680 source, 42938 target) ; Learning rate = 0.000289 ; Loss = 1.477115\n",
      "2024-12-02 09:14:45.676000: I runner.py:310] Step = 93600 ; steps/s = 1.62, tokens/s = 82446 (39089 source, 43357 target) ; Learning rate = 0.000289 ; Loss = 1.479124\n",
      "2024-12-02 09:15:47.284000: I runner.py:310] Step = 93700 ; steps/s = 1.62, tokens/s = 82463 (39091 source, 43372 target) ; Learning rate = 0.000289 ; Loss = 1.479006\n",
      "2024-12-02 09:16:48.826000: I runner.py:310] Step = 93800 ; steps/s = 1.63, tokens/s = 82552 (39152 source, 43400 target) ; Learning rate = 0.000289 ; Loss = 1.479128\n",
      "2024-12-02 09:17:49.840000: I runner.py:310] Step = 93900 ; steps/s = 1.64, tokens/s = 81695 (38712 source, 42983 target) ; Learning rate = 0.000288 ; Loss = 1.479457\n",
      "2024-12-02 09:18:51.396000: I runner.py:310] Step = 94000 ; steps/s = 1.62, tokens/s = 82530 (39126 source, 43404 target) ; Learning rate = 0.000288 ; Loss = 1.475460\n",
      "2024-12-02 09:19:53.022000: I runner.py:310] Step = 94100 ; steps/s = 1.62, tokens/s = 82412 (39099 source, 43313 target) ; Learning rate = 0.000288 ; Loss = 1.479293\n",
      "2024-12-02 09:20:54.592000: I runner.py:310] Step = 94200 ; steps/s = 1.62, tokens/s = 82524 (39109 source, 43415 target) ; Learning rate = 0.000288 ; Loss = 1.478790\n",
      "2024-12-02 09:21:55.650000: I runner.py:310] Step = 94300 ; steps/s = 1.64, tokens/s = 81624 (38701 source, 42923 target) ; Learning rate = 0.000288 ; Loss = 1.468017\n",
      "2024-12-02 09:22:57.160000: I runner.py:310] Step = 94400 ; steps/s = 1.63, tokens/s = 82596 (39167 source, 43429 target) ; Learning rate = 0.000288 ; Loss = 1.478343\n",
      "2024-12-02 09:23:58.799000: I runner.py:310] Step = 94500 ; steps/s = 1.62, tokens/s = 82452 (39096 source, 43356 target) ; Learning rate = 0.000288 ; Loss = 1.478344\n",
      "2024-12-02 09:25:00.421000: I runner.py:310] Step = 94600 ; steps/s = 1.62, tokens/s = 82417 (39060 source, 43357 target) ; Learning rate = 0.000287 ; Loss = 1.483667\n",
      "2024-12-02 09:26:01.420000: I runner.py:310] Step = 94700 ; steps/s = 1.64, tokens/s = 81696 (38729 source, 42967 target) ; Learning rate = 0.000287 ; Loss = 1.475915\n",
      "2024-12-02 09:27:02.996000: I runner.py:310] Step = 94800 ; steps/s = 1.62, tokens/s = 82491 (39118 source, 43373 target) ; Learning rate = 0.000287 ; Loss = 1.478613\n",
      "2024-12-02 09:28:04.548000: I runner.py:310] Step = 94900 ; steps/s = 1.62, tokens/s = 82546 (39102 source, 43444 target) ; Learning rate = 0.000287 ; Loss = 1.478010\n",
      "2024-12-02 09:29:06.142000: I runner.py:310] Step = 95000 ; steps/s = 1.62, tokens/s = 82486 (39126 source, 43360 target) ; Learning rate = 0.000287 ; Loss = 1.479593\n",
      "2024-12-02 09:29:06.145000: I training.py:192] Running evaluation for step 95000\n",
      "2024-12-02 09:33:09.283000: I training.py:192] Evaluation result for step 95000: loss = 1.260566 ; perplexity = 3.527419\n",
      "2024-12-02 09:34:10.115000: I runner.py:310] Step = 95100 ; steps/s = 1.64, tokens/s = 81931 (38834 source, 43097 target) ; Learning rate = 0.000287 ; Loss = 1.476972\n",
      "2024-12-02 09:35:11.620000: I runner.py:310] Step = 95200 ; steps/s = 1.63, tokens/s = 82611 (39187 source, 43424 target) ; Learning rate = 0.000286 ; Loss = 1.471402\n",
      "2024-12-02 09:36:13.185000: I runner.py:310] Step = 95300 ; steps/s = 1.62, tokens/s = 82498 (39094 source, 43404 target) ; Learning rate = 0.000286 ; Loss = 1.478297\n",
      "2024-12-02 09:37:14.768000: I runner.py:310] Step = 95400 ; steps/s = 1.62, tokens/s = 82492 (39118 source, 43374 target) ; Learning rate = 0.000286 ; Loss = 1.476023\n",
      "2024-12-02 09:38:15.789000: I runner.py:310] Step = 95500 ; steps/s = 1.64, tokens/s = 81656 (38706 source, 42950 target) ; Learning rate = 0.000286 ; Loss = 1.477965\n",
      "2024-12-02 09:39:17.368000: I runner.py:310] Step = 95600 ; steps/s = 1.62, tokens/s = 82512 (39105 source, 43407 target) ; Learning rate = 0.000286 ; Loss = 1.481042\n",
      "2024-12-02 09:40:18.889000: I runner.py:310] Step = 95700 ; steps/s = 1.63, tokens/s = 82584 (39170 source, 43414 target) ; Learning rate = 0.000286 ; Loss = 1.478499\n",
      "2024-12-02 09:41:19.866000: I runner.py:310] Step = 95800 ; steps/s = 1.64, tokens/s = 81709 (38738 source, 42971 target) ; Learning rate = 0.000286 ; Loss = 1.472256\n",
      "2024-12-02 09:42:21.488000: I runner.py:310] Step = 95900 ; steps/s = 1.62, tokens/s = 82414 (39047 source, 43367 target) ; Learning rate = 0.000285 ; Loss = 1.472133\n",
      "2024-12-02 09:43:23.073000: I runner.py:310] Step = 96000 ; steps/s = 1.62, tokens/s = 82505 (39117 source, 43388 target) ; Learning rate = 0.000285 ; Loss = 1.485324\n",
      "2024-12-02 09:44:24.681000: I runner.py:310] Step = 96100 ; steps/s = 1.62, tokens/s = 82476 (39099 source, 43377 target) ; Learning rate = 0.000285 ; Loss = 1.476536\n",
      "2024-12-02 09:45:25.755000: I runner.py:310] Step = 96200 ; steps/s = 1.64, tokens/s = 81604 (38710 source, 42894 target) ; Learning rate = 0.000285 ; Loss = 1.478950\n",
      "2024-12-02 09:46:27.298000: I runner.py:310] Step = 96300 ; steps/s = 1.63, tokens/s = 82567 (39157 source, 43410 target) ; Learning rate = 0.000285 ; Loss = 1.475192\n",
      "2024-12-02 09:47:28.859000: I runner.py:310] Step = 96400 ; steps/s = 1.62, tokens/s = 82503 (39120 source, 43383 target) ; Learning rate = 0.000285 ; Loss = 1.478161\n",
      "2024-12-02 09:48:30.396000: I runner.py:310] Step = 96500 ; steps/s = 1.63, tokens/s = 82557 (39128 source, 43429 target) ; Learning rate = 0.000285 ; Loss = 1.478379\n",
      "2024-12-02 09:49:31.372000: I runner.py:310] Step = 96600 ; steps/s = 1.64, tokens/s = 81722 (38737 source, 42985 target) ; Learning rate = 0.000284 ; Loss = 1.479066\n",
      "2024-12-02 09:50:33.005000: I runner.py:310] Step = 96700 ; steps/s = 1.62, tokens/s = 82419 (39089 source, 43330 target) ; Learning rate = 0.000284 ; Loss = 1.479999\n",
      "2024-12-02 09:51:34.592000: I runner.py:310] Step = 96800 ; steps/s = 1.62, tokens/s = 82495 (39131 source, 43364 target) ; Learning rate = 0.000284 ; Loss = 1.478688\n",
      "2024-12-02 09:52:36.199000: I runner.py:310] Step = 96900 ; steps/s = 1.62, tokens/s = 82464 (39086 source, 43378 target) ; Learning rate = 0.000284 ; Loss = 1.480502\n",
      "2024-12-02 09:53:38.633000: I runner.py:310] Step = 97000 ; steps/s = 1.60, tokens/s = 79814 (37822 source, 41992 target) ; Learning rate = 0.000284 ; Loss = 1.476987\n",
      "2024-12-02 09:54:41.835000: I runner.py:310] Step = 97100 ; steps/s = 1.58, tokens/s = 80391 (38102 source, 42289 target) ; Learning rate = 0.000284 ; Loss = 1.477236\n",
      "2024-12-02 09:55:43.577000: I runner.py:310] Step = 97200 ; steps/s = 1.62, tokens/s = 82288 (39022 source, 43266 target) ; Learning rate = 0.000284 ; Loss = 1.474941\n",
      "2024-12-02 09:56:45.246000: I runner.py:310] Step = 97300 ; steps/s = 1.62, tokens/s = 82376 (39041 source, 43335 target) ; Learning rate = 0.000283 ; Loss = 1.471937\n",
      "2024-12-02 09:57:46.329000: I runner.py:310] Step = 97400 ; steps/s = 1.64, tokens/s = 81545 (38672 source, 42873 target) ; Learning rate = 0.000283 ; Loss = 1.480855\n",
      "2024-12-02 09:58:47.912000: I runner.py:310] Step = 97500 ; steps/s = 1.62, tokens/s = 82538 (39127 source, 43411 target) ; Learning rate = 0.000283 ; Loss = 1.473901\n",
      "2024-12-02 09:59:49.564000: I runner.py:310] Step = 97600 ; steps/s = 1.62, tokens/s = 82396 (39087 source, 43309 target) ; Learning rate = 0.000283 ; Loss = 1.478671\n",
      "2024-12-02 10:00:51.214000: I runner.py:310] Step = 97700 ; steps/s = 1.62, tokens/s = 82385 (39037 source, 43348 target) ; Learning rate = 0.000283 ; Loss = 1.474848\n",
      "2024-12-02 10:01:52.351000: I runner.py:310] Step = 97800 ; steps/s = 1.64, tokens/s = 81519 (38642 source, 42877 target) ; Learning rate = 0.000283 ; Loss = 1.477717\n",
      "2024-12-02 10:02:54.025000: I runner.py:310] Step = 97900 ; steps/s = 1.62, tokens/s = 82411 (39064 source, 43347 target) ; Learning rate = 0.000282 ; Loss = 1.473485\n",
      "2024-12-02 10:03:55.705000: I runner.py:310] Step = 98000 ; steps/s = 1.62, tokens/s = 82336 (39062 source, 43274 target) ; Learning rate = 0.000282 ; Loss = 1.477546\n",
      "2024-12-02 10:04:57.341000: I runner.py:310] Step = 98100 ; steps/s = 1.62, tokens/s = 82436 (39060 source, 43376 target) ; Learning rate = 0.000282 ; Loss = 1.475029\n",
      "2024-12-02 10:05:58.405000: I runner.py:310] Step = 98200 ; steps/s = 1.64, tokens/s = 81603 (38692 source, 42911 target) ; Learning rate = 0.000282 ; Loss = 1.475633\n",
      "2024-12-02 10:07:00.079000: I runner.py:310] Step = 98300 ; steps/s = 1.62, tokens/s = 82362 (39021 source, 43341 target) ; Learning rate = 0.000282 ; Loss = 1.469844\n",
      "2024-12-02 10:08:01.697000: I runner.py:310] Step = 98400 ; steps/s = 1.62, tokens/s = 82446 (39120 source, 43326 target) ; Learning rate = 0.000282 ; Loss = 1.472567\n",
      "2024-12-02 10:09:03.337000: I runner.py:310] Step = 98500 ; steps/s = 1.62, tokens/s = 82402 (39055 source, 43347 target) ; Learning rate = 0.000282 ; Loss = 1.474822\n",
      "2024-12-02 10:10:04.365000: I runner.py:310] Step = 98600 ; steps/s = 1.64, tokens/s = 81679 (38734 source, 42945 target) ; Learning rate = 0.000281 ; Loss = 1.479367\n",
      "2024-12-02 10:11:06.084000: I runner.py:310] Step = 98700 ; steps/s = 1.62, tokens/s = 82303 (39010 source, 43293 target) ; Learning rate = 0.000281 ; Loss = 1.471345\n",
      "2024-12-02 10:12:07.673000: I runner.py:310] Step = 98800 ; steps/s = 1.62, tokens/s = 82504 (39116 source, 43388 target) ; Learning rate = 0.000281 ; Loss = 1.471676\n",
      "2024-12-02 10:13:09.350000: I runner.py:310] Step = 98900 ; steps/s = 1.62, tokens/s = 82362 (39054 source, 43308 target) ; Learning rate = 0.000281 ; Loss = 1.480010\n",
      "2024-12-02 10:14:10.414000: I runner.py:310] Step = 99000 ; steps/s = 1.64, tokens/s = 81577 (38624 source, 42953 target) ; Learning rate = 0.000281 ; Loss = 1.469250\n",
      "2024-12-02 10:15:12.088000: I runner.py:310] Step = 99100 ; steps/s = 1.62, tokens/s = 82402 (39084 source, 43318 target) ; Learning rate = 0.000281 ; Loss = 1.481357\n",
      "2024-12-02 10:16:13.669000: I runner.py:310] Step = 99200 ; steps/s = 1.62, tokens/s = 82488 (39123 source, 43365 target) ; Learning rate = 0.000281 ; Loss = 1.479705\n",
      "2024-12-02 10:17:15.237000: I runner.py:310] Step = 99300 ; steps/s = 1.62, tokens/s = 82510 (39136 source, 43374 target) ; Learning rate = 0.000280 ; Loss = 1.478417\n",
      "2024-12-02 10:18:16.313000: I runner.py:310] Step = 99400 ; steps/s = 1.64, tokens/s = 81602 (38686 source, 42916 target) ; Learning rate = 0.000280 ; Loss = 1.476198\n",
      "2024-12-02 10:19:18.033000: I runner.py:310] Step = 99500 ; steps/s = 1.62, tokens/s = 82310 (39017 source, 43293 target) ; Learning rate = 0.000280 ; Loss = 1.467601\n",
      "2024-12-02 10:20:19.740000: I runner.py:310] Step = 99600 ; steps/s = 1.62, tokens/s = 82317 (39031 source, 43286 target) ; Learning rate = 0.000280 ; Loss = 1.474482\n",
      "2024-12-02 10:21:21.435000: I runner.py:310] Step = 99700 ; steps/s = 1.62, tokens/s = 82342 (39044 source, 43298 target) ; Learning rate = 0.000280 ; Loss = 1.481675\n",
      "2024-12-02 10:22:22.558000: I runner.py:310] Step = 99800 ; steps/s = 1.64, tokens/s = 81527 (38647 source, 42880 target) ; Learning rate = 0.000280 ; Loss = 1.476733\n",
      "2024-12-02 10:23:25.350000: I runner.py:310] Step = 99900 ; steps/s = 1.59, tokens/s = 80907 (38362 source, 42545 target) ; Learning rate = 0.000280 ; Loss = 1.474799\n",
      "2024-12-02 10:24:27.699000: I runner.py:310] Step = 100000 ; steps/s = 1.60, tokens/s = 81498 (38642 source, 42856 target) ; Learning rate = 0.000280 ; Loss = 1.476305\n",
      "2024-12-02 10:24:29.222000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab/ckpt-100000\n",
      "2024-12-02 10:24:29.222000: I training.py:192] Running evaluation for step 100000\n",
      "2024-12-02 10:28:28.272000: I training.py:192] Evaluation result for step 100000: loss = 1.267096 ; perplexity = 3.550526\n",
      "2024-12-02 10:29:29.391000: I runner.py:310] Step = 100100 ; steps/s = 1.64, tokens/s = 81527 (38649 source, 42878 target) ; Learning rate = 0.000279 ; Loss = 1.478591\n",
      "2024-12-02 10:30:30.967000: I runner.py:310] Step = 100200 ; steps/s = 1.62, tokens/s = 82502 (39084 source, 43418 target) ; Learning rate = 0.000279 ; Loss = 1.470819\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Kk-En -> Tr-En (Tatoeba)\n",
    "!onmt-main --model kk-tr-en-shared.py --config data.yml --auto_config train --with_eval --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35f6fe27-9069-471c-8313-cc0249e113d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-02 10:41:29.858199: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-02 10:41:30.642218: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-02 10:41:30.642283: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-02 10:41:30.642291: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-02 10:41:31.646000: I onmt-main:8] Creating model directory KK-TR-EN-Shared-vocab-2\n",
      "2024-12-02 10:41:31.834000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-02 10:41:31.834000: I main.py:315] Using model:\n",
      "(model): MyCustomTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): WordEmbedder()\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): WordEmbedder()\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-02 10:41:31.836722: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-02 10:41:33.378456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-02 10:41:33.379178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7715 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-12-02 10:41:33.379751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 6099 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "2024-12-02 10:41:33.383000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file: Tatoeba_tokens_dev_shared\n",
      "  eval_labels_file: Tatoeba_dev_target_tokens_shared.txt\n",
      "  source_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file: Tatoeba_tokens_train_shared\n",
      "  train_labels_file: Tatoeba_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: KK-TR-EN-Shared-vocab-2\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 250000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-02 10:41:33.707000: I inputter.py:316] Initialized source input layer:\n",
      "2024-12-02 10:41:33.708000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-02 10:41:33.708000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-02 10:41:33.780000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-02 10:41:33.780000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-02 10:41:33.780000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-02 10:41:33.801000: I runner.py:269] Restored checkpoint KK-TR-EN-Shared-vocab/ckpt-100000\n",
      "2024-12-02 10:41:33.803000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "2024-12-02 10:41:33.846000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-02 10:41:34.741002: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-02 10:41:34.858000: I main.py:325] Accumulate gradients of 7 iterations to reach effective batch size of 25000\n",
      "2024-12-02 10:41:34.979000: I mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2024-12-02 10:41:35.092000: I dataset_ops.py:2542] Training on 647485 examples\n",
      "2024-12-02 10:42:40.631185: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-02 10:42:41.763056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-02 10:42:41.965180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-02 10:42:51.055000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-02 10:42:51.079000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-02 10:42:52.639000: I cross_device_ops.py:897] batch_all_reduce: 260 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-02 10:42:57.088000: I cross_device_ops.py:897] batch_all_reduce: 260 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-02 10:43:03.340000: I runner.py:310] Number of model parameters: 93326081\n",
      "2024-12-02 10:43:03.344000: I runner.py:310] Number of model weights: 260 (trainable = 260, non trainable = 0)\n",
      "2024-12-02 10:43:03.379000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-02 10:43:03.387000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-02 10:43:05.460000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-1\n",
      "2024-12-02 10:43:06.174000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-02 10:43:06.198000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-02 10:43:06.855000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-02 10:43:06.878000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-02 10:43:07.499000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-02 10:43:07.523000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-02 10:44:06.355000: I runner.py:310] Step = 100 ; steps/s = 1.63, tokens/s = 77792 (34234 source, 43558 target) ; Learning rate = 0.000009 ; Loss = 6.783453\n",
      "2024-12-02 10:45:07.700000: I runner.py:310] Step = 200 ; steps/s = 1.63, tokens/s = 77699 (34229 source, 43470 target) ; Learning rate = 0.000018 ; Loss = 5.624819\n",
      "2024-12-02 10:46:08.264000: I runner.py:310] Step = 300 ; steps/s = 1.65, tokens/s = 77286 (34073 source, 43213 target) ; Learning rate = 0.000027 ; Loss = 5.150777\n",
      "2024-12-02 10:47:08.929000: I runner.py:310] Step = 400 ; steps/s = 1.65, tokens/s = 78555 (34594 source, 43961 target) ; Learning rate = 0.000035 ; Loss = 4.979113\n",
      "2024-12-02 10:48:09.610000: I runner.py:310] Step = 500 ; steps/s = 1.65, tokens/s = 78538 (34596 source, 43942 target) ; Learning rate = 0.000044 ; Loss = 4.735331\n",
      "2024-12-02 10:49:09.756000: I runner.py:310] Step = 600 ; steps/s = 1.66, tokens/s = 77796 (34280 source, 43516 target) ; Learning rate = 0.000053 ; Loss = 4.530217\n",
      "2024-12-02 10:50:10.393000: I runner.py:310] Step = 700 ; steps/s = 1.65, tokens/s = 78594 (34626 source, 43968 target) ; Learning rate = 0.000062 ; Loss = 4.412112\n",
      "2024-12-02 10:51:10.589000: I runner.py:310] Step = 800 ; steps/s = 1.66, tokens/s = 77778 (34296 source, 43482 target) ; Learning rate = 0.000071 ; Loss = 4.106139\n",
      "2024-12-02 10:52:11.222000: I runner.py:310] Step = 900 ; steps/s = 1.65, tokens/s = 78556 (34575 source, 43981 target) ; Learning rate = 0.000080 ; Loss = 3.857601\n",
      "2024-12-02 10:53:11.886000: I runner.py:310] Step = 1000 ; steps/s = 1.65, tokens/s = 78581 (34625 source, 43956 target) ; Learning rate = 0.000088 ; Loss = 3.511087\n",
      "2024-12-02 10:54:12.053000: I runner.py:310] Step = 1100 ; steps/s = 1.66, tokens/s = 77806 (34310 source, 43496 target) ; Learning rate = 0.000097 ; Loss = 3.317749\n",
      "2024-12-02 10:55:12.638000: I runner.py:310] Step = 1200 ; steps/s = 1.65, tokens/s = 78665 (34650 source, 44015 target) ; Learning rate = 0.000106 ; Loss = 3.217192\n",
      "2024-12-02 10:56:12.783000: I runner.py:310] Step = 1300 ; steps/s = 1.66, tokens/s = 77839 (34320 source, 43519 target) ; Learning rate = 0.000115 ; Loss = 2.767023\n",
      "2024-12-02 10:57:13.331000: I runner.py:310] Step = 1400 ; steps/s = 1.65, tokens/s = 78684 (34637 source, 44047 target) ; Learning rate = 0.000124 ; Loss = 2.661947\n",
      "2024-12-02 10:58:13.949000: I runner.py:310] Step = 1500 ; steps/s = 1.65, tokens/s = 78615 (34627 source, 43988 target) ; Learning rate = 0.000133 ; Loss = 2.651446\n",
      "2024-12-02 10:59:14.078000: I runner.py:310] Step = 1600 ; steps/s = 1.66, tokens/s = 77842 (34320 source, 43522 target) ; Learning rate = 0.000142 ; Loss = 2.494593\n",
      "2024-12-02 11:00:14.793000: I runner.py:310] Step = 1700 ; steps/s = 1.65, tokens/s = 78501 (34567 source, 43934 target) ; Learning rate = 0.000150 ; Loss = 2.432493\n",
      "2024-12-02 11:01:15.411000: I runner.py:310] Step = 1800 ; steps/s = 1.65, tokens/s = 78599 (34614 source, 43985 target) ; Learning rate = 0.000159 ; Loss = 2.413857\n",
      "2024-12-02 11:02:15.630000: I runner.py:310] Step = 1900 ; steps/s = 1.66, tokens/s = 77724 (34266 source, 43458 target) ; Learning rate = 0.000168 ; Loss = 2.379181\n",
      "2024-12-02 11:03:16.272000: I runner.py:310] Step = 2000 ; steps/s = 1.65, tokens/s = 78595 (34618 source, 43977 target) ; Learning rate = 0.000177 ; Loss = 2.400509\n",
      "2024-12-02 11:04:16.476000: I runner.py:310] Step = 2100 ; steps/s = 1.66, tokens/s = 77738 (34270 source, 43468 target) ; Learning rate = 0.000186 ; Loss = 2.295210\n",
      "2024-12-02 11:05:17.149000: I runner.py:310] Step = 2200 ; steps/s = 1.65, tokens/s = 78537 (34586 source, 43951 target) ; Learning rate = 0.000195 ; Loss = 2.275128\n",
      "2024-12-02 11:06:17.812000: I runner.py:310] Step = 2300 ; steps/s = 1.65, tokens/s = 78560 (34602 source, 43958 target) ; Learning rate = 0.000203 ; Loss = 2.199406\n",
      "2024-12-02 11:07:17.986000: I runner.py:310] Step = 2400 ; steps/s = 1.66, tokens/s = 77774 (34274 source, 43500 target) ; Learning rate = 0.000212 ; Loss = 2.189535\n",
      "2024-12-02 11:08:18.629000: I runner.py:310] Step = 2500 ; steps/s = 1.65, tokens/s = 78582 (34612 source, 43970 target) ; Learning rate = 0.000221 ; Loss = 2.230145\n",
      "2024-12-02 11:09:18.779000: I runner.py:310] Step = 2600 ; steps/s = 1.66, tokens/s = 77839 (34337 source, 43502 target) ; Learning rate = 0.000230 ; Loss = 2.161594\n",
      "2024-12-02 11:10:19.392000: I runner.py:310] Step = 2700 ; steps/s = 1.65, tokens/s = 78593 (34602 source, 43991 target) ; Learning rate = 0.000239 ; Loss = 2.083828\n",
      "2024-12-02 11:11:20.026000: I runner.py:310] Step = 2800 ; steps/s = 1.65, tokens/s = 78597 (34609 source, 43988 target) ; Learning rate = 0.000248 ; Loss = 2.039476\n",
      "2024-12-02 11:12:20.258000: I runner.py:310] Step = 2900 ; steps/s = 1.66, tokens/s = 77686 (34239 source, 43447 target) ; Learning rate = 0.000256 ; Loss = 2.114854\n",
      "2024-12-02 11:13:20.935000: I runner.py:310] Step = 3000 ; steps/s = 1.65, tokens/s = 78527 (34584 source, 43943 target) ; Learning rate = 0.000265 ; Loss = 2.061752\n",
      "2024-12-02 11:14:21.652000: I runner.py:310] Step = 3100 ; steps/s = 1.65, tokens/s = 78511 (34587 source, 43924 target) ; Learning rate = 0.000274 ; Loss = 2.047026\n",
      "2024-12-02 11:15:21.839000: I runner.py:310] Step = 3200 ; steps/s = 1.66, tokens/s = 77813 (34327 source, 43486 target) ; Learning rate = 0.000283 ; Loss = 1.987482\n",
      "2024-12-02 11:16:22.447000: I runner.py:310] Step = 3300 ; steps/s = 1.65, tokens/s = 78611 (34611 source, 44000 target) ; Learning rate = 0.000292 ; Loss = 2.004927\n",
      "2024-12-02 11:17:22.627000: I runner.py:310] Step = 3400 ; steps/s = 1.66, tokens/s = 77769 (34283 source, 43486 target) ; Learning rate = 0.000301 ; Loss = 2.014385\n",
      "2024-12-02 11:18:23.263000: I runner.py:310] Step = 3500 ; steps/s = 1.65, tokens/s = 78597 (34606 source, 43991 target) ; Learning rate = 0.000309 ; Loss = 1.958501\n",
      "2024-12-02 11:19:23.907000: I runner.py:310] Step = 3600 ; steps/s = 1.65, tokens/s = 78580 (34620 source, 43960 target) ; Learning rate = 0.000318 ; Loss = 2.015309\n",
      "2024-12-02 11:20:25.003000: I runner.py:310] Step = 3700 ; steps/s = 1.64, tokens/s = 76588 (33736 source, 42852 target) ; Learning rate = 0.000327 ; Loss = 1.945432\n",
      "2024-12-02 11:21:26.556000: I runner.py:310] Step = 3800 ; steps/s = 1.62, tokens/s = 77414 (34095 source, 43319 target) ; Learning rate = 0.000336 ; Loss = 1.943073\n",
      "2024-12-02 11:22:28.902000: I runner.py:310] Step = 3900 ; steps/s = 1.60, tokens/s = 75107 (33140 source, 41967 target) ; Learning rate = 0.000345 ; Loss = 1.906582\n",
      "2024-12-02 11:23:29.529000: I runner.py:310] Step = 4000 ; steps/s = 1.65, tokens/s = 78591 (34601 source, 43990 target) ; Learning rate = 0.000354 ; Loss = 1.916277\n",
      "2024-12-02 11:24:30.197000: I runner.py:310] Step = 4100 ; steps/s = 1.65, tokens/s = 78538 (34585 source, 43953 target) ; Learning rate = 0.000362 ; Loss = 1.884291\n",
      "2024-12-02 11:25:30.379000: I runner.py:310] Step = 4200 ; steps/s = 1.66, tokens/s = 77764 (34284 source, 43480 target) ; Learning rate = 0.000371 ; Loss = 1.873190\n",
      "2024-12-02 11:26:31.012000: I runner.py:310] Step = 4300 ; steps/s = 1.65, tokens/s = 78594 (34609 source, 43985 target) ; Learning rate = 0.000380 ; Loss = 1.920190\n",
      "2024-12-02 11:27:31.581000: I runner.py:310] Step = 4400 ; steps/s = 1.65, tokens/s = 78700 (34670 source, 44030 target) ; Learning rate = 0.000389 ; Loss = 1.925007\n",
      "2024-12-02 11:28:31.782000: I runner.py:310] Step = 4500 ; steps/s = 1.66, tokens/s = 77739 (34266 source, 43473 target) ; Learning rate = 0.000398 ; Loss = 1.880535\n",
      "2024-12-02 11:29:32.503000: I runner.py:310] Step = 4600 ; steps/s = 1.65, tokens/s = 78495 (34579 source, 43916 target) ; Learning rate = 0.000407 ; Loss = 1.940878\n",
      "2024-12-02 11:30:32.718000: I runner.py:310] Step = 4700 ; steps/s = 1.66, tokens/s = 77722 (34255 source, 43467 target) ; Learning rate = 0.000416 ; Loss = 1.840437\n",
      "2024-12-02 11:31:33.646000: I runner.py:310] Step = 4800 ; steps/s = 1.64, tokens/s = 78204 (34434 source, 43770 target) ; Learning rate = 0.000424 ; Loss = 1.817382\n",
      "2024-12-02 11:32:34.327000: I runner.py:310] Step = 4900 ; steps/s = 1.65, tokens/s = 78551 (34602 source, 43949 target) ; Learning rate = 0.000433 ; Loss = 1.848525\n",
      "2024-12-02 11:33:34.828000: I runner.py:310] Step = 5000 ; steps/s = 1.65, tokens/s = 77366 (34116 source, 43250 target) ; Learning rate = 0.000442 ; Loss = 1.821324\n",
      "2024-12-02 11:33:34.829000: I training.py:192] Running evaluation for step 5000\n",
      "2024-12-02 11:34:36.490000: I training.py:192] Evaluation result for step 5000: loss = 0.682308 ; perplexity = 1.978438\n",
      "2024-12-02 11:35:37.195000: I runner.py:310] Step = 5100 ; steps/s = 1.65, tokens/s = 78493 (34550 source, 43943 target) ; Learning rate = 0.000451 ; Loss = 1.854527\n",
      "2024-12-02 11:36:37.620000: I runner.py:310] Step = 5200 ; steps/s = 1.66, tokens/s = 77498 (34192 source, 43306 target) ; Learning rate = 0.000460 ; Loss = 1.786509\n",
      "2024-12-02 11:37:38.510000: I runner.py:310] Step = 5300 ; steps/s = 1.64, tokens/s = 78263 (34454 source, 43809 target) ; Learning rate = 0.000469 ; Loss = 1.854068\n",
      "2024-12-02 11:38:39.595000: I runner.py:310] Step = 5400 ; steps/s = 1.64, tokens/s = 78005 (34353 source, 43652 target) ; Learning rate = 0.000477 ; Loss = 1.795363\n",
      "2024-12-02 11:39:40.066000: I runner.py:310] Step = 5500 ; steps/s = 1.65, tokens/s = 77394 (34120 source, 43274 target) ; Learning rate = 0.000486 ; Loss = 1.781916\n",
      "2024-12-02 11:40:40.934000: I runner.py:310] Step = 5600 ; steps/s = 1.64, tokens/s = 78285 (34470 source, 43815 target) ; Learning rate = 0.000495 ; Loss = 1.779230\n",
      "2024-12-02 11:41:41.827000: I runner.py:310] Step = 5700 ; steps/s = 1.64, tokens/s = 78273 (34485 source, 43788 target) ; Learning rate = 0.000504 ; Loss = 1.791057\n",
      "2024-12-02 11:42:42.238000: I runner.py:310] Step = 5800 ; steps/s = 1.66, tokens/s = 77472 (34140 source, 43332 target) ; Learning rate = 0.000513 ; Loss = 1.832745\n",
      "2024-12-02 11:43:43.095000: I runner.py:310] Step = 5900 ; steps/s = 1.64, tokens/s = 78311 (34503 source, 43808 target) ; Learning rate = 0.000522 ; Loss = 1.830062\n",
      "2024-12-02 11:44:43.597000: I runner.py:310] Step = 6000 ; steps/s = 1.65, tokens/s = 77361 (34101 source, 43260 target) ; Learning rate = 0.000530 ; Loss = 1.756726\n",
      "2024-12-02 11:45:44.329000: I runner.py:310] Step = 6100 ; steps/s = 1.65, tokens/s = 78444 (34542 source, 43902 target) ; Learning rate = 0.000539 ; Loss = 1.762466\n",
      "2024-12-02 11:46:45.098000: I runner.py:310] Step = 6200 ; steps/s = 1.65, tokens/s = 78449 (34559 source, 43890 target) ; Learning rate = 0.000548 ; Loss = 1.760849\n",
      "2024-12-02 11:47:45.513000: I runner.py:310] Step = 6300 ; steps/s = 1.66, tokens/s = 77471 (34153 source, 43318 target) ; Learning rate = 0.000557 ; Loss = 1.760712\n",
      "2024-12-02 11:48:46.218000: I runner.py:310] Step = 6400 ; steps/s = 1.65, tokens/s = 78515 (34583 source, 43932 target) ; Learning rate = 0.000566 ; Loss = 1.752695\n",
      "2024-12-02 11:49:46.604000: I runner.py:310] Step = 6500 ; steps/s = 1.66, tokens/s = 77514 (34185 source, 43329 target) ; Learning rate = 0.000575 ; Loss = 1.723310\n",
      "2024-12-02 11:50:47.339000: I runner.py:310] Step = 6600 ; steps/s = 1.65, tokens/s = 78451 (34544 source, 43907 target) ; Learning rate = 0.000583 ; Loss = 1.758732\n",
      "2024-12-02 11:51:48.047000: I runner.py:310] Step = 6700 ; steps/s = 1.65, tokens/s = 78479 (34558 source, 43921 target) ; Learning rate = 0.000592 ; Loss = 1.717504\n",
      "2024-12-02 11:52:48.442000: I runner.py:310] Step = 6800 ; steps/s = 1.66, tokens/s = 77498 (34155 source, 43343 target) ; Learning rate = 0.000601 ; Loss = 1.733754\n",
      "2024-12-02 11:53:49.279000: I runner.py:310] Step = 6900 ; steps/s = 1.64, tokens/s = 78334 (34500 source, 43834 target) ; Learning rate = 0.000610 ; Loss = 1.741799\n",
      "2024-12-02 11:54:50.144000: I runner.py:310] Step = 7000 ; steps/s = 1.64, tokens/s = 78289 (34482 source, 43807 target) ; Learning rate = 0.000619 ; Loss = 1.730838\n",
      "2024-12-02 11:55:50.413000: I runner.py:310] Step = 7100 ; steps/s = 1.66, tokens/s = 77657 (34245 source, 43412 target) ; Learning rate = 0.000628 ; Loss = 1.696727\n",
      "2024-12-02 11:56:51.061000: I runner.py:310] Step = 7200 ; steps/s = 1.65, tokens/s = 78560 (34590 source, 43970 target) ; Learning rate = 0.000636 ; Loss = 1.706845\n",
      "2024-12-02 11:57:51.323000: I runner.py:310] Step = 7300 ; steps/s = 1.66, tokens/s = 77688 (34256 source, 43432 target) ; Learning rate = 0.000645 ; Loss = 1.705550\n",
      "2024-12-02 11:58:52.094000: I runner.py:310] Step = 7400 ; steps/s = 1.65, tokens/s = 78401 (34520 source, 43881 target) ; Learning rate = 0.000654 ; Loss = 1.704097\n",
      "2024-12-02 11:59:52.899000: I runner.py:310] Step = 7500 ; steps/s = 1.64, tokens/s = 78380 (34523 source, 43857 target) ; Learning rate = 0.000663 ; Loss = 1.710424\n",
      "2024-12-02 12:00:53.126000: I runner.py:310] Step = 7600 ; steps/s = 1.66, tokens/s = 77710 (34254 source, 43456 target) ; Learning rate = 0.000672 ; Loss = 1.702602\n",
      "2024-12-02 12:01:53.830000: I runner.py:310] Step = 7700 ; steps/s = 1.65, tokens/s = 78512 (34582 source, 43930 target) ; Learning rate = 0.000681 ; Loss = 1.700168\n",
      "2024-12-02 12:02:54.097000: I runner.py:310] Step = 7800 ; steps/s = 1.66, tokens/s = 77681 (34259 source, 43422 target) ; Learning rate = 0.000690 ; Loss = 1.689670\n",
      "2024-12-02 12:03:54.834000: I runner.py:310] Step = 7900 ; steps/s = 1.65, tokens/s = 78432 (34525 source, 43907 target) ; Learning rate = 0.000698 ; Loss = 1.694352\n",
      "2024-12-02 12:04:55.623000: I runner.py:310] Step = 8000 ; steps/s = 1.65, tokens/s = 78405 (34538 source, 43867 target) ; Learning rate = 0.000707 ; Loss = 1.690885\n",
      "2024-12-02 12:05:55.939000: I runner.py:310] Step = 8100 ; steps/s = 1.66, tokens/s = 77576 (34188 source, 43388 target) ; Learning rate = 0.000716 ; Loss = 1.674082\n",
      "2024-12-02 12:06:56.726000: I runner.py:310] Step = 8200 ; steps/s = 1.65, tokens/s = 78392 (34512 source, 43880 target) ; Learning rate = 0.000725 ; Loss = 1.678898\n",
      "2024-12-02 12:07:57.426000: I runner.py:310] Step = 8300 ; steps/s = 1.65, tokens/s = 78527 (34606 source, 43921 target) ; Learning rate = 0.000734 ; Loss = 1.688907\n",
      "2024-12-02 12:08:57.714000: I runner.py:310] Step = 8400 ; steps/s = 1.66, tokens/s = 77632 (34222 source, 43410 target) ; Learning rate = 0.000743 ; Loss = 1.654351\n",
      "2024-12-02 12:09:58.451000: I runner.py:310] Step = 8500 ; steps/s = 1.65, tokens/s = 78456 (34563 source, 43893 target) ; Learning rate = 0.000751 ; Loss = 1.670141\n",
      "2024-12-02 12:10:58.761000: I runner.py:310] Step = 8600 ; steps/s = 1.66, tokens/s = 77640 (34234 source, 43406 target) ; Learning rate = 0.000760 ; Loss = 1.644165\n",
      "2024-12-02 12:11:59.542000: I runner.py:310] Step = 8700 ; steps/s = 1.65, tokens/s = 78392 (34513 source, 43879 target) ; Learning rate = 0.000769 ; Loss = 1.669227\n",
      "2024-12-02 12:13:00.234000: I runner.py:310] Step = 8800 ; steps/s = 1.65, tokens/s = 78529 (34593 source, 43936 target) ; Learning rate = 0.000778 ; Loss = 1.667579\n",
      "2024-12-02 12:14:00.441000: I runner.py:310] Step = 8900 ; steps/s = 1.66, tokens/s = 77721 (34254 source, 43467 target) ; Learning rate = 0.000787 ; Loss = 1.677427\n",
      "2024-12-02 12:15:01.125000: I runner.py:310] Step = 9000 ; steps/s = 1.65, tokens/s = 78529 (34594 source, 43935 target) ; Learning rate = 0.000796 ; Loss = 1.659505\n",
      "2024-12-02 12:16:01.422000: I runner.py:310] Step = 9100 ; steps/s = 1.66, tokens/s = 77652 (34243 source, 43409 target) ; Learning rate = 0.000804 ; Loss = 1.680684\n",
      "2024-12-02 12:17:02.112000: I runner.py:310] Step = 9200 ; steps/s = 1.65, tokens/s = 78510 (34564 source, 43946 target) ; Learning rate = 0.000813 ; Loss = 1.665349\n",
      "2024-12-02 12:18:02.855000: I runner.py:310] Step = 9300 ; steps/s = 1.65, tokens/s = 78447 (34550 source, 43897 target) ; Learning rate = 0.000822 ; Loss = 1.670415\n",
      "2024-12-02 12:19:03.167000: I runner.py:310] Step = 9400 ; steps/s = 1.66, tokens/s = 77596 (34212 source, 43384 target) ; Learning rate = 0.000831 ; Loss = 1.642101\n",
      "2024-12-02 12:20:03.852000: I runner.py:310] Step = 9500 ; steps/s = 1.65, tokens/s = 78508 (34573 source, 43935 target) ; Learning rate = 0.000840 ; Loss = 1.648860\n",
      "2024-12-02 12:21:04.550000: I runner.py:310] Step = 9600 ; steps/s = 1.65, tokens/s = 78537 (34591 source, 43946 target) ; Learning rate = 0.000849 ; Loss = 1.642581\n",
      "2024-12-02 12:22:04.807000: I runner.py:310] Step = 9700 ; steps/s = 1.66, tokens/s = 77658 (34230 source, 43428 target) ; Learning rate = 0.000857 ; Loss = 1.639751\n",
      "2024-12-02 12:23:05.616000: I runner.py:310] Step = 9800 ; steps/s = 1.64, tokens/s = 78402 (34549 source, 43853 target) ; Learning rate = 0.000866 ; Loss = 1.645203\n",
      "2024-12-02 12:24:05.854000: I runner.py:310] Step = 9900 ; steps/s = 1.66, tokens/s = 77688 (34236 source, 43452 target) ; Learning rate = 0.000875 ; Loss = 1.622668\n",
      "2024-12-02 12:25:06.567000: I runner.py:310] Step = 10000 ; steps/s = 1.65, tokens/s = 78484 (34551 source, 43933 target) ; Learning rate = 0.000884 ; Loss = 1.637749\n",
      "2024-12-02 12:25:08.324000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-10000\n",
      "2024-12-02 12:25:08.324000: I training.py:192] Running evaluation for step 10000\n",
      "2024-12-02 12:26:01.655000: I training.py:192] Evaluation result for step 10000: loss = 0.653913 ; perplexity = 1.923052\n",
      "2024-12-02 12:27:02.338000: I runner.py:310] Step = 10100 ; steps/s = 1.65, tokens/s = 78536 (34594 source, 43942 target) ; Learning rate = 0.000879 ; Loss = 1.649840\n",
      "2024-12-02 12:28:02.596000: I runner.py:310] Step = 10200 ; steps/s = 1.66, tokens/s = 77670 (34242 source, 43428 target) ; Learning rate = 0.000875 ; Loss = 1.628426\n",
      "2024-12-02 12:29:03.341000: I runner.py:310] Step = 10300 ; steps/s = 1.65, tokens/s = 78460 (34566 source, 43894 target) ; Learning rate = 0.000871 ; Loss = 1.625376\n",
      "2024-12-02 12:30:03.632000: I runner.py:310] Step = 10400 ; steps/s = 1.66, tokens/s = 77653 (34231 source, 43422 target) ; Learning rate = 0.000867 ; Loss = 1.614693\n",
      "2024-12-02 12:31:04.388000: I runner.py:310] Step = 10500 ; steps/s = 1.65, tokens/s = 78418 (34521 source, 43897 target) ; Learning rate = 0.000863 ; Loss = 1.618364\n",
      "2024-12-02 12:32:05.195000: I runner.py:310] Step = 10600 ; steps/s = 1.64, tokens/s = 78385 (34539 source, 43846 target) ; Learning rate = 0.000858 ; Loss = 1.620353\n",
      "2024-12-02 12:33:05.573000: I runner.py:310] Step = 10700 ; steps/s = 1.66, tokens/s = 77504 (34152 source, 43352 target) ; Learning rate = 0.000854 ; Loss = 1.609304\n",
      "2024-12-02 12:34:06.299000: I runner.py:310] Step = 10800 ; steps/s = 1.65, tokens/s = 78478 (34563 source, 43915 target) ; Learning rate = 0.000850 ; Loss = 1.623149\n",
      "2024-12-02 12:35:07.069000: I runner.py:310] Step = 10900 ; steps/s = 1.65, tokens/s = 78415 (34537 source, 43878 target) ; Learning rate = 0.000847 ; Loss = 1.614060\n",
      "2024-12-02 12:36:07.409000: I runner.py:310] Step = 11000 ; steps/s = 1.66, tokens/s = 77584 (34210 source, 43374 target) ; Learning rate = 0.000843 ; Loss = 1.602633\n",
      "2024-12-02 12:37:08.163000: I runner.py:310] Step = 11100 ; steps/s = 1.65, tokens/s = 78434 (34551 source, 43883 target) ; Learning rate = 0.000839 ; Loss = 1.610802\n",
      "2024-12-02 12:38:08.427000: I runner.py:310] Step = 11200 ; steps/s = 1.66, tokens/s = 77663 (34231 source, 43432 target) ; Learning rate = 0.000835 ; Loss = 1.593388\n",
      "2024-12-02 12:39:09.159000: I runner.py:310] Step = 11300 ; steps/s = 1.65, tokens/s = 78456 (34552 source, 43904 target) ; Learning rate = 0.000831 ; Loss = 1.604080\n",
      "2024-12-02 12:40:09.977000: I runner.py:310] Step = 11400 ; steps/s = 1.64, tokens/s = 78362 (34504 source, 43858 target) ; Learning rate = 0.000828 ; Loss = 1.614294\n",
      "2024-12-02 12:41:10.282000: I runner.py:310] Step = 11500 ; steps/s = 1.66, tokens/s = 77607 (34203 source, 43404 target) ; Learning rate = 0.000824 ; Loss = 1.599656\n",
      "2024-12-02 12:42:11.099000: I runner.py:310] Step = 11600 ; steps/s = 1.64, tokens/s = 78362 (34521 source, 43841 target) ; Learning rate = 0.000821 ; Loss = 1.598483\n",
      "2024-12-02 12:43:11.312000: I runner.py:310] Step = 11700 ; steps/s = 1.66, tokens/s = 77762 (34305 source, 43457 target) ; Learning rate = 0.000817 ; Loss = 1.592059\n",
      "2024-12-02 12:44:12.026000: I runner.py:310] Step = 11800 ; steps/s = 1.65, tokens/s = 78477 (34543 source, 43934 target) ; Learning rate = 0.000814 ; Loss = 1.595677\n",
      "2024-12-02 12:45:12.798000: I runner.py:310] Step = 11900 ; steps/s = 1.65, tokens/s = 78419 (34538 source, 43881 target) ; Learning rate = 0.000810 ; Loss = 1.602297\n",
      "2024-12-02 12:46:13.077000: I runner.py:310] Step = 12000 ; steps/s = 1.66, tokens/s = 77603 (34200 source, 43403 target) ; Learning rate = 0.000807 ; Loss = 1.583970\n",
      "2024-12-02 12:47:13.856000: I runner.py:310] Step = 12100 ; steps/s = 1.65, tokens/s = 78410 (34537 source, 43873 target) ; Learning rate = 0.000803 ; Loss = 1.595500\n",
      "2024-12-02 12:48:14.708000: I runner.py:310] Step = 12200 ; steps/s = 1.64, tokens/s = 78339 (34511 source, 43828 target) ; Learning rate = 0.000800 ; Loss = 1.604439\n",
      "2024-12-02 12:49:15.046000: I runner.py:310] Step = 12300 ; steps/s = 1.66, tokens/s = 77560 (34188 source, 43372 target) ; Learning rate = 0.000797 ; Loss = 1.577987\n",
      "2024-12-02 12:50:15.864000: I runner.py:310] Step = 12400 ; steps/s = 1.64, tokens/s = 78364 (34518 source, 43846 target) ; Learning rate = 0.000794 ; Loss = 1.587434\n",
      "2024-12-02 12:51:16.142000: I runner.py:310] Step = 12500 ; steps/s = 1.66, tokens/s = 77649 (34228 source, 43421 target) ; Learning rate = 0.000791 ; Loss = 1.578281\n",
      "2024-12-02 12:52:16.830000: I runner.py:310] Step = 12600 ; steps/s = 1.65, tokens/s = 78501 (34567 source, 43934 target) ; Learning rate = 0.000787 ; Loss = 1.589818\n",
      "2024-12-02 12:53:17.611000: I runner.py:310] Step = 12700 ; steps/s = 1.65, tokens/s = 78420 (34548 source, 43872 target) ; Learning rate = 0.000784 ; Loss = 1.580489\n",
      "2024-12-02 12:54:17.828000: I runner.py:310] Step = 12800 ; steps/s = 1.66, tokens/s = 77733 (34259 source, 43474 target) ; Learning rate = 0.000781 ; Loss = 1.577145\n",
      "2024-12-02 12:55:18.521000: I runner.py:310] Step = 12900 ; steps/s = 1.65, tokens/s = 78531 (34601 source, 43930 target) ; Learning rate = 0.000778 ; Loss = 1.589714\n",
      "2024-12-02 12:56:18.870000: I runner.py:310] Step = 13000 ; steps/s = 1.66, tokens/s = 77557 (34195 source, 43362 target) ; Learning rate = 0.000775 ; Loss = 1.567899\n",
      "2024-12-02 12:57:19.662000: I runner.py:310] Step = 13100 ; steps/s = 1.65, tokens/s = 78341 (34487 source, 43854 target) ; Learning rate = 0.000772 ; Loss = 1.571289\n",
      "2024-12-02 12:58:20.348000: I runner.py:310] Step = 13200 ; steps/s = 1.65, tokens/s = 78564 (34611 source, 43953 target) ; Learning rate = 0.000769 ; Loss = 1.581960\n",
      "2024-12-02 12:59:20.659000: I runner.py:310] Step = 13300 ; steps/s = 1.66, tokens/s = 77599 (34208 source, 43391 target) ; Learning rate = 0.000766 ; Loss = 1.569206\n",
      "2024-12-02 13:00:21.461000: I runner.py:310] Step = 13400 ; steps/s = 1.64, tokens/s = 78362 (34496 source, 43866 target) ; Learning rate = 0.000764 ; Loss = 1.576278\n",
      "2024-12-02 13:01:22.206000: I runner.py:310] Step = 13500 ; steps/s = 1.65, tokens/s = 78469 (34571 source, 43898 target) ; Learning rate = 0.000761 ; Loss = 1.582111\n",
      "2024-12-02 13:02:22.483000: I runner.py:310] Step = 13600 ; steps/s = 1.66, tokens/s = 77626 (34210 source, 43416 target) ; Learning rate = 0.000758 ; Loss = 1.564120\n",
      "2024-12-02 13:03:23.628000: I runner.py:310] Step = 13700 ; steps/s = 1.64, tokens/s = 77941 (34332 source, 43609 target) ; Learning rate = 0.000755 ; Loss = 1.569738\n",
      "2024-12-02 13:04:23.852000: I runner.py:310] Step = 13800 ; steps/s = 1.66, tokens/s = 77749 (34294 source, 43455 target) ; Learning rate = 0.000752 ; Loss = 1.559998\n",
      "2024-12-02 13:05:24.561000: I runner.py:310] Step = 13900 ; steps/s = 1.65, tokens/s = 78481 (34544 source, 43937 target) ; Learning rate = 0.000750 ; Loss = 1.573859\n",
      "2024-12-02 13:06:25.242000: I runner.py:310] Step = 14000 ; steps/s = 1.65, tokens/s = 78535 (34585 source, 43950 target) ; Learning rate = 0.000747 ; Loss = 1.560807\n",
      "2024-12-02 13:07:25.478000: I runner.py:310] Step = 14100 ; steps/s = 1.66, tokens/s = 77749 (34302 source, 43447 target) ; Learning rate = 0.000744 ; Loss = 1.559299\n",
      "2024-12-02 13:08:26.199000: I runner.py:310] Step = 14200 ; steps/s = 1.65, tokens/s = 78443 (34517 source, 43926 target) ; Learning rate = 0.000742 ; Loss = 1.562619\n",
      "2024-12-02 13:09:26.419000: I runner.py:310] Step = 14300 ; steps/s = 1.66, tokens/s = 77749 (34303 source, 43446 target) ; Learning rate = 0.000739 ; Loss = 1.547693\n",
      "2024-12-02 13:10:27.088000: I runner.py:310] Step = 14400 ; steps/s = 1.65, tokens/s = 78520 (34566 source, 43954 target) ; Learning rate = 0.000737 ; Loss = 1.552113\n",
      "2024-12-02 13:11:27.830000: I runner.py:310] Step = 14500 ; steps/s = 1.65, tokens/s = 78489 (34581 source, 43908 target) ; Learning rate = 0.000734 ; Loss = 1.568357\n",
      "2024-12-02 13:12:28.131000: I runner.py:310] Step = 14600 ; steps/s = 1.66, tokens/s = 77577 (34178 source, 43399 target) ; Learning rate = 0.000731 ; Loss = 1.555767\n",
      "2024-12-02 13:13:28.838000: I runner.py:310] Step = 14700 ; steps/s = 1.65, tokens/s = 78514 (34588 source, 43926 target) ; Learning rate = 0.000729 ; Loss = 1.561916\n",
      "2024-12-02 13:14:29.483000: I runner.py:310] Step = 14800 ; steps/s = 1.65, tokens/s = 78579 (34606 source, 43973 target) ; Learning rate = 0.000727 ; Loss = 1.565703\n",
      "2024-12-02 13:15:29.671000: I runner.py:310] Step = 14900 ; steps/s = 1.66, tokens/s = 77765 (34283 source, 43482 target) ; Learning rate = 0.000724 ; Loss = 1.548228\n",
      "2024-12-02 13:16:30.374000: I runner.py:310] Step = 15000 ; steps/s = 1.65, tokens/s = 78519 (34587 source, 43932 target) ; Learning rate = 0.000722 ; Loss = 1.550315\n",
      "2024-12-02 13:16:30.375000: I training.py:192] Running evaluation for step 15000\n",
      "2024-12-02 13:17:18.204000: I training.py:192] Evaluation result for step 15000: loss = 0.679400 ; perplexity = 1.972694\n",
      "2024-12-02 13:18:18.432000: I runner.py:310] Step = 15100 ; steps/s = 1.66, tokens/s = 77714 (34252 source, 43462 target) ; Learning rate = 0.000719 ; Loss = 1.544203\n",
      "2024-12-02 13:19:19.149000: I runner.py:310] Step = 15200 ; steps/s = 1.65, tokens/s = 78468 (34550 source, 43918 target) ; Learning rate = 0.000717 ; Loss = 1.548818\n",
      "2024-12-02 13:20:19.847000: I runner.py:310] Step = 15300 ; steps/s = 1.65, tokens/s = 78536 (34597 source, 43939 target) ; Learning rate = 0.000715 ; Loss = 1.554571\n",
      "2024-12-02 13:21:20.132000: I runner.py:310] Step = 15400 ; steps/s = 1.66, tokens/s = 77637 (34227 source, 43410 target) ; Learning rate = 0.000712 ; Loss = 1.547091\n",
      "2024-12-02 13:22:20.820000: I runner.py:310] Step = 15500 ; steps/s = 1.65, tokens/s = 78510 (34575 source, 43935 target) ; Learning rate = 0.000710 ; Loss = 1.548371\n",
      "2024-12-02 13:23:21.105000: I runner.py:310] Step = 15600 ; steps/s = 1.66, tokens/s = 77668 (34258 source, 43410 target) ; Learning rate = 0.000708 ; Loss = 1.527998\n",
      "2024-12-02 13:24:21.737000: I runner.py:310] Step = 15700 ; steps/s = 1.65, tokens/s = 78601 (34622 source, 43979 target) ; Learning rate = 0.000705 ; Loss = 1.544705\n",
      "2024-12-02 13:25:22.443000: I runner.py:310] Step = 15800 ; steps/s = 1.65, tokens/s = 78488 (34556 source, 43932 target) ; Learning rate = 0.000703 ; Loss = 1.546487\n",
      "2024-12-02 13:26:22.706000: I runner.py:310] Step = 15900 ; steps/s = 1.66, tokens/s = 77652 (34223 source, 43429 target) ; Learning rate = 0.000701 ; Loss = 1.543124\n",
      "2024-12-02 13:27:23.388000: I runner.py:310] Step = 16000 ; steps/s = 1.65, tokens/s = 78540 (34584 source, 43956 target) ; Learning rate = 0.000699 ; Loss = 1.549281\n",
      "2024-12-02 13:28:24.023000: I runner.py:310] Step = 16100 ; steps/s = 1.65, tokens/s = 78284 (34498 source, 43786 target) ; Learning rate = 0.000697 ; Loss = 1.558040\n",
      "2024-12-02 13:29:24.362000: I runner.py:310] Step = 16200 ; steps/s = 1.66, tokens/s = 77893 (34337 source, 43556 target) ; Learning rate = 0.000694 ; Loss = 1.538234\n",
      "2024-12-02 13:30:25.066000: I runner.py:310] Step = 16300 ; steps/s = 1.65, tokens/s = 78517 (34591 source, 43926 target) ; Learning rate = 0.000692 ; Loss = 1.540804\n",
      "2024-12-02 13:31:25.281000: I runner.py:310] Step = 16400 ; steps/s = 1.66, tokens/s = 77717 (34252 source, 43465 target) ; Learning rate = 0.000690 ; Loss = 1.539669\n",
      "2024-12-02 13:32:25.996000: I runner.py:310] Step = 16500 ; steps/s = 1.65, tokens/s = 78479 (34557 source, 43922 target) ; Learning rate = 0.000688 ; Loss = 1.542598\n",
      "2024-12-02 13:33:26.675000: I runner.py:310] Step = 16600 ; steps/s = 1.65, tokens/s = 78535 (34582 source, 43953 target) ; Learning rate = 0.000686 ; Loss = 1.531579\n",
      "2024-12-02 13:34:26.910000: I runner.py:310] Step = 16700 ; steps/s = 1.66, tokens/s = 77717 (34269 source, 43448 target) ; Learning rate = 0.000684 ; Loss = 1.543214\n",
      "2024-12-02 13:35:27.675000: I runner.py:310] Step = 16800 ; steps/s = 1.65, tokens/s = 78405 (34533 source, 43872 target) ; Learning rate = 0.000682 ; Loss = 1.536629\n",
      "2024-12-02 13:36:27.949000: I runner.py:310] Step = 16900 ; steps/s = 1.66, tokens/s = 77669 (34242 source, 43427 target) ; Learning rate = 0.000680 ; Loss = 1.526547\n",
      "2024-12-02 13:37:28.676000: I runner.py:310] Step = 17000 ; steps/s = 1.65, tokens/s = 78429 (34509 source, 43920 target) ; Learning rate = 0.000678 ; Loss = 1.538999\n",
      "2024-12-02 13:38:29.363000: I runner.py:310] Step = 17100 ; steps/s = 1.65, tokens/s = 78565 (34620 source, 43945 target) ; Learning rate = 0.000676 ; Loss = 1.535639\n",
      "2024-12-02 13:39:29.572000: I runner.py:310] Step = 17200 ; steps/s = 1.66, tokens/s = 77722 (34261 source, 43461 target) ; Learning rate = 0.000674 ; Loss = 1.528043\n",
      "2024-12-02 13:40:30.230000: I runner.py:310] Step = 17300 ; steps/s = 1.65, tokens/s = 78592 (34627 source, 43965 target) ; Learning rate = 0.000672 ; Loss = 1.534239\n",
      "2024-12-02 13:41:30.612000: I runner.py:310] Step = 17400 ; steps/s = 1.66, tokens/s = 78130 (34454 source, 43676 target) ; Learning rate = 0.000670 ; Loss = 1.562425\n",
      "2024-12-02 13:42:31.179000: I runner.py:310] Step = 17500 ; steps/s = 1.65, tokens/s = 78076 (34384 source, 43692 target) ; Learning rate = 0.000668 ; Loss = 1.528635\n",
      "2024-12-02 13:43:31.914000: I runner.py:310] Step = 17600 ; steps/s = 1.65, tokens/s = 78469 (34562 source, 43907 target) ; Learning rate = 0.000666 ; Loss = 1.531872\n",
      "2024-12-02 13:44:32.247000: I runner.py:310] Step = 17700 ; steps/s = 1.66, tokens/s = 77562 (34192 source, 43370 target) ; Learning rate = 0.000664 ; Loss = 1.510005\n",
      "2024-12-02 13:45:32.951000: I runner.py:310] Step = 17800 ; steps/s = 1.65, tokens/s = 78502 (34564 source, 43938 target) ; Learning rate = 0.000662 ; Loss = 1.548313\n",
      "2024-12-02 13:46:33.653000: I runner.py:310] Step = 17900 ; steps/s = 1.65, tokens/s = 78492 (34560 source, 43932 target) ; Learning rate = 0.000661 ; Loss = 1.531158\n",
      "2024-12-02 13:47:33.791000: I runner.py:310] Step = 18000 ; steps/s = 1.66, tokens/s = 77818 (34309 source, 43509 target) ; Learning rate = 0.000659 ; Loss = 1.529932\n",
      "2024-12-02 13:48:34.522000: I runner.py:310] Step = 18100 ; steps/s = 1.65, tokens/s = 78474 (34571 source, 43903 target) ; Learning rate = 0.000657 ; Loss = 1.533042\n",
      "2024-12-02 13:49:34.808000: I runner.py:310] Step = 18200 ; steps/s = 1.66, tokens/s = 77644 (34225 source, 43419 target) ; Learning rate = 0.000655 ; Loss = 1.520052\n",
      "2024-12-02 13:50:35.498000: I runner.py:310] Step = 18300 ; steps/s = 1.65, tokens/s = 78478 (34543 source, 43935 target) ; Learning rate = 0.000653 ; Loss = 1.518915\n",
      "2024-12-02 13:51:36.248000: I runner.py:310] Step = 18400 ; steps/s = 1.65, tokens/s = 78482 (34582 source, 43900 target) ; Learning rate = 0.000652 ; Loss = 1.537515\n",
      "2024-12-02 13:52:36.495000: I runner.py:310] Step = 18500 ; steps/s = 1.66, tokens/s = 77685 (34239 source, 43446 target) ; Learning rate = 0.000650 ; Loss = 1.529146\n",
      "2024-12-02 13:53:37.171000: I runner.py:310] Step = 18600 ; steps/s = 1.65, tokens/s = 78541 (34588 source, 43953 target) ; Learning rate = 0.000648 ; Loss = 1.529112\n",
      "2024-12-02 13:54:37.479000: I runner.py:310] Step = 18700 ; steps/s = 1.66, tokens/s = 77642 (34257 source, 43385 target) ; Learning rate = 0.000646 ; Loss = 1.510117\n",
      "2024-12-02 13:55:38.232000: I runner.py:310] Step = 18800 ; steps/s = 1.65, tokens/s = 78406 (34514 source, 43892 target) ; Learning rate = 0.000645 ; Loss = 1.516375\n",
      "2024-12-02 13:56:38.901000: I runner.py:310] Step = 18900 ; steps/s = 1.65, tokens/s = 78554 (34608 source, 43946 target) ; Learning rate = 0.000643 ; Loss = 1.538518\n",
      "2024-12-02 13:57:39.148000: I runner.py:310] Step = 19000 ; steps/s = 1.66, tokens/s = 77710 (34253 source, 43457 target) ; Learning rate = 0.000641 ; Loss = 1.520637\n",
      "2024-12-02 13:58:39.841000: I runner.py:310] Step = 19100 ; steps/s = 1.65, tokens/s = 78530 (34590 source, 43940 target) ; Learning rate = 0.000640 ; Loss = 1.539312\n",
      "2024-12-02 13:59:40.536000: I runner.py:310] Step = 19200 ; steps/s = 1.65, tokens/s = 78486 (34554 source, 43932 target) ; Learning rate = 0.000638 ; Loss = 1.520837\n",
      "2024-12-02 14:00:40.725000: I runner.py:310] Step = 19300 ; steps/s = 1.66, tokens/s = 77758 (34295 source, 43463 target) ; Learning rate = 0.000636 ; Loss = 1.522699\n",
      "2024-12-02 14:01:41.367000: I runner.py:310] Step = 19400 ; steps/s = 1.65, tokens/s = 78570 (34592 source, 43978 target) ; Learning rate = 0.000635 ; Loss = 1.522136\n",
      "2024-12-02 14:02:41.614000: I runner.py:310] Step = 19500 ; steps/s = 1.66, tokens/s = 77701 (34254 source, 43447 target) ; Learning rate = 0.000633 ; Loss = 1.505854\n",
      "2024-12-02 14:03:42.341000: I runner.py:310] Step = 19600 ; steps/s = 1.65, tokens/s = 78484 (34563 source, 43921 target) ; Learning rate = 0.000631 ; Loss = 1.519863\n",
      "2024-12-02 14:04:43.052000: I runner.py:310] Step = 19700 ; steps/s = 1.65, tokens/s = 78469 (34547 source, 43922 target) ; Learning rate = 0.000630 ; Loss = 1.520323\n",
      "2024-12-02 14:05:43.293000: I runner.py:310] Step = 19800 ; steps/s = 1.66, tokens/s = 77701 (34266 source, 43435 target) ; Learning rate = 0.000628 ; Loss = 1.519645\n",
      "2024-12-02 14:06:43.993000: I runner.py:310] Step = 19900 ; steps/s = 1.65, tokens/s = 78507 (34564 source, 43943 target) ; Learning rate = 0.000627 ; Loss = 1.515913\n",
      "2024-12-02 14:07:44.261000: I runner.py:310] Step = 20000 ; steps/s = 1.66, tokens/s = 77698 (34283 source, 43415 target) ; Learning rate = 0.000625 ; Loss = 1.508746\n",
      "2024-12-02 14:07:46.054000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-20000\n",
      "2024-12-02 14:07:46.054000: I training.py:192] Running evaluation for step 20000\n",
      "2024-12-02 14:08:34.976000: I training.py:192] Evaluation result for step 20000: loss = 0.708725 ; perplexity = 2.031399\n",
      "2024-12-02 14:09:35.570000: I runner.py:310] Step = 20100 ; steps/s = 1.65, tokens/s = 78663 (34649 source, 44014 target) ; Learning rate = 0.000623 ; Loss = 1.515147\n",
      "2024-12-02 14:10:36.359000: I runner.py:310] Step = 20200 ; steps/s = 1.65, tokens/s = 78378 (34504 source, 43874 target) ; Learning rate = 0.000622 ; Loss = 1.515806\n",
      "2024-12-02 14:11:36.590000: I runner.py:310] Step = 20300 ; steps/s = 1.66, tokens/s = 77714 (34253 source, 43461 target) ; Learning rate = 0.000620 ; Loss = 1.519273\n",
      "2024-12-02 14:12:37.343000: I runner.py:310] Step = 20400 ; steps/s = 1.65, tokens/s = 78411 (34519 source, 43892 target) ; Learning rate = 0.000619 ; Loss = 1.522164\n",
      "2024-12-02 14:13:38.107000: I runner.py:310] Step = 20500 ; steps/s = 1.65, tokens/s = 78435 (34551 source, 43884 target) ; Learning rate = 0.000617 ; Loss = 1.515431\n",
      "2024-12-02 14:14:38.395000: I runner.py:310] Step = 20600 ; steps/s = 1.66, tokens/s = 77637 (34229 source, 43408 target) ; Learning rate = 0.000616 ; Loss = 1.516040\n",
      "2024-12-02 14:15:39.126000: I runner.py:310] Step = 20700 ; steps/s = 1.65, tokens/s = 78454 (34545 source, 43909 target) ; Learning rate = 0.000614 ; Loss = 1.516247\n",
      "2024-12-02 14:16:39.375000: I runner.py:310] Step = 20800 ; steps/s = 1.66, tokens/s = 77715 (34276 source, 43439 target) ; Learning rate = 0.000613 ; Loss = 1.497957\n",
      "2024-12-02 14:17:40.096000: I runner.py:310] Step = 20900 ; steps/s = 1.65, tokens/s = 78455 (34544 source, 43911 target) ; Learning rate = 0.000611 ; Loss = 1.513786\n",
      "2024-12-02 14:18:40.806000: I runner.py:310] Step = 21000 ; steps/s = 1.65, tokens/s = 78494 (34571 source, 43923 target) ; Learning rate = 0.000610 ; Loss = 1.512910\n",
      "2024-12-02 14:19:41.067000: I runner.py:310] Step = 21100 ; steps/s = 1.66, tokens/s = 77686 (34245 source, 43441 target) ; Learning rate = 0.000608 ; Loss = 1.507293\n",
      "2024-12-02 14:20:41.754000: I runner.py:310] Step = 21200 ; steps/s = 1.65, tokens/s = 78533 (34596 source, 43937 target) ; Learning rate = 0.000607 ; Loss = 1.517282\n",
      "2024-12-02 14:21:42.061000: I runner.py:310] Step = 21300 ; steps/s = 1.66, tokens/s = 77638 (34238 source, 43400 target) ; Learning rate = 0.000606 ; Loss = 1.511005\n",
      "2024-12-02 14:22:42.801000: I runner.py:310] Step = 21400 ; steps/s = 1.65, tokens/s = 78442 (34537 source, 43905 target) ; Learning rate = 0.000604 ; Loss = 1.506451\n",
      "2024-12-02 14:23:43.541000: I runner.py:310] Step = 21500 ; steps/s = 1.65, tokens/s = 78462 (34565 source, 43897 target) ; Learning rate = 0.000603 ; Loss = 1.512866\n",
      "2024-12-02 14:24:43.839000: I runner.py:310] Step = 21600 ; steps/s = 1.66, tokens/s = 77611 (34212 source, 43399 target) ; Learning rate = 0.000601 ; Loss = 1.498764\n",
      "2024-12-02 14:25:44.538000: I runner.py:310] Step = 21700 ; steps/s = 1.65, tokens/s = 78523 (34575 source, 43948 target) ; Learning rate = 0.000600 ; Loss = 1.514135\n",
      "2024-12-02 14:26:45.210000: I runner.py:310] Step = 21800 ; steps/s = 1.65, tokens/s = 78537 (34593 source, 43944 target) ; Learning rate = 0.000599 ; Loss = 1.513367\n",
      "2024-12-02 14:27:45.435000: I runner.py:310] Step = 21900 ; steps/s = 1.66, tokens/s = 77741 (34283 source, 43458 target) ; Learning rate = 0.000597 ; Loss = 1.515756\n",
      "2024-12-02 14:28:46.169000: I runner.py:310] Step = 22000 ; steps/s = 1.65, tokens/s = 78456 (34543 source, 43913 target) ; Learning rate = 0.000596 ; Loss = 1.510168\n",
      "2024-12-02 14:29:46.407000: I runner.py:310] Step = 22100 ; steps/s = 1.66, tokens/s = 77694 (34253 source, 43441 target) ; Learning rate = 0.000595 ; Loss = 1.497129\n",
      "2024-12-02 14:30:47.140000: I runner.py:310] Step = 22200 ; steps/s = 1.65, tokens/s = 78440 (34541 source, 43899 target) ; Learning rate = 0.000593 ; Loss = 1.501644\n",
      "2024-12-02 14:31:47.874000: I runner.py:310] Step = 22300 ; steps/s = 1.65, tokens/s = 78480 (34569 source, 43911 target) ; Learning rate = 0.000592 ; Loss = 1.511751\n",
      "2024-12-02 14:32:48.102000: I runner.py:310] Step = 22400 ; steps/s = 1.66, tokens/s = 77713 (34261 source, 43452 target) ; Learning rate = 0.000591 ; Loss = 1.507074\n",
      "2024-12-02 14:33:48.808000: I runner.py:310] Step = 22500 ; steps/s = 1.65, tokens/s = 78479 (34553 source, 43926 target) ; Learning rate = 0.000589 ; Loss = 1.514515\n",
      "2024-12-02 14:34:49.047000: I runner.py:310] Step = 22600 ; steps/s = 1.66, tokens/s = 77739 (34286 source, 43453 target) ; Learning rate = 0.000588 ; Loss = 1.503328\n",
      "2024-12-02 14:35:49.749000: I runner.py:310] Step = 22700 ; steps/s = 1.65, tokens/s = 78473 (34546 source, 43927 target) ; Learning rate = 0.000587 ; Loss = 1.502687\n",
      "2024-12-02 14:36:50.465000: I runner.py:310] Step = 22800 ; steps/s = 1.65, tokens/s = 78500 (34582 source, 43918 target) ; Learning rate = 0.000585 ; Loss = 1.503658\n",
      "2024-12-02 14:37:50.743000: I runner.py:310] Step = 22900 ; steps/s = 1.66, tokens/s = 77644 (34220 source, 43424 target) ; Learning rate = 0.000584 ; Loss = 1.505322\n",
      "2024-12-02 14:38:51.506000: I runner.py:310] Step = 23000 ; steps/s = 1.65, tokens/s = 78441 (34548 source, 43893 target) ; Learning rate = 0.000583 ; Loss = 1.510894\n",
      "2024-12-02 14:39:52.213000: I runner.py:310] Step = 23100 ; steps/s = 1.65, tokens/s = 78485 (34571 source, 43914 target) ; Learning rate = 0.000582 ; Loss = 1.511631\n",
      "2024-12-02 14:40:52.451000: I runner.py:310] Step = 23200 ; steps/s = 1.66, tokens/s = 77698 (34244 source, 43454 target) ; Learning rate = 0.000580 ; Loss = 1.501559\n",
      "2024-12-02 14:41:53.181000: I runner.py:310] Step = 23300 ; steps/s = 1.65, tokens/s = 78473 (34570 source, 43903 target) ; Learning rate = 0.000579 ; Loss = 1.512480\n",
      "2024-12-02 14:42:53.532000: I runner.py:310] Step = 23400 ; steps/s = 1.66, tokens/s = 77558 (34188 source, 43370 target) ; Learning rate = 0.000578 ; Loss = 1.490767\n",
      "2024-12-02 14:43:54.194000: I runner.py:310] Step = 23500 ; steps/s = 1.65, tokens/s = 78570 (34609 source, 43961 target) ; Learning rate = 0.000577 ; Loss = 1.502880\n",
      "2024-12-02 14:44:54.920000: I runner.py:310] Step = 23600 ; steps/s = 1.65, tokens/s = 78472 (34557 source, 43915 target) ; Learning rate = 0.000575 ; Loss = 1.506344\n",
      "2024-12-02 14:45:55.221000: I runner.py:310] Step = 23700 ; steps/s = 1.66, tokens/s = 77597 (34202 source, 43395 target) ; Learning rate = 0.000574 ; Loss = 1.500328\n",
      "2024-12-02 14:46:55.997000: I runner.py:310] Step = 23800 ; steps/s = 1.65, tokens/s = 78410 (34522 source, 43888 target) ; Learning rate = 0.000573 ; Loss = 1.506598\n",
      "2024-12-02 14:47:56.218000: I runner.py:310] Step = 23900 ; steps/s = 1.66, tokens/s = 77762 (34313 source, 43449 target) ; Learning rate = 0.000572 ; Loss = 1.497477\n",
      "2024-12-02 14:48:56.896000: I runner.py:310] Step = 24000 ; steps/s = 1.65, tokens/s = 78495 (34550 source, 43945 target) ; Learning rate = 0.000571 ; Loss = 1.492583\n",
      "2024-12-02 14:49:57.590000: I runner.py:310] Step = 24100 ; steps/s = 1.65, tokens/s = 78515 (34571 source, 43944 target) ; Learning rate = 0.000569 ; Loss = 1.500692\n",
      "2024-12-02 14:50:57.839000: I runner.py:310] Step = 24200 ; steps/s = 1.66, tokens/s = 77692 (34256 source, 43436 target) ; Learning rate = 0.000568 ; Loss = 1.490873\n",
      "2024-12-02 14:51:58.512000: I runner.py:310] Step = 24300 ; steps/s = 1.65, tokens/s = 78531 (34573 source, 43958 target) ; Learning rate = 0.000567 ; Loss = 1.498947\n",
      "2024-12-02 14:52:59.193000: I runner.py:310] Step = 24400 ; steps/s = 1.65, tokens/s = 78552 (34610 source, 43942 target) ; Learning rate = 0.000566 ; Loss = 1.500063\n",
      "2024-12-02 14:53:59.429000: I runner.py:310] Step = 24500 ; steps/s = 1.66, tokens/s = 77703 (34256 source, 43447 target) ; Learning rate = 0.000565 ; Loss = 1.493615\n",
      "2024-12-02 14:55:00.084000: I runner.py:310] Step = 24600 ; steps/s = 1.65, tokens/s = 78572 (34618 source, 43954 target) ; Learning rate = 0.000564 ; Loss = 1.501278\n",
      "2024-12-02 14:56:00.322000: I runner.py:310] Step = 24700 ; steps/s = 1.66, tokens/s = 77732 (34266 source, 43466 target) ; Learning rate = 0.000562 ; Loss = 1.493606\n",
      "2024-12-02 14:57:00.998000: I runner.py:310] Step = 24800 ; steps/s = 1.65, tokens/s = 78510 (34562 source, 43948 target) ; Learning rate = 0.000561 ; Loss = 1.497290\n",
      "2024-12-02 14:58:01.722000: I runner.py:310] Step = 24900 ; steps/s = 1.65, tokens/s = 78476 (34568 source, 43908 target) ; Learning rate = 0.000560 ; Loss = 1.495313\n",
      "2024-12-02 14:59:01.963000: I runner.py:310] Step = 25000 ; steps/s = 1.66, tokens/s = 77733 (34279 source, 43454 target) ; Learning rate = 0.000559 ; Loss = 1.495822\n",
      "2024-12-02 14:59:01.964000: I training.py:192] Running evaluation for step 25000\n",
      "2024-12-02 14:59:49.394000: I training.py:192] Evaluation result for step 25000: loss = 0.729965 ; perplexity = 2.075008\n",
      "2024-12-02 15:00:50.040000: I runner.py:310] Step = 25100 ; steps/s = 1.65, tokens/s = 78609 (34625 source, 43984 target) ; Learning rate = 0.000558 ; Loss = 1.501668\n",
      "2024-12-02 15:01:50.288000: I runner.py:310] Step = 25200 ; steps/s = 1.66, tokens/s = 77659 (34234 source, 43425 target) ; Learning rate = 0.000557 ; Loss = 1.486559\n",
      "2024-12-02 15:02:50.986000: I runner.py:310] Step = 25300 ; steps/s = 1.65, tokens/s = 78478 (34542 source, 43936 target) ; Learning rate = 0.000556 ; Loss = 1.495698\n",
      "2024-12-02 15:03:51.727000: I runner.py:310] Step = 25400 ; steps/s = 1.65, tokens/s = 78459 (34554 source, 43905 target) ; Learning rate = 0.000555 ; Loss = 1.497055\n",
      "2024-12-02 15:04:52.022000: I runner.py:310] Step = 25500 ; steps/s = 1.66, tokens/s = 77624 (34226 source, 43398 target) ; Learning rate = 0.000553 ; Loss = 1.494410\n",
      "2024-12-02 15:05:52.773000: I runner.py:310] Step = 25600 ; steps/s = 1.65, tokens/s = 78424 (34526 source, 43898 target) ; Learning rate = 0.000552 ; Loss = 1.492945\n",
      "2024-12-02 15:06:53.565000: I runner.py:310] Step = 25700 ; steps/s = 1.65, tokens/s = 78431 (34561 source, 43870 target) ; Learning rate = 0.000551 ; Loss = 1.500773\n",
      "2024-12-02 15:07:53.796000: I runner.py:310] Step = 25800 ; steps/s = 1.66, tokens/s = 77728 (34270 source, 43458 target) ; Learning rate = 0.000550 ; Loss = 1.487629\n",
      "2024-12-02 15:08:54.490000: I runner.py:310] Step = 25900 ; steps/s = 1.65, tokens/s = 78509 (34577 source, 43932 target) ; Learning rate = 0.000549 ; Loss = 1.498936\n",
      "2024-12-02 15:09:54.768000: I runner.py:310] Step = 26000 ; steps/s = 1.66, tokens/s = 77640 (34229 source, 43411 target) ; Learning rate = 0.000548 ; Loss = 1.492878\n",
      "2024-12-02 15:10:55.444000: I runner.py:310] Step = 26100 ; steps/s = 1.65, tokens/s = 78542 (34594 source, 43948 target) ; Learning rate = 0.000547 ; Loss = 1.494355\n",
      "2024-12-02 15:11:56.210000: I runner.py:310] Step = 26200 ; steps/s = 1.65, tokens/s = 78424 (34530 source, 43894 target) ; Learning rate = 0.000546 ; Loss = 1.490963\n",
      "2024-12-02 15:12:56.456000: I runner.py:310] Step = 26300 ; steps/s = 1.66, tokens/s = 77676 (34243 source, 43433 target) ; Learning rate = 0.000545 ; Loss = 1.487037\n",
      "2024-12-02 15:13:57.185000: I runner.py:310] Step = 26400 ; steps/s = 1.65, tokens/s = 78477 (34567 source, 43910 target) ; Learning rate = 0.000544 ; Loss = 1.493655\n",
      "2024-12-02 15:14:57.393000: I runner.py:310] Step = 26500 ; steps/s = 1.66, tokens/s = 77758 (34284 source, 43474 target) ; Learning rate = 0.000543 ; Loss = 1.488243\n",
      "2024-12-02 15:15:58.075000: I runner.py:310] Step = 26600 ; steps/s = 1.65, tokens/s = 78508 (34556 source, 43952 target) ; Learning rate = 0.000542 ; Loss = 1.488046\n",
      "2024-12-02 15:16:58.801000: I runner.py:310] Step = 26700 ; steps/s = 1.65, tokens/s = 78485 (34583 source, 43902 target) ; Learning rate = 0.000541 ; Loss = 1.500984\n",
      "2024-12-02 15:17:59.057000: I runner.py:310] Step = 26800 ; steps/s = 1.66, tokens/s = 77664 (34234 source, 43430 target) ; Learning rate = 0.000540 ; Loss = 1.483892\n",
      "2024-12-02 15:18:59.826000: I runner.py:310] Step = 26900 ; steps/s = 1.65, tokens/s = 78404 (34514 source, 43890 target) ; Learning rate = 0.000539 ; Loss = 1.497744\n",
      "2024-12-02 15:20:00.543000: I runner.py:310] Step = 27000 ; steps/s = 1.65, tokens/s = 78513 (34600 source, 43913 target) ; Learning rate = 0.000538 ; Loss = 1.492345\n",
      "2024-12-02 15:21:00.765000: I runner.py:310] Step = 27100 ; steps/s = 1.66, tokens/s = 77706 (34244 source, 43462 target) ; Learning rate = 0.000537 ; Loss = 1.487562\n",
      "2024-12-02 15:22:01.485000: I runner.py:310] Step = 27200 ; steps/s = 1.65, tokens/s = 78477 (34571 source, 43906 target) ; Learning rate = 0.000536 ; Loss = 1.492111\n",
      "2024-12-02 15:23:01.749000: I runner.py:310] Step = 27300 ; steps/s = 1.66, tokens/s = 77692 (34258 source, 43434 target) ; Learning rate = 0.000535 ; Loss = 1.482906\n",
      "2024-12-02 15:24:02.416000: I runner.py:310] Step = 27400 ; steps/s = 1.65, tokens/s = 78529 (34573 source, 43956 target) ; Learning rate = 0.000534 ; Loss = 1.483549\n",
      "2024-12-02 15:25:03.179000: I runner.py:310] Step = 27500 ; steps/s = 1.65, tokens/s = 78441 (34551 source, 43890 target) ; Learning rate = 0.000533 ; Loss = 1.493731\n",
      "2024-12-02 15:26:03.512000: I runner.py:310] Step = 27600 ; steps/s = 1.66, tokens/s = 77580 (34205 source, 43375 target) ; Learning rate = 0.000532 ; Loss = 1.490869\n",
      "2024-12-02 15:27:04.304000: I runner.py:310] Step = 27700 ; steps/s = 1.65, tokens/s = 78389 (34516 source, 43873 target) ; Learning rate = 0.000531 ; Loss = 1.491658\n",
      "2024-12-02 15:28:04.585000: I runner.py:310] Step = 27800 ; steps/s = 1.66, tokens/s = 77664 (34255 source, 43409 target) ; Learning rate = 0.000530 ; Loss = 1.483526\n",
      "2024-12-02 15:29:05.309000: I runner.py:310] Step = 27900 ; steps/s = 1.65, tokens/s = 78480 (34571 source, 43909 target) ; Learning rate = 0.000529 ; Loss = 1.490455\n",
      "2024-12-02 15:30:05.995000: I runner.py:310] Step = 28000 ; steps/s = 1.65, tokens/s = 78508 (34568 source, 43940 target) ; Learning rate = 0.000528 ; Loss = 1.491939\n",
      "2024-12-02 15:31:06.208000: I runner.py:310] Step = 28100 ; steps/s = 1.66, tokens/s = 77735 (34263 source, 43472 target) ; Learning rate = 0.000527 ; Loss = 1.480013\n",
      "2024-12-02 15:32:06.922000: I runner.py:310] Step = 28200 ; steps/s = 1.65, tokens/s = 78518 (34589 source, 43929 target) ; Learning rate = 0.000526 ; Loss = 1.497104\n",
      "2024-12-02 15:33:07.632000: I runner.py:310] Step = 28300 ; steps/s = 1.65, tokens/s = 78460 (34542 source, 43918 target) ; Learning rate = 0.000525 ; Loss = 1.491316\n",
      "2024-12-02 15:34:07.958000: I runner.py:310] Step = 28400 ; steps/s = 1.66, tokens/s = 77599 (34211 source, 43388 target) ; Learning rate = 0.000524 ; Loss = 1.482061\n",
      "2024-12-02 15:35:08.658000: I runner.py:310] Step = 28500 ; steps/s = 1.65, tokens/s = 78506 (34577 source, 43929 target) ; Learning rate = 0.000524 ; Loss = 1.490130\n",
      "2024-12-02 15:36:08.928000: I runner.py:310] Step = 28600 ; steps/s = 1.66, tokens/s = 77660 (34247 source, 43413 target) ; Learning rate = 0.000523 ; Loss = 1.483407\n",
      "2024-12-02 15:37:09.593000: I runner.py:310] Step = 28700 ; steps/s = 1.65, tokens/s = 78513 (34552 source, 43961 target) ; Learning rate = 0.000522 ; Loss = 1.479452\n",
      "2024-12-02 15:38:10.310000: I runner.py:310] Step = 28800 ; steps/s = 1.65, tokens/s = 78504 (34586 source, 43918 target) ; Learning rate = 0.000521 ; Loss = 1.493453\n",
      "2024-12-02 15:39:10.558000: I runner.py:310] Step = 28900 ; steps/s = 1.66, tokens/s = 77711 (34265 source, 43446 target) ; Learning rate = 0.000520 ; Loss = 1.480732\n",
      "2024-12-02 15:40:11.272000: I runner.py:310] Step = 29000 ; steps/s = 1.65, tokens/s = 78484 (34564 source, 43920 target) ; Learning rate = 0.000519 ; Loss = 1.479529\n",
      "2024-12-02 15:41:11.539000: I runner.py:310] Step = 29100 ; steps/s = 1.66, tokens/s = 77669 (34245 source, 43424 target) ; Learning rate = 0.000518 ; Loss = 1.483038\n",
      "2024-12-02 15:42:12.340000: I runner.py:310] Step = 29200 ; steps/s = 1.64, tokens/s = 78350 (34490 source, 43860 target) ; Learning rate = 0.000517 ; Loss = 1.482281\n",
      "2024-12-02 15:43:13.102000: I runner.py:310] Step = 29300 ; steps/s = 1.65, tokens/s = 78433 (34551 source, 43882 target) ; Learning rate = 0.000516 ; Loss = 1.480906\n",
      "2024-12-02 15:44:13.303000: I runner.py:310] Step = 29400 ; steps/s = 1.66, tokens/s = 77761 (34286 source, 43475 target) ; Learning rate = 0.000515 ; Loss = 1.476374\n",
      "2024-12-02 15:45:14.059000: I runner.py:310] Step = 29500 ; steps/s = 1.65, tokens/s = 78448 (34552 source, 43896 target) ; Learning rate = 0.000515 ; Loss = 1.489404\n",
      "2024-12-02 15:46:14.772000: I runner.py:310] Step = 29600 ; steps/s = 1.65, tokens/s = 78468 (34552 source, 43916 target) ; Learning rate = 0.000514 ; Loss = 1.486302\n",
      "2024-12-02 15:47:15.063000: I runner.py:310] Step = 29700 ; steps/s = 1.66, tokens/s = 77625 (34221 source, 43404 target) ; Learning rate = 0.000513 ; Loss = 1.480753\n",
      "2024-12-02 15:48:15.795000: I runner.py:310] Step = 29800 ; steps/s = 1.65, tokens/s = 78485 (34567 source, 43918 target) ; Learning rate = 0.000512 ; Loss = 1.489232\n",
      "2024-12-02 15:49:16.088000: I runner.py:310] Step = 29900 ; steps/s = 1.66, tokens/s = 77623 (34223 source, 43400 target) ; Learning rate = 0.000511 ; Loss = 1.479401\n",
      "2024-12-02 15:50:16.806000: I runner.py:310] Step = 30000 ; steps/s = 1.65, tokens/s = 78487 (34560 source, 43927 target) ; Learning rate = 0.000510 ; Loss = 1.484668\n",
      "2024-12-02 15:50:18.605000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-30000\n",
      "2024-12-02 15:50:18.606000: I training.py:192] Running evaluation for step 30000\n",
      "2024-12-02 15:51:05.992000: I training.py:192] Evaluation result for step 30000: loss = 0.743458 ; perplexity = 2.103196\n",
      "2024-12-02 15:52:06.700000: I runner.py:310] Step = 30100 ; steps/s = 1.65, tokens/s = 78515 (34578 source, 43937 target) ; Learning rate = 0.000509 ; Loss = 1.484081\n",
      "2024-12-02 15:53:07.004000: I runner.py:310] Step = 30200 ; steps/s = 1.66, tokens/s = 77613 (34224 source, 43389 target) ; Learning rate = 0.000509 ; Loss = 1.481196\n",
      "2024-12-02 15:54:07.831000: I runner.py:310] Step = 30300 ; steps/s = 1.64, tokens/s = 78350 (34507 source, 43843 target) ; Learning rate = 0.000508 ; Loss = 1.483662\n",
      "2024-12-02 15:55:08.211000: I runner.py:310] Step = 30400 ; steps/s = 1.66, tokens/s = 77530 (34189 source, 43341 target) ; Learning rate = 0.000507 ; Loss = 1.477142\n",
      "2024-12-02 15:56:09.013000: I runner.py:310] Step = 30500 ; steps/s = 1.64, tokens/s = 78403 (34535 source, 43868 target) ; Learning rate = 0.000506 ; Loss = 1.489512\n",
      "2024-12-02 15:57:09.785000: I runner.py:310] Step = 30600 ; steps/s = 1.65, tokens/s = 78393 (34521 source, 43872 target) ; Learning rate = 0.000505 ; Loss = 1.482781\n",
      "2024-12-02 15:58:10.077000: I runner.py:310] Step = 30700 ; steps/s = 1.66, tokens/s = 77590 (34181 source, 43409 target) ; Learning rate = 0.000504 ; Loss = 1.481402\n",
      "2024-12-02 15:59:10.861000: I runner.py:310] Step = 30800 ; steps/s = 1.65, tokens/s = 78392 (34510 source, 43882 target) ; Learning rate = 0.000504 ; Loss = 1.488407\n",
      "2024-12-02 16:00:11.623000: I runner.py:310] Step = 30900 ; steps/s = 1.65, tokens/s = 78456 (34577 source, 43879 target) ; Learning rate = 0.000503 ; Loss = 1.487782\n",
      "2024-12-02 16:01:11.960000: I runner.py:310] Step = 31000 ; steps/s = 1.66, tokens/s = 77583 (34206 source, 43377 target) ; Learning rate = 0.000502 ; Loss = 1.477114\n",
      "2024-12-02 16:02:12.775000: I runner.py:310] Step = 31100 ; steps/s = 1.64, tokens/s = 78355 (34510 source, 43845 target) ; Learning rate = 0.000501 ; Loss = 1.482967\n",
      "2024-12-02 16:03:13.168000: I runner.py:310] Step = 31200 ; steps/s = 1.66, tokens/s = 77497 (34167 source, 43330 target) ; Learning rate = 0.000500 ; Loss = 1.472981\n",
      "2024-12-02 16:04:14.860000: I runner.py:310] Step = 31300 ; steps/s = 1.62, tokens/s = 77222 (34003 source, 43219 target) ; Learning rate = 0.000500 ; Loss = 1.474692\n",
      "2024-12-02 16:05:16.423000: I runner.py:310] Step = 31400 ; steps/s = 1.62, tokens/s = 77428 (34101 source, 43327 target) ; Learning rate = 0.000499 ; Loss = 1.483613\n",
      "2024-12-02 16:06:17.617000: I runner.py:310] Step = 31500 ; steps/s = 1.63, tokens/s = 76474 (33712 source, 42762 target) ; Learning rate = 0.000498 ; Loss = 1.478878\n",
      "2024-12-02 16:07:20.386000: I runner.py:310] Step = 31600 ; steps/s = 1.59, tokens/s = 75914 (33432 source, 42482 target) ; Learning rate = 0.000497 ; Loss = 1.481614\n",
      "2024-12-02 16:08:22.275000: I runner.py:310] Step = 31700 ; steps/s = 1.62, tokens/s = 75640 (33353 source, 42287 target) ; Learning rate = 0.000496 ; Loss = 1.469997\n",
      "2024-12-02 16:09:23.846000: I runner.py:310] Step = 31800 ; steps/s = 1.62, tokens/s = 77364 (34041 source, 43323 target) ; Learning rate = 0.000496 ; Loss = 1.478396\n",
      "2024-12-02 16:10:24.639000: I runner.py:310] Step = 31900 ; steps/s = 1.65, tokens/s = 78430 (34574 source, 43856 target) ; Learning rate = 0.000495 ; Loss = 1.481020\n",
      "2024-12-02 16:11:24.987000: I runner.py:310] Step = 32000 ; steps/s = 1.66, tokens/s = 77544 (34178 source, 43366 target) ; Learning rate = 0.000494 ; Loss = 1.479314\n",
      "2024-12-02 16:12:25.782000: I runner.py:310] Step = 32100 ; steps/s = 1.65, tokens/s = 78397 (34533 source, 43864 target) ; Learning rate = 0.000493 ; Loss = 1.479958\n",
      "2024-12-02 16:13:26.599000: I runner.py:310] Step = 32200 ; steps/s = 1.64, tokens/s = 78360 (34508 source, 43852 target) ; Learning rate = 0.000493 ; Loss = 1.484213\n",
      "2024-12-02 16:14:26.967000: I runner.py:310] Step = 32300 ; steps/s = 1.66, tokens/s = 77535 (34183 source, 43352 target) ; Learning rate = 0.000492 ; Loss = 1.474180\n",
      "2024-12-02 16:15:27.774000: I runner.py:310] Step = 32400 ; steps/s = 1.64, tokens/s = 78374 (34518 source, 43856 target) ; Learning rate = 0.000491 ; Loss = 1.482311\n",
      "2024-12-02 16:16:28.109000: I runner.py:310] Step = 32500 ; steps/s = 1.66, tokens/s = 77581 (34197 source, 43384 target) ; Learning rate = 0.000490 ; Loss = 1.477980\n",
      "2024-12-02 16:17:28.928000: I runner.py:310] Step = 32600 ; steps/s = 1.64, tokens/s = 78330 (34485 source, 43845 target) ; Learning rate = 0.000490 ; Loss = 1.475796\n",
      "2024-12-02 16:18:29.666000: I runner.py:310] Step = 32700 ; steps/s = 1.65, tokens/s = 78471 (34569 source, 43902 target) ; Learning rate = 0.000489 ; Loss = 1.481020\n",
      "2024-12-02 16:19:29.999000: I runner.py:310] Step = 32800 ; steps/s = 1.66, tokens/s = 77549 (34177 source, 43372 target) ; Learning rate = 0.000488 ; Loss = 1.473520\n",
      "2024-12-02 16:20:30.786000: I runner.py:310] Step = 32900 ; steps/s = 1.65, tokens/s = 78398 (34532 source, 43866 target) ; Learning rate = 0.000487 ; Loss = 1.480560\n",
      "2024-12-02 16:21:31.037000: I runner.py:310] Step = 33000 ; steps/s = 1.66, tokens/s = 77719 (34280 source, 43439 target) ; Learning rate = 0.000487 ; Loss = 1.472799\n",
      "2024-12-02 16:22:31.854000: I runner.py:310] Step = 33100 ; steps/s = 1.64, tokens/s = 78375 (34515 source, 43860 target) ; Learning rate = 0.000486 ; Loss = 1.476768\n",
      "2024-12-02 16:23:32.565000: I runner.py:310] Step = 33200 ; steps/s = 1.65, tokens/s = 78492 (34575 source, 43917 target) ; Learning rate = 0.000485 ; Loss = 1.480525\n",
      "2024-12-02 16:24:32.887000: I runner.py:310] Step = 33300 ; steps/s = 1.66, tokens/s = 77550 (34167 source, 43383 target) ; Learning rate = 0.000484 ; Loss = 1.472877\n",
      "2024-12-02 16:25:33.607000: I runner.py:310] Step = 33400 ; steps/s = 1.65, tokens/s = 78469 (34558 source, 43911 target) ; Learning rate = 0.000484 ; Loss = 1.478240\n",
      "2024-12-02 16:26:34.345000: I runner.py:310] Step = 33500 ; steps/s = 1.65, tokens/s = 78422 (34550 source, 43872 target) ; Learning rate = 0.000483 ; Loss = 1.490579\n",
      "2024-12-02 16:27:34.590000: I runner.py:310] Step = 33600 ; steps/s = 1.66, tokens/s = 77731 (34250 source, 43481 target) ; Learning rate = 0.000482 ; Loss = 1.472002\n",
      "2024-12-02 16:28:35.332000: I runner.py:310] Step = 33700 ; steps/s = 1.65, tokens/s = 78480 (34581 source, 43899 target) ; Learning rate = 0.000481 ; Loss = 1.481818\n",
      "2024-12-02 16:29:35.648000: I runner.py:310] Step = 33800 ; steps/s = 1.66, tokens/s = 77603 (34212 source, 43391 target) ; Learning rate = 0.000481 ; Loss = 1.475309\n",
      "2024-12-02 16:30:36.411000: I runner.py:310] Step = 33900 ; steps/s = 1.65, tokens/s = 78428 (34536 source, 43892 target) ; Learning rate = 0.000480 ; Loss = 1.480273\n",
      "2024-12-02 16:31:37.216000: I runner.py:310] Step = 34000 ; steps/s = 1.64, tokens/s = 78371 (34517 source, 43854 target) ; Learning rate = 0.000479 ; Loss = 1.481378\n",
      "2024-12-02 16:32:37.520000: I runner.py:310] Step = 34100 ; steps/s = 1.66, tokens/s = 77625 (34233 source, 43392 target) ; Learning rate = 0.000479 ; Loss = 1.475166\n",
      "2024-12-02 16:33:38.273000: I runner.py:310] Step = 34200 ; steps/s = 1.65, tokens/s = 78443 (34546 source, 43897 target) ; Learning rate = 0.000478 ; Loss = 1.477349\n",
      "2024-12-02 16:34:38.549000: I runner.py:310] Step = 34300 ; steps/s = 1.66, tokens/s = 77661 (34240 source, 43421 target) ; Learning rate = 0.000477 ; Loss = 1.468766\n",
      "2024-12-02 16:35:39.313000: I runner.py:310] Step = 34400 ; steps/s = 1.65, tokens/s = 78399 (34519 source, 43880 target) ; Learning rate = 0.000477 ; Loss = 1.468295\n",
      "2024-12-02 16:36:40.123000: I runner.py:310] Step = 34500 ; steps/s = 1.64, tokens/s = 78379 (34523 source, 43856 target) ; Learning rate = 0.000476 ; Loss = 1.474895\n",
      "2024-12-02 16:37:40.444000: I runner.py:310] Step = 34600 ; steps/s = 1.66, tokens/s = 77587 (34203 source, 43384 target) ; Learning rate = 0.000475 ; Loss = 1.468023\n",
      "2024-12-02 16:38:41.159000: I runner.py:310] Step = 34700 ; steps/s = 1.65, tokens/s = 78515 (34590 source, 43925 target) ; Learning rate = 0.000474 ; Loss = 1.477317\n",
      "2024-12-02 16:39:41.824000: I runner.py:310] Step = 34800 ; steps/s = 1.65, tokens/s = 78142 (34432 source, 43710 target) ; Learning rate = 0.000474 ; Loss = 1.499372\n",
      "2024-12-02 16:40:42.235000: I runner.py:310] Step = 34900 ; steps/s = 1.66, tokens/s = 77879 (34304 source, 43575 target) ; Learning rate = 0.000473 ; Loss = 1.478143\n",
      "2024-12-02 16:41:42.958000: I runner.py:310] Step = 35000 ; steps/s = 1.65, tokens/s = 78478 (34568 source, 43910 target) ; Learning rate = 0.000472 ; Loss = 1.479379\n",
      "2024-12-02 16:41:42.959000: I training.py:192] Running evaluation for step 35000\n",
      "2024-12-02 16:42:30.619000: I training.py:192] Evaluation result for step 35000: loss = 0.757549 ; perplexity = 2.133042\n",
      "2024-12-02 16:43:30.781000: I runner.py:310] Step = 35100 ; steps/s = 1.66, tokens/s = 77804 (34291 source, 43513 target) ; Learning rate = 0.000472 ; Loss = 1.470010\n",
      "2024-12-02 16:44:31.502000: I runner.py:310] Step = 35200 ; steps/s = 1.65, tokens/s = 78462 (34553 source, 43909 target) ; Learning rate = 0.000471 ; Loss = 1.472499\n",
      "2024-12-02 16:45:32.240000: I runner.py:310] Step = 35300 ; steps/s = 1.65, tokens/s = 78472 (34566 source, 43906 target) ; Learning rate = 0.000470 ; Loss = 1.479354\n",
      "2024-12-02 16:46:32.524000: I runner.py:310] Step = 35400 ; steps/s = 1.66, tokens/s = 77637 (34218 source, 43419 target) ; Learning rate = 0.000470 ; Loss = 1.472024\n",
      "2024-12-02 16:47:33.229000: I runner.py:310] Step = 35500 ; steps/s = 1.65, tokens/s = 78484 (34554 source, 43930 target) ; Learning rate = 0.000469 ; Loss = 1.471774\n",
      "2024-12-02 16:48:33.537000: I runner.py:310] Step = 35600 ; steps/s = 1.66, tokens/s = 77652 (34258 source, 43394 target) ; Learning rate = 0.000468 ; Loss = 1.464390\n",
      "2024-12-02 16:49:34.281000: I runner.py:310] Step = 35700 ; steps/s = 1.65, tokens/s = 78457 (34562 source, 43895 target) ; Learning rate = 0.000468 ; Loss = 1.475306\n",
      "2024-12-02 16:50:35.056000: I runner.py:310] Step = 35800 ; steps/s = 1.65, tokens/s = 78409 (34523 source, 43886 target) ; Learning rate = 0.000467 ; Loss = 1.479492\n",
      "2024-12-02 16:51:35.391000: I runner.py:310] Step = 35900 ; steps/s = 1.66, tokens/s = 77574 (34198 source, 43376 target) ; Learning rate = 0.000466 ; Loss = 1.473377\n",
      "2024-12-02 16:52:36.128000: I runner.py:310] Step = 36000 ; steps/s = 1.65, tokens/s = 78468 (34550 source, 43918 target) ; Learning rate = 0.000466 ; Loss = 1.477601\n",
      "2024-12-02 16:53:36.425000: I runner.py:310] Step = 36100 ; steps/s = 1.66, tokens/s = 77852 (34344 source, 43508 target) ; Learning rate = 0.000465 ; Loss = 1.461743\n",
      "2024-12-02 16:54:37.127000: I runner.py:310] Step = 36200 ; steps/s = 1.65, tokens/s = 78236 (34435 source, 43801 target) ; Learning rate = 0.000465 ; Loss = 1.468729\n",
      "2024-12-02 16:55:37.906000: I runner.py:310] Step = 36300 ; steps/s = 1.65, tokens/s = 78421 (34549 source, 43872 target) ; Learning rate = 0.000464 ; Loss = 1.479210\n",
      "2024-12-02 16:56:38.238000: I runner.py:310] Step = 36400 ; steps/s = 1.66, tokens/s = 77618 (34222 source, 43396 target) ; Learning rate = 0.000463 ; Loss = 1.469816\n",
      "2024-12-02 16:57:39.044000: I runner.py:310] Step = 36500 ; steps/s = 1.64, tokens/s = 78375 (34518 source, 43857 target) ; Learning rate = 0.000463 ; Loss = 1.475014\n",
      "2024-12-02 16:58:39.803000: I runner.py:310] Step = 36600 ; steps/s = 1.65, tokens/s = 78413 (34522 source, 43891 target) ; Learning rate = 0.000462 ; Loss = 1.476410\n",
      "2024-12-02 16:59:40.123000: I runner.py:310] Step = 36700 ; steps/s = 1.66, tokens/s = 77598 (34215 source, 43383 target) ; Learning rate = 0.000461 ; Loss = 1.472077\n",
      "2024-12-02 17:00:40.907000: I runner.py:310] Step = 36800 ; steps/s = 1.65, tokens/s = 78398 (34531 source, 43867 target) ; Learning rate = 0.000461 ; Loss = 1.471321\n",
      "2024-12-02 17:01:41.235000: I runner.py:310] Step = 36900 ; steps/s = 1.66, tokens/s = 77583 (34202 source, 43381 target) ; Learning rate = 0.000460 ; Loss = 1.474160\n",
      "2024-12-02 17:02:41.998000: I runner.py:310] Step = 37000 ; steps/s = 1.65, tokens/s = 78437 (34534 source, 43903 target) ; Learning rate = 0.000460 ; Loss = 1.470662\n",
      "2024-12-02 17:03:42.753000: I runner.py:310] Step = 37100 ; steps/s = 1.65, tokens/s = 78436 (34551 source, 43885 target) ; Learning rate = 0.000459 ; Loss = 1.472617\n",
      "2024-12-02 17:04:43.050000: I runner.py:310] Step = 37200 ; steps/s = 1.66, tokens/s = 77611 (34219 source, 43392 target) ; Learning rate = 0.000458 ; Loss = 1.469426\n",
      "2024-12-02 17:05:43.785000: I runner.py:310] Step = 37300 ; steps/s = 1.65, tokens/s = 78467 (34552 source, 43915 target) ; Learning rate = 0.000458 ; Loss = 1.470268\n",
      "2024-12-02 17:06:44.090000: I runner.py:310] Step = 37400 ; steps/s = 1.66, tokens/s = 77629 (34236 source, 43393 target) ; Learning rate = 0.000457 ; Loss = 1.469878\n",
      "2024-12-02 17:07:44.877000: I runner.py:310] Step = 37500 ; steps/s = 1.65, tokens/s = 78346 (34480 source, 43866 target) ; Learning rate = 0.000456 ; Loss = 1.469112\n",
      "2024-12-02 17:08:45.606000: I runner.py:310] Step = 37600 ; steps/s = 1.65, tokens/s = 78481 (34561 source, 43920 target) ; Learning rate = 0.000456 ; Loss = 1.473141\n",
      "2024-12-02 17:09:45.861000: I runner.py:310] Step = 37700 ; steps/s = 1.66, tokens/s = 77698 (34268 source, 43430 target) ; Learning rate = 0.000455 ; Loss = 1.471375\n",
      "2024-12-02 17:10:46.684000: I runner.py:310] Step = 37800 ; steps/s = 1.64, tokens/s = 78349 (34501 source, 43848 target) ; Learning rate = 0.000455 ; Loss = 1.478860\n",
      "2024-12-02 17:11:47.459000: I runner.py:310] Step = 37900 ; steps/s = 1.65, tokens/s = 78405 (34533 source, 43872 target) ; Learning rate = 0.000454 ; Loss = 1.464741\n",
      "2024-12-02 17:12:47.751000: I runner.py:310] Step = 38000 ; steps/s = 1.66, tokens/s = 77639 (34225 source, 43414 target) ; Learning rate = 0.000453 ; Loss = 1.472213\n",
      "2024-12-02 17:13:48.507000: I runner.py:310] Step = 38100 ; steps/s = 1.65, tokens/s = 78432 (34548 source, 43884 target) ; Learning rate = 0.000453 ; Loss = 1.475834\n",
      "2024-12-02 17:14:48.791000: I runner.py:310] Step = 38200 ; steps/s = 1.66, tokens/s = 77661 (34246 source, 43415 target) ; Learning rate = 0.000452 ; Loss = 1.469106\n",
      "2024-12-02 17:15:49.538000: I runner.py:310] Step = 38300 ; steps/s = 1.65, tokens/s = 78432 (34531 source, 43901 target) ; Learning rate = 0.000452 ; Loss = 1.466057\n",
      "2024-12-02 17:16:50.337000: I runner.py:310] Step = 38400 ; steps/s = 1.65, tokens/s = 78366 (34505 source, 43861 target) ; Learning rate = 0.000451 ; Loss = 1.465543\n",
      "2024-12-02 17:17:50.698000: I runner.py:310] Step = 38500 ; steps/s = 1.66, tokens/s = 77573 (34210 source, 43363 target) ; Learning rate = 0.000450 ; Loss = 1.471877\n",
      "2024-12-02 17:18:51.496000: I runner.py:310] Step = 38600 ; steps/s = 1.65, tokens/s = 78366 (34507 source, 43859 target) ; Learning rate = 0.000450 ; Loss = 1.464469\n",
      "2024-12-02 17:19:51.826000: I runner.py:310] Step = 38700 ; steps/s = 1.66, tokens/s = 77607 (34231 source, 43376 target) ; Learning rate = 0.000449 ; Loss = 1.466488\n",
      "2024-12-02 17:20:52.608000: I runner.py:310] Step = 38800 ; steps/s = 1.65, tokens/s = 78352 (34474 source, 43878 target) ; Learning rate = 0.000449 ; Loss = 1.456427\n",
      "2024-12-02 17:21:53.359000: I runner.py:310] Step = 38900 ; steps/s = 1.65, tokens/s = 78459 (34567 source, 43892 target) ; Learning rate = 0.000448 ; Loss = 1.477196\n",
      "2024-12-02 17:22:53.694000: I runner.py:310] Step = 39000 ; steps/s = 1.66, tokens/s = 77611 (34232 source, 43379 target) ; Learning rate = 0.000448 ; Loss = 1.465164\n",
      "2024-12-02 17:23:54.487000: I runner.py:310] Step = 39100 ; steps/s = 1.65, tokens/s = 78372 (34507 source, 43865 target) ; Learning rate = 0.000447 ; Loss = 1.464997\n",
      "2024-12-02 17:24:55.319000: I runner.py:310] Step = 39200 ; steps/s = 1.64, tokens/s = 78345 (34502 source, 43843 target) ; Learning rate = 0.000446 ; Loss = 1.468759\n",
      "2024-12-02 17:25:55.582000: I runner.py:310] Step = 39300 ; steps/s = 1.66, tokens/s = 77633 (34212 source, 43421 target) ; Learning rate = 0.000446 ; Loss = 1.464861\n",
      "2024-12-02 17:26:56.347000: I runner.py:310] Step = 39400 ; steps/s = 1.65, tokens/s = 78454 (34573 source, 43881 target) ; Learning rate = 0.000445 ; Loss = 1.469577\n",
      "2024-12-02 17:27:56.647000: I runner.py:310] Step = 39500 ; steps/s = 1.66, tokens/s = 77620 (34215 source, 43405 target) ; Learning rate = 0.000445 ; Loss = 1.464458\n",
      "2024-12-02 17:28:57.349000: I runner.py:310] Step = 39600 ; steps/s = 1.65, tokens/s = 78480 (34546 source, 43934 target) ; Learning rate = 0.000444 ; Loss = 1.462325\n",
      "2024-12-02 17:29:58.101000: I runner.py:310] Step = 39700 ; steps/s = 1.65, tokens/s = 78463 (34562 source, 43901 target) ; Learning rate = 0.000444 ; Loss = 1.474824\n",
      "2024-12-02 17:30:58.440000: I runner.py:310] Step = 39800 ; steps/s = 1.66, tokens/s = 77606 (34231 source, 43375 target) ; Learning rate = 0.000443 ; Loss = 1.471510\n",
      "2024-12-02 17:31:59.306000: I runner.py:310] Step = 39900 ; steps/s = 1.64, tokens/s = 78282 (34472 source, 43810 target) ; Learning rate = 0.000442 ; Loss = 1.470591\n",
      "2024-12-02 17:32:59.545000: I runner.py:310] Step = 40000 ; steps/s = 1.66, tokens/s = 77705 (34267 source, 43438 target) ; Learning rate = 0.000442 ; Loss = 1.466667\n",
      "2024-12-02 17:33:01.406000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-40000\n",
      "2024-12-02 17:33:01.406000: I training.py:192] Running evaluation for step 40000\n",
      "2024-12-02 17:33:49.431000: I training.py:192] Evaluation result for step 40000: loss = 0.769291 ; perplexity = 2.158235\n",
      "2024-12-02 17:34:50.172000: I runner.py:310] Step = 40100 ; steps/s = 1.65, tokens/s = 78432 (34515 source, 43917 target) ; Learning rate = 0.000441 ; Loss = 1.462907\n",
      "2024-12-02 17:35:51.004000: I runner.py:310] Step = 40200 ; steps/s = 1.64, tokens/s = 78355 (34523 source, 43832 target) ; Learning rate = 0.000441 ; Loss = 1.470637\n",
      "2024-12-02 17:36:51.290000: I runner.py:310] Step = 40300 ; steps/s = 1.66, tokens/s = 77646 (34238 source, 43408 target) ; Learning rate = 0.000440 ; Loss = 1.460559\n",
      "2024-12-02 17:37:52.074000: I runner.py:310] Step = 40400 ; steps/s = 1.65, tokens/s = 78389 (34518 source, 43871 target) ; Learning rate = 0.000440 ; Loss = 1.462694\n",
      "2024-12-02 17:38:52.847000: I runner.py:310] Step = 40500 ; steps/s = 1.65, tokens/s = 78424 (34541 source, 43883 target) ; Learning rate = 0.000439 ; Loss = 1.473154\n",
      "2024-12-02 17:39:53.147000: I runner.py:310] Step = 40600 ; steps/s = 1.66, tokens/s = 77593 (34193 source, 43400 target) ; Learning rate = 0.000439 ; Loss = 1.465501\n",
      "2024-12-02 17:40:53.946000: I runner.py:310] Step = 40700 ; steps/s = 1.64, tokens/s = 78381 (34516 source, 43865 target) ; Learning rate = 0.000438 ; Loss = 1.465819\n",
      "2024-12-02 17:41:54.212000: I runner.py:310] Step = 40800 ; steps/s = 1.66, tokens/s = 77700 (34271 source, 43429 target) ; Learning rate = 0.000438 ; Loss = 1.462201\n",
      "2024-12-02 17:42:54.952000: I runner.py:310] Step = 40900 ; steps/s = 1.65, tokens/s = 78447 (34544 source, 43903 target) ; Learning rate = 0.000437 ; Loss = 1.465534\n",
      "2024-12-02 17:43:55.696000: I runner.py:310] Step = 41000 ; steps/s = 1.65, tokens/s = 78446 (34549 source, 43897 target) ; Learning rate = 0.000437 ; Loss = 1.465250\n",
      "2024-12-02 17:44:56.053000: I runner.py:310] Step = 41100 ; steps/s = 1.66, tokens/s = 77563 (34191 source, 43372 target) ; Learning rate = 0.000436 ; Loss = 1.464818\n",
      "2024-12-02 17:45:56.713000: I runner.py:310] Step = 41200 ; steps/s = 1.65, tokens/s = 78568 (34611 source, 43957 target) ; Learning rate = 0.000435 ; Loss = 1.468743\n",
      "2024-12-02 17:46:57.003000: I runner.py:310] Step = 41300 ; steps/s = 1.66, tokens/s = 77631 (34229 source, 43402 target) ; Learning rate = 0.000435 ; Loss = 1.458227\n",
      "2024-12-02 17:47:57.778000: I runner.py:310] Step = 41400 ; steps/s = 1.65, tokens/s = 78399 (34509 source, 43890 target) ; Learning rate = 0.000434 ; Loss = 1.465162\n",
      "2024-12-02 17:48:58.524000: I runner.py:310] Step = 41500 ; steps/s = 1.65, tokens/s = 78456 (34554 source, 43902 target) ; Learning rate = 0.000434 ; Loss = 1.467845\n",
      "2024-12-02 17:49:58.804000: I runner.py:310] Step = 41600 ; steps/s = 1.66, tokens/s = 77662 (34250 source, 43412 target) ; Learning rate = 0.000433 ; Loss = 1.463072\n",
      "2024-12-02 17:50:59.561000: I runner.py:310] Step = 41700 ; steps/s = 1.65, tokens/s = 78439 (34549 source, 43890 target) ; Learning rate = 0.000433 ; Loss = 1.468829\n",
      "2024-12-02 17:52:00.317000: I runner.py:310] Step = 41800 ; steps/s = 1.65, tokens/s = 78436 (34547 source, 43889 target) ; Learning rate = 0.000432 ; Loss = 1.462069\n",
      "2024-12-02 17:53:00.593000: I runner.py:310] Step = 41900 ; steps/s = 1.66, tokens/s = 77633 (34205 source, 43428 target) ; Learning rate = 0.000432 ; Loss = 1.461372\n",
      "2024-12-02 17:54:01.336000: I runner.py:310] Step = 42000 ; steps/s = 1.65, tokens/s = 78456 (34566 source, 43890 target) ; Learning rate = 0.000431 ; Loss = 1.468668\n",
      "2024-12-02 17:55:01.649000: I runner.py:310] Step = 42100 ; steps/s = 1.66, tokens/s = 77612 (34217 source, 43395 target) ; Learning rate = 0.000431 ; Loss = 1.460447\n",
      "2024-12-02 17:56:02.413000: I runner.py:310] Step = 42200 ; steps/s = 1.65, tokens/s = 78392 (34502 source, 43890 target) ; Learning rate = 0.000430 ; Loss = 1.460129\n",
      "2024-12-02 17:57:03.211000: I runner.py:310] Step = 42300 ; steps/s = 1.65, tokens/s = 78407 (34544 source, 43863 target) ; Learning rate = 0.000430 ; Loss = 1.464633\n",
      "2024-12-02 17:58:03.501000: I runner.py:310] Step = 42400 ; steps/s = 1.66, tokens/s = 77625 (34219 source, 43406 target) ; Learning rate = 0.000429 ; Loss = 1.472477\n",
      "2024-12-02 17:59:04.256000: I runner.py:310] Step = 42500 ; steps/s = 1.65, tokens/s = 78437 (34552 source, 43885 target) ; Learning rate = 0.000429 ; Loss = 1.456560\n",
      "2024-12-02 18:00:04.622000: I runner.py:310] Step = 42600 ; steps/s = 1.66, tokens/s = 77555 (34201 source, 43354 target) ; Learning rate = 0.000428 ; Loss = 1.462172\n",
      "2024-12-02 18:01:05.370000: I runner.py:310] Step = 42700 ; steps/s = 1.65, tokens/s = 78453 (34543 source, 43910 target) ; Learning rate = 0.000428 ; Loss = 1.466605\n",
      "2024-12-02 18:02:06.159000: I runner.py:310] Step = 42800 ; steps/s = 1.65, tokens/s = 78393 (34536 source, 43857 target) ; Learning rate = 0.000427 ; Loss = 1.464874\n",
      "2024-12-02 18:03:06.510000: I runner.py:310] Step = 42900 ; steps/s = 1.66, tokens/s = 77545 (34179 source, 43366 target) ; Learning rate = 0.000427 ; Loss = 1.461027\n",
      "2024-12-02 18:04:07.196000: I runner.py:310] Step = 43000 ; steps/s = 1.65, tokens/s = 78511 (34569 source, 43942 target) ; Learning rate = 0.000426 ; Loss = 1.461031\n",
      "2024-12-02 18:05:07.915000: I runner.py:310] Step = 43100 ; steps/s = 1.65, tokens/s = 78497 (34577 source, 43920 target) ; Learning rate = 0.000426 ; Loss = 1.470828\n",
      "2024-12-02 18:06:08.203000: I runner.py:310] Step = 43200 ; steps/s = 1.66, tokens/s = 77655 (34242 source, 43413 target) ; Learning rate = 0.000425 ; Loss = 1.464026\n",
      "2024-12-02 18:07:09.032000: I runner.py:310] Step = 43300 ; steps/s = 1.64, tokens/s = 78337 (34498 source, 43839 target) ; Learning rate = 0.000425 ; Loss = 1.465249\n",
      "2024-12-02 18:08:09.308000: I runner.py:310] Step = 43400 ; steps/s = 1.66, tokens/s = 77643 (34226 source, 43417 target) ; Learning rate = 0.000424 ; Loss = 1.460883\n",
      "2024-12-02 18:09:10.011000: I runner.py:310] Step = 43500 ; steps/s = 1.65, tokens/s = 78487 (34562 source, 43925 target) ; Learning rate = 0.000424 ; Loss = 1.458264\n",
      "2024-12-02 18:10:10.812000: I runner.py:310] Step = 43600 ; steps/s = 1.64, tokens/s = 78380 (34523 source, 43857 target) ; Learning rate = 0.000423 ; Loss = 1.464149\n",
      "2024-12-02 18:11:11.164000: I runner.py:310] Step = 43700 ; steps/s = 1.66, tokens/s = 77530 (34163 source, 43367 target) ; Learning rate = 0.000423 ; Loss = 1.455984\n",
      "2024-12-02 18:12:11.944000: I runner.py:310] Step = 43800 ; steps/s = 1.65, tokens/s = 78435 (34559 source, 43876 target) ; Learning rate = 0.000422 ; Loss = 1.462199\n",
      "2024-12-02 18:13:12.270000: I runner.py:310] Step = 43900 ; steps/s = 1.66, tokens/s = 77603 (34224 source, 43379 target) ; Learning rate = 0.000422 ; Loss = 1.453505\n",
      "2024-12-02 18:14:13.067000: I runner.py:310] Step = 44000 ; steps/s = 1.65, tokens/s = 78396 (34509 source, 43887 target) ; Learning rate = 0.000421 ; Loss = 1.460872\n",
      "2024-12-02 18:15:13.831000: I runner.py:310] Step = 44100 ; steps/s = 1.65, tokens/s = 78412 (34537 source, 43875 target) ; Learning rate = 0.000421 ; Loss = 1.463079\n",
      "2024-12-02 18:16:14.132000: I runner.py:310] Step = 44200 ; steps/s = 1.66, tokens/s = 77601 (34206 source, 43395 target) ; Learning rate = 0.000420 ; Loss = 1.462200\n",
      "2024-12-02 18:17:14.909000: I runner.py:310] Step = 44300 ; steps/s = 1.65, tokens/s = 78409 (34538 source, 43871 target) ; Learning rate = 0.000420 ; Loss = 1.464025\n",
      "2024-12-02 18:18:15.591000: I runner.py:310] Step = 44400 ; steps/s = 1.65, tokens/s = 78541 (34597 source, 43944 target) ; Learning rate = 0.000419 ; Loss = 1.461724\n",
      "2024-12-02 18:19:15.956000: I runner.py:310] Step = 44500 ; steps/s = 1.66, tokens/s = 77546 (34191 source, 43355 target) ; Learning rate = 0.000419 ; Loss = 1.465063\n",
      "2024-12-02 18:20:16.715000: I runner.py:310] Step = 44600 ; steps/s = 1.65, tokens/s = 78438 (34546 source, 43892 target) ; Learning rate = 0.000419 ; Loss = 1.463333\n",
      "2024-12-02 18:21:17.005000: I runner.py:310] Step = 44700 ; steps/s = 1.66, tokens/s = 77635 (34231 source, 43404 target) ; Learning rate = 0.000418 ; Loss = 1.458118\n",
      "2024-12-02 18:22:17.766000: I runner.py:310] Step = 44800 ; steps/s = 1.65, tokens/s = 78409 (34520 source, 43889 target) ; Learning rate = 0.000418 ; Loss = 1.468569\n",
      "2024-12-02 18:23:18.503000: I runner.py:310] Step = 44900 ; steps/s = 1.65, tokens/s = 78465 (34559 source, 43906 target) ; Learning rate = 0.000417 ; Loss = 1.463178\n",
      "2024-12-02 18:24:18.757000: I runner.py:310] Step = 45000 ; steps/s = 1.66, tokens/s = 77668 (34232 source, 43436 target) ; Learning rate = 0.000417 ; Loss = 1.460587\n",
      "2024-12-02 18:24:18.759000: I training.py:192] Running evaluation for step 45000\n",
      "2024-12-02 18:25:06.760000: I training.py:192] Evaluation result for step 45000: loss = 0.784616 ; perplexity = 2.191566\n",
      "2024-12-02 18:26:07.377000: I runner.py:310] Step = 45100 ; steps/s = 1.65, tokens/s = 78654 (34647 source, 44007 target) ; Learning rate = 0.000416 ; Loss = 1.465952\n",
      "2024-12-02 18:27:07.620000: I runner.py:310] Step = 45200 ; steps/s = 1.66, tokens/s = 77689 (34255 source, 43434 target) ; Learning rate = 0.000416 ; Loss = 1.455924\n",
      "2024-12-02 18:28:08.408000: I runner.py:310] Step = 45300 ; steps/s = 1.65, tokens/s = 78334 (34470 source, 43864 target) ; Learning rate = 0.000415 ; Loss = 1.456642\n",
      "2024-12-02 18:29:09.182000: I runner.py:310] Step = 45400 ; steps/s = 1.65, tokens/s = 78450 (34566 source, 43884 target) ; Learning rate = 0.000415 ; Loss = 1.463571\n",
      "2024-12-02 18:30:09.485000: I runner.py:310] Step = 45500 ; steps/s = 1.66, tokens/s = 77631 (34228 source, 43403 target) ; Learning rate = 0.000414 ; Loss = 1.455402\n",
      "2024-12-02 18:31:10.209000: I runner.py:310] Step = 45600 ; steps/s = 1.65, tokens/s = 78491 (34577 source, 43914 target) ; Learning rate = 0.000414 ; Loss = 1.462963\n",
      "2024-12-02 18:32:10.913000: I runner.py:310] Step = 45700 ; steps/s = 1.65, tokens/s = 78491 (34567 source, 43924 target) ; Learning rate = 0.000413 ; Loss = 1.457863\n",
      "2024-12-02 18:33:11.203000: I runner.py:310] Step = 45800 ; steps/s = 1.66, tokens/s = 77637 (34225 source, 43412 target) ; Learning rate = 0.000413 ; Loss = 1.456190\n",
      "2024-12-02 18:34:11.986000: I runner.py:310] Step = 45900 ; steps/s = 1.65, tokens/s = 78431 (34564 source, 43867 target) ; Learning rate = 0.000413 ; Loss = 1.460794\n",
      "2024-12-02 18:35:12.365000: I runner.py:310] Step = 46000 ; steps/s = 1.66, tokens/s = 77491 (34140 source, 43351 target) ; Learning rate = 0.000412 ; Loss = 1.457905\n",
      "2024-12-02 18:36:13.007000: I runner.py:310] Step = 46100 ; steps/s = 1.65, tokens/s = 78572 (34605 source, 43967 target) ; Learning rate = 0.000412 ; Loss = 1.464448\n",
      "2024-12-02 18:37:13.792000: I runner.py:310] Step = 46200 ; steps/s = 1.65, tokens/s = 78394 (34521 source, 43873 target) ; Learning rate = 0.000411 ; Loss = 1.454776\n",
      "2024-12-02 18:38:14.099000: I runner.py:310] Step = 46300 ; steps/s = 1.66, tokens/s = 77613 (34217 source, 43396 target) ; Learning rate = 0.000411 ; Loss = 1.468328\n",
      "2024-12-02 18:39:14.838000: I runner.py:310] Step = 46400 ; steps/s = 1.65, tokens/s = 78455 (34558 source, 43897 target) ; Learning rate = 0.000410 ; Loss = 1.461343\n",
      "2024-12-02 18:40:15.063000: I runner.py:310] Step = 46500 ; steps/s = 1.66, tokens/s = 77722 (34266 source, 43456 target) ; Learning rate = 0.000410 ; Loss = 1.451675\n",
      "2024-12-02 18:41:15.800000: I runner.py:310] Step = 46600 ; steps/s = 1.65, tokens/s = 78446 (34540 source, 43906 target) ; Learning rate = 0.000409 ; Loss = 1.455722\n",
      "2024-12-02 18:42:16.550000: I runner.py:310] Step = 46700 ; steps/s = 1.65, tokens/s = 78460 (34559 source, 43901 target) ; Learning rate = 0.000409 ; Loss = 1.460573\n",
      "2024-12-02 18:43:16.884000: I runner.py:310] Step = 46800 ; steps/s = 1.66, tokens/s = 77576 (34188 source, 43388 target) ; Learning rate = 0.000409 ; Loss = 1.459621\n",
      "2024-12-02 18:44:17.576000: I runner.py:310] Step = 46900 ; steps/s = 1.65, tokens/s = 78537 (34596 source, 43941 target) ; Learning rate = 0.000408 ; Loss = 1.458984\n",
      "2024-12-02 18:45:18.381000: I runner.py:310] Step = 47000 ; steps/s = 1.64, tokens/s = 78353 (34505 source, 43848 target) ; Learning rate = 0.000408 ; Loss = 1.457448\n",
      "2024-12-02 18:46:18.708000: I runner.py:310] Step = 47100 ; steps/s = 1.66, tokens/s = 77583 (34201 source, 43382 target) ; Learning rate = 0.000407 ; Loss = 1.454842\n",
      "2024-12-02 18:47:19.474000: I runner.py:310] Step = 47200 ; steps/s = 1.65, tokens/s = 78425 (34530 source, 43895 target) ; Learning rate = 0.000407 ; Loss = 1.458465\n",
      "2024-12-02 18:48:19.726000: I runner.py:310] Step = 47300 ; steps/s = 1.66, tokens/s = 77695 (34270 source, 43425 target) ; Learning rate = 0.000406 ; Loss = 1.460645\n",
      "2024-12-02 18:49:20.468000: I runner.py:310] Step = 47400 ; steps/s = 1.65, tokens/s = 78444 (34539 source, 43905 target) ; Learning rate = 0.000406 ; Loss = 1.455612\n",
      "2024-12-02 18:50:21.263000: I runner.py:310] Step = 47500 ; steps/s = 1.65, tokens/s = 78376 (34517 source, 43859 target) ; Learning rate = 0.000406 ; Loss = 1.460092\n",
      "2024-12-02 18:51:21.590000: I runner.py:310] Step = 47600 ; steps/s = 1.66, tokens/s = 77615 (34228 source, 43387 target) ; Learning rate = 0.000405 ; Loss = 1.457941\n",
      "2024-12-02 18:52:22.270000: I runner.py:310] Step = 47700 ; steps/s = 1.65, tokens/s = 78507 (34565 source, 43942 target) ; Learning rate = 0.000405 ; Loss = 1.462536\n",
      "2024-12-02 18:53:22.610000: I runner.py:310] Step = 47800 ; steps/s = 1.66, tokens/s = 77591 (34216 source, 43375 target) ; Learning rate = 0.000404 ; Loss = 1.451264\n",
      "2024-12-02 18:54:23.409000: I runner.py:310] Step = 47900 ; steps/s = 1.64, tokens/s = 78348 (34494 source, 43854 target) ; Learning rate = 0.000404 ; Loss = 1.454375\n",
      "2024-12-02 18:55:24.198000: I runner.py:310] Step = 48000 ; steps/s = 1.65, tokens/s = 78411 (34540 source, 43871 target) ; Learning rate = 0.000403 ; Loss = 1.461877\n",
      "2024-12-02 18:56:24.454000: I runner.py:310] Step = 48100 ; steps/s = 1.66, tokens/s = 77676 (34241 source, 43435 target) ; Learning rate = 0.000403 ; Loss = 1.451765\n",
      "2024-12-02 18:57:25.215000: I runner.py:310] Step = 48200 ; steps/s = 1.65, tokens/s = 78432 (34549 source, 43883 target) ; Learning rate = 0.000403 ; Loss = 1.458162\n",
      "2024-12-02 18:58:25.927000: I runner.py:310] Step = 48300 ; steps/s = 1.65, tokens/s = 78499 (34569 source, 43930 target) ; Learning rate = 0.000402 ; Loss = 1.463195\n",
      "2024-12-02 18:59:26.199000: I runner.py:310] Step = 48400 ; steps/s = 1.66, tokens/s = 77647 (34225 source, 43422 target) ; Learning rate = 0.000402 ; Loss = 1.453227\n",
      "2024-12-02 19:00:26.955000: I runner.py:310] Step = 48500 ; steps/s = 1.65, tokens/s = 78442 (34557 source, 43885 target) ; Learning rate = 0.000401 ; Loss = 1.460372\n",
      "2024-12-02 19:01:27.298000: I runner.py:310] Step = 48600 ; steps/s = 1.66, tokens/s = 77583 (34202 source, 43381 target) ; Learning rate = 0.000401 ; Loss = 1.453527\n",
      "2024-12-02 19:02:28.064000: I runner.py:310] Step = 48700 ; steps/s = 1.65, tokens/s = 78443 (34553 source, 43890 target) ; Learning rate = 0.000401 ; Loss = 1.456683\n",
      "2024-12-02 19:03:28.823000: I runner.py:310] Step = 48800 ; steps/s = 1.65, tokens/s = 78418 (34533 source, 43885 target) ; Learning rate = 0.000400 ; Loss = 1.460966\n",
      "2024-12-02 19:04:29.053000: I runner.py:310] Step = 48900 ; steps/s = 1.66, tokens/s = 77704 (34252 source, 43452 target) ; Learning rate = 0.000400 ; Loss = 1.458120\n",
      "2024-12-02 19:05:29.784000: I runner.py:310] Step = 49000 ; steps/s = 1.65, tokens/s = 78451 (34552 source, 43899 target) ; Learning rate = 0.000399 ; Loss = 1.456113\n",
      "2024-12-02 19:06:30.138000: I runner.py:310] Step = 49100 ; steps/s = 1.66, tokens/s = 77572 (34200 source, 43372 target) ; Learning rate = 0.000399 ; Loss = 1.457300\n",
      "2024-12-02 19:07:30.856000: I runner.py:310] Step = 49200 ; steps/s = 1.65, tokens/s = 78477 (34560 source, 43917 target) ; Learning rate = 0.000398 ; Loss = 1.461202\n",
      "2024-12-02 19:08:31.557000: I runner.py:310] Step = 49300 ; steps/s = 1.65, tokens/s = 78497 (34563 source, 43934 target) ; Learning rate = 0.000398 ; Loss = 1.462154\n",
      "2024-12-02 19:09:31.844000: I runner.py:310] Step = 49400 ; steps/s = 1.66, tokens/s = 77637 (34220 source, 43417 target) ; Learning rate = 0.000398 ; Loss = 1.458947\n",
      "2024-12-02 19:10:32.643000: I runner.py:310] Step = 49500 ; steps/s = 1.64, tokens/s = 78361 (34511 source, 43850 target) ; Learning rate = 0.000397 ; Loss = 1.460439\n",
      "2024-12-02 19:11:33.429000: I runner.py:310] Step = 49600 ; steps/s = 1.65, tokens/s = 78415 (34545 source, 43870 target) ; Learning rate = 0.000397 ; Loss = 1.457044\n",
      "2024-12-02 19:12:33.732000: I runner.py:310] Step = 49700 ; steps/s = 1.66, tokens/s = 77595 (34198 source, 43397 target) ; Learning rate = 0.000396 ; Loss = 1.456455\n",
      "2024-12-02 19:13:34.552000: I runner.py:310] Step = 49800 ; steps/s = 1.64, tokens/s = 78371 (34535 source, 43836 target) ; Learning rate = 0.000396 ; Loss = 1.457783\n",
      "2024-12-02 19:14:34.891000: I runner.py:310] Step = 49900 ; steps/s = 1.66, tokens/s = 77584 (34203 source, 43381 target) ; Learning rate = 0.000396 ; Loss = 1.454181\n",
      "2024-12-02 19:15:35.644000: I runner.py:310] Step = 50000 ; steps/s = 1.65, tokens/s = 78449 (34551 source, 43898 target) ; Learning rate = 0.000395 ; Loss = 1.462248\n",
      "2024-12-02 19:15:37.497000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-50000\n",
      "2024-12-02 19:15:37.497000: I training.py:192] Running evaluation for step 50000\n",
      "2024-12-02 19:16:25.243000: I training.py:192] Evaluation result for step 50000: loss = 0.785799 ; perplexity = 2.194159\n",
      "2024-12-02 19:17:25.830000: I runner.py:310] Step = 50100 ; steps/s = 1.65, tokens/s = 78647 (34629 source, 44018 target) ; Learning rate = 0.000395 ; Loss = 1.459539\n",
      "2024-12-02 19:18:26.086000: I runner.py:310] Step = 50200 ; steps/s = 1.66, tokens/s = 77690 (34259 source, 43431 target) ; Learning rate = 0.000394 ; Loss = 1.459356\n",
      "2024-12-02 19:19:26.846000: I runner.py:310] Step = 50300 ; steps/s = 1.65, tokens/s = 78409 (34514 source, 43895 target) ; Learning rate = 0.000394 ; Loss = 1.451286\n",
      "2024-12-02 19:20:27.150000: I runner.py:310] Step = 50400 ; steps/s = 1.66, tokens/s = 77632 (34237 source, 43395 target) ; Learning rate = 0.000394 ; Loss = 1.453667\n",
      "2024-12-02 19:21:27.943000: I runner.py:310] Step = 50500 ; steps/s = 1.65, tokens/s = 78366 (34503 source, 43863 target) ; Learning rate = 0.000393 ; Loss = 1.452357\n",
      "2024-12-02 19:22:28.657000: I runner.py:310] Step = 50600 ; steps/s = 1.65, tokens/s = 78500 (34581 source, 43919 target) ; Learning rate = 0.000393 ; Loss = 1.457555\n",
      "2024-12-02 19:23:28.925000: I runner.py:310] Step = 50700 ; steps/s = 1.66, tokens/s = 77656 (34237 source, 43419 target) ; Learning rate = 0.000393 ; Loss = 1.455979\n",
      "2024-12-02 19:24:29.681000: I runner.py:310] Step = 50800 ; steps/s = 1.65, tokens/s = 78419 (34518 source, 43901 target) ; Learning rate = 0.000392 ; Loss = 1.456184\n",
      "2024-12-02 19:25:30.454000: I runner.py:310] Step = 50900 ; steps/s = 1.65, tokens/s = 78431 (34558 source, 43873 target) ; Learning rate = 0.000392 ; Loss = 1.457896\n",
      "2024-12-02 19:26:30.775000: I runner.py:310] Step = 51000 ; steps/s = 1.66, tokens/s = 77618 (34227 source, 43391 target) ; Learning rate = 0.000391 ; Loss = 1.451421\n",
      "2024-12-02 19:27:31.525000: I runner.py:310] Step = 51100 ; steps/s = 1.65, tokens/s = 78445 (34546 source, 43899 target) ; Learning rate = 0.000391 ; Loss = 1.457361\n",
      "2024-12-02 19:28:31.848000: I runner.py:310] Step = 51200 ; steps/s = 1.66, tokens/s = 77563 (34190 source, 43373 target) ; Learning rate = 0.000391 ; Loss = 1.451599\n",
      "2024-12-02 19:29:32.558000: I runner.py:310] Step = 51300 ; steps/s = 1.65, tokens/s = 78477 (34549 source, 43928 target) ; Learning rate = 0.000390 ; Loss = 1.456063\n",
      "2024-12-02 19:30:33.393000: I runner.py:310] Step = 51400 ; steps/s = 1.64, tokens/s = 78356 (34517 source, 43839 target) ; Learning rate = 0.000390 ; Loss = 1.461280\n",
      "2024-12-02 19:31:33.750000: I runner.py:310] Step = 51500 ; steps/s = 1.66, tokens/s = 77562 (34205 source, 43357 target) ; Learning rate = 0.000389 ; Loss = 1.460910\n",
      "2024-12-02 19:32:34.544000: I runner.py:310] Step = 51600 ; steps/s = 1.65, tokens/s = 78374 (34516 source, 43858 target) ; Learning rate = 0.000389 ; Loss = 1.459748\n",
      "2024-12-02 19:33:34.842000: I runner.py:310] Step = 51700 ; steps/s = 1.66, tokens/s = 77634 (34224 source, 43410 target) ; Learning rate = 0.000389 ; Loss = 1.456232\n",
      "2024-12-02 19:34:35.585000: I runner.py:310] Step = 51800 ; steps/s = 1.65, tokens/s = 78443 (34546 source, 43897 target) ; Learning rate = 0.000388 ; Loss = 1.450668\n",
      "2024-12-02 19:35:36.378000: I runner.py:310] Step = 51900 ; steps/s = 1.65, tokens/s = 78399 (34526 source, 43873 target) ; Learning rate = 0.000388 ; Loss = 1.458929\n",
      "2024-12-02 19:36:36.651000: I runner.py:310] Step = 52000 ; steps/s = 1.66, tokens/s = 77623 (34204 source, 43419 target) ; Learning rate = 0.000388 ; Loss = 1.454690\n",
      "2024-12-02 19:37:37.368000: I runner.py:310] Step = 52100 ; steps/s = 1.65, tokens/s = 78489 (34562 source, 43927 target) ; Learning rate = 0.000387 ; Loss = 1.457951\n",
      "2024-12-02 19:38:38.076000: I runner.py:310] Step = 52200 ; steps/s = 1.65, tokens/s = 78324 (34517 source, 43807 target) ; Learning rate = 0.000387 ; Loss = 1.471566\n",
      "2024-12-02 19:39:38.318000: I runner.py:310] Step = 52300 ; steps/s = 1.66, tokens/s = 77876 (34318 source, 43558 target) ; Learning rate = 0.000386 ; Loss = 1.456401\n",
      "2024-12-02 19:40:39.065000: I runner.py:310] Step = 52400 ; steps/s = 1.65, tokens/s = 78441 (34547 source, 43894 target) ; Learning rate = 0.000386 ; Loss = 1.455540\n",
      "2024-12-02 19:41:39.327000: I runner.py:310] Step = 52500 ; steps/s = 1.66, tokens/s = 77696 (34266 source, 43430 target) ; Learning rate = 0.000386 ; Loss = 1.454378\n",
      "2024-12-02 19:42:40.127000: I runner.py:310] Step = 52600 ; steps/s = 1.64, tokens/s = 78360 (34497 source, 43863 target) ; Learning rate = 0.000385 ; Loss = 1.451329\n",
      "2024-12-02 19:43:40.903000: I runner.py:310] Step = 52700 ; steps/s = 1.65, tokens/s = 78420 (34544 source, 43876 target) ; Learning rate = 0.000385 ; Loss = 1.458433\n",
      "2024-12-02 19:44:41.211000: I runner.py:310] Step = 52800 ; steps/s = 1.66, tokens/s = 77618 (34217 source, 43401 target) ; Learning rate = 0.000385 ; Loss = 1.454524\n",
      "2024-12-02 19:45:41.998000: I runner.py:310] Step = 52900 ; steps/s = 1.65, tokens/s = 78377 (34511 source, 43866 target) ; Learning rate = 0.000384 ; Loss = 1.449622\n",
      "2024-12-02 19:46:42.296000: I runner.py:310] Step = 53000 ; steps/s = 1.66, tokens/s = 77662 (34264 source, 43398 target) ; Learning rate = 0.000384 ; Loss = 1.455705\n",
      "2024-12-02 19:47:43.079000: I runner.py:310] Step = 53100 ; steps/s = 1.65, tokens/s = 78403 (34530 source, 43873 target) ; Learning rate = 0.000384 ; Loss = 1.454551\n",
      "2024-12-02 19:48:43.789000: I runner.py:310] Step = 53200 ; steps/s = 1.65, tokens/s = 78483 (34548 source, 43935 target) ; Learning rate = 0.000383 ; Loss = 1.453778\n",
      "2024-12-02 19:49:44.092000: I runner.py:310] Step = 53300 ; steps/s = 1.66, tokens/s = 77603 (34217 source, 43386 target) ; Learning rate = 0.000383 ; Loss = 1.453224\n",
      "2024-12-02 19:50:44.869000: I runner.py:310] Step = 53400 ; steps/s = 1.65, tokens/s = 78392 (34519 source, 43873 target) ; Learning rate = 0.000382 ; Loss = 1.457688\n",
      "2024-12-02 19:51:45.404000: I runner.py:310] Step = 53500 ; steps/s = 1.65, tokens/s = 78130 (34457 source, 43673 target) ; Learning rate = 0.000382 ; Loss = 1.471622\n",
      "2024-12-02 19:52:45.885000: I runner.py:310] Step = 53600 ; steps/s = 1.65, tokens/s = 78020 (34356 source, 43664 target) ; Learning rate = 0.000382 ; Loss = 1.451153\n",
      "2024-12-02 19:53:46.649000: I runner.py:310] Step = 53700 ; steps/s = 1.65, tokens/s = 78412 (34524 source, 43888 target) ; Learning rate = 0.000381 ; Loss = 1.450365\n",
      "2024-12-02 19:54:46.958000: I runner.py:310] Step = 53800 ; steps/s = 1.66, tokens/s = 77597 (34219 source, 43378 target) ; Learning rate = 0.000381 ; Loss = 1.454411\n",
      "2024-12-02 19:55:47.696000: I runner.py:310] Step = 53900 ; steps/s = 1.65, tokens/s = 78442 (34539 source, 43903 target) ; Learning rate = 0.000381 ; Loss = 1.450429\n",
      "2024-12-02 19:56:48.508000: I runner.py:310] Step = 54000 ; steps/s = 1.64, tokens/s = 78367 (34508 source, 43859 target) ; Learning rate = 0.000380 ; Loss = 1.456007\n",
      "2024-12-02 19:57:48.781000: I runner.py:310] Step = 54100 ; steps/s = 1.66, tokens/s = 77669 (34244 source, 43425 target) ; Learning rate = 0.000380 ; Loss = 1.457003\n",
      "2024-12-02 19:58:49.575000: I runner.py:310] Step = 54200 ; steps/s = 1.65, tokens/s = 78389 (34528 source, 43861 target) ; Learning rate = 0.000380 ; Loss = 1.455714\n",
      "2024-12-02 19:59:49.875000: I runner.py:310] Step = 54300 ; steps/s = 1.66, tokens/s = 77640 (34237 source, 43403 target) ; Learning rate = 0.000379 ; Loss = 1.449099\n",
      "2024-12-02 20:00:50.623000: I runner.py:310] Step = 54400 ; steps/s = 1.65, tokens/s = 78442 (34532 source, 43910 target) ; Learning rate = 0.000379 ; Loss = 1.456678\n",
      "2024-12-02 20:01:51.430000: I runner.py:310] Step = 54500 ; steps/s = 1.64, tokens/s = 78369 (34524 source, 43845 target) ; Learning rate = 0.000379 ; Loss = 1.454143\n",
      "2024-12-02 20:02:51.722000: I runner.py:310] Step = 54600 ; steps/s = 1.66, tokens/s = 77643 (34232 source, 43411 target) ; Learning rate = 0.000378 ; Loss = 1.451094\n",
      "2024-12-02 20:03:52.484000: I runner.py:310] Step = 54700 ; steps/s = 1.65, tokens/s = 78389 (34505 source, 43884 target) ; Learning rate = 0.000378 ; Loss = 1.453325\n",
      "2024-12-02 20:04:52.804000: I runner.py:310] Step = 54800 ; steps/s = 1.66, tokens/s = 77642 (34260 source, 43382 target) ; Learning rate = 0.000378 ; Loss = 1.451280\n",
      "2024-12-02 20:05:53.456000: I runner.py:310] Step = 54900 ; steps/s = 1.65, tokens/s = 78549 (34577 source, 43972 target) ; Learning rate = 0.000377 ; Loss = 1.451483\n",
      "2024-12-02 20:06:54.235000: I runner.py:310] Step = 55000 ; steps/s = 1.65, tokens/s = 78425 (34548 source, 43877 target) ; Learning rate = 0.000377 ; Loss = 1.454685\n",
      "2024-12-02 20:06:54.236000: I training.py:192] Running evaluation for step 55000\n",
      "2024-12-02 20:07:42.195000: I training.py:192] Evaluation result for step 55000: loss = 0.792830 ; perplexity = 2.209641\n",
      "2024-12-02 20:08:42.367000: I runner.py:310] Step = 55100 ; steps/s = 1.66, tokens/s = 77770 (34268 source, 43502 target) ; Learning rate = 0.000377 ; Loss = 1.445732\n",
      "2024-12-02 20:09:43.113000: I runner.py:310] Step = 55200 ; steps/s = 1.65, tokens/s = 78459 (34555 source, 43904 target) ; Learning rate = 0.000376 ; Loss = 1.458513\n",
      "2024-12-02 20:10:43.921000: I runner.py:310] Step = 55300 ; steps/s = 1.64, tokens/s = 78367 (34516 source, 43851 target) ; Learning rate = 0.000376 ; Loss = 1.453931\n",
      "2024-12-02 20:11:44.188000: I runner.py:310] Step = 55400 ; steps/s = 1.66, tokens/s = 77647 (34225 source, 43422 target) ; Learning rate = 0.000376 ; Loss = 1.453107\n",
      "2024-12-02 20:12:44.920000: I runner.py:310] Step = 55500 ; steps/s = 1.65, tokens/s = 78466 (34571 source, 43895 target) ; Learning rate = 0.000375 ; Loss = 1.450763\n",
      "2024-12-02 20:13:45.223000: I runner.py:310] Step = 55600 ; steps/s = 1.66, tokens/s = 77633 (34230 source, 43403 target) ; Learning rate = 0.000375 ; Loss = 1.456924\n",
      "2024-12-02 20:14:45.974000: I runner.py:310] Step = 55700 ; steps/s = 1.65, tokens/s = 78422 (34523 source, 43899 target) ; Learning rate = 0.000375 ; Loss = 1.451680\n",
      "2024-12-02 20:15:46.762000: I runner.py:310] Step = 55800 ; steps/s = 1.65, tokens/s = 78413 (34543 source, 43870 target) ; Learning rate = 0.000374 ; Loss = 1.453041\n",
      "2024-12-02 20:16:47.029000: I runner.py:310] Step = 55900 ; steps/s = 1.66, tokens/s = 77667 (34237 source, 43430 target) ; Learning rate = 0.000374 ; Loss = 1.446614\n",
      "2024-12-02 20:17:47.789000: I runner.py:310] Step = 56000 ; steps/s = 1.65, tokens/s = 78408 (34520 source, 43888 target) ; Learning rate = 0.000374 ; Loss = 1.456447\n",
      "2024-12-02 20:18:48.039000: I runner.py:310] Step = 56100 ; steps/s = 1.66, tokens/s = 77725 (34293 source, 43432 target) ; Learning rate = 0.000373 ; Loss = 1.453804\n",
      "2024-12-02 20:19:48.806000: I runner.py:310] Step = 56200 ; steps/s = 1.65, tokens/s = 78381 (34489 source, 43892 target) ; Learning rate = 0.000373 ; Loss = 1.444778\n",
      "2024-12-02 20:20:49.564000: I runner.py:310] Step = 56300 ; steps/s = 1.65, tokens/s = 78435 (34548 source, 43887 target) ; Learning rate = 0.000373 ; Loss = 1.454945\n",
      "2024-12-02 20:21:49.816000: I runner.py:310] Step = 56400 ; steps/s = 1.66, tokens/s = 77691 (34261 source, 43430 target) ; Learning rate = 0.000372 ; Loss = 1.450787\n",
      "2024-12-02 20:22:50.586000: I runner.py:310] Step = 56500 ; steps/s = 1.65, tokens/s = 78440 (34551 source, 43889 target) ; Learning rate = 0.000372 ; Loss = 1.452541\n",
      "2024-12-02 20:23:51.346000: I runner.py:310] Step = 56600 ; steps/s = 1.65, tokens/s = 78426 (34533 source, 43893 target) ; Learning rate = 0.000372 ; Loss = 1.454493\n",
      "2024-12-02 20:24:51.657000: I runner.py:310] Step = 56700 ; steps/s = 1.66, tokens/s = 77587 (34203 source, 43384 target) ; Learning rate = 0.000371 ; Loss = 1.447014\n",
      "2024-12-02 20:25:52.422000: I runner.py:310] Step = 56800 ; steps/s = 1.65, tokens/s = 78432 (34549 source, 43883 target) ; Learning rate = 0.000371 ; Loss = 1.456954\n",
      "2024-12-02 20:26:52.648000: I runner.py:310] Step = 56900 ; steps/s = 1.66, tokens/s = 77731 (34267 source, 43464 target) ; Learning rate = 0.000371 ; Loss = 1.454960\n",
      "2024-12-02 20:27:53.422000: I runner.py:310] Step = 57000 ; steps/s = 1.65, tokens/s = 78392 (34517 source, 43875 target) ; Learning rate = 0.000370 ; Loss = 1.449448\n",
      "2024-12-02 20:28:54.137000: I runner.py:310] Step = 57100 ; steps/s = 1.65, tokens/s = 78510 (34583 source, 43927 target) ; Learning rate = 0.000370 ; Loss = 1.449168\n",
      "2024-12-02 20:29:54.447000: I runner.py:310] Step = 57200 ; steps/s = 1.66, tokens/s = 77612 (34222 source, 43390 target) ; Learning rate = 0.000370 ; Loss = 1.454933\n",
      "2024-12-02 20:30:55.222000: I runner.py:310] Step = 57300 ; steps/s = 1.65, tokens/s = 78395 (34523 source, 43872 target) ; Learning rate = 0.000369 ; Loss = 1.452042\n",
      "2024-12-02 20:31:55.550000: I runner.py:310] Step = 57400 ; steps/s = 1.66, tokens/s = 77620 (34229 source, 43391 target) ; Learning rate = 0.000369 ; Loss = 1.449963\n",
      "2024-12-02 20:32:56.292000: I runner.py:310] Step = 57500 ; steps/s = 1.65, tokens/s = 78409 (34521 source, 43888 target) ; Learning rate = 0.000369 ; Loss = 1.450921\n",
      "2024-12-02 20:33:57.004000: I runner.py:310] Step = 57600 ; steps/s = 1.65, tokens/s = 78501 (34561 source, 43940 target) ; Learning rate = 0.000368 ; Loss = 1.453831\n",
      "2024-12-02 20:34:57.346000: I runner.py:310] Step = 57700 ; steps/s = 1.66, tokens/s = 77581 (34213 source, 43368 target) ; Learning rate = 0.000368 ; Loss = 1.452278\n",
      "2024-12-02 20:35:58.167000: I runner.py:310] Step = 57800 ; steps/s = 1.64, tokens/s = 78340 (34497 source, 43843 target) ; Learning rate = 0.000368 ; Loss = 1.452833\n",
      "2024-12-02 20:36:58.923000: I runner.py:310] Step = 57900 ; steps/s = 1.65, tokens/s = 78457 (34561 source, 43896 target) ; Learning rate = 0.000367 ; Loss = 1.453529\n",
      "2024-12-02 20:37:59.251000: I runner.py:310] Step = 58000 ; steps/s = 1.66, tokens/s = 77563 (34180 source, 43383 target) ; Learning rate = 0.000367 ; Loss = 1.449955\n",
      "2024-12-02 20:38:59.963000: I runner.py:310] Step = 58100 ; steps/s = 1.65, tokens/s = 78487 (34570 source, 43917 target) ; Learning rate = 0.000367 ; Loss = 1.452700\n",
      "2024-12-02 20:40:00.210000: I runner.py:310] Step = 58200 ; steps/s = 1.66, tokens/s = 77728 (34286 source, 43442 target) ; Learning rate = 0.000366 ; Loss = 1.453602\n",
      "2024-12-02 20:41:00.965000: I runner.py:310] Step = 58300 ; steps/s = 1.65, tokens/s = 78406 (34500 source, 43906 target) ; Learning rate = 0.000366 ; Loss = 1.454357\n",
      "2024-12-02 20:42:01.737000: I runner.py:310] Step = 58400 ; steps/s = 1.65, tokens/s = 78454 (34580 source, 43874 target) ; Learning rate = 0.000366 ; Loss = 1.454315\n",
      "2024-12-02 20:43:02.028000: I runner.py:310] Step = 58500 ; steps/s = 1.66, tokens/s = 77634 (34222 source, 43412 target) ; Learning rate = 0.000365 ; Loss = 1.449874\n",
      "2024-12-02 20:44:02.839000: I runner.py:310] Step = 58600 ; steps/s = 1.64, tokens/s = 78345 (34494 source, 43851 target) ; Learning rate = 0.000365 ; Loss = 1.447008\n",
      "2024-12-02 20:45:03.204000: I runner.py:310] Step = 58700 ; steps/s = 1.66, tokens/s = 77556 (34208 source, 43348 target) ; Learning rate = 0.000365 ; Loss = 1.449860\n",
      "2024-12-02 20:46:03.944000: I runner.py:310] Step = 58800 ; steps/s = 1.65, tokens/s = 78435 (34528 source, 43907 target) ; Learning rate = 0.000365 ; Loss = 1.448084\n",
      "2024-12-02 20:47:04.761000: I runner.py:310] Step = 58900 ; steps/s = 1.64, tokens/s = 78382 (34544 source, 43838 target) ; Learning rate = 0.000364 ; Loss = 1.453834\n",
      "2024-12-02 20:48:05.030000: I runner.py:310] Step = 59000 ; steps/s = 1.66, tokens/s = 77637 (34206 source, 43431 target) ; Learning rate = 0.000364 ; Loss = 1.451462\n",
      "2024-12-02 20:49:05.770000: I runner.py:310] Step = 59100 ; steps/s = 1.65, tokens/s = 78447 (34544 source, 43903 target) ; Learning rate = 0.000364 ; Loss = 1.459793\n",
      "2024-12-02 20:50:06.545000: I runner.py:310] Step = 59200 ; steps/s = 1.65, tokens/s = 78418 (34539 source, 43879 target) ; Learning rate = 0.000363 ; Loss = 1.456335\n",
      "2024-12-02 20:51:06.808000: I runner.py:310] Step = 59300 ; steps/s = 1.66, tokens/s = 77662 (34231 source, 43431 target) ; Learning rate = 0.000363 ; Loss = 1.453216\n",
      "2024-12-02 20:52:07.534000: I runner.py:310] Step = 59400 ; steps/s = 1.65, tokens/s = 78468 (34553 source, 43915 target) ; Learning rate = 0.000363 ; Loss = 1.447437\n",
      "2024-12-02 20:53:07.822000: I runner.py:310] Step = 59500 ; steps/s = 1.66, tokens/s = 77652 (34248 source, 43404 target) ; Learning rate = 0.000362 ; Loss = 1.446387\n",
      "2024-12-02 20:54:08.571000: I runner.py:310] Step = 59600 ; steps/s = 1.65, tokens/s = 78439 (34539 source, 43900 target) ; Learning rate = 0.000362 ; Loss = 1.447932\n",
      "2024-12-02 20:55:09.354000: I runner.py:310] Step = 59700 ; steps/s = 1.65, tokens/s = 78416 (34541 source, 43875 target) ; Learning rate = 0.000362 ; Loss = 1.453528\n",
      "2024-12-02 20:56:09.676000: I runner.py:310] Step = 59800 ; steps/s = 1.66, tokens/s = 77595 (34202 source, 43393 target) ; Learning rate = 0.000361 ; Loss = 1.450095\n",
      "2024-12-02 20:57:10.490000: I runner.py:310] Step = 59900 ; steps/s = 1.64, tokens/s = 78361 (34513 source, 43848 target) ; Learning rate = 0.000361 ; Loss = 1.451257\n",
      "2024-12-02 20:58:10.793000: I runner.py:310] Step = 60000 ; steps/s = 1.66, tokens/s = 77632 (34239 source, 43393 target) ; Learning rate = 0.000361 ; Loss = 1.447361\n",
      "2024-12-02 20:58:12.702000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-60000\n",
      "2024-12-02 20:58:12.702000: I training.py:192] Running evaluation for step 60000\n",
      "2024-12-02 20:59:01.579000: I training.py:192] Evaluation result for step 60000: loss = 0.805245 ; perplexity = 2.237245\n",
      "2024-12-02 21:00:02.251000: I runner.py:310] Step = 60100 ; steps/s = 1.65, tokens/s = 78561 (34597 source, 43964 target) ; Learning rate = 0.000361 ; Loss = 1.448931\n",
      "2024-12-02 21:01:02.999000: I runner.py:310] Step = 60200 ; steps/s = 1.65, tokens/s = 78435 (34542 source, 43893 target) ; Learning rate = 0.000360 ; Loss = 1.449295\n",
      "2024-12-02 21:02:03.314000: I runner.py:310] Step = 60300 ; steps/s = 1.66, tokens/s = 77606 (34216 source, 43390 target) ; Learning rate = 0.000360 ; Loss = 1.453521\n",
      "2024-12-02 21:03:04.104000: I runner.py:310] Step = 60400 ; steps/s = 1.65, tokens/s = 78369 (34493 source, 43876 target) ; Learning rate = 0.000360 ; Loss = 1.451176\n",
      "2024-12-02 21:04:04.933000: I runner.py:310] Step = 60500 ; steps/s = 1.64, tokens/s = 78355 (34521 source, 43834 target) ; Learning rate = 0.000359 ; Loss = 1.449869\n",
      "2024-12-02 21:05:05.211000: I runner.py:310] Step = 60600 ; steps/s = 1.66, tokens/s = 77661 (34245 source, 43416 target) ; Learning rate = 0.000359 ; Loss = 1.446924\n",
      "2024-12-02 21:06:05.963000: I runner.py:310] Step = 60700 ; steps/s = 1.65, tokens/s = 78438 (34549 source, 43889 target) ; Learning rate = 0.000359 ; Loss = 1.445887\n",
      "2024-12-02 21:07:06.300000: I runner.py:310] Step = 60800 ; steps/s = 1.66, tokens/s = 77569 (34191 source, 43378 target) ; Learning rate = 0.000358 ; Loss = 1.442518\n",
      "2024-12-02 21:08:07.059000: I runner.py:310] Step = 60900 ; steps/s = 1.65, tokens/s = 78424 (34532 source, 43892 target) ; Learning rate = 0.000358 ; Loss = 1.448939\n",
      "2024-12-02 21:09:07.891000: I runner.py:310] Step = 61000 ; steps/s = 1.64, tokens/s = 78342 (34502 source, 43840 target) ; Learning rate = 0.000358 ; Loss = 1.450321\n",
      "2024-12-02 21:10:08.204000: I runner.py:310] Step = 61100 ; steps/s = 1.66, tokens/s = 77610 (34223 source, 43387 target) ; Learning rate = 0.000358 ; Loss = 1.448961\n",
      "2024-12-02 21:11:08.947000: I runner.py:310] Step = 61200 ; steps/s = 1.65, tokens/s = 78428 (34526 source, 43902 target) ; Learning rate = 0.000357 ; Loss = 1.445912\n",
      "2024-12-02 21:12:09.242000: I runner.py:310] Step = 61300 ; steps/s = 1.66, tokens/s = 77665 (34261 source, 43404 target) ; Learning rate = 0.000357 ; Loss = 1.444574\n",
      "2024-12-02 21:13:09.994000: I runner.py:310] Step = 61400 ; steps/s = 1.65, tokens/s = 78414 (34506 source, 43908 target) ; Learning rate = 0.000357 ; Loss = 1.447085\n",
      "2024-12-02 21:14:10.780000: I runner.py:310] Step = 61500 ; steps/s = 1.65, tokens/s = 78410 (34550 source, 43860 target) ; Learning rate = 0.000356 ; Loss = 1.450077\n",
      "2024-12-02 21:15:11.095000: I runner.py:310] Step = 61600 ; steps/s = 1.66, tokens/s = 77618 (34217 source, 43401 target) ; Learning rate = 0.000356 ; Loss = 1.448999\n",
      "2024-12-02 21:16:11.828000: I runner.py:310] Step = 61700 ; steps/s = 1.65, tokens/s = 78450 (34540 source, 43910 target) ; Learning rate = 0.000356 ; Loss = 1.452105\n",
      "2024-12-02 21:17:12.609000: I runner.py:310] Step = 61800 ; steps/s = 1.65, tokens/s = 78406 (34542 source, 43864 target) ; Learning rate = 0.000356 ; Loss = 1.445313\n",
      "2024-12-02 21:18:12.913000: I runner.py:310] Step = 61900 ; steps/s = 1.66, tokens/s = 77611 (34220 source, 43391 target) ; Learning rate = 0.000355 ; Loss = 1.444493\n",
      "2024-12-02 21:19:13.689000: I runner.py:310] Step = 62000 ; steps/s = 1.65, tokens/s = 78417 (34531 source, 43886 target) ; Learning rate = 0.000355 ; Loss = 1.447361\n",
      "2024-12-02 21:20:13.977000: I runner.py:310] Step = 62100 ; steps/s = 1.66, tokens/s = 77635 (34225 source, 43410 target) ; Learning rate = 0.000355 ; Loss = 1.451660\n",
      "2024-12-02 21:21:14.785000: I runner.py:310] Step = 62200 ; steps/s = 1.64, tokens/s = 78354 (34500 source, 43854 target) ; Learning rate = 0.000354 ; Loss = 1.448070\n",
      "2024-12-02 21:22:15.574000: I runner.py:310] Step = 62300 ; steps/s = 1.65, tokens/s = 78411 (34546 source, 43865 target) ; Learning rate = 0.000354 ; Loss = 1.454028\n",
      "2024-12-02 21:23:15.825000: I runner.py:310] Step = 62400 ; steps/s = 1.66, tokens/s = 77691 (34242 source, 43449 target) ; Learning rate = 0.000354 ; Loss = 1.448768\n",
      "2024-12-02 21:24:16.592000: I runner.py:310] Step = 62500 ; steps/s = 1.65, tokens/s = 78396 (34523 source, 43873 target) ; Learning rate = 0.000354 ; Loss = 1.447030\n",
      "2024-12-02 21:25:16.885000: I runner.py:310] Step = 62600 ; steps/s = 1.66, tokens/s = 77670 (34257 source, 43413 target) ; Learning rate = 0.000353 ; Loss = 1.446193\n",
      "2024-12-02 21:26:17.676000: I runner.py:310] Step = 62700 ; steps/s = 1.65, tokens/s = 78362 (34494 source, 43868 target) ; Learning rate = 0.000353 ; Loss = 1.451235\n",
      "2024-12-02 21:27:18.422000: I runner.py:310] Step = 62800 ; steps/s = 1.65, tokens/s = 78458 (34564 source, 43894 target) ; Learning rate = 0.000353 ; Loss = 1.448772\n",
      "2024-12-02 21:28:18.748000: I runner.py:310] Step = 62900 ; steps/s = 1.66, tokens/s = 77585 (34203 source, 43382 target) ; Learning rate = 0.000352 ; Loss = 1.441639\n",
      "2024-12-02 21:29:19.514000: I runner.py:310] Step = 63000 ; steps/s = 1.65, tokens/s = 78455 (34574 source, 43881 target) ; Learning rate = 0.000352 ; Loss = 1.451908\n",
      "2024-12-02 21:30:20.311000: I runner.py:310] Step = 63100 ; steps/s = 1.65, tokens/s = 78361 (34495 source, 43866 target) ; Learning rate = 0.000352 ; Loss = 1.453691\n",
      "2024-12-02 21:31:20.587000: I runner.py:310] Step = 63200 ; steps/s = 1.66, tokens/s = 77660 (34223 source, 43437 target) ; Learning rate = 0.000352 ; Loss = 1.447780\n",
      "2024-12-02 21:32:21.404000: I runner.py:310] Step = 63300 ; steps/s = 1.64, tokens/s = 78342 (34501 source, 43841 target) ; Learning rate = 0.000351 ; Loss = 1.446816\n",
      "2024-12-02 21:33:21.685000: I runner.py:310] Step = 63400 ; steps/s = 1.66, tokens/s = 77665 (34255 source, 43410 target) ; Learning rate = 0.000351 ; Loss = 1.447584\n",
      "2024-12-02 21:34:22.463000: I runner.py:310] Step = 63500 ; steps/s = 1.65, tokens/s = 78377 (34493 source, 43884 target) ; Learning rate = 0.000351 ; Loss = 1.447916\n",
      "2024-12-02 21:35:23.307000: I runner.py:310] Step = 63600 ; steps/s = 1.64, tokens/s = 78331 (34503 source, 43828 target) ; Learning rate = 0.000350 ; Loss = 1.450118\n",
      "2024-12-02 21:36:23.598000: I runner.py:310] Step = 63700 ; steps/s = 1.66, tokens/s = 77643 (34243 source, 43400 target) ; Learning rate = 0.000350 ; Loss = 1.444697\n",
      "2024-12-02 21:37:24.357000: I runner.py:310] Step = 63800 ; steps/s = 1.65, tokens/s = 78423 (34542 source, 43881 target) ; Learning rate = 0.000350 ; Loss = 1.442077\n",
      "2024-12-02 21:38:24.646000: I runner.py:310] Step = 63900 ; steps/s = 1.66, tokens/s = 77659 (34244 source, 43415 target) ; Learning rate = 0.000350 ; Loss = 1.447443\n",
      "2024-12-02 21:39:25.386000: I runner.py:310] Step = 64000 ; steps/s = 1.65, tokens/s = 78439 (34522 source, 43917 target) ; Learning rate = 0.000349 ; Loss = 1.446103\n",
      "2024-12-02 21:40:26.176000: I runner.py:310] Step = 64100 ; steps/s = 1.65, tokens/s = 78415 (34547 source, 43868 target) ; Learning rate = 0.000349 ; Loss = 1.451084\n",
      "2024-12-02 21:41:26.505000: I runner.py:310] Step = 64200 ; steps/s = 1.66, tokens/s = 77570 (34195 source, 43375 target) ; Learning rate = 0.000349 ; Loss = 1.445499\n",
      "2024-12-02 21:42:27.286000: I runner.py:310] Step = 64300 ; steps/s = 1.65, tokens/s = 78425 (34552 source, 43873 target) ; Learning rate = 0.000349 ; Loss = 1.447463\n",
      "2024-12-02 21:43:28.045000: I runner.py:310] Step = 64400 ; steps/s = 1.65, tokens/s = 78409 (34523 source, 43886 target) ; Learning rate = 0.000348 ; Loss = 1.448724\n",
      "2024-12-02 21:44:28.301000: I runner.py:310] Step = 64500 ; steps/s = 1.66, tokens/s = 77664 (34236 source, 43428 target) ; Learning rate = 0.000348 ; Loss = 1.442952\n",
      "2024-12-02 21:45:29.104000: I runner.py:310] Step = 64600 ; steps/s = 1.64, tokens/s = 78388 (34536 source, 43852 target) ; Learning rate = 0.000348 ; Loss = 1.448420\n",
      "2024-12-02 21:46:29.460000: I runner.py:310] Step = 64700 ; steps/s = 1.66, tokens/s = 77548 (34180 source, 43368 target) ; Learning rate = 0.000347 ; Loss = 1.442000\n",
      "2024-12-02 21:47:30.252000: I runner.py:310] Step = 64800 ; steps/s = 1.65, tokens/s = 78414 (34552 source, 43862 target) ; Learning rate = 0.000347 ; Loss = 1.448593\n",
      "2024-12-02 21:48:30.994000: I runner.py:310] Step = 64900 ; steps/s = 1.65, tokens/s = 78426 (34517 source, 43909 target) ; Learning rate = 0.000347 ; Loss = 1.446761\n",
      "2024-12-02 21:49:31.334000: I runner.py:310] Step = 65000 ; steps/s = 1.66, tokens/s = 77552 (34183 source, 43369 target) ; Learning rate = 0.000347 ; Loss = 1.451282\n",
      "2024-12-02 21:49:31.335000: I training.py:192] Running evaluation for step 65000\n",
      "2024-12-02 21:50:19.087000: I training.py:192] Evaluation result for step 65000: loss = 0.806586 ; perplexity = 2.240247\n",
      "2024-12-02 21:51:19.674000: I runner.py:310] Step = 65100 ; steps/s = 1.65, tokens/s = 78651 (34634 source, 44017 target) ; Learning rate = 0.000346 ; Loss = 1.451347\n",
      "2024-12-02 21:52:19.982000: I runner.py:310] Step = 65200 ; steps/s = 1.66, tokens/s = 77683 (34277 source, 43406 target) ; Learning rate = 0.000346 ; Loss = 1.437555\n",
      "2024-12-02 21:53:20.766000: I runner.py:310] Step = 65300 ; steps/s = 1.65, tokens/s = 78375 (34505 source, 43870 target) ; Learning rate = 0.000346 ; Loss = 1.450395\n",
      "2024-12-02 21:54:21.509000: I runner.py:310] Step = 65400 ; steps/s = 1.65, tokens/s = 78460 (34564 source, 43896 target) ; Learning rate = 0.000346 ; Loss = 1.447395\n",
      "2024-12-02 21:55:21.868000: I runner.py:310] Step = 65500 ; steps/s = 1.66, tokens/s = 77533 (34163 source, 43370 target) ; Learning rate = 0.000345 ; Loss = 1.446517\n",
      "2024-12-02 21:56:22.650000: I runner.py:310] Step = 65600 ; steps/s = 1.65, tokens/s = 78417 (34546 source, 43871 target) ; Learning rate = 0.000345 ; Loss = 1.445839\n",
      "2024-12-02 21:57:23.440000: I runner.py:310] Step = 65700 ; steps/s = 1.65, tokens/s = 78374 (34512 source, 43862 target) ; Learning rate = 0.000345 ; Loss = 1.450566\n",
      "2024-12-02 21:58:23.740000: I runner.py:310] Step = 65800 ; steps/s = 1.66, tokens/s = 77610 (34215 source, 43395 target) ; Learning rate = 0.000345 ; Loss = 1.448883\n",
      "2024-12-02 21:59:24.531000: I runner.py:310] Step = 65900 ; steps/s = 1.65, tokens/s = 78384 (34520 source, 43864 target) ; Learning rate = 0.000344 ; Loss = 1.444894\n",
      "2024-12-02 22:00:24.849000: I runner.py:310] Step = 66000 ; steps/s = 1.66, tokens/s = 77618 (34219 source, 43399 target) ; Learning rate = 0.000344 ; Loss = 1.443073\n",
      "2024-12-02 22:01:25.625000: I runner.py:310] Step = 66100 ; steps/s = 1.65, tokens/s = 78416 (34527 source, 43889 target) ; Learning rate = 0.000344 ; Loss = 1.450361\n",
      "2024-12-02 22:02:26.373000: I runner.py:310] Step = 66200 ; steps/s = 1.65, tokens/s = 78448 (34562 source, 43886 target) ; Learning rate = 0.000344 ; Loss = 1.443250\n",
      "2024-12-02 22:03:26.708000: I runner.py:310] Step = 66300 ; steps/s = 1.66, tokens/s = 77555 (34179 source, 43376 target) ; Learning rate = 0.000343 ; Loss = 1.446016\n",
      "2024-12-02 22:04:27.502000: I runner.py:310] Step = 66400 ; steps/s = 1.65, tokens/s = 78386 (34528 source, 43858 target) ; Learning rate = 0.000343 ; Loss = 1.447120\n",
      "2024-12-02 22:05:27.849000: I runner.py:310] Step = 66500 ; steps/s = 1.66, tokens/s = 77589 (34219 source, 43370 target) ; Learning rate = 0.000343 ; Loss = 1.438219\n",
      "2024-12-02 22:06:28.614000: I runner.py:310] Step = 66600 ; steps/s = 1.65, tokens/s = 78415 (34541 source, 43874 target) ; Learning rate = 0.000342 ; Loss = 1.441194\n",
      "2024-12-02 22:07:29.356000: I runner.py:310] Step = 66700 ; steps/s = 1.65, tokens/s = 78464 (34556 source, 43908 target) ; Learning rate = 0.000342 ; Loss = 1.447412\n",
      "2024-12-02 22:08:29.695000: I runner.py:310] Step = 66800 ; steps/s = 1.66, tokens/s = 77564 (34187 source, 43377 target) ; Learning rate = 0.000342 ; Loss = 1.452264\n",
      "2024-12-02 22:09:30.498000: I runner.py:310] Step = 66900 ; steps/s = 1.64, tokens/s = 78353 (34499 source, 43854 target) ; Learning rate = 0.000342 ; Loss = 1.446196\n",
      "2024-12-02 22:10:31.242000: I runner.py:310] Step = 67000 ; steps/s = 1.65, tokens/s = 78451 (34547 source, 43904 target) ; Learning rate = 0.000341 ; Loss = 1.447890\n",
      "2024-12-02 22:11:31.582000: I runner.py:310] Step = 67100 ; steps/s = 1.66, tokens/s = 77562 (34193 source, 43369 target) ; Learning rate = 0.000341 ; Loss = 1.443518\n",
      "2024-12-02 22:12:32.321000: I runner.py:310] Step = 67200 ; steps/s = 1.65, tokens/s = 78458 (34555 source, 43903 target) ; Learning rate = 0.000341 ; Loss = 1.447394\n",
      "2024-12-02 22:13:32.565000: I runner.py:310] Step = 67300 ; steps/s = 1.66, tokens/s = 77712 (34256 source, 43456 target) ; Learning rate = 0.000341 ; Loss = 1.446214\n",
      "2024-12-02 22:14:33.339000: I runner.py:310] Step = 67400 ; steps/s = 1.65, tokens/s = 78382 (34513 source, 43869 target) ; Learning rate = 0.000340 ; Loss = 1.449649\n",
      "2024-12-02 22:15:34.073000: I runner.py:310] Step = 67500 ; steps/s = 1.65, tokens/s = 78479 (34565 source, 43914 target) ; Learning rate = 0.000340 ; Loss = 1.450745\n",
      "2024-12-02 22:16:34.376000: I runner.py:310] Step = 67600 ; steps/s = 1.66, tokens/s = 77609 (34226 source, 43383 target) ; Learning rate = 0.000340 ; Loss = 1.446229\n",
      "2024-12-02 22:17:35.194000: I runner.py:310] Step = 67700 ; steps/s = 1.64, tokens/s = 78366 (34525 source, 43841 target) ; Learning rate = 0.000340 ; Loss = 1.448438\n",
      "2024-12-02 22:18:35.581000: I runner.py:310] Step = 67800 ; steps/s = 1.66, tokens/s = 77525 (34171 source, 43354 target) ; Learning rate = 0.000339 ; Loss = 1.441362\n",
      "2024-12-02 22:19:36.339000: I runner.py:310] Step = 67900 ; steps/s = 1.65, tokens/s = 78403 (34516 source, 43887 target) ; Learning rate = 0.000339 ; Loss = 1.443991\n",
      "2024-12-02 22:20:37.107000: I runner.py:310] Step = 68000 ; steps/s = 1.65, tokens/s = 78447 (34557 source, 43890 target) ; Learning rate = 0.000339 ; Loss = 1.449677\n",
      "2024-12-02 22:21:37.380000: I runner.py:310] Step = 68100 ; steps/s = 1.66, tokens/s = 77653 (34231 source, 43422 target) ; Learning rate = 0.000339 ; Loss = 1.446265\n",
      "2024-12-02 22:22:38.200000: I runner.py:310] Step = 68200 ; steps/s = 1.64, tokens/s = 78344 (34499 source, 43845 target) ; Learning rate = 0.000338 ; Loss = 1.447986\n",
      "2024-12-02 22:23:38.954000: I runner.py:310] Step = 68300 ; steps/s = 1.65, tokens/s = 78443 (34554 source, 43889 target) ; Learning rate = 0.000338 ; Loss = 1.444018\n",
      "2024-12-02 22:24:39.216000: I runner.py:310] Step = 68400 ; steps/s = 1.66, tokens/s = 77684 (34248 source, 43436 target) ; Learning rate = 0.000338 ; Loss = 1.446166\n",
      "2024-12-02 22:25:39.998000: I runner.py:310] Step = 68500 ; steps/s = 1.65, tokens/s = 78408 (34541 source, 43867 target) ; Learning rate = 0.000338 ; Loss = 1.445208\n",
      "2024-12-02 22:26:40.401000: I runner.py:310] Step = 68600 ; steps/s = 1.66, tokens/s = 77461 (34140 source, 43321 target) ; Learning rate = 0.000337 ; Loss = 1.442992\n",
      "2024-12-02 22:27:41.152000: I runner.py:310] Step = 68700 ; steps/s = 1.65, tokens/s = 78449 (34550 source, 43899 target) ; Learning rate = 0.000337 ; Loss = 1.456015\n",
      "2024-12-02 22:28:41.947000: I runner.py:310] Step = 68800 ; steps/s = 1.65, tokens/s = 78390 (34521 source, 43869 target) ; Learning rate = 0.000337 ; Loss = 1.444890\n",
      "2024-12-02 22:29:42.209000: I runner.py:310] Step = 68900 ; steps/s = 1.66, tokens/s = 77650 (34227 source, 43423 target) ; Learning rate = 0.000337 ; Loss = 1.443645\n",
      "2024-12-02 22:30:42.975000: I runner.py:310] Step = 69000 ; steps/s = 1.65, tokens/s = 78448 (34566 source, 43882 target) ; Learning rate = 0.000336 ; Loss = 1.447410\n",
      "2024-12-02 22:31:43.297000: I runner.py:310] Step = 69100 ; steps/s = 1.66, tokens/s = 77594 (34207 source, 43387 target) ; Learning rate = 0.000336 ; Loss = 1.440682\n",
      "2024-12-02 22:32:44.067000: I runner.py:310] Step = 69200 ; steps/s = 1.65, tokens/s = 78434 (34538 source, 43896 target) ; Learning rate = 0.000336 ; Loss = 1.442074\n",
      "2024-12-02 22:33:44.742000: I runner.py:310] Step = 69300 ; steps/s = 1.65, tokens/s = 78548 (34600 source, 43948 target) ; Learning rate = 0.000336 ; Loss = 1.450083\n",
      "2024-12-02 22:34:45.085000: I runner.py:310] Step = 69400 ; steps/s = 1.66, tokens/s = 77538 (34171 source, 43367 target) ; Learning rate = 0.000336 ; Loss = 1.444258\n",
      "2024-12-02 22:35:45.903000: I runner.py:310] Step = 69500 ; steps/s = 1.64, tokens/s = 78352 (34507 source, 43845 target) ; Learning rate = 0.000335 ; Loss = 1.446471\n",
      "2024-12-02 22:36:46.627000: I runner.py:310] Step = 69600 ; steps/s = 1.65, tokens/s = 78476 (34565 source, 43911 target) ; Learning rate = 0.000335 ; Loss = 1.445847\n",
      "2024-12-02 22:37:46.877000: I runner.py:310] Step = 69700 ; steps/s = 1.66, tokens/s = 77663 (34228 source, 43435 target) ; Learning rate = 0.000335 ; Loss = 1.441199\n",
      "2024-12-02 22:38:47.695000: I runner.py:310] Step = 69800 ; steps/s = 1.64, tokens/s = 78361 (34517 source, 43844 target) ; Learning rate = 0.000335 ; Loss = 1.442864\n",
      "2024-12-02 22:39:48.019000: I runner.py:310] Step = 69900 ; steps/s = 1.66, tokens/s = 77600 (34206 source, 43394 target) ; Learning rate = 0.000334 ; Loss = 1.443798\n",
      "2024-12-02 22:40:48.861000: I runner.py:310] Step = 70000 ; steps/s = 1.64, tokens/s = 78353 (34524 source, 43829 target) ; Learning rate = 0.000334 ; Loss = 1.447993\n",
      "2024-12-02 22:40:50.909000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-70000\n",
      "2024-12-02 22:40:50.909000: I training.py:192] Running evaluation for step 70000\n",
      "2024-12-02 22:41:38.191000: I training.py:192] Evaluation result for step 70000: loss = 0.813175 ; perplexity = 2.255058\n",
      "2024-12-02 22:42:38.882000: I runner.py:310] Step = 70100 ; steps/s = 1.65, tokens/s = 78515 (34569 source, 43946 target) ; Learning rate = 0.000334 ; Loss = 1.451054\n",
      "2024-12-02 22:43:39.190000: I runner.py:310] Step = 70200 ; steps/s = 1.66, tokens/s = 77618 (34230 source, 43388 target) ; Learning rate = 0.000334 ; Loss = 1.445530\n",
      "2024-12-02 22:44:39.936000: I runner.py:310] Step = 70300 ; steps/s = 1.65, tokens/s = 78421 (34529 source, 43892 target) ; Learning rate = 0.000333 ; Loss = 1.440889\n",
      "2024-12-02 22:45:40.278000: I runner.py:310] Step = 70400 ; steps/s = 1.66, tokens/s = 77595 (34206 source, 43389 target) ; Learning rate = 0.000333 ; Loss = 1.440835\n",
      "2024-12-02 22:46:41.063000: I runner.py:310] Step = 70500 ; steps/s = 1.65, tokens/s = 78423 (34548 source, 43875 target) ; Learning rate = 0.000333 ; Loss = 1.447233\n",
      "2024-12-02 22:47:41.837000: I runner.py:310] Step = 70600 ; steps/s = 1.65, tokens/s = 78392 (34518 source, 43874 target) ; Learning rate = 0.000333 ; Loss = 1.444540\n",
      "2024-12-02 22:48:42.109000: I runner.py:310] Step = 70700 ; steps/s = 1.66, tokens/s = 77631 (34217 source, 43414 target) ; Learning rate = 0.000332 ; Loss = 1.446402\n",
      "2024-12-02 22:49:42.844000: I runner.py:310] Step = 70800 ; steps/s = 1.65, tokens/s = 78449 (34545 source, 43904 target) ; Learning rate = 0.000332 ; Loss = 1.445076\n",
      "2024-12-02 22:50:43.482000: I runner.py:310] Step = 70900 ; steps/s = 1.65, tokens/s = 78265 (34498 source, 43767 target) ; Learning rate = 0.000332 ; Loss = 1.453673\n",
      "2024-12-02 22:51:43.895000: I runner.py:310] Step = 71000 ; steps/s = 1.66, tokens/s = 77876 (34330 source, 43546 target) ; Learning rate = 0.000332 ; Loss = 1.441355\n",
      "2024-12-02 22:52:44.671000: I runner.py:310] Step = 71100 ; steps/s = 1.65, tokens/s = 78379 (34517 source, 43862 target) ; Learning rate = 0.000331 ; Loss = 1.443019\n",
      "2024-12-02 22:53:44.982000: I runner.py:310] Step = 71200 ; steps/s = 1.66, tokens/s = 77595 (34190 source, 43405 target) ; Learning rate = 0.000331 ; Loss = 1.442510\n",
      "2024-12-02 22:54:45.707000: I runner.py:310] Step = 71300 ; steps/s = 1.65, tokens/s = 78494 (34570 source, 43924 target) ; Learning rate = 0.000331 ; Loss = 1.444090\n",
      "2024-12-02 22:55:46.447000: I runner.py:310] Step = 71400 ; steps/s = 1.65, tokens/s = 78444 (34541 source, 43903 target) ; Learning rate = 0.000331 ; Loss = 1.442020\n",
      "2024-12-02 22:56:46.738000: I runner.py:310] Step = 71500 ; steps/s = 1.66, tokens/s = 77663 (34254 source, 43409 target) ; Learning rate = 0.000331 ; Loss = 1.443691\n",
      "2024-12-02 22:57:47.491000: I runner.py:310] Step = 71600 ; steps/s = 1.65, tokens/s = 78418 (34527 source, 43891 target) ; Learning rate = 0.000330 ; Loss = 1.446724\n",
      "2024-12-02 22:58:47.806000: I runner.py:310] Step = 71700 ; steps/s = 1.66, tokens/s = 77608 (34222 source, 43386 target) ; Learning rate = 0.000330 ; Loss = 1.446999\n",
      "2024-12-02 22:59:48.538000: I runner.py:310] Step = 71800 ; steps/s = 1.65, tokens/s = 78446 (34536 source, 43910 target) ; Learning rate = 0.000330 ; Loss = 1.445628\n",
      "2024-12-02 23:00:49.292000: I runner.py:310] Step = 71900 ; steps/s = 1.65, tokens/s = 78457 (34572 source, 43885 target) ; Learning rate = 0.000330 ; Loss = 1.439317\n",
      "2024-12-02 23:01:49.681000: I runner.py:310] Step = 72000 ; steps/s = 1.66, tokens/s = 77499 (34154 source, 43345 target) ; Learning rate = 0.000329 ; Loss = 1.443044\n",
      "2024-12-02 23:02:50.509000: I runner.py:310] Step = 72100 ; steps/s = 1.64, tokens/s = 78349 (34506 source, 43843 target) ; Learning rate = 0.000329 ; Loss = 1.444010\n",
      "2024-12-02 23:03:50.967000: I runner.py:310] Step = 72200 ; steps/s = 1.65, tokens/s = 77854 (34340 source, 43514 target) ; Learning rate = 0.000329 ; Loss = 1.446652\n",
      "2024-12-02 23:04:51.633000: I runner.py:310] Step = 72300 ; steps/s = 1.65, tokens/s = 78118 (34388 source, 43730 target) ; Learning rate = 0.000329 ; Loss = 1.444676\n",
      "2024-12-02 23:05:52.434000: I runner.py:310] Step = 72400 ; steps/s = 1.64, tokens/s = 78372 (34518 source, 43854 target) ; Learning rate = 0.000328 ; Loss = 1.444428\n",
      "2024-12-02 23:06:52.717000: I runner.py:310] Step = 72500 ; steps/s = 1.66, tokens/s = 77645 (34234 source, 43411 target) ; Learning rate = 0.000328 ; Loss = 1.442565\n",
      "2024-12-02 23:07:53.483000: I runner.py:310] Step = 72600 ; steps/s = 1.65, tokens/s = 78432 (34537 source, 43895 target) ; Learning rate = 0.000328 ; Loss = 1.445820\n",
      "2024-12-02 23:08:54.324000: I runner.py:310] Step = 72700 ; steps/s = 1.64, tokens/s = 78313 (34491 source, 43822 target) ; Learning rate = 0.000328 ; Loss = 1.446467\n",
      "2024-12-02 23:09:54.553000: I runner.py:310] Step = 72800 ; steps/s = 1.66, tokens/s = 77697 (34247 source, 43450 target) ; Learning rate = 0.000328 ; Loss = 1.444668\n",
      "2024-12-02 23:10:55.354000: I runner.py:310] Step = 72900 ; steps/s = 1.65, tokens/s = 78381 (34517 source, 43864 target) ; Learning rate = 0.000327 ; Loss = 1.446349\n",
      "2024-12-02 23:11:55.562000: I runner.py:310] Step = 73000 ; steps/s = 1.66, tokens/s = 77770 (34301 source, 43469 target) ; Learning rate = 0.000327 ; Loss = 1.442607\n",
      "2024-12-02 23:12:56.314000: I runner.py:310] Step = 73100 ; steps/s = 1.65, tokens/s = 78426 (34534 source, 43892 target) ; Learning rate = 0.000327 ; Loss = 1.442805\n",
      "2024-12-02 23:13:57.124000: I runner.py:310] Step = 73200 ; steps/s = 1.64, tokens/s = 78355 (34504 source, 43851 target) ; Learning rate = 0.000327 ; Loss = 1.438421\n",
      "2024-12-02 23:14:57.425000: I runner.py:310] Step = 73300 ; steps/s = 1.66, tokens/s = 77614 (34209 source, 43405 target) ; Learning rate = 0.000326 ; Loss = 1.440458\n",
      "2024-12-02 23:15:58.152000: I runner.py:310] Step = 73400 ; steps/s = 1.65, tokens/s = 78474 (34568 source, 43906 target) ; Learning rate = 0.000326 ; Loss = 1.445315\n",
      "2024-12-02 23:16:58.487000: I runner.py:310] Step = 73500 ; steps/s = 1.66, tokens/s = 77610 (34235 source, 43375 target) ; Learning rate = 0.000326 ; Loss = 1.442627\n",
      "2024-12-02 23:17:59.199000: I runner.py:310] Step = 73600 ; steps/s = 1.65, tokens/s = 78460 (34530 source, 43930 target) ; Learning rate = 0.000326 ; Loss = 1.446306\n",
      "2024-12-02 23:18:59.937000: I runner.py:310] Step = 73700 ; steps/s = 1.65, tokens/s = 78459 (34561 source, 43898 target) ; Learning rate = 0.000326 ; Loss = 1.446488\n",
      "2024-12-02 23:20:00.223000: I runner.py:310] Step = 73800 ; steps/s = 1.66, tokens/s = 77655 (34240 source, 43415 target) ; Learning rate = 0.000325 ; Loss = 1.446407\n",
      "2024-12-02 23:21:00.994000: I runner.py:310] Step = 73900 ; steps/s = 1.65, tokens/s = 78407 (34529 source, 43878 target) ; Learning rate = 0.000325 ; Loss = 1.439730\n",
      "2024-12-02 23:22:01.784000: I runner.py:310] Step = 74000 ; steps/s = 1.65, tokens/s = 78381 (34512 source, 43869 target) ; Learning rate = 0.000325 ; Loss = 1.443868\n",
      "2024-12-02 23:23:02.126000: I runner.py:310] Step = 74100 ; steps/s = 1.66, tokens/s = 77546 (34186 source, 43360 target) ; Learning rate = 0.000325 ; Loss = 1.441566\n",
      "2024-12-02 23:24:02.913000: I runner.py:310] Step = 74200 ; steps/s = 1.65, tokens/s = 78395 (34531 source, 43864 target) ; Learning rate = 0.000324 ; Loss = 1.442637\n",
      "2024-12-02 23:25:03.277000: I runner.py:310] Step = 74300 ; steps/s = 1.66, tokens/s = 77581 (34218 source, 43363 target) ; Learning rate = 0.000324 ; Loss = 1.436489\n",
      "2024-12-02 23:26:04.065000: I runner.py:310] Step = 74400 ; steps/s = 1.65, tokens/s = 78388 (34510 source, 43878 target) ; Learning rate = 0.000324 ; Loss = 1.445498\n",
      "2024-12-02 23:27:04.827000: I runner.py:310] Step = 74500 ; steps/s = 1.65, tokens/s = 78432 (34548 source, 43884 target) ; Learning rate = 0.000324 ; Loss = 1.444003\n",
      "2024-12-02 23:28:05.106000: I runner.py:310] Step = 74600 ; steps/s = 1.66, tokens/s = 77624 (34218 source, 43406 target) ; Learning rate = 0.000324 ; Loss = 1.439449\n",
      "2024-12-02 23:29:05.902000: I runner.py:310] Step = 74700 ; steps/s = 1.65, tokens/s = 78376 (34512 source, 43864 target) ; Learning rate = 0.000323 ; Loss = 1.444170\n",
      "2024-12-02 23:30:06.133000: I runner.py:310] Step = 74800 ; steps/s = 1.66, tokens/s = 77747 (34294 source, 43453 target) ; Learning rate = 0.000323 ; Loss = 1.441020\n",
      "2024-12-02 23:31:06.909000: I runner.py:310] Step = 74900 ; steps/s = 1.65, tokens/s = 78381 (34504 source, 43877 target) ; Learning rate = 0.000323 ; Loss = 1.435588\n",
      "2024-12-02 23:32:07.675000: I runner.py:310] Step = 75000 ; steps/s = 1.65, tokens/s = 78406 (34531 source, 43875 target) ; Learning rate = 0.000323 ; Loss = 1.441908\n",
      "2024-12-02 23:32:07.676000: I training.py:192] Running evaluation for step 75000\n",
      "2024-12-02 23:32:55.746000: I training.py:192] Evaluation result for step 75000: loss = 0.819605 ; perplexity = 2.269603\n",
      "2024-12-02 23:33:55.869000: I runner.py:310] Step = 75100 ; steps/s = 1.66, tokens/s = 77888 (34344 source, 43544 target) ; Learning rate = 0.000323 ; Loss = 1.440048\n",
      "2024-12-02 23:34:56.575000: I runner.py:310] Step = 75200 ; steps/s = 1.65, tokens/s = 78479 (34549 source, 43930 target) ; Learning rate = 0.000322 ; Loss = 1.442165\n",
      "2024-12-02 23:35:57.377000: I runner.py:310] Step = 75300 ; steps/s = 1.64, tokens/s = 78393 (34532 source, 43861 target) ; Learning rate = 0.000322 ; Loss = 1.442036\n",
      "2024-12-02 23:36:57.601000: I runner.py:310] Step = 75400 ; steps/s = 1.66, tokens/s = 77734 (34279 source, 43455 target) ; Learning rate = 0.000322 ; Loss = 1.438969\n",
      "2024-12-02 23:37:58.389000: I runner.py:310] Step = 75500 ; steps/s = 1.65, tokens/s = 78375 (34499 source, 43876 target) ; Learning rate = 0.000322 ; Loss = 1.443697\n",
      "2024-12-02 23:38:58.707000: I runner.py:310] Step = 75600 ; steps/s = 1.66, tokens/s = 77605 (34218 source, 43387 target) ; Learning rate = 0.000321 ; Loss = 1.441002\n",
      "2024-12-02 23:39:59.424000: I runner.py:310] Step = 75700 ; steps/s = 1.65, tokens/s = 78466 (34552 source, 43914 target) ; Learning rate = 0.000321 ; Loss = 1.437371\n",
      "2024-12-02 23:41:00.176000: I runner.py:310] Step = 75800 ; steps/s = 1.65, tokens/s = 78456 (34560 source, 43896 target) ; Learning rate = 0.000321 ; Loss = 1.445866\n",
      "2024-12-02 23:42:00.444000: I runner.py:310] Step = 75900 ; steps/s = 1.66, tokens/s = 77662 (34248 source, 43414 target) ; Learning rate = 0.000321 ; Loss = 1.443789\n",
      "2024-12-02 23:43:01.228000: I runner.py:310] Step = 76000 ; steps/s = 1.65, tokens/s = 78382 (34503 source, 43879 target) ; Learning rate = 0.000321 ; Loss = 1.442885\n",
      "2024-12-02 23:44:01.590000: I runner.py:310] Step = 76100 ; steps/s = 1.66, tokens/s = 77591 (34230 source, 43361 target) ; Learning rate = 0.000320 ; Loss = 1.435529\n",
      "2024-12-02 23:45:02.333000: I runner.py:310] Step = 76200 ; steps/s = 1.65, tokens/s = 78443 (34540 source, 43903 target) ; Learning rate = 0.000320 ; Loss = 1.444232\n",
      "2024-12-02 23:46:03.119000: I runner.py:310] Step = 76300 ; steps/s = 1.65, tokens/s = 78376 (34507 source, 43869 target) ; Learning rate = 0.000320 ; Loss = 1.439496\n",
      "2024-12-02 23:47:03.407000: I runner.py:310] Step = 76400 ; steps/s = 1.66, tokens/s = 77645 (34239 source, 43406 target) ; Learning rate = 0.000320 ; Loss = 1.442523\n",
      "2024-12-02 23:48:04.107000: I runner.py:310] Step = 76500 ; steps/s = 1.65, tokens/s = 78498 (34564 source, 43934 target) ; Learning rate = 0.000320 ; Loss = 1.445269\n",
      "2024-12-02 23:49:04.941000: I runner.py:310] Step = 76600 ; steps/s = 1.64, tokens/s = 78342 (34506 source, 43836 target) ; Learning rate = 0.000319 ; Loss = 1.446124\n",
      "2024-12-02 23:50:05.229000: I runner.py:310] Step = 76700 ; steps/s = 1.66, tokens/s = 77610 (34205 source, 43405 target) ; Learning rate = 0.000319 ; Loss = 1.442143\n",
      "2024-12-02 23:51:06.034000: I runner.py:310] Step = 76800 ; steps/s = 1.64, tokens/s = 78398 (34549 source, 43849 target) ; Learning rate = 0.000319 ; Loss = 1.442895\n",
      "2024-12-02 23:52:06.344000: I runner.py:310] Step = 76900 ; steps/s = 1.66, tokens/s = 77604 (34199 source, 43405 target) ; Learning rate = 0.000319 ; Loss = 1.439412\n",
      "2024-12-02 23:53:07.037000: I runner.py:310] Step = 77000 ; steps/s = 1.65, tokens/s = 78486 (34555 source, 43931 target) ; Learning rate = 0.000319 ; Loss = 1.440083\n",
      "2024-12-02 23:54:07.803000: I runner.py:310] Step = 77100 ; steps/s = 1.65, tokens/s = 78447 (34563 source, 43884 target) ; Learning rate = 0.000318 ; Loss = 1.446344\n",
      "2024-12-02 23:55:08.092000: I runner.py:310] Step = 77200 ; steps/s = 1.66, tokens/s = 77653 (34238 source, 43415 target) ; Learning rate = 0.000318 ; Loss = 1.440015\n",
      "2024-12-02 23:56:08.849000: I runner.py:310] Step = 77300 ; steps/s = 1.65, tokens/s = 78414 (34531 source, 43883 target) ; Learning rate = 0.000318 ; Loss = 1.442026\n",
      "2024-12-02 23:57:09.158000: I runner.py:310] Step = 77400 ; steps/s = 1.66, tokens/s = 77646 (34243 source, 43403 target) ; Learning rate = 0.000318 ; Loss = 1.439694\n",
      "2024-12-02 23:58:09.921000: I runner.py:310] Step = 77500 ; steps/s = 1.65, tokens/s = 78379 (34496 source, 43883 target) ; Learning rate = 0.000317 ; Loss = 1.440355\n",
      "2024-12-02 23:59:10.670000: I runner.py:310] Step = 77600 ; steps/s = 1.65, tokens/s = 78465 (34562 source, 43903 target) ; Learning rate = 0.000317 ; Loss = 1.445498\n",
      "2024-12-03 00:00:10.923000: I runner.py:310] Step = 77700 ; steps/s = 1.66, tokens/s = 77685 (34254 source, 43431 target) ; Learning rate = 0.000317 ; Loss = 1.437914\n",
      "2024-12-03 00:01:11.659000: I runner.py:310] Step = 77800 ; steps/s = 1.65, tokens/s = 78475 (34563 source, 43912 target) ; Learning rate = 0.000317 ; Loss = 1.446387\n",
      "2024-12-03 00:02:12.438000: I runner.py:310] Step = 77900 ; steps/s = 1.65, tokens/s = 78398 (34526 source, 43872 target) ; Learning rate = 0.000317 ; Loss = 1.443066\n",
      "2024-12-03 00:03:12.724000: I runner.py:310] Step = 78000 ; steps/s = 1.66, tokens/s = 77601 (34193 source, 43408 target) ; Learning rate = 0.000316 ; Loss = 1.439522\n",
      "2024-12-03 00:04:13.533000: I runner.py:310] Step = 78100 ; steps/s = 1.64, tokens/s = 78373 (34529 source, 43844 target) ; Learning rate = 0.000316 ; Loss = 1.442845\n",
      "2024-12-03 00:05:13.903000: I runner.py:310] Step = 78200 ; steps/s = 1.66, tokens/s = 77557 (34197 source, 43360 target) ; Learning rate = 0.000316 ; Loss = 1.438528\n",
      "2024-12-03 00:06:14.634000: I runner.py:310] Step = 78300 ; steps/s = 1.65, tokens/s = 78469 (34552 source, 43917 target) ; Learning rate = 0.000316 ; Loss = 1.436959\n",
      "2024-12-03 00:07:15.403000: I runner.py:310] Step = 78400 ; steps/s = 1.65, tokens/s = 78421 (34542 source, 43879 target) ; Learning rate = 0.000316 ; Loss = 1.444562\n",
      "2024-12-03 00:08:15.687000: I runner.py:310] Step = 78500 ; steps/s = 1.66, tokens/s = 77641 (34237 source, 43404 target) ; Learning rate = 0.000315 ; Loss = 1.440686\n",
      "2024-12-03 00:09:16.418000: I runner.py:310] Step = 78600 ; steps/s = 1.65, tokens/s = 78469 (34551 source, 43918 target) ; Learning rate = 0.000315 ; Loss = 1.440244\n",
      "2024-12-03 00:10:16.667000: I runner.py:310] Step = 78700 ; steps/s = 1.66, tokens/s = 77684 (34252 source, 43432 target) ; Learning rate = 0.000315 ; Loss = 1.440912\n",
      "2024-12-03 00:11:17.403000: I runner.py:310] Step = 78800 ; steps/s = 1.65, tokens/s = 78448 (34539 source, 43909 target) ; Learning rate = 0.000315 ; Loss = 1.443040\n",
      "2024-12-03 00:12:18.145000: I runner.py:310] Step = 78900 ; steps/s = 1.65, tokens/s = 78454 (34559 source, 43895 target) ; Learning rate = 0.000315 ; Loss = 1.438971\n",
      "2024-12-03 00:13:18.535000: I runner.py:310] Step = 79000 ; steps/s = 1.66, tokens/s = 77546 (34193 source, 43353 target) ; Learning rate = 0.000314 ; Loss = 1.438217\n",
      "2024-12-03 00:14:19.251000: I runner.py:310] Step = 79100 ; steps/s = 1.65, tokens/s = 78472 (34554 source, 43918 target) ; Learning rate = 0.000314 ; Loss = 1.441041\n",
      "2024-12-03 00:15:19.996000: I runner.py:310] Step = 79200 ; steps/s = 1.65, tokens/s = 78430 (34538 source, 43892 target) ; Learning rate = 0.000314 ; Loss = 1.445399\n",
      "2024-12-03 00:16:20.345000: I runner.py:310] Step = 79300 ; steps/s = 1.66, tokens/s = 77570 (34195 source, 43375 target) ; Learning rate = 0.000314 ; Loss = 1.441156\n",
      "2024-12-03 00:17:21.025000: I runner.py:310] Step = 79400 ; steps/s = 1.65, tokens/s = 78544 (34592 source, 43952 target) ; Learning rate = 0.000314 ; Loss = 1.446918\n",
      "2024-12-03 00:18:21.316000: I runner.py:310] Step = 79500 ; steps/s = 1.66, tokens/s = 77625 (34230 source, 43395 target) ; Learning rate = 0.000313 ; Loss = 1.437278\n",
      "2024-12-03 00:19:22.038000: I runner.py:310] Step = 79600 ; steps/s = 1.65, tokens/s = 78477 (34553 source, 43924 target) ; Learning rate = 0.000313 ; Loss = 1.443188\n",
      "2024-12-03 00:20:22.789000: I runner.py:310] Step = 79700 ; steps/s = 1.65, tokens/s = 78450 (34555 source, 43895 target) ; Learning rate = 0.000313 ; Loss = 1.442667\n",
      "2024-12-03 00:21:23.072000: I runner.py:310] Step = 79800 ; steps/s = 1.66, tokens/s = 77619 (34216 source, 43403 target) ; Learning rate = 0.000313 ; Loss = 1.438945\n",
      "2024-12-03 00:22:23.749000: I runner.py:310] Step = 79900 ; steps/s = 1.65, tokens/s = 78547 (34588 source, 43959 target) ; Learning rate = 0.000313 ; Loss = 1.436615\n",
      "2024-12-03 00:23:24.081000: I runner.py:310] Step = 80000 ; steps/s = 1.66, tokens/s = 77602 (34226 source, 43376 target) ; Learning rate = 0.000312 ; Loss = 1.440142\n",
      "2024-12-03 00:23:26.188000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-80000\n",
      "2024-12-03 00:23:26.188000: I training.py:192] Running evaluation for step 80000\n",
      "2024-12-03 00:24:13.716000: I training.py:192] Evaluation result for step 80000: loss = 0.824414 ; perplexity = 2.280544\n",
      "2024-12-03 00:25:14.371000: I runner.py:310] Step = 80100 ; steps/s = 1.65, tokens/s = 78548 (34572 source, 43976 target) ; Learning rate = 0.000312 ; Loss = 1.439009\n",
      "2024-12-03 00:26:15.172000: I runner.py:310] Step = 80200 ; steps/s = 1.65, tokens/s = 78394 (34534 source, 43860 target) ; Learning rate = 0.000312 ; Loss = 1.438898\n",
      "2024-12-03 00:27:15.489000: I runner.py:310] Step = 80300 ; steps/s = 1.66, tokens/s = 77588 (34206 source, 43382 target) ; Learning rate = 0.000312 ; Loss = 1.437014\n",
      "2024-12-03 00:28:16.218000: I runner.py:310] Step = 80400 ; steps/s = 1.65, tokens/s = 78478 (34567 source, 43911 target) ; Learning rate = 0.000312 ; Loss = 1.441713\n",
      "2024-12-03 00:29:16.927000: I runner.py:310] Step = 80500 ; steps/s = 1.65, tokens/s = 78510 (34581 source, 43929 target) ; Learning rate = 0.000312 ; Loss = 1.445046\n",
      "2024-12-03 00:30:17.193000: I runner.py:310] Step = 80600 ; steps/s = 1.66, tokens/s = 77665 (34233 source, 43432 target) ; Learning rate = 0.000311 ; Loss = 1.437605\n",
      "2024-12-03 00:31:18.034000: I runner.py:310] Step = 80700 ; steps/s = 1.64, tokens/s = 78330 (34504 source, 43826 target) ; Learning rate = 0.000311 ; Loss = 1.440376\n",
      "2024-12-03 00:32:18.293000: I runner.py:310] Step = 80800 ; steps/s = 1.66, tokens/s = 77652 (34223 source, 43429 target) ; Learning rate = 0.000311 ; Loss = 1.439324\n",
      "2024-12-03 00:33:19.056000: I runner.py:310] Step = 80900 ; steps/s = 1.65, tokens/s = 78421 (34537 source, 43884 target) ; Learning rate = 0.000311 ; Loss = 1.443435\n",
      "2024-12-03 00:34:20.158000: I runner.py:310] Step = 81000 ; steps/s = 1.64, tokens/s = 78009 (34360 source, 43649 target) ; Learning rate = 0.000311 ; Loss = 1.441590\n",
      "2024-12-03 00:35:20.468000: I runner.py:310] Step = 81100 ; steps/s = 1.66, tokens/s = 77604 (34215 source, 43389 target) ; Learning rate = 0.000310 ; Loss = 1.438282\n",
      "2024-12-03 00:36:21.209000: I runner.py:310] Step = 81200 ; steps/s = 1.65, tokens/s = 78473 (34575 source, 43898 target) ; Learning rate = 0.000310 ; Loss = 1.442808\n",
      "2024-12-03 00:37:21.532000: I runner.py:310] Step = 81300 ; steps/s = 1.66, tokens/s = 77595 (34208 source, 43387 target) ; Learning rate = 0.000310 ; Loss = 1.436582\n",
      "2024-12-03 00:38:22.321000: I runner.py:310] Step = 81400 ; steps/s = 1.65, tokens/s = 78367 (34499 source, 43868 target) ; Learning rate = 0.000310 ; Loss = 1.440707\n",
      "2024-12-03 00:39:23.094000: I runner.py:310] Step = 81500 ; steps/s = 1.65, tokens/s = 78425 (34545 source, 43880 target) ; Learning rate = 0.000310 ; Loss = 1.441526\n",
      "2024-12-03 00:40:23.404000: I runner.py:310] Step = 81600 ; steps/s = 1.66, tokens/s = 77612 (34208 source, 43404 target) ; Learning rate = 0.000309 ; Loss = 1.439116\n",
      "2024-12-03 00:41:24.174000: I runner.py:310] Step = 81700 ; steps/s = 1.65, tokens/s = 78436 (34551 source, 43885 target) ; Learning rate = 0.000309 ; Loss = 1.444684\n",
      "2024-12-03 00:42:25.005000: I runner.py:310] Step = 81800 ; steps/s = 1.64, tokens/s = 78324 (34490 source, 43834 target) ; Learning rate = 0.000309 ; Loss = 1.441434\n",
      "2024-12-03 00:43:25.230000: I runner.py:310] Step = 81900 ; steps/s = 1.66, tokens/s = 77703 (34250 source, 43453 target) ; Learning rate = 0.000309 ; Loss = 1.443307\n",
      "2024-12-03 00:44:25.977000: I runner.py:310] Step = 82000 ; steps/s = 1.65, tokens/s = 78441 (34554 source, 43887 target) ; Learning rate = 0.000309 ; Loss = 1.443910\n",
      "2024-12-03 00:45:26.258000: I runner.py:310] Step = 82100 ; steps/s = 1.66, tokens/s = 77659 (34236 source, 43423 target) ; Learning rate = 0.000308 ; Loss = 1.434239\n",
      "2024-12-03 00:46:26.991000: I runner.py:310] Step = 82200 ; steps/s = 1.65, tokens/s = 78460 (34552 source, 43908 target) ; Learning rate = 0.000308 ; Loss = 1.443059\n",
      "2024-12-03 00:47:27.758000: I runner.py:310] Step = 82300 ; steps/s = 1.65, tokens/s = 78433 (34540 source, 43893 target) ; Learning rate = 0.000308 ; Loss = 1.440903\n",
      "2024-12-03 00:48:28.027000: I runner.py:310] Step = 82400 ; steps/s = 1.66, tokens/s = 77668 (34255 source, 43413 target) ; Learning rate = 0.000308 ; Loss = 1.443430\n",
      "2024-12-03 00:49:28.823000: I runner.py:310] Step = 82500 ; steps/s = 1.65, tokens/s = 78378 (34512 source, 43866 target) ; Learning rate = 0.000308 ; Loss = 1.440149\n",
      "2024-12-03 00:50:29.161000: I runner.py:310] Step = 82600 ; steps/s = 1.66, tokens/s = 77590 (34218 source, 43372 target) ; Learning rate = 0.000308 ; Loss = 1.435181\n",
      "2024-12-03 00:51:29.930000: I runner.py:310] Step = 82700 ; steps/s = 1.65, tokens/s = 78381 (34498 source, 43883 target) ; Learning rate = 0.000307 ; Loss = 1.437193\n",
      "2024-12-03 00:52:30.680000: I runner.py:310] Step = 82800 ; steps/s = 1.65, tokens/s = 78465 (34574 source, 43891 target) ; Learning rate = 0.000307 ; Loss = 1.446797\n",
      "2024-12-03 00:53:31.038000: I runner.py:310] Step = 82900 ; steps/s = 1.66, tokens/s = 77534 (34179 source, 43355 target) ; Learning rate = 0.000307 ; Loss = 1.436334\n",
      "2024-12-03 00:54:31.788000: I runner.py:310] Step = 83000 ; steps/s = 1.65, tokens/s = 78440 (34534 source, 43906 target) ; Learning rate = 0.000307 ; Loss = 1.445573\n",
      "2024-12-03 00:55:32.618000: I runner.py:310] Step = 83100 ; steps/s = 1.64, tokens/s = 78348 (34508 source, 43840 target) ; Learning rate = 0.000307 ; Loss = 1.443693\n",
      "2024-12-03 00:56:32.928000: I runner.py:310] Step = 83200 ; steps/s = 1.66, tokens/s = 77607 (34213 source, 43394 target) ; Learning rate = 0.000306 ; Loss = 1.431794\n",
      "2024-12-03 00:57:33.703000: I runner.py:310] Step = 83300 ; steps/s = 1.65, tokens/s = 78413 (34544 source, 43869 target) ; Learning rate = 0.000306 ; Loss = 1.439489\n",
      "2024-12-03 00:58:34.037000: I runner.py:310] Step = 83400 ; steps/s = 1.66, tokens/s = 77581 (34200 source, 43381 target) ; Learning rate = 0.000306 ; Loss = 1.436839\n",
      "2024-12-03 00:59:34.794000: I runner.py:310] Step = 83500 ; steps/s = 1.65, tokens/s = 78438 (34547 source, 43891 target) ; Learning rate = 0.000306 ; Loss = 1.442945\n",
      "2024-12-03 01:00:35.588000: I runner.py:310] Step = 83600 ; steps/s = 1.65, tokens/s = 78374 (34503 source, 43871 target) ; Learning rate = 0.000306 ; Loss = 1.443970\n",
      "2024-12-03 01:01:35.895000: I runner.py:310] Step = 83700 ; steps/s = 1.66, tokens/s = 77635 (34235 source, 43400 target) ; Learning rate = 0.000306 ; Loss = 1.441897\n",
      "2024-12-03 01:02:36.668000: I runner.py:310] Step = 83800 ; steps/s = 1.65, tokens/s = 78405 (34525 source, 43880 target) ; Learning rate = 0.000305 ; Loss = 1.437697\n",
      "2024-12-03 01:03:36.990000: I runner.py:310] Step = 83900 ; steps/s = 1.66, tokens/s = 77595 (34222 source, 43373 target) ; Learning rate = 0.000305 ; Loss = 1.438823\n",
      "2024-12-03 01:04:37.782000: I runner.py:310] Step = 84000 ; steps/s = 1.65, tokens/s = 78357 (34491 source, 43866 target) ; Learning rate = 0.000305 ; Loss = 1.438181\n",
      "2024-12-03 01:05:38.533000: I runner.py:310] Step = 84100 ; steps/s = 1.65, tokens/s = 78463 (34572 source, 43891 target) ; Learning rate = 0.000305 ; Loss = 1.440088\n",
      "2024-12-03 01:06:38.868000: I runner.py:310] Step = 84200 ; steps/s = 1.66, tokens/s = 77574 (34185 source, 43389 target) ; Learning rate = 0.000305 ; Loss = 1.439948\n",
      "2024-12-03 01:07:39.671000: I runner.py:310] Step = 84300 ; steps/s = 1.64, tokens/s = 78352 (34504 source, 43848 target) ; Learning rate = 0.000304 ; Loss = 1.435060\n",
      "2024-12-03 01:08:40.483000: I runner.py:310] Step = 84400 ; steps/s = 1.64, tokens/s = 78385 (34534 source, 43851 target) ; Learning rate = 0.000304 ; Loss = 1.437749\n",
      "2024-12-03 01:09:40.733000: I runner.py:310] Step = 84500 ; steps/s = 1.66, tokens/s = 77670 (34231 source, 43439 target) ; Learning rate = 0.000304 ; Loss = 1.439809\n",
      "2024-12-03 01:10:41.440000: I runner.py:310] Step = 84600 ; steps/s = 1.65, tokens/s = 78522 (34601 source, 43921 target) ; Learning rate = 0.000304 ; Loss = 1.437089\n",
      "2024-12-03 01:11:41.785000: I runner.py:310] Step = 84700 ; steps/s = 1.66, tokens/s = 77552 (34173 source, 43379 target) ; Learning rate = 0.000304 ; Loss = 1.440833\n",
      "2024-12-03 01:12:42.575000: I runner.py:310] Step = 84800 ; steps/s = 1.65, tokens/s = 78373 (34503 source, 43870 target) ; Learning rate = 0.000304 ; Loss = 1.443947\n",
      "2024-12-03 01:13:43.308000: I runner.py:310] Step = 84900 ; steps/s = 1.65, tokens/s = 78482 (34580 source, 43902 target) ; Learning rate = 0.000303 ; Loss = 1.437914\n",
      "2024-12-03 01:14:43.629000: I runner.py:310] Step = 85000 ; steps/s = 1.66, tokens/s = 77599 (34208 source, 43391 target) ; Learning rate = 0.000303 ; Loss = 1.439183\n",
      "2024-12-03 01:14:43.630000: I training.py:192] Running evaluation for step 85000\n",
      "2024-12-03 01:15:31.236000: I training.py:192] Evaluation result for step 85000: loss = 0.825682 ; perplexity = 2.283437\n",
      "2024-12-03 01:16:31.809000: I runner.py:310] Step = 85100 ; steps/s = 1.65, tokens/s = 78675 (34648 source, 44027 target) ; Learning rate = 0.000303 ; Loss = 1.439077\n",
      "2024-12-03 01:17:32.195000: I runner.py:310] Step = 85200 ; steps/s = 1.66, tokens/s = 77548 (34203 source, 43345 target) ; Learning rate = 0.000303 ; Loss = 1.430964\n",
      "2024-12-03 01:18:32.956000: I runner.py:310] Step = 85300 ; steps/s = 1.65, tokens/s = 78425 (34542 source, 43883 target) ; Learning rate = 0.000303 ; Loss = 1.441768\n",
      "2024-12-03 01:19:33.736000: I runner.py:310] Step = 85400 ; steps/s = 1.65, tokens/s = 78415 (34536 source, 43879 target) ; Learning rate = 0.000302 ; Loss = 1.440144\n",
      "2024-12-03 01:20:34.030000: I runner.py:310] Step = 85500 ; steps/s = 1.66, tokens/s = 77618 (34216 source, 43402 target) ; Learning rate = 0.000302 ; Loss = 1.435587\n",
      "2024-12-03 01:21:34.839000: I runner.py:310] Step = 85600 ; steps/s = 1.64, tokens/s = 78362 (34495 source, 43867 target) ; Learning rate = 0.000302 ; Loss = 1.445715\n",
      "2024-12-03 01:22:35.665000: I runner.py:310] Step = 85700 ; steps/s = 1.64, tokens/s = 78343 (34510 source, 43833 target) ; Learning rate = 0.000302 ; Loss = 1.440376\n",
      "2024-12-03 01:23:35.964000: I runner.py:310] Step = 85800 ; steps/s = 1.66, tokens/s = 77639 (34246 source, 43393 target) ; Learning rate = 0.000302 ; Loss = 1.435790\n",
      "2024-12-03 01:24:36.776000: I runner.py:310] Step = 85900 ; steps/s = 1.64, tokens/s = 78349 (34496 source, 43853 target) ; Learning rate = 0.000302 ; Loss = 1.439924\n",
      "2024-12-03 01:25:37.106000: I runner.py:310] Step = 86000 ; steps/s = 1.66, tokens/s = 77598 (34208 source, 43390 target) ; Learning rate = 0.000301 ; Loss = 1.437043\n",
      "2024-12-03 01:26:37.894000: I runner.py:310] Step = 86100 ; steps/s = 1.65, tokens/s = 78387 (34521 source, 43866 target) ; Learning rate = 0.000301 ; Loss = 1.437079\n",
      "2024-12-03 01:27:38.649000: I runner.py:310] Step = 86200 ; steps/s = 1.65, tokens/s = 78433 (34541 source, 43892 target) ; Learning rate = 0.000301 ; Loss = 1.441614\n",
      "2024-12-03 01:28:39.033000: I runner.py:310] Step = 86300 ; steps/s = 1.66, tokens/s = 77496 (34152 source, 43344 target) ; Learning rate = 0.000301 ; Loss = 1.439331\n",
      "2024-12-03 01:29:39.796000: I runner.py:310] Step = 86400 ; steps/s = 1.65, tokens/s = 78430 (34543 source, 43887 target) ; Learning rate = 0.000301 ; Loss = 1.433109\n",
      "2024-12-03 01:30:40.141000: I runner.py:310] Step = 86500 ; steps/s = 1.66, tokens/s = 77593 (34232 source, 43361 target) ; Learning rate = 0.000301 ; Loss = 1.436207\n",
      "2024-12-03 01:31:40.893000: I runner.py:310] Step = 86600 ; steps/s = 1.65, tokens/s = 78406 (34514 source, 43892 target) ; Learning rate = 0.000300 ; Loss = 1.439126\n",
      "2024-12-03 01:32:41.710000: I runner.py:310] Step = 86700 ; steps/s = 1.64, tokens/s = 78388 (34539 source, 43849 target) ; Learning rate = 0.000300 ; Loss = 1.446741\n",
      "2024-12-03 01:33:42.022000: I runner.py:310] Step = 86800 ; steps/s = 1.66, tokens/s = 77587 (34187 source, 43400 target) ; Learning rate = 0.000300 ; Loss = 1.436273\n",
      "2024-12-03 01:34:42.831000: I runner.py:310] Step = 86900 ; steps/s = 1.64, tokens/s = 78360 (34509 source, 43851 target) ; Learning rate = 0.000300 ; Loss = 1.439028\n",
      "2024-12-03 01:35:43.611000: I runner.py:310] Step = 87000 ; steps/s = 1.65, tokens/s = 78416 (34541 source, 43875 target) ; Learning rate = 0.000300 ; Loss = 1.440416\n",
      "2024-12-03 01:36:43.929000: I runner.py:310] Step = 87100 ; steps/s = 1.66, tokens/s = 77591 (34200 source, 43391 target) ; Learning rate = 0.000299 ; Loss = 1.433563\n",
      "2024-12-03 01:37:44.716000: I runner.py:310] Step = 87200 ; steps/s = 1.65, tokens/s = 78396 (34530 source, 43866 target) ; Learning rate = 0.000299 ; Loss = 1.438980\n",
      "2024-12-03 01:38:44.982000: I runner.py:310] Step = 87300 ; steps/s = 1.66, tokens/s = 77678 (34253 source, 43425 target) ; Learning rate = 0.000299 ; Loss = 1.436860\n",
      "2024-12-03 01:39:45.783000: I runner.py:310] Step = 87400 ; steps/s = 1.64, tokens/s = 78349 (34483 source, 43866 target) ; Learning rate = 0.000299 ; Loss = 1.437120\n",
      "2024-12-03 01:40:46.548000: I runner.py:310] Step = 87500 ; steps/s = 1.65, tokens/s = 78452 (34571 source, 43881 target) ; Learning rate = 0.000299 ; Loss = 1.439696\n",
      "2024-12-03 01:41:46.818000: I runner.py:310] Step = 87600 ; steps/s = 1.66, tokens/s = 77645 (34226 source, 43419 target) ; Learning rate = 0.000299 ; Loss = 1.444561\n",
      "2024-12-03 01:42:47.547000: I runner.py:310] Step = 87700 ; steps/s = 1.65, tokens/s = 78465 (34554 source, 43911 target) ; Learning rate = 0.000298 ; Loss = 1.439252\n",
      "2024-12-03 01:43:47.923000: I runner.py:310] Step = 87800 ; steps/s = 1.66, tokens/s = 77558 (34203 source, 43355 target) ; Learning rate = 0.000298 ; Loss = 1.434149\n",
      "2024-12-03 01:44:48.637000: I runner.py:310] Step = 87900 ; steps/s = 1.65, tokens/s = 78434 (34513 source, 43921 target) ; Learning rate = 0.000298 ; Loss = 1.432558\n",
      "2024-12-03 01:45:49.420000: I runner.py:310] Step = 88000 ; steps/s = 1.65, tokens/s = 78408 (34541 source, 43867 target) ; Learning rate = 0.000298 ; Loss = 1.441173\n",
      "2024-12-03 01:46:49.770000: I runner.py:310] Step = 88100 ; steps/s = 1.66, tokens/s = 77579 (34211 source, 43368 target) ; Learning rate = 0.000298 ; Loss = 1.436560\n",
      "2024-12-03 01:47:50.566000: I runner.py:310] Step = 88200 ; steps/s = 1.65, tokens/s = 78407 (34533 source, 43874 target) ; Learning rate = 0.000298 ; Loss = 1.440415\n",
      "2024-12-03 01:48:51.337000: I runner.py:310] Step = 88300 ; steps/s = 1.65, tokens/s = 78271 (34473 source, 43798 target) ; Learning rate = 0.000297 ; Loss = 1.448622\n",
      "2024-12-03 01:49:51.616000: I runner.py:310] Step = 88400 ; steps/s = 1.66, tokens/s = 77798 (34301 source, 43497 target) ; Learning rate = 0.000297 ; Loss = 1.439433\n",
      "2024-12-03 01:50:52.384000: I runner.py:310] Step = 88500 ; steps/s = 1.65, tokens/s = 78424 (34550 source, 43874 target) ; Learning rate = 0.000297 ; Loss = 1.433856\n",
      "2024-12-03 01:51:52.703000: I runner.py:310] Step = 88600 ; steps/s = 1.66, tokens/s = 77579 (34190 source, 43389 target) ; Learning rate = 0.000297 ; Loss = 1.441263\n",
      "2024-12-03 01:52:53.440000: I runner.py:310] Step = 88700 ; steps/s = 1.65, tokens/s = 78430 (34528 source, 43902 target) ; Learning rate = 0.000297 ; Loss = 1.448907\n",
      "2024-12-03 01:53:54.165000: I runner.py:310] Step = 88800 ; steps/s = 1.65, tokens/s = 78479 (34567 source, 43912 target) ; Learning rate = 0.000297 ; Loss = 1.436542\n",
      "2024-12-03 01:54:54.472000: I runner.py:310] Step = 88900 ; steps/s = 1.66, tokens/s = 77622 (34220 source, 43402 target) ; Learning rate = 0.000296 ; Loss = 1.437054\n",
      "2024-12-03 01:55:55.280000: I runner.py:310] Step = 89000 ; steps/s = 1.64, tokens/s = 78379 (34535 source, 43844 target) ; Learning rate = 0.000296 ; Loss = 1.438843\n",
      "2024-12-03 01:56:55.585000: I runner.py:310] Step = 89100 ; steps/s = 1.66, tokens/s = 77619 (34211 source, 43408 target) ; Learning rate = 0.000296 ; Loss = 1.434891\n",
      "2024-12-03 01:57:56.341000: I runner.py:310] Step = 89200 ; steps/s = 1.65, tokens/s = 78399 (34503 source, 43896 target) ; Learning rate = 0.000296 ; Loss = 1.437918\n",
      "2024-12-03 01:58:57.080000: I runner.py:310] Step = 89300 ; steps/s = 1.65, tokens/s = 78497 (34606 source, 43891 target) ; Learning rate = 0.000296 ; Loss = 1.442332\n",
      "2024-12-03 01:59:57.381000: I runner.py:310] Step = 89400 ; steps/s = 1.66, tokens/s = 77612 (34214 source, 43398 target) ; Learning rate = 0.000296 ; Loss = 1.436998\n",
      "2024-12-03 02:00:58.139000: I runner.py:310] Step = 89500 ; steps/s = 1.65, tokens/s = 78450 (34555 source, 43895 target) ; Learning rate = 0.000295 ; Loss = 1.437477\n",
      "2024-12-03 02:01:58.740000: I runner.py:310] Step = 89600 ; steps/s = 1.65, tokens/s = 78161 (34448 source, 43713 target) ; Learning rate = 0.000295 ; Loss = 1.458202\n",
      "2024-12-03 02:02:59.206000: I runner.py:310] Step = 89700 ; steps/s = 1.65, tokens/s = 77860 (34282 source, 43578 target) ; Learning rate = 0.000295 ; Loss = 1.435051\n",
      "2024-12-03 02:03:59.943000: I runner.py:310] Step = 89800 ; steps/s = 1.65, tokens/s = 78452 (34558 source, 43894 target) ; Learning rate = 0.000295 ; Loss = 1.438540\n",
      "2024-12-03 02:05:00.297000: I runner.py:310] Step = 89900 ; steps/s = 1.66, tokens/s = 77580 (34211 source, 43369 target) ; Learning rate = 0.000295 ; Loss = 1.433637\n",
      "2024-12-03 02:06:01.095000: I runner.py:310] Step = 90000 ; steps/s = 1.64, tokens/s = 78377 (34512 source, 43865 target) ; Learning rate = 0.000295 ; Loss = 1.438347\n",
      "2024-12-03 02:06:03.232000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-90000\n",
      "2024-12-03 02:06:03.233000: I training.py:192] Running evaluation for step 90000\n",
      "2024-12-03 02:06:51.194000: I training.py:192] Evaluation result for step 90000: loss = 0.828836 ; perplexity = 2.290651\n",
      "2024-12-03 02:07:51.834000: I runner.py:310] Step = 90100 ; steps/s = 1.65, tokens/s = 78651 (34631 source, 44020 target) ; Learning rate = 0.000294 ; Loss = 1.438505\n",
      "2024-12-03 02:08:52.184000: I runner.py:310] Step = 90200 ; steps/s = 1.66, tokens/s = 77538 (34182 source, 43356 target) ; Learning rate = 0.000294 ; Loss = 1.440101\n",
      "2024-12-03 02:09:52.949000: I runner.py:310] Step = 90300 ; steps/s = 1.65, tokens/s = 78409 (34530 source, 43879 target) ; Learning rate = 0.000294 ; Loss = 1.433359\n",
      "2024-12-03 02:10:53.283000: I runner.py:310] Step = 90400 ; steps/s = 1.66, tokens/s = 77612 (34227 source, 43385 target) ; Learning rate = 0.000294 ; Loss = 1.437450\n",
      "2024-12-03 02:11:53.985000: I runner.py:310] Step = 90500 ; steps/s = 1.65, tokens/s = 78491 (34555 source, 43936 target) ; Learning rate = 0.000294 ; Loss = 1.436430\n",
      "2024-12-03 02:12:54.833000: I runner.py:310] Step = 90600 ; steps/s = 1.64, tokens/s = 78323 (34505 source, 43818 target) ; Learning rate = 0.000294 ; Loss = 1.439110\n",
      "2024-12-03 02:13:55.225000: I runner.py:310] Step = 90700 ; steps/s = 1.66, tokens/s = 77498 (34161 source, 43337 target) ; Learning rate = 0.000293 ; Loss = 1.435969\n",
      "2024-12-03 02:14:55.981000: I runner.py:310] Step = 90800 ; steps/s = 1.65, tokens/s = 78444 (34553 source, 43891 target) ; Learning rate = 0.000293 ; Loss = 1.437389\n",
      "2024-12-03 02:15:56.280000: I runner.py:310] Step = 90900 ; steps/s = 1.66, tokens/s = 77641 (34243 source, 43398 target) ; Learning rate = 0.000293 ; Loss = 1.433989\n",
      "2024-12-03 02:16:57.061000: I runner.py:310] Step = 91000 ; steps/s = 1.65, tokens/s = 78395 (34519 source, 43876 target) ; Learning rate = 0.000293 ; Loss = 1.439441\n",
      "2024-12-03 02:17:57.840000: I runner.py:310] Step = 91100 ; steps/s = 1.65, tokens/s = 78391 (34518 source, 43873 target) ; Learning rate = 0.000293 ; Loss = 1.434382\n",
      "2024-12-03 02:18:58.138000: I runner.py:310] Step = 91200 ; steps/s = 1.66, tokens/s = 77618 (34213 source, 43405 target) ; Learning rate = 0.000293 ; Loss = 1.440888\n",
      "2024-12-03 02:19:58.970000: I runner.py:310] Step = 91300 ; steps/s = 1.64, tokens/s = 78333 (34495 source, 43838 target) ; Learning rate = 0.000293 ; Loss = 1.438455\n",
      "2024-12-03 02:20:59.662000: I runner.py:310] Step = 91400 ; steps/s = 1.65, tokens/s = 78513 (34581 source, 43932 target) ; Learning rate = 0.000292 ; Loss = 1.433953\n",
      "2024-12-03 02:21:59.946000: I runner.py:310] Step = 91500 ; steps/s = 1.66, tokens/s = 77636 (34219 source, 43417 target) ; Learning rate = 0.000292 ; Loss = 1.438717\n",
      "2024-12-03 02:23:00.749000: I runner.py:310] Step = 91600 ; steps/s = 1.64, tokens/s = 78363 (34516 source, 43847 target) ; Learning rate = 0.000292 ; Loss = 1.439861\n",
      "2024-12-03 02:24:00.995000: I runner.py:310] Step = 91700 ; steps/s = 1.66, tokens/s = 77708 (34261 source, 43447 target) ; Learning rate = 0.000292 ; Loss = 1.437660\n",
      "2024-12-03 02:25:01.781000: I runner.py:310] Step = 91800 ; steps/s = 1.65, tokens/s = 78366 (34505 source, 43861 target) ; Learning rate = 0.000292 ; Loss = 1.431863\n",
      "2024-12-03 02:26:02.532000: I runner.py:310] Step = 91900 ; steps/s = 1.65, tokens/s = 78472 (34570 source, 43902 target) ; Learning rate = 0.000292 ; Loss = 1.440381\n",
      "2024-12-03 02:27:02.833000: I runner.py:310] Step = 92000 ; steps/s = 1.66, tokens/s = 77624 (34222 source, 43402 target) ; Learning rate = 0.000291 ; Loss = 1.430130\n",
      "2024-12-03 02:28:03.630000: I runner.py:310] Step = 92100 ; steps/s = 1.64, tokens/s = 78388 (34528 source, 43860 target) ; Learning rate = 0.000291 ; Loss = 1.439544\n",
      "2024-12-03 02:29:03.983000: I runner.py:310] Step = 92200 ; steps/s = 1.66, tokens/s = 77573 (34209 source, 43364 target) ; Learning rate = 0.000291 ; Loss = 1.435023\n",
      "2024-12-03 02:30:04.737000: I runner.py:310] Step = 92300 ; steps/s = 1.65, tokens/s = 78399 (34511 source, 43888 target) ; Learning rate = 0.000291 ; Loss = 1.432636\n",
      "2024-12-03 02:31:05.458000: I runner.py:310] Step = 92400 ; steps/s = 1.65, tokens/s = 78498 (34580 source, 43918 target) ; Learning rate = 0.000291 ; Loss = 1.441397\n",
      "2024-12-03 02:32:05.730000: I runner.py:310] Step = 92500 ; steps/s = 1.66, tokens/s = 77665 (34234 source, 43431 target) ; Learning rate = 0.000291 ; Loss = 1.435975\n",
      "2024-12-03 02:33:06.448000: I runner.py:310] Step = 92600 ; steps/s = 1.65, tokens/s = 78485 (34559 source, 43926 target) ; Learning rate = 0.000290 ; Loss = 1.436489\n",
      "2024-12-03 02:34:07.241000: I runner.py:310] Step = 92700 ; steps/s = 1.65, tokens/s = 78401 (34543 source, 43858 target) ; Learning rate = 0.000290 ; Loss = 1.435642\n",
      "2024-12-03 02:35:07.572000: I runner.py:310] Step = 92800 ; steps/s = 1.66, tokens/s = 77572 (34203 source, 43369 target) ; Learning rate = 0.000290 ; Loss = 1.436371\n",
      "2024-12-03 02:36:08.347000: I runner.py:310] Step = 92900 ; steps/s = 1.65, tokens/s = 78426 (34541 source, 43885 target) ; Learning rate = 0.000290 ; Loss = 1.438289\n",
      "2024-12-03 02:37:08.654000: I runner.py:310] Step = 93000 ; steps/s = 1.66, tokens/s = 77614 (34214 source, 43400 target) ; Learning rate = 0.000290 ; Loss = 1.438275\n",
      "2024-12-03 02:38:09.420000: I runner.py:310] Step = 93100 ; steps/s = 1.65, tokens/s = 78401 (34514 source, 43887 target) ; Learning rate = 0.000290 ; Loss = 1.438966\n",
      "2024-12-03 02:39:10.170000: I runner.py:310] Step = 93200 ; steps/s = 1.65, tokens/s = 78427 (34534 source, 43893 target) ; Learning rate = 0.000290 ; Loss = 1.434727\n",
      "2024-12-03 02:40:10.434000: I runner.py:310] Step = 93300 ; steps/s = 1.66, tokens/s = 77703 (34281 source, 43422 target) ; Learning rate = 0.000289 ; Loss = 1.435216\n",
      "2024-12-03 02:41:11.224000: I runner.py:310] Step = 93400 ; steps/s = 1.65, tokens/s = 78385 (34508 source, 43877 target) ; Learning rate = 0.000289 ; Loss = 1.439437\n",
      "2024-12-03 02:42:11.567000: I runner.py:310] Step = 93500 ; steps/s = 1.66, tokens/s = 77577 (34213 source, 43364 target) ; Learning rate = 0.000289 ; Loss = 1.434683\n",
      "2024-12-03 02:43:12.349000: I runner.py:310] Step = 93600 ; steps/s = 1.65, tokens/s = 78349 (34479 source, 43870 target) ; Learning rate = 0.000289 ; Loss = 1.427893\n",
      "2024-12-03 02:44:13.071000: I runner.py:310] Step = 93700 ; steps/s = 1.65, tokens/s = 78494 (34584 source, 43910 target) ; Learning rate = 0.000289 ; Loss = 1.437838\n",
      "2024-12-03 02:45:13.448000: I runner.py:310] Step = 93800 ; steps/s = 1.66, tokens/s = 77534 (34176 source, 43358 target) ; Learning rate = 0.000289 ; Loss = 1.437889\n",
      "2024-12-03 02:46:14.170000: I runner.py:310] Step = 93900 ; steps/s = 1.65, tokens/s = 78481 (34561 source, 43920 target) ; Learning rate = 0.000288 ; Loss = 1.439913\n",
      "2024-12-03 02:47:14.953000: I runner.py:310] Step = 94000 ; steps/s = 1.65, tokens/s = 78413 (34540 source, 43873 target) ; Learning rate = 0.000288 ; Loss = 1.435816\n",
      "2024-12-03 02:48:15.279000: I runner.py:310] Step = 94100 ; steps/s = 1.66, tokens/s = 77557 (34184 source, 43373 target) ; Learning rate = 0.000288 ; Loss = 1.434813\n",
      "2024-12-03 02:49:15.949000: I runner.py:310] Step = 94200 ; steps/s = 1.65, tokens/s = 78568 (34614 source, 43954 target) ; Learning rate = 0.000288 ; Loss = 1.440590\n",
      "2024-12-03 02:50:16.293000: I runner.py:310] Step = 94300 ; steps/s = 1.66, tokens/s = 77576 (34198 source, 43378 target) ; Learning rate = 0.000288 ; Loss = 1.434436\n",
      "2024-12-03 02:51:17.114000: I runner.py:310] Step = 94400 ; steps/s = 1.64, tokens/s = 78354 (34505 source, 43849 target) ; Learning rate = 0.000288 ; Loss = 1.441107\n",
      "2024-12-03 02:52:17.858000: I runner.py:310] Step = 94500 ; steps/s = 1.65, tokens/s = 78458 (34558 source, 43900 target) ; Learning rate = 0.000288 ; Loss = 1.436159\n",
      "2024-12-03 02:53:18.155000: I runner.py:310] Step = 94600 ; steps/s = 1.66, tokens/s = 77616 (34219 source, 43397 target) ; Learning rate = 0.000287 ; Loss = 1.436109\n",
      "2024-12-03 02:54:18.910000: I runner.py:310] Step = 94700 ; steps/s = 1.65, tokens/s = 78409 (34527 source, 43882 target) ; Learning rate = 0.000287 ; Loss = 1.438752\n",
      "2024-12-03 02:55:19.191000: I runner.py:310] Step = 94800 ; steps/s = 1.66, tokens/s = 77688 (34264 source, 43424 target) ; Learning rate = 0.000287 ; Loss = 1.433564\n",
      "2024-12-03 02:56:19.907000: I runner.py:310] Step = 94900 ; steps/s = 1.65, tokens/s = 78460 (34536 source, 43924 target) ; Learning rate = 0.000287 ; Loss = 1.432899\n",
      "2024-12-03 02:57:20.703000: I runner.py:310] Step = 95000 ; steps/s = 1.65, tokens/s = 78394 (34539 source, 43855 target) ; Learning rate = 0.000287 ; Loss = 1.438168\n",
      "2024-12-03 02:57:20.705000: I training.py:192] Running evaluation for step 95000\n",
      "2024-12-03 02:58:08.378000: I training.py:192] Evaluation result for step 95000: loss = 0.836422 ; perplexity = 2.308094\n",
      "2024-12-03 02:59:08.537000: I runner.py:310] Step = 95100 ; steps/s = 1.66, tokens/s = 77818 (34298 source, 43520 target) ; Learning rate = 0.000287 ; Loss = 1.435976\n",
      "2024-12-03 03:00:09.321000: I runner.py:310] Step = 95200 ; steps/s = 1.65, tokens/s = 78398 (34531 source, 43867 target) ; Learning rate = 0.000286 ; Loss = 1.438154\n",
      "2024-12-03 03:01:10.143000: I runner.py:310] Step = 95300 ; steps/s = 1.64, tokens/s = 78345 (34503 source, 43842 target) ; Learning rate = 0.000286 ; Loss = 1.437371\n",
      "2024-12-03 03:02:10.441000: I runner.py:310] Step = 95400 ; steps/s = 1.66, tokens/s = 77631 (34229 source, 43402 target) ; Learning rate = 0.000286 ; Loss = 1.438518\n",
      "2024-12-03 03:03:11.159000: I runner.py:310] Step = 95500 ; steps/s = 1.65, tokens/s = 78489 (34571 source, 43918 target) ; Learning rate = 0.000286 ; Loss = 1.432188\n",
      "2024-12-03 03:04:11.446000: I runner.py:310] Step = 95600 ; steps/s = 1.66, tokens/s = 77640 (34230 source, 43410 target) ; Learning rate = 0.000286 ; Loss = 1.432889\n",
      "2024-12-03 03:05:12.215000: I runner.py:310] Step = 95700 ; steps/s = 1.65, tokens/s = 78413 (34524 source, 43889 target) ; Learning rate = 0.000286 ; Loss = 1.438912\n",
      "2024-12-03 03:06:12.911000: I runner.py:310] Step = 95800 ; steps/s = 1.65, tokens/s = 78502 (34566 source, 43936 target) ; Learning rate = 0.000286 ; Loss = 1.438043\n",
      "2024-12-03 03:07:13.152000: I runner.py:310] Step = 95900 ; steps/s = 1.66, tokens/s = 77663 (34227 source, 43436 target) ; Learning rate = 0.000285 ; Loss = 1.437564\n",
      "2024-12-03 03:08:13.896000: I runner.py:310] Step = 96000 ; steps/s = 1.65, tokens/s = 78468 (34580 source, 43888 target) ; Learning rate = 0.000285 ; Loss = 1.438037\n",
      "2024-12-03 03:09:14.221000: I runner.py:310] Step = 96100 ; steps/s = 1.66, tokens/s = 77623 (34230 source, 43393 target) ; Learning rate = 0.000285 ; Loss = 1.432204\n",
      "2024-12-03 03:10:14.956000: I runner.py:310] Step = 96200 ; steps/s = 1.65, tokens/s = 78425 (34514 source, 43911 target) ; Learning rate = 0.000285 ; Loss = 1.435356\n",
      "2024-12-03 03:11:15.762000: I runner.py:310] Step = 96300 ; steps/s = 1.64, tokens/s = 78368 (34522 source, 43846 target) ; Learning rate = 0.000285 ; Loss = 1.437673\n",
      "2024-12-03 03:12:16.053000: I runner.py:310] Step = 96400 ; steps/s = 1.66, tokens/s = 77647 (34230 source, 43417 target) ; Learning rate = 0.000285 ; Loss = 1.438155\n",
      "2024-12-03 03:13:16.825000: I runner.py:310] Step = 96500 ; steps/s = 1.65, tokens/s = 78397 (34516 source, 43881 target) ; Learning rate = 0.000285 ; Loss = 1.436537\n",
      "2024-12-03 03:14:17.563000: I runner.py:310] Step = 96600 ; steps/s = 1.65, tokens/s = 78493 (34586 source, 43907 target) ; Learning rate = 0.000284 ; Loss = 1.437859\n",
      "2024-12-03 03:15:17.848000: I runner.py:310] Step = 96700 ; steps/s = 1.66, tokens/s = 77651 (34243 source, 43408 target) ; Learning rate = 0.000284 ; Loss = 1.441739\n",
      "2024-12-03 03:16:18.626000: I runner.py:310] Step = 96800 ; steps/s = 1.65, tokens/s = 78382 (34514 source, 43868 target) ; Learning rate = 0.000284 ; Loss = 1.431277\n",
      "2024-12-03 03:17:18.939000: I runner.py:310] Step = 96900 ; steps/s = 1.66, tokens/s = 77628 (34223 source, 43405 target) ; Learning rate = 0.000284 ; Loss = 1.426261\n",
      "2024-12-03 03:18:19.740000: I runner.py:310] Step = 97000 ; steps/s = 1.65, tokens/s = 78382 (34513 source, 43869 target) ; Learning rate = 0.000284 ; Loss = 1.436508\n",
      "2024-12-03 03:19:20.470000: I runner.py:310] Step = 97100 ; steps/s = 1.65, tokens/s = 78465 (34562 source, 43903 target) ; Learning rate = 0.000284 ; Loss = 1.433905\n",
      "2024-12-03 03:20:20.709000: I runner.py:310] Step = 97200 ; steps/s = 1.66, tokens/s = 77687 (34243 source, 43444 target) ; Learning rate = 0.000284 ; Loss = 1.434613\n",
      "2024-12-03 03:21:21.463000: I runner.py:310] Step = 97300 ; steps/s = 1.65, tokens/s = 78448 (34546 source, 43902 target) ; Learning rate = 0.000283 ; Loss = 1.432860\n",
      "2024-12-03 03:22:21.756000: I runner.py:310] Step = 97400 ; steps/s = 1.66, tokens/s = 77641 (34244 source, 43397 target) ; Learning rate = 0.000283 ; Loss = 1.433394\n",
      "2024-12-03 03:23:22.542000: I runner.py:310] Step = 97500 ; steps/s = 1.65, tokens/s = 78394 (34516 source, 43878 target) ; Learning rate = 0.000283 ; Loss = 1.437383\n",
      "2024-12-03 03:24:23.264000: I runner.py:310] Step = 97600 ; steps/s = 1.65, tokens/s = 78490 (34572 source, 43918 target) ; Learning rate = 0.000283 ; Loss = 1.436127\n",
      "2024-12-03 03:25:23.538000: I runner.py:310] Step = 97700 ; steps/s = 1.66, tokens/s = 77651 (34239 source, 43412 target) ; Learning rate = 0.000283 ; Loss = 1.435687\n",
      "2024-12-03 03:26:24.278000: I runner.py:310] Step = 97800 ; steps/s = 1.65, tokens/s = 78436 (34533 source, 43903 target) ; Learning rate = 0.000283 ; Loss = 1.437606\n",
      "2024-12-03 03:27:25.079000: I runner.py:310] Step = 97900 ; steps/s = 1.64, tokens/s = 78374 (34516 source, 43858 target) ; Learning rate = 0.000282 ; Loss = 1.430798\n",
      "2024-12-03 03:28:25.388000: I runner.py:310] Step = 98000 ; steps/s = 1.66, tokens/s = 77601 (34211 source, 43390 target) ; Learning rate = 0.000282 ; Loss = 1.434999\n",
      "2024-12-03 03:29:26.137000: I runner.py:310] Step = 98100 ; steps/s = 1.65, tokens/s = 78431 (34528 source, 43903 target) ; Learning rate = 0.000282 ; Loss = 1.437455\n",
      "2024-12-03 03:30:26.435000: I runner.py:310] Step = 98200 ; steps/s = 1.66, tokens/s = 77656 (34260 source, 43396 target) ; Learning rate = 0.000282 ; Loss = 1.434134\n",
      "2024-12-03 03:31:27.200000: I runner.py:310] Step = 98300 ; steps/s = 1.65, tokens/s = 78418 (34522 source, 43896 target) ; Learning rate = 0.000282 ; Loss = 1.434309\n",
      "2024-12-03 03:32:27.950000: I runner.py:310] Step = 98400 ; steps/s = 1.65, tokens/s = 78445 (34552 source, 43893 target) ; Learning rate = 0.000282 ; Loss = 1.440000\n",
      "2024-12-03 03:33:28.240000: I runner.py:310] Step = 98500 ; steps/s = 1.66, tokens/s = 77650 (34239 source, 43411 target) ; Learning rate = 0.000282 ; Loss = 1.433850\n",
      "2024-12-03 03:34:28.963000: I runner.py:310] Step = 98600 ; steps/s = 1.65, tokens/s = 78483 (34559 source, 43924 target) ; Learning rate = 0.000281 ; Loss = 1.437323\n",
      "2024-12-03 03:35:29.240000: I runner.py:310] Step = 98700 ; steps/s = 1.66, tokens/s = 77646 (34247 source, 43399 target) ; Learning rate = 0.000281 ; Loss = 1.435750\n",
      "2024-12-03 03:36:29.913000: I runner.py:310] Step = 98800 ; steps/s = 1.65, tokens/s = 78523 (34568 source, 43955 target) ; Learning rate = 0.000281 ; Loss = 1.436326\n",
      "2024-12-03 03:37:30.673000: I runner.py:310] Step = 98900 ; steps/s = 1.65, tokens/s = 78424 (34539 source, 43885 target) ; Learning rate = 0.000281 ; Loss = 1.431085\n",
      "2024-12-03 03:38:30.896000: I runner.py:310] Step = 99000 ; steps/s = 1.66, tokens/s = 77720 (34250 source, 43470 target) ; Learning rate = 0.000281 ; Loss = 1.437237\n",
      "2024-12-03 03:39:31.612000: I runner.py:310] Step = 99100 ; steps/s = 1.65, tokens/s = 78480 (34572 source, 43908 target) ; Learning rate = 0.000281 ; Loss = 1.437488\n",
      "2024-12-03 03:40:32.396000: I runner.py:310] Step = 99200 ; steps/s = 1.65, tokens/s = 78405 (34531 source, 43874 target) ; Learning rate = 0.000281 ; Loss = 1.430195\n",
      "2024-12-03 03:41:32.698000: I runner.py:310] Step = 99300 ; steps/s = 1.66, tokens/s = 77624 (34224 source, 43400 target) ; Learning rate = 0.000280 ; Loss = 1.437157\n",
      "2024-12-03 03:42:33.498000: I runner.py:310] Step = 99400 ; steps/s = 1.64, tokens/s = 78395 (34536 source, 43859 target) ; Learning rate = 0.000280 ; Loss = 1.433213\n",
      "2024-12-03 03:43:33.853000: I runner.py:310] Step = 99500 ; steps/s = 1.66, tokens/s = 77565 (34192 source, 43373 target) ; Learning rate = 0.000280 ; Loss = 1.436106\n",
      "2024-12-03 03:44:34.620000: I runner.py:310] Step = 99600 ; steps/s = 1.65, tokens/s = 78410 (34526 source, 43884 target) ; Learning rate = 0.000280 ; Loss = 1.430875\n",
      "2024-12-03 03:45:35.343000: I runner.py:310] Step = 99700 ; steps/s = 1.65, tokens/s = 78472 (34563 source, 43909 target) ; Learning rate = 0.000280 ; Loss = 1.433957\n",
      "2024-12-03 03:46:35.634000: I runner.py:310] Step = 99800 ; steps/s = 1.66, tokens/s = 77638 (34228 source, 43410 target) ; Learning rate = 0.000280 ; Loss = 1.435719\n",
      "2024-12-03 03:47:36.295000: I runner.py:310] Step = 99900 ; steps/s = 1.65, tokens/s = 78571 (34612 source, 43959 target) ; Learning rate = 0.000280 ; Loss = 1.437591\n",
      "2024-12-03 03:48:36.605000: I runner.py:310] Step = 100000 ; steps/s = 1.66, tokens/s = 77607 (34211 source, 43396 target) ; Learning rate = 0.000280 ; Loss = 1.430560\n",
      "2024-12-03 03:48:38.709000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-100000\n",
      "2024-12-03 03:48:38.709000: I training.py:192] Running evaluation for step 100000\n",
      "2024-12-03 03:49:26.281000: I training.py:192] Evaluation result for step 100000: loss = 0.840009 ; perplexity = 2.316388\n",
      "2024-12-03 03:50:26.906000: I runner.py:310] Step = 100100 ; steps/s = 1.65, tokens/s = 78616 (34620 source, 43996 target) ; Learning rate = 0.000279 ; Loss = 1.437722\n",
      "2024-12-03 03:51:27.723000: I runner.py:310] Step = 100200 ; steps/s = 1.64, tokens/s = 78348 (34497 source, 43851 target) ; Learning rate = 0.000279 ; Loss = 1.437056\n",
      "2024-12-03 03:52:28.024000: I runner.py:310] Step = 100300 ; steps/s = 1.66, tokens/s = 77610 (34215 source, 43395 target) ; Learning rate = 0.000279 ; Loss = 1.431909\n",
      "2024-12-03 03:53:28.777000: I runner.py:310] Step = 100400 ; steps/s = 1.65, tokens/s = 78438 (34541 source, 43897 target) ; Learning rate = 0.000279 ; Loss = 1.440119\n",
      "2024-12-03 03:54:29.616000: I runner.py:310] Step = 100500 ; steps/s = 1.64, tokens/s = 78335 (34508 source, 43827 target) ; Learning rate = 0.000279 ; Loss = 1.435463\n",
      "2024-12-03 03:55:29.981000: I runner.py:310] Step = 100600 ; steps/s = 1.66, tokens/s = 77536 (34181 source, 43355 target) ; Learning rate = 0.000279 ; Loss = 1.431411\n",
      "2024-12-03 03:56:30.718000: I runner.py:310] Step = 100700 ; steps/s = 1.65, tokens/s = 78471 (34566 source, 43905 target) ; Learning rate = 0.000279 ; Loss = 1.433002\n",
      "2024-12-03 03:57:31.031000: I runner.py:310] Step = 100800 ; steps/s = 1.66, tokens/s = 77598 (34199 source, 43399 target) ; Learning rate = 0.000278 ; Loss = 1.437736\n",
      "2024-12-03 03:58:31.752000: I runner.py:310] Step = 100900 ; steps/s = 1.65, tokens/s = 78468 (34558 source, 43910 target) ; Learning rate = 0.000278 ; Loss = 1.435752\n",
      "2024-12-03 03:59:32.472000: I runner.py:310] Step = 101000 ; steps/s = 1.65, tokens/s = 78510 (34587 source, 43923 target) ; Learning rate = 0.000278 ; Loss = 1.434103\n",
      "2024-12-03 04:00:32.780000: I runner.py:310] Step = 101100 ; steps/s = 1.66, tokens/s = 77618 (34217 source, 43401 target) ; Learning rate = 0.000278 ; Loss = 1.449076\n",
      "2024-12-03 04:01:33.536000: I runner.py:310] Step = 101200 ; steps/s = 1.65, tokens/s = 78423 (34538 source, 43885 target) ; Learning rate = 0.000278 ; Loss = 1.434306\n",
      "2024-12-03 04:02:33.812000: I runner.py:310] Step = 101300 ; steps/s = 1.66, tokens/s = 77642 (34228 source, 43414 target) ; Learning rate = 0.000278 ; Loss = 1.434552\n",
      "2024-12-03 04:03:34.558000: I runner.py:310] Step = 101400 ; steps/s = 1.65, tokens/s = 78429 (34531 source, 43898 target) ; Learning rate = 0.000278 ; Loss = 1.433851\n",
      "2024-12-03 04:04:35.279000: I runner.py:310] Step = 101500 ; steps/s = 1.65, tokens/s = 78506 (34588 source, 43918 target) ; Learning rate = 0.000277 ; Loss = 1.441370\n",
      "2024-12-03 04:05:35.553000: I runner.py:310] Step = 101600 ; steps/s = 1.66, tokens/s = 77646 (34223 source, 43423 target) ; Learning rate = 0.000277 ; Loss = 1.431255\n",
      "2024-12-03 04:06:36.331000: I runner.py:310] Step = 101700 ; steps/s = 1.65, tokens/s = 78389 (34518 source, 43871 target) ; Learning rate = 0.000277 ; Loss = 1.435313\n",
      "2024-12-03 04:07:37.139000: I runner.py:310] Step = 101800 ; steps/s = 1.64, tokens/s = 78384 (34530 source, 43854 target) ; Learning rate = 0.000277 ; Loss = 1.438423\n",
      "2024-12-03 04:08:37.378000: I runner.py:310] Step = 101900 ; steps/s = 1.66, tokens/s = 77660 (34214 source, 43446 target) ; Learning rate = 0.000277 ; Loss = 1.432001\n",
      "2024-12-03 04:09:38.243000: I runner.py:310] Step = 102000 ; steps/s = 1.64, tokens/s = 78301 (34494 source, 43807 target) ; Learning rate = 0.000277 ; Loss = 1.432378\n",
      "2024-12-03 04:10:38.544000: I runner.py:310] Step = 102100 ; steps/s = 1.66, tokens/s = 77662 (34255 source, 43407 target) ; Learning rate = 0.000277 ; Loss = 1.430459\n",
      "2024-12-03 04:11:39.298000: I runner.py:310] Step = 102200 ; steps/s = 1.65, tokens/s = 78434 (34535 source, 43899 target) ; Learning rate = 0.000276 ; Loss = 1.443244\n",
      "2024-12-03 04:12:40.072000: I runner.py:310] Step = 102300 ; steps/s = 1.65, tokens/s = 78413 (34537 source, 43876 target) ; Learning rate = 0.000276 ; Loss = 1.436425\n",
      "2024-12-03 04:13:40.365000: I runner.py:310] Step = 102400 ; steps/s = 1.66, tokens/s = 77629 (34225 source, 43404 target) ; Learning rate = 0.000276 ; Loss = 1.435082\n",
      "2024-12-03 04:14:41.123000: I runner.py:310] Step = 102500 ; steps/s = 1.65, tokens/s = 78420 (34526 source, 43894 target) ; Learning rate = 0.000276 ; Loss = 1.434374\n",
      "2024-12-03 04:15:41.472000: I runner.py:310] Step = 102600 ; steps/s = 1.66, tokens/s = 77573 (34217 source, 43356 target) ; Learning rate = 0.000276 ; Loss = 1.426728\n",
      "2024-12-03 04:16:42.203000: I runner.py:310] Step = 102700 ; steps/s = 1.65, tokens/s = 78461 (34547 source, 43914 target) ; Learning rate = 0.000276 ; Loss = 1.440182\n",
      "2024-12-03 04:17:42.933000: I runner.py:310] Step = 102800 ; steps/s = 1.65, tokens/s = 78484 (34571 source, 43913 target) ; Learning rate = 0.000276 ; Loss = 1.439060\n",
      "2024-12-03 04:18:43.215000: I runner.py:310] Step = 102900 ; steps/s = 1.66, tokens/s = 77615 (34201 source, 43414 target) ; Learning rate = 0.000276 ; Loss = 1.435690\n",
      "2024-12-03 04:19:43.960000: I runner.py:310] Step = 103000 ; steps/s = 1.65, tokens/s = 78442 (34541 source, 43901 target) ; Learning rate = 0.000275 ; Loss = 1.426465\n",
      "2024-12-03 04:20:44.707000: I runner.py:310] Step = 103100 ; steps/s = 1.65, tokens/s = 78464 (34570 source, 43894 target) ; Learning rate = 0.000275 ; Loss = 1.435771\n",
      "2024-12-03 04:21:44.964000: I runner.py:310] Step = 103200 ; steps/s = 1.66, tokens/s = 77651 (34220 source, 43431 target) ; Learning rate = 0.000275 ; Loss = 1.432427\n",
      "2024-12-03 04:22:45.704000: I runner.py:310] Step = 103300 ; steps/s = 1.65, tokens/s = 78462 (34569 source, 43893 target) ; Learning rate = 0.000275 ; Loss = 1.438969\n",
      "2024-12-03 04:23:45.996000: I runner.py:310] Step = 103400 ; steps/s = 1.66, tokens/s = 77650 (34231 source, 43419 target) ; Learning rate = 0.000275 ; Loss = 1.431178\n",
      "2024-12-03 04:24:46.694000: I runner.py:310] Step = 103500 ; steps/s = 1.65, tokens/s = 78484 (34555 source, 43929 target) ; Learning rate = 0.000275 ; Loss = 1.430868\n",
      "2024-12-03 04:25:47.525000: I runner.py:310] Step = 103600 ; steps/s = 1.64, tokens/s = 78352 (34512 source, 43840 target) ; Learning rate = 0.000275 ; Loss = 1.436406\n",
      "2024-12-03 04:26:47.791000: I runner.py:310] Step = 103700 ; steps/s = 1.66, tokens/s = 77669 (34244 source, 43425 target) ; Learning rate = 0.000274 ; Loss = 1.430061\n",
      "2024-12-03 04:27:48.606000: I runner.py:310] Step = 103800 ; steps/s = 1.64, tokens/s = 78354 (34504 source, 43850 target) ; Learning rate = 0.000274 ; Loss = 1.432158\n",
      "2024-12-03 04:28:48.896000: I runner.py:310] Step = 103900 ; steps/s = 1.66, tokens/s = 77669 (34260 source, 43409 target) ; Learning rate = 0.000274 ; Loss = 1.426011\n",
      "2024-12-03 04:29:49.651000: I runner.py:310] Step = 104000 ; steps/s = 1.65, tokens/s = 78432 (34534 source, 43898 target) ; Learning rate = 0.000274 ; Loss = 1.436255\n",
      "2024-12-03 04:30:50.403000: I runner.py:310] Step = 104100 ; steps/s = 1.65, tokens/s = 78454 (34557 source, 43897 target) ; Learning rate = 0.000274 ; Loss = 1.432598\n",
      "2024-12-03 04:31:50.734000: I runner.py:310] Step = 104200 ; steps/s = 1.66, tokens/s = 77583 (34194 source, 43389 target) ; Learning rate = 0.000274 ; Loss = 1.438460\n",
      "2024-12-03 04:32:51.460000: I runner.py:310] Step = 104300 ; steps/s = 1.65, tokens/s = 78446 (34543 source, 43903 target) ; Learning rate = 0.000274 ; Loss = 1.432827\n",
      "2024-12-03 04:33:52.158000: I runner.py:310] Step = 104400 ; steps/s = 1.65, tokens/s = 78518 (34589 source, 43929 target) ; Learning rate = 0.000274 ; Loss = 1.432323\n",
      "2024-12-03 04:34:52.376000: I runner.py:310] Step = 104500 ; steps/s = 1.66, tokens/s = 77743 (34286 source, 43457 target) ; Learning rate = 0.000273 ; Loss = 1.434612\n",
      "2024-12-03 04:35:53.155000: I runner.py:310] Step = 104600 ; steps/s = 1.65, tokens/s = 78392 (34519 source, 43873 target) ; Learning rate = 0.000273 ; Loss = 1.431184\n",
      "2024-12-03 04:36:53.457000: I runner.py:310] Step = 104700 ; steps/s = 1.66, tokens/s = 77603 (34202 source, 43401 target) ; Learning rate = 0.000273 ; Loss = 1.430895\n",
      "2024-12-03 04:37:54.204000: I runner.py:310] Step = 104800 ; steps/s = 1.65, tokens/s = 78438 (34535 source, 43903 target) ; Learning rate = 0.000273 ; Loss = 1.431666\n",
      "2024-12-03 04:38:54.971000: I runner.py:310] Step = 104900 ; steps/s = 1.65, tokens/s = 78413 (34534 source, 43879 target) ; Learning rate = 0.000273 ; Loss = 1.436750\n",
      "2024-12-03 04:39:55.256000: I runner.py:310] Step = 105000 ; steps/s = 1.66, tokens/s = 77688 (34276 source, 43412 target) ; Learning rate = 0.000273 ; Loss = 1.434925\n",
      "2024-12-03 04:39:55.258000: I training.py:192] Running evaluation for step 105000\n",
      "2024-12-03 04:40:42.830000: I training.py:192] Evaluation result for step 105000: loss = 0.844553 ; perplexity = 2.326938\n",
      "2024-12-03 04:41:43.354000: I runner.py:310] Step = 105100 ; steps/s = 1.65, tokens/s = 78746 (34672 source, 44074 target) ; Learning rate = 0.000273 ; Loss = 1.432980\n",
      "2024-12-03 04:42:43.664000: I runner.py:310] Step = 105200 ; steps/s = 1.66, tokens/s = 77599 (34208 source, 43391 target) ; Learning rate = 0.000273 ; Loss = 1.432939\n",
      "2024-12-03 04:43:44.399000: I runner.py:310] Step = 105300 ; steps/s = 1.65, tokens/s = 78421 (34516 source, 43905 target) ; Learning rate = 0.000272 ; Loss = 1.432650\n",
      "2024-12-03 04:44:45.188000: I runner.py:310] Step = 105400 ; steps/s = 1.65, tokens/s = 78436 (34570 source, 43866 target) ; Learning rate = 0.000272 ; Loss = 1.435268\n",
      "2024-12-03 04:45:45.517000: I runner.py:310] Step = 105500 ; steps/s = 1.66, tokens/s = 77593 (34194 source, 43399 target) ; Learning rate = 0.000272 ; Loss = 1.432699\n",
      "2024-12-03 04:46:46.251000: I runner.py:310] Step = 105600 ; steps/s = 1.65, tokens/s = 78460 (34556 source, 43904 target) ; Learning rate = 0.000272 ; Loss = 1.435566\n",
      "2024-12-03 04:47:46.986000: I runner.py:310] Step = 105700 ; steps/s = 1.65, tokens/s = 78449 (34550 source, 43899 target) ; Learning rate = 0.000272 ; Loss = 1.432576\n",
      "2024-12-03 04:48:47.284000: I runner.py:310] Step = 105800 ; steps/s = 1.66, tokens/s = 77625 (34216 source, 43409 target) ; Learning rate = 0.000272 ; Loss = 1.431077\n",
      "2024-12-03 04:49:48.017000: I runner.py:310] Step = 105900 ; steps/s = 1.65, tokens/s = 78481 (34572 source, 43909 target) ; Learning rate = 0.000272 ; Loss = 1.433944\n",
      "2024-12-03 04:50:48.319000: I runner.py:310] Step = 106000 ; steps/s = 1.66, tokens/s = 77622 (34231 source, 43391 target) ; Learning rate = 0.000271 ; Loss = 1.431674\n",
      "2024-12-03 04:51:49.098000: I runner.py:310] Step = 106100 ; steps/s = 1.65, tokens/s = 78405 (34523 source, 43882 target) ; Learning rate = 0.000271 ; Loss = 1.433563\n",
      "2024-12-03 04:52:49.812000: I runner.py:310] Step = 106200 ; steps/s = 1.65, tokens/s = 78484 (34563 source, 43921 target) ; Learning rate = 0.000271 ; Loss = 1.436421\n",
      "2024-12-03 04:53:50.094000: I runner.py:310] Step = 106300 ; steps/s = 1.66, tokens/s = 77653 (34236 source, 43417 target) ; Learning rate = 0.000271 ; Loss = 1.434382\n",
      "2024-12-03 04:54:50.848000: I runner.py:310] Step = 106400 ; steps/s = 1.65, tokens/s = 78422 (34526 source, 43896 target) ; Learning rate = 0.000271 ; Loss = 1.430071\n",
      "2024-12-03 04:55:51.078000: I runner.py:310] Step = 106500 ; steps/s = 1.66, tokens/s = 77724 (34277 source, 43447 target) ; Learning rate = 0.000271 ; Loss = 1.434319\n",
      "2024-12-03 04:56:51.793000: I runner.py:310] Step = 106600 ; steps/s = 1.65, tokens/s = 78475 (34551 source, 43924 target) ; Learning rate = 0.000271 ; Loss = 1.430027\n",
      "2024-12-03 04:57:52.541000: I runner.py:310] Step = 106700 ; steps/s = 1.65, tokens/s = 78464 (34574 source, 43890 target) ; Learning rate = 0.000271 ; Loss = 1.434303\n",
      "2024-12-03 04:58:52.841000: I runner.py:310] Step = 106800 ; steps/s = 1.66, tokens/s = 77591 (34184 source, 43407 target) ; Learning rate = 0.000270 ; Loss = 1.434446\n",
      "2024-12-03 04:59:53.610000: I runner.py:310] Step = 106900 ; steps/s = 1.65, tokens/s = 78458 (34569 source, 43889 target) ; Learning rate = 0.000270 ; Loss = 1.433732\n",
      "2024-12-03 05:00:54.236000: I runner.py:310] Step = 107000 ; steps/s = 1.65, tokens/s = 78262 (34472 source, 43790 target) ; Learning rate = 0.000270 ; Loss = 1.433512\n",
      "2024-12-03 05:01:54.556000: I runner.py:310] Step = 107100 ; steps/s = 1.66, tokens/s = 77909 (34339 source, 43570 target) ; Learning rate = 0.000270 ; Loss = 1.431804\n",
      "2024-12-03 05:02:55.344000: I runner.py:310] Step = 107200 ; steps/s = 1.65, tokens/s = 78399 (34529 source, 43870 target) ; Learning rate = 0.000270 ; Loss = 1.434911\n",
      "2024-12-03 05:03:55.581000: I runner.py:310] Step = 107300 ; steps/s = 1.66, tokens/s = 77719 (34279 source, 43440 target) ; Learning rate = 0.000270 ; Loss = 1.435047\n",
      "2024-12-03 05:04:56.342000: I runner.py:310] Step = 107400 ; steps/s = 1.65, tokens/s = 78386 (34502 source, 43884 target) ; Learning rate = 0.000270 ; Loss = 1.433563\n",
      "2024-12-03 05:05:57.120000: I runner.py:310] Step = 107500 ; steps/s = 1.65, tokens/s = 78422 (34535 source, 43887 target) ; Learning rate = 0.000270 ; Loss = 1.435181\n",
      "2024-12-03 05:06:57.440000: I runner.py:310] Step = 107600 ; steps/s = 1.66, tokens/s = 77592 (34203 source, 43389 target) ; Learning rate = 0.000269 ; Loss = 1.432229\n",
      "2024-12-03 05:07:58.149000: I runner.py:310] Step = 107700 ; steps/s = 1.65, tokens/s = 78514 (34589 source, 43925 target) ; Learning rate = 0.000269 ; Loss = 1.431082\n",
      "2024-12-03 05:08:58.486000: I runner.py:310] Step = 107800 ; steps/s = 1.66, tokens/s = 77587 (34218 source, 43369 target) ; Learning rate = 0.000269 ; Loss = 1.432698\n",
      "2024-12-03 05:09:59.261000: I runner.py:310] Step = 107900 ; steps/s = 1.65, tokens/s = 78382 (34506 source, 43876 target) ; Learning rate = 0.000269 ; Loss = 1.431383\n",
      "2024-12-03 05:11:00.041000: I runner.py:310] Step = 108000 ; steps/s = 1.65, tokens/s = 78417 (34541 source, 43876 target) ; Learning rate = 0.000269 ; Loss = 1.436325\n",
      "2024-12-03 05:12:00.319000: I runner.py:310] Step = 108100 ; steps/s = 1.66, tokens/s = 77650 (34228 source, 43422 target) ; Learning rate = 0.000269 ; Loss = 1.431977\n",
      "2024-12-03 05:13:01.084000: I runner.py:310] Step = 108200 ; steps/s = 1.65, tokens/s = 78432 (34548 source, 43884 target) ; Learning rate = 0.000269 ; Loss = 1.433730\n",
      "2024-12-03 05:14:01.514000: I runner.py:310] Step = 108300 ; steps/s = 1.65, tokens/s = 78064 (34423 source, 43641 target) ; Learning rate = 0.000269 ; Loss = 1.445625\n",
      "2024-12-03 05:15:02.103000: I runner.py:310] Step = 108400 ; steps/s = 1.65, tokens/s = 78052 (34367 source, 43685 target) ; Learning rate = 0.000268 ; Loss = 1.430361\n",
      "2024-12-03 05:16:02.870000: I runner.py:310] Step = 108500 ; steps/s = 1.65, tokens/s = 78411 (34530 source, 43881 target) ; Learning rate = 0.000268 ; Loss = 1.433675\n",
      "2024-12-03 05:17:03.177000: I runner.py:310] Step = 108600 ; steps/s = 1.66, tokens/s = 77609 (34211 source, 43398 target) ; Learning rate = 0.000268 ; Loss = 1.433214\n",
      "2024-12-03 05:18:03.950000: I runner.py:310] Step = 108700 ; steps/s = 1.65, tokens/s = 78421 (34544 source, 43877 target) ; Learning rate = 0.000268 ; Loss = 1.434688\n",
      "2024-12-03 05:19:04.679000: I runner.py:310] Step = 108800 ; steps/s = 1.65, tokens/s = 78460 (34546 source, 43914 target) ; Learning rate = 0.000268 ; Loss = 1.432989\n",
      "2024-12-03 05:20:04.956000: I runner.py:310] Step = 108900 ; steps/s = 1.66, tokens/s = 77657 (34244 source, 43413 target) ; Learning rate = 0.000268 ; Loss = 1.433245\n",
      "2024-12-03 05:21:05.716000: I runner.py:310] Step = 109000 ; steps/s = 1.65, tokens/s = 78425 (34539 source, 43886 target) ; Learning rate = 0.000268 ; Loss = 1.429378\n",
      "2024-12-03 05:22:06.058000: I runner.py:310] Step = 109100 ; steps/s = 1.66, tokens/s = 77585 (34216 source, 43369 target) ; Learning rate = 0.000268 ; Loss = 1.426906\n",
      "2024-12-03 05:23:06.835000: I runner.py:310] Step = 109200 ; steps/s = 1.65, tokens/s = 78387 (34505 source, 43882 target) ; Learning rate = 0.000267 ; Loss = 1.429346\n",
      "2024-12-03 05:24:07.629000: I runner.py:310] Step = 109300 ; steps/s = 1.65, tokens/s = 78402 (34536 source, 43866 target) ; Learning rate = 0.000267 ; Loss = 1.433580\n",
      "2024-12-03 05:25:07.934000: I runner.py:310] Step = 109400 ; steps/s = 1.66, tokens/s = 77586 (34198 source, 43388 target) ; Learning rate = 0.000267 ; Loss = 1.432421\n",
      "2024-12-03 05:26:08.697000: I runner.py:310] Step = 109500 ; steps/s = 1.65, tokens/s = 78421 (34528 source, 43893 target) ; Learning rate = 0.000267 ; Loss = 1.437735\n",
      "2024-12-03 05:27:08.993000: I runner.py:310] Step = 109600 ; steps/s = 1.66, tokens/s = 77667 (34268 source, 43399 target) ; Learning rate = 0.000267 ; Loss = 1.425546\n",
      "2024-12-03 05:28:09.701000: I runner.py:310] Step = 109700 ; steps/s = 1.65, tokens/s = 78485 (34559 source, 43926 target) ; Learning rate = 0.000267 ; Loss = 1.432699\n",
      "2024-12-03 05:29:10.440000: I runner.py:310] Step = 109800 ; steps/s = 1.65, tokens/s = 78457 (34552 source, 43905 target) ; Learning rate = 0.000267 ; Loss = 1.431167\n",
      "2024-12-03 05:30:10.695000: I runner.py:310] Step = 109900 ; steps/s = 1.66, tokens/s = 77672 (34239 source, 43433 target) ; Learning rate = 0.000267 ; Loss = 1.427529\n",
      "2024-12-03 05:31:11.393000: I runner.py:310] Step = 110000 ; steps/s = 1.65, tokens/s = 78486 (34556 source, 43930 target) ; Learning rate = 0.000266 ; Loss = 1.442029\n",
      "2024-12-03 05:31:13.950000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-110000\n",
      "2024-12-03 05:31:13.950000: I training.py:192] Running evaluation for step 110000\n",
      "2024-12-03 05:32:01.707000: I training.py:192] Evaluation result for step 110000: loss = 0.846227 ; perplexity = 2.330837\n",
      "2024-12-03 05:33:02.363000: I runner.py:310] Step = 110100 ; steps/s = 1.65, tokens/s = 78594 (34619 source, 43975 target) ; Learning rate = 0.000266 ; Loss = 1.437155\n",
      "2024-12-03 05:34:02.616000: I runner.py:310] Step = 110200 ; steps/s = 1.66, tokens/s = 77708 (34274 source, 43434 target) ; Learning rate = 0.000266 ; Loss = 1.428813\n",
      "2024-12-03 05:35:03.316000: I runner.py:310] Step = 110300 ; steps/s = 1.65, tokens/s = 78502 (34562 source, 43940 target) ; Learning rate = 0.000266 ; Loss = 1.437840\n",
      "2024-12-03 05:36:03.638000: I runner.py:310] Step = 110400 ; steps/s = 1.66, tokens/s = 77574 (34197 source, 43377 target) ; Learning rate = 0.000266 ; Loss = 1.428024\n",
      "2024-12-03 05:37:04.389000: I runner.py:310] Step = 110500 ; steps/s = 1.65, tokens/s = 78453 (34552 source, 43901 target) ; Learning rate = 0.000266 ; Loss = 1.434074\n",
      "2024-12-03 05:38:05.068000: I runner.py:310] Step = 110600 ; steps/s = 1.65, tokens/s = 78537 (34592 source, 43945 target) ; Learning rate = 0.000266 ; Loss = 1.435794\n",
      "2024-12-03 05:39:05.404000: I runner.py:310] Step = 110700 ; steps/s = 1.66, tokens/s = 77568 (34194 source, 43374 target) ; Learning rate = 0.000266 ; Loss = 1.425886\n",
      "2024-12-03 05:40:06.151000: I runner.py:310] Step = 110800 ; steps/s = 1.65, tokens/s = 78455 (34564 source, 43891 target) ; Learning rate = 0.000266 ; Loss = 1.433741\n",
      "2024-12-03 05:41:06.399000: I runner.py:310] Step = 110900 ; steps/s = 1.66, tokens/s = 77692 (34250 source, 43442 target) ; Learning rate = 0.000265 ; Loss = 1.429790\n",
      "2024-12-03 05:42:07.115000: I runner.py:310] Step = 111000 ; steps/s = 1.65, tokens/s = 78447 (34528 source, 43919 target) ; Learning rate = 0.000265 ; Loss = 1.429666\n",
      "2024-12-03 05:43:07.840000: I runner.py:310] Step = 111100 ; steps/s = 1.65, tokens/s = 78493 (34575 source, 43918 target) ; Learning rate = 0.000265 ; Loss = 1.433824\n",
      "2024-12-03 05:44:08.114000: I runner.py:310] Step = 111200 ; steps/s = 1.66, tokens/s = 77664 (34245 source, 43419 target) ; Learning rate = 0.000265 ; Loss = 1.429323\n",
      "2024-12-03 05:45:08.783000: I runner.py:310] Step = 111300 ; steps/s = 1.65, tokens/s = 78558 (34597 source, 43961 target) ; Learning rate = 0.000265 ; Loss = 1.433359\n",
      "2024-12-03 05:46:09.589000: I runner.py:310] Step = 111400 ; steps/s = 1.64, tokens/s = 78363 (34511 source, 43852 target) ; Learning rate = 0.000265 ; Loss = 1.436671\n",
      "2024-12-03 05:47:09.935000: I runner.py:310] Step = 111500 ; steps/s = 1.66, tokens/s = 77571 (34199 source, 43372 target) ; Learning rate = 0.000265 ; Loss = 1.433946\n",
      "2024-12-03 05:48:10.743000: I runner.py:310] Step = 111600 ; steps/s = 1.64, tokens/s = 78357 (34510 source, 43847 target) ; Learning rate = 0.000265 ; Loss = 1.434206\n",
      "2024-12-03 05:49:11.084000: I runner.py:310] Step = 111700 ; steps/s = 1.66, tokens/s = 77575 (34189 source, 43386 target) ; Learning rate = 0.000264 ; Loss = 1.435153\n",
      "2024-12-03 05:50:11.786000: I runner.py:310] Step = 111800 ; steps/s = 1.65, tokens/s = 78505 (34579 source, 43926 target) ; Learning rate = 0.000264 ; Loss = 1.434614\n",
      "2024-12-03 05:51:12.433000: I runner.py:310] Step = 111900 ; steps/s = 1.65, tokens/s = 78574 (34603 source, 43971 target) ; Learning rate = 0.000264 ; Loss = 1.434394\n",
      "2024-12-03 05:52:12.691000: I runner.py:310] Step = 112000 ; steps/s = 1.66, tokens/s = 77665 (34242 source, 43423 target) ; Learning rate = 0.000264 ; Loss = 1.432012\n",
      "2024-12-03 05:53:13.417000: I runner.py:310] Step = 112100 ; steps/s = 1.65, tokens/s = 78473 (34558 source, 43915 target) ; Learning rate = 0.000264 ; Loss = 1.428603\n",
      "2024-12-03 05:54:13.675000: I runner.py:310] Step = 112200 ; steps/s = 1.66, tokens/s = 77705 (34273 source, 43432 target) ; Learning rate = 0.000264 ; Loss = 1.431131\n",
      "2024-12-03 05:55:14.362000: I runner.py:310] Step = 112300 ; steps/s = 1.65, tokens/s = 78500 (34556 source, 43944 target) ; Learning rate = 0.000264 ; Loss = 1.427998\n",
      "2024-12-03 05:56:15.088000: I runner.py:310] Step = 112400 ; steps/s = 1.65, tokens/s = 78465 (34562 source, 43903 target) ; Learning rate = 0.000264 ; Loss = 1.432120\n",
      "2024-12-03 05:57:15.360000: I runner.py:310] Step = 112500 ; steps/s = 1.66, tokens/s = 77672 (34247 source, 43425 target) ; Learning rate = 0.000264 ; Loss = 1.429421\n",
      "2024-12-03 05:58:16.149000: I runner.py:310] Step = 112600 ; steps/s = 1.65, tokens/s = 78349 (34486 source, 43863 target) ; Learning rate = 0.000263 ; Loss = 1.434465\n",
      "2024-12-03 05:59:16.912000: I runner.py:310] Step = 112700 ; steps/s = 1.65, tokens/s = 78454 (34562 source, 43892 target) ; Learning rate = 0.000263 ; Loss = 1.435629\n",
      "2024-12-03 06:00:17.161000: I runner.py:310] Step = 112800 ; steps/s = 1.66, tokens/s = 77721 (34269 source, 43452 target) ; Learning rate = 0.000263 ; Loss = 1.427870\n",
      "2024-12-03 06:01:17.870000: I runner.py:310] Step = 112900 ; steps/s = 1.65, tokens/s = 78489 (34574 source, 43915 target) ; Learning rate = 0.000263 ; Loss = 1.432845\n",
      "2024-12-03 06:02:18.182000: I runner.py:310] Step = 113000 ; steps/s = 1.66, tokens/s = 77585 (34200 source, 43385 target) ; Learning rate = 0.000263 ; Loss = 1.431345\n",
      "2024-12-03 06:03:18.935000: I runner.py:310] Step = 113100 ; steps/s = 1.65, tokens/s = 78429 (34526 source, 43903 target) ; Learning rate = 0.000263 ; Loss = 1.433430\n",
      "2024-12-03 06:04:19.655000: I runner.py:310] Step = 113200 ; steps/s = 1.65, tokens/s = 78490 (34579 source, 43911 target) ; Learning rate = 0.000263 ; Loss = 1.433384\n",
      "2024-12-03 06:05:19.940000: I runner.py:310] Step = 113300 ; steps/s = 1.66, tokens/s = 77642 (34238 source, 43404 target) ; Learning rate = 0.000263 ; Loss = 1.432984\n",
      "2024-12-03 06:06:20.670000: I runner.py:310] Step = 113400 ; steps/s = 1.65, tokens/s = 78462 (34541 source, 43921 target) ; Learning rate = 0.000262 ; Loss = 1.429452\n",
      "2024-12-03 06:07:20.922000: I runner.py:310] Step = 113500 ; steps/s = 1.66, tokens/s = 77707 (34279 source, 43428 target) ; Learning rate = 0.000262 ; Loss = 1.430404\n",
      "2024-12-03 06:08:21.675000: I runner.py:310] Step = 113600 ; steps/s = 1.65, tokens/s = 78392 (34501 source, 43891 target) ; Learning rate = 0.000262 ; Loss = 1.429364\n",
      "2024-12-03 06:09:22.414000: I runner.py:310] Step = 113700 ; steps/s = 1.65, tokens/s = 78475 (34581 source, 43894 target) ; Learning rate = 0.000262 ; Loss = 1.430109\n",
      "2024-12-03 06:10:22.719000: I runner.py:310] Step = 113800 ; steps/s = 1.66, tokens/s = 77626 (34211 source, 43415 target) ; Learning rate = 0.000262 ; Loss = 1.434442\n",
      "2024-12-03 06:11:23.449000: I runner.py:310] Step = 113900 ; steps/s = 1.65, tokens/s = 78481 (34561 source, 43920 target) ; Learning rate = 0.000262 ; Loss = 1.434820\n",
      "2024-12-03 06:12:24.158000: I runner.py:310] Step = 114000 ; steps/s = 1.65, tokens/s = 78488 (34574 source, 43914 target) ; Learning rate = 0.000262 ; Loss = 1.435849\n",
      "2024-12-03 06:13:24.468000: I runner.py:310] Step = 114100 ; steps/s = 1.66, tokens/s = 77581 (34188 source, 43393 target) ; Learning rate = 0.000262 ; Loss = 1.433616\n",
      "2024-12-03 06:14:25.226000: I runner.py:310] Step = 114200 ; steps/s = 1.65, tokens/s = 78429 (34535 source, 43894 target) ; Learning rate = 0.000262 ; Loss = 1.433000\n",
      "2024-12-03 06:15:25.458000: I runner.py:310] Step = 114300 ; steps/s = 1.66, tokens/s = 77750 (34294 source, 43456 target) ; Learning rate = 0.000261 ; Loss = 1.433216\n",
      "2024-12-03 06:16:26.181000: I runner.py:310] Step = 114400 ; steps/s = 1.65, tokens/s = 78462 (34546 source, 43916 target) ; Learning rate = 0.000261 ; Loss = 1.432331\n",
      "2024-12-03 06:17:26.978000: I runner.py:310] Step = 114500 ; steps/s = 1.64, tokens/s = 78389 (34531 source, 43858 target) ; Learning rate = 0.000261 ; Loss = 1.430146\n",
      "2024-12-03 06:18:27.317000: I runner.py:310] Step = 114600 ; steps/s = 1.66, tokens/s = 77574 (34198 source, 43376 target) ; Learning rate = 0.000261 ; Loss = 1.430522\n",
      "2024-12-03 06:19:28.041000: I runner.py:310] Step = 114700 ; steps/s = 1.65, tokens/s = 78487 (34564 source, 43923 target) ; Learning rate = 0.000261 ; Loss = 1.432824\n",
      "2024-12-03 06:20:28.337000: I runner.py:310] Step = 114800 ; steps/s = 1.66, tokens/s = 77625 (34237 source, 43388 target) ; Learning rate = 0.000261 ; Loss = 1.428347\n",
      "2024-12-03 06:21:29.091000: I runner.py:310] Step = 114900 ; steps/s = 1.65, tokens/s = 78429 (34541 source, 43888 target) ; Learning rate = 0.000261 ; Loss = 1.433694\n",
      "2024-12-03 06:22:29.858000: I runner.py:310] Step = 115000 ; steps/s = 1.65, tokens/s = 78417 (34527 source, 43890 target) ; Learning rate = 0.000261 ; Loss = 1.427625\n",
      "2024-12-03 06:22:29.859000: I training.py:192] Running evaluation for step 115000\n",
      "2024-12-03 06:23:17.593000: I training.py:192] Evaluation result for step 115000: loss = 0.850199 ; perplexity = 2.340113\n",
      "2024-12-03 06:24:17.713000: I runner.py:310] Step = 115100 ; steps/s = 1.66, tokens/s = 77864 (34329 source, 43535 target) ; Learning rate = 0.000261 ; Loss = 1.435454\n",
      "2024-12-03 06:25:18.441000: I runner.py:310] Step = 115200 ; steps/s = 1.65, tokens/s = 78471 (34561 source, 43910 target) ; Learning rate = 0.000260 ; Loss = 1.433538\n",
      "2024-12-03 06:26:19.204000: I runner.py:310] Step = 115300 ; steps/s = 1.65, tokens/s = 78425 (34532 source, 43893 target) ; Learning rate = 0.000260 ; Loss = 1.434248\n",
      "2024-12-03 06:27:19.511000: I runner.py:310] Step = 115400 ; steps/s = 1.66, tokens/s = 77625 (34234 source, 43391 target) ; Learning rate = 0.000260 ; Loss = 1.432731\n",
      "2024-12-03 06:28:20.242000: I runner.py:310] Step = 115500 ; steps/s = 1.65, tokens/s = 78442 (34527 source, 43915 target) ; Learning rate = 0.000260 ; Loss = 1.431208\n",
      "2024-12-03 06:29:20.523000: I runner.py:310] Step = 115600 ; steps/s = 1.66, tokens/s = 77669 (34254 source, 43415 target) ; Learning rate = 0.000260 ; Loss = 1.430613\n",
      "2024-12-03 06:30:21.232000: I runner.py:310] Step = 115700 ; steps/s = 1.65, tokens/s = 78469 (34548 source, 43921 target) ; Learning rate = 0.000260 ; Loss = 1.425008\n",
      "2024-12-03 06:31:21.979000: I runner.py:310] Step = 115800 ; steps/s = 1.65, tokens/s = 78463 (34556 source, 43907 target) ; Learning rate = 0.000260 ; Loss = 1.431667\n",
      "2024-12-03 06:32:22.234000: I runner.py:310] Step = 115900 ; steps/s = 1.66, tokens/s = 77684 (34247 source, 43437 target) ; Learning rate = 0.000260 ; Loss = 1.431083\n",
      "2024-12-03 06:33:22.918000: I runner.py:310] Step = 116000 ; steps/s = 1.65, tokens/s = 78543 (34607 source, 43936 target) ; Learning rate = 0.000260 ; Loss = 1.429779\n",
      "2024-12-03 06:34:23.209000: I runner.py:310] Step = 116100 ; steps/s = 1.66, tokens/s = 77642 (34233 source, 43409 target) ; Learning rate = 0.000259 ; Loss = 1.428788\n",
      "2024-12-03 06:35:24.009000: I runner.py:310] Step = 116200 ; steps/s = 1.64, tokens/s = 78347 (34485 source, 43862 target) ; Learning rate = 0.000259 ; Loss = 1.429045\n",
      "2024-12-03 06:36:24.770000: I runner.py:310] Step = 116300 ; steps/s = 1.65, tokens/s = 78429 (34546 source, 43883 target) ; Learning rate = 0.000259 ; Loss = 1.437030\n",
      "2024-12-03 06:37:25.086000: I runner.py:310] Step = 116400 ; steps/s = 1.66, tokens/s = 77605 (34209 source, 43396 target) ; Learning rate = 0.000259 ; Loss = 1.429720\n",
      "2024-12-03 06:38:25.792000: I runner.py:310] Step = 116500 ; steps/s = 1.65, tokens/s = 78511 (34586 source, 43925 target) ; Learning rate = 0.000259 ; Loss = 1.435020\n",
      "2024-12-03 06:39:26.539000: I runner.py:310] Step = 116600 ; steps/s = 1.65, tokens/s = 78445 (34545 source, 43900 target) ; Learning rate = 0.000259 ; Loss = 1.435202\n",
      "2024-12-03 06:40:26.818000: I runner.py:310] Step = 116700 ; steps/s = 1.66, tokens/s = 77660 (34243 source, 43417 target) ; Learning rate = 0.000259 ; Loss = 1.430371\n",
      "2024-12-03 06:41:27.619000: I runner.py:310] Step = 116800 ; steps/s = 1.64, tokens/s = 78376 (34522 source, 43854 target) ; Learning rate = 0.000259 ; Loss = 1.432682\n",
      "2024-12-03 06:42:27.886000: I runner.py:310] Step = 116900 ; steps/s = 1.66, tokens/s = 77653 (34228 source, 43425 target) ; Learning rate = 0.000259 ; Loss = 1.429299\n",
      "2024-12-03 06:43:28.662000: I runner.py:310] Step = 117000 ; steps/s = 1.65, tokens/s = 78406 (34518 source, 43888 target) ; Learning rate = 0.000258 ; Loss = 1.433458\n",
      "2024-12-03 06:44:29.361000: I runner.py:310] Step = 117100 ; steps/s = 1.65, tokens/s = 78506 (34574 source, 43932 target) ; Learning rate = 0.000258 ; Loss = 1.432794\n",
      "2024-12-03 06:45:29.626000: I runner.py:310] Step = 117200 ; steps/s = 1.66, tokens/s = 77649 (34235 source, 43414 target) ; Learning rate = 0.000258 ; Loss = 1.431803\n",
      "2024-12-03 06:46:30.372000: I runner.py:310] Step = 117300 ; steps/s = 1.65, tokens/s = 78439 (34541 source, 43898 target) ; Learning rate = 0.000258 ; Loss = 1.431373\n",
      "2024-12-03 06:47:30.637000: I runner.py:310] Step = 117400 ; steps/s = 1.66, tokens/s = 77725 (34288 source, 43437 target) ; Learning rate = 0.000258 ; Loss = 1.430958\n",
      "2024-12-03 06:48:31.342000: I runner.py:310] Step = 117500 ; steps/s = 1.65, tokens/s = 78469 (34546 source, 43923 target) ; Learning rate = 0.000258 ; Loss = 1.432239\n",
      "2024-12-03 06:49:32.134000: I runner.py:310] Step = 117600 ; steps/s = 1.65, tokens/s = 78401 (34540 source, 43861 target) ; Learning rate = 0.000258 ; Loss = 1.434399\n",
      "2024-12-03 06:50:32.424000: I runner.py:310] Step = 117700 ; steps/s = 1.66, tokens/s = 77648 (34232 source, 43416 target) ; Learning rate = 0.000258 ; Loss = 1.431617\n",
      "2024-12-03 06:51:33.137000: I runner.py:310] Step = 117800 ; steps/s = 1.65, tokens/s = 78496 (34570 source, 43926 target) ; Learning rate = 0.000258 ; Loss = 1.432410\n",
      "2024-12-03 06:52:33.848000: I runner.py:310] Step = 117900 ; steps/s = 1.65, tokens/s = 78479 (34555 source, 43924 target) ; Learning rate = 0.000257 ; Loss = 1.432752\n",
      "2024-12-03 06:53:34.148000: I runner.py:310] Step = 118000 ; steps/s = 1.66, tokens/s = 77618 (34211 source, 43407 target) ; Learning rate = 0.000257 ; Loss = 1.432583\n",
      "2024-12-03 06:54:34.881000: I runner.py:310] Step = 118100 ; steps/s = 1.65, tokens/s = 78463 (34552 source, 43911 target) ; Learning rate = 0.000257 ; Loss = 1.432757\n",
      "2024-12-03 06:55:35.146000: I runner.py:310] Step = 118200 ; steps/s = 1.66, tokens/s = 77677 (34256 source, 43421 target) ; Learning rate = 0.000257 ; Loss = 1.430141\n",
      "2024-12-03 06:56:35.869000: I runner.py:310] Step = 118300 ; steps/s = 1.65, tokens/s = 78464 (34552 source, 43912 target) ; Learning rate = 0.000257 ; Loss = 1.432644\n",
      "2024-12-03 06:57:36.681000: I runner.py:310] Step = 118400 ; steps/s = 1.64, tokens/s = 78361 (34511 source, 43850 target) ; Learning rate = 0.000257 ; Loss = 1.433092\n",
      "2024-12-03 06:58:36.899000: I runner.py:310] Step = 118500 ; steps/s = 1.66, tokens/s = 77747 (34287 source, 43460 target) ; Learning rate = 0.000257 ; Loss = 1.440892\n",
      "2024-12-03 06:59:37.668000: I runner.py:310] Step = 118600 ; steps/s = 1.65, tokens/s = 78426 (34543 source, 43883 target) ; Learning rate = 0.000257 ; Loss = 1.430844\n",
      "2024-12-03 07:00:38.047000: I runner.py:310] Step = 118700 ; steps/s = 1.66, tokens/s = 77517 (34176 source, 43341 target) ; Learning rate = 0.000257 ; Loss = 1.430461\n",
      "2024-12-03 07:01:38.748000: I runner.py:310] Step = 118800 ; steps/s = 1.65, tokens/s = 78491 (34550 source, 43941 target) ; Learning rate = 0.000256 ; Loss = 1.430211\n",
      "2024-12-03 07:02:39.430000: I runner.py:310] Step = 118900 ; steps/s = 1.65, tokens/s = 78547 (34605 source, 43942 target) ; Learning rate = 0.000256 ; Loss = 1.432526\n",
      "2024-12-03 07:03:39.740000: I runner.py:310] Step = 119000 ; steps/s = 1.66, tokens/s = 77593 (34207 source, 43386 target) ; Learning rate = 0.000256 ; Loss = 1.429019\n",
      "2024-12-03 07:04:40.471000: I runner.py:310] Step = 119100 ; steps/s = 1.65, tokens/s = 78453 (34536 source, 43917 target) ; Learning rate = 0.000256 ; Loss = 1.433245\n",
      "2024-12-03 07:05:41.218000: I runner.py:310] Step = 119200 ; steps/s = 1.65, tokens/s = 78462 (34567 source, 43895 target) ; Learning rate = 0.000256 ; Loss = 1.433095\n",
      "2024-12-03 07:06:41.390000: I runner.py:310] Step = 119300 ; steps/s = 1.66, tokens/s = 77767 (34277 source, 43490 target) ; Learning rate = 0.000256 ; Loss = 1.433277\n",
      "2024-12-03 07:07:42.171000: I runner.py:310] Step = 119400 ; steps/s = 1.65, tokens/s = 78423 (34555 source, 43868 target) ; Learning rate = 0.000256 ; Loss = 1.432122\n",
      "2024-12-03 07:08:42.403000: I runner.py:310] Step = 119500 ; steps/s = 1.66, tokens/s = 77710 (34255 source, 43455 target) ; Learning rate = 0.000256 ; Loss = 1.429498\n",
      "2024-12-03 07:09:43.161000: I runner.py:310] Step = 119600 ; steps/s = 1.65, tokens/s = 78416 (34535 source, 43881 target) ; Learning rate = 0.000256 ; Loss = 1.431643\n",
      "2024-12-03 07:10:43.883000: I runner.py:310] Step = 119700 ; steps/s = 1.65, tokens/s = 78499 (34576 source, 43923 target) ; Learning rate = 0.000255 ; Loss = 1.430665\n",
      "2024-12-03 07:11:44.180000: I runner.py:310] Step = 119800 ; steps/s = 1.66, tokens/s = 77615 (34197 source, 43418 target) ; Learning rate = 0.000255 ; Loss = 1.436984\n",
      "2024-12-03 07:12:44.963000: I runner.py:310] Step = 119900 ; steps/s = 1.65, tokens/s = 78416 (34542 source, 43874 target) ; Learning rate = 0.000255 ; Loss = 1.433766\n",
      "2024-12-03 07:13:45.213000: I runner.py:310] Step = 120000 ; steps/s = 1.66, tokens/s = 77686 (34258 source, 43428 target) ; Learning rate = 0.000255 ; Loss = 1.426932\n",
      "2024-12-03 07:13:47.395000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-120000\n",
      "2024-12-03 07:13:47.395000: I training.py:192] Running evaluation for step 120000\n",
      "2024-12-03 07:14:35.585000: I training.py:192] Evaluation result for step 120000: loss = 0.852234 ; perplexity = 2.344880\n",
      "2024-12-03 07:15:36.285000: I runner.py:310] Step = 120100 ; steps/s = 1.65, tokens/s = 78515 (34567 source, 43948 target) ; Learning rate = 0.000255 ; Loss = 1.432186\n",
      "2024-12-03 07:16:37.029000: I runner.py:310] Step = 120200 ; steps/s = 1.65, tokens/s = 78474 (34572 source, 43902 target) ; Learning rate = 0.000255 ; Loss = 1.438661\n",
      "2024-12-03 07:17:37.299000: I runner.py:310] Step = 120300 ; steps/s = 1.66, tokens/s = 77624 (34207 source, 43417 target) ; Learning rate = 0.000255 ; Loss = 1.430211\n",
      "2024-12-03 07:18:38.097000: I runner.py:310] Step = 120400 ; steps/s = 1.64, tokens/s = 78407 (34547 source, 43860 target) ; Learning rate = 0.000255 ; Loss = 1.431195\n",
      "2024-12-03 07:19:38.889000: I runner.py:310] Step = 120500 ; steps/s = 1.65, tokens/s = 78385 (34512 source, 43873 target) ; Learning rate = 0.000255 ; Loss = 1.433011\n",
      "2024-12-03 07:20:39.220000: I runner.py:310] Step = 120600 ; steps/s = 1.66, tokens/s = 77598 (34219 source, 43379 target) ; Learning rate = 0.000255 ; Loss = 1.430924\n",
      "2024-12-03 07:21:39.924000: I runner.py:310] Step = 120700 ; steps/s = 1.65, tokens/s = 78517 (34580 source, 43937 target) ; Learning rate = 0.000254 ; Loss = 1.429850\n",
      "2024-12-03 07:22:40.208000: I runner.py:310] Step = 120800 ; steps/s = 1.66, tokens/s = 77613 (34219 source, 43394 target) ; Learning rate = 0.000254 ; Loss = 1.429381\n",
      "2024-12-03 07:23:40.978000: I runner.py:310] Step = 120900 ; steps/s = 1.65, tokens/s = 78406 (34518 source, 43888 target) ; Learning rate = 0.000254 ; Loss = 1.428474\n",
      "2024-12-03 07:24:41.685000: I runner.py:310] Step = 121000 ; steps/s = 1.65, tokens/s = 78506 (34578 source, 43928 target) ; Learning rate = 0.000254 ; Loss = 1.431651\n",
      "2024-12-03 07:25:41.947000: I runner.py:310] Step = 121100 ; steps/s = 1.66, tokens/s = 77644 (34215 source, 43429 target) ; Learning rate = 0.000254 ; Loss = 1.428263\n",
      "2024-12-03 07:26:42.670000: I runner.py:310] Step = 121200 ; steps/s = 1.65, tokens/s = 78493 (34582 source, 43911 target) ; Learning rate = 0.000254 ; Loss = 1.433046\n",
      "2024-12-03 07:27:43.013000: I runner.py:310] Step = 121300 ; steps/s = 1.66, tokens/s = 77586 (34212 source, 43374 target) ; Learning rate = 0.000254 ; Loss = 1.430744\n",
      "2024-12-03 07:28:43.758000: I runner.py:310] Step = 121400 ; steps/s = 1.65, tokens/s = 78454 (34548 source, 43906 target) ; Learning rate = 0.000254 ; Loss = 1.431284\n",
      "2024-12-03 07:29:44.509000: I runner.py:310] Step = 121500 ; steps/s = 1.65, tokens/s = 78445 (34550 source, 43895 target) ; Learning rate = 0.000254 ; Loss = 1.440755\n",
      "2024-12-03 07:30:44.744000: I runner.py:310] Step = 121600 ; steps/s = 1.66, tokens/s = 77673 (34229 source, 43444 target) ; Learning rate = 0.000253 ; Loss = 1.426528\n",
      "2024-12-03 07:31:45.430000: I runner.py:310] Step = 121700 ; steps/s = 1.65, tokens/s = 78521 (34579 source, 43942 target) ; Learning rate = 0.000253 ; Loss = 1.432254\n",
      "2024-12-03 07:32:46.170000: I runner.py:310] Step = 121800 ; steps/s = 1.65, tokens/s = 78477 (34571 source, 43906 target) ; Learning rate = 0.000253 ; Loss = 1.430836\n",
      "2024-12-03 07:33:46.432000: I runner.py:310] Step = 121900 ; steps/s = 1.66, tokens/s = 77680 (34261 source, 43419 target) ; Learning rate = 0.000253 ; Loss = 1.428333\n",
      "2024-12-03 07:34:47.227000: I runner.py:310] Step = 122000 ; steps/s = 1.65, tokens/s = 78364 (34504 source, 43860 target) ; Learning rate = 0.000253 ; Loss = 1.433606\n",
      "2024-12-03 07:35:47.525000: I runner.py:310] Step = 122100 ; steps/s = 1.66, tokens/s = 77644 (34230 source, 43414 target) ; Learning rate = 0.000253 ; Loss = 1.425933\n",
      "2024-12-03 07:36:48.248000: I runner.py:310] Step = 122200 ; steps/s = 1.65, tokens/s = 78489 (34575 source, 43914 target) ; Learning rate = 0.000253 ; Loss = 1.429839\n",
      "2024-12-03 07:37:49.010000: I runner.py:310] Step = 122300 ; steps/s = 1.65, tokens/s = 78414 (34529 source, 43885 target) ; Learning rate = 0.000253 ; Loss = 1.432954\n",
      "2024-12-03 07:38:49.312000: I runner.py:310] Step = 122400 ; steps/s = 1.66, tokens/s = 77603 (34202 source, 43401 target) ; Learning rate = 0.000253 ; Loss = 1.430086\n",
      "2024-12-03 07:39:50.027000: I runner.py:310] Step = 122500 ; steps/s = 1.65, tokens/s = 78498 (34584 source, 43914 target) ; Learning rate = 0.000253 ; Loss = 1.434531\n",
      "2024-12-03 07:40:50.321000: I runner.py:310] Step = 122600 ; steps/s = 1.66, tokens/s = 77618 (34208 source, 43410 target) ; Learning rate = 0.000252 ; Loss = 1.431050\n",
      "2024-12-03 07:41:51.090000: I runner.py:310] Step = 122700 ; steps/s = 1.65, tokens/s = 78387 (34511 source, 43876 target) ; Learning rate = 0.000252 ; Loss = 1.430891\n",
      "2024-12-03 07:42:51.838000: I runner.py:310] Step = 122800 ; steps/s = 1.65, tokens/s = 78478 (34574 source, 43904 target) ; Learning rate = 0.000252 ; Loss = 1.433508\n",
      "2024-12-03 07:43:52.151000: I runner.py:310] Step = 122900 ; steps/s = 1.66, tokens/s = 77609 (34220 source, 43389 target) ; Learning rate = 0.000252 ; Loss = 1.433863\n",
      "2024-12-03 07:44:52.854000: I runner.py:310] Step = 123000 ; steps/s = 1.65, tokens/s = 78525 (34589 source, 43936 target) ; Learning rate = 0.000252 ; Loss = 1.433275\n",
      "2024-12-03 07:45:53.603000: I runner.py:310] Step = 123100 ; steps/s = 1.65, tokens/s = 78421 (34527 source, 43894 target) ; Learning rate = 0.000252 ; Loss = 1.430585\n",
      "2024-12-03 07:46:53.889000: I runner.py:310] Step = 123200 ; steps/s = 1.66, tokens/s = 77607 (34205 source, 43402 target) ; Learning rate = 0.000252 ; Loss = 1.429876\n",
      "2024-12-03 07:47:54.641000: I runner.py:310] Step = 123300 ; steps/s = 1.65, tokens/s = 78455 (34566 source, 43889 target) ; Learning rate = 0.000252 ; Loss = 1.436205\n",
      "2024-12-03 07:48:54.953000: I runner.py:310] Step = 123400 ; steps/s = 1.66, tokens/s = 77625 (34222 source, 43403 target) ; Learning rate = 0.000252 ; Loss = 1.425793\n",
      "2024-12-03 07:49:55.744000: I runner.py:310] Step = 123500 ; steps/s = 1.65, tokens/s = 78384 (34513 source, 43871 target) ; Learning rate = 0.000252 ; Loss = 1.432070\n",
      "2024-12-03 07:50:56.477000: I runner.py:310] Step = 123600 ; steps/s = 1.65, tokens/s = 78459 (34552 source, 43907 target) ; Learning rate = 0.000251 ; Loss = 1.432595\n",
      "2024-12-03 07:51:56.799000: I runner.py:310] Step = 123700 ; steps/s = 1.66, tokens/s = 77596 (34211 source, 43385 target) ; Learning rate = 0.000251 ; Loss = 1.429806\n",
      "2024-12-03 07:52:57.548000: I runner.py:310] Step = 123800 ; steps/s = 1.65, tokens/s = 78455 (34562 source, 43893 target) ; Learning rate = 0.000251 ; Loss = 1.428493\n",
      "2024-12-03 07:53:57.841000: I runner.py:310] Step = 123900 ; steps/s = 1.66, tokens/s = 77638 (34231 source, 43407 target) ; Learning rate = 0.000251 ; Loss = 1.427521\n",
      "2024-12-03 07:54:58.582000: I runner.py:310] Step = 124000 ; steps/s = 1.65, tokens/s = 78468 (34557 source, 43911 target) ; Learning rate = 0.000251 ; Loss = 1.432509\n",
      "2024-12-03 07:55:59.367000: I runner.py:310] Step = 124100 ; steps/s = 1.65, tokens/s = 78390 (34522 source, 43868 target) ; Learning rate = 0.000251 ; Loss = 1.431651\n",
      "2024-12-03 07:56:59.708000: I runner.py:310] Step = 124200 ; steps/s = 1.66, tokens/s = 77547 (34181 source, 43366 target) ; Learning rate = 0.000251 ; Loss = 1.424520\n",
      "2024-12-03 07:58:00.439000: I runner.py:310] Step = 124300 ; steps/s = 1.65, tokens/s = 78459 (34560 source, 43899 target) ; Learning rate = 0.000251 ; Loss = 1.429069\n",
      "2024-12-03 07:59:01.200000: I runner.py:310] Step = 124400 ; steps/s = 1.65, tokens/s = 78367 (34507 source, 43860 target) ; Learning rate = 0.000251 ; Loss = 1.439591\n",
      "2024-12-03 08:00:01.485000: I runner.py:310] Step = 124500 ; steps/s = 1.66, tokens/s = 77701 (34252 source, 43449 target) ; Learning rate = 0.000251 ; Loss = 1.431148\n",
      "2024-12-03 08:01:02.312000: I runner.py:310] Step = 124600 ; steps/s = 1.64, tokens/s = 78346 (34512 source, 43834 target) ; Learning rate = 0.000250 ; Loss = 1.429891\n",
      "2024-12-03 08:02:02.565000: I runner.py:310] Step = 124700 ; steps/s = 1.66, tokens/s = 77690 (34242 source, 43448 target) ; Learning rate = 0.000250 ; Loss = 1.429835\n",
      "2024-12-03 08:03:03.400000: I runner.py:310] Step = 124800 ; steps/s = 1.64, tokens/s = 78311 (34481 source, 43830 target) ; Learning rate = 0.000250 ; Loss = 1.429961\n",
      "2024-12-03 08:04:04.140000: I runner.py:310] Step = 124900 ; steps/s = 1.65, tokens/s = 78478 (34568 source, 43910 target) ; Learning rate = 0.000250 ; Loss = 1.427733\n",
      "2024-12-03 08:05:04.529000: I runner.py:310] Step = 125000 ; steps/s = 1.66, tokens/s = 77533 (34197 source, 43336 target) ; Learning rate = 0.000250 ; Loss = 1.431095\n",
      "2024-12-03 08:05:04.530000: I training.py:192] Running evaluation for step 125000\n",
      "2024-12-03 08:05:52.535000: I training.py:192] Evaluation result for step 125000: loss = 0.853206 ; perplexity = 2.347159\n",
      "2024-12-03 08:06:53.152000: I runner.py:310] Step = 125100 ; steps/s = 1.65, tokens/s = 78614 (34617 source, 43997 target) ; Learning rate = 0.000250 ; Loss = 1.427551\n",
      "2024-12-03 08:07:53.387000: I runner.py:310] Step = 125200 ; steps/s = 1.66, tokens/s = 77719 (34266 source, 43453 target) ; Learning rate = 0.000250 ; Loss = 1.431544\n",
      "2024-12-03 08:08:54.107000: I runner.py:310] Step = 125300 ; steps/s = 1.65, tokens/s = 78446 (34529 source, 43917 target) ; Learning rate = 0.000250 ; Loss = 1.432801\n",
      "2024-12-03 08:09:54.947000: I runner.py:310] Step = 125400 ; steps/s = 1.64, tokens/s = 78343 (34517 source, 43826 target) ; Learning rate = 0.000250 ; Loss = 1.428395\n",
      "2024-12-03 08:10:55.209000: I runner.py:310] Step = 125500 ; steps/s = 1.66, tokens/s = 77658 (34230 source, 43428 target) ; Learning rate = 0.000250 ; Loss = 1.427294\n",
      "2024-12-03 08:11:56.014000: I runner.py:310] Step = 125600 ; steps/s = 1.64, tokens/s = 78359 (34509 source, 43850 target) ; Learning rate = 0.000249 ; Loss = 1.431565\n",
      "2024-12-03 08:12:56.646000: I runner.py:310] Step = 125700 ; steps/s = 1.65, tokens/s = 78233 (34492 source, 43741 target) ; Learning rate = 0.000249 ; Loss = 1.446280\n",
      "2024-12-03 08:13:57.088000: I runner.py:310] Step = 125800 ; steps/s = 1.65, tokens/s = 77847 (34291 source, 43556 target) ; Learning rate = 0.000249 ; Loss = 1.431639\n",
      "2024-12-03 08:14:57.847000: I runner.py:310] Step = 125900 ; steps/s = 1.65, tokens/s = 78426 (34542 source, 43884 target) ; Learning rate = 0.000249 ; Loss = 1.429870\n",
      "2024-12-03 08:15:58.190000: I runner.py:310] Step = 126000 ; steps/s = 1.66, tokens/s = 77563 (34199 source, 43364 target) ; Learning rate = 0.000249 ; Loss = 1.432519\n",
      "2024-12-03 08:16:58.923000: I runner.py:310] Step = 126100 ; steps/s = 1.65, tokens/s = 78435 (34524 source, 43911 target) ; Learning rate = 0.000249 ; Loss = 1.426236\n",
      "2024-12-03 08:17:59.649000: I runner.py:310] Step = 126200 ; steps/s = 1.65, tokens/s = 78486 (34565 source, 43921 target) ; Learning rate = 0.000249 ; Loss = 1.428367\n",
      "2024-12-03 08:18:59.932000: I runner.py:310] Step = 126300 ; steps/s = 1.66, tokens/s = 77649 (34241 source, 43408 target) ; Learning rate = 0.000249 ; Loss = 1.433329\n",
      "2024-12-03 08:20:00.626000: I runner.py:310] Step = 126400 ; steps/s = 1.65, tokens/s = 78510 (34569 source, 43941 target) ; Learning rate = 0.000249 ; Loss = 1.430178\n",
      "2024-12-03 08:21:00.952000: I runner.py:310] Step = 126500 ; steps/s = 1.66, tokens/s = 77621 (34239 source, 43382 target) ; Learning rate = 0.000249 ; Loss = 1.425143\n",
      "2024-12-03 08:22:01.698000: I runner.py:310] Step = 126600 ; steps/s = 1.65, tokens/s = 78446 (34547 source, 43899 target) ; Learning rate = 0.000248 ; Loss = 1.431396\n",
      "2024-12-03 08:23:02.433000: I runner.py:310] Step = 126700 ; steps/s = 1.65, tokens/s = 78454 (34539 source, 43915 target) ; Learning rate = 0.000248 ; Loss = 1.430521\n",
      "2024-12-03 08:24:02.699000: I runner.py:310] Step = 126800 ; steps/s = 1.66, tokens/s = 77672 (34253 source, 43419 target) ; Learning rate = 0.000248 ; Loss = 1.430055\n",
      "2024-12-03 08:25:03.498000: I runner.py:310] Step = 126900 ; steps/s = 1.64, tokens/s = 78351 (34498 source, 43853 target) ; Learning rate = 0.000248 ; Loss = 1.431587\n",
      "2024-12-03 08:26:03.857000: I runner.py:310] Step = 127000 ; steps/s = 1.66, tokens/s = 77805 (34323 source, 43482 target) ; Learning rate = 0.000248 ; Loss = 1.424423\n",
      "2024-12-03 08:27:04.616000: I runner.py:310] Step = 127100 ; steps/s = 1.65, tokens/s = 78208 (34438 source, 43770 target) ; Learning rate = 0.000248 ; Loss = 1.425558\n",
      "2024-12-03 08:28:05.409000: I runner.py:310] Step = 127200 ; steps/s = 1.65, tokens/s = 78384 (34519 source, 43865 target) ; Learning rate = 0.000248 ; Loss = 1.431289\n",
      "2024-12-03 08:29:05.745000: I runner.py:310] Step = 127300 ; steps/s = 1.66, tokens/s = 77565 (34183 source, 43382 target) ; Learning rate = 0.000248 ; Loss = 1.428530\n",
      "2024-12-03 08:30:06.513000: I runner.py:310] Step = 127400 ; steps/s = 1.65, tokens/s = 78395 (34518 source, 43877 target) ; Learning rate = 0.000248 ; Loss = 1.429068\n",
      "2024-12-03 08:31:07.263000: I runner.py:310] Step = 127500 ; steps/s = 1.65, tokens/s = 78469 (34569 source, 43900 target) ; Learning rate = 0.000248 ; Loss = 1.429753\n",
      "2024-12-03 08:32:07.590000: I runner.py:310] Step = 127600 ; steps/s = 1.66, tokens/s = 77576 (34199 source, 43377 target) ; Learning rate = 0.000247 ; Loss = 1.425911\n",
      "2024-12-03 08:33:08.351000: I runner.py:310] Step = 127700 ; steps/s = 1.65, tokens/s = 78422 (34525 source, 43897 target) ; Learning rate = 0.000247 ; Loss = 1.428533\n",
      "2024-12-03 08:34:08.708000: I runner.py:310] Step = 127800 ; steps/s = 1.66, tokens/s = 77579 (34219 source, 43360 target) ; Learning rate = 0.000247 ; Loss = 1.426718\n",
      "2024-12-03 08:35:09.429000: I runner.py:310] Step = 127900 ; steps/s = 1.65, tokens/s = 78482 (34548 source, 43934 target) ; Learning rate = 0.000247 ; Loss = 1.431662\n",
      "2024-12-03 08:36:10.205000: I runner.py:310] Step = 128000 ; steps/s = 1.65, tokens/s = 78404 (34544 source, 43860 target) ; Learning rate = 0.000247 ; Loss = 1.431273\n",
      "2024-12-03 08:37:10.439000: I runner.py:310] Step = 128100 ; steps/s = 1.66, tokens/s = 77701 (34245 source, 43456 target) ; Learning rate = 0.000247 ; Loss = 1.436256\n",
      "2024-12-03 08:38:11.183000: I runner.py:310] Step = 128200 ; steps/s = 1.65, tokens/s = 78446 (34543 source, 43903 target) ; Learning rate = 0.000247 ; Loss = 1.431051\n",
      "2024-12-03 08:39:11.502000: I runner.py:310] Step = 128300 ; steps/s = 1.66, tokens/s = 77632 (34258 source, 43374 target) ; Learning rate = 0.000247 ; Loss = 1.427790\n",
      "2024-12-03 08:40:12.243000: I runner.py:310] Step = 128400 ; steps/s = 1.65, tokens/s = 78422 (34514 source, 43908 target) ; Learning rate = 0.000247 ; Loss = 1.428813\n",
      "2024-12-03 08:41:12.954000: I runner.py:310] Step = 128500 ; steps/s = 1.65, tokens/s = 78484 (34562 source, 43922 target) ; Learning rate = 0.000247 ; Loss = 1.434610\n",
      "2024-12-03 08:42:13.258000: I runner.py:310] Step = 128600 ; steps/s = 1.66, tokens/s = 77630 (34236 source, 43394 target) ; Learning rate = 0.000246 ; Loss = 1.426548\n",
      "2024-12-03 08:43:13.984000: I runner.py:310] Step = 128700 ; steps/s = 1.65, tokens/s = 78467 (34550 source, 43917 target) ; Learning rate = 0.000246 ; Loss = 1.430338\n",
      "2024-12-03 08:44:14.660000: I runner.py:310] Step = 128800 ; steps/s = 1.65, tokens/s = 78534 (34586 source, 43948 target) ; Learning rate = 0.000246 ; Loss = 1.433578\n",
      "2024-12-03 08:45:14.983000: I runner.py:310] Step = 128900 ; steps/s = 1.66, tokens/s = 77598 (34206 source, 43392 target) ; Learning rate = 0.000246 ; Loss = 1.427115\n",
      "2024-12-03 08:46:15.674000: I runner.py:310] Step = 129000 ; steps/s = 1.65, tokens/s = 78519 (34588 source, 43931 target) ; Learning rate = 0.000246 ; Loss = 1.434659\n",
      "2024-12-03 08:47:15.986000: I runner.py:310] Step = 129100 ; steps/s = 1.66, tokens/s = 77618 (34223 source, 43395 target) ; Learning rate = 0.000246 ; Loss = 1.429454\n",
      "2024-12-03 08:48:16.746000: I runner.py:310] Step = 129200 ; steps/s = 1.65, tokens/s = 78422 (34530 source, 43892 target) ; Learning rate = 0.000246 ; Loss = 1.432474\n",
      "2024-12-03 08:49:17.426000: I runner.py:310] Step = 129300 ; steps/s = 1.65, tokens/s = 78536 (34592 source, 43944 target) ; Learning rate = 0.000246 ; Loss = 1.430876\n",
      "2024-12-03 08:50:17.750000: I runner.py:310] Step = 129400 ; steps/s = 1.66, tokens/s = 77611 (34231 source, 43380 target) ; Learning rate = 0.000246 ; Loss = 1.427017\n",
      "2024-12-03 08:51:18.501000: I runner.py:310] Step = 129500 ; steps/s = 1.65, tokens/s = 78428 (34529 source, 43899 target) ; Learning rate = 0.000246 ; Loss = 1.431349\n",
      "2024-12-03 08:52:18.800000: I runner.py:310] Step = 129600 ; steps/s = 1.66, tokens/s = 77638 (34240 source, 43398 target) ; Learning rate = 0.000246 ; Loss = 1.424047\n",
      "2024-12-03 08:53:19.540000: I runner.py:310] Step = 129700 ; steps/s = 1.65, tokens/s = 78427 (34518 source, 43909 target) ; Learning rate = 0.000245 ; Loss = 1.427593\n",
      "2024-12-03 08:54:20.317000: I runner.py:310] Step = 129800 ; steps/s = 1.65, tokens/s = 78412 (34541 source, 43871 target) ; Learning rate = 0.000245 ; Loss = 1.430999\n",
      "2024-12-03 08:55:20.664000: I runner.py:310] Step = 129900 ; steps/s = 1.66, tokens/s = 77564 (34200 source, 43364 target) ; Learning rate = 0.000245 ; Loss = 1.425998\n",
      "2024-12-03 08:56:21.455000: I runner.py:310] Step = 130000 ; steps/s = 1.65, tokens/s = 78399 (34530 source, 43869 target) ; Learning rate = 0.000245 ; Loss = 1.434056\n",
      "2024-12-03 08:56:23.723000: I training.py:176] Saved checkpoint KK-TR-EN-Shared-vocab-2/ckpt-130000\n",
      "2024-12-03 08:56:23.723000: I training.py:192] Running evaluation for step 130000\n",
      "2024-12-03 08:57:11.229000: I training.py:192] Evaluation result for step 130000: loss = 0.858182 ; perplexity = 2.358869\n",
      "2024-12-03 08:58:11.920000: I runner.py:310] Step = 130100 ; steps/s = 1.65, tokens/s = 78539 (34595 source, 43944 target) ; Learning rate = 0.000245 ; Loss = 1.431622\n",
      "2024-12-03 08:59:12.196000: I runner.py:310] Step = 130200 ; steps/s = 1.66, tokens/s = 77649 (34229 source, 43420 target) ; Learning rate = 0.000245 ; Loss = 1.428674\n",
      "2024-12-03 09:00:12.905000: I runner.py:310] Step = 130300 ; steps/s = 1.65, tokens/s = 78493 (34563 source, 43930 target) ; Learning rate = 0.000245 ; Loss = 1.427584\n",
      "2024-12-03 09:01:13.219000: I runner.py:310] Step = 130400 ; steps/s = 1.66, tokens/s = 77604 (34212 source, 43392 target) ; Learning rate = 0.000245 ; Loss = 1.428771\n",
      "2024-12-03 09:02:13.910000: I runner.py:310] Step = 130500 ; steps/s = 1.65, tokens/s = 78497 (34567 source, 43930 target) ; Learning rate = 0.000245 ; Loss = 1.429164\n",
      "2024-12-03 09:03:14.723000: I runner.py:310] Step = 130600 ; steps/s = 1.64, tokens/s = 78368 (34516 source, 43852 target) ; Learning rate = 0.000245 ; Loss = 1.428507\n",
      "2024-12-03 09:04:14.945000: I runner.py:310] Step = 130700 ; steps/s = 1.66, tokens/s = 77717 (34252 source, 43465 target) ; Learning rate = 0.000244 ; Loss = 1.429779\n",
      "2024-12-03 09:05:15.704000: I runner.py:310] Step = 130800 ; steps/s = 1.65, tokens/s = 78418 (34526 source, 43892 target) ; Learning rate = 0.000244 ; Loss = 1.428708\n",
      "2024-12-03 09:06:15.955000: I runner.py:310] Step = 130900 ; steps/s = 1.66, tokens/s = 77718 (34293 source, 43425 target) ; Learning rate = 0.000244 ; Loss = 1.429769\n",
      "2024-12-03 09:07:16.668000: I runner.py:310] Step = 131000 ; steps/s = 1.65, tokens/s = 78476 (34551 source, 43925 target) ; Learning rate = 0.000244 ; Loss = 1.427422\n",
      "2024-12-03 09:08:17.347000: I runner.py:310] Step = 131100 ; steps/s = 1.65, tokens/s = 78530 (34580 source, 43950 target) ; Learning rate = 0.000244 ; Loss = 1.426210\n",
      "2024-12-03 09:09:17.607000: I runner.py:310] Step = 131200 ; steps/s = 1.66, tokens/s = 77660 (34232 source, 43428 target) ; Learning rate = 0.000244 ; Loss = 1.431540\n",
      "2024-12-03 09:10:18.329000: I runner.py:310] Step = 131300 ; steps/s = 1.65, tokens/s = 78487 (34576 source, 43911 target) ; Learning rate = 0.000244 ; Loss = 1.425818\n",
      "2024-12-03 09:11:19.049000: I runner.py:310] Step = 131400 ; steps/s = 1.65, tokens/s = 78492 (34574 source, 43918 target) ; Learning rate = 0.000244 ; Loss = 1.431839\n",
      "2024-12-03 09:12:19.298000: I runner.py:310] Step = 131500 ; steps/s = 1.66, tokens/s = 77706 (34258 source, 43448 target) ; Learning rate = 0.000244 ; Loss = 1.424934\n",
      "2024-12-03 09:13:20.063000: I runner.py:310] Step = 131600 ; steps/s = 1.65, tokens/s = 78408 (34525 source, 43883 target) ; Learning rate = 0.000244 ; Loss = 1.432654\n",
      "2024-12-03 09:14:20.394000: I runner.py:310] Step = 131700 ; steps/s = 1.66, tokens/s = 77592 (34215 source, 43377 target) ; Learning rate = 0.000244 ; Loss = 1.424315\n",
      "2024-12-03 09:15:21.152000: I runner.py:310] Step = 131800 ; steps/s = 1.65, tokens/s = 78445 (34546 source, 43899 target) ; Learning rate = 0.000243 ; Loss = 1.427654\n",
      "2024-12-03 09:16:21.866000: I runner.py:310] Step = 131900 ; steps/s = 1.65, tokens/s = 78491 (34573 source, 43918 target) ; Learning rate = 0.000243 ; Loss = 1.428115\n",
      "2024-12-03 09:17:22.086000: I runner.py:310] Step = 132000 ; steps/s = 1.66, tokens/s = 77704 (34249 source, 43455 target) ; Learning rate = 0.000243 ; Loss = 1.427974\n",
      "2024-12-03 09:18:22.811000: I runner.py:310] Step = 132100 ; steps/s = 1.65, tokens/s = 78459 (34545 source, 43914 target) ; Learning rate = 0.000243 ; Loss = 1.435031\n",
      "2024-12-03 09:19:23.127000: I runner.py:310] Step = 132200 ; steps/s = 1.66, tokens/s = 77634 (34246 source, 43388 target) ; Learning rate = 0.000243 ; Loss = 1.425114\n",
      "2024-12-03 09:20:23.823000: I runner.py:310] Step = 132300 ; steps/s = 1.65, tokens/s = 78500 (34564 source, 43936 target) ; Learning rate = 0.000243 ; Loss = 1.430774\n",
      "2024-12-03 09:21:24.616000: I runner.py:310] Step = 132400 ; steps/s = 1.65, tokens/s = 78396 (34528 source, 43868 target) ; Learning rate = 0.000243 ; Loss = 1.425720\n",
      "2024-12-03 09:22:24.942000: I runner.py:310] Step = 132500 ; steps/s = 1.66, tokens/s = 77572 (34186 source, 43386 target) ; Learning rate = 0.000243 ; Loss = 1.431653\n",
      "2024-12-03 09:23:25.678000: I runner.py:310] Step = 132600 ; steps/s = 1.65, tokens/s = 78470 (34563 source, 43907 target) ; Learning rate = 0.000243 ; Loss = 1.430403\n",
      "2024-12-03 09:24:26.445000: I runner.py:310] Step = 132700 ; steps/s = 1.65, tokens/s = 78433 (34546 source, 43887 target) ; Learning rate = 0.000243 ; Loss = 1.434099\n",
      "2024-12-03 09:25:26.720000: I runner.py:310] Step = 132800 ; steps/s = 1.66, tokens/s = 77654 (34231 source, 43423 target) ; Learning rate = 0.000243 ; Loss = 1.427012\n",
      "2024-12-03 09:26:27.429000: I runner.py:310] Step = 132900 ; steps/s = 1.65, tokens/s = 78485 (34568 source, 43917 target) ; Learning rate = 0.000242 ; Loss = 1.426853\n",
      "2024-12-03 09:27:27.654000: I runner.py:310] Step = 133000 ; steps/s = 1.66, tokens/s = 77724 (34271 source, 43453 target) ; Learning rate = 0.000242 ; Loss = 1.425212\n",
      "2024-12-03 09:28:28.381000: I runner.py:310] Step = 133100 ; steps/s = 1.65, tokens/s = 78456 (34541 source, 43915 target) ; Learning rate = 0.000242 ; Loss = 1.424905\n",
      "2024-12-03 09:29:29.178000: I runner.py:310] Step = 133200 ; steps/s = 1.65, tokens/s = 78392 (34531 source, 43861 target) ; Learning rate = 0.000242 ; Loss = 1.433900\n",
      "2024-12-03 09:30:29.473000: I runner.py:310] Step = 133300 ; steps/s = 1.66, tokens/s = 77629 (34226 source, 43403 target) ; Learning rate = 0.000242 ; Loss = 1.430502\n",
      "2024-12-03 09:31:30.243000: I runner.py:310] Step = 133400 ; steps/s = 1.65, tokens/s = 78396 (34518 source, 43878 target) ; Learning rate = 0.000242 ; Loss = 1.431114\n",
      "2024-12-03 09:32:30.551000: I runner.py:310] Step = 133500 ; steps/s = 1.66, tokens/s = 77654 (34256 source, 43398 target) ; Learning rate = 0.000242 ; Loss = 1.422250\n",
      "2024-12-03 09:33:31.257000: I runner.py:310] Step = 133600 ; steps/s = 1.65, tokens/s = 78494 (34560 source, 43934 target) ; Learning rate = 0.000242 ; Loss = 1.427098\n",
      "2024-12-03 09:34:31.966000: I runner.py:310] Step = 133700 ; steps/s = 1.65, tokens/s = 78484 (34564 source, 43920 target) ; Learning rate = 0.000242 ; Loss = 1.430273\n",
      "2024-12-03 09:35:32.264000: I runner.py:310] Step = 133800 ; steps/s = 1.66, tokens/s = 77610 (34203 source, 43407 target) ; Learning rate = 0.000242 ; Loss = 1.429443\n",
      "2024-12-03 09:36:33.050000: I runner.py:310] Step = 133900 ; steps/s = 1.65, tokens/s = 78402 (34533 source, 43869 target) ; Learning rate = 0.000242 ; Loss = 1.432258\n",
      "2024-12-03 09:37:33.867000: I runner.py:310] Step = 134000 ; steps/s = 1.64, tokens/s = 78363 (34522 source, 43841 target) ; Learning rate = 0.000241 ; Loss = 1.429207\n",
      "2024-12-03 09:38:34.189000: I runner.py:310] Step = 134100 ; steps/s = 1.66, tokens/s = 77616 (34230 source, 43386 target) ; Learning rate = 0.000241 ; Loss = 1.424069\n",
      "2024-12-03 09:39:34.893000: I runner.py:310] Step = 134200 ; steps/s = 1.65, tokens/s = 78502 (34572 source, 43930 target) ; Learning rate = 0.000241 ; Loss = 1.427141\n",
      "2024-12-03 09:40:35.154000: I runner.py:310] Step = 134300 ; steps/s = 1.66, tokens/s = 77644 (34213 source, 43431 target) ; Learning rate = 0.000241 ; Loss = 1.426557\n",
      "2024-12-03 09:41:35.947000: I runner.py:310] Step = 134400 ; steps/s = 1.65, tokens/s = 78401 (34538 source, 43863 target) ; Learning rate = 0.000241 ; Loss = 1.432843\n",
      "2024-12-03 09:42:36.734000: I runner.py:310] Step = 134500 ; steps/s = 1.65, tokens/s = 78383 (34512 source, 43871 target) ; Learning rate = 0.000241 ; Loss = 1.432612\n",
      "2024-12-03 09:43:37.057000: I runner.py:310] Step = 134600 ; steps/s = 1.66, tokens/s = 77596 (34210 source, 43386 target) ; Learning rate = 0.000241 ; Loss = 1.426793\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Kk-En -> Tr-En (Tatoeba)\n",
    "!onmt-main --model kk-tr-en-shared.py --config kk-tr-en-2.yml --auto_config --checkpoint_path KK-TR-EN-Shared-vocab/ckpt-100000 train --with_eval --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "418ee93e-cec4-43d7-b232-0a370dcd0c5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 09:44:54.066331: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 09:44:54.865685: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-03 09:44:54.865759: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-03 09:44:54.865781: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-03 09:44:55.818000: I main.py:308] Loading model description from KK-TR-EN-Shared-vocab/model_description.py\n",
      "2024-12-03 09:44:56.018000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-03 09:44:56.018000: I main.py:315] Using model:\n",
      "(model): MyCustomTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): WordEmbedder()\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): WordEmbedder()\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-03 09:44:56.023000: I main.py:340] Using parameters:\n",
      "data:\n",
      "  eval_features_file: KK_tokens_valid_shared\n",
      "  eval_labels_file: KK_valid_target_tokens_shared.txt\n",
      "  source_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file: KK_tokens_train_shared\n",
      "  train_labels_file: KK_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: KK-TR-EN-Shared-vocab\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 250000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-03 09:44:56.210526: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 09:44:56.806119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-03 09:44:56.958000: I inputter.py:316] Initialized source input layer:\n",
      "2024-12-03 09:44:56.958000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-03 09:44:56.958000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-03 09:44:57.029000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-03 09:44:57.029000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-03 09:44:57.029000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-03 09:44:57.050000: I runner.py:462] Restored checkpoint KK-TR-EN-Shared-vocab-2/ckpt-100000\n",
      "2024-12-03 09:44:57.088000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-03 09:44:57.599662: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-03 09:44:57.710000: I runner.py:471] Tracing and optimizing the inference graph...\n",
      "2024-12-03 09:45:10.975455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-03 09:45:11.836161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-03 09:45:22.868000: I runner.py:471] 2377 predictions are buffered, but waiting for the prediction of queued line 25 to advance the output...\n",
      "2024-12-03 09:45:32.946000: I runner.py:471] 4649 predictions are buffered, but waiting for the prediction of queued line 25 to advance the output...\n",
      "2024-12-03 09:45:43.078000: I runner.py:471] 6921 predictions are buffered, but waiting for the prediction of queued line 25 to advance the output...\n",
      "2024-12-03 09:45:53.123000: I runner.py:471] 9193 predictions are buffered, but waiting for the prediction of queued line 25 to advance the output...\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 onmt-main --config data.yml --auto_config --checkpoint_path KK-TR-EN-Shared-vocab-2/ckpt-100000 infer --features_file Tatoeba_tokens_test_shared --predictions_file output_kk_en_tr_shared_vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c97a435-ef85-43dc-b8f7-15064c63bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 MT-Preparation/subwording/3-desubword.py en_shared_vocab.model output_kk_en_tr_shared_vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e5476d-106c-4d28-b39b-2e0c5535b365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference first sentence: I won't stay there very long.\n",
      "Translated first sentence: I won't stay there very long .\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "BLEU:  BLEU = 60.31 80.7/65.0/54.5/46.3 (BP = 1.000 ratio = 1.012 hyp_len = 78525 ref_len = 77587)\n",
      "CHRF:  chrF2 = 74.08\n"
     ]
    }
   ],
   "source": [
    "# BLEU and chrF scores\n",
    "!python3 compute-bleu.py Tatoeba.en-tr.en-filtered.en.test output_kk_en_tr_shared_vocab.txt.desubword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1193a4b4-bf28-4a0d-b335-1905c73b48a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama METEOR Puanı: 0.8145330897292876\n"
     ]
    }
   ],
   "source": [
    "# Average METEOR score (Ortalama METEOR Puanı)\n",
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "def read_and_tokenize_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return [nltk.word_tokenize(line.strip()) for line in lines]\n",
    "\n",
    "def calculate_meteor(reference_file, hypothesis_file):\n",
    "    references = read_and_tokenize_file(reference_file)\n",
    "    hypotheses = read_and_tokenize_file(hypothesis_file)\n",
    "    \n",
    "    if len(references) != len(hypotheses):\n",
    "        raise ValueError(\"Dosyaların satır sayıları eşleşmiyor\")\n",
    "\n",
    "    total_meteor_score = 0.0\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        total_meteor_score += meteor_score([ref], hyp)\n",
    "\n",
    "    average_meteor_score = total_meteor_score / len(references)\n",
    "    return average_meteor_score\n",
    "\n",
    "reference_file = 'Tatoeba.en-tr.en-filtered.en.test'\n",
    "hypothesis_file = 'output_kk_en_tr_shared_vocab.txt.desubword'\n",
    "\n",
    "score = calculate_meteor(reference_file, hypothesis_file)\n",
    "print(f\"Ortalama METEOR Puanı: {score}\") #Average METEOR score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "102e1990-5b87-408b-a2a7-26d8a07a91f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 09:51:18.128381: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 09:51:18.931154: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-03 09:51:18.931225: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-03 09:51:18.931233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-03 09:51:19.895000: I onmt-main:8] Creating model directory TR-EN-Shared-vocab\n",
      "2024-12-03 09:51:20.088000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-03 09:51:20.088000: I main.py:315] Using model:\n",
      "(model): MyCustomTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): WordEmbedder()\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): WordEmbedder()\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-03 09:51:20.091802: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 09:51:21.652690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-03 09:51:21.653387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7701 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-12-03 09:51:21.653916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 6099 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "2024-12-03 09:51:21.658000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file: Tatoeba_tokens_dev_shared\n",
      "  eval_labels_file: Tatoeba_dev_target_tokens_shared.txt\n",
      "  source_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file: Tatoeba_tokens_train_shared\n",
      "  train_labels_file: Tatoeba_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: TR-EN-Shared-vocab\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 250000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-03 09:51:21.995000: I inputter.py:316] Initialized source input layer:\n",
      "2024-12-03 09:51:21.995000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-03 09:51:21.995000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-03 09:51:22.079000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-03 09:51:22.079000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-03 09:51:22.079000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-03 09:51:22.084000: W runner.py:269] No checkpoint to restore in TR-EN-Shared-vocab\n",
      "2024-12-03 09:51:22.086000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "2024-12-03 09:51:22.130000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-03 09:51:23.067041: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-03 09:51:23.186000: I main.py:325] Accumulate gradients of 7 iterations to reach effective batch size of 25000\n",
      "2024-12-03 09:51:23.308000: I mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2024-12-03 09:51:23.422000: I dataset_ops.py:2542] Training on 647485 examples\n",
      "2024-12-03 09:52:28.577879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-03 09:52:29.676543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-03 09:52:29.945993: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-03 09:52:38.941000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-03 09:52:38.961000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-03 09:52:40.510000: I cross_device_ops.py:897] batch_all_reduce: 260 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-03 09:52:45.498000: I cross_device_ops.py:897] batch_all_reduce: 260 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-03 09:52:52.244000: I runner.py:310] Number of model parameters: 93326081\n",
      "2024-12-03 09:52:52.249000: I runner.py:310] Number of model weights: 260 (trainable = 260, non trainable = 0)\n",
      "2024-12-03 09:52:52.284000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-03 09:52:52.291000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-03 09:52:54.378000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-1\n",
      "2024-12-03 09:52:55.091000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-03 09:52:55.115000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-03 09:52:55.784000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-03 09:52:55.809000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-03 09:52:56.434000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-03 09:52:56.458000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-03 09:53:55.240000: I runner.py:310] Step = 100 ; steps/s = 1.63, tokens/s = 77861 (34274 source, 43587 target) ; Learning rate = 0.000009 ; Loss = 9.570997\n",
      "2024-12-03 09:54:56.023000: I runner.py:310] Step = 200 ; steps/s = 1.65, tokens/s = 78399 (34529 source, 43870 target) ; Learning rate = 0.000018 ; Loss = 8.516644\n",
      "2024-12-03 09:55:57.646000: I runner.py:310] Step = 300 ; steps/s = 1.62, tokens/s = 75963 (33501 source, 42462 target) ; Learning rate = 0.000027 ; Loss = 7.179461\n",
      "2024-12-03 09:56:58.452000: I runner.py:310] Step = 400 ; steps/s = 1.64, tokens/s = 78355 (34493 source, 43862 target) ; Learning rate = 0.000035 ; Loss = 6.211273\n",
      "2024-12-03 09:57:59.784000: I runner.py:310] Step = 500 ; steps/s = 1.63, tokens/s = 77712 (34230 source, 43482 target) ; Learning rate = 0.000044 ; Loss = 5.740379\n",
      "2024-12-03 09:59:00.284000: I runner.py:310] Step = 600 ; steps/s = 1.65, tokens/s = 77379 (34120 source, 43259 target) ; Learning rate = 0.000053 ; Loss = 5.281104\n",
      "2024-12-03 10:00:01.369000: I runner.py:310] Step = 700 ; steps/s = 1.64, tokens/s = 77988 (34337 source, 43651 target) ; Learning rate = 0.000062 ; Loss = 4.999580\n",
      "2024-12-03 10:01:01.698000: I runner.py:310] Step = 800 ; steps/s = 1.66, tokens/s = 77620 (34232 source, 43388 target) ; Learning rate = 0.000071 ; Loss = 4.726516\n",
      "2024-12-03 10:02:02.448000: I runner.py:310] Step = 900 ; steps/s = 1.65, tokens/s = 78411 (34516 source, 43895 target) ; Learning rate = 0.000080 ; Loss = 4.521067\n",
      "2024-12-03 10:03:03.243000: I runner.py:310] Step = 1000 ; steps/s = 1.65, tokens/s = 78399 (34539 source, 43860 target) ; Learning rate = 0.000088 ; Loss = 4.400920\n",
      "2024-12-03 10:04:04.156000: I runner.py:310] Step = 1100 ; steps/s = 1.64, tokens/s = 76849 (33870 source, 42979 target) ; Learning rate = 0.000097 ; Loss = 4.194840\n",
      "2024-12-03 10:05:04.937000: I runner.py:310] Step = 1200 ; steps/s = 1.65, tokens/s = 78416 (34545 source, 43871 target) ; Learning rate = 0.000106 ; Loss = 4.003337\n",
      "2024-12-03 10:06:05.267000: I runner.py:310] Step = 1300 ; steps/s = 1.66, tokens/s = 77585 (34212 source, 43373 target) ; Learning rate = 0.000115 ; Loss = 4.256440\n",
      "2024-12-03 10:07:06.098000: I runner.py:310] Step = 1400 ; steps/s = 1.64, tokens/s = 78335 (34494 source, 43841 target) ; Learning rate = 0.000124 ; Loss = 3.546816\n",
      "2024-12-03 10:08:06.926000: I runner.py:310] Step = 1500 ; steps/s = 1.64, tokens/s = 78323 (34488 source, 43835 target) ; Learning rate = 0.000133 ; Loss = 3.279524\n",
      "2024-12-03 10:09:07.292000: I runner.py:310] Step = 1600 ; steps/s = 1.66, tokens/s = 77549 (34193 source, 43356 target) ; Learning rate = 0.000142 ; Loss = 3.085843\n",
      "2024-12-03 10:10:08.017000: I runner.py:310] Step = 1700 ; steps/s = 1.65, tokens/s = 78457 (34537 source, 43920 target) ; Learning rate = 0.000150 ; Loss = 2.996960\n",
      "2024-12-03 10:11:08.838000: I runner.py:310] Step = 1800 ; steps/s = 1.64, tokens/s = 78359 (34518 source, 43841 target) ; Learning rate = 0.000159 ; Loss = 3.051695\n",
      "2024-12-03 10:12:09.080000: I runner.py:310] Step = 1900 ; steps/s = 1.66, tokens/s = 77685 (34241 source, 43444 target) ; Learning rate = 0.000168 ; Loss = 2.790934\n",
      "2024-12-03 10:13:09.812000: I runner.py:310] Step = 2000 ; steps/s = 1.65, tokens/s = 78464 (34567 source, 43897 target) ; Learning rate = 0.000177 ; Loss = 2.679838\n",
      "2024-12-03 10:14:10.091000: I runner.py:310] Step = 2100 ; steps/s = 1.66, tokens/s = 77686 (34262 source, 43424 target) ; Learning rate = 0.000186 ; Loss = 2.841711\n",
      "2024-12-03 10:15:10.868000: I runner.py:310] Step = 2200 ; steps/s = 1.65, tokens/s = 78410 (34520 source, 43890 target) ; Learning rate = 0.000195 ; Loss = 2.516466\n",
      "2024-12-03 10:16:11.622000: I runner.py:310] Step = 2300 ; steps/s = 1.65, tokens/s = 78433 (34540 source, 43893 target) ; Learning rate = 0.000203 ; Loss = 2.495262\n",
      "2024-12-03 10:17:11.918000: I runner.py:310] Step = 2400 ; steps/s = 1.66, tokens/s = 77616 (34220 source, 43396 target) ; Learning rate = 0.000212 ; Loss = 2.579707\n",
      "2024-12-03 10:18:12.666000: I runner.py:310] Step = 2500 ; steps/s = 1.65, tokens/s = 78467 (34569 source, 43898 target) ; Learning rate = 0.000221 ; Loss = 2.511280\n",
      "2024-12-03 10:19:13.015000: I runner.py:310] Step = 2600 ; steps/s = 1.66, tokens/s = 77563 (34197 source, 43366 target) ; Learning rate = 0.000230 ; Loss = 2.633264\n",
      "2024-12-03 10:20:13.758000: I runner.py:310] Step = 2700 ; steps/s = 1.65, tokens/s = 78402 (34501 source, 43901 target) ; Learning rate = 0.000239 ; Loss = 2.435496\n",
      "2024-12-03 10:21:14.507000: I runner.py:310] Step = 2800 ; steps/s = 1.65, tokens/s = 78439 (34551 source, 43888 target) ; Learning rate = 0.000248 ; Loss = 2.392958\n",
      "2024-12-03 10:22:14.870000: I runner.py:310] Step = 2900 ; steps/s = 1.66, tokens/s = 77562 (34200 source, 43362 target) ; Learning rate = 0.000256 ; Loss = 2.340910\n",
      "2024-12-03 10:23:15.649000: I runner.py:310] Step = 3000 ; steps/s = 1.65, tokens/s = 78395 (34529 source, 43866 target) ; Learning rate = 0.000265 ; Loss = 2.322909\n",
      "2024-12-03 10:24:16.483000: I runner.py:310] Step = 3100 ; steps/s = 1.64, tokens/s = 78352 (34510 source, 43842 target) ; Learning rate = 0.000274 ; Loss = 2.415832\n",
      "2024-12-03 10:25:16.789000: I runner.py:310] Step = 3200 ; steps/s = 1.66, tokens/s = 77617 (34220 source, 43397 target) ; Learning rate = 0.000283 ; Loss = 2.201257\n",
      "2024-12-03 10:26:17.527000: I runner.py:310] Step = 3300 ; steps/s = 1.65, tokens/s = 78467 (34559 source, 43908 target) ; Learning rate = 0.000292 ; Loss = 2.250568\n",
      "2024-12-03 10:27:17.801000: I runner.py:310] Step = 3400 ; steps/s = 1.66, tokens/s = 77654 (34235 source, 43419 target) ; Learning rate = 0.000301 ; Loss = 2.310098\n",
      "2024-12-03 10:28:18.525000: I runner.py:310] Step = 3500 ; steps/s = 1.65, tokens/s = 78457 (34535 source, 43922 target) ; Learning rate = 0.000309 ; Loss = 2.138340\n",
      "2024-12-03 10:29:19.373000: I runner.py:310] Step = 3600 ; steps/s = 1.64, tokens/s = 78331 (34511 source, 43820 target) ; Learning rate = 0.000318 ; Loss = 2.220183\n",
      "2024-12-03 10:30:19.721000: I runner.py:310] Step = 3700 ; steps/s = 1.66, tokens/s = 77577 (34219 source, 43358 target) ; Learning rate = 0.000327 ; Loss = 2.297429\n",
      "2024-12-03 10:31:20.543000: I runner.py:310] Step = 3800 ; steps/s = 1.64, tokens/s = 78347 (34497 source, 43850 target) ; Learning rate = 0.000336 ; Loss = 2.164362\n",
      "2024-12-03 10:32:20.887000: I runner.py:310] Step = 3900 ; steps/s = 1.66, tokens/s = 77564 (34197 source, 43367 target) ; Learning rate = 0.000345 ; Loss = 2.250371\n",
      "2024-12-03 10:33:21.613000: I runner.py:310] Step = 4000 ; steps/s = 1.65, tokens/s = 78446 (34529 source, 43917 target) ; Learning rate = 0.000354 ; Loss = 2.070609\n",
      "2024-12-03 10:34:22.435000: I runner.py:310] Step = 4100 ; steps/s = 1.64, tokens/s = 78359 (34519 source, 43840 target) ; Learning rate = 0.000362 ; Loss = 2.132790\n",
      "2024-12-03 10:35:22.717000: I runner.py:310] Step = 4200 ; steps/s = 1.66, tokens/s = 77626 (34209 source, 43417 target) ; Learning rate = 0.000371 ; Loss = 2.140152\n",
      "2024-12-03 10:36:23.444000: I runner.py:310] Step = 4300 ; steps/s = 1.65, tokens/s = 78473 (34549 source, 43924 target) ; Learning rate = 0.000380 ; Loss = 2.096676\n",
      "2024-12-03 10:37:24.250000: I runner.py:310] Step = 4400 ; steps/s = 1.64, tokens/s = 78401 (34555 source, 43846 target) ; Learning rate = 0.000389 ; Loss = 2.038264\n",
      "2024-12-03 10:38:25.747000: I runner.py:310] Step = 4500 ; steps/s = 1.63, tokens/s = 76112 (33549 source, 42563 target) ; Learning rate = 0.000398 ; Loss = 2.067419\n",
      "2024-12-03 10:39:26.572000: I runner.py:310] Step = 4600 ; steps/s = 1.64, tokens/s = 78362 (34520 source, 43842 target) ; Learning rate = 0.000407 ; Loss = 2.049499\n",
      "2024-12-03 10:40:26.899000: I runner.py:310] Step = 4700 ; steps/s = 1.66, tokens/s = 77557 (34183 source, 43374 target) ; Learning rate = 0.000416 ; Loss = 1.995841\n",
      "2024-12-03 10:41:27.639000: I runner.py:310] Step = 4800 ; steps/s = 1.65, tokens/s = 78433 (34530 source, 43903 target) ; Learning rate = 0.000424 ; Loss = 2.015767\n",
      "2024-12-03 10:42:28.452000: I runner.py:310] Step = 4900 ; steps/s = 1.64, tokens/s = 78385 (34530 source, 43855 target) ; Learning rate = 0.000433 ; Loss = 1.988435\n",
      "2024-12-03 10:43:28.799000: I runner.py:310] Step = 5000 ; steps/s = 1.66, tokens/s = 77575 (34203 source, 43372 target) ; Learning rate = 0.000442 ; Loss = 2.032667\n",
      "2024-12-03 10:43:28.800000: I training.py:192] Running evaluation for step 5000\n",
      "2024-12-03 10:44:38.218000: I training.py:192] Evaluation result for step 5000: loss = 0.833539 ; perplexity = 2.301449\n",
      "2024-12-03 10:45:38.953000: I runner.py:310] Step = 5100 ; steps/s = 1.65, tokens/s = 78474 (34566 source, 43908 target) ; Learning rate = 0.000451 ; Loss = 2.003012\n",
      "2024-12-03 10:46:39.284000: I runner.py:310] Step = 5200 ; steps/s = 1.66, tokens/s = 77603 (34223 source, 43380 target) ; Learning rate = 0.000460 ; Loss = 2.694260\n",
      "2024-12-03 10:47:40.196000: I runner.py:310] Step = 5300 ; steps/s = 1.64, tokens/s = 78213 (34424 source, 43789 target) ; Learning rate = 0.000469 ; Loss = 2.045096\n",
      "2024-12-03 10:48:41.059000: I runner.py:310] Step = 5400 ; steps/s = 1.64, tokens/s = 78302 (34496 source, 43806 target) ; Learning rate = 0.000477 ; Loss = 2.041780\n",
      "2024-12-03 10:49:41.492000: I runner.py:310] Step = 5500 ; steps/s = 1.65, tokens/s = 77446 (34137 source, 43309 target) ; Learning rate = 0.000486 ; Loss = 1.977199\n",
      "2024-12-03 10:50:42.399000: I runner.py:310] Step = 5600 ; steps/s = 1.64, tokens/s = 78276 (34501 source, 43775 target) ; Learning rate = 0.000495 ; Loss = 1.965424\n",
      "2024-12-03 10:51:43.213000: I runner.py:310] Step = 5700 ; steps/s = 1.64, tokens/s = 78342 (34487 source, 43855 target) ; Learning rate = 0.000504 ; Loss = 1.989589\n",
      "2024-12-03 10:52:43.566000: I runner.py:310] Step = 5800 ; steps/s = 1.66, tokens/s = 77539 (34174 source, 43365 target) ; Learning rate = 0.000513 ; Loss = 1.893723\n",
      "2024-12-03 10:53:44.372000: I runner.py:310] Step = 5900 ; steps/s = 1.64, tokens/s = 78353 (34497 source, 43856 target) ; Learning rate = 0.000522 ; Loss = 1.896717\n",
      "2024-12-03 10:54:44.728000: I runner.py:310] Step = 6000 ; steps/s = 1.66, tokens/s = 77586 (34216 source, 43370 target) ; Learning rate = 0.000530 ; Loss = 1.978034\n",
      "2024-12-03 10:55:45.587000: I runner.py:310] Step = 6100 ; steps/s = 1.64, tokens/s = 78264 (34447 source, 43817 target) ; Learning rate = 0.000539 ; Loss = 1.874979\n",
      "2024-12-03 10:56:46.419000: I runner.py:310] Step = 6200 ; steps/s = 1.64, tokens/s = 78350 (34520 source, 43830 target) ; Learning rate = 0.000548 ; Loss = 1.934708\n",
      "2024-12-03 10:57:46.824000: I runner.py:310] Step = 6300 ; steps/s = 1.66, tokens/s = 77505 (34169 source, 43336 target) ; Learning rate = 0.000557 ; Loss = 1.866644\n",
      "2024-12-03 10:58:47.679000: I runner.py:310] Step = 6400 ; steps/s = 1.64, tokens/s = 78305 (34483 source, 43822 target) ; Learning rate = 0.000566 ; Loss = 1.872904\n",
      "2024-12-03 10:59:48.132000: I runner.py:310] Step = 6500 ; steps/s = 1.65, tokens/s = 77433 (34153 source, 43280 target) ; Learning rate = 0.000575 ; Loss = 1.892281\n",
      "2024-12-03 11:00:48.926000: I runner.py:310] Step = 6600 ; steps/s = 1.65, tokens/s = 78388 (34518 source, 43870 target) ; Learning rate = 0.000583 ; Loss = 1.871638\n",
      "2024-12-03 11:01:49.697000: I runner.py:310] Step = 6700 ; steps/s = 1.65, tokens/s = 78419 (34549 source, 43870 target) ; Learning rate = 0.000592 ; Loss = 1.952299\n",
      "2024-12-03 11:02:50.054000: I runner.py:310] Step = 6800 ; steps/s = 1.66, tokens/s = 77524 (34152 source, 43372 target) ; Learning rate = 0.000601 ; Loss = 1.868469\n",
      "2024-12-03 11:03:50.847000: I runner.py:310] Step = 6900 ; steps/s = 1.65, tokens/s = 78386 (34524 source, 43862 target) ; Learning rate = 0.000610 ; Loss = 1.899383\n",
      "2024-12-03 11:04:51.647000: I runner.py:310] Step = 7000 ; steps/s = 1.65, tokens/s = 78398 (34538 source, 43860 target) ; Learning rate = 0.000619 ; Loss = 1.901635\n",
      "2024-12-03 11:05:51.933000: I runner.py:310] Step = 7100 ; steps/s = 1.66, tokens/s = 77653 (34250 source, 43403 target) ; Learning rate = 0.000628 ; Loss = 1.822656\n",
      "2024-12-03 11:06:52.682000: I runner.py:310] Step = 7200 ; steps/s = 1.65, tokens/s = 78439 (34530 source, 43909 target) ; Learning rate = 0.000636 ; Loss = 1.805235\n",
      "2024-12-03 11:07:52.931000: I runner.py:310] Step = 7300 ; steps/s = 1.66, tokens/s = 77690 (34257 source, 43433 target) ; Learning rate = 0.000645 ; Loss = 1.856683\n",
      "2024-12-03 11:08:53.712000: I runner.py:310] Step = 7400 ; steps/s = 1.65, tokens/s = 78382 (34499 source, 43883 target) ; Learning rate = 0.000654 ; Loss = 1.858567\n",
      "2024-12-03 11:09:55.149000: I runner.py:310] Step = 7500 ; steps/s = 1.63, tokens/s = 77575 (34175 source, 43400 target) ; Learning rate = 0.000663 ; Loss = 1.801239\n",
      "2024-12-03 11:10:56.460000: I runner.py:310] Step = 7600 ; steps/s = 1.63, tokens/s = 76327 (33650 source, 42677 target) ; Learning rate = 0.000672 ; Loss = 1.780804\n",
      "2024-12-03 11:11:57.690000: I runner.py:310] Step = 7700 ; steps/s = 1.63, tokens/s = 77829 (34275 source, 43554 target) ; Learning rate = 0.000681 ; Loss = 1.817503\n",
      "2024-12-03 11:12:58.095000: I runner.py:310] Step = 7800 ; steps/s = 1.66, tokens/s = 77514 (34188 source, 43326 target) ; Learning rate = 0.000690 ; Loss = 1.781157\n",
      "2024-12-03 11:13:59.030000: I runner.py:310] Step = 7900 ; steps/s = 1.64, tokens/s = 78207 (34440 source, 43767 target) ; Learning rate = 0.000698 ; Loss = 1.793273\n",
      "2024-12-03 11:14:59.898000: I runner.py:310] Step = 8000 ; steps/s = 1.64, tokens/s = 78274 (34466 source, 43808 target) ; Learning rate = 0.000707 ; Loss = 1.857035\n",
      "2024-12-03 11:16:00.333000: I runner.py:310] Step = 8100 ; steps/s = 1.65, tokens/s = 77462 (34157 source, 43305 target) ; Learning rate = 0.000716 ; Loss = 1.799425\n",
      "2024-12-03 11:17:01.189000: I runner.py:310] Step = 8200 ; steps/s = 1.64, tokens/s = 78295 (34474 source, 43821 target) ; Learning rate = 0.000725 ; Loss = 1.787194\n",
      "2024-12-03 11:18:02.037000: I runner.py:310] Step = 8300 ; steps/s = 1.64, tokens/s = 78315 (34491 source, 43824 target) ; Learning rate = 0.000734 ; Loss = 1.763470\n",
      "2024-12-03 11:19:02.484000: I runner.py:310] Step = 8400 ; steps/s = 1.65, tokens/s = 77454 (34154 source, 43300 target) ; Learning rate = 0.000743 ; Loss = 1.811183\n",
      "2024-12-03 11:20:03.366000: I runner.py:310] Step = 8500 ; steps/s = 1.64, tokens/s = 78259 (34459 source, 43800 target) ; Learning rate = 0.000751 ; Loss = 1.758184\n",
      "2024-12-03 11:21:03.708000: I runner.py:310] Step = 8600 ; steps/s = 1.66, tokens/s = 77571 (34206 source, 43365 target) ; Learning rate = 0.000760 ; Loss = 1.737520\n",
      "2024-12-03 11:22:04.526000: I runner.py:310] Step = 8700 ; steps/s = 1.64, tokens/s = 78352 (34497 source, 43855 target) ; Learning rate = 0.000769 ; Loss = 1.768852\n",
      "2024-12-03 11:23:05.360000: I runner.py:310] Step = 8800 ; steps/s = 1.64, tokens/s = 78338 (34505 source, 43833 target) ; Learning rate = 0.000778 ; Loss = 1.791220\n",
      "2024-12-03 11:24:05.703000: I runner.py:310] Step = 8900 ; steps/s = 1.66, tokens/s = 77572 (34205 source, 43367 target) ; Learning rate = 0.000787 ; Loss = 1.763822\n",
      "2024-12-03 11:25:06.544000: I runner.py:310] Step = 9000 ; steps/s = 1.64, tokens/s = 78313 (34483 source, 43830 target) ; Learning rate = 0.000796 ; Loss = 1.744100\n",
      "2024-12-03 11:26:07.016000: I runner.py:310] Step = 9100 ; steps/s = 1.65, tokens/s = 77404 (34125 source, 43279 target) ; Learning rate = 0.000804 ; Loss = 1.797052\n",
      "2024-12-03 11:27:07.872000: I runner.py:310] Step = 9200 ; steps/s = 1.64, tokens/s = 78287 (34465 source, 43822 target) ; Learning rate = 0.000813 ; Loss = 1.738691\n",
      "2024-12-03 11:28:08.717000: I runner.py:310] Step = 9300 ; steps/s = 1.64, tokens/s = 78330 (34505 source, 43825 target) ; Learning rate = 0.000822 ; Loss = 1.727416\n",
      "2024-12-03 11:29:09.128000: I runner.py:310] Step = 9400 ; steps/s = 1.66, tokens/s = 77472 (34153 source, 43319 target) ; Learning rate = 0.000831 ; Loss = 1.709365\n",
      "2024-12-03 11:30:09.998000: I runner.py:310] Step = 9500 ; steps/s = 1.64, tokens/s = 78292 (34477 source, 43815 target) ; Learning rate = 0.000840 ; Loss = 1.746889\n",
      "2024-12-03 11:31:10.875000: I runner.py:310] Step = 9600 ; steps/s = 1.64, tokens/s = 78290 (34488 source, 43802 target) ; Learning rate = 0.000849 ; Loss = 1.749065\n",
      "2024-12-03 11:32:11.232000: I runner.py:310] Step = 9700 ; steps/s = 1.66, tokens/s = 77565 (34207 source, 43358 target) ; Learning rate = 0.000857 ; Loss = 1.706391\n",
      "2024-12-03 11:33:12.105000: I runner.py:310] Step = 9800 ; steps/s = 1.64, tokens/s = 78281 (34473 source, 43808 target) ; Learning rate = 0.000866 ; Loss = 1.761232\n",
      "2024-12-03 11:34:12.463000: I runner.py:310] Step = 9900 ; steps/s = 1.66, tokens/s = 77525 (34160 source, 43365 target) ; Learning rate = 0.000875 ; Loss = 1.707610\n",
      "2024-12-03 11:35:13.260000: I runner.py:310] Step = 10000 ; steps/s = 1.64, tokens/s = 78357 (34503 source, 43854 target) ; Learning rate = 0.000884 ; Loss = 1.703869\n",
      "2024-12-03 11:35:14.944000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-10000\n",
      "2024-12-03 11:35:14.944000: I training.py:192] Running evaluation for step 10000\n",
      "2024-12-03 11:36:18.088000: I training.py:192] Evaluation result for step 10000: loss = 0.728309 ; perplexity = 2.071574\n",
      "2024-12-03 11:37:18.810000: I runner.py:310] Step = 10100 ; steps/s = 1.65, tokens/s = 78523 (34597 source, 43926 target) ; Learning rate = 0.000879 ; Loss = 1.729544\n",
      "2024-12-03 11:38:19.217000: I runner.py:310] Step = 10200 ; steps/s = 1.66, tokens/s = 77468 (34148 source, 43320 target) ; Learning rate = 0.000875 ; Loss = 1.686059\n",
      "2024-12-03 11:39:20.112000: I runner.py:310] Step = 10300 ; steps/s = 1.64, tokens/s = 78272 (34467 source, 43805 target) ; Learning rate = 0.000871 ; Loss = 1.700035\n",
      "2024-12-03 11:40:20.555000: I runner.py:310] Step = 10400 ; steps/s = 1.65, tokens/s = 77441 (34149 source, 43292 target) ; Learning rate = 0.000867 ; Loss = 1.663768\n",
      "2024-12-03 11:41:21.397000: I runner.py:310] Step = 10500 ; steps/s = 1.64, tokens/s = 78343 (34506 source, 43837 target) ; Learning rate = 0.000863 ; Loss = 1.698088\n",
      "2024-12-03 11:42:22.216000: I runner.py:310] Step = 10600 ; steps/s = 1.64, tokens/s = 78333 (34495 source, 43838 target) ; Learning rate = 0.000858 ; Loss = 1.717450\n",
      "2024-12-03 11:43:22.585000: I runner.py:310] Step = 10700 ; steps/s = 1.66, tokens/s = 77536 (34177 source, 43359 target) ; Learning rate = 0.000854 ; Loss = 1.674716\n",
      "2024-12-03 11:44:23.427000: I runner.py:310] Step = 10800 ; steps/s = 1.64, tokens/s = 78319 (34493 source, 43826 target) ; Learning rate = 0.000850 ; Loss = 1.671629\n",
      "2024-12-03 11:45:24.261000: I runner.py:310] Step = 10900 ; steps/s = 1.64, tokens/s = 78341 (34506 source, 43835 target) ; Learning rate = 0.000847 ; Loss = 1.708895\n",
      "2024-12-03 11:46:24.665000: I runner.py:310] Step = 11000 ; steps/s = 1.66, tokens/s = 77505 (34169 source, 43336 target) ; Learning rate = 0.000843 ; Loss = 1.653955\n",
      "2024-12-03 11:47:25.557000: I runner.py:310] Step = 11100 ; steps/s = 1.64, tokens/s = 78273 (34484 source, 43789 target) ; Learning rate = 0.000839 ; Loss = 1.691204\n",
      "2024-12-03 11:48:25.887000: I runner.py:310] Step = 11200 ; steps/s = 1.66, tokens/s = 77553 (34188 source, 43365 target) ; Learning rate = 0.000835 ; Loss = 1.646621\n",
      "2024-12-03 11:49:26.767000: I runner.py:310] Step = 11300 ; steps/s = 1.64, tokens/s = 78289 (34476 source, 43813 target) ; Learning rate = 0.000831 ; Loss = 1.688830\n",
      "2024-12-03 11:50:27.663000: I runner.py:310] Step = 11400 ; steps/s = 1.64, tokens/s = 78255 (34457 source, 43798 target) ; Learning rate = 0.000828 ; Loss = 1.676261\n",
      "2024-12-03 11:51:28.082000: I runner.py:310] Step = 11500 ; steps/s = 1.66, tokens/s = 77479 (34165 source, 43314 target) ; Learning rate = 0.000824 ; Loss = 1.652730\n",
      "2024-12-03 11:52:28.900000: I runner.py:310] Step = 11600 ; steps/s = 1.64, tokens/s = 78345 (34492 source, 43853 target) ; Learning rate = 0.000821 ; Loss = 1.656612\n",
      "2024-12-03 11:53:29.367000: I runner.py:310] Step = 11700 ; steps/s = 1.65, tokens/s = 77429 (34151 source, 43278 target) ; Learning rate = 0.000817 ; Loss = 1.644657\n",
      "2024-12-03 11:54:30.254000: I runner.py:310] Step = 11800 ; steps/s = 1.64, tokens/s = 78284 (34473 source, 43811 target) ; Learning rate = 0.000814 ; Loss = 1.644198\n",
      "2024-12-03 11:55:31.054000: I runner.py:310] Step = 11900 ; steps/s = 1.64, tokens/s = 78350 (34507 source, 43843 target) ; Learning rate = 0.000810 ; Loss = 1.658375\n",
      "2024-12-03 11:56:31.459000: I runner.py:310] Step = 12000 ; steps/s = 1.66, tokens/s = 77454 (34133 source, 43321 target) ; Learning rate = 0.000807 ; Loss = 1.649117\n",
      "2024-12-03 11:57:32.352000: I runner.py:310] Step = 12100 ; steps/s = 1.64, tokens/s = 78264 (34473 source, 43791 target) ; Learning rate = 0.000803 ; Loss = 1.645425\n",
      "2024-12-03 11:58:33.264000: I runner.py:310] Step = 12200 ; steps/s = 1.64, tokens/s = 78246 (34462 source, 43784 target) ; Learning rate = 0.000800 ; Loss = 1.652548\n",
      "2024-12-03 11:59:33.694000: I runner.py:310] Step = 12300 ; steps/s = 1.66, tokens/s = 77479 (34160 source, 43319 target) ; Learning rate = 0.000797 ; Loss = 1.638757\n",
      "2024-12-03 12:00:34.530000: I runner.py:310] Step = 12400 ; steps/s = 1.64, tokens/s = 78334 (34510 source, 43824 target) ; Learning rate = 0.000794 ; Loss = 1.632443\n",
      "2024-12-03 12:01:34.949000: I runner.py:310] Step = 12500 ; steps/s = 1.66, tokens/s = 77447 (34132 source, 43315 target) ; Learning rate = 0.000791 ; Loss = 1.608348\n",
      "2024-12-03 12:02:35.777000: I runner.py:310] Step = 12600 ; steps/s = 1.64, tokens/s = 78318 (34475 source, 43843 target) ; Learning rate = 0.000787 ; Loss = 1.621313\n",
      "2024-12-03 12:03:36.667000: I runner.py:310] Step = 12700 ; steps/s = 1.64, tokens/s = 78271 (34482 source, 43789 target) ; Learning rate = 0.000784 ; Loss = 1.642788\n",
      "2024-12-03 12:04:37.092000: I runner.py:310] Step = 12800 ; steps/s = 1.66, tokens/s = 77476 (34158 source, 43318 target) ; Learning rate = 0.000781 ; Loss = 1.622924\n",
      "2024-12-03 12:05:37.934000: I runner.py:310] Step = 12900 ; steps/s = 1.64, tokens/s = 78327 (34503 source, 43824 target) ; Learning rate = 0.000778 ; Loss = 1.653657\n",
      "2024-12-03 12:06:38.327000: I runner.py:310] Step = 13000 ; steps/s = 1.66, tokens/s = 77491 (34156 source, 43335 target) ; Learning rate = 0.000775 ; Loss = 1.602987\n",
      "2024-12-03 12:07:39.137000: I runner.py:310] Step = 13100 ; steps/s = 1.64, tokens/s = 78350 (34498 source, 43852 target) ; Learning rate = 0.000772 ; Loss = 1.613471\n",
      "2024-12-03 12:08:39.913000: I runner.py:310] Step = 13200 ; steps/s = 1.65, tokens/s = 78417 (34542 source, 43875 target) ; Learning rate = 0.000769 ; Loss = 1.612238\n",
      "2024-12-03 12:09:40.290000: I runner.py:310] Step = 13300 ; steps/s = 1.66, tokens/s = 77547 (34190 source, 43357 target) ; Learning rate = 0.000766 ; Loss = 1.603160\n",
      "2024-12-03 12:10:41.168000: I runner.py:310] Step = 13400 ; steps/s = 1.64, tokens/s = 78258 (34456 source, 43802 target) ; Learning rate = 0.000764 ; Loss = 1.610594\n",
      "2024-12-03 12:11:42.000000: I runner.py:310] Step = 13500 ; steps/s = 1.64, tokens/s = 78350 (34516 source, 43834 target) ; Learning rate = 0.000761 ; Loss = 1.629856\n",
      "2024-12-03 12:12:42.402000: I runner.py:310] Step = 13600 ; steps/s = 1.66, tokens/s = 77521 (34185 source, 43336 target) ; Learning rate = 0.000758 ; Loss = 1.603665\n",
      "2024-12-03 12:13:43.266000: I runner.py:310] Step = 13700 ; steps/s = 1.64, tokens/s = 78266 (34463 source, 43803 target) ; Learning rate = 0.000755 ; Loss = 1.610651\n",
      "2024-12-03 12:14:43.633000: I runner.py:310] Step = 13800 ; steps/s = 1.66, tokens/s = 77532 (34173 source, 43359 target) ; Learning rate = 0.000752 ; Loss = 1.591020\n",
      "2024-12-03 12:15:44.505000: I runner.py:310] Step = 13900 ; steps/s = 1.64, tokens/s = 78278 (34473 source, 43805 target) ; Learning rate = 0.000750 ; Loss = 1.602926\n",
      "2024-12-03 12:16:45.300000: I runner.py:310] Step = 14000 ; steps/s = 1.65, tokens/s = 78383 (34523 source, 43860 target) ; Learning rate = 0.000747 ; Loss = 1.609967\n",
      "2024-12-03 12:17:45.715000: I runner.py:310] Step = 14100 ; steps/s = 1.66, tokens/s = 77470 (34144 source, 43326 target) ; Learning rate = 0.000744 ; Loss = 1.596708\n",
      "2024-12-03 12:18:46.611000: I runner.py:310] Step = 14200 ; steps/s = 1.64, tokens/s = 78265 (34481 source, 43784 target) ; Learning rate = 0.000742 ; Loss = 1.607737\n",
      "2024-12-03 12:19:47.063000: I runner.py:310] Step = 14300 ; steps/s = 1.65, tokens/s = 77450 (34155 source, 43295 target) ; Learning rate = 0.000739 ; Loss = 1.581889\n",
      "2024-12-03 12:20:47.889000: I runner.py:310] Step = 14400 ; steps/s = 1.64, tokens/s = 78336 (34493 source, 43843 target) ; Learning rate = 0.000737 ; Loss = 1.608847\n",
      "2024-12-03 12:21:48.760000: I runner.py:310] Step = 14500 ; steps/s = 1.64, tokens/s = 78289 (34484 source, 43805 target) ; Learning rate = 0.000734 ; Loss = 1.612192\n",
      "2024-12-03 12:22:49.099000: I runner.py:310] Step = 14600 ; steps/s = 1.66, tokens/s = 77553 (34175 source, 43378 target) ; Learning rate = 0.000731 ; Loss = 1.587634\n",
      "2024-12-03 12:23:49.911000: I runner.py:310] Step = 14700 ; steps/s = 1.64, tokens/s = 78351 (34499 source, 43852 target) ; Learning rate = 0.000729 ; Loss = 1.595977\n",
      "2024-12-03 12:24:50.738000: I runner.py:310] Step = 14800 ; steps/s = 1.64, tokens/s = 78356 (34519 source, 43837 target) ; Learning rate = 0.000727 ; Loss = 1.591046\n",
      "2024-12-03 12:25:51.106000: I runner.py:310] Step = 14900 ; steps/s = 1.66, tokens/s = 77528 (34178 source, 43350 target) ; Learning rate = 0.000724 ; Loss = 1.577369\n",
      "2024-12-03 12:26:52.005000: I runner.py:310] Step = 15000 ; steps/s = 1.64, tokens/s = 78250 (34458 source, 43792 target) ; Learning rate = 0.000722 ; Loss = 1.584317\n",
      "2024-12-03 12:26:52.007000: I training.py:192] Running evaluation for step 15000\n",
      "2024-12-03 12:27:43.793000: I training.py:192] Evaluation result for step 15000: loss = 0.728384 ; perplexity = 2.071729\n",
      "2024-12-03 12:28:44.032000: I runner.py:310] Step = 15100 ; steps/s = 1.66, tokens/s = 77725 (34276 source, 43449 target) ; Learning rate = 0.000719 ; Loss = 1.579316\n",
      "2024-12-03 12:29:44.831000: I runner.py:310] Step = 15200 ; steps/s = 1.64, tokens/s = 78372 (34509 source, 43863 target) ; Learning rate = 0.000717 ; Loss = 1.583722\n",
      "2024-12-03 12:30:45.651000: I runner.py:310] Step = 15300 ; steps/s = 1.64, tokens/s = 78367 (34521 source, 43846 target) ; Learning rate = 0.000715 ; Loss = 1.588241\n",
      "2024-12-03 12:31:45.999000: I runner.py:310] Step = 15400 ; steps/s = 1.66, tokens/s = 77563 (34196 source, 43367 target) ; Learning rate = 0.000712 ; Loss = 1.577528\n",
      "2024-12-03 12:32:46.818000: I runner.py:310] Step = 15500 ; steps/s = 1.64, tokens/s = 78341 (34488 source, 43853 target) ; Learning rate = 0.000710 ; Loss = 1.577448\n",
      "2024-12-03 12:33:47.289000: I runner.py:310] Step = 15600 ; steps/s = 1.65, tokens/s = 77432 (34156 source, 43276 target) ; Learning rate = 0.000708 ; Loss = 1.562692\n",
      "2024-12-03 12:34:48.101000: I runner.py:310] Step = 15700 ; steps/s = 1.64, tokens/s = 78350 (34489 source, 43861 target) ; Learning rate = 0.000705 ; Loss = 1.572473\n",
      "2024-12-03 12:35:49.037000: I runner.py:310] Step = 15800 ; steps/s = 1.64, tokens/s = 78206 (34450 source, 43756 target) ; Learning rate = 0.000703 ; Loss = 1.584423\n",
      "2024-12-03 12:36:49.418000: I runner.py:310] Step = 15900 ; steps/s = 1.66, tokens/s = 77532 (34197 source, 43335 target) ; Learning rate = 0.000701 ; Loss = 1.571992\n",
      "2024-12-03 12:37:50.272000: I runner.py:310] Step = 16000 ; steps/s = 1.64, tokens/s = 78301 (34475 source, 43826 target) ; Learning rate = 0.000699 ; Loss = 1.585686\n",
      "2024-12-03 12:38:51.068000: I runner.py:310] Step = 16100 ; steps/s = 1.65, tokens/s = 78058 (34379 source, 43679 target) ; Learning rate = 0.000697 ; Loss = 1.617293\n",
      "2024-12-03 12:39:51.534000: I runner.py:310] Step = 16200 ; steps/s = 1.65, tokens/s = 77716 (34254 source, 43462 target) ; Learning rate = 0.000694 ; Loss = 1.554298\n",
      "2024-12-03 12:40:52.384000: I runner.py:310] Step = 16300 ; steps/s = 1.64, tokens/s = 78309 (34475 source, 43834 target) ; Learning rate = 0.000692 ; Loss = 1.569651\n",
      "2024-12-03 12:41:52.776000: I runner.py:310] Step = 16400 ; steps/s = 1.66, tokens/s = 77531 (34196 source, 43335 target) ; Learning rate = 0.000690 ; Loss = 1.566965\n",
      "2024-12-03 12:42:53.575000: I runner.py:310] Step = 16500 ; steps/s = 1.64, tokens/s = 78374 (34511 source, 43863 target) ; Learning rate = 0.000688 ; Loss = 1.573990\n",
      "2024-12-03 12:43:54.390000: I runner.py:310] Step = 16600 ; steps/s = 1.64, tokens/s = 78341 (34494 source, 43847 target) ; Learning rate = 0.000686 ; Loss = 1.564729\n",
      "2024-12-03 12:44:54.759000: I runner.py:310] Step = 16700 ; steps/s = 1.66, tokens/s = 77529 (34173 source, 43356 target) ; Learning rate = 0.000684 ; Loss = 1.564133\n",
      "2024-12-03 12:45:55.590000: I runner.py:310] Step = 16800 ; steps/s = 1.64, tokens/s = 78339 (34515 source, 43824 target) ; Learning rate = 0.000682 ; Loss = 1.564002\n",
      "2024-12-03 12:46:55.961000: I runner.py:310] Step = 16900 ; steps/s = 1.66, tokens/s = 77553 (34200 source, 43353 target) ; Learning rate = 0.000680 ; Loss = 1.552443\n",
      "2024-12-03 12:47:56.762000: I runner.py:310] Step = 17000 ; steps/s = 1.64, tokens/s = 78349 (34483 source, 43866 target) ; Learning rate = 0.000678 ; Loss = 1.547689\n",
      "2024-12-03 12:48:57.612000: I runner.py:310] Step = 17100 ; steps/s = 1.64, tokens/s = 78325 (34510 source, 43815 target) ; Learning rate = 0.000676 ; Loss = 1.564257\n",
      "2024-12-03 12:49:58.019000: I runner.py:310] Step = 17200 ; steps/s = 1.66, tokens/s = 77503 (34182 source, 43321 target) ; Learning rate = 0.000674 ; Loss = 1.546764\n",
      "2024-12-03 12:50:58.889000: I runner.py:310] Step = 17300 ; steps/s = 1.64, tokens/s = 78308 (34492 source, 43816 target) ; Learning rate = 0.000672 ; Loss = 1.560840\n",
      "2024-12-03 12:51:59.446000: I runner.py:310] Step = 17400 ; steps/s = 1.65, tokens/s = 77889 (34335 source, 43554 target) ; Learning rate = 0.000670 ; Loss = 1.670623\n",
      "2024-12-03 12:53:00.103000: I runner.py:310] Step = 17500 ; steps/s = 1.65, tokens/s = 77924 (34300 source, 43624 target) ; Learning rate = 0.000668 ; Loss = 1.541503\n",
      "2024-12-03 12:54:00.993000: I runner.py:310] Step = 17600 ; steps/s = 1.64, tokens/s = 78283 (34485 source, 43798 target) ; Learning rate = 0.000666 ; Loss = 1.560851\n",
      "2024-12-03 12:55:01.371000: I runner.py:310] Step = 17700 ; steps/s = 1.66, tokens/s = 77514 (34174 source, 43340 target) ; Learning rate = 0.000664 ; Loss = 1.542728\n",
      "2024-12-03 12:56:02.224000: I runner.py:310] Step = 17800 ; steps/s = 1.64, tokens/s = 78327 (34497 source, 43830 target) ; Learning rate = 0.000662 ; Loss = 1.562445\n",
      "2024-12-03 12:57:03.068000: I runner.py:310] Step = 17900 ; steps/s = 1.64, tokens/s = 78310 (34478 source, 43832 target) ; Learning rate = 0.000661 ; Loss = 1.551262\n",
      "2024-12-03 12:58:03.501000: I runner.py:310] Step = 18000 ; steps/s = 1.65, tokens/s = 77451 (34154 source, 43297 target) ; Learning rate = 0.000659 ; Loss = 1.548207\n",
      "2024-12-03 12:59:04.306000: I runner.py:310] Step = 18100 ; steps/s = 1.64, tokens/s = 78356 (34505 source, 43851 target) ; Learning rate = 0.000657 ; Loss = 1.553454\n",
      "2024-12-03 13:00:04.647000: I runner.py:310] Step = 18200 ; steps/s = 1.66, tokens/s = 77600 (34222 source, 43378 target) ; Learning rate = 0.000655 ; Loss = 1.534223\n",
      "2024-12-03 13:01:05.471000: I runner.py:310] Step = 18300 ; steps/s = 1.64, tokens/s = 78319 (34481 source, 43838 target) ; Learning rate = 0.000653 ; Loss = 1.547936\n",
      "2024-12-03 13:02:06.297000: I runner.py:310] Step = 18400 ; steps/s = 1.64, tokens/s = 78371 (34525 source, 43846 target) ; Learning rate = 0.000652 ; Loss = 1.561727\n",
      "2024-12-03 13:03:06.718000: I runner.py:310] Step = 18500 ; steps/s = 1.66, tokens/s = 77456 (34142 source, 43314 target) ; Learning rate = 0.000650 ; Loss = 1.539460\n",
      "2024-12-03 13:04:07.612000: I runner.py:310] Step = 18600 ; steps/s = 1.64, tokens/s = 78237 (34443 source, 43794 target) ; Learning rate = 0.000648 ; Loss = 1.542830\n",
      "2024-12-03 13:05:08.047000: I runner.py:310] Step = 18700 ; steps/s = 1.65, tokens/s = 77494 (34191 source, 43303 target) ; Learning rate = 0.000646 ; Loss = 1.534454\n",
      "2024-12-03 13:06:08.912000: I runner.py:310] Step = 18800 ; steps/s = 1.64, tokens/s = 78290 (34466 source, 43824 target) ; Learning rate = 0.000645 ; Loss = 1.539073\n",
      "2024-12-03 13:07:09.760000: I runner.py:310] Step = 18900 ; steps/s = 1.64, tokens/s = 78307 (34487 source, 43820 target) ; Learning rate = 0.000643 ; Loss = 1.543234\n",
      "2024-12-03 13:08:10.151000: I runner.py:310] Step = 19000 ; steps/s = 1.66, tokens/s = 77505 (34170 source, 43335 target) ; Learning rate = 0.000641 ; Loss = 1.530808\n",
      "2024-12-03 13:09:10.988000: I runner.py:310] Step = 19100 ; steps/s = 1.64, tokens/s = 78344 (34501 source, 43843 target) ; Learning rate = 0.000640 ; Loss = 1.548095\n",
      "2024-12-03 13:10:11.869000: I runner.py:310] Step = 19200 ; steps/s = 1.64, tokens/s = 78253 (34458 source, 43795 target) ; Learning rate = 0.000638 ; Loss = 1.538248\n",
      "2024-12-03 13:11:12.256000: I runner.py:310] Step = 19300 ; steps/s = 1.66, tokens/s = 77524 (34188 source, 43336 target) ; Learning rate = 0.000636 ; Loss = 1.530617\n",
      "2024-12-03 13:12:13.078000: I runner.py:310] Step = 19400 ; steps/s = 1.64, tokens/s = 78338 (34487 source, 43851 target) ; Learning rate = 0.000635 ; Loss = 1.540295\n",
      "2024-12-03 13:13:13.476000: I runner.py:310] Step = 19500 ; steps/s = 1.66, tokens/s = 77501 (34179 source, 43322 target) ; Learning rate = 0.000633 ; Loss = 1.517802\n",
      "2024-12-03 13:14:14.347000: I runner.py:310] Step = 19600 ; steps/s = 1.64, tokens/s = 78309 (34500 source, 43809 target) ; Learning rate = 0.000631 ; Loss = 1.543573\n",
      "2024-12-03 13:15:15.251000: I runner.py:310] Step = 19700 ; steps/s = 1.64, tokens/s = 78228 (34439 source, 43789 target) ; Learning rate = 0.000630 ; Loss = 1.532823\n",
      "2024-12-03 13:16:15.632000: I runner.py:310] Step = 19800 ; steps/s = 1.66, tokens/s = 77511 (34164 source, 43347 target) ; Learning rate = 0.000628 ; Loss = 1.534586\n",
      "2024-12-03 13:17:16.508000: I runner.py:310] Step = 19900 ; steps/s = 1.64, tokens/s = 78270 (34477 source, 43793 target) ; Learning rate = 0.000627 ; Loss = 1.536698\n",
      "2024-12-03 13:18:16.889000: I runner.py:310] Step = 20000 ; steps/s = 1.66, tokens/s = 77546 (34198 source, 43348 target) ; Learning rate = 0.000625 ; Loss = 1.529227\n",
      "2024-12-03 13:18:18.577000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-20000\n",
      "2024-12-03 13:18:18.577000: I training.py:192] Running evaluation for step 20000\n",
      "2024-12-03 13:19:30.712000: I training.py:192] Evaluation result for step 20000: loss = 0.754346 ; perplexity = 2.126220\n",
      "2024-12-03 13:20:31.468000: I runner.py:310] Step = 20100 ; steps/s = 1.65, tokens/s = 78388 (34490 source, 43898 target) ; Learning rate = 0.000623 ; Loss = 1.526916\n",
      "2024-12-03 13:21:32.346000: I runner.py:310] Step = 20200 ; steps/s = 1.64, tokens/s = 78306 (34511 source, 43795 target) ; Learning rate = 0.000622 ; Loss = 1.536738\n",
      "2024-12-03 13:22:32.771000: I runner.py:310] Step = 20300 ; steps/s = 1.66, tokens/s = 77481 (34164 source, 43317 target) ; Learning rate = 0.000620 ; Loss = 1.523135\n",
      "2024-12-03 13:23:33.693000: I runner.py:310] Step = 20400 ; steps/s = 1.64, tokens/s = 78213 (34447 source, 43766 target) ; Learning rate = 0.000619 ; Loss = 1.532703\n",
      "2024-12-03 13:24:34.573000: I runner.py:310] Step = 20500 ; steps/s = 1.64, tokens/s = 78267 (34454 source, 43813 target) ; Learning rate = 0.000617 ; Loss = 1.536363\n",
      "2024-12-03 13:25:35.016000: I runner.py:310] Step = 20600 ; steps/s = 1.65, tokens/s = 77439 (34140 source, 43299 target) ; Learning rate = 0.000616 ; Loss = 1.527865\n",
      "2024-12-03 13:26:35.916000: I runner.py:310] Step = 20700 ; steps/s = 1.64, tokens/s = 78228 (34443 source, 43785 target) ; Learning rate = 0.000614 ; Loss = 1.521533\n",
      "2024-12-03 13:27:36.303000: I runner.py:310] Step = 20800 ; steps/s = 1.66, tokens/s = 77544 (34208 source, 43336 target) ; Learning rate = 0.000613 ; Loss = 1.531386\n",
      "2024-12-03 13:28:37.217000: I runner.py:310] Step = 20900 ; steps/s = 1.64, tokens/s = 78197 (34416 source, 43781 target) ; Learning rate = 0.000611 ; Loss = 1.518898\n",
      "2024-12-03 13:29:38.054000: I runner.py:310] Step = 21000 ; steps/s = 1.64, tokens/s = 78363 (34530 source, 43833 target) ; Learning rate = 0.000610 ; Loss = 1.535712\n",
      "2024-12-03 13:30:38.409000: I runner.py:310] Step = 21100 ; steps/s = 1.66, tokens/s = 77545 (34184 source, 43361 target) ; Learning rate = 0.000608 ; Loss = 1.525749\n",
      "2024-12-03 13:31:39.305000: I runner.py:310] Step = 21200 ; steps/s = 1.64, tokens/s = 78242 (34460 source, 43782 target) ; Learning rate = 0.000607 ; Loss = 1.527441\n",
      "2024-12-03 13:32:39.761000: I runner.py:310] Step = 21300 ; steps/s = 1.65, tokens/s = 77462 (34161 source, 43301 target) ; Learning rate = 0.000606 ; Loss = 1.523138\n",
      "2024-12-03 13:33:40.686000: I runner.py:310] Step = 21400 ; steps/s = 1.64, tokens/s = 78190 (34426 source, 43764 target) ; Learning rate = 0.000604 ; Loss = 1.516189\n",
      "2024-12-03 13:34:41.636000: I runner.py:310] Step = 21500 ; steps/s = 1.64, tokens/s = 78187 (34437 source, 43750 target) ; Learning rate = 0.000603 ; Loss = 1.523809\n",
      "2024-12-03 13:35:42.040000: I runner.py:310] Step = 21600 ; steps/s = 1.66, tokens/s = 77513 (34170 source, 43343 target) ; Learning rate = 0.000601 ; Loss = 1.515780\n",
      "2024-12-03 13:36:42.920000: I runner.py:310] Step = 21700 ; steps/s = 1.64, tokens/s = 78274 (34480 source, 43794 target) ; Learning rate = 0.000600 ; Loss = 1.531470\n",
      "2024-12-03 13:37:43.836000: I runner.py:310] Step = 21800 ; steps/s = 1.64, tokens/s = 78229 (34447 source, 43782 target) ; Learning rate = 0.000599 ; Loss = 1.527496\n",
      "2024-12-03 13:38:44.217000: I runner.py:310] Step = 21900 ; steps/s = 1.66, tokens/s = 77475 (34143 source, 43332 target) ; Learning rate = 0.000597 ; Loss = 1.512883\n",
      "2024-12-03 13:39:45.140000: I runner.py:310] Step = 22000 ; steps/s = 1.64, tokens/s = 78227 (34455 source, 43772 target) ; Learning rate = 0.000596 ; Loss = 1.535337\n",
      "2024-12-03 13:40:45.498000: I runner.py:310] Step = 22100 ; steps/s = 1.66, tokens/s = 77583 (34214 source, 43369 target) ; Learning rate = 0.000595 ; Loss = 1.512429\n",
      "2024-12-03 13:41:46.441000: I runner.py:310] Step = 22200 ; steps/s = 1.64, tokens/s = 78169 (34409 source, 43760 target) ; Learning rate = 0.000593 ; Loss = 1.522588\n",
      "2024-12-03 13:42:47.279000: I runner.py:310] Step = 22300 ; steps/s = 1.64, tokens/s = 78347 (34521 source, 43826 target) ; Learning rate = 0.000592 ; Loss = 1.520477\n",
      "2024-12-03 13:43:47.777000: I runner.py:310] Step = 22400 ; steps/s = 1.65, tokens/s = 77368 (34094 source, 43274 target) ; Learning rate = 0.000591 ; Loss = 1.516766\n",
      "2024-12-03 13:44:48.658000: I runner.py:310] Step = 22500 ; steps/s = 1.64, tokens/s = 78281 (34482 source, 43799 target) ; Learning rate = 0.000589 ; Loss = 1.510644\n",
      "2024-12-03 13:45:49.119000: I runner.py:310] Step = 22600 ; steps/s = 1.65, tokens/s = 77421 (34144 source, 43277 target) ; Learning rate = 0.000588 ; Loss = 1.510906\n",
      "2024-12-03 13:46:50.030000: I runner.py:310] Step = 22700 ; steps/s = 1.64, tokens/s = 78225 (34446 source, 43779 target) ; Learning rate = 0.000587 ; Loss = 1.519929\n",
      "2024-12-03 13:47:50.911000: I runner.py:310] Step = 22800 ; steps/s = 1.64, tokens/s = 78289 (34494 source, 43795 target) ; Learning rate = 0.000585 ; Loss = 1.523511\n",
      "2024-12-03 13:48:51.300000: I runner.py:310] Step = 22900 ; steps/s = 1.66, tokens/s = 77485 (34130 source, 43355 target) ; Learning rate = 0.000584 ; Loss = 1.515119\n",
      "2024-12-03 13:49:52.105000: I runner.py:310] Step = 23000 ; steps/s = 1.64, tokens/s = 78394 (34539 source, 43855 target) ; Learning rate = 0.000583 ; Loss = 1.532320\n",
      "2024-12-03 13:50:53.012000: I runner.py:310] Step = 23100 ; steps/s = 1.64, tokens/s = 78223 (34446 source, 43777 target) ; Learning rate = 0.000582 ; Loss = 1.517577\n",
      "2024-12-03 13:51:53.439000: I runner.py:310] Step = 23200 ; steps/s = 1.66, tokens/s = 77447 (34137 source, 43310 target) ; Learning rate = 0.000580 ; Loss = 1.514105\n",
      "2024-12-03 13:52:54.234000: I runner.py:310] Step = 23300 ; steps/s = 1.65, tokens/s = 78387 (34531 source, 43856 target) ; Learning rate = 0.000579 ; Loss = 1.514374\n",
      "2024-12-03 13:53:54.658000: I runner.py:310] Step = 23400 ; steps/s = 1.66, tokens/s = 77478 (34154 source, 43324 target) ; Learning rate = 0.000578 ; Loss = 1.512581\n",
      "2024-12-03 13:54:55.538000: I runner.py:310] Step = 23500 ; steps/s = 1.64, tokens/s = 78254 (34457 source, 43797 target) ; Learning rate = 0.000577 ; Loss = 1.515517\n",
      "2024-12-03 13:55:56.438000: I runner.py:310] Step = 23600 ; steps/s = 1.64, tokens/s = 78271 (34484 source, 43787 target) ; Learning rate = 0.000575 ; Loss = 1.519381\n",
      "2024-12-03 13:56:56.851000: I runner.py:310] Step = 23700 ; steps/s = 1.66, tokens/s = 77478 (34163 source, 43315 target) ; Learning rate = 0.000574 ; Loss = 1.503782\n",
      "2024-12-03 13:57:57.704000: I runner.py:310] Step = 23800 ; steps/s = 1.64, tokens/s = 78306 (34485 source, 43821 target) ; Learning rate = 0.000573 ; Loss = 1.513791\n",
      "2024-12-03 13:58:58.064000: I runner.py:310] Step = 23900 ; steps/s = 1.66, tokens/s = 77561 (34198 source, 43363 target) ; Learning rate = 0.000572 ; Loss = 1.498964\n",
      "2024-12-03 13:59:58.913000: I runner.py:310] Step = 24000 ; steps/s = 1.64, tokens/s = 78301 (34472 source, 43829 target) ; Learning rate = 0.000571 ; Loss = 1.513853\n",
      "2024-12-03 14:00:59.760000: I runner.py:310] Step = 24100 ; steps/s = 1.64, tokens/s = 78334 (34511 source, 43823 target) ; Learning rate = 0.000569 ; Loss = 1.513583\n",
      "2024-12-03 14:02:00.167000: I runner.py:310] Step = 24200 ; steps/s = 1.66, tokens/s = 77440 (34120 source, 43320 target) ; Learning rate = 0.000568 ; Loss = 1.505180\n",
      "2024-12-03 14:03:01.003000: I runner.py:310] Step = 24300 ; steps/s = 1.64, tokens/s = 78346 (34508 source, 43838 target) ; Learning rate = 0.000567 ; Loss = 1.512835\n",
      "2024-12-03 14:04:01.930000: I runner.py:310] Step = 24400 ; steps/s = 1.64, tokens/s = 78217 (34452 source, 43765 target) ; Learning rate = 0.000566 ; Loss = 1.512344\n",
      "2024-12-03 14:05:02.398000: I runner.py:310] Step = 24500 ; steps/s = 1.65, tokens/s = 77400 (34120 source, 43280 target) ; Learning rate = 0.000565 ; Loss = 1.505112\n",
      "2024-12-03 14:06:03.265000: I runner.py:310] Step = 24600 ; steps/s = 1.64, tokens/s = 78283 (34473 source, 43810 target) ; Learning rate = 0.000564 ; Loss = 1.516024\n",
      "2024-12-03 14:07:03.625000: I runner.py:310] Step = 24700 ; steps/s = 1.66, tokens/s = 77577 (34210 source, 43367 target) ; Learning rate = 0.000562 ; Loss = 1.498920\n",
      "2024-12-03 14:08:04.464000: I runner.py:310] Step = 24800 ; steps/s = 1.64, tokens/s = 78335 (34501 source, 43834 target) ; Learning rate = 0.000561 ; Loss = 1.505460\n",
      "2024-12-03 14:09:05.354000: I runner.py:310] Step = 24900 ; steps/s = 1.64, tokens/s = 78251 (34457 source, 43794 target) ; Learning rate = 0.000560 ; Loss = 1.517753\n",
      "2024-12-03 14:10:05.813000: I runner.py:310] Step = 25000 ; steps/s = 1.65, tokens/s = 77414 (34136 source, 43278 target) ; Learning rate = 0.000559 ; Loss = 1.507256\n",
      "2024-12-03 14:10:05.814000: I training.py:192] Running evaluation for step 25000\n",
      "2024-12-03 14:10:54.300000: I training.py:192] Evaluation result for step 25000: loss = 0.766577 ; perplexity = 2.152386\n",
      "2024-12-03 14:11:55.027000: I runner.py:310] Step = 25100 ; steps/s = 1.65, tokens/s = 78474 (34558 source, 43916 target) ; Learning rate = 0.000558 ; Loss = 1.502029\n",
      "2024-12-03 14:12:55.435000: I runner.py:310] Step = 25200 ; steps/s = 1.66, tokens/s = 77503 (34170 source, 43333 target) ; Learning rate = 0.000557 ; Loss = 1.498993\n",
      "2024-12-03 14:13:56.630000: I runner.py:310] Step = 25300 ; steps/s = 1.63, tokens/s = 77869 (34292 source, 43577 target) ; Learning rate = 0.000556 ; Loss = 1.506263\n",
      "2024-12-03 14:14:58.903000: I runner.py:310] Step = 25400 ; steps/s = 1.61, tokens/s = 76534 (33719 source, 42815 target) ; Learning rate = 0.000555 ; Loss = 1.509061\n",
      "2024-12-03 14:15:59.574000: I runner.py:310] Step = 25500 ; steps/s = 1.65, tokens/s = 77136 (33984 source, 43152 target) ; Learning rate = 0.000553 ; Loss = 1.506445\n",
      "2024-12-03 14:17:01.209000: I runner.py:310] Step = 25600 ; steps/s = 1.62, tokens/s = 77316 (34048 source, 43268 target) ; Learning rate = 0.000552 ; Loss = 1.503051\n",
      "2024-12-03 14:18:02.478000: I runner.py:310] Step = 25700 ; steps/s = 1.63, tokens/s = 77793 (34276 source, 43517 target) ; Learning rate = 0.000551 ; Loss = 1.509203\n",
      "2024-12-03 14:19:03.221000: I runner.py:310] Step = 25800 ; steps/s = 1.65, tokens/s = 77060 (33965 source, 43095 target) ; Learning rate = 0.000550 ; Loss = 1.503378\n",
      "2024-12-03 14:20:04.408000: I runner.py:310] Step = 25900 ; steps/s = 1.63, tokens/s = 77893 (34312 source, 43581 target) ; Learning rate = 0.000549 ; Loss = 1.503301\n",
      "2024-12-03 14:21:05.028000: I runner.py:310] Step = 26000 ; steps/s = 1.65, tokens/s = 77197 (34028 source, 43169 target) ; Learning rate = 0.000548 ; Loss = 1.502292\n",
      "2024-12-03 14:22:06.190000: I runner.py:310] Step = 26100 ; steps/s = 1.64, tokens/s = 77900 (34297 source, 43603 target) ; Learning rate = 0.000547 ; Loss = 1.501456\n",
      "2024-12-03 14:23:07.340000: I runner.py:310] Step = 26200 ; steps/s = 1.64, tokens/s = 77932 (34324 source, 43608 target) ; Learning rate = 0.000546 ; Loss = 1.510072\n",
      "2024-12-03 14:24:07.713000: I runner.py:310] Step = 26300 ; steps/s = 1.66, tokens/s = 77527 (34181 source, 43346 target) ; Learning rate = 0.000545 ; Loss = 1.507510\n",
      "2024-12-03 14:25:08.584000: I runner.py:310] Step = 26400 ; steps/s = 1.64, tokens/s = 78301 (34490 source, 43811 target) ; Learning rate = 0.000544 ; Loss = 1.499493\n",
      "2024-12-03 14:26:09.012000: I runner.py:310] Step = 26500 ; steps/s = 1.66, tokens/s = 77474 (34165 source, 43309 target) ; Learning rate = 0.000543 ; Loss = 1.500534\n",
      "2024-12-03 14:27:09.832000: I runner.py:310] Step = 26600 ; steps/s = 1.64, tokens/s = 78344 (34490 source, 43854 target) ; Learning rate = 0.000542 ; Loss = 1.501360\n",
      "2024-12-03 14:28:10.728000: I runner.py:310] Step = 26700 ; steps/s = 1.64, tokens/s = 78276 (34493 source, 43783 target) ; Learning rate = 0.000541 ; Loss = 1.512068\n",
      "2024-12-03 14:29:11.125000: I runner.py:310] Step = 26800 ; steps/s = 1.66, tokens/s = 77459 (34130 source, 43329 target) ; Learning rate = 0.000540 ; Loss = 1.494636\n",
      "2024-12-03 14:30:11.935000: I runner.py:310] Step = 26900 ; steps/s = 1.64, tokens/s = 78368 (34521 source, 43847 target) ; Learning rate = 0.000539 ; Loss = 1.501099\n",
      "2024-12-03 14:31:12.815000: I runner.py:310] Step = 27000 ; steps/s = 1.64, tokens/s = 78279 (34468 source, 43811 target) ; Learning rate = 0.000538 ; Loss = 1.507291\n",
      "2024-12-03 14:32:13.267000: I runner.py:310] Step = 27100 ; steps/s = 1.65, tokens/s = 77443 (34150 source, 43293 target) ; Learning rate = 0.000537 ; Loss = 1.496508\n",
      "2024-12-03 14:33:14.150000: I runner.py:310] Step = 27200 ; steps/s = 1.64, tokens/s = 78260 (34463 source, 43797 target) ; Learning rate = 0.000536 ; Loss = 1.498637\n",
      "2024-12-03 14:34:14.558000: I runner.py:310] Step = 27300 ; steps/s = 1.66, tokens/s = 77488 (34160 source, 43328 target) ; Learning rate = 0.000535 ; Loss = 1.490666\n",
      "2024-12-03 14:35:15.411000: I runner.py:310] Step = 27400 ; steps/s = 1.64, tokens/s = 78280 (34458 source, 43822 target) ; Learning rate = 0.000534 ; Loss = 1.492513\n",
      "2024-12-03 14:36:16.271000: I runner.py:310] Step = 27500 ; steps/s = 1.64, tokens/s = 78330 (34510 source, 43820 target) ; Learning rate = 0.000533 ; Loss = 1.501134\n",
      "2024-12-03 14:37:16.689000: I runner.py:310] Step = 27600 ; steps/s = 1.66, tokens/s = 77500 (34180 source, 43320 target) ; Learning rate = 0.000532 ; Loss = 1.515301\n",
      "2024-12-03 14:38:17.566000: I runner.py:310] Step = 27700 ; steps/s = 1.64, tokens/s = 78260 (34461 source, 43799 target) ; Learning rate = 0.000531 ; Loss = 1.496595\n",
      "2024-12-03 14:39:17.959000: I runner.py:310] Step = 27800 ; steps/s = 1.66, tokens/s = 77512 (34179 source, 43333 target) ; Learning rate = 0.000530 ; Loss = 1.485165\n",
      "2024-12-03 14:40:18.898000: I runner.py:310] Step = 27900 ; steps/s = 1.64, tokens/s = 78191 (34423 source, 43768 target) ; Learning rate = 0.000529 ; Loss = 1.500084\n",
      "2024-12-03 14:41:19.794000: I runner.py:310] Step = 28000 ; steps/s = 1.64, tokens/s = 78265 (34480 source, 43785 target) ; Learning rate = 0.000528 ; Loss = 1.496511\n",
      "2024-12-03 14:42:20.220000: I runner.py:310] Step = 28100 ; steps/s = 1.66, tokens/s = 77445 (34134 source, 43311 target) ; Learning rate = 0.000527 ; Loss = 1.495043\n",
      "2024-12-03 14:43:21.096000: I runner.py:310] Step = 28200 ; steps/s = 1.64, tokens/s = 78305 (34496 source, 43809 target) ; Learning rate = 0.000526 ; Loss = 1.500110\n",
      "2024-12-03 14:44:21.973000: I runner.py:310] Step = 28300 ; steps/s = 1.64, tokens/s = 78266 (34465 source, 43801 target) ; Learning rate = 0.000525 ; Loss = 1.497353\n",
      "2024-12-03 14:45:22.432000: I runner.py:310] Step = 28400 ; steps/s = 1.65, tokens/s = 77431 (34137 source, 43294 target) ; Learning rate = 0.000524 ; Loss = 1.493190\n",
      "2024-12-03 14:46:23.266000: I runner.py:310] Step = 28500 ; steps/s = 1.64, tokens/s = 78327 (34492 source, 43835 target) ; Learning rate = 0.000524 ; Loss = 1.493142\n",
      "2024-12-03 14:47:23.758000: I runner.py:310] Step = 28600 ; steps/s = 1.65, tokens/s = 77371 (34099 source, 43272 target) ; Learning rate = 0.000523 ; Loss = 1.492441\n",
      "2024-12-03 14:48:24.717000: I runner.py:310] Step = 28700 ; steps/s = 1.64, tokens/s = 78150 (34407 source, 43743 target) ; Learning rate = 0.000522 ; Loss = 1.494555\n",
      "2024-12-03 14:49:25.601000: I runner.py:310] Step = 28800 ; steps/s = 1.64, tokens/s = 78288 (34488 source, 43800 target) ; Learning rate = 0.000521 ; Loss = 1.496099\n",
      "2024-12-03 14:50:26.019000: I runner.py:310] Step = 28900 ; steps/s = 1.66, tokens/s = 77463 (34161 source, 43302 target) ; Learning rate = 0.000520 ; Loss = 1.489799\n",
      "2024-12-03 14:51:26.925000: I runner.py:310] Step = 29000 ; steps/s = 1.64, tokens/s = 78245 (34456 source, 43789 target) ; Learning rate = 0.000519 ; Loss = 1.494922\n",
      "2024-12-03 14:52:27.324000: I runner.py:310] Step = 29100 ; steps/s = 1.66, tokens/s = 77520 (34189 source, 43331 target) ; Learning rate = 0.000518 ; Loss = 1.485000\n",
      "2024-12-03 14:53:28.263000: I runner.py:310] Step = 29200 ; steps/s = 1.64, tokens/s = 78201 (34439 source, 43762 target) ; Learning rate = 0.000517 ; Loss = 1.490597\n",
      "2024-12-03 14:54:29.050000: I runner.py:310] Step = 29300 ; steps/s = 1.65, tokens/s = 78372 (34499 source, 43873 target) ; Learning rate = 0.000516 ; Loss = 1.507904\n",
      "2024-12-03 14:55:29.513000: I runner.py:310] Step = 29400 ; steps/s = 1.65, tokens/s = 77406 (34128 source, 43278 target) ; Learning rate = 0.000515 ; Loss = 1.489289\n",
      "2024-12-03 14:56:30.377000: I runner.py:310] Step = 29500 ; steps/s = 1.64, tokens/s = 78314 (34506 source, 43808 target) ; Learning rate = 0.000515 ; Loss = 1.495923\n",
      "2024-12-03 14:57:31.280000: I runner.py:310] Step = 29600 ; steps/s = 1.64, tokens/s = 78249 (34450 source, 43799 target) ; Learning rate = 0.000514 ; Loss = 1.496904\n",
      "2024-12-03 14:58:31.672000: I runner.py:310] Step = 29700 ; steps/s = 1.66, tokens/s = 77503 (34161 source, 43342 target) ; Learning rate = 0.000513 ; Loss = 1.486568\n",
      "2024-12-03 14:59:32.525000: I runner.py:310] Step = 29800 ; steps/s = 1.64, tokens/s = 78325 (34502 source, 43823 target) ; Learning rate = 0.000512 ; Loss = 1.493248\n",
      "2024-12-03 15:00:32.938000: I runner.py:310] Step = 29900 ; steps/s = 1.66, tokens/s = 77462 (34143 source, 43319 target) ; Learning rate = 0.000511 ; Loss = 1.490471\n",
      "2024-12-03 15:01:33.823000: I runner.py:310] Step = 30000 ; steps/s = 1.64, tokens/s = 78240 (34447 source, 43793 target) ; Learning rate = 0.000510 ; Loss = 1.496819\n",
      "2024-12-03 15:01:35.530000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-30000\n",
      "2024-12-03 15:01:35.530000: I training.py:192] Running evaluation for step 30000\n",
      "2024-12-03 15:02:23.172000: I training.py:192] Evaluation result for step 30000: loss = 0.778465 ; perplexity = 2.178125\n",
      "2024-12-03 15:03:24.070000: I runner.py:310] Step = 30100 ; steps/s = 1.64, tokens/s = 78295 (34492 source, 43803 target) ; Learning rate = 0.000509 ; Loss = 1.488207\n",
      "2024-12-03 15:04:24.525000: I runner.py:310] Step = 30200 ; steps/s = 1.65, tokens/s = 77429 (34145 source, 43284 target) ; Learning rate = 0.000509 ; Loss = 1.478719\n",
      "2024-12-03 15:05:25.489000: I runner.py:310] Step = 30300 ; steps/s = 1.64, tokens/s = 78183 (34441 source, 43742 target) ; Learning rate = 0.000508 ; Loss = 1.490478\n",
      "2024-12-03 15:06:25.923000: I runner.py:310] Step = 30400 ; steps/s = 1.65, tokens/s = 77437 (34134 source, 43303 target) ; Learning rate = 0.000507 ; Loss = 1.484127\n",
      "2024-12-03 15:07:26.827000: I runner.py:310] Step = 30500 ; steps/s = 1.64, tokens/s = 78214 (34427 source, 43787 target) ; Learning rate = 0.000506 ; Loss = 1.485528\n",
      "2024-12-03 15:08:27.740000: I runner.py:310] Step = 30600 ; steps/s = 1.64, tokens/s = 78260 (34478 source, 43782 target) ; Learning rate = 0.000505 ; Loss = 1.492641\n",
      "2024-12-03 15:09:28.153000: I runner.py:310] Step = 30700 ; steps/s = 1.66, tokens/s = 77484 (34160 source, 43324 target) ; Learning rate = 0.000504 ; Loss = 1.488775\n",
      "2024-12-03 15:10:29.030000: I runner.py:310] Step = 30800 ; steps/s = 1.64, tokens/s = 78269 (34465 source, 43804 target) ; Learning rate = 0.000504 ; Loss = 1.486010\n",
      "2024-12-03 15:11:29.912000: I runner.py:310] Step = 30900 ; steps/s = 1.64, tokens/s = 78275 (34478 source, 43797 target) ; Learning rate = 0.000503 ; Loss = 1.492909\n",
      "2024-12-03 15:12:30.338000: I runner.py:310] Step = 31000 ; steps/s = 1.66, tokens/s = 77480 (34167 source, 43313 target) ; Learning rate = 0.000502 ; Loss = 1.489892\n",
      "2024-12-03 15:13:31.218000: I runner.py:310] Step = 31100 ; steps/s = 1.64, tokens/s = 78273 (34471 source, 43802 target) ; Learning rate = 0.000501 ; Loss = 1.491163\n",
      "2024-12-03 15:14:31.624000: I runner.py:310] Step = 31200 ; steps/s = 1.66, tokens/s = 77463 (34145 source, 43318 target) ; Learning rate = 0.000500 ; Loss = 1.479422\n",
      "2024-12-03 15:15:32.437000: I runner.py:310] Step = 31300 ; steps/s = 1.64, tokens/s = 78341 (34488 source, 43853 target) ; Learning rate = 0.000500 ; Loss = 1.485173\n",
      "2024-12-03 15:16:33.317000: I runner.py:310] Step = 31400 ; steps/s = 1.64, tokens/s = 78302 (34499 source, 43803 target) ; Learning rate = 0.000499 ; Loss = 1.487228\n",
      "2024-12-03 15:17:33.790000: I runner.py:310] Step = 31500 ; steps/s = 1.65, tokens/s = 77408 (34127 source, 43281 target) ; Learning rate = 0.000498 ; Loss = 1.484346\n",
      "2024-12-03 15:18:34.687000: I runner.py:310] Step = 31600 ; steps/s = 1.64, tokens/s = 78251 (34463 source, 43788 target) ; Learning rate = 0.000497 ; Loss = 1.486838\n",
      "2024-12-03 15:19:35.154000: I runner.py:310] Step = 31700 ; steps/s = 1.65, tokens/s = 77429 (34142 source, 43287 target) ; Learning rate = 0.000496 ; Loss = 1.480335\n",
      "2024-12-03 15:20:36.057000: I runner.py:310] Step = 31800 ; steps/s = 1.64, tokens/s = 78236 (34442 source, 43794 target) ; Learning rate = 0.000496 ; Loss = 1.481389\n",
      "2024-12-03 15:21:36.955000: I runner.py:310] Step = 31900 ; steps/s = 1.64, tokens/s = 78261 (34476 source, 43785 target) ; Learning rate = 0.000495 ; Loss = 1.489833\n",
      "2024-12-03 15:22:37.392000: I runner.py:310] Step = 32000 ; steps/s = 1.65, tokens/s = 77392 (34095 source, 43297 target) ; Learning rate = 0.000494 ; Loss = 1.483589\n",
      "2024-12-03 15:23:38.325000: I runner.py:310] Step = 32100 ; steps/s = 1.64, tokens/s = 78206 (34441 source, 43765 target) ; Learning rate = 0.000493 ; Loss = 1.490070\n",
      "2024-12-03 15:24:39.198000: I runner.py:310] Step = 32200 ; steps/s = 1.64, tokens/s = 78318 (34511 source, 43807 target) ; Learning rate = 0.000493 ; Loss = 1.492491\n",
      "2024-12-03 15:25:39.636000: I runner.py:310] Step = 32300 ; steps/s = 1.65, tokens/s = 77440 (34134 source, 43306 target) ; Learning rate = 0.000492 ; Loss = 1.481054\n",
      "2024-12-03 15:26:40.537000: I runner.py:310] Step = 32400 ; steps/s = 1.64, tokens/s = 78253 (34467 source, 43786 target) ; Learning rate = 0.000491 ; Loss = 1.480773\n",
      "2024-12-03 15:27:40.967000: I runner.py:310] Step = 32500 ; steps/s = 1.66, tokens/s = 77460 (34148 source, 43312 target) ; Learning rate = 0.000490 ; Loss = 1.479760\n",
      "2024-12-03 15:28:41.869000: I runner.py:310] Step = 32600 ; steps/s = 1.64, tokens/s = 78264 (34484 source, 43780 target) ; Learning rate = 0.000490 ; Loss = 1.485723\n",
      "2024-12-03 15:29:42.791000: I runner.py:310] Step = 32700 ; steps/s = 1.64, tokens/s = 78216 (34437 source, 43779 target) ; Learning rate = 0.000489 ; Loss = 1.485565\n",
      "2024-12-03 15:30:43.123000: I runner.py:310] Step = 32800 ; steps/s = 1.66, tokens/s = 77558 (34193 source, 43365 target) ; Learning rate = 0.000488 ; Loss = 1.485804\n",
      "2024-12-03 15:31:44.063000: I runner.py:310] Step = 32900 ; steps/s = 1.64, tokens/s = 78201 (34431 source, 43770 target) ; Learning rate = 0.000487 ; Loss = 1.480496\n",
      "2024-12-03 15:32:44.503000: I runner.py:310] Step = 33000 ; steps/s = 1.65, tokens/s = 77481 (34177 source, 43304 target) ; Learning rate = 0.000487 ; Loss = 1.478919\n",
      "2024-12-03 15:33:45.427000: I runner.py:310] Step = 33100 ; steps/s = 1.64, tokens/s = 78216 (34442 source, 43774 target) ; Learning rate = 0.000486 ; Loss = 1.482245\n",
      "2024-12-03 15:34:46.350000: I runner.py:310] Step = 33200 ; steps/s = 1.64, tokens/s = 78209 (34434 source, 43775 target) ; Learning rate = 0.000485 ; Loss = 1.485186\n",
      "2024-12-03 15:35:46.876000: I runner.py:310] Step = 33300 ; steps/s = 1.65, tokens/s = 77328 (34094 source, 43234 target) ; Learning rate = 0.000484 ; Loss = 1.481072\n",
      "2024-12-03 15:36:47.740000: I runner.py:310] Step = 33400 ; steps/s = 1.64, tokens/s = 78285 (34465 source, 43820 target) ; Learning rate = 0.000484 ; Loss = 1.484295\n",
      "2024-12-03 15:37:48.623000: I runner.py:310] Step = 33500 ; steps/s = 1.64, tokens/s = 78214 (34459 source, 43755 target) ; Learning rate = 0.000483 ; Loss = 1.491119\n",
      "2024-12-03 15:38:49.062000: I runner.py:310] Step = 33600 ; steps/s = 1.65, tokens/s = 77527 (34182 source, 43345 target) ; Learning rate = 0.000482 ; Loss = 1.477859\n",
      "2024-12-03 15:39:50.003000: I runner.py:310] Step = 33700 ; steps/s = 1.64, tokens/s = 78204 (34445 source, 43759 target) ; Learning rate = 0.000481 ; Loss = 1.482574\n",
      "2024-12-03 15:40:50.447000: I runner.py:310] Step = 33800 ; steps/s = 1.65, tokens/s = 77427 (34128 source, 43299 target) ; Learning rate = 0.000481 ; Loss = 1.477155\n",
      "2024-12-03 15:41:51.373000: I runner.py:310] Step = 33900 ; steps/s = 1.64, tokens/s = 78222 (34455 source, 43767 target) ; Learning rate = 0.000480 ; Loss = 1.472035\n",
      "2024-12-03 15:42:52.241000: I runner.py:310] Step = 34000 ; steps/s = 1.64, tokens/s = 78274 (34462 source, 43812 target) ; Learning rate = 0.000479 ; Loss = 1.487602\n",
      "2024-12-03 15:43:52.691000: I runner.py:310] Step = 34100 ; steps/s = 1.65, tokens/s = 77462 (34160 source, 43302 target) ; Learning rate = 0.000479 ; Loss = 1.475765\n",
      "2024-12-03 15:44:53.598000: I runner.py:310] Step = 34200 ; steps/s = 1.64, tokens/s = 78246 (34465 source, 43781 target) ; Learning rate = 0.000478 ; Loss = 1.481018\n",
      "2024-12-03 15:45:54.028000: I runner.py:310] Step = 34300 ; steps/s = 1.65, tokens/s = 77429 (34129 source, 43300 target) ; Learning rate = 0.000477 ; Loss = 1.473426\n",
      "2024-12-03 15:46:54.959000: I runner.py:310] Step = 34400 ; steps/s = 1.64, tokens/s = 78202 (34435 source, 43767 target) ; Learning rate = 0.000477 ; Loss = 1.472354\n",
      "2024-12-03 15:47:55.913000: I runner.py:310] Step = 34500 ; steps/s = 1.64, tokens/s = 78202 (34451 source, 43751 target) ; Learning rate = 0.000476 ; Loss = 1.477530\n",
      "2024-12-03 15:48:56.304000: I runner.py:310] Step = 34600 ; steps/s = 1.66, tokens/s = 77470 (34138 source, 43332 target) ; Learning rate = 0.000475 ; Loss = 1.477021\n",
      "2024-12-03 15:49:57.222000: I runner.py:310] Step = 34700 ; steps/s = 1.64, tokens/s = 78239 (34465 source, 43774 target) ; Learning rate = 0.000474 ; Loss = 1.481099\n",
      "2024-12-03 15:50:58.039000: I runner.py:310] Step = 34800 ; steps/s = 1.64, tokens/s = 77979 (34374 source, 43605 target) ; Learning rate = 0.000474 ; Loss = 1.538206\n",
      "2024-12-03 15:51:58.600000: I runner.py:310] Step = 34900 ; steps/s = 1.65, tokens/s = 77698 (34231 source, 43467 target) ; Learning rate = 0.000473 ; Loss = 1.473372\n",
      "2024-12-03 15:52:59.471000: I runner.py:310] Step = 35000 ; steps/s = 1.64, tokens/s = 78277 (34477 source, 43800 target) ; Learning rate = 0.000472 ; Loss = 1.477758\n",
      "2024-12-03 15:52:59.472000: I training.py:192] Running evaluation for step 35000\n",
      "2024-12-03 15:53:46.580000: I training.py:192] Evaluation result for step 35000: loss = 0.791423 ; perplexity = 2.206535\n",
      "2024-12-03 15:54:47.003000: I runner.py:310] Step = 35100 ; steps/s = 1.66, tokens/s = 77468 (34144 source, 43324 target) ; Learning rate = 0.000472 ; Loss = 1.478804\n",
      "2024-12-03 15:55:47.898000: I runner.py:310] Step = 35200 ; steps/s = 1.64, tokens/s = 78226 (34426 source, 43800 target) ; Learning rate = 0.000471 ; Loss = 1.478886\n",
      "2024-12-03 15:56:48.779000: I runner.py:310] Step = 35300 ; steps/s = 1.64, tokens/s = 78289 (34494 source, 43795 target) ; Learning rate = 0.000470 ; Loss = 1.476949\n",
      "2024-12-03 15:57:49.212000: I runner.py:310] Step = 35400 ; steps/s = 1.65, tokens/s = 77467 (34169 source, 43298 target) ; Learning rate = 0.000470 ; Loss = 1.479280\n",
      "2024-12-03 15:58:50.104000: I runner.py:310] Step = 35500 ; steps/s = 1.64, tokens/s = 78280 (34476 source, 43804 target) ; Learning rate = 0.000469 ; Loss = 1.478092\n",
      "2024-12-03 15:59:50.516000: I runner.py:310] Step = 35600 ; steps/s = 1.66, tokens/s = 77475 (34161 source, 43314 target) ; Learning rate = 0.000468 ; Loss = 1.473105\n",
      "2024-12-03 16:00:51.445000: I runner.py:310] Step = 35700 ; steps/s = 1.64, tokens/s = 78195 (34429 source, 43766 target) ; Learning rate = 0.000468 ; Loss = 1.480253\n",
      "2024-12-03 16:01:52.349000: I runner.py:310] Step = 35800 ; steps/s = 1.64, tokens/s = 78253 (34470 source, 43783 target) ; Learning rate = 0.000467 ; Loss = 1.481433\n",
      "2024-12-03 16:02:52.830000: I runner.py:310] Step = 35900 ; steps/s = 1.65, tokens/s = 77392 (34109 source, 43283 target) ; Learning rate = 0.000466 ; Loss = 1.475323\n",
      "2024-12-03 16:03:53.745000: I runner.py:310] Step = 36000 ; steps/s = 1.64, tokens/s = 78240 (34461 source, 43779 target) ; Learning rate = 0.000466 ; Loss = 1.481682\n",
      "2024-12-03 16:04:54.287000: I runner.py:310] Step = 36100 ; steps/s = 1.65, tokens/s = 77530 (34193 source, 43337 target) ; Learning rate = 0.000465 ; Loss = 1.508831\n",
      "2024-12-03 16:05:55.157000: I runner.py:310] Step = 36200 ; steps/s = 1.64, tokens/s = 78071 (34377 source, 43694 target) ; Learning rate = 0.000465 ; Loss = 1.473966\n",
      "2024-12-03 16:06:56.121000: I runner.py:310] Step = 36300 ; steps/s = 1.64, tokens/s = 78145 (34407 source, 43738 target) ; Learning rate = 0.000464 ; Loss = 1.471267\n",
      "2024-12-03 16:07:56.582000: I runner.py:310] Step = 36400 ; steps/s = 1.65, tokens/s = 77416 (34132 source, 43284 target) ; Learning rate = 0.000463 ; Loss = 1.467370\n",
      "2024-12-03 16:08:57.945000: I runner.py:310] Step = 36500 ; steps/s = 1.63, tokens/s = 77665 (34195 source, 43470 target) ; Learning rate = 0.000463 ; Loss = 1.479398\n",
      "2024-12-03 16:09:58.687000: I runner.py:310] Step = 36600 ; steps/s = 1.65, tokens/s = 78458 (34565 source, 43893 target) ; Learning rate = 0.000462 ; Loss = 1.483504\n",
      "2024-12-03 16:10:59.030000: I runner.py:310] Step = 36700 ; steps/s = 1.66, tokens/s = 77564 (34189 source, 43375 target) ; Learning rate = 0.000461 ; Loss = 1.467712\n",
      "2024-12-03 16:11:59.812000: I runner.py:310] Step = 36800 ; steps/s = 1.65, tokens/s = 78398 (34525 source, 43873 target) ; Learning rate = 0.000461 ; Loss = 1.475022\n",
      "2024-12-03 16:13:00.154000: I runner.py:310] Step = 36900 ; steps/s = 1.66, tokens/s = 77581 (34208 source, 43373 target) ; Learning rate = 0.000460 ; Loss = 1.470376\n",
      "2024-12-03 16:14:00.977000: I runner.py:310] Step = 37000 ; steps/s = 1.64, tokens/s = 78316 (34483 source, 43833 target) ; Learning rate = 0.000460 ; Loss = 1.465104\n",
      "2024-12-03 16:15:01.797000: I runner.py:310] Step = 37100 ; steps/s = 1.64, tokens/s = 78376 (34527 source, 43849 target) ; Learning rate = 0.000459 ; Loss = 1.476259\n",
      "2024-12-03 16:16:02.115000: I runner.py:310] Step = 37200 ; steps/s = 1.66, tokens/s = 77583 (34185 source, 43398 target) ; Learning rate = 0.000458 ; Loss = 1.471855\n",
      "2024-12-03 16:17:02.900000: I runner.py:310] Step = 37300 ; steps/s = 1.65, tokens/s = 78402 (34536 source, 43866 target) ; Learning rate = 0.000458 ; Loss = 1.477333\n",
      "2024-12-03 16:18:03.221000: I runner.py:310] Step = 37400 ; steps/s = 1.66, tokens/s = 77651 (34258 source, 43393 target) ; Learning rate = 0.000457 ; Loss = 1.471399\n",
      "2024-12-03 16:19:04.050000: I runner.py:310] Step = 37500 ; steps/s = 1.64, tokens/s = 78310 (34473 source, 43837 target) ; Learning rate = 0.000456 ; Loss = 1.478654\n",
      "2024-12-03 16:20:04.852000: I runner.py:310] Step = 37600 ; steps/s = 1.64, tokens/s = 78379 (34515 source, 43864 target) ; Learning rate = 0.000456 ; Loss = 1.475755\n",
      "2024-12-03 16:21:05.251000: I runner.py:310] Step = 37700 ; steps/s = 1.66, tokens/s = 77509 (34179 source, 43330 target) ; Learning rate = 0.000455 ; Loss = 1.471310\n",
      "2024-12-03 16:22:06.083000: I runner.py:310] Step = 37800 ; steps/s = 1.64, tokens/s = 78322 (34484 source, 43838 target) ; Learning rate = 0.000455 ; Loss = 1.483284\n",
      "2024-12-03 16:23:06.918000: I runner.py:310] Step = 37900 ; steps/s = 1.64, tokens/s = 78348 (34509 source, 43839 target) ; Learning rate = 0.000454 ; Loss = 1.477531\n",
      "2024-12-03 16:24:07.226000: I runner.py:310] Step = 38000 ; steps/s = 1.66, tokens/s = 77646 (34238 source, 43408 target) ; Learning rate = 0.000453 ; Loss = 1.469811\n",
      "2024-12-03 16:25:07.995000: I runner.py:310] Step = 38100 ; steps/s = 1.65, tokens/s = 78412 (34540 source, 43872 target) ; Learning rate = 0.000453 ; Loss = 1.469980\n",
      "2024-12-03 16:26:08.326000: I runner.py:310] Step = 38200 ; steps/s = 1.66, tokens/s = 77579 (34197 source, 43382 target) ; Learning rate = 0.000452 ; Loss = 1.472269\n",
      "2024-12-03 16:27:09.166000: I runner.py:310] Step = 38300 ; steps/s = 1.64, tokens/s = 78323 (34482 source, 43841 target) ; Learning rate = 0.000452 ; Loss = 1.478400\n",
      "2024-12-03 16:28:09.969000: I runner.py:310] Step = 38400 ; steps/s = 1.64, tokens/s = 78368 (34518 source, 43850 target) ; Learning rate = 0.000451 ; Loss = 1.471339\n",
      "2024-12-03 16:29:10.274000: I runner.py:310] Step = 38500 ; steps/s = 1.66, tokens/s = 77590 (34192 source, 43398 target) ; Learning rate = 0.000450 ; Loss = 1.468823\n",
      "2024-12-03 16:30:11.026000: I runner.py:310] Step = 38600 ; steps/s = 1.65, tokens/s = 78433 (34542 source, 43891 target) ; Learning rate = 0.000450 ; Loss = 1.476343\n",
      "2024-12-03 16:31:11.409000: I runner.py:310] Step = 38700 ; steps/s = 1.66, tokens/s = 77553 (34217 source, 43336 target) ; Learning rate = 0.000449 ; Loss = 1.467029\n",
      "2024-12-03 16:32:12.254000: I runner.py:310] Step = 38800 ; steps/s = 1.64, tokens/s = 78308 (34479 source, 43829 target) ; Learning rate = 0.000449 ; Loss = 1.469328\n",
      "2024-12-03 16:33:13.077000: I runner.py:310] Step = 38900 ; steps/s = 1.64, tokens/s = 78342 (34499 source, 43843 target) ; Learning rate = 0.000448 ; Loss = 1.470073\n",
      "2024-12-03 16:34:13.377000: I runner.py:310] Step = 39000 ; steps/s = 1.66, tokens/s = 77600 (34198 source, 43402 target) ; Learning rate = 0.000448 ; Loss = 1.466107\n",
      "2024-12-03 16:35:14.152000: I runner.py:310] Step = 39100 ; steps/s = 1.65, tokens/s = 78433 (34551 source, 43882 target) ; Learning rate = 0.000447 ; Loss = 1.481219\n",
      "2024-12-03 16:36:14.916000: I runner.py:310] Step = 39200 ; steps/s = 1.65, tokens/s = 78416 (34529 source, 43887 target) ; Learning rate = 0.000446 ; Loss = 1.467643\n",
      "2024-12-03 16:37:15.271000: I runner.py:310] Step = 39300 ; steps/s = 1.66, tokens/s = 77547 (34189 source, 43358 target) ; Learning rate = 0.000446 ; Loss = 1.466494\n",
      "2024-12-03 16:38:16.033000: I runner.py:310] Step = 39400 ; steps/s = 1.65, tokens/s = 78432 (34546 source, 43886 target) ; Learning rate = 0.000445 ; Loss = 1.474071\n",
      "2024-12-03 16:39:16.465000: I runner.py:310] Step = 39500 ; steps/s = 1.65, tokens/s = 77461 (34160 source, 43301 target) ; Learning rate = 0.000445 ; Loss = 1.470318\n",
      "2024-12-03 16:40:17.262000: I runner.py:310] Step = 39600 ; steps/s = 1.64, tokens/s = 78362 (34501 source, 43861 target) ; Learning rate = 0.000444 ; Loss = 1.471345\n",
      "2024-12-03 16:41:18.083000: I runner.py:310] Step = 39700 ; steps/s = 1.64, tokens/s = 78348 (34501 source, 43847 target) ; Learning rate = 0.000444 ; Loss = 1.469094\n",
      "2024-12-03 16:42:18.425000: I runner.py:310] Step = 39800 ; steps/s = 1.66, tokens/s = 77561 (34194 source, 43367 target) ; Learning rate = 0.000443 ; Loss = 1.470572\n",
      "2024-12-03 16:43:19.227000: I runner.py:310] Step = 39900 ; steps/s = 1.64, tokens/s = 78370 (34517 source, 43853 target) ; Learning rate = 0.000442 ; Loss = 1.471874\n",
      "2024-12-03 16:44:19.569000: I runner.py:310] Step = 40000 ; steps/s = 1.66, tokens/s = 77602 (34226 source, 43376 target) ; Learning rate = 0.000442 ; Loss = 1.462917\n",
      "2024-12-03 16:44:21.306000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-40000\n",
      "2024-12-03 16:44:21.306000: I training.py:192] Running evaluation for step 40000\n",
      "2024-12-03 16:45:11.374000: I training.py:192] Evaluation result for step 40000: loss = 0.802985 ; perplexity = 2.232195\n",
      "2024-12-03 16:46:12.134000: I runner.py:310] Step = 40100 ; steps/s = 1.65, tokens/s = 78419 (34521 source, 43898 target) ; Learning rate = 0.000441 ; Loss = 1.466865\n",
      "2024-12-03 16:47:12.964000: I runner.py:310] Step = 40200 ; steps/s = 1.64, tokens/s = 78359 (34525 source, 43834 target) ; Learning rate = 0.000441 ; Loss = 1.473430\n",
      "2024-12-03 16:48:13.297000: I runner.py:310] Step = 40300 ; steps/s = 1.66, tokens/s = 77590 (34201 source, 43389 target) ; Learning rate = 0.000440 ; Loss = 1.467150\n",
      "2024-12-03 16:49:14.188000: I runner.py:310] Step = 40400 ; steps/s = 1.64, tokens/s = 78276 (34473 source, 43803 target) ; Learning rate = 0.000440 ; Loss = 1.479993\n",
      "2024-12-03 16:50:15.061000: I runner.py:310] Step = 40500 ; steps/s = 1.64, tokens/s = 78277 (34480 source, 43797 target) ; Learning rate = 0.000439 ; Loss = 1.472652\n",
      "2024-12-03 16:51:15.450000: I runner.py:310] Step = 40600 ; steps/s = 1.66, tokens/s = 77510 (34172 source, 43338 target) ; Learning rate = 0.000439 ; Loss = 1.467375\n",
      "2024-12-03 16:52:16.295000: I runner.py:310] Step = 40700 ; steps/s = 1.64, tokens/s = 78302 (34479 source, 43823 target) ; Learning rate = 0.000438 ; Loss = 1.467165\n",
      "2024-12-03 16:53:16.555000: I runner.py:310] Step = 40800 ; steps/s = 1.66, tokens/s = 77676 (34236 source, 43440 target) ; Learning rate = 0.000438 ; Loss = 1.457271\n",
      "2024-12-03 16:54:17.364000: I runner.py:310] Step = 40900 ; steps/s = 1.64, tokens/s = 78354 (34509 source, 43845 target) ; Learning rate = 0.000437 ; Loss = 1.466845\n",
      "2024-12-03 16:55:18.227000: I runner.py:310] Step = 41000 ; steps/s = 1.64, tokens/s = 78317 (34502 source, 43815 target) ; Learning rate = 0.000437 ; Loss = 1.469804\n",
      "2024-12-03 16:56:18.633000: I runner.py:310] Step = 41100 ; steps/s = 1.66, tokens/s = 77476 (34141 source, 43335 target) ; Learning rate = 0.000436 ; Loss = 1.465293\n",
      "2024-12-03 16:57:19.451000: I runner.py:310] Step = 41200 ; steps/s = 1.64, tokens/s = 78384 (34533 source, 43851 target) ; Learning rate = 0.000435 ; Loss = 1.466913\n",
      "2024-12-03 16:58:19.783000: I runner.py:310] Step = 41300 ; steps/s = 1.66, tokens/s = 77584 (34214 source, 43370 target) ; Learning rate = 0.000435 ; Loss = 1.466341\n",
      "2024-12-03 16:59:20.591000: I runner.py:310] Step = 41400 ; steps/s = 1.64, tokens/s = 78348 (34486 source, 43862 target) ; Learning rate = 0.000434 ; Loss = 1.466000\n",
      "2024-12-03 17:00:21.480000: I runner.py:310] Step = 41500 ; steps/s = 1.64, tokens/s = 78277 (34492 source, 43785 target) ; Learning rate = 0.000434 ; Loss = 1.467660\n",
      "2024-12-03 17:01:21.859000: I runner.py:310] Step = 41600 ; steps/s = 1.66, tokens/s = 77516 (34170 source, 43346 target) ; Learning rate = 0.000433 ; Loss = 1.461437\n",
      "2024-12-03 17:02:22.720000: I runner.py:310] Step = 41700 ; steps/s = 1.64, tokens/s = 78308 (34486 source, 43822 target) ; Learning rate = 0.000433 ; Loss = 1.470574\n",
      "2024-12-03 17:03:23.543000: I runner.py:310] Step = 41800 ; steps/s = 1.64, tokens/s = 78333 (34489 source, 43844 target) ; Learning rate = 0.000432 ; Loss = 1.475227\n",
      "2024-12-03 17:04:23.777000: I runner.py:310] Step = 41900 ; steps/s = 1.66, tokens/s = 77701 (34260 source, 43441 target) ; Learning rate = 0.000432 ; Loss = 1.464996\n",
      "2024-12-03 17:05:24.531000: I runner.py:310] Step = 42000 ; steps/s = 1.65, tokens/s = 78438 (34550 source, 43888 target) ; Learning rate = 0.000431 ; Loss = 1.469564\n",
      "2024-12-03 17:06:24.918000: I runner.py:310] Step = 42100 ; steps/s = 1.66, tokens/s = 77530 (34178 source, 43352 target) ; Learning rate = 0.000431 ; Loss = 1.464256\n",
      "2024-12-03 17:07:25.723000: I runner.py:310] Step = 42200 ; steps/s = 1.64, tokens/s = 78348 (34493 source, 43855 target) ; Learning rate = 0.000430 ; Loss = 1.467104\n",
      "2024-12-03 17:08:26.534000: I runner.py:310] Step = 42300 ; steps/s = 1.64, tokens/s = 78372 (34516 source, 43856 target) ; Learning rate = 0.000430 ; Loss = 1.475859\n",
      "2024-12-03 17:09:26.912000: I runner.py:310] Step = 42400 ; steps/s = 1.66, tokens/s = 77534 (34193 source, 43341 target) ; Learning rate = 0.000429 ; Loss = 1.468977\n",
      "2024-12-03 17:10:27.719000: I runner.py:310] Step = 42500 ; steps/s = 1.64, tokens/s = 78373 (34513 source, 43860 target) ; Learning rate = 0.000429 ; Loss = 1.466088\n",
      "2024-12-03 17:11:28.073000: I runner.py:310] Step = 42600 ; steps/s = 1.66, tokens/s = 77560 (34203 source, 43357 target) ; Learning rate = 0.000428 ; Loss = 1.457901\n",
      "2024-12-03 17:12:28.888000: I runner.py:310] Step = 42700 ; steps/s = 1.64, tokens/s = 78299 (34456 source, 43843 target) ; Learning rate = 0.000428 ; Loss = 1.464169\n",
      "2024-12-03 17:13:29.763000: I runner.py:310] Step = 42800 ; steps/s = 1.64, tokens/s = 78315 (34514 source, 43801 target) ; Learning rate = 0.000427 ; Loss = 1.463041\n",
      "2024-12-03 17:14:30.113000: I runner.py:310] Step = 42900 ; steps/s = 1.66, tokens/s = 77534 (34159 source, 43375 target) ; Learning rate = 0.000427 ; Loss = 1.464414\n",
      "2024-12-03 17:15:30.934000: I runner.py:310] Step = 43000 ; steps/s = 1.64, tokens/s = 78348 (34511 source, 43837 target) ; Learning rate = 0.000426 ; Loss = 1.462330\n",
      "2024-12-03 17:16:31.724000: I runner.py:310] Step = 43100 ; steps/s = 1.65, tokens/s = 78428 (34553 source, 43875 target) ; Learning rate = 0.000426 ; Loss = 1.470052\n",
      "2024-12-03 17:17:32.113000: I runner.py:310] Step = 43200 ; steps/s = 1.66, tokens/s = 77505 (34163 source, 43342 target) ; Learning rate = 0.000425 ; Loss = 1.464675\n",
      "2024-12-03 17:18:32.929000: I runner.py:310] Step = 43300 ; steps/s = 1.64, tokens/s = 78369 (34524 source, 43845 target) ; Learning rate = 0.000425 ; Loss = 1.467451\n",
      "2024-12-03 17:19:33.213000: I runner.py:310] Step = 43400 ; steps/s = 1.66, tokens/s = 77632 (34216 source, 43416 target) ; Learning rate = 0.000424 ; Loss = 1.459183\n",
      "2024-12-03 17:20:33.977000: I runner.py:310] Step = 43500 ; steps/s = 1.65, tokens/s = 78405 (34521 source, 43884 target) ; Learning rate = 0.000424 ; Loss = 1.471190\n",
      "2024-12-03 17:21:34.741000: I runner.py:310] Step = 43600 ; steps/s = 1.65, tokens/s = 78450 (34570 source, 43880 target) ; Learning rate = 0.000423 ; Loss = 1.463433\n",
      "2024-12-03 17:22:34.978000: I runner.py:310] Step = 43700 ; steps/s = 1.66, tokens/s = 77675 (34223 source, 43452 target) ; Learning rate = 0.000423 ; Loss = 1.467584\n",
      "2024-12-03 17:23:35.741000: I runner.py:310] Step = 43800 ; steps/s = 1.65, tokens/s = 78429 (34538 source, 43891 target) ; Learning rate = 0.000422 ; Loss = 1.467631\n",
      "2024-12-03 17:24:36.031000: I runner.py:310] Step = 43900 ; steps/s = 1.66, tokens/s = 77680 (34272 source, 43408 target) ; Learning rate = 0.000422 ; Loss = 1.462980\n",
      "2024-12-03 17:25:36.870000: I runner.py:310] Step = 44000 ; steps/s = 1.64, tokens/s = 78301 (34459 source, 43842 target) ; Learning rate = 0.000421 ; Loss = 1.467881\n",
      "2024-12-03 17:26:37.610000: I runner.py:310] Step = 44100 ; steps/s = 1.65, tokens/s = 78462 (34564 source, 43898 target) ; Learning rate = 0.000421 ; Loss = 1.466684\n",
      "2024-12-03 17:27:37.968000: I runner.py:310] Step = 44200 ; steps/s = 1.66, tokens/s = 77560 (34205 source, 43355 target) ; Learning rate = 0.000420 ; Loss = 1.464457\n",
      "2024-12-03 17:28:38.725000: I runner.py:310] Step = 44300 ; steps/s = 1.65, tokens/s = 78418 (34536 source, 43882 target) ; Learning rate = 0.000420 ; Loss = 1.459281\n",
      "2024-12-03 17:29:39.485000: I runner.py:310] Step = 44400 ; steps/s = 1.65, tokens/s = 78445 (34546 source, 43899 target) ; Learning rate = 0.000419 ; Loss = 1.467318\n",
      "2024-12-03 17:30:39.719000: I runner.py:310] Step = 44500 ; steps/s = 1.66, tokens/s = 77711 (34260 source, 43451 target) ; Learning rate = 0.000419 ; Loss = 1.464248\n",
      "2024-12-03 17:31:40.522000: I runner.py:310] Step = 44600 ; steps/s = 1.64, tokens/s = 78372 (34514 source, 43858 target) ; Learning rate = 0.000419 ; Loss = 1.463933\n",
      "2024-12-03 17:32:40.782000: I runner.py:310] Step = 44700 ; steps/s = 1.66, tokens/s = 77670 (34245 source, 43425 target) ; Learning rate = 0.000418 ; Loss = 1.459741\n",
      "2024-12-03 17:33:41.583000: I runner.py:310] Step = 44800 ; steps/s = 1.64, tokens/s = 78353 (34505 source, 43848 target) ; Learning rate = 0.000418 ; Loss = 1.460207\n",
      "2024-12-03 17:34:42.306000: I runner.py:310] Step = 44900 ; steps/s = 1.65, tokens/s = 78486 (34562 source, 43924 target) ; Learning rate = 0.000417 ; Loss = 1.468165\n",
      "2024-12-03 17:35:42.608000: I runner.py:310] Step = 45000 ; steps/s = 1.66, tokens/s = 77627 (34218 source, 43409 target) ; Learning rate = 0.000417 ; Loss = 1.464978\n",
      "2024-12-03 17:35:42.610000: I training.py:192] Running evaluation for step 45000\n",
      "2024-12-03 17:36:32.184000: I training.py:192] Evaluation result for step 45000: loss = 0.815306 ; perplexity = 2.259867\n",
      "2024-12-03 17:37:32.895000: I runner.py:310] Step = 45100 ; steps/s = 1.65, tokens/s = 78503 (34562 source, 43941 target) ; Learning rate = 0.000416 ; Loss = 1.466917\n",
      "2024-12-03 17:38:33.290000: I runner.py:310] Step = 45200 ; steps/s = 1.66, tokens/s = 77522 (34203 source, 43319 target) ; Learning rate = 0.000416 ; Loss = 1.461065\n",
      "2024-12-03 17:39:34.142000: I runner.py:310] Step = 45300 ; steps/s = 1.64, tokens/s = 78292 (34468 source, 43824 target) ; Learning rate = 0.000415 ; Loss = 1.458286\n",
      "2024-12-03 17:40:34.981000: I runner.py:310] Step = 45400 ; steps/s = 1.64, tokens/s = 78330 (34503 source, 43827 target) ; Learning rate = 0.000415 ; Loss = 1.463730\n",
      "2024-12-03 17:41:35.396000: I runner.py:310] Step = 45500 ; steps/s = 1.66, tokens/s = 77479 (34159 source, 43320 target) ; Learning rate = 0.000414 ; Loss = 1.459156\n",
      "2024-12-03 17:42:36.227000: I runner.py:310] Step = 45600 ; steps/s = 1.64, tokens/s = 78353 (34512 source, 43841 target) ; Learning rate = 0.000414 ; Loss = 1.463838\n",
      "2024-12-03 17:43:37.009000: I runner.py:310] Step = 45700 ; steps/s = 1.65, tokens/s = 78383 (34514 source, 43869 target) ; Learning rate = 0.000413 ; Loss = 1.465387\n",
      "2024-12-03 17:44:37.351000: I runner.py:310] Step = 45800 ; steps/s = 1.66, tokens/s = 77559 (34189 source, 43370 target) ; Learning rate = 0.000413 ; Loss = 1.458177\n",
      "2024-12-03 17:45:38.138000: I runner.py:310] Step = 45900 ; steps/s = 1.65, tokens/s = 78378 (34512 source, 43866 target) ; Learning rate = 0.000413 ; Loss = 1.461165\n",
      "2024-12-03 17:46:38.472000: I runner.py:310] Step = 46000 ; steps/s = 1.66, tokens/s = 77612 (34230 source, 43382 target) ; Learning rate = 0.000412 ; Loss = 1.458517\n",
      "2024-12-03 17:47:39.529000: I runner.py:310] Step = 46100 ; steps/s = 1.64, tokens/s = 78045 (34370 source, 43675 target) ; Learning rate = 0.000412 ; Loss = 1.456887\n",
      "2024-12-03 17:48:40.248000: I runner.py:310] Step = 46200 ; steps/s = 1.65, tokens/s = 78483 (34562 source, 43921 target) ; Learning rate = 0.000411 ; Loss = 1.465462\n",
      "2024-12-03 17:49:40.536000: I runner.py:310] Step = 46300 ; steps/s = 1.66, tokens/s = 77643 (34223 source, 43420 target) ; Learning rate = 0.000411 ; Loss = 1.459238\n",
      "2024-12-03 17:50:41.280000: I runner.py:310] Step = 46400 ; steps/s = 1.65, tokens/s = 78468 (34572 source, 43896 target) ; Learning rate = 0.000410 ; Loss = 1.460274\n",
      "2024-12-03 17:51:41.577000: I runner.py:310] Step = 46500 ; steps/s = 1.66, tokens/s = 77605 (34203 source, 43402 target) ; Learning rate = 0.000410 ; Loss = 1.452770\n",
      "2024-12-03 17:52:42.286000: I runner.py:310] Step = 46600 ; steps/s = 1.65, tokens/s = 78504 (34577 source, 43927 target) ; Learning rate = 0.000409 ; Loss = 1.461632\n",
      "2024-12-03 17:53:43.087000: I runner.py:310] Step = 46700 ; steps/s = 1.64, tokens/s = 78376 (34523 source, 43853 target) ; Learning rate = 0.000409 ; Loss = 1.463448\n",
      "2024-12-03 17:54:43.311000: I runner.py:310] Step = 46800 ; steps/s = 1.66, tokens/s = 77719 (34268 source, 43451 target) ; Learning rate = 0.000409 ; Loss = 1.453788\n",
      "2024-12-03 17:55:44.057000: I runner.py:310] Step = 46900 ; steps/s = 1.65, tokens/s = 78430 (34528 source, 43902 target) ; Learning rate = 0.000408 ; Loss = 1.461177\n",
      "2024-12-03 17:56:44.777000: I runner.py:310] Step = 47000 ; steps/s = 1.65, tokens/s = 78505 (34577 source, 43928 target) ; Learning rate = 0.000408 ; Loss = 1.466347\n",
      "2024-12-03 17:57:45.086000: I runner.py:310] Step = 47100 ; steps/s = 1.66, tokens/s = 77618 (34220 source, 43398 target) ; Learning rate = 0.000407 ; Loss = 1.462420\n",
      "2024-12-03 17:58:45.809000: I runner.py:310] Step = 47200 ; steps/s = 1.65, tokens/s = 78487 (34576 source, 43911 target) ; Learning rate = 0.000407 ; Loss = 1.465546\n",
      "2024-12-03 17:59:46.105000: I runner.py:310] Step = 47300 ; steps/s = 1.66, tokens/s = 77621 (34206 source, 43415 target) ; Learning rate = 0.000406 ; Loss = 1.461218\n",
      "2024-12-03 18:00:46.902000: I runner.py:310] Step = 47400 ; steps/s = 1.64, tokens/s = 78391 (34523 source, 43868 target) ; Learning rate = 0.000406 ; Loss = 1.461366\n",
      "2024-12-03 18:01:47.606000: I runner.py:310] Step = 47500 ; steps/s = 1.65, tokens/s = 78494 (34574 source, 43920 target) ; Learning rate = 0.000406 ; Loss = 1.463039\n",
      "2024-12-03 18:02:47.891000: I runner.py:310] Step = 47600 ; steps/s = 1.66, tokens/s = 77583 (34188 source, 43395 target) ; Learning rate = 0.000405 ; Loss = 1.461978\n",
      "2024-12-03 18:03:48.671000: I runner.py:310] Step = 47700 ; steps/s = 1.65, tokens/s = 78428 (34548 source, 43880 target) ; Learning rate = 0.000405 ; Loss = 1.459742\n",
      "2024-12-03 18:04:48.935000: I runner.py:310] Step = 47800 ; steps/s = 1.66, tokens/s = 77703 (34272 source, 43431 target) ; Learning rate = 0.000404 ; Loss = 1.451797\n",
      "2024-12-03 18:05:49.670000: I runner.py:310] Step = 47900 ; steps/s = 1.65, tokens/s = 78451 (34539 source, 43912 target) ; Learning rate = 0.000404 ; Loss = 1.458273\n",
      "2024-12-03 18:06:50.384000: I runner.py:310] Step = 48000 ; steps/s = 1.65, tokens/s = 78504 (34581 source, 43923 target) ; Learning rate = 0.000403 ; Loss = 1.464932\n",
      "2024-12-03 18:07:50.615000: I runner.py:310] Step = 48100 ; steps/s = 1.66, tokens/s = 77678 (34236 source, 43442 target) ; Learning rate = 0.000403 ; Loss = 1.463676\n",
      "2024-12-03 18:08:51.329000: I runner.py:310] Step = 48200 ; steps/s = 1.65, tokens/s = 78497 (34568 source, 43929 target) ; Learning rate = 0.000403 ; Loss = 1.454802\n",
      "2024-12-03 18:09:52.043000: I runner.py:310] Step = 48300 ; steps/s = 1.65, tokens/s = 78508 (34581 source, 43927 target) ; Learning rate = 0.000402 ; Loss = 1.467524\n",
      "2024-12-03 18:10:52.354000: I runner.py:310] Step = 48400 ; steps/s = 1.66, tokens/s = 77634 (34235 source, 43399 target) ; Learning rate = 0.000402 ; Loss = 1.457712\n",
      "2024-12-03 18:11:53.146000: I runner.py:310] Step = 48500 ; steps/s = 1.65, tokens/s = 78388 (34523 source, 43865 target) ; Learning rate = 0.000401 ; Loss = 1.460929\n",
      "2024-12-03 18:12:53.403000: I runner.py:310] Step = 48600 ; steps/s = 1.66, tokens/s = 77656 (34230 source, 43426 target) ; Learning rate = 0.000401 ; Loss = 1.459851\n",
      "2024-12-03 18:13:54.167000: I runner.py:310] Step = 48700 ; steps/s = 1.65, tokens/s = 78400 (34508 source, 43892 target) ; Learning rate = 0.000401 ; Loss = 1.462987\n",
      "2024-12-03 18:14:54.944000: I runner.py:310] Step = 48800 ; steps/s = 1.65, tokens/s = 78441 (34562 source, 43879 target) ; Learning rate = 0.000400 ; Loss = 1.459112\n",
      "2024-12-03 18:15:55.209000: I runner.py:310] Step = 48900 ; steps/s = 1.66, tokens/s = 77663 (34239 source, 43424 target) ; Learning rate = 0.000400 ; Loss = 1.461970\n",
      "2024-12-03 18:16:55.979000: I runner.py:310] Step = 49000 ; steps/s = 1.65, tokens/s = 78425 (34541 source, 43884 target) ; Learning rate = 0.000399 ; Loss = 1.456406\n",
      "2024-12-03 18:17:56.315000: I runner.py:310] Step = 49100 ; steps/s = 1.66, tokens/s = 77588 (34207 source, 43381 target) ; Learning rate = 0.000399 ; Loss = 1.452492\n",
      "2024-12-03 18:18:57.061000: I runner.py:310] Step = 49200 ; steps/s = 1.65, tokens/s = 78421 (34518 source, 43903 target) ; Learning rate = 0.000398 ; Loss = 1.456996\n",
      "2024-12-03 18:19:57.818000: I runner.py:310] Step = 49300 ; steps/s = 1.65, tokens/s = 78460 (34575 source, 43885 target) ; Learning rate = 0.000398 ; Loss = 1.460312\n",
      "2024-12-03 18:20:58.091000: I runner.py:310] Step = 49400 ; steps/s = 1.66, tokens/s = 77624 (34213 source, 43411 target) ; Learning rate = 0.000398 ; Loss = 1.454549\n",
      "2024-12-03 18:21:58.908000: I runner.py:310] Step = 49500 ; steps/s = 1.64, tokens/s = 78350 (34488 source, 43862 target) ; Learning rate = 0.000397 ; Loss = 1.460238\n",
      "2024-12-03 18:22:59.651000: I runner.py:310] Step = 49600 ; steps/s = 1.65, tokens/s = 78477 (34582 source, 43895 target) ; Learning rate = 0.000397 ; Loss = 1.462559\n",
      "2024-12-03 18:23:59.927000: I runner.py:310] Step = 49700 ; steps/s = 1.66, tokens/s = 77674 (34249 source, 43425 target) ; Learning rate = 0.000396 ; Loss = 1.458952\n",
      "2024-12-03 18:25:00.741000: I runner.py:310] Step = 49800 ; steps/s = 1.64, tokens/s = 78350 (34511 source, 43839 target) ; Learning rate = 0.000396 ; Loss = 1.459777\n",
      "2024-12-03 18:26:01.032000: I runner.py:310] Step = 49900 ; steps/s = 1.66, tokens/s = 77639 (34219 source, 43420 target) ; Learning rate = 0.000396 ; Loss = 1.455167\n",
      "2024-12-03 18:27:01.739000: I runner.py:310] Step = 50000 ; steps/s = 1.65, tokens/s = 78482 (34551 source, 43931 target) ; Learning rate = 0.000395 ; Loss = 1.454744\n",
      "2024-12-03 18:27:03.501000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-50000\n",
      "2024-12-03 18:27:03.502000: I training.py:192] Running evaluation for step 50000\n",
      "2024-12-03 18:27:50.315000: I training.py:192] Evaluation result for step 50000: loss = 0.823903 ; perplexity = 2.279379\n",
      "2024-12-03 18:28:50.983000: I runner.py:310] Step = 50100 ; steps/s = 1.65, tokens/s = 78571 (34615 source, 43956 target) ; Learning rate = 0.000395 ; Loss = 1.454532\n",
      "2024-12-03 18:29:51.365000: I runner.py:310] Step = 50200 ; steps/s = 1.66, tokens/s = 77521 (34177 source, 43344 target) ; Learning rate = 0.000394 ; Loss = 1.461723\n",
      "2024-12-03 18:30:52.201000: I runner.py:310] Step = 50300 ; steps/s = 1.64, tokens/s = 78325 (34489 source, 43836 target) ; Learning rate = 0.000394 ; Loss = 1.463621\n",
      "2024-12-03 18:31:52.569000: I runner.py:310] Step = 50400 ; steps/s = 1.66, tokens/s = 77546 (34203 source, 43343 target) ; Learning rate = 0.000394 ; Loss = 1.449766\n",
      "2024-12-03 18:32:53.373000: I runner.py:310] Step = 50500 ; steps/s = 1.64, tokens/s = 78363 (34502 source, 43861 target) ; Learning rate = 0.000393 ; Loss = 1.453108\n",
      "2024-12-03 18:33:54.160000: I runner.py:310] Step = 50600 ; steps/s = 1.65, tokens/s = 78406 (34535 source, 43871 target) ; Learning rate = 0.000393 ; Loss = 1.458321\n",
      "2024-12-03 18:34:54.555000: I runner.py:310] Step = 50700 ; steps/s = 1.66, tokens/s = 77497 (34165 source, 43332 target) ; Learning rate = 0.000393 ; Loss = 1.452829\n",
      "2024-12-03 18:35:55.462000: I runner.py:310] Step = 50800 ; steps/s = 1.64, tokens/s = 78253 (34466 source, 43787 target) ; Learning rate = 0.000392 ; Loss = 1.460538\n",
      "2024-12-03 18:36:56.261000: I runner.py:310] Step = 50900 ; steps/s = 1.65, tokens/s = 78365 (34505 source, 43860 target) ; Learning rate = 0.000392 ; Loss = 1.460871\n",
      "2024-12-03 18:37:56.578000: I runner.py:310] Step = 51000 ; steps/s = 1.66, tokens/s = 77599 (34221 source, 43378 target) ; Learning rate = 0.000391 ; Loss = 1.454423\n",
      "2024-12-03 18:38:57.359000: I runner.py:310] Step = 51100 ; steps/s = 1.65, tokens/s = 78403 (34517 source, 43886 target) ; Learning rate = 0.000391 ; Loss = 1.463091\n",
      "2024-12-03 18:39:57.663000: I runner.py:310] Step = 51200 ; steps/s = 1.66, tokens/s = 77621 (34223 source, 43398 target) ; Learning rate = 0.000391 ; Loss = 1.455120\n",
      "2024-12-03 18:40:58.515000: I runner.py:310] Step = 51300 ; steps/s = 1.64, tokens/s = 78284 (34474 source, 43810 target) ; Learning rate = 0.000390 ; Loss = 1.457780\n",
      "2024-12-03 18:41:59.360000: I runner.py:310] Step = 51400 ; steps/s = 1.64, tokens/s = 78336 (34502 source, 43834 target) ; Learning rate = 0.000390 ; Loss = 1.460951\n",
      "2024-12-03 18:42:59.726000: I runner.py:310] Step = 51500 ; steps/s = 1.66, tokens/s = 77543 (34172 source, 43371 target) ; Learning rate = 0.000389 ; Loss = 1.458581\n",
      "2024-12-03 18:44:00.498000: I runner.py:310] Step = 51600 ; steps/s = 1.65, tokens/s = 78392 (34515 source, 43877 target) ; Learning rate = 0.000389 ; Loss = 1.458382\n",
      "2024-12-03 18:45:00.894000: I runner.py:310] Step = 51700 ; steps/s = 1.66, tokens/s = 77535 (34213 source, 43322 target) ; Learning rate = 0.000389 ; Loss = 1.449081\n",
      "2024-12-03 18:46:01.685000: I runner.py:310] Step = 51800 ; steps/s = 1.65, tokens/s = 78405 (34540 source, 43865 target) ; Learning rate = 0.000388 ; Loss = 1.457440\n",
      "2024-12-03 18:47:02.489000: I runner.py:310] Step = 51900 ; steps/s = 1.64, tokens/s = 78356 (34500 source, 43856 target) ; Learning rate = 0.000388 ; Loss = 1.458386\n",
      "2024-12-03 18:48:02.825000: I runner.py:310] Step = 52000 ; steps/s = 1.66, tokens/s = 77563 (34185 source, 43378 target) ; Learning rate = 0.000388 ; Loss = 1.450541\n",
      "2024-12-03 18:49:03.587000: I runner.py:310] Step = 52100 ; steps/s = 1.65, tokens/s = 78425 (34538 source, 43887 target) ; Learning rate = 0.000387 ; Loss = 1.465146\n",
      "2024-12-03 18:50:04.364000: I runner.py:310] Step = 52200 ; steps/s = 1.65, tokens/s = 78219 (34453 source, 43766 target) ; Learning rate = 0.000387 ; Loss = 1.462906\n",
      "2024-12-03 18:51:04.636000: I runner.py:310] Step = 52300 ; steps/s = 1.66, tokens/s = 77823 (34295 source, 43528 target) ; Learning rate = 0.000386 ; Loss = 1.451926\n",
      "2024-12-03 18:52:05.430000: I runner.py:310] Step = 52400 ; steps/s = 1.65, tokens/s = 78412 (34546 source, 43866 target) ; Learning rate = 0.000386 ; Loss = 1.462083\n",
      "2024-12-03 18:53:05.807000: I runner.py:310] Step = 52500 ; steps/s = 1.66, tokens/s = 77543 (34192 source, 43351 target) ; Learning rate = 0.000386 ; Loss = 1.452286\n",
      "2024-12-03 18:54:06.618000: I runner.py:310] Step = 52600 ; steps/s = 1.64, tokens/s = 78357 (34508 source, 43849 target) ; Learning rate = 0.000385 ; Loss = 1.453576\n",
      "2024-12-03 18:55:07.407000: I runner.py:310] Step = 52700 ; steps/s = 1.65, tokens/s = 78397 (34527 source, 43870 target) ; Learning rate = 0.000385 ; Loss = 1.457325\n",
      "2024-12-03 18:56:07.750000: I runner.py:310] Step = 52800 ; steps/s = 1.66, tokens/s = 77556 (34185 source, 43371 target) ; Learning rate = 0.000385 ; Loss = 1.451245\n",
      "2024-12-03 18:57:08.604000: I runner.py:310] Step = 52900 ; steps/s = 1.64, tokens/s = 78318 (34491 source, 43827 target) ; Learning rate = 0.000384 ; Loss = 1.455212\n",
      "2024-12-03 18:58:08.917000: I runner.py:310] Step = 53000 ; steps/s = 1.66, tokens/s = 77619 (34230 source, 43389 target) ; Learning rate = 0.000384 ; Loss = 1.454954\n",
      "2024-12-03 18:59:09.803000: I runner.py:310] Step = 53100 ; steps/s = 1.64, tokens/s = 78223 (34428 source, 43795 target) ; Learning rate = 0.000384 ; Loss = 1.452561\n",
      "2024-12-03 19:00:10.620000: I runner.py:310] Step = 53200 ; steps/s = 1.64, tokens/s = 78390 (34545 source, 43845 target) ; Learning rate = 0.000383 ; Loss = 1.451782\n",
      "2024-12-03 19:01:10.950000: I runner.py:310] Step = 53300 ; steps/s = 1.66, tokens/s = 77612 (34228 source, 43384 target) ; Learning rate = 0.000383 ; Loss = 1.449175\n",
      "2024-12-03 19:02:11.739000: I runner.py:310] Step = 53400 ; steps/s = 1.65, tokens/s = 78396 (34522 source, 43874 target) ; Learning rate = 0.000382 ; Loss = 1.458457\n",
      "2024-12-03 19:03:12.336000: I runner.py:310] Step = 53500 ; steps/s = 1.65, tokens/s = 78006 (34384 source, 43622 target) ; Learning rate = 0.000382 ; Loss = 1.501851\n",
      "2024-12-03 19:04:12.947000: I runner.py:310] Step = 53600 ; steps/s = 1.65, tokens/s = 77831 (34269 source, 43562 target) ; Learning rate = 0.000382 ; Loss = 1.454996\n",
      "2024-12-03 19:05:13.724000: I runner.py:310] Step = 53700 ; steps/s = 1.65, tokens/s = 78413 (34533 source, 43880 target) ; Learning rate = 0.000381 ; Loss = 1.456304\n",
      "2024-12-03 19:06:14.058000: I runner.py:310] Step = 53800 ; steps/s = 1.66, tokens/s = 77599 (34216 source, 43383 target) ; Learning rate = 0.000381 ; Loss = 1.453978\n",
      "2024-12-03 19:07:14.824000: I runner.py:310] Step = 53900 ; steps/s = 1.65, tokens/s = 78410 (34524 source, 43886 target) ; Learning rate = 0.000381 ; Loss = 1.461385\n",
      "2024-12-03 19:08:15.632000: I runner.py:310] Step = 54000 ; steps/s = 1.64, tokens/s = 78362 (34515 source, 43847 target) ; Learning rate = 0.000380 ; Loss = 1.458006\n",
      "2024-12-03 19:09:15.953000: I runner.py:310] Step = 54100 ; steps/s = 1.66, tokens/s = 77587 (34207 source, 43380 target) ; Learning rate = 0.000380 ; Loss = 1.453648\n",
      "2024-12-03 19:10:16.778000: I runner.py:310] Step = 54200 ; steps/s = 1.64, tokens/s = 78348 (34500 source, 43848 target) ; Learning rate = 0.000380 ; Loss = 1.452062\n",
      "2024-12-03 19:11:17.133000: I runner.py:310] Step = 54300 ; steps/s = 1.66, tokens/s = 77554 (34197 source, 43357 target) ; Learning rate = 0.000379 ; Loss = 1.450660\n",
      "2024-12-03 19:12:17.884000: I runner.py:310] Step = 54400 ; steps/s = 1.65, tokens/s = 78428 (34534 source, 43894 target) ; Learning rate = 0.000379 ; Loss = 1.456794\n",
      "2024-12-03 19:13:18.736000: I runner.py:310] Step = 54500 ; steps/s = 1.64, tokens/s = 78329 (34507 source, 43822 target) ; Learning rate = 0.000379 ; Loss = 1.454160\n",
      "2024-12-03 19:14:19.120000: I runner.py:310] Step = 54600 ; steps/s = 1.66, tokens/s = 77497 (34158 source, 43339 target) ; Learning rate = 0.000378 ; Loss = 1.458546\n",
      "2024-12-03 19:15:19.896000: I runner.py:310] Step = 54700 ; steps/s = 1.65, tokens/s = 78416 (34527 source, 43889 target) ; Learning rate = 0.000378 ; Loss = 1.456155\n",
      "2024-12-03 19:16:20.226000: I runner.py:310] Step = 54800 ; steps/s = 1.66, tokens/s = 77612 (34243 source, 43369 target) ; Learning rate = 0.000378 ; Loss = 1.456329\n",
      "2024-12-03 19:17:20.996000: I runner.py:310] Step = 54900 ; steps/s = 1.65, tokens/s = 78375 (34492 source, 43883 target) ; Learning rate = 0.000377 ; Loss = 1.452619\n",
      "2024-12-03 19:18:21.785000: I runner.py:310] Step = 55000 ; steps/s = 1.65, tokens/s = 78408 (34533 source, 43875 target) ; Learning rate = 0.000377 ; Loss = 1.448871\n",
      "2024-12-03 19:18:21.787000: I training.py:192] Running evaluation for step 55000\n",
      "2024-12-03 19:19:11.635000: I training.py:192] Evaluation result for step 55000: loss = 0.829825 ; perplexity = 2.292917\n",
      "2024-12-03 19:20:11.833000: I runner.py:310] Step = 55100 ; steps/s = 1.66, tokens/s = 77763 (34283 source, 43480 target) ; Learning rate = 0.000377 ; Loss = 1.449474\n",
      "2024-12-03 19:21:12.602000: I runner.py:310] Step = 55200 ; steps/s = 1.65, tokens/s = 78409 (34540 source, 43869 target) ; Learning rate = 0.000376 ; Loss = 1.458203\n",
      "2024-12-03 19:22:13.374000: I runner.py:310] Step = 55300 ; steps/s = 1.65, tokens/s = 78428 (34539 source, 43889 target) ; Learning rate = 0.000376 ; Loss = 1.459703\n",
      "2024-12-03 19:23:13.726000: I runner.py:310] Step = 55400 ; steps/s = 1.66, tokens/s = 77572 (34202 source, 43370 target) ; Learning rate = 0.000376 ; Loss = 1.450008\n",
      "2024-12-03 19:24:14.508000: I runner.py:310] Step = 55500 ; steps/s = 1.65, tokens/s = 78370 (34500 source, 43870 target) ; Learning rate = 0.000375 ; Loss = 1.453473\n",
      "2024-12-03 19:25:14.776000: I runner.py:310] Step = 55600 ; steps/s = 1.66, tokens/s = 77673 (34246 source, 43427 target) ; Learning rate = 0.000375 ; Loss = 1.448689\n",
      "2024-12-03 19:26:15.535000: I runner.py:310] Step = 55700 ; steps/s = 1.65, tokens/s = 78423 (34538 source, 43885 target) ; Learning rate = 0.000375 ; Loss = 1.448066\n",
      "2024-12-03 19:27:16.277000: I runner.py:310] Step = 55800 ; steps/s = 1.65, tokens/s = 78475 (34572 source, 43903 target) ; Learning rate = 0.000374 ; Loss = 1.455304\n",
      "2024-12-03 19:28:16.582000: I runner.py:310] Step = 55900 ; steps/s = 1.66, tokens/s = 77599 (34202 source, 43397 target) ; Learning rate = 0.000374 ; Loss = 1.446304\n",
      "2024-12-03 19:29:17.357000: I runner.py:310] Step = 56000 ; steps/s = 1.65, tokens/s = 78383 (34513 source, 43870 target) ; Learning rate = 0.000374 ; Loss = 1.460706\n",
      "2024-12-03 19:30:17.757000: I runner.py:310] Step = 56100 ; steps/s = 1.66, tokens/s = 77562 (34221 source, 43341 target) ; Learning rate = 0.000373 ; Loss = 1.446914\n",
      "2024-12-03 19:31:18.472000: I runner.py:310] Step = 56200 ; steps/s = 1.65, tokens/s = 78487 (34545 source, 43942 target) ; Learning rate = 0.000373 ; Loss = 1.453671\n",
      "2024-12-03 19:32:19.248000: I runner.py:310] Step = 56300 ; steps/s = 1.65, tokens/s = 78394 (34533 source, 43861 target) ; Learning rate = 0.000373 ; Loss = 1.456973\n",
      "2024-12-03 19:33:19.519000: I runner.py:310] Step = 56400 ; steps/s = 1.66, tokens/s = 77635 (34226 source, 43409 target) ; Learning rate = 0.000372 ; Loss = 1.452390\n",
      "2024-12-03 19:34:20.325000: I runner.py:310] Step = 56500 ; steps/s = 1.64, tokens/s = 78368 (34505 source, 43863 target) ; Learning rate = 0.000372 ; Loss = 1.451874\n",
      "2024-12-03 19:35:21.113000: I runner.py:310] Step = 56600 ; steps/s = 1.65, tokens/s = 78402 (34531 source, 43871 target) ; Learning rate = 0.000372 ; Loss = 1.453529\n",
      "2024-12-03 19:36:21.421000: I runner.py:310] Step = 56700 ; steps/s = 1.66, tokens/s = 77650 (34256 source, 43394 target) ; Learning rate = 0.000371 ; Loss = 1.447783\n",
      "2024-12-03 19:37:22.144000: I runner.py:310] Step = 56800 ; steps/s = 1.65, tokens/s = 78463 (34551 source, 43912 target) ; Learning rate = 0.000371 ; Loss = 1.451006\n",
      "2024-12-03 19:38:22.424000: I runner.py:310] Step = 56900 ; steps/s = 1.66, tokens/s = 77629 (34213 source, 43416 target) ; Learning rate = 0.000371 ; Loss = 1.444574\n",
      "2024-12-03 19:39:23.165000: I runner.py:310] Step = 57000 ; steps/s = 1.65, tokens/s = 78447 (34534 source, 43913 target) ; Learning rate = 0.000370 ; Loss = 1.453760\n",
      "2024-12-03 19:40:23.931000: I runner.py:310] Step = 57100 ; steps/s = 1.65, tokens/s = 78442 (34562 source, 43880 target) ; Learning rate = 0.000370 ; Loss = 1.456134\n",
      "2024-12-03 19:41:24.196000: I runner.py:310] Step = 57200 ; steps/s = 1.66, tokens/s = 77638 (34226 source, 43412 target) ; Learning rate = 0.000370 ; Loss = 1.454537\n",
      "2024-12-03 19:42:24.959000: I runner.py:310] Step = 57300 ; steps/s = 1.65, tokens/s = 78439 (34557 source, 43882 target) ; Learning rate = 0.000369 ; Loss = 1.453037\n",
      "2024-12-03 19:43:25.265000: I runner.py:310] Step = 57400 ; steps/s = 1.66, tokens/s = 77643 (34234 source, 43409 target) ; Learning rate = 0.000369 ; Loss = 1.448705\n",
      "2024-12-03 19:44:26.073000: I runner.py:310] Step = 57500 ; steps/s = 1.64, tokens/s = 78329 (34470 source, 43859 target) ; Learning rate = 0.000369 ; Loss = 1.448487\n",
      "2024-12-03 19:45:26.759000: I runner.py:310] Step = 57600 ; steps/s = 1.65, tokens/s = 78530 (34590 source, 43940 target) ; Learning rate = 0.000368 ; Loss = 1.457708\n",
      "2024-12-03 19:46:27.084000: I runner.py:310] Step = 57700 ; steps/s = 1.66, tokens/s = 77580 (34198 source, 43382 target) ; Learning rate = 0.000368 ; Loss = 1.448902\n",
      "2024-12-03 19:47:27.858000: I runner.py:310] Step = 57800 ; steps/s = 1.65, tokens/s = 78416 (34538 source, 43878 target) ; Learning rate = 0.000368 ; Loss = 1.454268\n",
      "2024-12-03 19:48:28.574000: I runner.py:310] Step = 57900 ; steps/s = 1.65, tokens/s = 78507 (34591 source, 43916 target) ; Learning rate = 0.000367 ; Loss = 1.456099\n",
      "2024-12-03 19:49:28.874000: I runner.py:310] Step = 58000 ; steps/s = 1.66, tokens/s = 77637 (34226 source, 43411 target) ; Learning rate = 0.000367 ; Loss = 1.452692\n",
      "2024-12-03 19:50:29.724000: I runner.py:310] Step = 58100 ; steps/s = 1.64, tokens/s = 78297 (34482 source, 43815 target) ; Learning rate = 0.000367 ; Loss = 1.449960\n",
      "2024-12-03 19:51:30.045000: I runner.py:310] Step = 58200 ; steps/s = 1.66, tokens/s = 77597 (34209 source, 43388 target) ; Learning rate = 0.000366 ; Loss = 1.448538\n",
      "2024-12-03 19:52:30.772000: I runner.py:310] Step = 58300 ; steps/s = 1.65, tokens/s = 78435 (34525 source, 43910 target) ; Learning rate = 0.000366 ; Loss = 1.451704\n",
      "2024-12-03 19:53:31.542000: I runner.py:310] Step = 58400 ; steps/s = 1.65, tokens/s = 78426 (34549 source, 43877 target) ; Learning rate = 0.000366 ; Loss = 1.454922\n",
      "2024-12-03 19:54:31.818000: I runner.py:310] Step = 58500 ; steps/s = 1.66, tokens/s = 77675 (34245 source, 43430 target) ; Learning rate = 0.000365 ; Loss = 1.452271\n",
      "2024-12-03 19:55:32.597000: I runner.py:310] Step = 58600 ; steps/s = 1.65, tokens/s = 78397 (34518 source, 43879 target) ; Learning rate = 0.000365 ; Loss = 1.452748\n",
      "2024-12-03 19:56:32.911000: I runner.py:310] Step = 58700 ; steps/s = 1.66, tokens/s = 77636 (34252 source, 43384 target) ; Learning rate = 0.000365 ; Loss = 1.449893\n",
      "2024-12-03 19:57:33.639000: I runner.py:310] Step = 58800 ; steps/s = 1.65, tokens/s = 78424 (34520 source, 43904 target) ; Learning rate = 0.000365 ; Loss = 1.445647\n",
      "2024-12-03 19:58:34.366000: I runner.py:310] Step = 58900 ; steps/s = 1.65, tokens/s = 78493 (34588 source, 43905 target) ; Learning rate = 0.000364 ; Loss = 1.457349\n",
      "2024-12-03 19:59:34.622000: I runner.py:310] Step = 59000 ; steps/s = 1.66, tokens/s = 77689 (34239 source, 43450 target) ; Learning rate = 0.000364 ; Loss = 1.452228\n",
      "2024-12-03 20:00:35.382000: I runner.py:310] Step = 59100 ; steps/s = 1.65, tokens/s = 78418 (34521 source, 43897 target) ; Learning rate = 0.000364 ; Loss = 1.451188\n",
      "2024-12-03 20:01:36.199000: I runner.py:310] Step = 59200 ; steps/s = 1.64, tokens/s = 78362 (34527 source, 43835 target) ; Learning rate = 0.000363 ; Loss = 1.453840\n",
      "2024-12-03 20:02:36.547000: I runner.py:310] Step = 59300 ; steps/s = 1.66, tokens/s = 77546 (34177 source, 43369 target) ; Learning rate = 0.000363 ; Loss = 1.451399\n",
      "2024-12-03 20:03:37.320000: I runner.py:310] Step = 59400 ; steps/s = 1.65, tokens/s = 78407 (34536 source, 43871 target) ; Learning rate = 0.000363 ; Loss = 1.449499\n",
      "2024-12-03 20:04:37.648000: I runner.py:310] Step = 59500 ; steps/s = 1.66, tokens/s = 77609 (34216 source, 43393 target) ; Learning rate = 0.000362 ; Loss = 1.451632\n",
      "2024-12-03 20:05:38.413000: I runner.py:310] Step = 59600 ; steps/s = 1.65, tokens/s = 78419 (34534 source, 43885 target) ; Learning rate = 0.000362 ; Loss = 1.453190\n",
      "2024-12-03 20:06:39.125000: I runner.py:310] Step = 59700 ; steps/s = 1.65, tokens/s = 78493 (34571 source, 43922 target) ; Learning rate = 0.000362 ; Loss = 1.451600\n",
      "2024-12-03 20:07:39.395000: I runner.py:310] Step = 59800 ; steps/s = 1.66, tokens/s = 77676 (34246 source, 43430 target) ; Learning rate = 0.000361 ; Loss = 1.450820\n",
      "2024-12-03 20:08:40.140000: I runner.py:310] Step = 59900 ; steps/s = 1.65, tokens/s = 78436 (34533 source, 43903 target) ; Learning rate = 0.000361 ; Loss = 1.448499\n",
      "2024-12-03 20:09:40.498000: I runner.py:310] Step = 60000 ; steps/s = 1.66, tokens/s = 77562 (34218 source, 43344 target) ; Learning rate = 0.000361 ; Loss = 1.442979\n",
      "2024-12-03 20:09:42.352000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-60000\n",
      "2024-12-03 20:09:42.352000: I training.py:192] Running evaluation for step 60000\n",
      "2024-12-03 20:10:31.066000: I training.py:192] Evaluation result for step 60000: loss = 0.845268 ; perplexity = 2.328601\n",
      "2024-12-03 20:11:31.811000: I runner.py:310] Step = 60100 ; steps/s = 1.65, tokens/s = 78532 (34583 source, 43949 target) ; Learning rate = 0.000361 ; Loss = 1.450009\n",
      "2024-12-03 20:12:32.619000: I runner.py:310] Step = 60200 ; steps/s = 1.64, tokens/s = 78361 (34508 source, 43853 target) ; Learning rate = 0.000360 ; Loss = 1.449758\n",
      "2024-12-03 20:13:32.927000: I runner.py:310] Step = 60300 ; steps/s = 1.66, tokens/s = 77569 (34180 source, 43389 target) ; Learning rate = 0.000360 ; Loss = 1.449508\n",
      "2024-12-03 20:14:33.788000: I runner.py:310] Step = 60400 ; steps/s = 1.64, tokens/s = 78305 (34483 source, 43822 target) ; Learning rate = 0.000360 ; Loss = 1.449435\n",
      "2024-12-03 20:15:34.587000: I runner.py:310] Step = 60500 ; steps/s = 1.64, tokens/s = 78398 (34538 source, 43860 target) ; Learning rate = 0.000359 ; Loss = 1.446282\n",
      "2024-12-03 20:16:34.877000: I runner.py:310] Step = 60600 ; steps/s = 1.66, tokens/s = 77629 (34215 source, 43414 target) ; Learning rate = 0.000359 ; Loss = 1.448540\n",
      "2024-12-03 20:17:35.640000: I runner.py:310] Step = 60700 ; steps/s = 1.65, tokens/s = 78422 (34546 source, 43876 target) ; Learning rate = 0.000359 ; Loss = 1.450084\n",
      "2024-12-03 20:18:35.963000: I runner.py:310] Step = 60800 ; steps/s = 1.66, tokens/s = 77633 (34245 source, 43388 target) ; Learning rate = 0.000358 ; Loss = 1.443959\n",
      "2024-12-03 20:19:36.808000: I runner.py:310] Step = 60900 ; steps/s = 1.64, tokens/s = 78306 (34462 source, 43844 target) ; Learning rate = 0.000358 ; Loss = 1.451418\n",
      "2024-12-03 20:20:37.541000: I runner.py:310] Step = 61000 ; steps/s = 1.65, tokens/s = 78472 (34573 source, 43899 target) ; Learning rate = 0.000358 ; Loss = 1.452276\n",
      "2024-12-03 20:21:37.879000: I runner.py:310] Step = 61100 ; steps/s = 1.66, tokens/s = 77565 (34201 source, 43364 target) ; Learning rate = 0.000358 ; Loss = 1.446929\n",
      "2024-12-03 20:22:38.657000: I runner.py:310] Step = 61200 ; steps/s = 1.65, tokens/s = 78411 (34524 source, 43887 target) ; Learning rate = 0.000357 ; Loss = 1.446537\n",
      "2024-12-03 20:23:38.993000: I runner.py:310] Step = 61300 ; steps/s = 1.66, tokens/s = 77586 (34218 source, 43368 target) ; Learning rate = 0.000357 ; Loss = 1.448229\n",
      "2024-12-03 20:24:39.795000: I runner.py:310] Step = 61400 ; steps/s = 1.64, tokens/s = 78364 (34482 source, 43882 target) ; Learning rate = 0.000357 ; Loss = 1.448145\n",
      "2024-12-03 20:25:40.492000: I runner.py:310] Step = 61500 ; steps/s = 1.65, tokens/s = 78514 (34596 source, 43918 target) ; Learning rate = 0.000356 ; Loss = 1.447461\n",
      "2024-12-03 20:26:40.763000: I runner.py:310] Step = 61600 ; steps/s = 1.66, tokens/s = 77648 (34232 source, 43416 target) ; Learning rate = 0.000356 ; Loss = 1.446932\n",
      "2024-12-03 20:27:41.527000: I runner.py:310] Step = 61700 ; steps/s = 1.65, tokens/s = 78444 (34551 source, 43893 target) ; Learning rate = 0.000356 ; Loss = 1.453569\n",
      "2024-12-03 20:28:42.361000: I runner.py:310] Step = 61800 ; steps/s = 1.64, tokens/s = 78318 (34487 source, 43831 target) ; Learning rate = 0.000356 ; Loss = 1.448835\n",
      "2024-12-03 20:29:42.607000: I runner.py:310] Step = 61900 ; steps/s = 1.66, tokens/s = 77672 (34229 source, 43443 target) ; Learning rate = 0.000355 ; Loss = 1.445469\n",
      "2024-12-03 20:30:43.347000: I runner.py:310] Step = 62000 ; steps/s = 1.65, tokens/s = 78466 (34573 source, 43893 target) ; Learning rate = 0.000355 ; Loss = 1.449074\n",
      "2024-12-03 20:31:43.722000: I runner.py:310] Step = 62100 ; steps/s = 1.66, tokens/s = 77533 (34178 source, 43355 target) ; Learning rate = 0.000355 ; Loss = 1.444090\n",
      "2024-12-03 20:32:44.489000: I runner.py:310] Step = 62200 ; steps/s = 1.65, tokens/s = 78395 (34515 source, 43880 target) ; Learning rate = 0.000354 ; Loss = 1.451057\n",
      "2024-12-03 20:33:45.348000: I runner.py:310] Step = 62300 ; steps/s = 1.64, tokens/s = 78336 (34507 source, 43829 target) ; Learning rate = 0.000354 ; Loss = 1.449268\n",
      "2024-12-03 20:34:45.650000: I runner.py:310] Step = 62400 ; steps/s = 1.66, tokens/s = 77624 (34219 source, 43405 target) ; Learning rate = 0.000354 ; Loss = 1.448135\n",
      "2024-12-03 20:35:46.421000: I runner.py:310] Step = 62500 ; steps/s = 1.65, tokens/s = 78415 (34531 source, 43884 target) ; Learning rate = 0.000354 ; Loss = 1.446860\n",
      "2024-12-03 20:36:46.692000: I runner.py:310] Step = 62600 ; steps/s = 1.66, tokens/s = 77670 (34266 source, 43404 target) ; Learning rate = 0.000353 ; Loss = 1.447330\n",
      "2024-12-03 20:37:47.496000: I runner.py:310] Step = 62700 ; steps/s = 1.64, tokens/s = 78359 (34503 source, 43856 target) ; Learning rate = 0.000353 ; Loss = 1.446311\n",
      "2024-12-03 20:38:48.274000: I runner.py:310] Step = 62800 ; steps/s = 1.65, tokens/s = 78401 (34521 source, 43880 target) ; Learning rate = 0.000353 ; Loss = 1.451918\n",
      "2024-12-03 20:39:48.610000: I runner.py:310] Step = 62900 ; steps/s = 1.66, tokens/s = 77554 (34175 source, 43379 target) ; Learning rate = 0.000352 ; Loss = 1.451532\n",
      "2024-12-03 20:40:49.378000: I runner.py:310] Step = 63000 ; steps/s = 1.65, tokens/s = 78428 (34555 source, 43873 target) ; Learning rate = 0.000352 ; Loss = 1.451250\n",
      "2024-12-03 20:41:50.228000: I runner.py:310] Step = 63100 ; steps/s = 1.64, tokens/s = 78336 (34506 source, 43830 target) ; Learning rate = 0.000352 ; Loss = 1.449718\n",
      "2024-12-03 20:42:50.563000: I runner.py:310] Step = 63200 ; steps/s = 1.66, tokens/s = 77582 (34208 source, 43374 target) ; Learning rate = 0.000352 ; Loss = 1.448219\n",
      "2024-12-03 20:43:51.333000: I runner.py:310] Step = 63300 ; steps/s = 1.65, tokens/s = 78420 (34535 source, 43885 target) ; Learning rate = 0.000351 ; Loss = 1.451955\n",
      "2024-12-03 20:44:51.690000: I runner.py:310] Step = 63400 ; steps/s = 1.66, tokens/s = 77547 (34189 source, 43358 target) ; Learning rate = 0.000351 ; Loss = 1.447720\n",
      "2024-12-03 20:45:52.459000: I runner.py:310] Step = 63500 ; steps/s = 1.65, tokens/s = 78401 (34515 source, 43886 target) ; Learning rate = 0.000351 ; Loss = 1.446401\n",
      "2024-12-03 20:46:53.233000: I runner.py:310] Step = 63600 ; steps/s = 1.65, tokens/s = 78426 (34552 source, 43874 target) ; Learning rate = 0.000350 ; Loss = 1.450289\n",
      "2024-12-03 20:47:53.527000: I runner.py:310] Step = 63700 ; steps/s = 1.66, tokens/s = 77645 (34229 source, 43416 target) ; Learning rate = 0.000350 ; Loss = 1.456393\n",
      "2024-12-03 20:48:54.229000: I runner.py:310] Step = 63800 ; steps/s = 1.65, tokens/s = 78490 (34561 source, 43929 target) ; Learning rate = 0.000350 ; Loss = 1.449703\n",
      "2024-12-03 20:49:54.532000: I runner.py:310] Step = 63900 ; steps/s = 1.66, tokens/s = 77639 (34240 source, 43399 target) ; Learning rate = 0.000350 ; Loss = 1.439971\n",
      "2024-12-03 20:50:55.299000: I runner.py:310] Step = 64000 ; steps/s = 1.65, tokens/s = 78378 (34507 source, 43871 target) ; Learning rate = 0.000349 ; Loss = 1.438446\n",
      "2024-12-03 20:51:56.048000: I runner.py:310] Step = 64100 ; steps/s = 1.65, tokens/s = 78468 (34565 source, 43903 target) ; Learning rate = 0.000349 ; Loss = 1.461089\n",
      "2024-12-03 20:52:56.414000: I runner.py:310] Step = 64200 ; steps/s = 1.66, tokens/s = 77515 (34158 source, 43357 target) ; Learning rate = 0.000349 ; Loss = 1.442777\n",
      "2024-12-03 20:53:57.214000: I runner.py:310] Step = 64300 ; steps/s = 1.64, tokens/s = 78397 (34530 source, 43867 target) ; Learning rate = 0.000349 ; Loss = 1.454632\n",
      "2024-12-03 20:54:58.031000: I runner.py:310] Step = 64400 ; steps/s = 1.64, tokens/s = 78353 (34513 source, 43840 target) ; Learning rate = 0.000348 ; Loss = 1.446893\n",
      "2024-12-03 20:55:58.315000: I runner.py:310] Step = 64500 ; steps/s = 1.66, tokens/s = 77623 (34209 source, 43414 target) ; Learning rate = 0.000348 ; Loss = 1.443524\n",
      "2024-12-03 20:56:59.029000: I runner.py:310] Step = 64600 ; steps/s = 1.65, tokens/s = 78523 (34600 source, 43923 target) ; Learning rate = 0.000348 ; Loss = 1.452670\n",
      "2024-12-03 20:57:59.299000: I runner.py:310] Step = 64700 ; steps/s = 1.66, tokens/s = 77640 (34221 source, 43419 target) ; Learning rate = 0.000347 ; Loss = 1.445660\n",
      "2024-12-03 20:59:00.072000: I runner.py:310] Step = 64800 ; steps/s = 1.65, tokens/s = 78402 (34523 source, 43879 target) ; Learning rate = 0.000347 ; Loss = 1.442658\n",
      "2024-12-03 21:00:00.878000: I runner.py:310] Step = 64900 ; steps/s = 1.64, tokens/s = 78389 (34529 source, 43860 target) ; Learning rate = 0.000347 ; Loss = 1.453950\n",
      "2024-12-03 21:01:01.243000: I runner.py:310] Step = 65000 ; steps/s = 1.66, tokens/s = 77570 (34205 source, 43365 target) ; Learning rate = 0.000347 ; Loss = 1.457676\n",
      "2024-12-03 21:01:01.244000: I training.py:192] Running evaluation for step 65000\n",
      "2024-12-03 21:01:49.052000: I training.py:192] Evaluation result for step 65000: loss = 0.841372 ; perplexity = 2.319548\n",
      "2024-12-03 21:02:49.690000: I runner.py:310] Step = 65100 ; steps/s = 1.65, tokens/s = 78616 (34636 source, 43980 target) ; Learning rate = 0.000346 ; Loss = 1.446703\n",
      "2024-12-03 21:03:49.996000: I runner.py:310] Step = 65200 ; steps/s = 1.66, tokens/s = 77580 (34186 source, 43394 target) ; Learning rate = 0.000346 ; Loss = 1.439562\n",
      "2024-12-03 21:04:50.823000: I runner.py:310] Step = 65300 ; steps/s = 1.64, tokens/s = 78336 (34491 source, 43845 target) ; Learning rate = 0.000346 ; Loss = 1.446460\n",
      "2024-12-03 21:05:51.584000: I runner.py:310] Step = 65400 ; steps/s = 1.65, tokens/s = 78436 (34559 source, 43877 target) ; Learning rate = 0.000346 ; Loss = 1.444498\n",
      "2024-12-03 21:06:51.863000: I runner.py:310] Step = 65500 ; steps/s = 1.66, tokens/s = 77624 (34199 source, 43425 target) ; Learning rate = 0.000345 ; Loss = 1.445341\n",
      "2024-12-03 21:07:52.669000: I runner.py:310] Step = 65600 ; steps/s = 1.64, tokens/s = 78382 (34528 source, 43854 target) ; Learning rate = 0.000345 ; Loss = 1.452585\n",
      "2024-12-03 21:08:53.377000: I runner.py:310] Step = 65700 ; steps/s = 1.65, tokens/s = 78518 (34584 source, 43934 target) ; Learning rate = 0.000345 ; Loss = 1.449602\n",
      "2024-12-03 21:09:53.733000: I runner.py:310] Step = 65800 ; steps/s = 1.66, tokens/s = 77544 (34182 source, 43362 target) ; Learning rate = 0.000345 ; Loss = 1.446844\n",
      "2024-12-03 21:10:54.554000: I runner.py:310] Step = 65900 ; steps/s = 1.64, tokens/s = 78353 (34515 source, 43838 target) ; Learning rate = 0.000344 ; Loss = 1.447000\n",
      "2024-12-03 21:11:54.944000: I runner.py:310] Step = 66000 ; steps/s = 1.66, tokens/s = 77504 (34170 source, 43334 target) ; Learning rate = 0.000344 ; Loss = 1.439837\n",
      "2024-12-03 21:12:55.763000: I runner.py:310] Step = 66100 ; steps/s = 1.64, tokens/s = 78355 (34501 source, 43854 target) ; Learning rate = 0.000344 ; Loss = 1.451639\n",
      "2024-12-03 21:13:56.556000: I runner.py:310] Step = 66200 ; steps/s = 1.65, tokens/s = 78390 (34517 source, 43873 target) ; Learning rate = 0.000344 ; Loss = 1.447122\n",
      "2024-12-03 21:14:56.818000: I runner.py:310] Step = 66300 ; steps/s = 1.66, tokens/s = 77710 (34270 source, 43440 target) ; Learning rate = 0.000343 ; Loss = 1.445952\n",
      "2024-12-03 21:15:57.581000: I runner.py:310] Step = 66400 ; steps/s = 1.65, tokens/s = 78397 (34514 source, 43883 target) ; Learning rate = 0.000343 ; Loss = 1.441948\n",
      "2024-12-03 21:16:57.885000: I runner.py:310] Step = 66500 ; steps/s = 1.66, tokens/s = 77635 (34242 source, 43393 target) ; Learning rate = 0.000343 ; Loss = 1.449476\n",
      "2024-12-03 21:17:58.685000: I runner.py:310] Step = 66600 ; steps/s = 1.64, tokens/s = 78357 (34500 source, 43857 target) ; Learning rate = 0.000342 ; Loss = 1.443526\n",
      "2024-12-03 21:18:59.464000: I runner.py:310] Step = 66700 ; steps/s = 1.65, tokens/s = 78419 (34546 source, 43873 target) ; Learning rate = 0.000342 ; Loss = 1.444130\n",
      "2024-12-03 21:19:59.790000: I runner.py:310] Step = 66800 ; steps/s = 1.66, tokens/s = 77567 (34180 source, 43387 target) ; Learning rate = 0.000342 ; Loss = 1.446927\n",
      "2024-12-03 21:21:00.555000: I runner.py:310] Step = 66900 ; steps/s = 1.65, tokens/s = 78424 (34550 source, 43874 target) ; Learning rate = 0.000342 ; Loss = 1.448266\n",
      "2024-12-03 21:22:01.371000: I runner.py:310] Step = 67000 ; steps/s = 1.64, tokens/s = 78371 (34515 source, 43856 target) ; Learning rate = 0.000341 ; Loss = 1.447207\n",
      "2024-12-03 21:23:01.615000: I runner.py:310] Step = 67100 ; steps/s = 1.66, tokens/s = 77699 (34254 source, 43445 target) ; Learning rate = 0.000341 ; Loss = 1.442754\n",
      "2024-12-03 21:24:02.390000: I runner.py:310] Step = 67200 ; steps/s = 1.65, tokens/s = 78407 (34538 source, 43869 target) ; Learning rate = 0.000341 ; Loss = 1.446125\n",
      "2024-12-03 21:25:02.668000: I runner.py:310] Step = 67300 ; steps/s = 1.66, tokens/s = 77659 (34234 source, 43425 target) ; Learning rate = 0.000341 ; Loss = 1.443117\n",
      "2024-12-03 21:26:03.409000: I runner.py:310] Step = 67400 ; steps/s = 1.65, tokens/s = 78426 (34535 source, 43891 target) ; Learning rate = 0.000340 ; Loss = 1.446469\n",
      "2024-12-03 21:27:04.210000: I runner.py:310] Step = 67500 ; steps/s = 1.64, tokens/s = 78400 (34533 source, 43867 target) ; Learning rate = 0.000340 ; Loss = 1.445907\n",
      "2024-12-03 21:28:04.487000: I runner.py:310] Step = 67600 ; steps/s = 1.66, tokens/s = 77671 (34247 source, 43424 target) ; Learning rate = 0.000340 ; Loss = 1.443739\n",
      "2024-12-03 21:29:05.234000: I runner.py:310] Step = 67700 ; steps/s = 1.65, tokens/s = 78439 (34542 source, 43897 target) ; Learning rate = 0.000340 ; Loss = 1.443662\n",
      "2024-12-03 21:30:05.578000: I runner.py:310] Step = 67800 ; steps/s = 1.66, tokens/s = 77578 (34210 source, 43368 target) ; Learning rate = 0.000339 ; Loss = 1.442278\n",
      "2024-12-03 21:31:06.303000: I runner.py:310] Step = 67900 ; steps/s = 1.65, tokens/s = 78446 (34532 source, 43914 target) ; Learning rate = 0.000339 ; Loss = 1.445078\n",
      "2024-12-03 21:32:07.056000: I runner.py:310] Step = 68000 ; steps/s = 1.65, tokens/s = 78454 (34549 source, 43905 target) ; Learning rate = 0.000339 ; Loss = 1.445924\n",
      "2024-12-03 21:33:07.396000: I runner.py:310] Step = 68100 ; steps/s = 1.66, tokens/s = 77565 (34199 source, 43366 target) ; Learning rate = 0.000339 ; Loss = 1.450620\n",
      "2024-12-03 21:34:08.082000: I runner.py:310] Step = 68200 ; steps/s = 1.65, tokens/s = 78526 (34578 source, 43948 target) ; Learning rate = 0.000338 ; Loss = 1.445790\n",
      "2024-12-03 21:35:08.850000: I runner.py:310] Step = 68300 ; steps/s = 1.65, tokens/s = 78419 (34544 source, 43875 target) ; Learning rate = 0.000338 ; Loss = 1.451634\n",
      "2024-12-03 21:36:09.176000: I runner.py:310] Step = 68400 ; steps/s = 1.66, tokens/s = 77564 (34185 source, 43379 target) ; Learning rate = 0.000338 ; Loss = 1.438313\n",
      "2024-12-03 21:37:09.888000: I runner.py:310] Step = 68500 ; steps/s = 1.65, tokens/s = 78518 (34592 source, 43926 target) ; Learning rate = 0.000338 ; Loss = 1.449712\n",
      "2024-12-03 21:38:10.200000: I runner.py:310] Step = 68600 ; steps/s = 1.66, tokens/s = 77608 (34211 source, 43397 target) ; Learning rate = 0.000337 ; Loss = 1.442355\n",
      "2024-12-03 21:39:11.014000: I runner.py:310] Step = 68700 ; steps/s = 1.64, tokens/s = 78337 (34490 source, 43847 target) ; Learning rate = 0.000337 ; Loss = 1.447778\n",
      "2024-12-03 21:40:11.854000: I runner.py:310] Step = 68800 ; steps/s = 1.64, tokens/s = 78350 (34521 source, 43829 target) ; Learning rate = 0.000337 ; Loss = 1.440447\n",
      "2024-12-03 21:41:12.169000: I runner.py:310] Step = 68900 ; steps/s = 1.66, tokens/s = 77598 (34200 source, 43398 target) ; Learning rate = 0.000337 ; Loss = 1.443152\n",
      "2024-12-03 21:42:12.947000: I runner.py:310] Step = 69000 ; steps/s = 1.65, tokens/s = 78418 (34551 source, 43867 target) ; Learning rate = 0.000336 ; Loss = 1.445141\n",
      "2024-12-03 21:43:13.208000: I runner.py:310] Step = 69100 ; steps/s = 1.66, tokens/s = 77694 (34256 source, 43438 target) ; Learning rate = 0.000336 ; Loss = 1.439435\n",
      "2024-12-03 21:44:14.027000: I runner.py:310] Step = 69200 ; steps/s = 1.64, tokens/s = 78319 (34469 source, 43850 target) ; Learning rate = 0.000336 ; Loss = 1.442227\n",
      "2024-12-03 21:45:14.816000: I runner.py:310] Step = 69300 ; steps/s = 1.65, tokens/s = 78421 (34556 source, 43865 target) ; Learning rate = 0.000336 ; Loss = 1.456147\n",
      "2024-12-03 21:46:15.149000: I runner.py:310] Step = 69400 ; steps/s = 1.66, tokens/s = 77558 (34185 source, 43373 target) ; Learning rate = 0.000336 ; Loss = 1.443653\n",
      "2024-12-03 21:47:15.919000: I runner.py:310] Step = 69500 ; steps/s = 1.65, tokens/s = 78403 (34525 source, 43878 target) ; Learning rate = 0.000335 ; Loss = 1.454299\n",
      "2024-12-03 21:48:16.698000: I runner.py:310] Step = 69600 ; steps/s = 1.65, tokens/s = 78419 (34540 source, 43879 target) ; Learning rate = 0.000335 ; Loss = 1.447372\n",
      "2024-12-03 21:49:17.014000: I runner.py:310] Step = 69700 ; steps/s = 1.66, tokens/s = 77591 (34204 source, 43387 target) ; Learning rate = 0.000335 ; Loss = 1.441715\n",
      "2024-12-03 21:50:17.786000: I runner.py:310] Step = 69800 ; steps/s = 1.65, tokens/s = 78416 (34543 source, 43873 target) ; Learning rate = 0.000335 ; Loss = 1.446687\n",
      "2024-12-03 21:51:18.104000: I runner.py:310] Step = 69900 ; steps/s = 1.66, tokens/s = 77605 (34210 source, 43395 target) ; Learning rate = 0.000334 ; Loss = 1.445316\n",
      "2024-12-03 21:52:18.881000: I runner.py:310] Step = 70000 ; steps/s = 1.65, tokens/s = 78391 (34517 source, 43874 target) ; Learning rate = 0.000334 ; Loss = 1.445043\n",
      "2024-12-03 21:52:20.817000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-70000\n",
      "2024-12-03 21:52:20.817000: I training.py:192] Running evaluation for step 70000\n",
      "2024-12-03 21:53:07.772000: I training.py:192] Evaluation result for step 70000: loss = 0.847257 ; perplexity = 2.333238\n",
      "2024-12-03 21:54:08.516000: I runner.py:310] Step = 70100 ; steps/s = 1.65, tokens/s = 78498 (34578 source, 43920 target) ; Learning rate = 0.000334 ; Loss = 1.446337\n",
      "2024-12-03 21:55:08.865000: I runner.py:310] Step = 70200 ; steps/s = 1.66, tokens/s = 77539 (34175 source, 43364 target) ; Learning rate = 0.000334 ; Loss = 1.449039\n",
      "2024-12-03 21:56:09.649000: I runner.py:310] Step = 70300 ; steps/s = 1.65, tokens/s = 78418 (34539 source, 43879 target) ; Learning rate = 0.000333 ; Loss = 1.444897\n",
      "2024-12-03 21:57:09.987000: I runner.py:310] Step = 70400 ; steps/s = 1.66, tokens/s = 77584 (34216 source, 43368 target) ; Learning rate = 0.000333 ; Loss = 1.445146\n",
      "2024-12-03 21:58:10.690000: I runner.py:310] Step = 70500 ; steps/s = 1.65, tokens/s = 78499 (34574 source, 43925 target) ; Learning rate = 0.000333 ; Loss = 1.448073\n",
      "2024-12-03 21:59:11.468000: I runner.py:310] Step = 70600 ; steps/s = 1.65, tokens/s = 78404 (34532 source, 43872 target) ; Learning rate = 0.000333 ; Loss = 1.443325\n",
      "2024-12-03 22:00:11.838000: I runner.py:310] Step = 70700 ; steps/s = 1.66, tokens/s = 77532 (34172 source, 43360 target) ; Learning rate = 0.000332 ; Loss = 1.441621\n",
      "2024-12-03 22:01:12.648000: I runner.py:310] Step = 70800 ; steps/s = 1.64, tokens/s = 78362 (34507 source, 43855 target) ; Learning rate = 0.000332 ; Loss = 1.447728\n",
      "2024-12-03 22:02:13.316000: I runner.py:310] Step = 70900 ; steps/s = 1.65, tokens/s = 78185 (34457 source, 43728 target) ; Learning rate = 0.000332 ; Loss = 1.464951\n",
      "2024-12-03 22:03:13.619000: I runner.py:310] Step = 71000 ; steps/s = 1.66, tokens/s = 77973 (34350 source, 43623 target) ; Learning rate = 0.000332 ; Loss = 1.437576\n",
      "2024-12-03 22:04:14.385000: I runner.py:310] Step = 71100 ; steps/s = 1.65, tokens/s = 78422 (34546 source, 43876 target) ; Learning rate = 0.000331 ; Loss = 1.447633\n",
      "2024-12-03 22:05:14.712000: I runner.py:310] Step = 71200 ; steps/s = 1.66, tokens/s = 77611 (34230 source, 43381 target) ; Learning rate = 0.000331 ; Loss = 1.442578\n",
      "2024-12-03 22:06:15.568000: I runner.py:310] Step = 71300 ; steps/s = 1.64, tokens/s = 78256 (34440 source, 43816 target) ; Learning rate = 0.000331 ; Loss = 1.442105\n",
      "2024-12-03 22:07:16.382000: I runner.py:310] Step = 71400 ; steps/s = 1.64, tokens/s = 78369 (34514 source, 43855 target) ; Learning rate = 0.000331 ; Loss = 1.448076\n",
      "2024-12-03 22:08:16.675000: I runner.py:310] Step = 71500 ; steps/s = 1.66, tokens/s = 77654 (34243 source, 43411 target) ; Learning rate = 0.000331 ; Loss = 1.439930\n",
      "2024-12-03 22:09:17.402000: I runner.py:310] Step = 71600 ; steps/s = 1.65, tokens/s = 78459 (34546 source, 43913 target) ; Learning rate = 0.000330 ; Loss = 1.447034\n",
      "2024-12-03 22:10:17.782000: I runner.py:310] Step = 71700 ; steps/s = 1.66, tokens/s = 77554 (34212 source, 43342 target) ; Learning rate = 0.000330 ; Loss = 1.441070\n",
      "2024-12-03 22:11:18.589000: I runner.py:310] Step = 71800 ; steps/s = 1.64, tokens/s = 78369 (34514 source, 43855 target) ; Learning rate = 0.000330 ; Loss = 1.445884\n",
      "2024-12-03 22:12:19.364000: I runner.py:310] Step = 71900 ; steps/s = 1.65, tokens/s = 78394 (34518 source, 43876 target) ; Learning rate = 0.000330 ; Loss = 1.442139\n",
      "2024-12-03 22:13:19.734000: I runner.py:310] Step = 72000 ; steps/s = 1.66, tokens/s = 77495 (34157 source, 43338 target) ; Learning rate = 0.000329 ; Loss = 1.440999\n",
      "2024-12-03 22:14:20.580000: I runner.py:310] Step = 72100 ; steps/s = 1.64, tokens/s = 78342 (34502 source, 43840 target) ; Learning rate = 0.000329 ; Loss = 1.443913\n",
      "2024-12-03 22:15:21.016000: I runner.py:310] Step = 72200 ; steps/s = 1.65, tokens/s = 77891 (34358 source, 43533 target) ; Learning rate = 0.000329 ; Loss = 1.467996\n",
      "2024-12-03 22:16:21.616000: I runner.py:310] Step = 72300 ; steps/s = 1.65, tokens/s = 78191 (34424 source, 43767 target) ; Learning rate = 0.000329 ; Loss = 1.438416\n",
      "2024-12-03 22:17:22.334000: I runner.py:310] Step = 72400 ; steps/s = 1.65, tokens/s = 78488 (34572 source, 43916 target) ; Learning rate = 0.000328 ; Loss = 1.448115\n",
      "2024-12-03 22:18:22.670000: I runner.py:310] Step = 72500 ; steps/s = 1.66, tokens/s = 77576 (34195 source, 43381 target) ; Learning rate = 0.000328 ; Loss = 1.440990\n",
      "2024-12-03 22:19:23.407000: I runner.py:310] Step = 72600 ; steps/s = 1.65, tokens/s = 78451 (34553 source, 43898 target) ; Learning rate = 0.000328 ; Loss = 1.443796\n",
      "2024-12-03 22:20:24.225000: I runner.py:310] Step = 72700 ; steps/s = 1.64, tokens/s = 78355 (34502 source, 43853 target) ; Learning rate = 0.000328 ; Loss = 1.444072\n",
      "2024-12-03 22:21:24.490000: I runner.py:310] Step = 72800 ; steps/s = 1.66, tokens/s = 77665 (34234 source, 43431 target) ; Learning rate = 0.000328 ; Loss = 1.445914\n",
      "2024-12-03 22:22:25.234000: I runner.py:310] Step = 72900 ; steps/s = 1.65, tokens/s = 78455 (34554 source, 43901 target) ; Learning rate = 0.000327 ; Loss = 1.443121\n",
      "2024-12-03 22:23:25.546000: I runner.py:310] Step = 73000 ; steps/s = 1.66, tokens/s = 77610 (34223 source, 43387 target) ; Learning rate = 0.000327 ; Loss = 1.440172\n",
      "2024-12-03 22:24:26.326000: I runner.py:310] Step = 73100 ; steps/s = 1.65, tokens/s = 78391 (34507 source, 43884 target) ; Learning rate = 0.000327 ; Loss = 1.444624\n",
      "2024-12-03 22:25:27.171000: I runner.py:310] Step = 73200 ; steps/s = 1.64, tokens/s = 78338 (34518 source, 43820 target) ; Learning rate = 0.000327 ; Loss = 1.448239\n",
      "2024-12-03 22:26:27.536000: I runner.py:310] Step = 73300 ; steps/s = 1.66, tokens/s = 77534 (34186 source, 43348 target) ; Learning rate = 0.000326 ; Loss = 1.441639\n",
      "2024-12-03 22:27:28.338000: I runner.py:310] Step = 73400 ; steps/s = 1.64, tokens/s = 78395 (34527 source, 43868 target) ; Learning rate = 0.000326 ; Loss = 1.446697\n",
      "2024-12-03 22:28:28.554000: I runner.py:310] Step = 73500 ; steps/s = 1.66, tokens/s = 77733 (34276 source, 43457 target) ; Learning rate = 0.000326 ; Loss = 1.442258\n",
      "2024-12-03 22:29:29.347000: I runner.py:310] Step = 73600 ; steps/s = 1.65, tokens/s = 78367 (34493 source, 43874 target) ; Learning rate = 0.000326 ; Loss = 1.439640\n",
      "2024-12-03 22:30:30.144000: I runner.py:310] Step = 73700 ; steps/s = 1.64, tokens/s = 78369 (34507 source, 43862 target) ; Learning rate = 0.000326 ; Loss = 1.443264\n",
      "2024-12-03 22:31:30.478000: I runner.py:310] Step = 73800 ; steps/s = 1.66, tokens/s = 77596 (34225 source, 43371 target) ; Learning rate = 0.000325 ; Loss = 1.441315\n",
      "2024-12-03 22:32:31.269000: I runner.py:310] Step = 73900 ; steps/s = 1.65, tokens/s = 78415 (34547 source, 43868 target) ; Learning rate = 0.000325 ; Loss = 1.454260\n",
      "2024-12-03 22:33:32.060000: I runner.py:310] Step = 74000 ; steps/s = 1.65, tokens/s = 78370 (34510 source, 43860 target) ; Learning rate = 0.000325 ; Loss = 1.451506\n",
      "2024-12-03 22:34:32.369000: I runner.py:310] Step = 74100 ; steps/s = 1.66, tokens/s = 77595 (34202 source, 43393 target) ; Learning rate = 0.000325 ; Loss = 1.443034\n",
      "2024-12-03 22:35:33.158000: I runner.py:310] Step = 74200 ; steps/s = 1.65, tokens/s = 78383 (34511 source, 43872 target) ; Learning rate = 0.000324 ; Loss = 1.447060\n",
      "2024-12-03 22:36:33.486000: I runner.py:310] Step = 74300 ; steps/s = 1.66, tokens/s = 77605 (34229 source, 43376 target) ; Learning rate = 0.000324 ; Loss = 1.440961\n",
      "2024-12-03 22:37:34.241000: I runner.py:310] Step = 74400 ; steps/s = 1.65, tokens/s = 78419 (34532 source, 43887 target) ; Learning rate = 0.000324 ; Loss = 1.441064\n",
      "2024-12-03 22:38:34.991000: I runner.py:310] Step = 74500 ; steps/s = 1.65, tokens/s = 78453 (34546 source, 43907 target) ; Learning rate = 0.000324 ; Loss = 1.442318\n",
      "2024-12-03 22:39:35.314000: I runner.py:310] Step = 74600 ; steps/s = 1.66, tokens/s = 77592 (34203 source, 43389 target) ; Learning rate = 0.000324 ; Loss = 1.442993\n",
      "2024-12-03 22:40:36.103000: I runner.py:310] Step = 74700 ; steps/s = 1.65, tokens/s = 78395 (34527 source, 43868 target) ; Learning rate = 0.000323 ; Loss = 1.443870\n",
      "2024-12-03 22:41:36.437000: I runner.py:310] Step = 74800 ; steps/s = 1.66, tokens/s = 77602 (34229 source, 43373 target) ; Learning rate = 0.000323 ; Loss = 1.439345\n",
      "2024-12-03 22:42:37.193000: I runner.py:310] Step = 74900 ; steps/s = 1.65, tokens/s = 78419 (34526 source, 43893 target) ; Learning rate = 0.000323 ; Loss = 1.445422\n",
      "2024-12-03 22:43:37.958000: I runner.py:310] Step = 75000 ; steps/s = 1.65, tokens/s = 78419 (34534 source, 43885 target) ; Learning rate = 0.000323 ; Loss = 1.443690\n",
      "2024-12-03 22:43:37.960000: I training.py:192] Running evaluation for step 75000\n",
      "2024-12-03 22:44:24.501000: I training.py:192] Evaluation result for step 75000: loss = 0.850762 ; perplexity = 2.341429\n",
      "2024-12-03 22:45:24.702000: I runner.py:310] Step = 75100 ; steps/s = 1.66, tokens/s = 77762 (34284 source, 43478 target) ; Learning rate = 0.000323 ; Loss = 1.442274\n",
      "2024-12-03 22:46:25.513000: I runner.py:310] Step = 75200 ; steps/s = 1.64, tokens/s = 78354 (34513 source, 43841 target) ; Learning rate = 0.000322 ; Loss = 1.442553\n",
      "2024-12-03 22:47:26.308000: I runner.py:310] Step = 75300 ; steps/s = 1.65, tokens/s = 78390 (34520 source, 43870 target) ; Learning rate = 0.000322 ; Loss = 1.447403\n",
      "2024-12-03 22:48:26.589000: I runner.py:310] Step = 75400 ; steps/s = 1.66, tokens/s = 77655 (34235 source, 43420 target) ; Learning rate = 0.000322 ; Loss = 1.443001\n",
      "2024-12-03 22:49:27.378000: I runner.py:310] Step = 75500 ; steps/s = 1.65, tokens/s = 78369 (34500 source, 43869 target) ; Learning rate = 0.000322 ; Loss = 1.448609\n",
      "2024-12-03 22:50:27.709000: I runner.py:310] Step = 75600 ; steps/s = 1.66, tokens/s = 77599 (34221 source, 43378 target) ; Learning rate = 0.000321 ; Loss = 1.436611\n",
      "2024-12-03 22:51:28.508000: I runner.py:310] Step = 75700 ; steps/s = 1.64, tokens/s = 78363 (34510 source, 43853 target) ; Learning rate = 0.000321 ; Loss = 1.439559\n",
      "2024-12-03 22:52:29.320000: I runner.py:310] Step = 75800 ; steps/s = 1.64, tokens/s = 78379 (34522 source, 43857 target) ; Learning rate = 0.000321 ; Loss = 1.442901\n",
      "2024-12-03 22:53:29.708000: I runner.py:310] Step = 75900 ; steps/s = 1.66, tokens/s = 77503 (34159 source, 43344 target) ; Learning rate = 0.000321 ; Loss = 1.440549\n",
      "2024-12-03 22:54:30.529000: I runner.py:310] Step = 76000 ; steps/s = 1.64, tokens/s = 78361 (34510 source, 43851 target) ; Learning rate = 0.000321 ; Loss = 1.442968\n",
      "2024-12-03 22:55:30.812000: I runner.py:310] Step = 76100 ; steps/s = 1.66, tokens/s = 77660 (34260 source, 43400 target) ; Learning rate = 0.000320 ; Loss = 1.440956\n",
      "2024-12-03 22:56:31.578000: I runner.py:310] Step = 76200 ; steps/s = 1.65, tokens/s = 78383 (34500 source, 43883 target) ; Learning rate = 0.000320 ; Loss = 1.438079\n",
      "2024-12-03 22:57:32.375000: I runner.py:310] Step = 76300 ; steps/s = 1.64, tokens/s = 78411 (34543 source, 43868 target) ; Learning rate = 0.000320 ; Loss = 1.447476\n",
      "2024-12-03 22:58:32.689000: I runner.py:310] Step = 76400 ; steps/s = 1.66, tokens/s = 77603 (34213 source, 43390 target) ; Learning rate = 0.000320 ; Loss = 1.442512\n",
      "2024-12-03 22:59:33.535000: I runner.py:310] Step = 76500 ; steps/s = 1.64, tokens/s = 78309 (34483 source, 43826 target) ; Learning rate = 0.000320 ; Loss = 1.441631\n",
      "2024-12-03 23:00:34.290000: I runner.py:310] Step = 76600 ; steps/s = 1.65, tokens/s = 78453 (34558 source, 43895 target) ; Learning rate = 0.000319 ; Loss = 1.442042\n",
      "2024-12-03 23:01:34.614000: I runner.py:310] Step = 76700 ; steps/s = 1.66, tokens/s = 77601 (34210 source, 43391 target) ; Learning rate = 0.000319 ; Loss = 1.436209\n",
      "2024-12-03 23:02:35.414000: I runner.py:310] Step = 76800 ; steps/s = 1.64, tokens/s = 78360 (34501 source, 43859 target) ; Learning rate = 0.000319 ; Loss = 1.445540\n",
      "2024-12-03 23:03:35.755000: I runner.py:310] Step = 76900 ; steps/s = 1.66, tokens/s = 77586 (34221 source, 43365 target) ; Learning rate = 0.000319 ; Loss = 1.436218\n",
      "2024-12-03 23:04:36.523000: I runner.py:310] Step = 77000 ; steps/s = 1.65, tokens/s = 78418 (34529 source, 43889 target) ; Learning rate = 0.000319 ; Loss = 1.443383\n",
      "2024-12-03 23:05:37.291000: I runner.py:310] Step = 77100 ; steps/s = 1.65, tokens/s = 78407 (34528 source, 43879 target) ; Learning rate = 0.000318 ; Loss = 1.443102\n",
      "2024-12-03 23:06:37.583000: I runner.py:310] Step = 77200 ; steps/s = 1.66, tokens/s = 77640 (34240 source, 43400 target) ; Learning rate = 0.000318 ; Loss = 1.443800\n",
      "2024-12-03 23:07:38.398000: I runner.py:310] Step = 77300 ; steps/s = 1.64, tokens/s = 78347 (34492 source, 43855 target) ; Learning rate = 0.000318 ; Loss = 1.443020\n",
      "2024-12-03 23:08:38.689000: I runner.py:310] Step = 77400 ; steps/s = 1.66, tokens/s = 77655 (34248 source, 43407 target) ; Learning rate = 0.000318 ; Loss = 1.441822\n",
      "2024-12-03 23:09:39.434000: I runner.py:310] Step = 77500 ; steps/s = 1.65, tokens/s = 78393 (34491 source, 43902 target) ; Learning rate = 0.000317 ; Loss = 1.437902\n",
      "2024-12-03 23:10:40.198000: I runner.py:310] Step = 77600 ; steps/s = 1.65, tokens/s = 78436 (34558 source, 43878 target) ; Learning rate = 0.000317 ; Loss = 1.441413\n",
      "2024-12-03 23:11:40.590000: I runner.py:310] Step = 77700 ; steps/s = 1.66, tokens/s = 77538 (34188 source, 43350 target) ; Learning rate = 0.000317 ; Loss = 1.440497\n",
      "2024-12-03 23:12:41.346000: I runner.py:310] Step = 77800 ; steps/s = 1.65, tokens/s = 78444 (34561 source, 43883 target) ; Learning rate = 0.000317 ; Loss = 1.438636\n",
      "2024-12-03 23:13:42.172000: I runner.py:310] Step = 77900 ; steps/s = 1.64, tokens/s = 78327 (34489 source, 43838 target) ; Learning rate = 0.000317 ; Loss = 1.442621\n",
      "2024-12-03 23:14:42.471000: I runner.py:310] Step = 78000 ; steps/s = 1.66, tokens/s = 77636 (34216 source, 43420 target) ; Learning rate = 0.000316 ; Loss = 1.437061\n",
      "2024-12-03 23:15:43.300000: I runner.py:310] Step = 78100 ; steps/s = 1.64, tokens/s = 78333 (34503 source, 43830 target) ; Learning rate = 0.000316 ; Loss = 1.439967\n",
      "2024-12-03 23:16:43.598000: I runner.py:310] Step = 78200 ; steps/s = 1.66, tokens/s = 77638 (34239 source, 43399 target) ; Learning rate = 0.000316 ; Loss = 1.441112\n",
      "2024-12-03 23:17:44.386000: I runner.py:310] Step = 78300 ; steps/s = 1.65, tokens/s = 78366 (34498 source, 43868 target) ; Learning rate = 0.000316 ; Loss = 1.439590\n",
      "2024-12-03 23:18:45.185000: I runner.py:310] Step = 78400 ; steps/s = 1.64, tokens/s = 78396 (34533 source, 43863 target) ; Learning rate = 0.000316 ; Loss = 1.444254\n",
      "2024-12-03 23:19:45.511000: I runner.py:310] Step = 78500 ; steps/s = 1.66, tokens/s = 77618 (34230 source, 43388 target) ; Learning rate = 0.000315 ; Loss = 1.439555\n",
      "2024-12-03 23:20:46.296000: I runner.py:310] Step = 78600 ; steps/s = 1.65, tokens/s = 78386 (34518 source, 43868 target) ; Learning rate = 0.000315 ; Loss = 1.441823\n",
      "2024-12-03 23:21:46.753000: I runner.py:310] Step = 78700 ; steps/s = 1.65, tokens/s = 77425 (34137 source, 43288 target) ; Learning rate = 0.000315 ; Loss = 1.436513\n",
      "2024-12-03 23:22:47.480000: I runner.py:310] Step = 78800 ; steps/s = 1.65, tokens/s = 78430 (34526 source, 43904 target) ; Learning rate = 0.000315 ; Loss = 1.434983\n",
      "2024-12-03 23:23:48.227000: I runner.py:310] Step = 78900 ; steps/s = 1.65, tokens/s = 78480 (34579 source, 43901 target) ; Learning rate = 0.000315 ; Loss = 1.446007\n",
      "2024-12-03 23:24:48.596000: I runner.py:310] Step = 79000 ; steps/s = 1.66, tokens/s = 77499 (34141 source, 43358 target) ; Learning rate = 0.000314 ; Loss = 1.442497\n",
      "2024-12-03 23:25:49.429000: I runner.py:310] Step = 79100 ; steps/s = 1.64, tokens/s = 78346 (34507 source, 43839 target) ; Learning rate = 0.000314 ; Loss = 1.444362\n",
      "2024-12-03 23:26:50.211000: I runner.py:310] Step = 79200 ; steps/s = 1.65, tokens/s = 78417 (34545 source, 43872 target) ; Learning rate = 0.000314 ; Loss = 1.440402\n",
      "2024-12-03 23:27:50.566000: I runner.py:310] Step = 79300 ; steps/s = 1.66, tokens/s = 77535 (34178 source, 43357 target) ; Learning rate = 0.000314 ; Loss = 1.445999\n",
      "2024-12-03 23:28:51.352000: I runner.py:310] Step = 79400 ; steps/s = 1.65, tokens/s = 78416 (34537 source, 43879 target) ; Learning rate = 0.000314 ; Loss = 1.441373\n",
      "2024-12-03 23:29:51.696000: I runner.py:310] Step = 79500 ; steps/s = 1.66, tokens/s = 77552 (34190 source, 43362 target) ; Learning rate = 0.000313 ; Loss = 1.434192\n",
      "2024-12-03 23:30:52.464000: I runner.py:310] Step = 79600 ; steps/s = 1.65, tokens/s = 78394 (34519 source, 43875 target) ; Learning rate = 0.000313 ; Loss = 1.442310\n",
      "2024-12-03 23:31:53.262000: I runner.py:310] Step = 79700 ; steps/s = 1.64, tokens/s = 78405 (34534 source, 43871 target) ; Learning rate = 0.000313 ; Loss = 1.442644\n",
      "2024-12-03 23:32:53.500000: I runner.py:310] Step = 79800 ; steps/s = 1.66, tokens/s = 77687 (34241 source, 43446 target) ; Learning rate = 0.000313 ; Loss = 1.450172\n",
      "2024-12-03 23:33:54.288000: I runner.py:310] Step = 79900 ; steps/s = 1.65, tokens/s = 78400 (34538 source, 43862 target) ; Learning rate = 0.000313 ; Loss = 1.440974\n",
      "2024-12-03 23:34:54.614000: I runner.py:310] Step = 80000 ; steps/s = 1.66, tokens/s = 77628 (34239 source, 43389 target) ; Learning rate = 0.000312 ; Loss = 1.438606\n",
      "2024-12-03 23:34:56.685000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-80000\n",
      "2024-12-03 23:34:56.685000: I training.py:192] Running evaluation for step 80000\n",
      "2024-12-03 23:35:46.085000: I training.py:192] Evaluation result for step 80000: loss = 0.862845 ; perplexity = 2.369893\n",
      "2024-12-03 23:36:46.765000: I runner.py:310] Step = 80100 ; steps/s = 1.65, tokens/s = 78559 (34604 source, 43955 target) ; Learning rate = 0.000312 ; Loss = 1.445105\n",
      "2024-12-03 23:37:47.575000: I runner.py:310] Step = 80200 ; steps/s = 1.64, tokens/s = 78351 (34500 source, 43851 target) ; Learning rate = 0.000312 ; Loss = 1.441568\n",
      "2024-12-03 23:38:47.955000: I runner.py:310] Step = 80300 ; steps/s = 1.66, tokens/s = 77510 (34164 source, 43346 target) ; Learning rate = 0.000312 ; Loss = 1.439803\n",
      "2024-12-03 23:39:48.793000: I runner.py:310] Step = 80400 ; steps/s = 1.64, tokens/s = 78313 (34482 source, 43831 target) ; Learning rate = 0.000312 ; Loss = 1.438651\n",
      "2024-12-03 23:40:49.565000: I runner.py:310] Step = 80500 ; steps/s = 1.65, tokens/s = 78429 (34545 source, 43884 target) ; Learning rate = 0.000312 ; Loss = 1.439804\n",
      "2024-12-03 23:41:49.911000: I runner.py:310] Step = 80600 ; steps/s = 1.66, tokens/s = 77576 (34202 source, 43374 target) ; Learning rate = 0.000311 ; Loss = 1.437081\n",
      "2024-12-03 23:42:50.680000: I runner.py:310] Step = 80700 ; steps/s = 1.65, tokens/s = 78394 (34516 source, 43878 target) ; Learning rate = 0.000311 ; Loss = 1.440852\n",
      "2024-12-03 23:43:51.085000: I runner.py:310] Step = 80800 ; steps/s = 1.66, tokens/s = 77528 (34200 source, 43328 target) ; Learning rate = 0.000311 ; Loss = 1.435161\n",
      "2024-12-03 23:44:51.863000: I runner.py:310] Step = 80900 ; steps/s = 1.65, tokens/s = 78399 (34522 source, 43877 target) ; Learning rate = 0.000311 ; Loss = 1.437870\n",
      "2024-12-03 23:45:52.583000: I runner.py:310] Step = 81000 ; steps/s = 1.65, tokens/s = 78479 (34560 source, 43919 target) ; Learning rate = 0.000311 ; Loss = 1.437286\n",
      "2024-12-03 23:46:53.004000: I runner.py:310] Step = 81100 ; steps/s = 1.66, tokens/s = 77472 (34155 source, 43317 target) ; Learning rate = 0.000310 ; Loss = 1.441113\n",
      "2024-12-03 23:47:53.782000: I runner.py:310] Step = 81200 ; steps/s = 1.65, tokens/s = 78389 (34517 source, 43872 target) ; Learning rate = 0.000310 ; Loss = 1.441184\n",
      "2024-12-03 23:48:54.088000: I runner.py:310] Step = 81300 ; steps/s = 1.66, tokens/s = 77629 (34239 source, 43390 target) ; Learning rate = 0.000310 ; Loss = 1.434558\n",
      "2024-12-03 23:49:54.846000: I runner.py:310] Step = 81400 ; steps/s = 1.65, tokens/s = 78413 (34521 source, 43892 target) ; Learning rate = 0.000310 ; Loss = 1.443880\n",
      "2024-12-03 23:50:55.680000: I runner.py:310] Step = 81500 ; steps/s = 1.64, tokens/s = 78336 (34503 source, 43833 target) ; Learning rate = 0.000310 ; Loss = 1.438886\n",
      "2024-12-03 23:51:56.041000: I runner.py:310] Step = 81600 ; steps/s = 1.66, tokens/s = 77533 (34171 source, 43362 target) ; Learning rate = 0.000309 ; Loss = 1.443270\n",
      "2024-12-03 23:52:56.831000: I runner.py:310] Step = 81700 ; steps/s = 1.65, tokens/s = 78385 (34517 source, 43868 target) ; Learning rate = 0.000309 ; Loss = 1.440013\n",
      "2024-12-03 23:53:57.658000: I runner.py:310] Step = 81800 ; steps/s = 1.64, tokens/s = 78356 (34520 source, 43836 target) ; Learning rate = 0.000309 ; Loss = 1.444466\n",
      "2024-12-03 23:54:58.021000: I runner.py:310] Step = 81900 ; steps/s = 1.66, tokens/s = 77542 (34189 source, 43353 target) ; Learning rate = 0.000309 ; Loss = 1.436500\n",
      "2024-12-03 23:55:58.809000: I runner.py:310] Step = 82000 ; steps/s = 1.65, tokens/s = 78407 (34537 source, 43870 target) ; Learning rate = 0.000309 ; Loss = 1.441174\n",
      "2024-12-03 23:56:59.145000: I runner.py:310] Step = 82100 ; steps/s = 1.66, tokens/s = 77551 (34176 source, 43375 target) ; Learning rate = 0.000308 ; Loss = 1.437374\n",
      "2024-12-03 23:57:59.934000: I runner.py:310] Step = 82200 ; steps/s = 1.65, tokens/s = 78380 (34501 source, 43879 target) ; Learning rate = 0.000308 ; Loss = 1.442190\n",
      "2024-12-03 23:59:00.727000: I runner.py:310] Step = 82300 ; steps/s = 1.65, tokens/s = 78400 (34545 source, 43855 target) ; Learning rate = 0.000308 ; Loss = 1.444507\n",
      "2024-12-04 00:00:01.034000: I runner.py:310] Step = 82400 ; steps/s = 1.66, tokens/s = 77619 (34215 source, 43404 target) ; Learning rate = 0.000308 ; Loss = 1.439266\n",
      "2024-12-04 00:01:01.808000: I runner.py:310] Step = 82500 ; steps/s = 1.65, tokens/s = 78420 (34544 source, 43876 target) ; Learning rate = 0.000308 ; Loss = 1.439784\n",
      "2024-12-04 00:02:02.166000: I runner.py:310] Step = 82600 ; steps/s = 1.66, tokens/s = 77549 (34198 source, 43351 target) ; Learning rate = 0.000308 ; Loss = 1.434198\n",
      "2024-12-04 00:03:03.024000: I runner.py:310] Step = 82700 ; steps/s = 1.64, tokens/s = 78271 (34457 source, 43814 target) ; Learning rate = 0.000307 ; Loss = 1.435825\n",
      "2024-12-04 00:04:03.834000: I runner.py:310] Step = 82800 ; steps/s = 1.64, tokens/s = 78389 (34532 source, 43857 target) ; Learning rate = 0.000307 ; Loss = 1.454274\n",
      "2024-12-04 00:05:04.244000: I runner.py:310] Step = 82900 ; steps/s = 1.66, tokens/s = 77469 (34144 source, 43325 target) ; Learning rate = 0.000307 ; Loss = 1.438634\n",
      "2024-12-04 00:06:05.057000: I runner.py:310] Step = 83000 ; steps/s = 1.64, tokens/s = 78363 (34517 source, 43846 target) ; Learning rate = 0.000307 ; Loss = 1.441329\n",
      "2024-12-04 00:07:05.810000: I runner.py:310] Step = 83100 ; steps/s = 1.65, tokens/s = 78459 (34555 source, 43904 target) ; Learning rate = 0.000307 ; Loss = 1.445270\n",
      "2024-12-04 00:08:06.116000: I runner.py:310] Step = 83200 ; steps/s = 1.66, tokens/s = 77620 (34219 source, 43401 target) ; Learning rate = 0.000306 ; Loss = 1.435978\n",
      "2024-12-04 00:09:06.886000: I runner.py:310] Step = 83300 ; steps/s = 1.65, tokens/s = 78420 (34533 source, 43887 target) ; Learning rate = 0.000306 ; Loss = 1.435680\n",
      "2024-12-04 00:10:07.244000: I runner.py:310] Step = 83400 ; steps/s = 1.66, tokens/s = 77556 (34200 source, 43356 target) ; Learning rate = 0.000306 ; Loss = 1.436547\n",
      "2024-12-04 00:11:08.008000: I runner.py:310] Step = 83500 ; steps/s = 1.65, tokens/s = 78409 (34521 source, 43888 target) ; Learning rate = 0.000306 ; Loss = 1.442137\n",
      "2024-12-04 00:12:08.837000: I runner.py:310] Step = 83600 ; steps/s = 1.64, tokens/s = 78346 (34513 source, 43833 target) ; Learning rate = 0.000306 ; Loss = 1.441525\n",
      "2024-12-04 00:13:09.163000: I runner.py:310] Step = 83700 ; steps/s = 1.66, tokens/s = 77560 (34187 source, 43373 target) ; Learning rate = 0.000306 ; Loss = 1.440839\n",
      "2024-12-04 00:14:09.886000: I runner.py:310] Step = 83800 ; steps/s = 1.65, tokens/s = 78455 (34545 source, 43910 target) ; Learning rate = 0.000305 ; Loss = 1.442553\n",
      "2024-12-04 00:15:10.194000: I runner.py:310] Step = 83900 ; steps/s = 1.66, tokens/s = 77666 (34264 source, 43402 target) ; Learning rate = 0.000305 ; Loss = 1.436315\n",
      "2024-12-04 00:16:10.902000: I runner.py:310] Step = 84000 ; steps/s = 1.65, tokens/s = 78497 (34566 source, 43931 target) ; Learning rate = 0.000305 ; Loss = 1.439554\n",
      "2024-12-04 00:17:11.680000: I runner.py:310] Step = 84100 ; steps/s = 1.65, tokens/s = 78402 (34530 source, 43872 target) ; Learning rate = 0.000305 ; Loss = 1.438346\n",
      "2024-12-04 00:18:12.079000: I runner.py:310] Step = 84200 ; steps/s = 1.66, tokens/s = 77485 (34151 source, 43334 target) ; Learning rate = 0.000305 ; Loss = 1.433890\n",
      "2024-12-04 00:19:12.916000: I runner.py:310] Step = 84300 ; steps/s = 1.64, tokens/s = 78341 (34500 source, 43841 target) ; Learning rate = 0.000304 ; Loss = 1.445265\n",
      "2024-12-04 00:20:13.747000: I runner.py:310] Step = 84400 ; steps/s = 1.64, tokens/s = 78322 (34492 source, 43830 target) ; Learning rate = 0.000304 ; Loss = 1.439521\n",
      "2024-12-04 00:21:14.024000: I runner.py:310] Step = 84500 ; steps/s = 1.66, tokens/s = 77658 (34236 source, 43422 target) ; Learning rate = 0.000304 ; Loss = 1.436242\n",
      "2024-12-04 00:22:14.821000: I runner.py:310] Step = 84600 ; steps/s = 1.65, tokens/s = 78383 (34518 source, 43865 target) ; Learning rate = 0.000304 ; Loss = 1.436053\n",
      "2024-12-04 00:23:15.163000: I runner.py:310] Step = 84700 ; steps/s = 1.66, tokens/s = 77584 (34217 source, 43367 target) ; Learning rate = 0.000304 ; Loss = 1.435731\n",
      "2024-12-04 00:24:15.921000: I runner.py:310] Step = 84800 ; steps/s = 1.65, tokens/s = 78432 (34536 source, 43896 target) ; Learning rate = 0.000304 ; Loss = 1.439517\n",
      "2024-12-04 00:25:16.767000: I runner.py:310] Step = 84900 ; steps/s = 1.64, tokens/s = 78313 (34493 source, 43820 target) ; Learning rate = 0.000303 ; Loss = 1.443690\n",
      "2024-12-04 00:26:17.092000: I runner.py:310] Step = 85000 ; steps/s = 1.66, tokens/s = 77563 (34181 source, 43382 target) ; Learning rate = 0.000303 ; Loss = 1.448421\n",
      "2024-12-04 00:26:17.093000: I training.py:192] Running evaluation for step 85000\n",
      "2024-12-04 00:27:05.384000: I training.py:192] Evaluation result for step 85000: loss = 0.863680 ; perplexity = 2.371873\n",
      "2024-12-04 00:28:06.053000: I runner.py:310] Step = 85100 ; steps/s = 1.65, tokens/s = 78565 (34609 source, 43956 target) ; Learning rate = 0.000303 ; Loss = 1.440530\n",
      "2024-12-04 00:29:06.416000: I runner.py:310] Step = 85200 ; steps/s = 1.66, tokens/s = 77576 (34213 source, 43363 target) ; Learning rate = 0.000303 ; Loss = 1.430643\n",
      "2024-12-04 00:30:07.196000: I runner.py:310] Step = 85300 ; steps/s = 1.65, tokens/s = 78410 (34534 source, 43876 target) ; Learning rate = 0.000303 ; Loss = 1.441205\n",
      "2024-12-04 00:31:07.963000: I runner.py:310] Step = 85400 ; steps/s = 1.65, tokens/s = 78424 (34535 source, 43889 target) ; Learning rate = 0.000302 ; Loss = 1.440303\n",
      "2024-12-04 00:32:08.265000: I runner.py:310] Step = 85500 ; steps/s = 1.66, tokens/s = 77599 (34199 source, 43400 target) ; Learning rate = 0.000302 ; Loss = 1.439163\n",
      "2024-12-04 00:33:09.061000: I runner.py:310] Step = 85600 ; steps/s = 1.65, tokens/s = 78388 (34532 source, 43856 target) ; Learning rate = 0.000302 ; Loss = 1.438582\n",
      "2024-12-04 00:34:09.826000: I runner.py:310] Step = 85700 ; steps/s = 1.65, tokens/s = 78431 (34535 source, 43896 target) ; Learning rate = 0.000302 ; Loss = 1.443463\n",
      "2024-12-04 00:35:10.166000: I runner.py:310] Step = 85800 ; steps/s = 1.66, tokens/s = 77556 (34190 source, 43366 target) ; Learning rate = 0.000302 ; Loss = 1.441353\n",
      "2024-12-04 00:36:11.027000: I runner.py:310] Step = 85900 ; steps/s = 1.64, tokens/s = 78328 (34502 source, 43826 target) ; Learning rate = 0.000302 ; Loss = 1.435820\n",
      "2024-12-04 00:37:11.369000: I runner.py:310] Step = 86000 ; steps/s = 1.66, tokens/s = 77594 (34208 source, 43386 target) ; Learning rate = 0.000301 ; Loss = 1.433187\n",
      "2024-12-04 00:38:12.160000: I runner.py:310] Step = 86100 ; steps/s = 1.65, tokens/s = 78360 (34501 source, 43859 target) ; Learning rate = 0.000301 ; Loss = 1.439639\n",
      "2024-12-04 00:39:13.005000: I runner.py:310] Step = 86200 ; steps/s = 1.64, tokens/s = 78327 (34504 source, 43823 target) ; Learning rate = 0.000301 ; Loss = 1.437355\n",
      "2024-12-04 00:40:13.332000: I runner.py:310] Step = 86300 ; steps/s = 1.66, tokens/s = 77573 (34192 source, 43381 target) ; Learning rate = 0.000301 ; Loss = 1.437155\n",
      "2024-12-04 00:41:14.117000: I runner.py:310] Step = 86400 ; steps/s = 1.65, tokens/s = 78413 (34539 source, 43874 target) ; Learning rate = 0.000301 ; Loss = 1.442114\n",
      "2024-12-04 00:42:14.445000: I runner.py:310] Step = 86500 ; steps/s = 1.66, tokens/s = 77598 (34217 source, 43381 target) ; Learning rate = 0.000301 ; Loss = 1.436044\n",
      "2024-12-04 00:43:15.220000: I runner.py:310] Step = 86600 ; steps/s = 1.65, tokens/s = 78390 (34502 source, 43888 target) ; Learning rate = 0.000300 ; Loss = 1.438356\n",
      "2024-12-04 00:44:16.018000: I runner.py:310] Step = 86700 ; steps/s = 1.64, tokens/s = 78393 (34541 source, 43852 target) ; Learning rate = 0.000300 ; Loss = 1.441610\n",
      "2024-12-04 00:45:16.353000: I runner.py:310] Step = 86800 ; steps/s = 1.66, tokens/s = 77563 (34186 source, 43377 target) ; Learning rate = 0.000300 ; Loss = 1.439549\n",
      "2024-12-04 00:46:17.182000: I runner.py:310] Step = 86900 ; steps/s = 1.64, tokens/s = 78339 (34498 source, 43841 target) ; Learning rate = 0.000300 ; Loss = 1.440010\n",
      "2024-12-04 00:47:17.906000: I runner.py:310] Step = 87000 ; steps/s = 1.65, tokens/s = 78486 (34574 source, 43912 target) ; Learning rate = 0.000300 ; Loss = 1.441747\n",
      "2024-12-04 00:48:18.272000: I runner.py:310] Step = 87100 ; steps/s = 1.66, tokens/s = 77574 (34213 source, 43361 target) ; Learning rate = 0.000299 ; Loss = 1.438378\n",
      "2024-12-04 00:49:19.038000: I runner.py:310] Step = 87200 ; steps/s = 1.65, tokens/s = 78403 (34534 source, 43869 target) ; Learning rate = 0.000299 ; Loss = 1.437371\n",
      "2024-12-04 00:50:19.314000: I runner.py:310] Step = 87300 ; steps/s = 1.66, tokens/s = 77630 (34205 source, 43425 target) ; Learning rate = 0.000299 ; Loss = 1.431965\n",
      "2024-12-04 00:51:20.070000: I runner.py:310] Step = 87400 ; steps/s = 1.65, tokens/s = 78422 (34529 source, 43893 target) ; Learning rate = 0.000299 ; Loss = 1.441028\n",
      "2024-12-04 00:52:20.852000: I runner.py:310] Step = 87500 ; steps/s = 1.65, tokens/s = 78411 (34542 source, 43869 target) ; Learning rate = 0.000299 ; Loss = 1.439950\n",
      "2024-12-04 00:53:21.216000: I runner.py:310] Step = 87600 ; steps/s = 1.66, tokens/s = 77531 (34179 source, 43352 target) ; Learning rate = 0.000299 ; Loss = 1.442369\n",
      "2024-12-04 00:54:21.989000: I runner.py:310] Step = 87700 ; steps/s = 1.65, tokens/s = 78387 (34511 source, 43876 target) ; Learning rate = 0.000298 ; Loss = 1.440434\n",
      "2024-12-04 00:55:22.340000: I runner.py:310] Step = 87800 ; steps/s = 1.66, tokens/s = 77607 (34229 source, 43378 target) ; Learning rate = 0.000298 ; Loss = 1.437539\n",
      "2024-12-04 00:56:23.140000: I runner.py:310] Step = 87900 ; steps/s = 1.64, tokens/s = 78353 (34494 source, 43859 target) ; Learning rate = 0.000298 ; Loss = 1.436179\n",
      "2024-12-04 00:57:23.938000: I runner.py:310] Step = 88000 ; steps/s = 1.65, tokens/s = 78417 (34559 source, 43858 target) ; Learning rate = 0.000298 ; Loss = 1.439343\n",
      "2024-12-04 00:58:24.272000: I runner.py:310] Step = 88100 ; steps/s = 1.66, tokens/s = 77562 (34192 source, 43370 target) ; Learning rate = 0.000298 ; Loss = 1.435760\n",
      "2024-12-04 00:59:25.111000: I runner.py:310] Step = 88200 ; steps/s = 1.64, tokens/s = 78325 (34482 source, 43843 target) ; Learning rate = 0.000298 ; Loss = 1.443506\n",
      "2024-12-04 01:00:25.845000: I runner.py:310] Step = 88300 ; steps/s = 1.65, tokens/s = 78347 (34510 source, 43837 target) ; Learning rate = 0.000297 ; Loss = 1.449122\n",
      "2024-12-04 01:01:26.202000: I runner.py:310] Step = 88400 ; steps/s = 1.66, tokens/s = 77661 (34233 source, 43428 target) ; Learning rate = 0.000297 ; Loss = 1.434311\n",
      "2024-12-04 01:02:26.986000: I runner.py:310] Step = 88500 ; steps/s = 1.65, tokens/s = 78410 (34544 source, 43866 target) ; Learning rate = 0.000297 ; Loss = 1.439286\n",
      "2024-12-04 01:03:27.380000: I runner.py:310] Step = 88600 ; steps/s = 1.66, tokens/s = 77507 (34169 source, 43338 target) ; Learning rate = 0.000297 ; Loss = 1.434068\n",
      "2024-12-04 01:04:28.212000: I runner.py:310] Step = 88700 ; steps/s = 1.64, tokens/s = 78355 (34511 source, 43844 target) ; Learning rate = 0.000297 ; Loss = 1.441116\n",
      "2024-12-04 01:05:28.994000: I runner.py:310] Step = 88800 ; steps/s = 1.65, tokens/s = 78375 (34501 source, 43874 target) ; Learning rate = 0.000297 ; Loss = 1.434898\n",
      "2024-12-04 01:06:29.314000: I runner.py:310] Step = 88900 ; steps/s = 1.66, tokens/s = 77620 (34241 source, 43379 target) ; Learning rate = 0.000296 ; Loss = 1.435320\n",
      "2024-12-04 01:07:30.033000: I runner.py:310] Step = 89000 ; steps/s = 1.65, tokens/s = 78477 (34551 source, 43926 target) ; Learning rate = 0.000296 ; Loss = 1.437553\n",
      "2024-12-04 01:08:30.370000: I runner.py:310] Step = 89100 ; steps/s = 1.66, tokens/s = 77575 (34202 source, 43373 target) ; Learning rate = 0.000296 ; Loss = 1.432716\n",
      "2024-12-04 01:09:31.092000: I runner.py:310] Step = 89200 ; steps/s = 1.65, tokens/s = 78485 (34563 source, 43922 target) ; Learning rate = 0.000296 ; Loss = 1.438737\n",
      "2024-12-04 01:10:31.853000: I runner.py:310] Step = 89300 ; steps/s = 1.65, tokens/s = 78419 (34537 source, 43882 target) ; Learning rate = 0.000296 ; Loss = 1.441281\n",
      "2024-12-04 01:11:32.177000: I runner.py:310] Step = 89400 ; steps/s = 1.66, tokens/s = 77581 (34202 source, 43379 target) ; Learning rate = 0.000296 ; Loss = 1.437304\n",
      "2024-12-04 01:12:32.989000: I runner.py:310] Step = 89500 ; steps/s = 1.64, tokens/s = 78345 (34492 source, 43853 target) ; Learning rate = 0.000295 ; Loss = 1.440652\n",
      "2024-12-04 01:13:33.653000: I runner.py:310] Step = 89600 ; steps/s = 1.65, tokens/s = 78116 (34452 source, 43664 target) ; Learning rate = 0.000295 ; Loss = 1.472042\n",
      "2024-12-04 01:14:34.025000: I runner.py:310] Step = 89700 ; steps/s = 1.66, tokens/s = 77969 (34321 source, 43648 target) ; Learning rate = 0.000295 ; Loss = 1.432953\n",
      "2024-12-04 01:15:34.830000: I runner.py:310] Step = 89800 ; steps/s = 1.64, tokens/s = 78394 (34539 source, 43855 target) ; Learning rate = 0.000295 ; Loss = 1.437943\n",
      "2024-12-04 01:16:35.145000: I runner.py:310] Step = 89900 ; steps/s = 1.66, tokens/s = 77605 (34204 source, 43401 target) ; Learning rate = 0.000295 ; Loss = 1.437050\n",
      "2024-12-04 01:17:35.906000: I runner.py:310] Step = 90000 ; steps/s = 1.65, tokens/s = 78407 (34526 source, 43881 target) ; Learning rate = 0.000295 ; Loss = 1.435013\n",
      "2024-12-04 01:17:38.444000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-90000\n",
      "2024-12-04 01:17:38.444000: I training.py:192] Running evaluation for step 90000\n",
      "2024-12-04 01:18:27.095000: I training.py:192] Evaluation result for step 90000: loss = 0.866239 ; perplexity = 2.377950\n",
      "2024-12-04 01:19:27.723000: I runner.py:310] Step = 90100 ; steps/s = 1.65, tokens/s = 78631 (34635 source, 43996 target) ; Learning rate = 0.000294 ; Loss = 1.442204\n",
      "2024-12-04 01:20:28.025000: I runner.py:310] Step = 90200 ; steps/s = 1.66, tokens/s = 77613 (34226 source, 43387 target) ; Learning rate = 0.000294 ; Loss = 1.434683\n",
      "2024-12-04 01:21:28.891000: I runner.py:310] Step = 90300 ; steps/s = 1.64, tokens/s = 78286 (34472 source, 43814 target) ; Learning rate = 0.000294 ; Loss = 1.437265\n",
      "2024-12-04 01:22:29.272000: I runner.py:310] Step = 90400 ; steps/s = 1.66, tokens/s = 77553 (34200 source, 43353 target) ; Learning rate = 0.000294 ; Loss = 1.435109\n",
      "2024-12-04 01:23:30.087000: I runner.py:310] Step = 90500 ; steps/s = 1.64, tokens/s = 78309 (34468 source, 43841 target) ; Learning rate = 0.000294 ; Loss = 1.432863\n",
      "2024-12-04 01:24:30.856000: I runner.py:310] Step = 90600 ; steps/s = 1.65, tokens/s = 78459 (34576 source, 43883 target) ; Learning rate = 0.000294 ; Loss = 1.439220\n",
      "2024-12-04 01:25:31.130000: I runner.py:310] Step = 90700 ; steps/s = 1.66, tokens/s = 77670 (34243 source, 43427 target) ; Learning rate = 0.000293 ; Loss = 1.434349\n",
      "2024-12-04 01:26:31.956000: I runner.py:310] Step = 90800 ; steps/s = 1.64, tokens/s = 78342 (34501 source, 43841 target) ; Learning rate = 0.000293 ; Loss = 1.436965\n",
      "2024-12-04 01:27:32.299000: I runner.py:310] Step = 90900 ; steps/s = 1.66, tokens/s = 77586 (34215 source, 43371 target) ; Learning rate = 0.000293 ; Loss = 1.451418\n",
      "2024-12-04 01:28:33.116000: I runner.py:310] Step = 91000 ; steps/s = 1.64, tokens/s = 78327 (34467 source, 43860 target) ; Learning rate = 0.000293 ; Loss = 1.433299\n",
      "2024-12-04 01:29:33.911000: I runner.py:310] Step = 91100 ; steps/s = 1.65, tokens/s = 78391 (34534 source, 43857 target) ; Learning rate = 0.000293 ; Loss = 1.442570\n",
      "2024-12-04 01:30:34.239000: I runner.py:310] Step = 91200 ; steps/s = 1.66, tokens/s = 77575 (34196 source, 43379 target) ; Learning rate = 0.000293 ; Loss = 1.434393\n",
      "2024-12-04 01:31:34.998000: I runner.py:310] Step = 91300 ; steps/s = 1.65, tokens/s = 78433 (34547 source, 43886 target) ; Learning rate = 0.000293 ; Loss = 1.434502\n",
      "2024-12-04 01:32:35.823000: I runner.py:310] Step = 91400 ; steps/s = 1.64, tokens/s = 78342 (34495 source, 43847 target) ; Learning rate = 0.000292 ; Loss = 1.437947\n",
      "2024-12-04 01:33:36.149000: I runner.py:310] Step = 91500 ; steps/s = 1.66, tokens/s = 77626 (34231 source, 43395 target) ; Learning rate = 0.000292 ; Loss = 1.434042\n",
      "2024-12-04 01:34:37.010000: I runner.py:310] Step = 91600 ; steps/s = 1.64, tokens/s = 78285 (34481 source, 43804 target) ; Learning rate = 0.000292 ; Loss = 1.443634\n",
      "2024-12-04 01:35:37.287000: I runner.py:310] Step = 91700 ; steps/s = 1.66, tokens/s = 77654 (34241 source, 43413 target) ; Learning rate = 0.000292 ; Loss = 1.431248\n",
      "2024-12-04 01:36:38.091000: I runner.py:310] Step = 91800 ; steps/s = 1.64, tokens/s = 78390 (34521 source, 43869 target) ; Learning rate = 0.000292 ; Loss = 1.438138\n",
      "2024-12-04 01:37:38.924000: I runner.py:310] Step = 91900 ; steps/s = 1.64, tokens/s = 78348 (34508 source, 43840 target) ; Learning rate = 0.000292 ; Loss = 1.438052\n",
      "2024-12-04 01:38:39.293000: I runner.py:310] Step = 92000 ; steps/s = 1.66, tokens/s = 77511 (34159 source, 43352 target) ; Learning rate = 0.000291 ; Loss = 1.437680\n",
      "2024-12-04 01:39:40.061000: I runner.py:310] Step = 92100 ; steps/s = 1.65, tokens/s = 78399 (34516 source, 43883 target) ; Learning rate = 0.000291 ; Loss = 1.437448\n",
      "2024-12-04 01:40:40.475000: I runner.py:310] Step = 92200 ; steps/s = 1.66, tokens/s = 77513 (34200 source, 43313 target) ; Learning rate = 0.000291 ; Loss = 1.436299\n",
      "2024-12-04 01:41:41.260000: I runner.py:310] Step = 92300 ; steps/s = 1.65, tokens/s = 78357 (34481 source, 43876 target) ; Learning rate = 0.000291 ; Loss = 1.431819\n",
      "2024-12-04 01:42:42.053000: I runner.py:310] Step = 92400 ; steps/s = 1.65, tokens/s = 78406 (34556 source, 43850 target) ; Learning rate = 0.000291 ; Loss = 1.443478\n",
      "2024-12-04 01:43:42.368000: I runner.py:310] Step = 92500 ; steps/s = 1.66, tokens/s = 77601 (34209 source, 43392 target) ; Learning rate = 0.000291 ; Loss = 1.434057\n",
      "2024-12-04 01:44:43.101000: I runner.py:310] Step = 92600 ; steps/s = 1.65, tokens/s = 78472 (34549 source, 43923 target) ; Learning rate = 0.000290 ; Loss = 1.441783\n",
      "2024-12-04 01:45:43.921000: I runner.py:310] Step = 92700 ; steps/s = 1.64, tokens/s = 78342 (34500 source, 43842 target) ; Learning rate = 0.000290 ; Loss = 1.434812\n",
      "2024-12-04 01:46:44.253000: I runner.py:310] Step = 92800 ; steps/s = 1.66, tokens/s = 77602 (34221 source, 43381 target) ; Learning rate = 0.000290 ; Loss = 1.438226\n",
      "2024-12-04 01:47:45.052000: I runner.py:310] Step = 92900 ; steps/s = 1.65, tokens/s = 78368 (34514 source, 43854 target) ; Learning rate = 0.000290 ; Loss = 1.434426\n",
      "2024-12-04 01:48:45.402000: I runner.py:310] Step = 93000 ; steps/s = 1.66, tokens/s = 77559 (34189 source, 43370 target) ; Learning rate = 0.000290 ; Loss = 1.439768\n",
      "2024-12-04 01:49:46.189000: I runner.py:310] Step = 93100 ; steps/s = 1.65, tokens/s = 78387 (34509 source, 43878 target) ; Learning rate = 0.000290 ; Loss = 1.431732\n",
      "2024-12-04 01:50:47.032000: I runner.py:310] Step = 93200 ; steps/s = 1.64, tokens/s = 78339 (34509 source, 43830 target) ; Learning rate = 0.000290 ; Loss = 1.439426\n",
      "2024-12-04 01:51:47.329000: I runner.py:310] Step = 93300 ; steps/s = 1.66, tokens/s = 77639 (34241 source, 43398 target) ; Learning rate = 0.000289 ; Loss = 1.438426\n",
      "2024-12-04 01:52:48.115000: I runner.py:310] Step = 93400 ; steps/s = 1.65, tokens/s = 78381 (34511 source, 43870 target) ; Learning rate = 0.000289 ; Loss = 1.438136\n",
      "2024-12-04 01:53:48.514000: I runner.py:310] Step = 93500 ; steps/s = 1.66, tokens/s = 77510 (34184 source, 43326 target) ; Learning rate = 0.000289 ; Loss = 1.434805\n",
      "2024-12-04 01:54:49.321000: I runner.py:310] Step = 93600 ; steps/s = 1.64, tokens/s = 78360 (34509 source, 43851 target) ; Learning rate = 0.000289 ; Loss = 1.439578\n",
      "2024-12-04 01:55:50.098000: I runner.py:310] Step = 93700 ; steps/s = 1.65, tokens/s = 78420 (34545 source, 43875 target) ; Learning rate = 0.000289 ; Loss = 1.441663\n",
      "2024-12-04 01:56:50.409000: I runner.py:310] Step = 93800 ; steps/s = 1.66, tokens/s = 77573 (34173 source, 43400 target) ; Learning rate = 0.000289 ; Loss = 1.433596\n",
      "2024-12-04 01:57:51.182000: I runner.py:310] Step = 93900 ; steps/s = 1.65, tokens/s = 78398 (34533 source, 43865 target) ; Learning rate = 0.000288 ; Loss = 1.440761\n",
      "2024-12-04 01:58:51.968000: I runner.py:310] Step = 94000 ; steps/s = 1.65, tokens/s = 78425 (34549 source, 43876 target) ; Learning rate = 0.000288 ; Loss = 1.438296\n",
      "2024-12-04 01:59:52.325000: I runner.py:310] Step = 94100 ; steps/s = 1.66, tokens/s = 77546 (34173 source, 43373 target) ; Learning rate = 0.000288 ; Loss = 1.438807\n",
      "2024-12-04 02:00:53.065000: I runner.py:310] Step = 94200 ; steps/s = 1.65, tokens/s = 78450 (34546 source, 43904 target) ; Learning rate = 0.000288 ; Loss = 1.439002\n",
      "2024-12-04 02:01:53.364000: I runner.py:310] Step = 94300 ; steps/s = 1.66, tokens/s = 77632 (34234 source, 43398 target) ; Learning rate = 0.000288 ; Loss = 1.429735\n",
      "2024-12-04 02:02:54.166000: I runner.py:310] Step = 94400 ; steps/s = 1.64, tokens/s = 78359 (34491 source, 43868 target) ; Learning rate = 0.000288 ; Loss = 1.439572\n",
      "2024-12-04 02:03:54.865000: I runner.py:310] Step = 94500 ; steps/s = 1.65, tokens/s = 78520 (34594 source, 43926 target) ; Learning rate = 0.000288 ; Loss = 1.431843\n",
      "2024-12-04 02:04:55.158000: I runner.py:310] Step = 94600 ; steps/s = 1.66, tokens/s = 77604 (34206 source, 43398 target) ; Learning rate = 0.000287 ; Loss = 1.437951\n",
      "2024-12-04 02:05:55.936000: I runner.py:310] Step = 94700 ; steps/s = 1.65, tokens/s = 78435 (34555 source, 43880 target) ; Learning rate = 0.000287 ; Loss = 1.439743\n",
      "2024-12-04 02:06:56.235000: I runner.py:310] Step = 94800 ; steps/s = 1.66, tokens/s = 77648 (34246 source, 43402 target) ; Learning rate = 0.000287 ; Loss = 1.435272\n",
      "2024-12-04 02:07:56.932000: I runner.py:310] Step = 94900 ; steps/s = 1.65, tokens/s = 78496 (34555 source, 43941 target) ; Learning rate = 0.000287 ; Loss = 1.434004\n",
      "2024-12-04 02:08:57.681000: I runner.py:310] Step = 95000 ; steps/s = 1.65, tokens/s = 78439 (34547 source, 43892 target) ; Learning rate = 0.000287 ; Loss = 1.435811\n",
      "2024-12-04 02:08:57.683000: I training.py:192] Running evaluation for step 95000\n",
      "2024-12-04 02:09:45.108000: I training.py:192] Evaluation result for step 95000: loss = 0.870037 ; perplexity = 2.387000\n",
      "2024-12-04 02:10:45.324000: I runner.py:310] Step = 95100 ; steps/s = 1.66, tokens/s = 77741 (34274 source, 43467 target) ; Learning rate = 0.000287 ; Loss = 1.433298\n",
      "2024-12-04 02:11:46.180000: I runner.py:310] Step = 95200 ; steps/s = 1.64, tokens/s = 78333 (34509 source, 43824 target) ; Learning rate = 0.000286 ; Loss = 1.438756\n",
      "2024-12-04 02:12:46.956000: I runner.py:310] Step = 95300 ; steps/s = 1.65, tokens/s = 78394 (34517 source, 43877 target) ; Learning rate = 0.000286 ; Loss = 1.440362\n",
      "2024-12-04 02:13:47.358000: I runner.py:310] Step = 95400 ; steps/s = 1.66, tokens/s = 77495 (34168 source, 43327 target) ; Learning rate = 0.000286 ; Loss = 1.434996\n",
      "2024-12-04 02:14:48.250000: I runner.py:310] Step = 95500 ; steps/s = 1.64, tokens/s = 78277 (34475 source, 43802 target) ; Learning rate = 0.000286 ; Loss = 1.434862\n",
      "2024-12-04 02:15:48.652000: I runner.py:310] Step = 95600 ; steps/s = 1.66, tokens/s = 77461 (34142 source, 43319 target) ; Learning rate = 0.000286 ; Loss = 1.433716\n",
      "2024-12-04 02:16:49.518000: I runner.py:310] Step = 95700 ; steps/s = 1.64, tokens/s = 78247 (34440 source, 43807 target) ; Learning rate = 0.000286 ; Loss = 1.434269\n",
      "2024-12-04 02:17:50.298000: I runner.py:310] Step = 95800 ; steps/s = 1.65, tokens/s = 78439 (34561 source, 43878 target) ; Learning rate = 0.000286 ; Loss = 1.438791\n",
      "2024-12-04 02:18:50.631000: I runner.py:310] Step = 95900 ; steps/s = 1.66, tokens/s = 77595 (34210 source, 43385 target) ; Learning rate = 0.000285 ; Loss = 1.437361\n",
      "2024-12-04 02:19:51.510000: I runner.py:310] Step = 96000 ; steps/s = 1.64, tokens/s = 78264 (34462 source, 43802 target) ; Learning rate = 0.000285 ; Loss = 1.434032\n",
      "2024-12-04 02:20:51.874000: I runner.py:310] Step = 96100 ; steps/s = 1.66, tokens/s = 77556 (34208 source, 43348 target) ; Learning rate = 0.000285 ; Loss = 1.435511\n",
      "2024-12-04 02:21:52.735000: I runner.py:310] Step = 96200 ; steps/s = 1.64, tokens/s = 78269 (34466 source, 43803 target) ; Learning rate = 0.000285 ; Loss = 1.430900\n",
      "2024-12-04 02:22:53.554000: I runner.py:310] Step = 96300 ; steps/s = 1.64, tokens/s = 78374 (34518 source, 43856 target) ; Learning rate = 0.000285 ; Loss = 1.436985\n",
      "2024-12-04 02:23:53.912000: I runner.py:310] Step = 96400 ; steps/s = 1.66, tokens/s = 77546 (34178 source, 43368 target) ; Learning rate = 0.000285 ; Loss = 1.431404\n",
      "2024-12-04 02:24:54.696000: I runner.py:310] Step = 96500 ; steps/s = 1.65, tokens/s = 78412 (34539 source, 43873 target) ; Learning rate = 0.000285 ; Loss = 1.438366\n",
      "2024-12-04 02:25:55.493000: I runner.py:310] Step = 96600 ; steps/s = 1.64, tokens/s = 78366 (34509 source, 43857 target) ; Learning rate = 0.000284 ; Loss = 1.433113\n",
      "2024-12-04 02:26:55.778000: I runner.py:310] Step = 96700 ; steps/s = 1.66, tokens/s = 77633 (34221 source, 43412 target) ; Learning rate = 0.000284 ; Loss = 1.434570\n",
      "2024-12-04 02:27:56.590000: I runner.py:310] Step = 96800 ; steps/s = 1.64, tokens/s = 78368 (34522 source, 43846 target) ; Learning rate = 0.000284 ; Loss = 1.435412\n",
      "2024-12-04 02:28:56.909000: I runner.py:310] Step = 96900 ; steps/s = 1.66, tokens/s = 77619 (34230 source, 43389 target) ; Learning rate = 0.000284 ; Loss = 1.433703\n",
      "2024-12-04 02:29:57.682000: I runner.py:310] Step = 97000 ; steps/s = 1.65, tokens/s = 78385 (34502 source, 43883 target) ; Learning rate = 0.000284 ; Loss = 1.435872\n",
      "2024-12-04 02:30:58.448000: I runner.py:310] Step = 97100 ; steps/s = 1.65, tokens/s = 78455 (34563 source, 43892 target) ; Learning rate = 0.000284 ; Loss = 1.439823\n",
      "2024-12-04 02:31:58.806000: I runner.py:310] Step = 97200 ; steps/s = 1.66, tokens/s = 77539 (34186 source, 43353 target) ; Learning rate = 0.000284 ; Loss = 1.434625\n",
      "2024-12-04 02:32:59.571000: I runner.py:310] Step = 97300 ; steps/s = 1.65, tokens/s = 78431 (34550 source, 43881 target) ; Learning rate = 0.000283 ; Loss = 1.434506\n",
      "2024-12-04 02:33:59.934000: I runner.py:310] Step = 97400 ; steps/s = 1.66, tokens/s = 77544 (34184 source, 43360 target) ; Learning rate = 0.000283 ; Loss = 1.432409\n",
      "2024-12-04 02:35:00.709000: I runner.py:310] Step = 97500 ; steps/s = 1.65, tokens/s = 78381 (34506 source, 43875 target) ; Learning rate = 0.000283 ; Loss = 1.433130\n",
      "2024-12-04 02:36:01.530000: I runner.py:310] Step = 97600 ; steps/s = 1.64, tokens/s = 78376 (34537 source, 43839 target) ; Learning rate = 0.000283 ; Loss = 1.432144\n",
      "2024-12-04 02:37:01.825000: I runner.py:310] Step = 97700 ; steps/s = 1.66, tokens/s = 77609 (34198 source, 43411 target) ; Learning rate = 0.000283 ; Loss = 1.434141\n",
      "2024-12-04 02:38:02.611000: I runner.py:310] Step = 97800 ; steps/s = 1.65, tokens/s = 78395 (34517 source, 43878 target) ; Learning rate = 0.000283 ; Loss = 1.434166\n",
      "2024-12-04 02:39:03.396000: I runner.py:310] Step = 97900 ; steps/s = 1.65, tokens/s = 78412 (34542 source, 43870 target) ; Learning rate = 0.000282 ; Loss = 1.434272\n",
      "2024-12-04 02:40:03.690000: I runner.py:310] Step = 98000 ; steps/s = 1.66, tokens/s = 77648 (34246 source, 43402 target) ; Learning rate = 0.000282 ; Loss = 1.433087\n",
      "2024-12-04 02:41:04.469000: I runner.py:310] Step = 98100 ; steps/s = 1.65, tokens/s = 78389 (34515 source, 43874 target) ; Learning rate = 0.000282 ; Loss = 1.436945\n",
      "2024-12-04 02:42:04.811000: I runner.py:310] Step = 98200 ; steps/s = 1.66, tokens/s = 77571 (34202 source, 43369 target) ; Learning rate = 0.000282 ; Loss = 1.432296\n",
      "2024-12-04 02:43:05.585000: I runner.py:310] Step = 98300 ; steps/s = 1.65, tokens/s = 78400 (34523 source, 43877 target) ; Learning rate = 0.000282 ; Loss = 1.428892\n",
      "2024-12-04 02:44:06.363000: I runner.py:310] Step = 98400 ; steps/s = 1.65, tokens/s = 78411 (34533 source, 43878 target) ; Learning rate = 0.000282 ; Loss = 1.435854\n",
      "2024-12-04 02:45:06.751000: I runner.py:310] Step = 98500 ; steps/s = 1.66, tokens/s = 77491 (34150 source, 43341 target) ; Learning rate = 0.000282 ; Loss = 1.436244\n",
      "2024-12-04 02:46:07.479000: I runner.py:310] Step = 98600 ; steps/s = 1.65, tokens/s = 78486 (34573 source, 43913 target) ; Learning rate = 0.000281 ; Loss = 1.434845\n",
      "2024-12-04 02:47:07.780000: I runner.py:310] Step = 98700 ; steps/s = 1.66, tokens/s = 77642 (34242 source, 43400 target) ; Learning rate = 0.000281 ; Loss = 1.432404\n",
      "2024-12-04 02:48:08.547000: I runner.py:310] Step = 98800 ; steps/s = 1.65, tokens/s = 78361 (34479 source, 43882 target) ; Learning rate = 0.000281 ; Loss = 1.432639\n",
      "2024-12-04 02:49:09.327000: I runner.py:310] Step = 98900 ; steps/s = 1.65, tokens/s = 78444 (34567 source, 43877 target) ; Learning rate = 0.000281 ; Loss = 1.437443\n",
      "2024-12-04 02:50:09.602000: I runner.py:310] Step = 99000 ; steps/s = 1.66, tokens/s = 77638 (34217 source, 43421 target) ; Learning rate = 0.000281 ; Loss = 1.431656\n",
      "2024-12-04 02:51:10.380000: I runner.py:310] Step = 99100 ; steps/s = 1.65, tokens/s = 78424 (34553 source, 43871 target) ; Learning rate = 0.000281 ; Loss = 1.436959\n",
      "2024-12-04 02:52:11.115000: I runner.py:310] Step = 99200 ; steps/s = 1.65, tokens/s = 78475 (34564 source, 43911 target) ; Learning rate = 0.000281 ; Loss = 1.436979\n",
      "2024-12-04 02:53:11.405000: I runner.py:310] Step = 99300 ; steps/s = 1.66, tokens/s = 77620 (34209 source, 43411 target) ; Learning rate = 0.000280 ; Loss = 1.436611\n",
      "2024-12-04 02:54:12.263000: I runner.py:310] Step = 99400 ; steps/s = 1.64, tokens/s = 78292 (34480 source, 43812 target) ; Learning rate = 0.000280 ; Loss = 1.435152\n",
      "2024-12-04 02:55:12.621000: I runner.py:310] Step = 99500 ; steps/s = 1.66, tokens/s = 77580 (34221 source, 43359 target) ; Learning rate = 0.000280 ; Loss = 1.435986\n",
      "2024-12-04 02:56:13.390000: I runner.py:310] Step = 99600 ; steps/s = 1.65, tokens/s = 78398 (34512 source, 43886 target) ; Learning rate = 0.000280 ; Loss = 1.437082\n",
      "2024-12-04 02:57:14.188000: I runner.py:310] Step = 99700 ; steps/s = 1.65, tokens/s = 78383 (34518 source, 43865 target) ; Learning rate = 0.000280 ; Loss = 1.435926\n",
      "2024-12-04 02:58:14.533000: I runner.py:310] Step = 99800 ; steps/s = 1.66, tokens/s = 77556 (34186 source, 43370 target) ; Learning rate = 0.000280 ; Loss = 1.434005\n",
      "2024-12-04 02:59:15.321000: I runner.py:310] Step = 99900 ; steps/s = 1.65, tokens/s = 78404 (34529 source, 43875 target) ; Learning rate = 0.000280 ; Loss = 1.434247\n",
      "2024-12-04 03:00:15.656000: I runner.py:310] Step = 100000 ; steps/s = 1.66, tokens/s = 77592 (34225 source, 43367 target) ; Learning rate = 0.000280 ; Loss = 1.431322\n",
      "2024-12-04 03:00:17.363000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-100000\n",
      "2024-12-04 03:00:17.363000: I training.py:192] Running evaluation for step 100000\n",
      "2024-12-04 03:01:04.560000: I training.py:192] Evaluation result for step 100000: loss = 0.876103 ; perplexity = 2.401522\n",
      "2024-12-04 03:02:05.200000: I runner.py:310] Step = 100100 ; steps/s = 1.65, tokens/s = 78566 (34587 source, 43979 target) ; Learning rate = 0.000279 ; Loss = 1.432567\n",
      "2024-12-04 03:03:06.004000: I runner.py:310] Step = 100200 ; steps/s = 1.64, tokens/s = 78399 (34540 source, 43859 target) ; Learning rate = 0.000279 ; Loss = 1.439004\n",
      "2024-12-04 03:04:06.347000: I runner.py:310] Step = 100300 ; steps/s = 1.66, tokens/s = 77557 (34187 source, 43370 target) ; Learning rate = 0.000279 ; Loss = 1.436565\n",
      "2024-12-04 03:05:07.141000: I runner.py:310] Step = 100400 ; steps/s = 1.65, tokens/s = 78399 (34519 source, 43880 target) ; Learning rate = 0.000279 ; Loss = 1.438234\n",
      "2024-12-04 03:06:08.010000: I runner.py:310] Step = 100500 ; steps/s = 1.64, tokens/s = 78289 (34488 source, 43801 target) ; Learning rate = 0.000279 ; Loss = 1.433977\n",
      "2024-12-04 03:07:08.358000: I runner.py:310] Step = 100600 ; steps/s = 1.66, tokens/s = 77540 (34179 source, 43361 target) ; Learning rate = 0.000279 ; Loss = 1.433772\n",
      "2024-12-04 03:08:09.156000: I runner.py:310] Step = 100700 ; steps/s = 1.65, tokens/s = 78395 (34536 source, 43859 target) ; Learning rate = 0.000279 ; Loss = 1.439088\n",
      "2024-12-04 03:09:09.445000: I runner.py:310] Step = 100800 ; steps/s = 1.66, tokens/s = 77640 (34223 source, 43417 target) ; Learning rate = 0.000278 ; Loss = 1.434339\n",
      "2024-12-04 03:10:10.248000: I runner.py:310] Step = 100900 ; steps/s = 1.64, tokens/s = 78362 (34501 source, 43861 target) ; Learning rate = 0.000278 ; Loss = 1.434534\n",
      "2024-12-04 03:11:11.053000: I runner.py:310] Step = 101000 ; steps/s = 1.64, tokens/s = 78386 (34534 source, 43852 target) ; Learning rate = 0.000278 ; Loss = 1.434168\n",
      "2024-12-04 03:12:11.346000: I runner.py:310] Step = 101100 ; steps/s = 1.66, tokens/s = 77637 (34233 source, 43404 target) ; Learning rate = 0.000278 ; Loss = 1.435367\n",
      "2024-12-04 03:13:12.141000: I runner.py:310] Step = 101200 ; steps/s = 1.65, tokens/s = 78378 (34514 source, 43864 target) ; Learning rate = 0.000278 ; Loss = 1.439233\n",
      "2024-12-04 03:14:12.484000: I runner.py:310] Step = 101300 ; steps/s = 1.66, tokens/s = 77582 (34210 source, 43372 target) ; Learning rate = 0.000278 ; Loss = 1.427481\n",
      "2024-12-04 03:15:13.277000: I runner.py:310] Step = 101400 ; steps/s = 1.65, tokens/s = 78380 (34519 source, 43861 target) ; Learning rate = 0.000278 ; Loss = 1.430127\n",
      "2024-12-04 03:16:14.101000: I runner.py:310] Step = 101500 ; steps/s = 1.64, tokens/s = 78353 (34514 source, 43839 target) ; Learning rate = 0.000277 ; Loss = 1.439612\n",
      "2024-12-04 03:17:14.374000: I runner.py:310] Step = 101600 ; steps/s = 1.66, tokens/s = 77646 (34219 source, 43427 target) ; Learning rate = 0.000277 ; Loss = 1.435670\n",
      "2024-12-04 03:18:15.136000: I runner.py:310] Step = 101700 ; steps/s = 1.65, tokens/s = 78423 (34528 source, 43895 target) ; Learning rate = 0.000277 ; Loss = 1.433257\n",
      "2024-12-04 03:19:15.940000: I runner.py:310] Step = 101800 ; steps/s = 1.64, tokens/s = 78387 (34531 source, 43856 target) ; Learning rate = 0.000277 ; Loss = 1.436908\n",
      "2024-12-04 03:20:16.278000: I runner.py:310] Step = 101900 ; steps/s = 1.66, tokens/s = 77571 (34198 source, 43373 target) ; Learning rate = 0.000277 ; Loss = 1.431602\n",
      "2024-12-04 03:21:17.091000: I runner.py:310] Step = 102000 ; steps/s = 1.64, tokens/s = 78355 (34509 source, 43846 target) ; Learning rate = 0.000277 ; Loss = 1.433176\n",
      "2024-12-04 03:22:17.396000: I runner.py:310] Step = 102100 ; steps/s = 1.66, tokens/s = 77637 (34231 source, 43406 target) ; Learning rate = 0.000277 ; Loss = 1.437388\n",
      "2024-12-04 03:23:18.148000: I runner.py:310] Step = 102200 ; steps/s = 1.65, tokens/s = 78418 (34529 source, 43889 target) ; Learning rate = 0.000276 ; Loss = 1.433319\n",
      "2024-12-04 03:24:18.917000: I runner.py:310] Step = 102300 ; steps/s = 1.65, tokens/s = 78429 (34548 source, 43881 target) ; Learning rate = 0.000276 ; Loss = 1.434968\n",
      "2024-12-04 03:25:19.261000: I runner.py:310] Step = 102400 ; steps/s = 1.66, tokens/s = 77554 (34189 source, 43365 target) ; Learning rate = 0.000276 ; Loss = 1.433657\n",
      "2024-12-04 03:26:20.058000: I runner.py:310] Step = 102500 ; steps/s = 1.65, tokens/s = 78389 (34522 source, 43867 target) ; Learning rate = 0.000276 ; Loss = 1.432895\n",
      "2024-12-04 03:27:20.380000: I runner.py:310] Step = 102600 ; steps/s = 1.66, tokens/s = 77606 (34217 source, 43389 target) ; Learning rate = 0.000276 ; Loss = 1.427320\n",
      "2024-12-04 03:28:21.163000: I runner.py:310] Step = 102700 ; steps/s = 1.65, tokens/s = 78391 (34513 source, 43878 target) ; Learning rate = 0.000276 ; Loss = 1.428358\n",
      "2024-12-04 03:29:21.950000: I runner.py:310] Step = 102800 ; steps/s = 1.65, tokens/s = 78382 (34518 source, 43864 target) ; Learning rate = 0.000276 ; Loss = 1.434614\n",
      "2024-12-04 03:30:22.257000: I runner.py:310] Step = 102900 ; steps/s = 1.66, tokens/s = 77629 (34232 source, 43397 target) ; Learning rate = 0.000276 ; Loss = 1.434489\n",
      "2024-12-04 03:31:23.087000: I runner.py:310] Step = 103000 ; steps/s = 1.64, tokens/s = 78326 (34480 source, 43846 target) ; Learning rate = 0.000275 ; Loss = 1.434986\n",
      "2024-12-04 03:32:23.900000: I runner.py:310] Step = 103100 ; steps/s = 1.64, tokens/s = 78368 (34529 source, 43839 target) ; Learning rate = 0.000275 ; Loss = 1.434274\n",
      "2024-12-04 03:33:24.249000: I runner.py:310] Step = 103200 ; steps/s = 1.66, tokens/s = 77554 (34190 source, 43364 target) ; Learning rate = 0.000275 ; Loss = 1.430747\n",
      "2024-12-04 03:34:25.063000: I runner.py:310] Step = 103300 ; steps/s = 1.64, tokens/s = 78341 (34503 source, 43838 target) ; Learning rate = 0.000275 ; Loss = 1.436156\n",
      "2024-12-04 03:35:25.380000: I runner.py:310] Step = 103400 ; steps/s = 1.66, tokens/s = 77622 (34221 source, 43401 target) ; Learning rate = 0.000275 ; Loss = 1.429038\n",
      "2024-12-04 03:36:26.218000: I runner.py:310] Step = 103500 ; steps/s = 1.64, tokens/s = 78320 (34486 source, 43834 target) ; Learning rate = 0.000275 ; Loss = 1.434483\n",
      "2024-12-04 03:37:27.025000: I runner.py:310] Step = 103600 ; steps/s = 1.64, tokens/s = 78364 (34510 source, 43854 target) ; Learning rate = 0.000275 ; Loss = 1.434845\n",
      "2024-12-04 03:38:27.281000: I runner.py:310] Step = 103700 ; steps/s = 1.66, tokens/s = 77697 (34262 source, 43435 target) ; Learning rate = 0.000274 ; Loss = 1.431952\n",
      "2024-12-04 03:39:28.089000: I runner.py:310] Step = 103800 ; steps/s = 1.64, tokens/s = 78381 (34527 source, 43854 target) ; Learning rate = 0.000274 ; Loss = 1.431490\n",
      "2024-12-04 03:40:28.451000: I runner.py:310] Step = 103900 ; steps/s = 1.66, tokens/s = 77547 (34193 source, 43354 target) ; Learning rate = 0.000274 ; Loss = 1.425247\n",
      "2024-12-04 03:41:29.174000: I runner.py:310] Step = 104000 ; steps/s = 1.65, tokens/s = 78466 (34552 source, 43914 target) ; Learning rate = 0.000274 ; Loss = 1.438972\n",
      "2024-12-04 03:42:29.989000: I runner.py:310] Step = 104100 ; steps/s = 1.64, tokens/s = 78354 (34508 source, 43846 target) ; Learning rate = 0.000274 ; Loss = 1.434119\n",
      "2024-12-04 03:43:30.274000: I runner.py:310] Step = 104200 ; steps/s = 1.66, tokens/s = 77627 (34210 source, 43417 target) ; Learning rate = 0.000274 ; Loss = 1.432076\n",
      "2024-12-04 03:44:31.058000: I runner.py:310] Step = 104300 ; steps/s = 1.65, tokens/s = 78400 (34526 source, 43874 target) ; Learning rate = 0.000274 ; Loss = 1.432828\n",
      "2024-12-04 03:45:31.949000: I runner.py:310] Step = 104400 ; steps/s = 1.64, tokens/s = 78267 (34475 source, 43792 target) ; Learning rate = 0.000274 ; Loss = 1.434590\n",
      "2024-12-04 03:46:32.227000: I runner.py:310] Step = 104500 ; steps/s = 1.66, tokens/s = 77639 (34219 source, 43420 target) ; Learning rate = 0.000273 ; Loss = 1.434182\n",
      "2024-12-04 03:47:33.043000: I runner.py:310] Step = 104600 ; steps/s = 1.64, tokens/s = 78365 (34520 source, 43845 target) ; Learning rate = 0.000273 ; Loss = 1.432565\n",
      "2024-12-04 03:48:33.411000: I runner.py:310] Step = 104700 ; steps/s = 1.66, tokens/s = 77551 (34199 source, 43352 target) ; Learning rate = 0.000273 ; Loss = 1.432461\n",
      "2024-12-04 03:49:34.246000: I runner.py:310] Step = 104800 ; steps/s = 1.64, tokens/s = 78357 (34522 source, 43835 target) ; Learning rate = 0.000273 ; Loss = 1.436549\n",
      "2024-12-04 03:50:35.070000: I runner.py:310] Step = 104900 ; steps/s = 1.64, tokens/s = 78313 (34468 source, 43845 target) ; Learning rate = 0.000273 ; Loss = 1.434596\n",
      "2024-12-04 03:51:35.429000: I runner.py:310] Step = 105000 ; steps/s = 1.66, tokens/s = 77540 (34188 source, 43352 target) ; Learning rate = 0.000273 ; Loss = 1.432145\n",
      "2024-12-04 03:51:35.430000: I training.py:192] Running evaluation for step 105000\n",
      "2024-12-04 03:52:23.121000: I training.py:192] Evaluation result for step 105000: loss = 0.878194 ; perplexity = 2.406549\n",
      "2024-12-04 03:53:23.827000: I runner.py:310] Step = 105100 ; steps/s = 1.65, tokens/s = 78488 (34554 source, 43934 target) ; Learning rate = 0.000273 ; Loss = 1.432791\n",
      "2024-12-04 03:54:24.192000: I runner.py:310] Step = 105200 ; steps/s = 1.66, tokens/s = 77585 (34224 source, 43361 target) ; Learning rate = 0.000273 ; Loss = 1.430722\n",
      "2024-12-04 03:55:24.969000: I runner.py:310] Step = 105300 ; steps/s = 1.65, tokens/s = 78399 (34510 source, 43889 target) ; Learning rate = 0.000272 ; Loss = 1.431199\n",
      "2024-12-04 03:56:25.787000: I runner.py:310] Step = 105400 ; steps/s = 1.64, tokens/s = 78369 (34522 source, 43847 target) ; Learning rate = 0.000272 ; Loss = 1.433726\n",
      "2024-12-04 03:57:26.146000: I runner.py:310] Step = 105500 ; steps/s = 1.66, tokens/s = 77526 (34180 source, 43346 target) ; Learning rate = 0.000272 ; Loss = 1.434116\n",
      "2024-12-04 03:58:26.945000: I runner.py:310] Step = 105600 ; steps/s = 1.64, tokens/s = 78386 (34517 source, 43869 target) ; Learning rate = 0.000272 ; Loss = 1.437269\n",
      "2024-12-04 03:59:27.718000: I runner.py:310] Step = 105700 ; steps/s = 1.65, tokens/s = 78409 (34535 source, 43874 target) ; Learning rate = 0.000272 ; Loss = 1.435460\n",
      "2024-12-04 04:00:28.005000: I runner.py:310] Step = 105800 ; steps/s = 1.66, tokens/s = 77647 (34236 source, 43411 target) ; Learning rate = 0.000272 ; Loss = 1.433732\n",
      "2024-12-04 04:01:28.765000: I runner.py:310] Step = 105900 ; steps/s = 1.65, tokens/s = 78434 (34549 source, 43885 target) ; Learning rate = 0.000272 ; Loss = 1.435651\n",
      "2024-12-04 04:02:29.093000: I runner.py:310] Step = 106000 ; steps/s = 1.66, tokens/s = 77563 (34183 source, 43380 target) ; Learning rate = 0.000271 ; Loss = 1.431448\n",
      "2024-12-04 04:03:29.942000: I runner.py:310] Step = 106100 ; steps/s = 1.64, tokens/s = 78308 (34495 source, 43813 target) ; Learning rate = 0.000271 ; Loss = 1.431458\n",
      "2024-12-04 04:04:30.729000: I runner.py:310] Step = 106200 ; steps/s = 1.65, tokens/s = 78402 (34517 source, 43885 target) ; Learning rate = 0.000271 ; Loss = 1.438387\n",
      "2024-12-04 04:05:31.032000: I runner.py:310] Step = 106300 ; steps/s = 1.66, tokens/s = 77607 (34211 source, 43396 target) ; Learning rate = 0.000271 ; Loss = 1.432490\n",
      "2024-12-04 04:06:31.835000: I runner.py:310] Step = 106400 ; steps/s = 1.64, tokens/s = 78405 (34545 source, 43860 target) ; Learning rate = 0.000271 ; Loss = 1.432188\n",
      "2024-12-04 04:07:32.202000: I runner.py:310] Step = 106500 ; steps/s = 1.66, tokens/s = 77549 (34191 source, 43358 target) ; Learning rate = 0.000271 ; Loss = 1.428235\n",
      "2024-12-04 04:08:33.005000: I runner.py:310] Step = 106600 ; steps/s = 1.64, tokens/s = 78387 (34522 source, 43865 target) ; Learning rate = 0.000271 ; Loss = 1.437119\n",
      "2024-12-04 04:09:33.760000: I runner.py:310] Step = 106700 ; steps/s = 1.65, tokens/s = 78433 (34543 source, 43890 target) ; Learning rate = 0.000271 ; Loss = 1.434903\n",
      "2024-12-04 04:10:34.111000: I runner.py:310] Step = 106800 ; steps/s = 1.66, tokens/s = 77518 (34166 source, 43352 target) ; Learning rate = 0.000270 ; Loss = 1.434125\n",
      "2024-12-04 04:11:34.900000: I runner.py:310] Step = 106900 ; steps/s = 1.65, tokens/s = 78393 (34516 source, 43877 target) ; Learning rate = 0.000270 ; Loss = 1.434152\n",
      "2024-12-04 04:12:35.609000: I runner.py:310] Step = 107000 ; steps/s = 1.65, tokens/s = 78190 (34452 source, 43738 target) ; Learning rate = 0.000270 ; Loss = 1.435188\n",
      "2024-12-04 04:13:36.032000: I runner.py:310] Step = 107100 ; steps/s = 1.66, tokens/s = 77762 (34265 source, 43497 target) ; Learning rate = 0.000270 ; Loss = 1.429677\n",
      "2024-12-04 04:14:36.830000: I runner.py:310] Step = 107200 ; steps/s = 1.64, tokens/s = 78399 (34545 source, 43854 target) ; Learning rate = 0.000270 ; Loss = 1.432939\n",
      "2024-12-04 04:15:37.114000: I runner.py:310] Step = 107300 ; steps/s = 1.66, tokens/s = 77641 (34226 source, 43415 target) ; Learning rate = 0.000270 ; Loss = 1.430998\n",
      "2024-12-04 04:16:37.919000: I runner.py:310] Step = 107400 ; steps/s = 1.64, tokens/s = 78368 (34522 source, 43846 target) ; Learning rate = 0.000270 ; Loss = 1.432253\n",
      "2024-12-04 04:17:38.687000: I runner.py:310] Step = 107500 ; steps/s = 1.65, tokens/s = 78431 (34532 source, 43899 target) ; Learning rate = 0.000270 ; Loss = 1.427790\n",
      "2024-12-04 04:18:39.092000: I runner.py:310] Step = 107600 ; steps/s = 1.66, tokens/s = 77468 (34146 source, 43322 target) ; Learning rate = 0.000269 ; Loss = 1.432451\n",
      "2024-12-04 04:19:39.858000: I runner.py:310] Step = 107700 ; steps/s = 1.65, tokens/s = 78423 (34540 source, 43883 target) ; Learning rate = 0.000269 ; Loss = 1.431298\n",
      "2024-12-04 04:20:40.151000: I runner.py:310] Step = 107800 ; steps/s = 1.66, tokens/s = 77643 (34230 source, 43413 target) ; Learning rate = 0.000269 ; Loss = 1.429655\n",
      "2024-12-04 04:21:40.934000: I runner.py:310] Step = 107900 ; steps/s = 1.65, tokens/s = 78397 (34520 source, 43877 target) ; Learning rate = 0.000269 ; Loss = 1.434986\n",
      "2024-12-04 04:22:41.757000: I runner.py:310] Step = 108000 ; steps/s = 1.64, tokens/s = 78373 (34538 source, 43835 target) ; Learning rate = 0.000269 ; Loss = 1.436725\n",
      "2024-12-04 04:23:42.077000: I runner.py:310] Step = 108100 ; steps/s = 1.66, tokens/s = 77593 (34193 source, 43400 target) ; Learning rate = 0.000269 ; Loss = 1.432343\n",
      "2024-12-04 04:24:42.858000: I runner.py:310] Step = 108200 ; steps/s = 1.65, tokens/s = 78397 (34528 source, 43869 target) ; Learning rate = 0.000269 ; Loss = 1.432357\n",
      "2024-12-04 04:25:43.406000: I runner.py:310] Step = 108300 ; steps/s = 1.65, tokens/s = 77917 (34363 source, 43554 target) ; Learning rate = 0.000269 ; Loss = 1.465570\n",
      "2024-12-04 04:26:43.985000: I runner.py:310] Step = 108400 ; steps/s = 1.65, tokens/s = 78048 (34350 source, 43698 target) ; Learning rate = 0.000268 ; Loss = 1.430997\n",
      "2024-12-04 04:27:44.735000: I runner.py:310] Step = 108500 ; steps/s = 1.65, tokens/s = 78438 (34547 source, 43891 target) ; Learning rate = 0.000268 ; Loss = 1.434522\n",
      "2024-12-04 04:28:45.058000: I runner.py:310] Step = 108600 ; steps/s = 1.66, tokens/s = 77602 (34221 source, 43381 target) ; Learning rate = 0.000268 ; Loss = 1.433763\n",
      "2024-12-04 04:29:45.791000: I runner.py:310] Step = 108700 ; steps/s = 1.65, tokens/s = 78473 (34559 source, 43914 target) ; Learning rate = 0.000268 ; Loss = 1.433124\n",
      "2024-12-04 04:30:46.573000: I runner.py:310] Step = 108800 ; steps/s = 1.65, tokens/s = 78386 (34518 source, 43868 target) ; Learning rate = 0.000268 ; Loss = 1.434164\n",
      "2024-12-04 04:31:46.883000: I runner.py:310] Step = 108900 ; steps/s = 1.66, tokens/s = 77600 (34207 source, 43393 target) ; Learning rate = 0.000268 ; Loss = 1.434849\n",
      "2024-12-04 04:32:47.673000: I runner.py:310] Step = 109000 ; steps/s = 1.65, tokens/s = 78382 (34527 source, 43855 target) ; Learning rate = 0.000268 ; Loss = 1.433543\n",
      "2024-12-04 04:33:47.992000: I runner.py:310] Step = 109100 ; steps/s = 1.66, tokens/s = 77630 (34239 source, 43391 target) ; Learning rate = 0.000268 ; Loss = 1.431464\n",
      "2024-12-04 04:34:48.749000: I runner.py:310] Step = 109200 ; steps/s = 1.65, tokens/s = 78410 (34512 source, 43898 target) ; Learning rate = 0.000267 ; Loss = 1.433629\n",
      "2024-12-04 04:35:49.516000: I runner.py:310] Step = 109300 ; steps/s = 1.65, tokens/s = 78423 (34544 source, 43879 target) ; Learning rate = 0.000267 ; Loss = 1.428818\n",
      "2024-12-04 04:36:49.880000: I runner.py:310] Step = 109400 ; steps/s = 1.66, tokens/s = 77530 (34172 source, 43358 target) ; Learning rate = 0.000267 ; Loss = 1.434290\n",
      "2024-12-04 04:37:50.679000: I runner.py:310] Step = 109500 ; steps/s = 1.64, tokens/s = 78388 (34532 source, 43856 target) ; Learning rate = 0.000267 ; Loss = 1.429979\n",
      "2024-12-04 04:38:51.042000: I runner.py:310] Step = 109600 ; steps/s = 1.66, tokens/s = 77567 (34212 source, 43355 target) ; Learning rate = 0.000267 ; Loss = 1.433366\n",
      "2024-12-04 04:39:51.882000: I runner.py:310] Step = 109700 ; steps/s = 1.64, tokens/s = 78306 (34462 source, 43844 target) ; Learning rate = 0.000267 ; Loss = 1.433440\n",
      "2024-12-04 04:40:52.748000: I runner.py:310] Step = 109800 ; steps/s = 1.64, tokens/s = 78288 (34478 source, 43810 target) ; Learning rate = 0.000267 ; Loss = 1.433511\n",
      "2024-12-04 04:41:53.083000: I runner.py:310] Step = 109900 ; steps/s = 1.66, tokens/s = 77584 (34216 source, 43368 target) ; Learning rate = 0.000267 ; Loss = 1.426849\n",
      "2024-12-04 04:42:53.893000: I runner.py:310] Step = 110000 ; steps/s = 1.64, tokens/s = 78342 (34487 source, 43855 target) ; Learning rate = 0.000266 ; Loss = 1.432899\n",
      "2024-12-04 04:42:55.612000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-110000\n",
      "2024-12-04 04:42:55.612000: I training.py:192] Running evaluation for step 110000\n",
      "2024-12-04 04:43:42.774000: I training.py:192] Evaluation result for step 110000: loss = 0.884238 ; perplexity = 2.421138\n",
      "2024-12-04 04:44:43.500000: I runner.py:310] Step = 110100 ; steps/s = 1.65, tokens/s = 78543 (34601 source, 43942 target) ; Learning rate = 0.000266 ; Loss = 1.430473\n",
      "2024-12-04 04:45:43.789000: I runner.py:310] Step = 110200 ; steps/s = 1.66, tokens/s = 77630 (34233 source, 43397 target) ; Learning rate = 0.000266 ; Loss = 1.434126\n",
      "2024-12-04 04:46:44.639000: I runner.py:310] Step = 110300 ; steps/s = 1.64, tokens/s = 78297 (34473 source, 43824 target) ; Learning rate = 0.000266 ; Loss = 1.434374\n",
      "2024-12-04 04:47:45.036000: I runner.py:310] Step = 110400 ; steps/s = 1.66, tokens/s = 77514 (34180 source, 43334 target) ; Learning rate = 0.000266 ; Loss = 1.430196\n",
      "2024-12-04 04:48:45.806000: I runner.py:310] Step = 110500 ; steps/s = 1.65, tokens/s = 78379 (34496 source, 43883 target) ; Learning rate = 0.000266 ; Loss = 1.429272\n",
      "2024-12-04 04:49:46.608000: I runner.py:310] Step = 110600 ; steps/s = 1.64, tokens/s = 78416 (34553 source, 43863 target) ; Learning rate = 0.000266 ; Loss = 1.431007\n",
      "2024-12-04 04:50:46.928000: I runner.py:310] Step = 110700 ; steps/s = 1.66, tokens/s = 77592 (34220 source, 43372 target) ; Learning rate = 0.000266 ; Loss = 1.431992\n",
      "2024-12-04 04:51:47.778000: I runner.py:310] Step = 110800 ; steps/s = 1.64, tokens/s = 78321 (34495 source, 43826 target) ; Learning rate = 0.000266 ; Loss = 1.432548\n",
      "2024-12-04 04:52:48.209000: I runner.py:310] Step = 110900 ; steps/s = 1.65, tokens/s = 77472 (34157 source, 43315 target) ; Learning rate = 0.000265 ; Loss = 1.424287\n",
      "2024-12-04 04:53:49.025000: I runner.py:310] Step = 111000 ; steps/s = 1.64, tokens/s = 78348 (34493 source, 43855 target) ; Learning rate = 0.000265 ; Loss = 1.434197\n",
      "2024-12-04 04:54:49.792000: I runner.py:310] Step = 111100 ; steps/s = 1.65, tokens/s = 78413 (34539 source, 43874 target) ; Learning rate = 0.000265 ; Loss = 1.430605\n",
      "2024-12-04 04:55:50.137000: I runner.py:310] Step = 111200 ; steps/s = 1.66, tokens/s = 77555 (34187 source, 43368 target) ; Learning rate = 0.000265 ; Loss = 1.431698\n",
      "2024-12-04 04:56:50.950000: I runner.py:310] Step = 111300 ; steps/s = 1.64, tokens/s = 78360 (34506 source, 43854 target) ; Learning rate = 0.000265 ; Loss = 1.427805\n",
      "2024-12-04 04:57:51.723000: I runner.py:310] Step = 111400 ; steps/s = 1.65, tokens/s = 78407 (34527 source, 43880 target) ; Learning rate = 0.000265 ; Loss = 1.436549\n",
      "2024-12-04 04:58:52.031000: I runner.py:310] Step = 111500 ; steps/s = 1.66, tokens/s = 77606 (34211 source, 43395 target) ; Learning rate = 0.000265 ; Loss = 1.429665\n",
      "2024-12-04 04:59:52.846000: I runner.py:310] Step = 111600 ; steps/s = 1.64, tokens/s = 78374 (34527 source, 43847 target) ; Learning rate = 0.000265 ; Loss = 1.433534\n",
      "2024-12-04 05:00:53.195000: I runner.py:310] Step = 111700 ; steps/s = 1.66, tokens/s = 77562 (34194 source, 43368 target) ; Learning rate = 0.000264 ; Loss = 1.428459\n",
      "2024-12-04 05:01:53.952000: I runner.py:310] Step = 111800 ; steps/s = 1.65, tokens/s = 78429 (34537 source, 43892 target) ; Learning rate = 0.000264 ; Loss = 1.435735\n",
      "2024-12-04 05:02:54.744000: I runner.py:310] Step = 111900 ; steps/s = 1.65, tokens/s = 78389 (34524 source, 43865 target) ; Learning rate = 0.000264 ; Loss = 1.437644\n",
      "2024-12-04 05:03:55.122000: I runner.py:310] Step = 112000 ; steps/s = 1.66, tokens/s = 77504 (34165 source, 43339 target) ; Learning rate = 0.000264 ; Loss = 1.433629\n",
      "2024-12-04 05:04:55.866000: I runner.py:310] Step = 112100 ; steps/s = 1.65, tokens/s = 78476 (34565 source, 43911 target) ; Learning rate = 0.000264 ; Loss = 1.430652\n",
      "2024-12-04 05:05:56.143000: I runner.py:310] Step = 112200 ; steps/s = 1.66, tokens/s = 77671 (34256 source, 43415 target) ; Learning rate = 0.000264 ; Loss = 1.430966\n",
      "2024-12-04 05:06:56.895000: I runner.py:310] Step = 112300 ; steps/s = 1.65, tokens/s = 78389 (34496 source, 43893 target) ; Learning rate = 0.000264 ; Loss = 1.434849\n",
      "2024-12-04 05:07:57.715000: I runner.py:310] Step = 112400 ; steps/s = 1.64, tokens/s = 78388 (34539 source, 43849 target) ; Learning rate = 0.000264 ; Loss = 1.430863\n",
      "2024-12-04 05:08:58.035000: I runner.py:310] Step = 112500 ; steps/s = 1.66, tokens/s = 77602 (34212 source, 43390 target) ; Learning rate = 0.000264 ; Loss = 1.432000\n",
      "2024-12-04 05:09:58.809000: I runner.py:310] Step = 112600 ; steps/s = 1.65, tokens/s = 78409 (34528 source, 43881 target) ; Learning rate = 0.000263 ; Loss = 1.433852\n",
      "2024-12-04 05:10:59.581000: I runner.py:310] Step = 112700 ; steps/s = 1.65, tokens/s = 78415 (34539 source, 43876 target) ; Learning rate = 0.000263 ; Loss = 1.429458\n",
      "2024-12-04 05:11:59.855000: I runner.py:310] Step = 112800 ; steps/s = 1.66, tokens/s = 77630 (34215 source, 43415 target) ; Learning rate = 0.000263 ; Loss = 1.431677\n",
      "2024-12-04 05:13:00.696000: I runner.py:310] Step = 112900 ; steps/s = 1.64, tokens/s = 78336 (34495 source, 43841 target) ; Learning rate = 0.000263 ; Loss = 1.429077\n",
      "2024-12-04 05:14:01.056000: I runner.py:310] Step = 113000 ; steps/s = 1.66, tokens/s = 77555 (34199 source, 43356 target) ; Learning rate = 0.000263 ; Loss = 1.430828\n",
      "2024-12-04 05:15:01.783000: I runner.py:310] Step = 113100 ; steps/s = 1.65, tokens/s = 78451 (34551 source, 43900 target) ; Learning rate = 0.000263 ; Loss = 1.429586\n",
      "2024-12-04 05:16:02.615000: I runner.py:310] Step = 113200 ; steps/s = 1.64, tokens/s = 78366 (34516 source, 43850 target) ; Learning rate = 0.000263 ; Loss = 1.432269\n",
      "2024-12-04 05:17:02.931000: I runner.py:310] Step = 113300 ; steps/s = 1.66, tokens/s = 77601 (34212 source, 43389 target) ; Learning rate = 0.000263 ; Loss = 1.431957\n",
      "2024-12-04 05:18:03.776000: I runner.py:310] Step = 113400 ; steps/s = 1.64, tokens/s = 78334 (34504 source, 43830 target) ; Learning rate = 0.000262 ; Loss = 1.432469\n",
      "2024-12-04 05:19:04.137000: I runner.py:310] Step = 113500 ; steps/s = 1.66, tokens/s = 77551 (34193 source, 43358 target) ; Learning rate = 0.000262 ; Loss = 1.430005\n",
      "2024-12-04 05:20:04.919000: I runner.py:310] Step = 113600 ; steps/s = 1.65, tokens/s = 78381 (34509 source, 43872 target) ; Learning rate = 0.000262 ; Loss = 1.427480\n",
      "2024-12-04 05:21:05.720000: I runner.py:310] Step = 113700 ; steps/s = 1.64, tokens/s = 78380 (34517 source, 43863 target) ; Learning rate = 0.000262 ; Loss = 1.430808\n",
      "2024-12-04 05:22:05.921000: I runner.py:310] Step = 113800 ; steps/s = 1.66, tokens/s = 77744 (34275 source, 43469 target) ; Learning rate = 0.000262 ; Loss = 1.429103\n",
      "2024-12-04 05:23:06.730000: I runner.py:310] Step = 113900 ; steps/s = 1.64, tokens/s = 78365 (34515 source, 43850 target) ; Learning rate = 0.000262 ; Loss = 1.432017\n",
      "2024-12-04 05:24:07.514000: I runner.py:310] Step = 114000 ; steps/s = 1.65, tokens/s = 78411 (34532 source, 43879 target) ; Learning rate = 0.000262 ; Loss = 1.433804\n",
      "2024-12-04 05:25:07.796000: I runner.py:310] Step = 114100 ; steps/s = 1.66, tokens/s = 77637 (34227 source, 43410 target) ; Learning rate = 0.000262 ; Loss = 1.427507\n",
      "2024-12-04 05:26:08.626000: I runner.py:310] Step = 114200 ; steps/s = 1.64, tokens/s = 78327 (34487 source, 43840 target) ; Learning rate = 0.000262 ; Loss = 1.430106\n",
      "2024-12-04 05:27:08.920000: I runner.py:310] Step = 114300 ; steps/s = 1.66, tokens/s = 77668 (34260 source, 43408 target) ; Learning rate = 0.000261 ; Loss = 1.433732\n",
      "2024-12-04 05:28:09.642000: I runner.py:310] Step = 114400 ; steps/s = 1.65, tokens/s = 78464 (34549 source, 43915 target) ; Learning rate = 0.000261 ; Loss = 1.428873\n",
      "2024-12-04 05:29:10.453000: I runner.py:310] Step = 114500 ; steps/s = 1.64, tokens/s = 78345 (34495 source, 43850 target) ; Learning rate = 0.000261 ; Loss = 1.428355\n",
      "2024-12-04 05:30:10.769000: I runner.py:310] Step = 114600 ; steps/s = 1.66, tokens/s = 77599 (34211 source, 43388 target) ; Learning rate = 0.000261 ; Loss = 1.429965\n",
      "2024-12-04 05:31:11.515000: I runner.py:310] Step = 114700 ; steps/s = 1.65, tokens/s = 78459 (34559 source, 43900 target) ; Learning rate = 0.000261 ; Loss = 1.434024\n",
      "2024-12-04 05:32:11.873000: I runner.py:310] Step = 114800 ; steps/s = 1.66, tokens/s = 77569 (34208 source, 43361 target) ; Learning rate = 0.000261 ; Loss = 1.425692\n",
      "2024-12-04 05:33:12.659000: I runner.py:310] Step = 114900 ; steps/s = 1.65, tokens/s = 78416 (34540 source, 43876 target) ; Learning rate = 0.000261 ; Loss = 1.431614\n",
      "2024-12-04 05:34:13.447000: I runner.py:310] Step = 115000 ; steps/s = 1.65, tokens/s = 78357 (34495 source, 43862 target) ; Learning rate = 0.000261 ; Loss = 1.431523\n",
      "2024-12-04 05:34:13.448000: I training.py:192] Running evaluation for step 115000\n",
      "2024-12-04 05:35:00.923000: I training.py:192] Evaluation result for step 115000: loss = 0.879939 ; perplexity = 2.410753\n",
      "2024-12-04 05:36:01.074000: I runner.py:310] Step = 115100 ; steps/s = 1.66, tokens/s = 77812 (34297 source, 43515 target) ; Learning rate = 0.000261 ; Loss = 1.429948\n",
      "2024-12-04 05:37:01.917000: I runner.py:310] Step = 115200 ; steps/s = 1.64, tokens/s = 78310 (34483 source, 43827 target) ; Learning rate = 0.000260 ; Loss = 1.433156\n",
      "2024-12-04 05:38:02.747000: I runner.py:310] Step = 115300 ; steps/s = 1.64, tokens/s = 78360 (34525 source, 43835 target) ; Learning rate = 0.000260 ; Loss = 1.432732\n",
      "2024-12-04 05:39:03.125000: I runner.py:310] Step = 115400 ; steps/s = 1.66, tokens/s = 77541 (34187 source, 43354 target) ; Learning rate = 0.000260 ; Loss = 1.434118\n",
      "2024-12-04 05:40:03.882000: I runner.py:310] Step = 115500 ; steps/s = 1.65, tokens/s = 78434 (34546 source, 43888 target) ; Learning rate = 0.000260 ; Loss = 1.430689\n",
      "2024-12-04 05:41:04.224000: I runner.py:310] Step = 115600 ; steps/s = 1.66, tokens/s = 77561 (34190 source, 43371 target) ; Learning rate = 0.000260 ; Loss = 1.432203\n",
      "2024-12-04 05:42:05.061000: I runner.py:310] Step = 115700 ; steps/s = 1.64, tokens/s = 78329 (34485 source, 43844 target) ; Learning rate = 0.000260 ; Loss = 1.429753\n",
      "2024-12-04 05:43:05.865000: I runner.py:310] Step = 115800 ; steps/s = 1.64, tokens/s = 78383 (34535 source, 43848 target) ; Learning rate = 0.000260 ; Loss = 1.433099\n",
      "2024-12-04 05:44:06.257000: I runner.py:310] Step = 115900 ; steps/s = 1.66, tokens/s = 77474 (34144 source, 43330 target) ; Learning rate = 0.000260 ; Loss = 1.430325\n",
      "2024-12-04 05:45:07.059000: I runner.py:310] Step = 116000 ; steps/s = 1.64, tokens/s = 78376 (34509 source, 43867 target) ; Learning rate = 0.000260 ; Loss = 1.431586\n",
      "2024-12-04 05:46:07.348000: I runner.py:310] Step = 116100 ; steps/s = 1.66, tokens/s = 77660 (34259 source, 43401 target) ; Learning rate = 0.000259 ; Loss = 1.430668\n",
      "2024-12-04 05:47:08.165000: I runner.py:310] Step = 116200 ; steps/s = 1.64, tokens/s = 78334 (34477 source, 43857 target) ; Learning rate = 0.000259 ; Loss = 1.427633\n",
      "2024-12-04 05:48:08.898000: I runner.py:310] Step = 116300 ; steps/s = 1.65, tokens/s = 78484 (34580 source, 43904 target) ; Learning rate = 0.000259 ; Loss = 1.435037\n",
      "2024-12-04 05:49:09.182000: I runner.py:310] Step = 116400 ; steps/s = 1.66, tokens/s = 77640 (34218 source, 43422 target) ; Learning rate = 0.000259 ; Loss = 1.430002\n",
      "2024-12-04 05:50:09.964000: I runner.py:310] Step = 116500 ; steps/s = 1.65, tokens/s = 78388 (34526 source, 43862 target) ; Learning rate = 0.000259 ; Loss = 1.432763\n",
      "2024-12-04 05:51:10.779000: I runner.py:310] Step = 116600 ; steps/s = 1.64, tokens/s = 78369 (34515 source, 43854 target) ; Learning rate = 0.000259 ; Loss = 1.433652\n",
      "2024-12-04 05:52:11.093000: I runner.py:310] Step = 116700 ; steps/s = 1.66, tokens/s = 77606 (34222 source, 43384 target) ; Learning rate = 0.000259 ; Loss = 1.429444\n",
      "2024-12-04 05:53:11.847000: I runner.py:310] Step = 116800 ; steps/s = 1.65, tokens/s = 78448 (34547 source, 43901 target) ; Learning rate = 0.000259 ; Loss = 1.430708\n",
      "2024-12-04 05:54:12.255000: I runner.py:310] Step = 116900 ; steps/s = 1.66, tokens/s = 77481 (34165 source, 43316 target) ; Learning rate = 0.000259 ; Loss = 1.430807\n",
      "2024-12-04 05:55:13.065000: I runner.py:310] Step = 117000 ; steps/s = 1.64, tokens/s = 78352 (34494 source, 43858 target) ; Learning rate = 0.000258 ; Loss = 1.430392\n",
      "2024-12-04 05:56:13.874000: I runner.py:310] Step = 117100 ; steps/s = 1.64, tokens/s = 78385 (34532 source, 43853 target) ; Learning rate = 0.000258 ; Loss = 1.428176\n",
      "2024-12-04 05:57:14.151000: I runner.py:310] Step = 117200 ; steps/s = 1.66, tokens/s = 77632 (34213 source, 43419 target) ; Learning rate = 0.000258 ; Loss = 1.426039\n",
      "2024-12-04 05:58:14.951000: I runner.py:310] Step = 117300 ; steps/s = 1.64, tokens/s = 78388 (34540 source, 43848 target) ; Learning rate = 0.000258 ; Loss = 1.430189\n",
      "2024-12-04 05:59:15.332000: I runner.py:310] Step = 117400 ; steps/s = 1.66, tokens/s = 77519 (34169 source, 43350 target) ; Learning rate = 0.000258 ; Loss = 1.425018\n",
      "2024-12-04 06:00:16.177000: I runner.py:310] Step = 117500 ; steps/s = 1.64, tokens/s = 78292 (34458 source, 43834 target) ; Learning rate = 0.000258 ; Loss = 1.432357\n",
      "2024-12-04 06:01:16.914000: I runner.py:310] Step = 117600 ; steps/s = 1.65, tokens/s = 78474 (34570 source, 43904 target) ; Learning rate = 0.000258 ; Loss = 1.434963\n",
      "2024-12-04 06:02:17.221000: I runner.py:310] Step = 117700 ; steps/s = 1.66, tokens/s = 77617 (34226 source, 43391 target) ; Learning rate = 0.000258 ; Loss = 1.427990\n",
      "2024-12-04 06:03:18.031000: I runner.py:310] Step = 117800 ; steps/s = 1.64, tokens/s = 78362 (34505 source, 43857 target) ; Learning rate = 0.000258 ; Loss = 1.424684\n",
      "2024-12-04 06:04:18.836000: I runner.py:310] Step = 117900 ; steps/s = 1.64, tokens/s = 78388 (34531 source, 43857 target) ; Learning rate = 0.000257 ; Loss = 1.428924\n",
      "2024-12-04 06:05:19.125000: I runner.py:310] Step = 118000 ; steps/s = 1.66, tokens/s = 77659 (34246 source, 43413 target) ; Learning rate = 0.000257 ; Loss = 1.425592\n",
      "2024-12-04 06:06:19.911000: I runner.py:310] Step = 118100 ; steps/s = 1.65, tokens/s = 78393 (34514 source, 43879 target) ; Learning rate = 0.000257 ; Loss = 1.436687\n",
      "2024-12-04 06:07:20.194000: I runner.py:310] Step = 118200 ; steps/s = 1.66, tokens/s = 77619 (34221 source, 43398 target) ; Learning rate = 0.000257 ; Loss = 1.425402\n",
      "2024-12-04 06:08:20.946000: I runner.py:310] Step = 118300 ; steps/s = 1.65, tokens/s = 78422 (34517 source, 43905 target) ; Learning rate = 0.000257 ; Loss = 1.430555\n",
      "2024-12-04 06:09:21.758000: I runner.py:310] Step = 118400 ; steps/s = 1.64, tokens/s = 78391 (34540 source, 43851 target) ; Learning rate = 0.000257 ; Loss = 1.433224\n",
      "2024-12-04 06:10:22.016000: I runner.py:310] Step = 118500 ; steps/s = 1.66, tokens/s = 77681 (34243 source, 43438 target) ; Learning rate = 0.000257 ; Loss = 1.429692\n",
      "2024-12-04 06:11:22.775000: I runner.py:310] Step = 118600 ; steps/s = 1.65, tokens/s = 78425 (34559 source, 43866 target) ; Learning rate = 0.000257 ; Loss = 1.433147\n",
      "2024-12-04 06:12:23.104000: I runner.py:310] Step = 118700 ; steps/s = 1.66, tokens/s = 77605 (34207 source, 43398 target) ; Learning rate = 0.000257 ; Loss = 1.425333\n",
      "2024-12-04 06:13:23.902000: I runner.py:310] Step = 118800 ; steps/s = 1.65, tokens/s = 78370 (34512 source, 43858 target) ; Learning rate = 0.000256 ; Loss = 1.429924\n",
      "2024-12-04 06:14:24.666000: I runner.py:310] Step = 118900 ; steps/s = 1.65, tokens/s = 78427 (34546 source, 43881 target) ; Learning rate = 0.000256 ; Loss = 1.431334\n",
      "2024-12-04 06:15:24.955000: I runner.py:310] Step = 119000 ; steps/s = 1.66, tokens/s = 77643 (34220 source, 43423 target) ; Learning rate = 0.000256 ; Loss = 1.432417\n",
      "2024-12-04 06:16:25.786000: I runner.py:310] Step = 119100 ; steps/s = 1.64, tokens/s = 78338 (34496 source, 43842 target) ; Learning rate = 0.000256 ; Loss = 1.429313\n",
      "2024-12-04 06:17:26.575000: I runner.py:310] Step = 119200 ; steps/s = 1.65, tokens/s = 78394 (34537 source, 43857 target) ; Learning rate = 0.000256 ; Loss = 1.428435\n",
      "2024-12-04 06:18:26.929000: I runner.py:310] Step = 119300 ; steps/s = 1.66, tokens/s = 77555 (34189 source, 43366 target) ; Learning rate = 0.000256 ; Loss = 1.428518\n",
      "2024-12-04 06:19:27.704000: I runner.py:310] Step = 119400 ; steps/s = 1.65, tokens/s = 78407 (34537 source, 43870 target) ; Learning rate = 0.000256 ; Loss = 1.439010\n",
      "2024-12-04 06:20:28.058000: I runner.py:310] Step = 119500 ; steps/s = 1.66, tokens/s = 77537 (34178 source, 43359 target) ; Learning rate = 0.000256 ; Loss = 1.429911\n",
      "2024-12-04 06:21:28.792000: I runner.py:310] Step = 119600 ; steps/s = 1.65, tokens/s = 78466 (34547 source, 43919 target) ; Learning rate = 0.000256 ; Loss = 1.430948\n",
      "2024-12-04 06:22:29.565000: I runner.py:310] Step = 119700 ; steps/s = 1.65, tokens/s = 78418 (34540 source, 43878 target) ; Learning rate = 0.000255 ; Loss = 1.427886\n",
      "2024-12-04 06:23:29.893000: I runner.py:310] Step = 119800 ; steps/s = 1.66, tokens/s = 77594 (34213 source, 43381 target) ; Learning rate = 0.000255 ; Loss = 1.429940\n",
      "2024-12-04 06:24:30.699000: I runner.py:310] Step = 119900 ; steps/s = 1.64, tokens/s = 78379 (34513 source, 43866 target) ; Learning rate = 0.000255 ; Loss = 1.431447\n",
      "2024-12-04 06:25:31.017000: I runner.py:310] Step = 120000 ; steps/s = 1.66, tokens/s = 77596 (34223 source, 43373 target) ; Learning rate = 0.000255 ; Loss = 1.435669\n",
      "2024-12-04 06:25:32.619000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-120000\n",
      "2024-12-04 06:25:32.619000: I training.py:192] Running evaluation for step 120000\n",
      "2024-12-04 06:26:19.303000: I training.py:192] Evaluation result for step 120000: loss = 0.891610 ; perplexity = 2.439053\n",
      "2024-12-04 06:27:19.990000: I runner.py:310] Step = 120100 ; steps/s = 1.65, tokens/s = 78513 (34559 source, 43954 target) ; Learning rate = 0.000255 ; Loss = 1.431247\n",
      "2024-12-04 06:28:20.725000: I runner.py:310] Step = 120200 ; steps/s = 1.65, tokens/s = 78477 (34567 source, 43910 target) ; Learning rate = 0.000255 ; Loss = 1.428737\n",
      "2024-12-04 06:29:21.054000: I runner.py:310] Step = 120300 ; steps/s = 1.66, tokens/s = 77594 (34221 source, 43373 target) ; Learning rate = 0.000255 ; Loss = 1.425255\n",
      "2024-12-04 06:30:21.874000: I runner.py:310] Step = 120400 ; steps/s = 1.64, tokens/s = 78360 (34505 source, 43855 target) ; Learning rate = 0.000255 ; Loss = 1.432195\n",
      "2024-12-04 06:31:22.749000: I runner.py:310] Step = 120500 ; steps/s = 1.64, tokens/s = 78272 (34467 source, 43805 target) ; Learning rate = 0.000255 ; Loss = 1.434663\n",
      "2024-12-04 06:32:23.058000: I runner.py:310] Step = 120600 ; steps/s = 1.66, tokens/s = 77603 (34211 source, 43392 target) ; Learning rate = 0.000255 ; Loss = 1.427992\n",
      "2024-12-04 06:33:23.862000: I runner.py:310] Step = 120700 ; steps/s = 1.64, tokens/s = 78369 (34506 source, 43863 target) ; Learning rate = 0.000254 ; Loss = 1.431255\n",
      "2024-12-04 06:34:24.279000: I runner.py:310] Step = 120800 ; steps/s = 1.66, tokens/s = 77494 (34179 source, 43315 target) ; Learning rate = 0.000254 ; Loss = 1.429608\n",
      "2024-12-04 06:35:25.031000: I runner.py:310] Step = 120900 ; steps/s = 1.65, tokens/s = 78457 (34560 source, 43897 target) ; Learning rate = 0.000254 ; Loss = 1.432796\n",
      "2024-12-04 06:36:25.778000: I runner.py:310] Step = 121000 ; steps/s = 1.65, tokens/s = 78441 (34544 source, 43897 target) ; Learning rate = 0.000254 ; Loss = 1.430530\n",
      "2024-12-04 06:37:26.087000: I runner.py:310] Step = 121100 ; steps/s = 1.66, tokens/s = 77588 (34183 source, 43405 target) ; Learning rate = 0.000254 ; Loss = 1.433007\n",
      "2024-12-04 06:38:26.907000: I runner.py:310] Step = 121200 ; steps/s = 1.64, tokens/s = 78356 (34518 source, 43838 target) ; Learning rate = 0.000254 ; Loss = 1.428790\n",
      "2024-12-04 06:39:27.251000: I runner.py:310] Step = 121300 ; steps/s = 1.66, tokens/s = 77582 (34212 source, 43370 target) ; Learning rate = 0.000254 ; Loss = 1.427459\n",
      "2024-12-04 06:40:28.020000: I runner.py:310] Step = 121400 ; steps/s = 1.65, tokens/s = 78405 (34511 source, 43894 target) ; Learning rate = 0.000254 ; Loss = 1.426996\n",
      "2024-12-04 06:41:28.863000: I runner.py:310] Step = 121500 ; steps/s = 1.64, tokens/s = 78329 (34510 source, 43819 target) ; Learning rate = 0.000254 ; Loss = 1.434764\n",
      "2024-12-04 06:42:29.184000: I runner.py:310] Step = 121600 ; steps/s = 1.66, tokens/s = 77588 (34205 source, 43383 target) ; Learning rate = 0.000253 ; Loss = 1.430460\n",
      "2024-12-04 06:43:29.951000: I runner.py:310] Step = 121700 ; steps/s = 1.65, tokens/s = 78403 (34521 source, 43882 target) ; Learning rate = 0.000253 ; Loss = 1.427931\n",
      "2024-12-04 06:44:30.796000: I runner.py:310] Step = 121800 ; steps/s = 1.64, tokens/s = 78344 (34511 source, 43833 target) ; Learning rate = 0.000253 ; Loss = 1.428807\n",
      "2024-12-04 06:45:31.117000: I runner.py:310] Step = 121900 ; steps/s = 1.66, tokens/s = 77614 (34227 source, 43387 target) ; Learning rate = 0.000253 ; Loss = 1.432013\n",
      "2024-12-04 06:46:31.970000: I runner.py:310] Step = 122000 ; steps/s = 1.64, tokens/s = 78323 (34498 source, 43825 target) ; Learning rate = 0.000253 ; Loss = 1.433143\n",
      "2024-12-04 06:47:32.253000: I runner.py:310] Step = 122100 ; steps/s = 1.66, tokens/s = 77620 (34207 source, 43413 target) ; Learning rate = 0.000253 ; Loss = 1.424516\n",
      "2024-12-04 06:48:33.051000: I runner.py:310] Step = 122200 ; steps/s = 1.65, tokens/s = 78384 (34525 source, 43859 target) ; Learning rate = 0.000253 ; Loss = 1.431000\n",
      "2024-12-04 06:49:33.870000: I runner.py:310] Step = 122300 ; steps/s = 1.64, tokens/s = 78343 (34496 source, 43847 target) ; Learning rate = 0.000253 ; Loss = 1.431309\n",
      "2024-12-04 06:50:34.299000: I runner.py:310] Step = 122400 ; steps/s = 1.65, tokens/s = 77463 (34155 source, 43308 target) ; Learning rate = 0.000253 ; Loss = 1.428205\n",
      "2024-12-04 06:51:35.041000: I runner.py:310] Step = 122500 ; steps/s = 1.65, tokens/s = 78430 (34527 source, 43903 target) ; Learning rate = 0.000253 ; Loss = 1.429639\n",
      "2024-12-04 06:52:35.435000: I runner.py:310] Step = 122600 ; steps/s = 1.66, tokens/s = 77541 (34205 source, 43336 target) ; Learning rate = 0.000252 ; Loss = 1.425481\n",
      "2024-12-04 06:53:36.188000: I runner.py:310] Step = 122700 ; steps/s = 1.65, tokens/s = 78450 (34541 source, 43909 target) ; Learning rate = 0.000252 ; Loss = 1.429950\n",
      "2024-12-04 06:54:37.061000: I runner.py:310] Step = 122800 ; steps/s = 1.64, tokens/s = 78263 (34464 source, 43799 target) ; Learning rate = 0.000252 ; Loss = 1.428219\n",
      "2024-12-04 06:55:37.385000: I runner.py:310] Step = 122900 ; steps/s = 1.66, tokens/s = 77599 (34214 source, 43385 target) ; Learning rate = 0.000252 ; Loss = 1.433983\n",
      "2024-12-04 06:56:38.154000: I runner.py:310] Step = 123000 ; steps/s = 1.65, tokens/s = 78411 (34536 source, 43875 target) ; Learning rate = 0.000252 ; Loss = 1.430440\n",
      "2024-12-04 06:57:38.939000: I runner.py:310] Step = 123100 ; steps/s = 1.65, tokens/s = 78405 (34528 source, 43877 target) ; Learning rate = 0.000252 ; Loss = 1.430431\n",
      "2024-12-04 06:58:39.237000: I runner.py:310] Step = 123200 ; steps/s = 1.66, tokens/s = 77618 (34211 source, 43407 target) ; Learning rate = 0.000252 ; Loss = 1.429170\n",
      "2024-12-04 06:59:39.995000: I runner.py:310] Step = 123300 ; steps/s = 1.65, tokens/s = 78429 (34539 source, 43890 target) ; Learning rate = 0.000252 ; Loss = 1.432342\n",
      "2024-12-04 07:00:40.295000: I runner.py:310] Step = 123400 ; steps/s = 1.66, tokens/s = 77635 (34240 source, 43395 target) ; Learning rate = 0.000252 ; Loss = 1.425086\n",
      "2024-12-04 07:01:41.080000: I runner.py:310] Step = 123500 ; steps/s = 1.65, tokens/s = 78390 (34520 source, 43870 target) ; Learning rate = 0.000252 ; Loss = 1.427887\n",
      "2024-12-04 07:02:41.916000: I runner.py:310] Step = 123600 ; steps/s = 1.64, tokens/s = 78340 (34505 source, 43835 target) ; Learning rate = 0.000251 ; Loss = 1.431489\n",
      "2024-12-04 07:03:42.289000: I runner.py:310] Step = 123700 ; steps/s = 1.66, tokens/s = 77557 (34206 source, 43351 target) ; Learning rate = 0.000251 ; Loss = 1.427940\n",
      "2024-12-04 07:04:43.052000: I runner.py:310] Step = 123800 ; steps/s = 1.65, tokens/s = 78401 (34505 source, 43896 target) ; Learning rate = 0.000251 ; Loss = 1.427998\n",
      "2024-12-04 07:05:43.375000: I runner.py:310] Step = 123900 ; steps/s = 1.66, tokens/s = 77591 (34218 source, 43373 target) ; Learning rate = 0.000251 ; Loss = 1.425636\n",
      "2024-12-04 07:06:44.177000: I runner.py:310] Step = 124000 ; steps/s = 1.65, tokens/s = 78364 (34501 source, 43863 target) ; Learning rate = 0.000251 ; Loss = 1.427549\n",
      "2024-12-04 07:07:44.944000: I runner.py:310] Step = 124100 ; steps/s = 1.65, tokens/s = 78440 (34562 source, 43878 target) ; Learning rate = 0.000251 ; Loss = 1.433066\n",
      "2024-12-04 07:08:45.248000: I runner.py:310] Step = 124200 ; steps/s = 1.66, tokens/s = 77591 (34183 source, 43408 target) ; Learning rate = 0.000251 ; Loss = 1.431711\n",
      "2024-12-04 07:09:45.995000: I runner.py:310] Step = 124300 ; steps/s = 1.65, tokens/s = 78469 (34573 source, 43896 target) ; Learning rate = 0.000251 ; Loss = 1.429944\n",
      "2024-12-04 07:10:46.744000: I runner.py:310] Step = 124400 ; steps/s = 1.65, tokens/s = 78376 (34519 source, 43857 target) ; Learning rate = 0.000251 ; Loss = 1.443094\n",
      "2024-12-04 07:11:47.098000: I runner.py:310] Step = 124500 ; steps/s = 1.66, tokens/s = 77662 (34247 source, 43415 target) ; Learning rate = 0.000251 ; Loss = 1.431566\n",
      "2024-12-04 07:12:47.853000: I runner.py:310] Step = 124600 ; steps/s = 1.65, tokens/s = 78425 (34543 source, 43882 target) ; Learning rate = 0.000250 ; Loss = 1.427055\n",
      "2024-12-04 07:13:48.146000: I runner.py:310] Step = 124700 ; steps/s = 1.66, tokens/s = 77614 (34206 source, 43408 target) ; Learning rate = 0.000250 ; Loss = 1.428124\n",
      "2024-12-04 07:14:48.944000: I runner.py:310] Step = 124800 ; steps/s = 1.64, tokens/s = 78364 (34502 source, 43862 target) ; Learning rate = 0.000250 ; Loss = 1.431016\n",
      "2024-12-04 07:15:49.697000: I runner.py:310] Step = 124900 ; steps/s = 1.65, tokens/s = 78441 (34544 source, 43897 target) ; Learning rate = 0.000250 ; Loss = 1.430082\n",
      "2024-12-04 07:16:50.039000: I runner.py:310] Step = 125000 ; steps/s = 1.66, tokens/s = 77585 (34213 source, 43372 target) ; Learning rate = 0.000250 ; Loss = 1.430237\n",
      "2024-12-04 07:16:50.041000: I training.py:192] Running evaluation for step 125000\n",
      "2024-12-04 07:17:36.821000: I training.py:192] Evaluation result for step 125000: loss = 0.891375 ; perplexity = 2.438480\n",
      "2024-12-04 07:18:37.446000: I runner.py:310] Step = 125100 ; steps/s = 1.65, tokens/s = 78599 (34600 source, 43999 target) ; Learning rate = 0.000250 ; Loss = 1.429338\n",
      "2024-12-04 07:19:37.717000: I runner.py:310] Step = 125200 ; steps/s = 1.66, tokens/s = 77683 (34266 source, 43417 target) ; Learning rate = 0.000250 ; Loss = 1.427596\n",
      "2024-12-04 07:20:38.588000: I runner.py:310] Step = 125300 ; steps/s = 1.64, tokens/s = 78265 (34451 source, 43814 target) ; Learning rate = 0.000250 ; Loss = 1.428080\n",
      "2024-12-04 07:21:39.386000: I runner.py:310] Step = 125400 ; steps/s = 1.65, tokens/s = 78418 (34556 source, 43862 target) ; Learning rate = 0.000250 ; Loss = 1.429306\n",
      "2024-12-04 07:22:39.670000: I runner.py:310] Step = 125500 ; steps/s = 1.66, tokens/s = 77634 (34221 source, 43413 target) ; Learning rate = 0.000250 ; Loss = 1.433983\n",
      "2024-12-04 07:23:40.554000: I runner.py:310] Step = 125600 ; steps/s = 1.64, tokens/s = 78257 (34446 source, 43811 target) ; Learning rate = 0.000249 ; Loss = 1.429778\n",
      "2024-12-04 07:24:41.207000: I runner.py:310] Step = 125700 ; steps/s = 1.65, tokens/s = 78186 (34484 source, 43702 target) ; Learning rate = 0.000249 ; Loss = 1.458886\n",
      "2024-12-04 07:25:41.676000: I runner.py:310] Step = 125800 ; steps/s = 1.65, tokens/s = 77787 (34261 source, 43526 target) ; Learning rate = 0.000249 ; Loss = 1.428555\n",
      "2024-12-04 07:26:42.471000: I runner.py:310] Step = 125900 ; steps/s = 1.65, tokens/s = 78388 (34525 source, 43863 target) ; Learning rate = 0.000249 ; Loss = 1.428429\n",
      "2024-12-04 07:27:42.871000: I runner.py:310] Step = 126000 ; steps/s = 1.66, tokens/s = 77491 (34160 source, 43331 target) ; Learning rate = 0.000249 ; Loss = 1.428636\n",
      "2024-12-04 07:28:43.683000: I runner.py:310] Step = 126100 ; steps/s = 1.64, tokens/s = 78339 (34492 source, 43847 target) ; Learning rate = 0.000249 ; Loss = 1.428619\n",
      "2024-12-04 07:29:44.481000: I runner.py:310] Step = 126200 ; steps/s = 1.65, tokens/s = 78408 (34539 source, 43869 target) ; Learning rate = 0.000249 ; Loss = 1.432463\n",
      "2024-12-04 07:30:44.752000: I runner.py:310] Step = 126300 ; steps/s = 1.66, tokens/s = 77634 (34213 source, 43421 target) ; Learning rate = 0.000249 ; Loss = 1.430519\n",
      "2024-12-04 07:31:45.585000: I runner.py:310] Step = 126400 ; steps/s = 1.64, tokens/s = 78348 (34522 source, 43826 target) ; Learning rate = 0.000249 ; Loss = 1.431065\n",
      "2024-12-04 07:32:45.908000: I runner.py:310] Step = 126500 ; steps/s = 1.66, tokens/s = 77602 (34209 source, 43393 target) ; Learning rate = 0.000249 ; Loss = 1.426850\n",
      "2024-12-04 07:33:46.708000: I runner.py:310] Step = 126600 ; steps/s = 1.64, tokens/s = 78361 (34509 source, 43852 target) ; Learning rate = 0.000248 ; Loss = 1.431602\n",
      "2024-12-04 07:34:47.554000: I runner.py:310] Step = 126700 ; steps/s = 1.64, tokens/s = 78357 (34521 source, 43836 target) ; Learning rate = 0.000248 ; Loss = 1.428167\n",
      "2024-12-04 07:35:47.833000: I runner.py:310] Step = 126800 ; steps/s = 1.66, tokens/s = 77609 (34190 source, 43419 target) ; Learning rate = 0.000248 ; Loss = 1.429051\n",
      "2024-12-04 07:36:48.672000: I runner.py:310] Step = 126900 ; steps/s = 1.64, tokens/s = 78341 (34506 source, 43835 target) ; Learning rate = 0.000248 ; Loss = 1.431141\n",
      "2024-12-04 07:37:49.039000: I runner.py:310] Step = 127000 ; steps/s = 1.66, tokens/s = 77797 (34327 source, 43470 target) ; Learning rate = 0.000248 ; Loss = 1.439216\n",
      "2024-12-04 07:38:49.750000: I runner.py:310] Step = 127100 ; steps/s = 1.65, tokens/s = 78266 (34467 source, 43799 target) ; Learning rate = 0.000248 ; Loss = 1.424253\n",
      "2024-12-04 07:39:50.583000: I runner.py:310] Step = 127200 ; steps/s = 1.64, tokens/s = 78316 (34483 source, 43833 target) ; Learning rate = 0.000248 ; Loss = 1.427484\n",
      "2024-12-04 07:40:50.919000: I runner.py:310] Step = 127300 ; steps/s = 1.66, tokens/s = 77583 (34200 source, 43383 target) ; Learning rate = 0.000248 ; Loss = 1.429987\n",
      "2024-12-04 07:41:51.727000: I runner.py:310] Step = 127400 ; steps/s = 1.64, tokens/s = 78373 (34519 source, 43854 target) ; Learning rate = 0.000248 ; Loss = 1.431599\n",
      "2024-12-04 07:42:52.528000: I runner.py:310] Step = 127500 ; steps/s = 1.64, tokens/s = 78375 (34511 source, 43864 target) ; Learning rate = 0.000248 ; Loss = 1.431348\n",
      "2024-12-04 07:43:52.878000: I runner.py:310] Step = 127600 ; steps/s = 1.66, tokens/s = 77553 (34186 source, 43367 target) ; Learning rate = 0.000247 ; Loss = 1.426757\n",
      "2024-12-04 07:44:53.710000: I runner.py:310] Step = 127700 ; steps/s = 1.64, tokens/s = 78340 (34495 source, 43845 target) ; Learning rate = 0.000247 ; Loss = 1.427934\n",
      "2024-12-04 07:45:54.063000: I runner.py:310] Step = 127800 ; steps/s = 1.66, tokens/s = 77577 (34221 source, 43356 target) ; Learning rate = 0.000247 ; Loss = 1.428180\n",
      "2024-12-04 07:46:54.843000: I runner.py:310] Step = 127900 ; steps/s = 1.65, tokens/s = 78377 (34506 source, 43871 target) ; Learning rate = 0.000247 ; Loss = 1.426258\n",
      "2024-12-04 07:47:55.633000: I runner.py:310] Step = 128000 ; steps/s = 1.65, tokens/s = 78392 (34526 source, 43866 target) ; Learning rate = 0.000247 ; Loss = 1.431386\n",
      "2024-12-04 07:48:55.939000: I runner.py:310] Step = 128100 ; steps/s = 1.66, tokens/s = 77636 (34237 source, 43399 target) ; Learning rate = 0.000247 ; Loss = 1.433825\n",
      "2024-12-04 07:49:56.721000: I runner.py:310] Step = 128200 ; steps/s = 1.65, tokens/s = 78372 (34497 source, 43875 target) ; Learning rate = 0.000247 ; Loss = 1.428045\n",
      "2024-12-04 07:50:57.099000: I runner.py:310] Step = 128300 ; steps/s = 1.66, tokens/s = 77566 (34221 source, 43345 target) ; Learning rate = 0.000247 ; Loss = 1.429175\n",
      "2024-12-04 07:51:57.852000: I runner.py:310] Step = 128400 ; steps/s = 1.65, tokens/s = 78405 (34516 source, 43889 target) ; Learning rate = 0.000247 ; Loss = 1.422220\n",
      "2024-12-04 07:52:58.651000: I runner.py:310] Step = 128500 ; steps/s = 1.64, tokens/s = 78381 (34518 source, 43863 target) ; Learning rate = 0.000247 ; Loss = 1.432518\n",
      "2024-12-04 07:53:59.018000: I runner.py:310] Step = 128600 ; steps/s = 1.66, tokens/s = 77538 (34177 source, 43361 target) ; Learning rate = 0.000246 ; Loss = 1.428899\n",
      "2024-12-04 07:54:59.833000: I runner.py:310] Step = 128700 ; steps/s = 1.64, tokens/s = 78342 (34499 source, 43843 target) ; Learning rate = 0.000246 ; Loss = 1.440633\n",
      "2024-12-04 07:56:00.588000: I runner.py:310] Step = 128800 ; steps/s = 1.65, tokens/s = 78454 (34564 source, 43890 target) ; Learning rate = 0.000246 ; Loss = 1.433795\n",
      "2024-12-04 07:57:00.938000: I runner.py:310] Step = 128900 ; steps/s = 1.66, tokens/s = 77546 (34179 source, 43367 target) ; Learning rate = 0.000246 ; Loss = 1.430529\n",
      "2024-12-04 07:58:01.719000: I runner.py:310] Step = 129000 ; steps/s = 1.65, tokens/s = 78394 (34527 source, 43867 target) ; Learning rate = 0.000246 ; Loss = 1.431114\n",
      "2024-12-04 07:59:02.164000: I runner.py:310] Step = 129100 ; steps/s = 1.65, tokens/s = 77466 (34162 source, 43304 target) ; Learning rate = 0.000246 ; Loss = 1.428893\n",
      "2024-12-04 08:00:02.955000: I runner.py:310] Step = 129200 ; steps/s = 1.65, tokens/s = 78400 (34522 source, 43878 target) ; Learning rate = 0.000246 ; Loss = 1.429498\n",
      "2024-12-04 08:01:03.764000: I runner.py:310] Step = 129300 ; steps/s = 1.64, tokens/s = 78366 (34520 source, 43846 target) ; Learning rate = 0.000246 ; Loss = 1.430443\n",
      "2024-12-04 08:02:04.062000: I runner.py:310] Step = 129400 ; steps/s = 1.66, tokens/s = 77593 (34188 source, 43405 target) ; Learning rate = 0.000246 ; Loss = 1.429982\n",
      "2024-12-04 08:03:04.853000: I runner.py:310] Step = 129500 ; steps/s = 1.65, tokens/s = 78402 (34542 source, 43860 target) ; Learning rate = 0.000246 ; Loss = 1.426032\n",
      "2024-12-04 08:04:05.235000: I runner.py:310] Step = 129600 ; steps/s = 1.66, tokens/s = 77548 (34201 source, 43347 target) ; Learning rate = 0.000246 ; Loss = 1.427531\n",
      "2024-12-04 08:05:06.015000: I runner.py:310] Step = 129700 ; steps/s = 1.65, tokens/s = 78364 (34502 source, 43862 target) ; Learning rate = 0.000245 ; Loss = 1.423375\n",
      "2024-12-04 08:06:06.816000: I runner.py:310] Step = 129800 ; steps/s = 1.64, tokens/s = 78388 (34524 source, 43864 target) ; Learning rate = 0.000245 ; Loss = 1.428893\n",
      "2024-12-04 08:07:07.135000: I runner.py:310] Step = 129900 ; steps/s = 1.66, tokens/s = 77606 (34207 source, 43399 target) ; Learning rate = 0.000245 ; Loss = 1.426699\n",
      "2024-12-04 08:08:07.934000: I runner.py:310] Step = 130000 ; steps/s = 1.65, tokens/s = 78396 (34525 source, 43871 target) ; Learning rate = 0.000245 ; Loss = 1.433182\n",
      "2024-12-04 08:08:09.710000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-130000\n",
      "2024-12-04 08:08:09.710000: I training.py:192] Running evaluation for step 130000\n",
      "2024-12-04 08:08:56.693000: I training.py:192] Evaluation result for step 130000: loss = 0.895261 ; perplexity = 2.447975\n",
      "2024-12-04 08:09:57.393000: I runner.py:310] Step = 130100 ; steps/s = 1.65, tokens/s = 78517 (34584 source, 43933 target) ; Learning rate = 0.000245 ; Loss = 1.427166\n",
      "2024-12-04 08:10:57.710000: I runner.py:310] Step = 130200 ; steps/s = 1.66, tokens/s = 77620 (34227 source, 43393 target) ; Learning rate = 0.000245 ; Loss = 1.425054\n",
      "2024-12-04 08:11:58.562000: I runner.py:310] Step = 130300 ; steps/s = 1.64, tokens/s = 78286 (34466 source, 43820 target) ; Learning rate = 0.000245 ; Loss = 1.429853\n",
      "2024-12-04 08:12:58.918000: I runner.py:310] Step = 130400 ; steps/s = 1.66, tokens/s = 77569 (34215 source, 43354 target) ; Learning rate = 0.000245 ; Loss = 1.424833\n",
      "2024-12-04 08:13:59.647000: I runner.py:310] Step = 130500 ; steps/s = 1.65, tokens/s = 78485 (34564 source, 43921 target) ; Learning rate = 0.000245 ; Loss = 1.429427\n",
      "2024-12-04 08:15:00.361000: I runner.py:310] Step = 130600 ; steps/s = 1.65, tokens/s = 78468 (34550 source, 43918 target) ; Learning rate = 0.000245 ; Loss = 1.422973\n",
      "2024-12-04 08:16:00.676000: I runner.py:310] Step = 130700 ; steps/s = 1.66, tokens/s = 77582 (34188 source, 43394 target) ; Learning rate = 0.000244 ; Loss = 1.427065\n",
      "2024-12-04 08:17:01.420000: I runner.py:310] Step = 130800 ; steps/s = 1.65, tokens/s = 78476 (34582 source, 43894 target) ; Learning rate = 0.000244 ; Loss = 1.429609\n",
      "2024-12-04 08:18:01.732000: I runner.py:310] Step = 130900 ; steps/s = 1.66, tokens/s = 77618 (34225 source, 43393 target) ; Learning rate = 0.000244 ; Loss = 1.425600\n",
      "2024-12-04 08:19:02.463000: I runner.py:310] Step = 131000 ; steps/s = 1.65, tokens/s = 78447 (34529 source, 43918 target) ; Learning rate = 0.000244 ; Loss = 1.427003\n",
      "2024-12-04 08:20:03.183000: I runner.py:310] Step = 131100 ; steps/s = 1.65, tokens/s = 78494 (34581 source, 43913 target) ; Learning rate = 0.000244 ; Loss = 1.430290\n",
      "2024-12-04 08:21:03.505000: I runner.py:310] Step = 131200 ; steps/s = 1.66, tokens/s = 77596 (34205 source, 43391 target) ; Learning rate = 0.000244 ; Loss = 1.428507\n",
      "2024-12-04 08:22:04.268000: I runner.py:310] Step = 131300 ; steps/s = 1.65, tokens/s = 78434 (34554 source, 43880 target) ; Learning rate = 0.000244 ; Loss = 1.429481\n",
      "2024-12-04 08:23:05.015000: I runner.py:310] Step = 131400 ; steps/s = 1.65, tokens/s = 78421 (34522 source, 43899 target) ; Learning rate = 0.000244 ; Loss = 1.429202\n",
      "2024-12-04 08:24:05.337000: I runner.py:310] Step = 131500 ; steps/s = 1.66, tokens/s = 77613 (34222 source, 43391 target) ; Learning rate = 0.000244 ; Loss = 1.427956\n",
      "2024-12-04 08:25:06.105000: I runner.py:310] Step = 131600 ; steps/s = 1.65, tokens/s = 78408 (34529 source, 43879 target) ; Learning rate = 0.000244 ; Loss = 1.422286\n",
      "2024-12-04 08:26:06.368000: I runner.py:310] Step = 131700 ; steps/s = 1.66, tokens/s = 77666 (34249 source, 43417 target) ; Learning rate = 0.000244 ; Loss = 1.425085\n",
      "2024-12-04 08:27:07.122000: I runner.py:310] Step = 131800 ; steps/s = 1.65, tokens/s = 78393 (34502 source, 43891 target) ; Learning rate = 0.000243 ; Loss = 1.424336\n",
      "2024-12-04 08:28:07.878000: I runner.py:310] Step = 131900 ; steps/s = 1.65, tokens/s = 78474 (34575 source, 43899 target) ; Learning rate = 0.000243 ; Loss = 1.430358\n",
      "2024-12-04 08:29:08.210000: I runner.py:310] Step = 132000 ; steps/s = 1.66, tokens/s = 77582 (34203 source, 43379 target) ; Learning rate = 0.000243 ; Loss = 1.422423\n",
      "2024-12-04 08:30:08.984000: I runner.py:310] Step = 132100 ; steps/s = 1.65, tokens/s = 78434 (34551 source, 43883 target) ; Learning rate = 0.000243 ; Loss = 1.426091\n",
      "2024-12-04 08:31:09.276000: I runner.py:310] Step = 132200 ; steps/s = 1.66, tokens/s = 77624 (34221 source, 43403 target) ; Learning rate = 0.000243 ; Loss = 1.424125\n",
      "2024-12-04 08:32:09.979000: I runner.py:310] Step = 132300 ; steps/s = 1.65, tokens/s = 78516 (34579 source, 43937 target) ; Learning rate = 0.000243 ; Loss = 1.429229\n",
      "2024-12-04 08:33:10.688000: I runner.py:310] Step = 132400 ; steps/s = 1.65, tokens/s = 78482 (34559 source, 43923 target) ; Learning rate = 0.000243 ; Loss = 1.430088\n",
      "2024-12-04 08:34:11.032000: I runner.py:310] Step = 132500 ; steps/s = 1.66, tokens/s = 77547 (34182 source, 43365 target) ; Learning rate = 0.000243 ; Loss = 1.422727\n",
      "2024-12-04 08:35:11.802000: I runner.py:310] Step = 132600 ; steps/s = 1.65, tokens/s = 78430 (34549 source, 43881 target) ; Learning rate = 0.000243 ; Loss = 1.428018\n",
      "2024-12-04 08:36:12.539000: I runner.py:310] Step = 132700 ; steps/s = 1.65, tokens/s = 78455 (34545 source, 43910 target) ; Learning rate = 0.000243 ; Loss = 1.429868\n",
      "2024-12-04 08:37:12.825000: I runner.py:310] Step = 132800 ; steps/s = 1.66, tokens/s = 77629 (34224 source, 43405 target) ; Learning rate = 0.000243 ; Loss = 1.425686\n",
      "2024-12-04 08:38:13.608000: I runner.py:310] Step = 132900 ; steps/s = 1.65, tokens/s = 78407 (34536 source, 43871 target) ; Learning rate = 0.000242 ; Loss = 1.429667\n",
      "2024-12-04 08:39:13.933000: I runner.py:310] Step = 133000 ; steps/s = 1.66, tokens/s = 77596 (34211 source, 43385 target) ; Learning rate = 0.000242 ; Loss = 1.427979\n",
      "2024-12-04 08:40:14.731000: I runner.py:310] Step = 133100 ; steps/s = 1.65, tokens/s = 78370 (34522 source, 43848 target) ; Learning rate = 0.000242 ; Loss = 1.428693\n",
      "2024-12-04 08:41:15.491000: I runner.py:310] Step = 133200 ; steps/s = 1.65, tokens/s = 78431 (34525 source, 43906 target) ; Learning rate = 0.000242 ; Loss = 1.430578\n",
      "2024-12-04 08:42:15.826000: I runner.py:310] Step = 133300 ; steps/s = 1.66, tokens/s = 77600 (34212 source, 43388 target) ; Learning rate = 0.000242 ; Loss = 1.437064\n",
      "2024-12-04 08:43:16.537000: I runner.py:310] Step = 133400 ; steps/s = 1.65, tokens/s = 78504 (34586 source, 43918 target) ; Learning rate = 0.000242 ; Loss = 1.426228\n",
      "2024-12-04 08:44:16.859000: I runner.py:310] Step = 133500 ; steps/s = 1.66, tokens/s = 77585 (34201 source, 43384 target) ; Learning rate = 0.000242 ; Loss = 1.422432\n",
      "2024-12-04 08:45:17.606000: I runner.py:310] Step = 133600 ; steps/s = 1.65, tokens/s = 78417 (34517 source, 43900 target) ; Learning rate = 0.000242 ; Loss = 1.423181\n",
      "2024-12-04 08:46:18.409000: I runner.py:310] Step = 133700 ; steps/s = 1.64, tokens/s = 78381 (34528 source, 43853 target) ; Learning rate = 0.000242 ; Loss = 1.432470\n",
      "2024-12-04 08:47:18.736000: I runner.py:310] Step = 133800 ; steps/s = 1.66, tokens/s = 77621 (34227 source, 43394 target) ; Learning rate = 0.000242 ; Loss = 1.428293\n",
      "2024-12-04 08:48:19.555000: I runner.py:310] Step = 133900 ; steps/s = 1.64, tokens/s = 78363 (34521 source, 43842 target) ; Learning rate = 0.000242 ; Loss = 1.429886\n",
      "2024-12-04 08:49:20.279000: I runner.py:310] Step = 134000 ; steps/s = 1.65, tokens/s = 78453 (34541 source, 43912 target) ; Learning rate = 0.000241 ; Loss = 1.432932\n",
      "2024-12-04 08:50:20.628000: I runner.py:310] Step = 134100 ; steps/s = 1.66, tokens/s = 77528 (34164 source, 43364 target) ; Learning rate = 0.000241 ; Loss = 1.428862\n",
      "2024-12-04 08:51:21.380000: I runner.py:310] Step = 134200 ; steps/s = 1.65, tokens/s = 78463 (34580 source, 43883 target) ; Learning rate = 0.000241 ; Loss = 1.425522\n",
      "2024-12-04 08:52:21.684000: I runner.py:310] Step = 134300 ; steps/s = 1.66, tokens/s = 77630 (34215 source, 43415 target) ; Learning rate = 0.000241 ; Loss = 1.424959\n",
      "2024-12-04 08:53:22.444000: I runner.py:310] Step = 134400 ; steps/s = 1.65, tokens/s = 78410 (34531 source, 43879 target) ; Learning rate = 0.000241 ; Loss = 1.427816\n",
      "2024-12-04 08:54:23.264000: I runner.py:310] Step = 134500 ; steps/s = 1.64, tokens/s = 78375 (34521 source, 43854 target) ; Learning rate = 0.000241 ; Loss = 1.430491\n",
      "2024-12-04 08:55:23.562000: I runner.py:310] Step = 134600 ; steps/s = 1.66, tokens/s = 77586 (34185 source, 43401 target) ; Learning rate = 0.000241 ; Loss = 1.428368\n",
      "2024-12-04 08:56:24.375000: I runner.py:310] Step = 134700 ; steps/s = 1.64, tokens/s = 78374 (34532 source, 43842 target) ; Learning rate = 0.000241 ; Loss = 1.427237\n",
      "2024-12-04 08:57:24.716000: I runner.py:310] Step = 134800 ; steps/s = 1.66, tokens/s = 77611 (34228 source, 43383 target) ; Learning rate = 0.000241 ; Loss = 1.424166\n",
      "2024-12-04 08:58:25.519000: I runner.py:310] Step = 134900 ; steps/s = 1.64, tokens/s = 78373 (34508 source, 43865 target) ; Learning rate = 0.000241 ; Loss = 1.432682\n",
      "2024-12-04 08:59:26.288000: I runner.py:310] Step = 135000 ; steps/s = 1.65, tokens/s = 78418 (34539 source, 43879 target) ; Learning rate = 0.000241 ; Loss = 1.430351\n",
      "2024-12-04 08:59:26.290000: I training.py:192] Running evaluation for step 135000\n",
      "2024-12-04 09:00:14.239000: I training.py:192] Evaluation result for step 135000: loss = 0.894518 ; perplexity = 2.446158\n",
      "2024-12-04 09:01:14.422000: I runner.py:310] Step = 135100 ; steps/s = 1.66, tokens/s = 77788 (34284 source, 43504 target) ; Learning rate = 0.000240 ; Loss = 1.426723\n",
      "2024-12-04 09:02:15.175000: I runner.py:310] Step = 135200 ; steps/s = 1.65, tokens/s = 78428 (34539 source, 43889 target) ; Learning rate = 0.000240 ; Loss = 1.425705\n",
      "2024-12-04 09:03:16.011000: I runner.py:310] Step = 135300 ; steps/s = 1.64, tokens/s = 78336 (34508 source, 43828 target) ; Learning rate = 0.000240 ; Loss = 1.428924\n",
      "2024-12-04 09:04:16.286000: I runner.py:310] Step = 135400 ; steps/s = 1.66, tokens/s = 77635 (34221 source, 43414 target) ; Learning rate = 0.000240 ; Loss = 1.426452\n",
      "2024-12-04 09:05:17.069000: I runner.py:310] Step = 135500 ; steps/s = 1.65, tokens/s = 78416 (34544 source, 43872 target) ; Learning rate = 0.000240 ; Loss = 1.430659\n",
      "2024-12-04 09:06:17.395000: I runner.py:310] Step = 135600 ; steps/s = 1.66, tokens/s = 77609 (34208 source, 43401 target) ; Learning rate = 0.000240 ; Loss = 1.428237\n",
      "2024-12-04 09:07:18.236000: I runner.py:310] Step = 135700 ; steps/s = 1.64, tokens/s = 78296 (34466 source, 43830 target) ; Learning rate = 0.000240 ; Loss = 1.433153\n",
      "2024-12-04 09:08:19.072000: I runner.py:310] Step = 135800 ; steps/s = 1.64, tokens/s = 78335 (34512 source, 43823 target) ; Learning rate = 0.000240 ; Loss = 1.431377\n",
      "2024-12-04 09:09:19.417000: I runner.py:310] Step = 135900 ; steps/s = 1.66, tokens/s = 77558 (34183 source, 43375 target) ; Learning rate = 0.000240 ; Loss = 1.426435\n",
      "2024-12-04 09:10:20.239000: I runner.py:310] Step = 136000 ; steps/s = 1.64, tokens/s = 78352 (34506 source, 43846 target) ; Learning rate = 0.000240 ; Loss = 1.429431\n",
      "2024-12-04 09:11:20.572000: I runner.py:310] Step = 136100 ; steps/s = 1.66, tokens/s = 77620 (34245 source, 43375 target) ; Learning rate = 0.000240 ; Loss = 1.422928\n",
      "2024-12-04 09:12:21.335000: I runner.py:310] Step = 136200 ; steps/s = 1.65, tokens/s = 78410 (34518 source, 43892 target) ; Learning rate = 0.000239 ; Loss = 1.426981\n",
      "2024-12-04 09:13:22.147000: I runner.py:310] Step = 136300 ; steps/s = 1.64, tokens/s = 78370 (34517 source, 43853 target) ; Learning rate = 0.000239 ; Loss = 1.428984\n",
      "2024-12-04 09:14:22.417000: I runner.py:310] Step = 136400 ; steps/s = 1.66, tokens/s = 77622 (34211 source, 43411 target) ; Learning rate = 0.000239 ; Loss = 1.427522\n",
      "2024-12-04 09:15:23.179000: I runner.py:310] Step = 136500 ; steps/s = 1.65, tokens/s = 78420 (34528 source, 43892 target) ; Learning rate = 0.000239 ; Loss = 1.431000\n",
      "2024-12-04 09:16:24.019000: I runner.py:310] Step = 136600 ; steps/s = 1.64, tokens/s = 78357 (34528 source, 43829 target) ; Learning rate = 0.000239 ; Loss = 1.426712\n",
      "2024-12-04 09:17:24.345000: I runner.py:310] Step = 136700 ; steps/s = 1.66, tokens/s = 77575 (34191 source, 43384 target) ; Learning rate = 0.000239 ; Loss = 1.421629\n",
      "2024-12-04 09:18:25.134000: I runner.py:310] Step = 136800 ; steps/s = 1.65, tokens/s = 78386 (34522 source, 43864 target) ; Learning rate = 0.000239 ; Loss = 1.425772\n",
      "2024-12-04 09:19:25.377000: I runner.py:310] Step = 136900 ; steps/s = 1.66, tokens/s = 77727 (34276 source, 43451 target) ; Learning rate = 0.000239 ; Loss = 1.430102\n",
      "2024-12-04 09:20:26.189000: I runner.py:310] Step = 137000 ; steps/s = 1.64, tokens/s = 78345 (34502 source, 43843 target) ; Learning rate = 0.000239 ; Loss = 1.426762\n",
      "2024-12-04 09:21:26.977000: I runner.py:310] Step = 137100 ; steps/s = 1.65, tokens/s = 78402 (34532 source, 43870 target) ; Learning rate = 0.000239 ; Loss = 1.430278\n",
      "2024-12-04 09:22:27.340000: I runner.py:310] Step = 137200 ; steps/s = 1.66, tokens/s = 77535 (34170 source, 43365 target) ; Learning rate = 0.000239 ; Loss = 1.429761\n",
      "2024-12-04 09:23:28.152000: I runner.py:310] Step = 137300 ; steps/s = 1.64, tokens/s = 78358 (34522 source, 43836 target) ; Learning rate = 0.000239 ; Loss = 1.428799\n",
      "2024-12-04 09:24:28.433000: I runner.py:310] Step = 137400 ; steps/s = 1.66, tokens/s = 77667 (34242 source, 43425 target) ; Learning rate = 0.000238 ; Loss = 1.427144\n",
      "2024-12-04 09:25:29.217000: I runner.py:310] Step = 137500 ; steps/s = 1.65, tokens/s = 78375 (34503 source, 43872 target) ; Learning rate = 0.000238 ; Loss = 1.428001\n",
      "2024-12-04 09:26:30.032000: I runner.py:310] Step = 137600 ; steps/s = 1.64, tokens/s = 78379 (34536 source, 43843 target) ; Learning rate = 0.000238 ; Loss = 1.426478\n",
      "2024-12-04 09:27:30.316000: I runner.py:310] Step = 137700 ; steps/s = 1.66, tokens/s = 77628 (34213 source, 43415 target) ; Learning rate = 0.000238 ; Loss = 1.424494\n",
      "2024-12-04 09:28:31.119000: I runner.py:310] Step = 137800 ; steps/s = 1.64, tokens/s = 78369 (34503 source, 43866 target) ; Learning rate = 0.000238 ; Loss = 1.430267\n",
      "2024-12-04 09:29:31.956000: I runner.py:310] Step = 137900 ; steps/s = 1.64, tokens/s = 78355 (34520 source, 43835 target) ; Learning rate = 0.000238 ; Loss = 1.431607\n",
      "2024-12-04 09:30:32.300000: I runner.py:310] Step = 138000 ; steps/s = 1.66, tokens/s = 77567 (34204 source, 43363 target) ; Learning rate = 0.000238 ; Loss = 1.424383\n",
      "2024-12-04 09:31:33.040000: I runner.py:310] Step = 138100 ; steps/s = 1.65, tokens/s = 78442 (34532 source, 43910 target) ; Learning rate = 0.000238 ; Loss = 1.425723\n",
      "2024-12-04 09:32:33.363000: I runner.py:310] Step = 138200 ; steps/s = 1.66, tokens/s = 77605 (34224 source, 43381 target) ; Learning rate = 0.000238 ; Loss = 1.425818\n",
      "2024-12-04 09:33:34.146000: I runner.py:310] Step = 138300 ; steps/s = 1.65, tokens/s = 78400 (34528 source, 43872 target) ; Learning rate = 0.000238 ; Loss = 1.426903\n",
      "2024-12-04 09:34:34.922000: I runner.py:310] Step = 138400 ; steps/s = 1.65, tokens/s = 78411 (34532 source, 43879 target) ; Learning rate = 0.000238 ; Loss = 1.426177\n",
      "2024-12-04 09:35:35.259000: I runner.py:310] Step = 138500 ; steps/s = 1.66, tokens/s = 77580 (34205 source, 43375 target) ; Learning rate = 0.000238 ; Loss = 1.426498\n",
      "2024-12-04 09:36:36.067000: I runner.py:310] Step = 138600 ; steps/s = 1.64, tokens/s = 78381 (34526 source, 43855 target) ; Learning rate = 0.000237 ; Loss = 1.423298\n",
      "2024-12-04 09:37:36.392000: I runner.py:310] Step = 138700 ; steps/s = 1.66, tokens/s = 77569 (34188 source, 43381 target) ; Learning rate = 0.000237 ; Loss = 1.425575\n",
      "2024-12-04 09:38:37.177000: I runner.py:310] Step = 138800 ; steps/s = 1.65, tokens/s = 78373 (34510 source, 43863 target) ; Learning rate = 0.000237 ; Loss = 1.427439\n",
      "2024-12-04 09:39:37.996000: I runner.py:310] Step = 138900 ; steps/s = 1.64, tokens/s = 78374 (34520 source, 43854 target) ; Learning rate = 0.000237 ; Loss = 1.428821\n",
      "2024-12-04 09:40:38.321000: I runner.py:310] Step = 139000 ; steps/s = 1.66, tokens/s = 77588 (34208 source, 43380 target) ; Learning rate = 0.000237 ; Loss = 1.426226\n",
      "2024-12-04 09:41:39.104000: I runner.py:310] Step = 139100 ; steps/s = 1.65, tokens/s = 78377 (34508 source, 43869 target) ; Learning rate = 0.000237 ; Loss = 1.429176\n",
      "2024-12-04 09:42:39.817000: I runner.py:310] Step = 139200 ; steps/s = 1.65, tokens/s = 78511 (34581 source, 43930 target) ; Learning rate = 0.000237 ; Loss = 1.432182\n",
      "2024-12-04 09:43:40.151000: I runner.py:310] Step = 139300 ; steps/s = 1.66, tokens/s = 77589 (34210 source, 43379 target) ; Learning rate = 0.000237 ; Loss = 1.423976\n",
      "2024-12-04 09:44:40.972000: I runner.py:310] Step = 139400 ; steps/s = 1.64, tokens/s = 78348 (34508 source, 43840 target) ; Learning rate = 0.000237 ; Loss = 1.429715\n",
      "2024-12-04 09:45:41.295000: I runner.py:310] Step = 139500 ; steps/s = 1.66, tokens/s = 77569 (34190 source, 43379 target) ; Learning rate = 0.000237 ; Loss = 1.422082\n",
      "2024-12-04 09:46:42.116000: I runner.py:310] Step = 139600 ; steps/s = 1.64, tokens/s = 78330 (34492 source, 43838 target) ; Learning rate = 0.000237 ; Loss = 1.427220\n",
      "2024-12-04 09:47:42.916000: I runner.py:310] Step = 139700 ; steps/s = 1.64, tokens/s = 78415 (34541 source, 43874 target) ; Learning rate = 0.000236 ; Loss = 1.432939\n",
      "2024-12-04 09:48:43.281000: I runner.py:310] Step = 139800 ; steps/s = 1.66, tokens/s = 77543 (34185 source, 43358 target) ; Learning rate = 0.000236 ; Loss = 1.432927\n",
      "2024-12-04 09:49:44.075000: I runner.py:310] Step = 139900 ; steps/s = 1.65, tokens/s = 78370 (34517 source, 43853 target) ; Learning rate = 0.000236 ; Loss = 1.429355\n",
      "2024-12-04 09:50:44.413000: I runner.py:310] Step = 140000 ; steps/s = 1.66, tokens/s = 77596 (34215 source, 43381 target) ; Learning rate = 0.000236 ; Loss = 1.425554\n",
      "2024-12-04 09:50:46.083000: I training.py:176] Saved checkpoint TR-EN-Shared-vocab/ckpt-140000\n",
      "2024-12-04 09:50:46.083000: I training.py:192] Running evaluation for step 140000\n",
      "2024-12-04 09:51:32.367000: I training.py:192] Evaluation result for step 140000: loss = 0.905305 ; perplexity = 2.472685\n",
      "2024-12-04 09:52:33.125000: I runner.py:310] Step = 140100 ; steps/s = 1.65, tokens/s = 78404 (34511 source, 43893 target) ; Learning rate = 0.000236 ; Loss = 1.422085\n",
      "2024-12-04 09:53:33.951000: I runner.py:310] Step = 140200 ; steps/s = 1.64, tokens/s = 78374 (34532 source, 43842 target) ; Learning rate = 0.000236 ; Loss = 1.427803\n",
      "2024-12-04 09:54:34.283000: I runner.py:310] Step = 140300 ; steps/s = 1.66, tokens/s = 77574 (34194 source, 43380 target) ; Learning rate = 0.000236 ; Loss = 1.429006\n",
      "2024-12-04 09:55:35.008000: I runner.py:310] Step = 140400 ; steps/s = 1.65, tokens/s = 78477 (34558 source, 43919 target) ; Learning rate = 0.000236 ; Loss = 1.428013\n",
      "2024-12-04 09:56:35.801000: I runner.py:310] Step = 140500 ; steps/s = 1.65, tokens/s = 78409 (34539 source, 43870 target) ; Learning rate = 0.000236 ; Loss = 1.430526\n",
      "2024-12-04 09:57:36.071000: I runner.py:310] Step = 140600 ; steps/s = 1.66, tokens/s = 77675 (34259 source, 43416 target) ; Learning rate = 0.000236 ; Loss = 1.426232\n",
      "2024-12-04 09:58:36.888000: I runner.py:310] Step = 140700 ; steps/s = 1.64, tokens/s = 78350 (34502 source, 43848 target) ; Learning rate = 0.000236 ; Loss = 1.425423\n",
      "2024-12-04 09:59:37.244000: I runner.py:310] Step = 140800 ; steps/s = 1.66, tokens/s = 77527 (34169 source, 43358 target) ; Learning rate = 0.000236 ; Loss = 1.425747\n",
      "2024-12-04 10:00:38.111000: I runner.py:310] Step = 140900 ; steps/s = 1.64, tokens/s = 78311 (34489 source, 43822 target) ; Learning rate = 0.000235 ; Loss = 1.430646\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/omuceng/deneme/bin/onmt-main\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/opennmt/bin/main.py\", line 325, in main\n",
      "    runner.train(\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/opennmt/runner.py\", line 310, in train\n",
      "    summary = trainer(\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/opennmt/training.py\", line 111, in __call__\n",
      "    for i, loss in enumerate(self._steps(dataset, accum_steps=accum_steps)):\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/opennmt/training.py\", line 229, in _steps\n",
      "    accumulate_gradients(batch)\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 880, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 912, in _call\n",
      "    return self._no_variable_creation_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 134, in __call__\n",
      "    return concrete_function._call_flat(\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\", line 1745, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\", line 378, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/eager/execute.py\", line 52, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Tr-En (Tatoeba) -> Kk-En\n",
    "!onmt-main --model kk-tr-en-shared.py --config tr-en-shared-vocab.yml --auto_config train --with_eval --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dce6303-d475-469f-b1c2-08b07101a785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-04 10:02:44.683064: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 10:02:45.478635: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-04 10:02:45.478703: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-04 10:02:45.478712: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-04 10:02:46.469000: I onmt-main:8] Creating model directory TR-KK-EN-Shared-vocab\n",
      "2024-12-04 10:02:46.663000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-04 10:02:46.663000: I main.py:315] Using model:\n",
      "(model): MyCustomTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): WordEmbedder()\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): WordEmbedder()\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-04 10:02:46.666204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-04 10:02:48.262333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-04 10:02:48.262981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7703 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-12-04 10:02:48.263472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 6099 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "2024-12-04 10:02:48.267000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file: KK_tokens_valid_shared\n",
      "  eval_labels_file: KK_valid_target_tokens_shared.txt\n",
      "  source_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file: KK_tokens_train_shared\n",
      "  train_labels_file: KK_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: TR-KK-EN-Shared-vocab\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 250000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-04 10:02:48.604000: I inputter.py:316] Initialized source input layer:\n",
      "2024-12-04 10:02:48.604000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-04 10:02:48.604000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-04 10:02:48.678000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-04 10:02:48.679000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-04 10:02:48.679000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-04 10:02:48.701000: I runner.py:269] Restored checkpoint TR-EN-Shared-vocab/ckpt-100000\n",
      "2024-12-04 10:02:48.703000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "2024-12-04 10:02:48.749000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-04 10:02:49.736294: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-04 10:02:49.869000: I main.py:325] Accumulate gradients of 7 iterations to reach effective batch size of 25000\n",
      "2024-12-04 10:02:49.998000: I mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2024-12-04 10:02:50.212000: I dataset_ops.py:2542] Training on 318032 examples\n",
      "2024-12-04 10:03:56.602846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-04 10:03:57.657264: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-04 10:03:57.935578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-04 10:04:07.119000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-04 10:04:07.138000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-04 10:04:08.717000: I cross_device_ops.py:897] batch_all_reduce: 260 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-04 10:04:13.189000: I cross_device_ops.py:897] batch_all_reduce: 260 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-04 10:04:20.573000: I runner.py:310] Number of model parameters: 93326081\n",
      "2024-12-04 10:04:20.577000: I runner.py:310] Number of model weights: 260 (trainable = 260, non trainable = 0)\n",
      "2024-12-04 10:04:20.613000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-04 10:04:20.620000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-04 10:04:22.701000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-1\n",
      "2024-12-04 10:04:23.307000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-04 10:04:23.331000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-04 10:04:23.949000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-04 10:04:23.968000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-04 10:04:24.606000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-04 10:04:24.632000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-04 10:05:24.763000: I runner.py:310] Step = 100 ; steps/s = 1.60, tokens/s = 81265 (38484 source, 42781 target) ; Learning rate = 0.000009 ; Loss = 8.403070\n",
      "2024-12-04 10:06:26.130000: I runner.py:310] Step = 200 ; steps/s = 1.63, tokens/s = 82771 (39261 source, 43510 target) ; Learning rate = 0.000018 ; Loss = 7.324019\n",
      "2024-12-04 10:07:27.647000: I runner.py:310] Step = 300 ; steps/s = 1.63, tokens/s = 82603 (39170 source, 43433 target) ; Learning rate = 0.000027 ; Loss = 6.928751\n",
      "2024-12-04 10:08:29.038000: I runner.py:310] Step = 400 ; steps/s = 1.63, tokens/s = 81157 (38486 source, 42671 target) ; Learning rate = 0.000035 ; Loss = 6.655913\n",
      "2024-12-04 10:09:30.450000: I runner.py:310] Step = 500 ; steps/s = 1.63, tokens/s = 82743 (39231 source, 43512 target) ; Learning rate = 0.000044 ; Loss = 6.374588\n",
      "2024-12-04 10:10:31.850000: I runner.py:310] Step = 600 ; steps/s = 1.63, tokens/s = 82737 (39210 source, 43527 target) ; Learning rate = 0.000053 ; Loss = 6.232490\n",
      "2024-12-04 10:11:33.270000: I runner.py:310] Step = 700 ; steps/s = 1.63, tokens/s = 82719 (39209 source, 43510 target) ; Learning rate = 0.000062 ; Loss = 6.033752\n",
      "2024-12-04 10:12:34.088000: I runner.py:310] Step = 800 ; steps/s = 1.64, tokens/s = 81935 (38856 source, 43079 target) ; Learning rate = 0.000071 ; Loss = 5.857160\n",
      "2024-12-04 10:13:35.577000: I runner.py:310] Step = 900 ; steps/s = 1.63, tokens/s = 82619 (39148 source, 43471 target) ; Learning rate = 0.000080 ; Loss = 5.768423\n",
      "2024-12-04 10:14:37.011000: I runner.py:310] Step = 1000 ; steps/s = 1.63, tokens/s = 82708 (39222 source, 43486 target) ; Learning rate = 0.000088 ; Loss = 5.626747\n",
      "2024-12-04 10:15:38.468000: I runner.py:310] Step = 1100 ; steps/s = 1.63, tokens/s = 82651 (39186 source, 43465 target) ; Learning rate = 0.000097 ; Loss = 5.421736\n",
      "2024-12-04 10:16:39.292000: I runner.py:310] Step = 1200 ; steps/s = 1.64, tokens/s = 81932 (38873 source, 43059 target) ; Learning rate = 0.000106 ; Loss = 5.209574\n",
      "2024-12-04 10:17:40.719000: I runner.py:310] Step = 1300 ; steps/s = 1.63, tokens/s = 82676 (39180 source, 43496 target) ; Learning rate = 0.000115 ; Loss = 4.963210\n",
      "2024-12-04 10:18:42.111000: I runner.py:310] Step = 1400 ; steps/s = 1.63, tokens/s = 82782 (39233 source, 43549 target) ; Learning rate = 0.000124 ; Loss = 4.695537\n",
      "2024-12-04 10:19:43.470000: I runner.py:310] Step = 1500 ; steps/s = 1.63, tokens/s = 82808 (39273 source, 43535 target) ; Learning rate = 0.000133 ; Loss = 4.452199\n",
      "2024-12-04 10:20:44.270000: I runner.py:310] Step = 1600 ; steps/s = 1.64, tokens/s = 81940 (38833 source, 43107 target) ; Learning rate = 0.000142 ; Loss = 4.260265\n",
      "2024-12-04 10:21:45.632000: I runner.py:310] Step = 1700 ; steps/s = 1.63, tokens/s = 82799 (39249 source, 43550 target) ; Learning rate = 0.000150 ; Loss = 3.932851\n",
      "2024-12-04 10:22:47.034000: I runner.py:310] Step = 1800 ; steps/s = 1.63, tokens/s = 82768 (39226 source, 43542 target) ; Learning rate = 0.000159 ; Loss = 3.732768\n",
      "2024-12-04 10:23:48.421000: I runner.py:310] Step = 1900 ; steps/s = 1.63, tokens/s = 82743 (39250 source, 43493 target) ; Learning rate = 0.000168 ; Loss = 3.772170\n",
      "2024-12-04 10:24:49.237000: I runner.py:310] Step = 2000 ; steps/s = 1.64, tokens/s = 81937 (38866 source, 43071 target) ; Learning rate = 0.000177 ; Loss = 3.669966\n",
      "2024-12-04 10:25:50.609000: I runner.py:310] Step = 2100 ; steps/s = 1.63, tokens/s = 82773 (39192 source, 43581 target) ; Learning rate = 0.000186 ; Loss = 3.569810\n",
      "2024-12-04 10:26:51.992000: I runner.py:310] Step = 2200 ; steps/s = 1.63, tokens/s = 82780 (39268 source, 43512 target) ; Learning rate = 0.000195 ; Loss = 3.416915\n",
      "2024-12-04 10:27:53.414000: I runner.py:310] Step = 2300 ; steps/s = 1.63, tokens/s = 82691 (39209 source, 43482 target) ; Learning rate = 0.000203 ; Loss = 3.388595\n",
      "2024-12-04 10:28:54.219000: I runner.py:310] Step = 2400 ; steps/s = 1.64, tokens/s = 81970 (38849 source, 43121 target) ; Learning rate = 0.000212 ; Loss = 3.233240\n",
      "2024-12-04 10:29:55.586000: I runner.py:310] Step = 2500 ; steps/s = 1.63, tokens/s = 82803 (39260 source, 43543 target) ; Learning rate = 0.000221 ; Loss = 3.068325\n",
      "2024-12-04 10:30:56.904000: I runner.py:310] Step = 2600 ; steps/s = 1.63, tokens/s = 82843 (39272 source, 43571 target) ; Learning rate = 0.000230 ; Loss = 3.046789\n",
      "2024-12-04 10:31:58.264000: I runner.py:310] Step = 2700 ; steps/s = 1.63, tokens/s = 82768 (39258 source, 43510 target) ; Learning rate = 0.000239 ; Loss = 2.976771\n",
      "2024-12-04 10:32:59.106000: I runner.py:310] Step = 2800 ; steps/s = 1.64, tokens/s = 81911 (38827 source, 43084 target) ; Learning rate = 0.000248 ; Loss = 2.971720\n",
      "2024-12-04 10:34:00.555000: I runner.py:310] Step = 2900 ; steps/s = 1.63, tokens/s = 82681 (39219 source, 43462 target) ; Learning rate = 0.000256 ; Loss = 2.918435\n",
      "2024-12-04 10:35:01.910000: I runner.py:310] Step = 3000 ; steps/s = 1.63, tokens/s = 82798 (39256 source, 43542 target) ; Learning rate = 0.000265 ; Loss = 2.905654\n",
      "2024-12-04 10:36:03.308000: I runner.py:310] Step = 3100 ; steps/s = 1.63, tokens/s = 82743 (39214 source, 43529 target) ; Learning rate = 0.000274 ; Loss = 2.909055\n",
      "2024-12-04 10:37:04.126000: I runner.py:310] Step = 3200 ; steps/s = 1.64, tokens/s = 81940 (38852 source, 43088 target) ; Learning rate = 0.000283 ; Loss = 2.768188\n",
      "2024-12-04 10:38:05.572000: I runner.py:310] Step = 3300 ; steps/s = 1.63, tokens/s = 82659 (39197 source, 43462 target) ; Learning rate = 0.000292 ; Loss = 2.767959\n",
      "2024-12-04 10:39:06.893000: I runner.py:310] Step = 3400 ; steps/s = 1.63, tokens/s = 82813 (39269 source, 43544 target) ; Learning rate = 0.000301 ; Loss = 2.666334\n",
      "2024-12-04 10:40:08.301000: I runner.py:310] Step = 3500 ; steps/s = 1.63, tokens/s = 82772 (39230 source, 43542 target) ; Learning rate = 0.000309 ; Loss = 2.738521\n",
      "2024-12-04 10:41:09.143000: I runner.py:310] Step = 3600 ; steps/s = 1.64, tokens/s = 81920 (38819 source, 43101 target) ; Learning rate = 0.000318 ; Loss = 2.674301\n",
      "2024-12-04 10:42:10.509000: I runner.py:310] Step = 3700 ; steps/s = 1.63, tokens/s = 82785 (39268 source, 43517 target) ; Learning rate = 0.000327 ; Loss = 2.786364\n",
      "2024-12-04 10:43:11.899000: I runner.py:310] Step = 3800 ; steps/s = 1.63, tokens/s = 82765 (39264 source, 43501 target) ; Learning rate = 0.000336 ; Loss = 2.614925\n",
      "2024-12-04 10:44:13.349000: I runner.py:310] Step = 3900 ; steps/s = 1.63, tokens/s = 82678 (39177 source, 43501 target) ; Learning rate = 0.000345 ; Loss = 2.567096\n",
      "2024-12-04 10:45:14.162000: I runner.py:310] Step = 4000 ; steps/s = 1.64, tokens/s = 81960 (38831 source, 43129 target) ; Learning rate = 0.000354 ; Loss = 2.527354\n",
      "2024-12-04 10:46:15.618000: I runner.py:310] Step = 4100 ; steps/s = 1.63, tokens/s = 82650 (39197 source, 43453 target) ; Learning rate = 0.000362 ; Loss = 2.519529\n",
      "2024-12-04 10:47:17.062000: I runner.py:310] Step = 4200 ; steps/s = 1.63, tokens/s = 82690 (39192 source, 43498 target) ; Learning rate = 0.000371 ; Loss = 2.525924\n",
      "2024-12-04 10:48:18.052000: I runner.py:310] Step = 4300 ; steps/s = 1.64, tokens/s = 82220 (38995 source, 43225 target) ; Learning rate = 0.000380 ; Loss = 2.632674\n",
      "2024-12-04 10:49:19.232000: I runner.py:310] Step = 4400 ; steps/s = 1.63, tokens/s = 82518 (39149 source, 43369 target) ; Learning rate = 0.000389 ; Loss = 2.467675\n",
      "2024-12-04 10:50:20.671000: I runner.py:310] Step = 4500 ; steps/s = 1.63, tokens/s = 82681 (39175 source, 43506 target) ; Learning rate = 0.000398 ; Loss = 2.437916\n",
      "2024-12-04 10:51:22.044000: I runner.py:310] Step = 4600 ; steps/s = 1.63, tokens/s = 82785 (39254 source, 43531 target) ; Learning rate = 0.000407 ; Loss = 2.439057\n",
      "2024-12-04 10:52:23.110000: I runner.py:310] Step = 4700 ; steps/s = 1.64, tokens/s = 81590 (38668 source, 42922 target) ; Learning rate = 0.000416 ; Loss = 2.364421\n",
      "2024-12-04 10:53:24.534000: I runner.py:310] Step = 4800 ; steps/s = 1.63, tokens/s = 82721 (39221 source, 43500 target) ; Learning rate = 0.000424 ; Loss = 2.340939\n",
      "2024-12-04 10:54:26.067000: I runner.py:310] Step = 4900 ; steps/s = 1.63, tokens/s = 82540 (39109 source, 43431 target) ; Learning rate = 0.000433 ; Loss = 2.406459\n",
      "2024-12-04 10:55:27.472000: I runner.py:310] Step = 5000 ; steps/s = 1.63, tokens/s = 82753 (39226 source, 43527 target) ; Learning rate = 0.000442 ; Loss = 2.394250\n",
      "2024-12-04 10:55:27.473000: I training.py:192] Running evaluation for step 5000\n",
      "2024-12-04 11:01:13.968000: I training.py:192] Evaluation result for step 5000: loss = 1.234308 ; perplexity = 3.436000\n",
      "2024-12-04 11:02:14.803000: I runner.py:310] Step = 5100 ; steps/s = 1.64, tokens/s = 81910 (38852 source, 43058 target) ; Learning rate = 0.000451 ; Loss = 2.364942\n",
      "2024-12-04 11:03:16.375000: I runner.py:310] Step = 5200 ; steps/s = 1.62, tokens/s = 82505 (39094 source, 43411 target) ; Learning rate = 0.000460 ; Loss = 2.302480\n",
      "2024-12-04 11:04:17.920000: I runner.py:310] Step = 5300 ; steps/s = 1.63, tokens/s = 82545 (39119 source, 43426 target) ; Learning rate = 0.000469 ; Loss = 2.247182\n",
      "2024-12-04 11:05:19.490000: I runner.py:310] Step = 5400 ; steps/s = 1.62, tokens/s = 82533 (39160 source, 43373 target) ; Learning rate = 0.000477 ; Loss = 2.328963\n",
      "2024-12-04 11:06:20.527000: I runner.py:310] Step = 5500 ; steps/s = 1.64, tokens/s = 81638 (38711 source, 42927 target) ; Learning rate = 0.000486 ; Loss = 2.284823\n",
      "2024-12-04 11:07:22.073000: I runner.py:310] Step = 5600 ; steps/s = 1.62, tokens/s = 82538 (39140 source, 43398 target) ; Learning rate = 0.000495 ; Loss = 2.242431\n",
      "2024-12-04 11:08:23.689000: I runner.py:310] Step = 5700 ; steps/s = 1.62, tokens/s = 82456 (39063 source, 43393 target) ; Learning rate = 0.000504 ; Loss = 2.250511\n",
      "2024-12-04 11:09:25.135000: I runner.py:310] Step = 5800 ; steps/s = 1.63, tokens/s = 82699 (39199 source, 43500 target) ; Learning rate = 0.000513 ; Loss = 2.296836\n",
      "2024-12-04 11:10:26.126000: I runner.py:310] Step = 5900 ; steps/s = 1.64, tokens/s = 81685 (38760 source, 42925 target) ; Learning rate = 0.000522 ; Loss = 2.186851\n",
      "2024-12-04 11:11:27.682000: I runner.py:310] Step = 6000 ; steps/s = 1.62, tokens/s = 82518 (39113 source, 43405 target) ; Learning rate = 0.000530 ; Loss = 2.189825\n",
      "2024-12-04 11:12:29.208000: I runner.py:310] Step = 6100 ; steps/s = 1.63, tokens/s = 82576 (39150 source, 43426 target) ; Learning rate = 0.000539 ; Loss = 2.217287\n",
      "2024-12-04 11:13:30.666000: I runner.py:310] Step = 6200 ; steps/s = 1.63, tokens/s = 82666 (39209 source, 43457 target) ; Learning rate = 0.000548 ; Loss = 2.164488\n",
      "2024-12-04 11:14:31.661000: I runner.py:310] Step = 6300 ; steps/s = 1.64, tokens/s = 81682 (38684 source, 42998 target) ; Learning rate = 0.000557 ; Loss = 2.159468\n",
      "2024-12-04 11:15:33.245000: I runner.py:310] Step = 6400 ; steps/s = 1.62, tokens/s = 82493 (39136 source, 43357 target) ; Learning rate = 0.000566 ; Loss = 2.131763\n",
      "2024-12-04 11:16:34.737000: I runner.py:310] Step = 6500 ; steps/s = 1.63, tokens/s = 82626 (39167 source, 43459 target) ; Learning rate = 0.000575 ; Loss = 2.173471\n",
      "2024-12-04 11:17:36.300000: I runner.py:310] Step = 6600 ; steps/s = 1.62, tokens/s = 82545 (39130 source, 43415 target) ; Learning rate = 0.000583 ; Loss = 2.187434\n",
      "2024-12-04 11:18:37.288000: I runner.py:310] Step = 6700 ; steps/s = 1.64, tokens/s = 81686 (38748 source, 42938 target) ; Learning rate = 0.000592 ; Loss = 2.111209\n",
      "2024-12-04 11:19:38.832000: I runner.py:310] Step = 6800 ; steps/s = 1.63, tokens/s = 82527 (39116 source, 43411 target) ; Learning rate = 0.000601 ; Loss = 2.067894\n",
      "2024-12-04 11:20:40.300000: I runner.py:310] Step = 6900 ; steps/s = 1.63, tokens/s = 82654 (39198 source, 43456 target) ; Learning rate = 0.000610 ; Loss = 2.148177\n",
      "2024-12-04 11:21:41.754000: I runner.py:310] Step = 7000 ; steps/s = 1.63, tokens/s = 82687 (39198 source, 43489 target) ; Learning rate = 0.000619 ; Loss = 2.098776\n",
      "2024-12-04 11:22:42.724000: I runner.py:310] Step = 7100 ; steps/s = 1.64, tokens/s = 81704 (38721 source, 42983 target) ; Learning rate = 0.000628 ; Loss = 2.087666\n",
      "2024-12-04 11:23:44.167000: I runner.py:310] Step = 7200 ; steps/s = 1.63, tokens/s = 82681 (39203 source, 43478 target) ; Learning rate = 0.000636 ; Loss = 2.081730\n",
      "2024-12-04 11:24:45.695000: I runner.py:310] Step = 7300 ; steps/s = 1.63, tokens/s = 82611 (39181 source, 43430 target) ; Learning rate = 0.000645 ; Loss = 2.129057\n",
      "2024-12-04 11:25:47.135000: I runner.py:310] Step = 7400 ; steps/s = 1.63, tokens/s = 82686 (39200 source, 43486 target) ; Learning rate = 0.000654 ; Loss = 2.107314\n",
      "2024-12-04 11:26:48.024000: I runner.py:310] Step = 7500 ; steps/s = 1.64, tokens/s = 81861 (38803 source, 43058 target) ; Learning rate = 0.000663 ; Loss = 2.050545\n",
      "2024-12-04 11:27:49.442000: I runner.py:310] Step = 7600 ; steps/s = 1.63, tokens/s = 82710 (39232 source, 43478 target) ; Learning rate = 0.000672 ; Loss = 2.051859\n",
      "2024-12-04 11:28:50.951000: I runner.py:310] Step = 7700 ; steps/s = 1.63, tokens/s = 82566 (39120 source, 43446 target) ; Learning rate = 0.000681 ; Loss = 2.011684\n",
      "2024-12-04 11:29:52.387000: I runner.py:310] Step = 7800 ; steps/s = 1.63, tokens/s = 82697 (39206 source, 43491 target) ; Learning rate = 0.000690 ; Loss = 2.019178\n",
      "2024-12-04 11:30:53.502000: I runner.py:310] Step = 7900 ; steps/s = 1.64, tokens/s = 81588 (38671 source, 42917 target) ; Learning rate = 0.000698 ; Loss = 2.051411\n",
      "2024-12-04 11:31:54.949000: I runner.py:310] Step = 8000 ; steps/s = 1.63, tokens/s = 82678 (39195 source, 43483 target) ; Learning rate = 0.000707 ; Loss = 2.015358\n",
      "2024-12-04 11:32:56.479000: I runner.py:310] Step = 8100 ; steps/s = 1.63, tokens/s = 82556 (39156 source, 43400 target) ; Learning rate = 0.000716 ; Loss = 2.019987\n",
      "2024-12-04 11:33:57.917000: I runner.py:310] Step = 8200 ; steps/s = 1.63, tokens/s = 82660 (39187 source, 43473 target) ; Learning rate = 0.000725 ; Loss = 2.028335\n",
      "2024-12-04 11:34:58.816000: I runner.py:310] Step = 8300 ; steps/s = 1.64, tokens/s = 81813 (38758 source, 43055 target) ; Learning rate = 0.000734 ; Loss = 1.942576\n",
      "2024-12-04 11:36:00.276000: I runner.py:310] Step = 8400 ; steps/s = 1.63, tokens/s = 82655 (39192 source, 43463 target) ; Learning rate = 0.000743 ; Loss = 2.017366\n",
      "2024-12-04 11:37:01.758000: I runner.py:310] Step = 8500 ; steps/s = 1.63, tokens/s = 82636 (39197 source, 43439 target) ; Learning rate = 0.000751 ; Loss = 1.971045\n",
      "2024-12-04 11:38:02.963000: I runner.py:310] Step = 8600 ; steps/s = 1.63, tokens/s = 82400 (39069 source, 43331 target) ; Learning rate = 0.000760 ; Loss = 1.987295\n",
      "2024-12-04 11:39:04.037000: I runner.py:310] Step = 8700 ; steps/s = 1.64, tokens/s = 82165 (38950 source, 43215 target) ; Learning rate = 0.000769 ; Loss = 1.935062\n",
      "2024-12-04 11:40:05.543000: I runner.py:310] Step = 8800 ; steps/s = 1.63, tokens/s = 82600 (39149 source, 43451 target) ; Learning rate = 0.000778 ; Loss = 1.966966\n",
      "2024-12-04 11:41:06.974000: I runner.py:310] Step = 8900 ; steps/s = 1.63, tokens/s = 82765 (39246 source, 43519 target) ; Learning rate = 0.000787 ; Loss = 1.973225\n",
      "2024-12-04 11:42:07.864000: I runner.py:310] Step = 9000 ; steps/s = 1.64, tokens/s = 81813 (38789 source, 43024 target) ; Learning rate = 0.000796 ; Loss = 1.944355\n",
      "2024-12-04 11:43:09.251000: I runner.py:310] Step = 9100 ; steps/s = 1.63, tokens/s = 82753 (39223 source, 43530 target) ; Learning rate = 0.000804 ; Loss = 1.952320\n",
      "2024-12-04 11:44:10.724000: I runner.py:310] Step = 9200 ; steps/s = 1.63, tokens/s = 82664 (39188 source, 43476 target) ; Learning rate = 0.000813 ; Loss = 1.901880\n",
      "2024-12-04 11:45:12.116000: I runner.py:310] Step = 9300 ; steps/s = 1.63, tokens/s = 82746 (39249 source, 43497 target) ; Learning rate = 0.000822 ; Loss = 1.936377\n",
      "2024-12-04 11:46:13.051000: I runner.py:310] Step = 9400 ; steps/s = 1.64, tokens/s = 81779 (38768 source, 43011 target) ; Learning rate = 0.000831 ; Loss = 1.912663\n",
      "2024-12-04 11:47:14.445000: I runner.py:310] Step = 9500 ; steps/s = 1.63, tokens/s = 82752 (39226 source, 43526 target) ; Learning rate = 0.000840 ; Loss = 1.912861\n",
      "2024-12-04 11:48:15.909000: I runner.py:310] Step = 9600 ; steps/s = 1.63, tokens/s = 82642 (39200 source, 43442 target) ; Learning rate = 0.000849 ; Loss = 1.898696\n",
      "2024-12-04 11:49:17.343000: I runner.py:310] Step = 9700 ; steps/s = 1.63, tokens/s = 82677 (39196 source, 43481 target) ; Learning rate = 0.000857 ; Loss = 1.907800\n",
      "2024-12-04 11:50:18.288000: I runner.py:310] Step = 9800 ; steps/s = 1.64, tokens/s = 81787 (38753 source, 43034 target) ; Learning rate = 0.000866 ; Loss = 1.902882\n",
      "2024-12-04 11:51:19.740000: I runner.py:310] Step = 9900 ; steps/s = 1.63, tokens/s = 82678 (39219 source, 43459 target) ; Learning rate = 0.000875 ; Loss = 1.878330\n",
      "2024-12-04 11:52:21.209000: I runner.py:310] Step = 10000 ; steps/s = 1.63, tokens/s = 82665 (39211 source, 43454 target) ; Learning rate = 0.000884 ; Loss = 1.905026\n",
      "2024-12-04 11:52:23.406000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-10000\n",
      "2024-12-04 11:52:23.406000: I training.py:192] Running evaluation for step 10000\n",
      "2024-12-04 11:57:46.104000: I training.py:192] Evaluation result for step 10000: loss = 1.072685 ; perplexity = 2.923219\n",
      "2024-12-04 11:58:47.495000: I runner.py:310] Step = 10100 ; steps/s = 1.63, tokens/s = 82768 (39221 source, 43547 target) ; Learning rate = 0.000879 ; Loss = 1.910609\n",
      "2024-12-04 11:59:48.339000: I runner.py:310] Step = 10200 ; steps/s = 1.64, tokens/s = 81871 (38779 source, 43092 target) ; Learning rate = 0.000875 ; Loss = 1.869578\n",
      "2024-12-04 12:00:49.767000: I runner.py:310] Step = 10300 ; steps/s = 1.63, tokens/s = 82685 (39177 source, 43508 target) ; Learning rate = 0.000871 ; Loss = 1.859311\n",
      "2024-12-04 12:01:51.240000: I runner.py:310] Step = 10400 ; steps/s = 1.63, tokens/s = 82662 (39205 source, 43457 target) ; Learning rate = 0.000867 ; Loss = 1.893488\n",
      "2024-12-04 12:02:52.717000: I runner.py:310] Step = 10500 ; steps/s = 1.63, tokens/s = 82670 (39209 source, 43461 target) ; Learning rate = 0.000863 ; Loss = 1.879576\n",
      "2024-12-04 12:03:53.675000: I runner.py:310] Step = 10600 ; steps/s = 1.64, tokens/s = 81711 (38744 source, 42967 target) ; Learning rate = 0.000858 ; Loss = 1.856872\n",
      "2024-12-04 12:04:55.137000: I runner.py:310] Step = 10700 ; steps/s = 1.63, tokens/s = 82640 (39168 source, 43472 target) ; Learning rate = 0.000854 ; Loss = 1.842887\n",
      "2024-12-04 12:05:56.537000: I runner.py:310] Step = 10800 ; steps/s = 1.63, tokens/s = 82733 (39255 source, 43478 target) ; Learning rate = 0.000850 ; Loss = 1.840273\n",
      "2024-12-04 12:06:58.073000: I runner.py:310] Step = 10900 ; steps/s = 1.63, tokens/s = 82571 (39151 source, 43420 target) ; Learning rate = 0.000847 ; Loss = 1.875849\n",
      "2024-12-04 12:07:58.974000: I runner.py:310] Step = 11000 ; steps/s = 1.64, tokens/s = 81875 (38789 source, 43086 target) ; Learning rate = 0.000843 ; Loss = 1.851107\n",
      "2024-12-04 12:09:00.430000: I runner.py:310] Step = 11100 ; steps/s = 1.63, tokens/s = 82637 (39185 source, 43452 target) ; Learning rate = 0.000839 ; Loss = 1.864428\n",
      "2024-12-04 12:10:01.938000: I runner.py:310] Step = 11200 ; steps/s = 1.63, tokens/s = 82583 (39167 source, 43416 target) ; Learning rate = 0.000835 ; Loss = 1.831770\n",
      "2024-12-04 12:11:03.461000: I runner.py:310] Step = 11300 ; steps/s = 1.63, tokens/s = 82582 (39134 source, 43448 target) ; Learning rate = 0.000831 ; Loss = 1.840680\n",
      "2024-12-04 12:12:04.305000: I runner.py:310] Step = 11400 ; steps/s = 1.64, tokens/s = 81914 (38828 source, 43086 target) ; Learning rate = 0.000828 ; Loss = 1.823849\n",
      "2024-12-04 12:13:05.777000: I runner.py:310] Step = 11500 ; steps/s = 1.63, tokens/s = 82655 (39193 source, 43462 target) ; Learning rate = 0.000824 ; Loss = 1.821467\n",
      "2024-12-04 12:14:07.230000: I runner.py:310] Step = 11600 ; steps/s = 1.63, tokens/s = 82657 (39176 source, 43481 target) ; Learning rate = 0.000821 ; Loss = 1.823357\n",
      "2024-12-04 12:15:08.715000: I runner.py:310] Step = 11700 ; steps/s = 1.63, tokens/s = 82619 (39171 source, 43448 target) ; Learning rate = 0.000817 ; Loss = 1.807774\n",
      "2024-12-04 12:16:09.632000: I runner.py:310] Step = 11800 ; steps/s = 1.64, tokens/s = 81839 (38802 source, 43037 target) ; Learning rate = 0.000814 ; Loss = 1.806351\n",
      "2024-12-04 12:17:11.099000: I runner.py:310] Step = 11900 ; steps/s = 1.63, tokens/s = 82615 (39163 source, 43452 target) ; Learning rate = 0.000810 ; Loss = 1.803069\n",
      "2024-12-04 12:18:12.511000: I runner.py:310] Step = 12000 ; steps/s = 1.63, tokens/s = 82716 (39216 source, 43500 target) ; Learning rate = 0.000807 ; Loss = 1.799499\n",
      "2024-12-04 12:19:13.970000: I runner.py:310] Step = 12100 ; steps/s = 1.63, tokens/s = 82676 (39212 source, 43464 target) ; Learning rate = 0.000803 ; Loss = 1.810373\n",
      "2024-12-04 12:20:14.800000: I runner.py:310] Step = 12200 ; steps/s = 1.64, tokens/s = 81928 (38822 source, 43106 target) ; Learning rate = 0.000800 ; Loss = 1.806882\n",
      "2024-12-04 12:21:16.193000: I runner.py:310] Step = 12300 ; steps/s = 1.63, tokens/s = 82735 (39223 source, 43512 target) ; Learning rate = 0.000797 ; Loss = 1.783543\n",
      "2024-12-04 12:22:17.711000: I runner.py:310] Step = 12400 ; steps/s = 1.63, tokens/s = 82603 (39192 source, 43411 target) ; Learning rate = 0.000794 ; Loss = 1.784038\n",
      "2024-12-04 12:23:19.205000: I runner.py:310] Step = 12500 ; steps/s = 1.63, tokens/s = 82620 (39170 source, 43450 target) ; Learning rate = 0.000791 ; Loss = 1.802157\n",
      "2024-12-04 12:24:20.105000: I runner.py:310] Step = 12600 ; steps/s = 1.64, tokens/s = 81806 (38750 source, 43056 target) ; Learning rate = 0.000787 ; Loss = 1.747939\n",
      "2024-12-04 12:25:21.525000: I runner.py:310] Step = 12700 ; steps/s = 1.63, tokens/s = 82746 (39244 source, 43502 target) ; Learning rate = 0.000784 ; Loss = 1.790278\n",
      "2024-12-04 12:26:23.046000: I runner.py:310] Step = 12800 ; steps/s = 1.63, tokens/s = 82560 (39162 source, 43398 target) ; Learning rate = 0.000781 ; Loss = 1.809696\n",
      "2024-12-04 12:27:24.468000: I runner.py:310] Step = 12900 ; steps/s = 1.63, tokens/s = 82488 (39098 source, 43390 target) ; Learning rate = 0.000778 ; Loss = 1.819767\n",
      "2024-12-04 12:28:25.413000: I runner.py:310] Step = 13000 ; steps/s = 1.64, tokens/s = 82020 (38846 source, 43174 target) ; Learning rate = 0.000775 ; Loss = 1.739919\n",
      "2024-12-04 12:29:26.904000: I runner.py:310] Step = 13100 ; steps/s = 1.63, tokens/s = 82601 (39158 source, 43443 target) ; Learning rate = 0.000772 ; Loss = 1.755413\n",
      "2024-12-04 12:30:28.342000: I runner.py:310] Step = 13200 ; steps/s = 1.63, tokens/s = 82703 (39223 source, 43480 target) ; Learning rate = 0.000769 ; Loss = 1.784935\n",
      "2024-12-04 12:31:29.189000: I runner.py:310] Step = 13300 ; steps/s = 1.64, tokens/s = 81876 (38845 source, 43031 target) ; Learning rate = 0.000766 ; Loss = 1.736796\n",
      "2024-12-04 12:32:30.683000: I runner.py:310] Step = 13400 ; steps/s = 1.63, tokens/s = 82629 (39158 source, 43471 target) ; Learning rate = 0.000764 ; Loss = 1.760826\n",
      "2024-12-04 12:33:32.210000: I runner.py:310] Step = 13500 ; steps/s = 1.63, tokens/s = 82511 (39094 source, 43417 target) ; Learning rate = 0.000761 ; Loss = 1.757269\n",
      "2024-12-04 12:34:33.704000: I runner.py:310] Step = 13600 ; steps/s = 1.63, tokens/s = 82639 (39209 source, 43430 target) ; Learning rate = 0.000758 ; Loss = 1.765118\n",
      "2024-12-04 12:35:34.635000: I runner.py:310] Step = 13700 ; steps/s = 1.64, tokens/s = 81790 (38769 source, 43021 target) ; Learning rate = 0.000755 ; Loss = 1.730909\n",
      "2024-12-04 12:36:36.136000: I runner.py:310] Step = 13800 ; steps/s = 1.63, tokens/s = 82601 (39141 source, 43460 target) ; Learning rate = 0.000752 ; Loss = 1.713265\n",
      "2024-12-04 12:37:37.561000: I runner.py:310] Step = 13900 ; steps/s = 1.63, tokens/s = 82712 (39229 source, 43483 target) ; Learning rate = 0.000750 ; Loss = 1.761996\n",
      "2024-12-04 12:38:39.075000: I runner.py:310] Step = 14000 ; steps/s = 1.63, tokens/s = 82585 (39156 source, 43429 target) ; Learning rate = 0.000747 ; Loss = 1.747572\n",
      "2024-12-04 12:39:39.922000: I runner.py:310] Step = 14100 ; steps/s = 1.64, tokens/s = 81913 (38843 source, 43070 target) ; Learning rate = 0.000744 ; Loss = 1.726164\n",
      "2024-12-04 12:40:41.376000: I runner.py:310] Step = 14200 ; steps/s = 1.63, tokens/s = 82688 (39174 source, 43514 target) ; Learning rate = 0.000742 ; Loss = 1.721552\n",
      "2024-12-04 12:41:42.825000: I runner.py:310] Step = 14300 ; steps/s = 1.63, tokens/s = 82689 (39201 source, 43488 target) ; Learning rate = 0.000739 ; Loss = 1.738348\n",
      "2024-12-04 12:42:44.274000: I runner.py:310] Step = 14400 ; steps/s = 1.63, tokens/s = 82663 (39220 source, 43443 target) ; Learning rate = 0.000737 ; Loss = 1.725115\n",
      "2024-12-04 12:43:45.170000: I runner.py:310] Step = 14500 ; steps/s = 1.64, tokens/s = 81825 (38789 source, 43036 target) ; Learning rate = 0.000734 ; Loss = 1.742207\n",
      "2024-12-04 12:44:46.624000: I runner.py:310] Step = 14600 ; steps/s = 1.63, tokens/s = 82666 (39194 source, 43472 target) ; Learning rate = 0.000731 ; Loss = 1.716292\n",
      "2024-12-04 12:45:48.073000: I runner.py:310] Step = 14700 ; steps/s = 1.63, tokens/s = 82672 (39188 source, 43484 target) ; Learning rate = 0.000729 ; Loss = 1.725404\n",
      "2024-12-04 12:46:49.538000: I runner.py:310] Step = 14800 ; steps/s = 1.63, tokens/s = 82635 (39188 source, 43447 target) ; Learning rate = 0.000727 ; Loss = 1.719396\n",
      "2024-12-04 12:47:50.503000: I runner.py:310] Step = 14900 ; steps/s = 1.64, tokens/s = 81747 (38756 source, 42991 target) ; Learning rate = 0.000724 ; Loss = 1.719987\n",
      "2024-12-04 12:48:51.913000: I runner.py:310] Step = 15000 ; steps/s = 1.63, tokens/s = 82739 (39214 source, 43525 target) ; Learning rate = 0.000722 ; Loss = 1.707445\n",
      "2024-12-04 12:48:51.914000: I training.py:192] Running evaluation for step 15000\n",
      "2024-12-04 12:53:48.634000: I training.py:192] Evaluation result for step 15000: loss = 1.087050 ; perplexity = 2.965513\n",
      "2024-12-04 12:54:49.975000: I runner.py:310] Step = 15100 ; steps/s = 1.63, tokens/s = 82820 (39268 source, 43552 target) ; Learning rate = 0.000719 ; Loss = 1.707718\n",
      "2024-12-04 12:55:51.440000: I runner.py:310] Step = 15200 ; steps/s = 1.63, tokens/s = 82644 (39185 source, 43459 target) ; Learning rate = 0.000717 ; Loss = 1.710935\n",
      "2024-12-04 12:56:52.352000: I runner.py:310] Step = 15300 ; steps/s = 1.64, tokens/s = 81802 (38782 source, 43020 target) ; Learning rate = 0.000715 ; Loss = 1.719967\n",
      "2024-12-04 12:57:53.920000: I runner.py:310] Step = 15400 ; steps/s = 1.62, tokens/s = 82549 (39142 source, 43407 target) ; Learning rate = 0.000712 ; Loss = 1.701686\n",
      "2024-12-04 12:58:55.421000: I runner.py:310] Step = 15500 ; steps/s = 1.63, tokens/s = 82590 (39154 source, 43436 target) ; Learning rate = 0.000710 ; Loss = 1.708483\n",
      "2024-12-04 12:59:56.921000: I runner.py:310] Step = 15600 ; steps/s = 1.63, tokens/s = 82603 (39155 source, 43448 target) ; Learning rate = 0.000708 ; Loss = 1.718422\n",
      "2024-12-04 13:00:57.802000: I runner.py:310] Step = 15700 ; steps/s = 1.64, tokens/s = 81865 (38817 source, 43048 target) ; Learning rate = 0.000705 ; Loss = 1.712472\n",
      "2024-12-04 13:01:59.375000: I runner.py:310] Step = 15800 ; steps/s = 1.62, tokens/s = 82505 (39124 source, 43381 target) ; Learning rate = 0.000703 ; Loss = 1.695895\n",
      "2024-12-04 13:03:00.922000: I runner.py:310] Step = 15900 ; steps/s = 1.62, tokens/s = 82510 (39119 source, 43391 target) ; Learning rate = 0.000701 ; Loss = 1.690472\n",
      "2024-12-04 13:04:02.394000: I runner.py:310] Step = 16000 ; steps/s = 1.63, tokens/s = 82654 (39186 source, 43468 target) ; Learning rate = 0.000699 ; Loss = 1.697717\n",
      "2024-12-04 13:05:03.308000: I runner.py:310] Step = 16100 ; steps/s = 1.64, tokens/s = 81808 (38795 source, 43013 target) ; Learning rate = 0.000697 ; Loss = 1.673929\n",
      "2024-12-04 13:06:04.774000: I runner.py:310] Step = 16200 ; steps/s = 1.63, tokens/s = 82644 (39184 source, 43460 target) ; Learning rate = 0.000694 ; Loss = 1.699627\n",
      "2024-12-04 13:07:06.306000: I runner.py:310] Step = 16300 ; steps/s = 1.63, tokens/s = 82581 (39126 source, 43455 target) ; Learning rate = 0.000692 ; Loss = 1.697050\n",
      "2024-12-04 13:08:07.831000: I runner.py:310] Step = 16400 ; steps/s = 1.63, tokens/s = 82570 (39142 source, 43428 target) ; Learning rate = 0.000690 ; Loss = 1.718686\n",
      "2024-12-04 13:09:08.764000: I runner.py:310] Step = 16500 ; steps/s = 1.64, tokens/s = 81822 (38803 source, 43019 target) ; Learning rate = 0.000688 ; Loss = 1.704212\n",
      "2024-12-04 13:10:10.244000: I runner.py:310] Step = 16600 ; steps/s = 1.63, tokens/s = 82625 (39170 source, 43455 target) ; Learning rate = 0.000686 ; Loss = 1.685081\n",
      "2024-12-04 13:11:11.722000: I runner.py:310] Step = 16700 ; steps/s = 1.63, tokens/s = 82624 (39168 source, 43456 target) ; Learning rate = 0.000684 ; Loss = 1.687620\n",
      "2024-12-04 13:12:13.203000: I runner.py:310] Step = 16800 ; steps/s = 1.63, tokens/s = 82629 (39186 source, 43443 target) ; Learning rate = 0.000682 ; Loss = 1.688977\n",
      "2024-12-04 13:13:14.128000: I runner.py:310] Step = 16900 ; steps/s = 1.64, tokens/s = 81794 (38771 source, 43023 target) ; Learning rate = 0.000680 ; Loss = 1.684023\n",
      "2024-12-04 13:14:15.568000: I runner.py:310] Step = 17000 ; steps/s = 1.63, tokens/s = 82679 (39179 source, 43500 target) ; Learning rate = 0.000678 ; Loss = 1.674416\n",
      "2024-12-04 13:15:17.093000: I runner.py:310] Step = 17100 ; steps/s = 1.63, tokens/s = 82593 (39166 source, 43427 target) ; Learning rate = 0.000676 ; Loss = 1.673744\n",
      "2024-12-04 13:16:18.638000: I runner.py:310] Step = 17200 ; steps/s = 1.62, tokens/s = 82515 (39134 source, 43381 target) ; Learning rate = 0.000674 ; Loss = 1.698192\n",
      "2024-12-04 13:17:19.502000: I runner.py:310] Step = 17300 ; steps/s = 1.64, tokens/s = 81851 (38764 source, 43087 target) ; Learning rate = 0.000672 ; Loss = 1.667620\n",
      "2024-12-04 13:18:20.959000: I runner.py:310] Step = 17400 ; steps/s = 1.63, tokens/s = 82647 (39181 source, 43466 target) ; Learning rate = 0.000670 ; Loss = 1.679207\n",
      "2024-12-04 13:19:22.488000: I runner.py:310] Step = 17500 ; steps/s = 1.63, tokens/s = 82597 (39179 source, 43418 target) ; Learning rate = 0.000668 ; Loss = 1.679683\n",
      "2024-12-04 13:20:23.462000: I runner.py:310] Step = 17600 ; steps/s = 1.64, tokens/s = 81732 (38769 source, 42963 target) ; Learning rate = 0.000666 ; Loss = 1.679162\n",
      "2024-12-04 13:21:24.967000: I runner.py:310] Step = 17700 ; steps/s = 1.63, tokens/s = 82616 (39169 source, 43447 target) ; Learning rate = 0.000664 ; Loss = 1.689499\n",
      "2024-12-04 13:22:27.327000: I runner.py:310] Step = 17800 ; steps/s = 1.60, tokens/s = 81501 (38627 source, 42874 target) ; Learning rate = 0.000662 ; Loss = 1.660392\n",
      "2024-12-04 13:23:30.806000: I runner.py:310] Step = 17900 ; steps/s = 1.58, tokens/s = 80029 (37933 source, 42096 target) ; Learning rate = 0.000661 ; Loss = 1.673903\n",
      "2024-12-04 13:24:32.995000: I runner.py:310] Step = 18000 ; steps/s = 1.61, tokens/s = 80085 (37991 source, 42094 target) ; Learning rate = 0.000659 ; Loss = 1.658691\n",
      "2024-12-04 13:25:34.574000: I runner.py:310] Step = 18100 ; steps/s = 1.62, tokens/s = 82534 (39131 source, 43403 target) ; Learning rate = 0.000657 ; Loss = 1.640938\n",
      "2024-12-04 13:26:36.055000: I runner.py:310] Step = 18200 ; steps/s = 1.63, tokens/s = 82639 (39177 source, 43462 target) ; Learning rate = 0.000655 ; Loss = 1.668745\n",
      "2024-12-04 13:27:37.506000: I runner.py:310] Step = 18300 ; steps/s = 1.63, tokens/s = 82646 (39161 source, 43485 target) ; Learning rate = 0.000653 ; Loss = 1.666380\n",
      "2024-12-04 13:28:38.491000: I runner.py:310] Step = 18400 ; steps/s = 1.64, tokens/s = 81723 (38756 source, 42967 target) ; Learning rate = 0.000652 ; Loss = 1.642801\n",
      "2024-12-04 13:29:39.980000: I runner.py:310] Step = 18500 ; steps/s = 1.63, tokens/s = 82623 (39165 source, 43458 target) ; Learning rate = 0.000650 ; Loss = 1.642511\n",
      "2024-12-04 13:30:41.514000: I runner.py:310] Step = 18600 ; steps/s = 1.63, tokens/s = 82531 (39166 source, 43365 target) ; Learning rate = 0.000648 ; Loss = 1.654260\n",
      "2024-12-04 13:31:43.029000: I runner.py:310] Step = 18700 ; steps/s = 1.63, tokens/s = 82589 (39143 source, 43446 target) ; Learning rate = 0.000646 ; Loss = 1.676248\n",
      "2024-12-04 13:32:43.924000: I runner.py:310] Step = 18800 ; steps/s = 1.64, tokens/s = 81837 (38785 source, 43052 target) ; Learning rate = 0.000645 ; Loss = 1.658251\n",
      "2024-12-04 13:33:45.396000: I runner.py:310] Step = 18900 ; steps/s = 1.63, tokens/s = 82662 (39207 source, 43455 target) ; Learning rate = 0.000643 ; Loss = 1.640849\n",
      "2024-12-04 13:34:46.871000: I runner.py:310] Step = 19000 ; steps/s = 1.63, tokens/s = 82629 (39166 source, 43463 target) ; Learning rate = 0.000641 ; Loss = 1.653695\n",
      "2024-12-04 13:35:48.353000: I runner.py:310] Step = 19100 ; steps/s = 1.63, tokens/s = 82638 (39176 source, 43462 target) ; Learning rate = 0.000640 ; Loss = 1.652232\n",
      "2024-12-04 13:36:49.299000: I runner.py:310] Step = 19200 ; steps/s = 1.64, tokens/s = 81770 (38757 source, 43013 target) ; Learning rate = 0.000638 ; Loss = 1.637348\n",
      "2024-12-04 13:37:50.760000: I runner.py:310] Step = 19300 ; steps/s = 1.63, tokens/s = 82594 (39160 source, 43434 target) ; Learning rate = 0.000636 ; Loss = 1.650902\n",
      "2024-12-04 13:38:52.294000: I runner.py:310] Step = 19400 ; steps/s = 1.63, tokens/s = 82617 (39187 source, 43430 target) ; Learning rate = 0.000635 ; Loss = 1.656803\n",
      "2024-12-04 13:39:53.815000: I runner.py:310] Step = 19500 ; steps/s = 1.63, tokens/s = 82581 (39136 source, 43445 target) ; Learning rate = 0.000633 ; Loss = 1.668130\n",
      "2024-12-04 13:40:54.742000: I runner.py:310] Step = 19600 ; steps/s = 1.64, tokens/s = 81798 (38768 source, 43030 target) ; Learning rate = 0.000631 ; Loss = 1.649834\n",
      "2024-12-04 13:41:56.305000: I runner.py:310] Step = 19700 ; steps/s = 1.62, tokens/s = 82516 (39123 source, 43393 target) ; Learning rate = 0.000630 ; Loss = 1.644682\n",
      "2024-12-04 13:42:57.737000: I runner.py:310] Step = 19800 ; steps/s = 1.63, tokens/s = 82716 (39232 source, 43484 target) ; Learning rate = 0.000628 ; Loss = 1.637473\n",
      "2024-12-04 13:43:59.192000: I runner.py:310] Step = 19900 ; steps/s = 1.63, tokens/s = 82637 (39161 source, 43476 target) ; Learning rate = 0.000627 ; Loss = 1.643496\n",
      "2024-12-04 13:45:00.165000: I runner.py:310] Step = 20000 ; steps/s = 1.64, tokens/s = 81739 (38757 source, 42982 target) ; Learning rate = 0.000625 ; Loss = 1.660663\n",
      "2024-12-04 13:45:02.413000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-20000\n",
      "2024-12-04 13:45:02.413000: I training.py:192] Running evaluation for step 20000\n",
      "2024-12-04 13:49:37.843000: I training.py:192] Evaluation result for step 20000: loss = 1.121050 ; perplexity = 3.068075\n",
      "2024-12-04 13:50:39.218000: I runner.py:310] Step = 20100 ; steps/s = 1.63, tokens/s = 82767 (39212 source, 43555 target) ; Learning rate = 0.000623 ; Loss = 1.627476\n",
      "2024-12-04 13:51:40.762000: I runner.py:310] Step = 20200 ; steps/s = 1.63, tokens/s = 82573 (39164 source, 43409 target) ; Learning rate = 0.000622 ; Loss = 1.642215\n",
      "2024-12-04 13:52:42.249000: I runner.py:310] Step = 20300 ; steps/s = 1.63, tokens/s = 82612 (39187 source, 43425 target) ; Learning rate = 0.000620 ; Loss = 1.641647\n",
      "2024-12-04 13:53:43.162000: I runner.py:310] Step = 20400 ; steps/s = 1.64, tokens/s = 81840 (38786 source, 43054 target) ; Learning rate = 0.000619 ; Loss = 1.641885\n",
      "2024-12-04 13:54:44.741000: I runner.py:310] Step = 20500 ; steps/s = 1.62, tokens/s = 82475 (39085 source, 43390 target) ; Learning rate = 0.000617 ; Loss = 1.626178\n",
      "2024-12-04 13:55:46.240000: I runner.py:310] Step = 20600 ; steps/s = 1.63, tokens/s = 82621 (39176 source, 43445 target) ; Learning rate = 0.000616 ; Loss = 1.645697\n",
      "2024-12-04 13:56:47.736000: I runner.py:310] Step = 20700 ; steps/s = 1.63, tokens/s = 82617 (39178 source, 43439 target) ; Learning rate = 0.000614 ; Loss = 1.643674\n",
      "2024-12-04 13:57:48.633000: I runner.py:310] Step = 20800 ; steps/s = 1.64, tokens/s = 81852 (38788 source, 43064 target) ; Learning rate = 0.000613 ; Loss = 1.621567\n",
      "2024-12-04 13:58:50.100000: I runner.py:310] Step = 20900 ; steps/s = 1.63, tokens/s = 82648 (39203 source, 43445 target) ; Learning rate = 0.000611 ; Loss = 1.623747\n",
      "2024-12-04 13:59:51.572000: I runner.py:310] Step = 21000 ; steps/s = 1.63, tokens/s = 82640 (39156 source, 43484 target) ; Learning rate = 0.000610 ; Loss = 1.641067\n",
      "2024-12-04 14:00:53.079000: I runner.py:310] Step = 21100 ; steps/s = 1.63, tokens/s = 82587 (39175 source, 43412 target) ; Learning rate = 0.000608 ; Loss = 1.645905\n",
      "2024-12-04 14:01:53.947000: I runner.py:310] Step = 21200 ; steps/s = 1.64, tokens/s = 81858 (38801 source, 43057 target) ; Learning rate = 0.000607 ; Loss = 1.634046\n",
      "2024-12-04 14:02:55.308000: I runner.py:310] Step = 21300 ; steps/s = 1.63, tokens/s = 82828 (39242 source, 43586 target) ; Learning rate = 0.000606 ; Loss = 1.629670\n",
      "2024-12-04 14:03:56.806000: I runner.py:310] Step = 21400 ; steps/s = 1.63, tokens/s = 82620 (39205 source, 43415 target) ; Learning rate = 0.000604 ; Loss = 1.626466\n",
      "2024-12-04 14:04:58.238000: I runner.py:310] Step = 21500 ; steps/s = 1.63, tokens/s = 82653 (39191 source, 43462 target) ; Learning rate = 0.000603 ; Loss = 1.637155\n",
      "2024-12-04 14:05:59.190000: I runner.py:310] Step = 21600 ; steps/s = 1.64, tokens/s = 81762 (38776 source, 42986 target) ; Learning rate = 0.000601 ; Loss = 1.613389\n",
      "2024-12-04 14:07:00.666000: I runner.py:310] Step = 21700 ; steps/s = 1.63, tokens/s = 82619 (39152 source, 43467 target) ; Learning rate = 0.000600 ; Loss = 1.630865\n",
      "2024-12-04 14:08:02.160000: I runner.py:310] Step = 21800 ; steps/s = 1.63, tokens/s = 82616 (39154 source, 43462 target) ; Learning rate = 0.000599 ; Loss = 1.631667\n",
      "2024-12-04 14:09:03.184000: I runner.py:310] Step = 21900 ; steps/s = 1.64, tokens/s = 81670 (38740 source, 42930 target) ; Learning rate = 0.000597 ; Loss = 1.628743\n",
      "2024-12-04 14:10:04.718000: I runner.py:310] Step = 22000 ; steps/s = 1.63, tokens/s = 82564 (39117 source, 43447 target) ; Learning rate = 0.000596 ; Loss = 1.620155\n",
      "2024-12-04 14:11:06.266000: I runner.py:310] Step = 22100 ; steps/s = 1.62, tokens/s = 82555 (39148 source, 43407 target) ; Learning rate = 0.000595 ; Loss = 1.616783\n",
      "2024-12-04 14:12:07.754000: I runner.py:310] Step = 22200 ; steps/s = 1.63, tokens/s = 82620 (39182 source, 43438 target) ; Learning rate = 0.000593 ; Loss = 1.616676\n",
      "2024-12-04 14:13:08.775000: I runner.py:310] Step = 22300 ; steps/s = 1.64, tokens/s = 81672 (38724 source, 42948 target) ; Learning rate = 0.000592 ; Loss = 1.627537\n",
      "2024-12-04 14:14:10.309000: I runner.py:310] Step = 22400 ; steps/s = 1.63, tokens/s = 82545 (39110 source, 43435 target) ; Learning rate = 0.000591 ; Loss = 1.621381\n",
      "2024-12-04 14:15:11.867000: I runner.py:310] Step = 22500 ; steps/s = 1.62, tokens/s = 82543 (39135 source, 43408 target) ; Learning rate = 0.000589 ; Loss = 1.618082\n",
      "2024-12-04 14:16:13.407000: I runner.py:310] Step = 22600 ; steps/s = 1.63, tokens/s = 82532 (39118 source, 43414 target) ; Learning rate = 0.000588 ; Loss = 1.630617\n",
      "2024-12-04 14:17:14.348000: I runner.py:310] Step = 22700 ; steps/s = 1.64, tokens/s = 81790 (38807 source, 42983 target) ; Learning rate = 0.000587 ; Loss = 1.618131\n",
      "2024-12-04 14:18:15.930000: I runner.py:310] Step = 22800 ; steps/s = 1.62, tokens/s = 82512 (39118 source, 43394 target) ; Learning rate = 0.000585 ; Loss = 1.607784\n",
      "2024-12-04 14:19:17.458000: I runner.py:310] Step = 22900 ; steps/s = 1.63, tokens/s = 82565 (39143 source, 43422 target) ; Learning rate = 0.000584 ; Loss = 1.612761\n",
      "2024-12-04 14:20:18.994000: I runner.py:310] Step = 23000 ; steps/s = 1.63, tokens/s = 82543 (39120 source, 43423 target) ; Learning rate = 0.000583 ; Loss = 1.616686\n",
      "2024-12-04 14:21:19.971000: I runner.py:310] Step = 23100 ; steps/s = 1.64, tokens/s = 81722 (38748 source, 42974 target) ; Learning rate = 0.000582 ; Loss = 1.601659\n",
      "2024-12-04 14:22:21.519000: I runner.py:310] Step = 23200 ; steps/s = 1.62, tokens/s = 82554 (39130 source, 43424 target) ; Learning rate = 0.000580 ; Loss = 1.610794\n",
      "2024-12-04 14:23:23.105000: I runner.py:310] Step = 23300 ; steps/s = 1.62, tokens/s = 82492 (39113 source, 43379 target) ; Learning rate = 0.000579 ; Loss = 1.599428\n",
      "2024-12-04 14:24:24.703000: I runner.py:310] Step = 23400 ; steps/s = 1.62, tokens/s = 82489 (39120 source, 43369 target) ; Learning rate = 0.000578 ; Loss = 1.613436\n",
      "2024-12-04 14:25:25.638000: I runner.py:310] Step = 23500 ; steps/s = 1.64, tokens/s = 81777 (38761 source, 43016 target) ; Learning rate = 0.000577 ; Loss = 1.615072\n",
      "2024-12-04 14:26:27.194000: I runner.py:310] Step = 23600 ; steps/s = 1.62, tokens/s = 82531 (39118 source, 43413 target) ; Learning rate = 0.000575 ; Loss = 1.603853\n",
      "2024-12-04 14:27:28.720000: I runner.py:310] Step = 23700 ; steps/s = 1.63, tokens/s = 82548 (39149 source, 43399 target) ; Learning rate = 0.000574 ; Loss = 1.610474\n",
      "2024-12-04 14:28:30.310000: I runner.py:310] Step = 23800 ; steps/s = 1.62, tokens/s = 82491 (39098 source, 43393 target) ; Learning rate = 0.000573 ; Loss = 1.603643\n",
      "2024-12-04 14:29:31.206000: I runner.py:310] Step = 23900 ; steps/s = 1.64, tokens/s = 81851 (38834 source, 43017 target) ; Learning rate = 0.000572 ; Loss = 1.619439\n",
      "2024-12-04 14:30:32.693000: I runner.py:310] Step = 24000 ; steps/s = 1.63, tokens/s = 82641 (39160 source, 43481 target) ; Learning rate = 0.000571 ; Loss = 1.593566\n",
      "2024-12-04 14:31:34.186000: I runner.py:310] Step = 24100 ; steps/s = 1.63, tokens/s = 82624 (39170 source, 43454 target) ; Learning rate = 0.000569 ; Loss = 1.600958\n",
      "2024-12-04 14:32:35.697000: I runner.py:310] Step = 24200 ; steps/s = 1.63, tokens/s = 82559 (39156 source, 43403 target) ; Learning rate = 0.000568 ; Loss = 1.604490\n",
      "2024-12-04 14:33:36.642000: I runner.py:310] Step = 24300 ; steps/s = 1.64, tokens/s = 81757 (38740 source, 43017 target) ; Learning rate = 0.000567 ; Loss = 1.588529\n",
      "2024-12-04 14:34:38.196000: I runner.py:310] Step = 24400 ; steps/s = 1.62, tokens/s = 82526 (39129 source, 43397 target) ; Learning rate = 0.000566 ; Loss = 1.600833\n",
      "2024-12-04 14:35:39.665000: I runner.py:310] Step = 24500 ; steps/s = 1.63, tokens/s = 82637 (39199 source, 43438 target) ; Learning rate = 0.000565 ; Loss = 1.609604\n",
      "2024-12-04 14:36:41.238000: I runner.py:310] Step = 24600 ; steps/s = 1.62, tokens/s = 82531 (39130 source, 43401 target) ; Learning rate = 0.000564 ; Loss = 1.610698\n",
      "2024-12-04 14:37:42.199000: I runner.py:310] Step = 24700 ; steps/s = 1.64, tokens/s = 81741 (38733 source, 43008 target) ; Learning rate = 0.000562 ; Loss = 1.617143\n",
      "2024-12-04 14:38:43.703000: I runner.py:310] Step = 24800 ; steps/s = 1.63, tokens/s = 82581 (39157 source, 43424 target) ; Learning rate = 0.000561 ; Loss = 1.608372\n",
      "2024-12-04 14:39:45.219000: I runner.py:310] Step = 24900 ; steps/s = 1.63, tokens/s = 82610 (39154 source, 43456 target) ; Learning rate = 0.000560 ; Loss = 1.595362\n",
      "2024-12-04 14:40:46.820000: I runner.py:310] Step = 25000 ; steps/s = 1.62, tokens/s = 82444 (39091 source, 43353 target) ; Learning rate = 0.000559 ; Loss = 1.603777\n",
      "2024-12-04 14:40:46.821000: I training.py:192] Running evaluation for step 25000\n",
      "2024-12-04 14:45:13.930000: I training.py:192] Evaluation result for step 25000: loss = 1.138952 ; perplexity = 3.123494\n",
      "2024-12-04 14:46:14.777000: I runner.py:310] Step = 25100 ; steps/s = 1.64, tokens/s = 81919 (38825 source, 43094 target) ; Learning rate = 0.000558 ; Loss = 1.578457\n",
      "2024-12-04 14:47:16.234000: I runner.py:310] Step = 25200 ; steps/s = 1.63, tokens/s = 82681 (39173 source, 43508 target) ; Learning rate = 0.000557 ; Loss = 1.607352\n",
      "2024-12-04 14:48:17.824000: I runner.py:310] Step = 25300 ; steps/s = 1.62, tokens/s = 82480 (39138 source, 43342 target) ; Learning rate = 0.000556 ; Loss = 1.611864\n",
      "2024-12-04 14:49:19.391000: I runner.py:310] Step = 25400 ; steps/s = 1.62, tokens/s = 82516 (39129 source, 43387 target) ; Learning rate = 0.000555 ; Loss = 1.613299\n",
      "2024-12-04 14:50:20.290000: I runner.py:310] Step = 25500 ; steps/s = 1.64, tokens/s = 81820 (38814 source, 43006 target) ; Learning rate = 0.000553 ; Loss = 1.578768\n",
      "2024-12-04 14:51:21.837000: I runner.py:310] Step = 25600 ; steps/s = 1.63, tokens/s = 82543 (39098 source, 43445 target) ; Learning rate = 0.000552 ; Loss = 1.594124\n",
      "2024-12-04 14:52:23.426000: I runner.py:310] Step = 25700 ; steps/s = 1.62, tokens/s = 82502 (39116 source, 43386 target) ; Learning rate = 0.000551 ; Loss = 1.604268\n",
      "2024-12-04 14:53:24.927000: I runner.py:310] Step = 25800 ; steps/s = 1.63, tokens/s = 82591 (39174 source, 43417 target) ; Learning rate = 0.000550 ; Loss = 1.609349\n",
      "2024-12-04 14:54:25.842000: I runner.py:310] Step = 25900 ; steps/s = 1.64, tokens/s = 81783 (38746 source, 43037 target) ; Learning rate = 0.000549 ; Loss = 1.594516\n",
      "2024-12-04 14:55:27.387000: I runner.py:310] Step = 26000 ; steps/s = 1.63, tokens/s = 82557 (39149 source, 43408 target) ; Learning rate = 0.000548 ; Loss = 1.596091\n",
      "2024-12-04 14:56:28.886000: I runner.py:310] Step = 26100 ; steps/s = 1.63, tokens/s = 82608 (39173 source, 43435 target) ; Learning rate = 0.000547 ; Loss = 1.593945\n",
      "2024-12-04 14:57:29.835000: I runner.py:310] Step = 26200 ; steps/s = 1.64, tokens/s = 81762 (38772 source, 42990 target) ; Learning rate = 0.000546 ; Loss = 1.592783\n",
      "2024-12-04 14:58:31.374000: I runner.py:310] Step = 26300 ; steps/s = 1.63, tokens/s = 82553 (39138 source, 43415 target) ; Learning rate = 0.000545 ; Loss = 1.578018\n",
      "2024-12-04 14:59:32.916000: I runner.py:310] Step = 26400 ; steps/s = 1.63, tokens/s = 82529 (39119 source, 43410 target) ; Learning rate = 0.000544 ; Loss = 1.591531\n",
      "2024-12-04 15:00:34.465000: I runner.py:310] Step = 26500 ; steps/s = 1.62, tokens/s = 82555 (39137 source, 43418 target) ; Learning rate = 0.000543 ; Loss = 1.586407\n",
      "2024-12-04 15:01:35.429000: I runner.py:310] Step = 26600 ; steps/s = 1.64, tokens/s = 81765 (38771 source, 42994 target) ; Learning rate = 0.000542 ; Loss = 1.586748\n",
      "2024-12-04 15:02:36.939000: I runner.py:310] Step = 26700 ; steps/s = 1.63, tokens/s = 82587 (39171 source, 43416 target) ; Learning rate = 0.000541 ; Loss = 1.587392\n",
      "2024-12-04 15:03:38.477000: I runner.py:310] Step = 26800 ; steps/s = 1.63, tokens/s = 82544 (39122 source, 43422 target) ; Learning rate = 0.000540 ; Loss = 1.593995\n",
      "2024-12-04 15:04:40.017000: I runner.py:310] Step = 26900 ; steps/s = 1.63, tokens/s = 82538 (39128 source, 43410 target) ; Learning rate = 0.000539 ; Loss = 1.598964\n",
      "2024-12-04 15:05:41.006000: I runner.py:310] Step = 27000 ; steps/s = 1.64, tokens/s = 81721 (38734 source, 42987 target) ; Learning rate = 0.000538 ; Loss = 1.589511\n",
      "2024-12-04 15:06:42.566000: I runner.py:310] Step = 27100 ; steps/s = 1.62, tokens/s = 82521 (39111 source, 43410 target) ; Learning rate = 0.000537 ; Loss = 1.569993\n",
      "2024-12-04 15:07:44.133000: I runner.py:310] Step = 27200 ; steps/s = 1.62, tokens/s = 82528 (39130 source, 43398 target) ; Learning rate = 0.000536 ; Loss = 1.592762\n",
      "2024-12-04 15:08:45.687000: I runner.py:310] Step = 27300 ; steps/s = 1.62, tokens/s = 82522 (39129 source, 43393 target) ; Learning rate = 0.000535 ; Loss = 1.602276\n",
      "2024-12-04 15:09:46.661000: I runner.py:310] Step = 27400 ; steps/s = 1.64, tokens/s = 81753 (38745 source, 43008 target) ; Learning rate = 0.000534 ; Loss = 1.588395\n",
      "2024-12-04 15:10:48.209000: I runner.py:310] Step = 27500 ; steps/s = 1.62, tokens/s = 82527 (39135 source, 43392 target) ; Learning rate = 0.000533 ; Loss = 1.581126\n",
      "2024-12-04 15:11:49.756000: I runner.py:310] Step = 27600 ; steps/s = 1.62, tokens/s = 82565 (39116 source, 43449 target) ; Learning rate = 0.000532 ; Loss = 1.578126\n",
      "2024-12-04 15:12:51.274000: I runner.py:310] Step = 27700 ; steps/s = 1.63, tokens/s = 82578 (39183 source, 43395 target) ; Learning rate = 0.000531 ; Loss = 1.589347\n",
      "2024-12-04 15:13:52.236000: I runner.py:310] Step = 27800 ; steps/s = 1.64, tokens/s = 81757 (38760 source, 42997 target) ; Learning rate = 0.000530 ; Loss = 1.577474\n",
      "2024-12-04 15:14:53.793000: I runner.py:310] Step = 27900 ; steps/s = 1.62, tokens/s = 82555 (39127 source, 43428 target) ; Learning rate = 0.000529 ; Loss = 1.577514\n",
      "2024-12-04 15:15:55.303000: I runner.py:310] Step = 28000 ; steps/s = 1.63, tokens/s = 82546 (39122 source, 43424 target) ; Learning rate = 0.000528 ; Loss = 1.575092\n",
      "2024-12-04 15:16:56.857000: I runner.py:310] Step = 28100 ; steps/s = 1.62, tokens/s = 82527 (39149 source, 43378 target) ; Learning rate = 0.000527 ; Loss = 1.587665\n",
      "2024-12-04 15:17:57.823000: I runner.py:310] Step = 28200 ; steps/s = 1.64, tokens/s = 81753 (38779 source, 42974 target) ; Learning rate = 0.000526 ; Loss = 1.576281\n",
      "2024-12-04 15:18:59.333000: I runner.py:310] Step = 28300 ; steps/s = 1.63, tokens/s = 82578 (39155 source, 43423 target) ; Learning rate = 0.000525 ; Loss = 1.581174\n",
      "2024-12-04 15:20:00.863000: I runner.py:310] Step = 28400 ; steps/s = 1.63, tokens/s = 82570 (39121 source, 43449 target) ; Learning rate = 0.000524 ; Loss = 1.590758\n",
      "2024-12-04 15:21:02.449000: I runner.py:310] Step = 28500 ; steps/s = 1.62, tokens/s = 82497 (39138 source, 43359 target) ; Learning rate = 0.000524 ; Loss = 1.584354\n",
      "2024-12-04 15:22:03.452000: I runner.py:310] Step = 28600 ; steps/s = 1.64, tokens/s = 81701 (38732 source, 42969 target) ; Learning rate = 0.000523 ; Loss = 1.575396\n",
      "2024-12-04 15:23:05.035000: I runner.py:310] Step = 28700 ; steps/s = 1.62, tokens/s = 82483 (39087 source, 43396 target) ; Learning rate = 0.000522 ; Loss = 1.586526\n",
      "2024-12-04 15:24:06.574000: I runner.py:310] Step = 28800 ; steps/s = 1.63, tokens/s = 82568 (39139 source, 43429 target) ; Learning rate = 0.000521 ; Loss = 1.590042\n",
      "2024-12-04 15:25:08.047000: I runner.py:310] Step = 28900 ; steps/s = 1.63, tokens/s = 82634 (39179 source, 43455 target) ; Learning rate = 0.000520 ; Loss = 1.587085\n",
      "2024-12-04 15:26:08.949000: I runner.py:310] Step = 29000 ; steps/s = 1.64, tokens/s = 81820 (38777 source, 43043 target) ; Learning rate = 0.000519 ; Loss = 1.578903\n",
      "2024-12-04 15:27:10.452000: I runner.py:310] Step = 29100 ; steps/s = 1.63, tokens/s = 82592 (39165 source, 43427 target) ; Learning rate = 0.000518 ; Loss = 1.576416\n",
      "2024-12-04 15:28:11.987000: I runner.py:310] Step = 29200 ; steps/s = 1.63, tokens/s = 82553 (39136 source, 43417 target) ; Learning rate = 0.000517 ; Loss = 1.569099\n",
      "2024-12-04 15:29:13.513000: I runner.py:310] Step = 29300 ; steps/s = 1.63, tokens/s = 82592 (39165 source, 43427 target) ; Learning rate = 0.000516 ; Loss = 1.583271\n",
      "2024-12-04 15:30:14.438000: I runner.py:310] Step = 29400 ; steps/s = 1.64, tokens/s = 81790 (38766 source, 43024 target) ; Learning rate = 0.000515 ; Loss = 1.559165\n",
      "2024-12-04 15:31:16.005000: I runner.py:310] Step = 29500 ; steps/s = 1.62, tokens/s = 82524 (39116 source, 43408 target) ; Learning rate = 0.000515 ; Loss = 1.577947\n",
      "2024-12-04 15:32:17.525000: I runner.py:310] Step = 29600 ; steps/s = 1.63, tokens/s = 82572 (39139 source, 43433 target) ; Learning rate = 0.000514 ; Loss = 1.587111\n",
      "2024-12-04 15:33:19.070000: I runner.py:310] Step = 29700 ; steps/s = 1.63, tokens/s = 82543 (39150 source, 43393 target) ; Learning rate = 0.000513 ; Loss = 1.586930\n",
      "2024-12-04 15:34:19.985000: I runner.py:310] Step = 29800 ; steps/s = 1.64, tokens/s = 81815 (38802 source, 43013 target) ; Learning rate = 0.000512 ; Loss = 1.573531\n",
      "2024-12-04 15:35:21.511000: I runner.py:310] Step = 29900 ; steps/s = 1.63, tokens/s = 82558 (39129 source, 43429 target) ; Learning rate = 0.000511 ; Loss = 1.573785\n",
      "2024-12-04 15:36:23.076000: I runner.py:310] Step = 30000 ; steps/s = 1.62, tokens/s = 82536 (39141 source, 43395 target) ; Learning rate = 0.000510 ; Loss = 1.567351\n",
      "2024-12-04 15:36:25.197000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-30000\n",
      "2024-12-04 15:36:25.197000: I training.py:192] Running evaluation for step 30000\n",
      "2024-12-04 15:40:59.496000: I training.py:192] Evaluation result for step 30000: loss = 1.163639 ; perplexity = 3.201562\n",
      "2024-12-04 15:42:00.862000: I runner.py:310] Step = 30100 ; steps/s = 1.63, tokens/s = 82789 (39256 source, 43533 target) ; Learning rate = 0.000509 ; Loss = 1.569842\n",
      "2024-12-04 15:43:01.890000: I runner.py:310] Step = 30200 ; steps/s = 1.64, tokens/s = 81640 (38672 source, 42968 target) ; Learning rate = 0.000509 ; Loss = 1.572606\n",
      "2024-12-04 15:44:03.523000: I runner.py:310] Step = 30300 ; steps/s = 1.62, tokens/s = 82414 (39086 source, 43328 target) ; Learning rate = 0.000508 ; Loss = 1.560665\n",
      "2024-12-04 15:45:05.043000: I runner.py:310] Step = 30400 ; steps/s = 1.63, tokens/s = 82581 (39164 source, 43417 target) ; Learning rate = 0.000507 ; Loss = 1.569842\n",
      "2024-12-04 15:46:06.090000: I runner.py:310] Step = 30500 ; steps/s = 1.64, tokens/s = 81635 (38706 source, 42929 target) ; Learning rate = 0.000506 ; Loss = 1.569141\n",
      "2024-12-04 15:47:07.611000: I runner.py:310] Step = 30600 ; steps/s = 1.63, tokens/s = 82595 (39128 source, 43467 target) ; Learning rate = 0.000505 ; Loss = 1.558862\n",
      "2024-12-04 15:48:09.128000: I runner.py:310] Step = 30700 ; steps/s = 1.63, tokens/s = 82562 (39147 source, 43415 target) ; Learning rate = 0.000504 ; Loss = 1.568370\n",
      "2024-12-04 15:49:10.697000: I runner.py:310] Step = 30800 ; steps/s = 1.62, tokens/s = 82518 (39124 source, 43394 target) ; Learning rate = 0.000504 ; Loss = 1.578201\n",
      "2024-12-04 15:50:11.627000: I runner.py:310] Step = 30900 ; steps/s = 1.64, tokens/s = 81792 (38794 source, 42998 target) ; Learning rate = 0.000503 ; Loss = 1.571771\n",
      "2024-12-04 15:51:13.096000: I runner.py:310] Step = 31000 ; steps/s = 1.63, tokens/s = 82657 (39199 source, 43458 target) ; Learning rate = 0.000502 ; Loss = 1.574772\n",
      "2024-12-04 15:52:14.614000: I runner.py:310] Step = 31100 ; steps/s = 1.63, tokens/s = 82583 (39166 source, 43417 target) ; Learning rate = 0.000501 ; Loss = 1.560299\n",
      "2024-12-04 15:53:16.126000: I runner.py:310] Step = 31200 ; steps/s = 1.63, tokens/s = 82577 (39127 source, 43450 target) ; Learning rate = 0.000500 ; Loss = 1.569977\n",
      "2024-12-04 15:54:17.109000: I runner.py:310] Step = 31300 ; steps/s = 1.64, tokens/s = 81705 (38727 source, 42978 target) ; Learning rate = 0.000500 ; Loss = 1.570852\n",
      "2024-12-04 15:55:18.580000: I runner.py:310] Step = 31400 ; steps/s = 1.63, tokens/s = 82658 (39202 source, 43456 target) ; Learning rate = 0.000499 ; Loss = 1.561525\n",
      "2024-12-04 15:56:20.077000: I runner.py:310] Step = 31500 ; steps/s = 1.63, tokens/s = 82609 (39168 source, 43441 target) ; Learning rate = 0.000498 ; Loss = 1.562591\n",
      "2024-12-04 15:57:21.675000: I runner.py:310] Step = 31600 ; steps/s = 1.62, tokens/s = 82466 (39094 source, 43372 target) ; Learning rate = 0.000497 ; Loss = 1.569889\n",
      "2024-12-04 15:58:22.665000: I runner.py:310] Step = 31700 ; steps/s = 1.64, tokens/s = 81702 (38715 source, 42987 target) ; Learning rate = 0.000496 ; Loss = 1.560521\n",
      "2024-12-04 15:59:24.224000: I runner.py:310] Step = 31800 ; steps/s = 1.62, tokens/s = 82540 (39140 source, 43400 target) ; Learning rate = 0.000496 ; Loss = 1.562755\n",
      "2024-12-04 16:00:25.763000: I runner.py:310] Step = 31900 ; steps/s = 1.63, tokens/s = 82529 (39110 source, 43419 target) ; Learning rate = 0.000495 ; Loss = 1.567378\n",
      "2024-12-04 16:01:27.287000: I runner.py:310] Step = 32000 ; steps/s = 1.63, tokens/s = 82581 (39173 source, 43408 target) ; Learning rate = 0.000494 ; Loss = 1.569255\n",
      "2024-12-04 16:02:28.210000: I runner.py:310] Step = 32100 ; steps/s = 1.64, tokens/s = 81820 (38770 source, 43050 target) ; Learning rate = 0.000493 ; Loss = 1.565282\n",
      "2024-12-04 16:03:29.763000: I runner.py:310] Step = 32200 ; steps/s = 1.62, tokens/s = 82542 (39129 source, 43413 target) ; Learning rate = 0.000493 ; Loss = 1.555144\n",
      "2024-12-04 16:04:31.281000: I runner.py:310] Step = 32300 ; steps/s = 1.63, tokens/s = 82567 (39154 source, 43413 target) ; Learning rate = 0.000492 ; Loss = 1.574375\n",
      "2024-12-04 16:05:32.837000: I runner.py:310] Step = 32400 ; steps/s = 1.62, tokens/s = 82531 (39146 source, 43385 target) ; Learning rate = 0.000491 ; Loss = 1.576627\n",
      "2024-12-04 16:06:33.823000: I runner.py:310] Step = 32500 ; steps/s = 1.64, tokens/s = 81693 (38724 source, 42969 target) ; Learning rate = 0.000490 ; Loss = 1.568131\n",
      "2024-12-04 16:07:35.408000: I runner.py:310] Step = 32600 ; steps/s = 1.62, tokens/s = 82498 (39102 source, 43396 target) ; Learning rate = 0.000490 ; Loss = 1.562377\n",
      "2024-12-04 16:08:36.935000: I runner.py:310] Step = 32700 ; steps/s = 1.63, tokens/s = 82556 (39156 source, 43400 target) ; Learning rate = 0.000489 ; Loss = 1.556463\n",
      "2024-12-04 16:09:38.402000: I runner.py:310] Step = 32800 ; steps/s = 1.63, tokens/s = 82670 (39192 source, 43478 target) ; Learning rate = 0.000488 ; Loss = 1.576121\n",
      "2024-12-04 16:10:39.384000: I runner.py:310] Step = 32900 ; steps/s = 1.64, tokens/s = 81726 (38743 source, 42983 target) ; Learning rate = 0.000487 ; Loss = 1.568649\n",
      "2024-12-04 16:11:40.887000: I runner.py:310] Step = 33000 ; steps/s = 1.63, tokens/s = 82572 (39148 source, 43424 target) ; Learning rate = 0.000487 ; Loss = 1.560806\n",
      "2024-12-04 16:12:42.420000: I runner.py:310] Step = 33100 ; steps/s = 1.63, tokens/s = 82567 (39153 source, 43414 target) ; Learning rate = 0.000486 ; Loss = 1.555248\n",
      "2024-12-04 16:13:43.875000: I runner.py:310] Step = 33200 ; steps/s = 1.63, tokens/s = 82681 (39189 source, 43492 target) ; Learning rate = 0.000485 ; Loss = 1.551049\n",
      "2024-12-04 16:14:44.810000: I runner.py:310] Step = 33300 ; steps/s = 1.64, tokens/s = 81785 (38773 source, 43012 target) ; Learning rate = 0.000484 ; Loss = 1.571075\n",
      "2024-12-04 16:15:46.270000: I runner.py:310] Step = 33400 ; steps/s = 1.63, tokens/s = 82673 (39184 source, 43489 target) ; Learning rate = 0.000484 ; Loss = 1.548592\n",
      "2024-12-04 16:16:47.750000: I runner.py:310] Step = 33500 ; steps/s = 1.63, tokens/s = 82672 (39202 source, 43470 target) ; Learning rate = 0.000483 ; Loss = 1.562249\n",
      "2024-12-04 16:17:49.256000: I runner.py:310] Step = 33600 ; steps/s = 1.63, tokens/s = 82569 (39148 source, 43421 target) ; Learning rate = 0.000482 ; Loss = 1.565747\n",
      "2024-12-04 16:18:50.239000: I runner.py:310] Step = 33700 ; steps/s = 1.64, tokens/s = 81679 (38727 source, 42952 target) ; Learning rate = 0.000481 ; Loss = 1.550292\n",
      "2024-12-04 16:19:51.768000: I runner.py:310] Step = 33800 ; steps/s = 1.63, tokens/s = 82562 (39120 source, 43442 target) ; Learning rate = 0.000481 ; Loss = 1.555491\n",
      "2024-12-04 16:20:53.312000: I runner.py:310] Step = 33900 ; steps/s = 1.63, tokens/s = 82592 (39161 source, 43431 target) ; Learning rate = 0.000480 ; Loss = 1.553141\n",
      "2024-12-04 16:21:54.798000: I runner.py:310] Step = 34000 ; steps/s = 1.63, tokens/s = 82614 (39192 source, 43422 target) ; Learning rate = 0.000479 ; Loss = 1.565968\n",
      "2024-12-04 16:22:55.770000: I runner.py:310] Step = 34100 ; steps/s = 1.64, tokens/s = 81721 (38712 source, 43009 target) ; Learning rate = 0.000479 ; Loss = 1.545149\n",
      "2024-12-04 16:23:57.313000: I runner.py:310] Step = 34200 ; steps/s = 1.63, tokens/s = 82546 (39138 source, 43408 target) ; Learning rate = 0.000478 ; Loss = 1.545949\n",
      "2024-12-04 16:24:58.873000: I runner.py:310] Step = 34300 ; steps/s = 1.62, tokens/s = 82511 (39129 source, 43382 target) ; Learning rate = 0.000477 ; Loss = 1.566901\n",
      "2024-12-04 16:26:00.285000: I runner.py:310] Step = 34400 ; steps/s = 1.63, tokens/s = 82753 (39236 source, 43517 target) ; Learning rate = 0.000477 ; Loss = 1.555592\n",
      "2024-12-04 16:27:01.301000: I runner.py:310] Step = 34500 ; steps/s = 1.64, tokens/s = 81634 (38693 source, 42941 target) ; Learning rate = 0.000476 ; Loss = 1.557297\n",
      "2024-12-04 16:28:02.882000: I runner.py:310] Step = 34600 ; steps/s = 1.62, tokens/s = 82538 (39095 source, 43443 target) ; Learning rate = 0.000475 ; Loss = 1.554862\n",
      "2024-12-04 16:29:04.349000: I runner.py:310] Step = 34700 ; steps/s = 1.63, tokens/s = 82661 (39227 source, 43434 target) ; Learning rate = 0.000474 ; Loss = 1.561302\n",
      "2024-12-04 16:30:05.289000: I runner.py:310] Step = 34800 ; steps/s = 1.64, tokens/s = 81738 (38767 source, 42971 target) ; Learning rate = 0.000474 ; Loss = 1.550887\n",
      "2024-12-04 16:31:06.825000: I runner.py:310] Step = 34900 ; steps/s = 1.63, tokens/s = 82598 (39141 source, 43457 target) ; Learning rate = 0.000473 ; Loss = 1.549211\n",
      "2024-12-04 16:32:08.328000: I runner.py:310] Step = 35000 ; steps/s = 1.63, tokens/s = 82603 (39206 source, 43397 target) ; Learning rate = 0.000472 ; Loss = 1.552963\n",
      "2024-12-04 16:32:08.330000: I training.py:192] Running evaluation for step 35000\n",
      "2024-12-04 16:36:34.936000: I training.py:192] Evaluation result for step 35000: loss = 1.177433 ; perplexity = 3.246031\n",
      "2024-12-04 16:37:36.328000: I runner.py:310] Step = 35100 ; steps/s = 1.63, tokens/s = 82760 (39242 source, 43518 target) ; Learning rate = 0.000472 ; Loss = 1.562274\n",
      "2024-12-04 16:38:37.239000: I runner.py:310] Step = 35200 ; steps/s = 1.64, tokens/s = 81786 (38732 source, 43054 target) ; Learning rate = 0.000471 ; Loss = 1.558861\n",
      "2024-12-04 16:39:38.739000: I runner.py:310] Step = 35300 ; steps/s = 1.63, tokens/s = 82607 (39159 source, 43448 target) ; Learning rate = 0.000470 ; Loss = 1.553033\n",
      "2024-12-04 16:40:40.232000: I runner.py:310] Step = 35400 ; steps/s = 1.63, tokens/s = 82625 (39205 source, 43420 target) ; Learning rate = 0.000470 ; Loss = 1.549435\n",
      "2024-12-04 16:41:41.869000: I runner.py:310] Step = 35500 ; steps/s = 1.62, tokens/s = 82438 (39058 source, 43380 target) ; Learning rate = 0.000469 ; Loss = 1.551230\n",
      "2024-12-04 16:42:42.752000: I runner.py:310] Step = 35600 ; steps/s = 1.64, tokens/s = 81849 (38804 source, 43045 target) ; Learning rate = 0.000468 ; Loss = 1.542275\n",
      "2024-12-04 16:43:44.259000: I runner.py:310] Step = 35700 ; steps/s = 1.63, tokens/s = 82577 (39116 source, 43461 target) ; Learning rate = 0.000468 ; Loss = 1.537718\n",
      "2024-12-04 16:44:45.828000: I runner.py:310] Step = 35800 ; steps/s = 1.62, tokens/s = 82513 (39115 source, 43398 target) ; Learning rate = 0.000467 ; Loss = 1.550721\n",
      "2024-12-04 16:45:47.290000: I runner.py:310] Step = 35900 ; steps/s = 1.63, tokens/s = 82638 (39207 source, 43431 target) ; Learning rate = 0.000466 ; Loss = 1.555539\n",
      "2024-12-04 16:46:48.179000: I runner.py:310] Step = 36000 ; steps/s = 1.64, tokens/s = 81859 (38810 source, 43049 target) ; Learning rate = 0.000466 ; Loss = 1.550975\n",
      "2024-12-04 16:47:49.699000: I runner.py:310] Step = 36100 ; steps/s = 1.63, tokens/s = 82574 (39128 source, 43446 target) ; Learning rate = 0.000465 ; Loss = 1.550476\n",
      "2024-12-04 16:48:51.171000: I runner.py:310] Step = 36200 ; steps/s = 1.63, tokens/s = 82646 (39160 source, 43486 target) ; Learning rate = 0.000465 ; Loss = 1.540985\n",
      "2024-12-04 16:49:52.649000: I runner.py:310] Step = 36300 ; steps/s = 1.63, tokens/s = 82627 (39201 source, 43426 target) ; Learning rate = 0.000464 ; Loss = 1.554030\n",
      "2024-12-04 16:50:53.522000: I runner.py:310] Step = 36400 ; steps/s = 1.64, tokens/s = 81883 (38836 source, 43047 target) ; Learning rate = 0.000463 ; Loss = 1.541331\n",
      "2024-12-04 16:51:54.999000: I runner.py:310] Step = 36500 ; steps/s = 1.63, tokens/s = 82634 (39176 source, 43458 target) ; Learning rate = 0.000463 ; Loss = 1.543758\n",
      "2024-12-04 16:52:56.490000: I runner.py:310] Step = 36600 ; steps/s = 1.63, tokens/s = 82604 (39184 source, 43420 target) ; Learning rate = 0.000462 ; Loss = 1.550841\n",
      "2024-12-04 16:53:57.982000: I runner.py:310] Step = 36700 ; steps/s = 1.63, tokens/s = 82613 (39140 source, 43473 target) ; Learning rate = 0.000461 ; Loss = 1.550963\n",
      "2024-12-04 16:54:58.958000: I runner.py:310] Step = 36800 ; steps/s = 1.64, tokens/s = 81762 (38792 source, 42970 target) ; Learning rate = 0.000461 ; Loss = 1.548191\n",
      "2024-12-04 16:56:00.380000: I runner.py:310] Step = 36900 ; steps/s = 1.63, tokens/s = 82722 (39210 source, 43512 target) ; Learning rate = 0.000460 ; Loss = 1.553603\n",
      "2024-12-04 16:57:01.845000: I runner.py:310] Step = 37000 ; steps/s = 1.63, tokens/s = 82635 (39180 source, 43455 target) ; Learning rate = 0.000460 ; Loss = 1.554248\n",
      "2024-12-04 16:58:03.303000: I runner.py:310] Step = 37100 ; steps/s = 1.63, tokens/s = 82653 (39181 source, 43472 target) ; Learning rate = 0.000459 ; Loss = 1.552352\n",
      "2024-12-04 16:59:04.212000: I runner.py:310] Step = 37200 ; steps/s = 1.64, tokens/s = 81805 (38790 source, 43015 target) ; Learning rate = 0.000458 ; Loss = 1.550858\n",
      "2024-12-04 17:00:05.687000: I runner.py:310] Step = 37300 ; steps/s = 1.63, tokens/s = 82683 (39178 source, 43505 target) ; Learning rate = 0.000458 ; Loss = 1.532593\n",
      "2024-12-04 17:01:07.139000: I runner.py:310] Step = 37400 ; steps/s = 1.63, tokens/s = 82651 (39190 source, 43461 target) ; Learning rate = 0.000457 ; Loss = 1.555921\n",
      "2024-12-04 17:02:08.670000: I runner.py:310] Step = 37500 ; steps/s = 1.63, tokens/s = 82537 (39127 source, 43410 target) ; Learning rate = 0.000456 ; Loss = 1.551597\n",
      "2024-12-04 17:03:09.656000: I runner.py:310] Step = 37600 ; steps/s = 1.64, tokens/s = 81750 (38760 source, 42990 target) ; Learning rate = 0.000456 ; Loss = 1.549770\n",
      "2024-12-04 17:04:11.154000: I runner.py:310] Step = 37700 ; steps/s = 1.63, tokens/s = 82602 (39144 source, 43458 target) ; Learning rate = 0.000455 ; Loss = 1.540722\n",
      "2024-12-04 17:05:12.633000: I runner.py:310] Step = 37800 ; steps/s = 1.63, tokens/s = 82631 (39188 source, 43443 target) ; Learning rate = 0.000455 ; Loss = 1.539640\n",
      "2024-12-04 17:06:14.134000: I runner.py:310] Step = 37900 ; steps/s = 1.63, tokens/s = 82589 (39159 source, 43430 target) ; Learning rate = 0.000454 ; Loss = 1.547475\n",
      "2024-12-04 17:07:15.073000: I runner.py:310] Step = 38000 ; steps/s = 1.64, tokens/s = 81765 (38762 source, 43003 target) ; Learning rate = 0.000453 ; Loss = 1.554274\n",
      "2024-12-04 17:08:16.526000: I runner.py:310] Step = 38100 ; steps/s = 1.63, tokens/s = 82659 (39202 source, 43457 target) ; Learning rate = 0.000453 ; Loss = 1.545898\n",
      "2024-12-04 17:09:18.027000: I runner.py:310] Step = 38200 ; steps/s = 1.63, tokens/s = 82596 (39174 source, 43422 target) ; Learning rate = 0.000452 ; Loss = 1.545261\n",
      "2024-12-04 17:10:19.553000: I runner.py:310] Step = 38300 ; steps/s = 1.63, tokens/s = 82595 (39140 source, 43455 target) ; Learning rate = 0.000452 ; Loss = 1.551554\n",
      "2024-12-04 17:11:20.422000: I runner.py:310] Step = 38400 ; steps/s = 1.64, tokens/s = 81883 (38814 source, 43069 target) ; Learning rate = 0.000451 ; Loss = 1.535813\n",
      "2024-12-04 17:12:21.921000: I runner.py:310] Step = 38500 ; steps/s = 1.63, tokens/s = 82628 (39160 source, 43468 target) ; Learning rate = 0.000450 ; Loss = 1.545243\n",
      "2024-12-04 17:13:23.440000: I runner.py:310] Step = 38600 ; steps/s = 1.63, tokens/s = 82564 (39170 source, 43394 target) ; Learning rate = 0.000450 ; Loss = 1.549033\n",
      "2024-12-04 17:14:24.942000: I runner.py:310] Step = 38700 ; steps/s = 1.63, tokens/s = 82595 (39147 source, 43448 target) ; Learning rate = 0.000449 ; Loss = 1.562747\n",
      "2024-12-04 17:15:25.846000: I runner.py:310] Step = 38800 ; steps/s = 1.64, tokens/s = 81803 (38757 source, 43046 target) ; Learning rate = 0.000449 ; Loss = 1.526094\n",
      "2024-12-04 17:16:27.410000: I runner.py:310] Step = 38900 ; steps/s = 1.62, tokens/s = 82520 (39148 source, 43372 target) ; Learning rate = 0.000448 ; Loss = 1.539410\n",
      "2024-12-04 17:17:28.810000: I runner.py:310] Step = 39000 ; steps/s = 1.63, tokens/s = 82764 (39246 source, 43518 target) ; Learning rate = 0.000448 ; Loss = 1.546132\n",
      "2024-12-04 17:18:29.731000: I runner.py:310] Step = 39100 ; steps/s = 1.64, tokens/s = 81778 (38772 source, 43006 target) ; Learning rate = 0.000447 ; Loss = 1.582614\n",
      "2024-12-04 17:19:31.240000: I runner.py:310] Step = 39200 ; steps/s = 1.63, tokens/s = 82621 (39161 source, 43460 target) ; Learning rate = 0.000446 ; Loss = 1.535535\n",
      "2024-12-04 17:20:32.731000: I runner.py:310] Step = 39300 ; steps/s = 1.63, tokens/s = 82614 (39176 source, 43438 target) ; Learning rate = 0.000446 ; Loss = 1.543361\n",
      "2024-12-04 17:21:34.217000: I runner.py:310] Step = 39400 ; steps/s = 1.63, tokens/s = 82611 (39156 source, 43455 target) ; Learning rate = 0.000445 ; Loss = 1.546041\n",
      "2024-12-04 17:22:35.157000: I runner.py:310] Step = 39500 ; steps/s = 1.64, tokens/s = 81761 (38760 source, 43001 target) ; Learning rate = 0.000445 ; Loss = 1.532299\n",
      "2024-12-04 17:23:36.608000: I runner.py:310] Step = 39600 ; steps/s = 1.63, tokens/s = 82668 (39196 source, 43472 target) ; Learning rate = 0.000444 ; Loss = 1.540081\n",
      "2024-12-04 17:24:38.000000: I runner.py:310] Step = 39700 ; steps/s = 1.63, tokens/s = 82738 (39207 source, 43531 target) ; Learning rate = 0.000444 ; Loss = 1.543379\n",
      "2024-12-04 17:25:39.410000: I runner.py:310] Step = 39800 ; steps/s = 1.63, tokens/s = 82730 (39216 source, 43514 target) ; Learning rate = 0.000443 ; Loss = 1.541351\n",
      "2024-12-04 17:26:40.335000: I runner.py:310] Step = 39900 ; steps/s = 1.64, tokens/s = 81816 (38803 source, 43013 target) ; Learning rate = 0.000442 ; Loss = 1.540231\n",
      "2024-12-04 17:27:41.844000: I runner.py:310] Step = 40000 ; steps/s = 1.63, tokens/s = 82597 (39115 source, 43482 target) ; Learning rate = 0.000442 ; Loss = 1.545258\n",
      "2024-12-04 17:27:43.224000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-40000\n",
      "2024-12-04 17:27:43.224000: I training.py:192] Running evaluation for step 40000\n",
      "2024-12-04 17:32:05.742000: I training.py:192] Evaluation result for step 40000: loss = 1.186023 ; perplexity = 3.274034\n",
      "2024-12-04 17:33:07.142000: I runner.py:310] Step = 40100 ; steps/s = 1.63, tokens/s = 82762 (39253 source, 43509 target) ; Learning rate = 0.000441 ; Loss = 1.534820\n",
      "2024-12-04 17:34:08.666000: I runner.py:310] Step = 40200 ; steps/s = 1.63, tokens/s = 82554 (39158 source, 43396 target) ; Learning rate = 0.000441 ; Loss = 1.532007\n",
      "2024-12-04 17:35:09.592000: I runner.py:310] Step = 40300 ; steps/s = 1.64, tokens/s = 81811 (38792 source, 43019 target) ; Learning rate = 0.000440 ; Loss = 1.543365\n",
      "2024-12-04 17:36:11.128000: I runner.py:310] Step = 40400 ; steps/s = 1.63, tokens/s = 82599 (39176 source, 43423 target) ; Learning rate = 0.000440 ; Loss = 1.539934\n",
      "2024-12-04 17:37:12.614000: I runner.py:310] Step = 40500 ; steps/s = 1.63, tokens/s = 82623 (39149 source, 43474 target) ; Learning rate = 0.000439 ; Loss = 1.537718\n",
      "2024-12-04 17:38:14.084000: I runner.py:310] Step = 40600 ; steps/s = 1.63, tokens/s = 82614 (39175 source, 43439 target) ; Learning rate = 0.000439 ; Loss = 1.537121\n",
      "2024-12-04 17:39:15.000000: I runner.py:310] Step = 40700 ; steps/s = 1.64, tokens/s = 81805 (38770 source, 43035 target) ; Learning rate = 0.000438 ; Loss = 1.548012\n",
      "2024-12-04 17:40:16.543000: I runner.py:310] Step = 40800 ; steps/s = 1.63, tokens/s = 82567 (39137 source, 43430 target) ; Learning rate = 0.000438 ; Loss = 1.534011\n",
      "2024-12-04 17:41:17.997000: I runner.py:310] Step = 40900 ; steps/s = 1.63, tokens/s = 82642 (39205 source, 43437 target) ; Learning rate = 0.000437 ; Loss = 1.539163\n",
      "2024-12-04 17:42:19.524000: I runner.py:310] Step = 41000 ; steps/s = 1.63, tokens/s = 82561 (39149 source, 43412 target) ; Learning rate = 0.000437 ; Loss = 1.542005\n",
      "2024-12-04 17:43:20.429000: I runner.py:310] Step = 41100 ; steps/s = 1.64, tokens/s = 81809 (38755 source, 43054 target) ; Learning rate = 0.000436 ; Loss = 1.540347\n",
      "2024-12-04 17:44:21.907000: I runner.py:310] Step = 41200 ; steps/s = 1.63, tokens/s = 82630 (39194 source, 43436 target) ; Learning rate = 0.000435 ; Loss = 1.535409\n",
      "2024-12-04 17:45:23.412000: I runner.py:310] Step = 41300 ; steps/s = 1.63, tokens/s = 82613 (39180 source, 43433 target) ; Learning rate = 0.000435 ; Loss = 1.534309\n",
      "2024-12-04 17:46:24.890000: I runner.py:310] Step = 41400 ; steps/s = 1.63, tokens/s = 82635 (39161 source, 43474 target) ; Learning rate = 0.000434 ; Loss = 1.552307\n",
      "2024-12-04 17:47:25.789000: I runner.py:310] Step = 41500 ; steps/s = 1.64, tokens/s = 81837 (38804 source, 43033 target) ; Learning rate = 0.000434 ; Loss = 1.538664\n",
      "2024-12-04 17:48:27.250000: I runner.py:310] Step = 41600 ; steps/s = 1.63, tokens/s = 82648 (39182 source, 43466 target) ; Learning rate = 0.000433 ; Loss = 1.540048\n",
      "2024-12-04 17:49:28.646000: I runner.py:310] Step = 41700 ; steps/s = 1.63, tokens/s = 82733 (39233 source, 43500 target) ; Learning rate = 0.000433 ; Loss = 1.532780\n",
      "2024-12-04 17:50:30.166000: I runner.py:310] Step = 41800 ; steps/s = 1.63, tokens/s = 82597 (39172 source, 43425 target) ; Learning rate = 0.000432 ; Loss = 1.537734\n",
      "2024-12-04 17:51:31.037000: I runner.py:310] Step = 41900 ; steps/s = 1.64, tokens/s = 81862 (38788 source, 43074 target) ; Learning rate = 0.000432 ; Loss = 1.543158\n",
      "2024-12-04 17:52:32.503000: I runner.py:310] Step = 42000 ; steps/s = 1.63, tokens/s = 82657 (39188 source, 43469 target) ; Learning rate = 0.000431 ; Loss = 1.533115\n",
      "2024-12-04 17:53:33.922000: I runner.py:310] Step = 42100 ; steps/s = 1.63, tokens/s = 82737 (39259 source, 43478 target) ; Learning rate = 0.000431 ; Loss = 1.528671\n",
      "2024-12-04 17:54:35.426000: I runner.py:310] Step = 42200 ; steps/s = 1.63, tokens/s = 82595 (39139 source, 43456 target) ; Learning rate = 0.000430 ; Loss = 1.536182\n",
      "2024-12-04 17:55:36.383000: I runner.py:310] Step = 42300 ; steps/s = 1.64, tokens/s = 81735 (38747 source, 42988 target) ; Learning rate = 0.000430 ; Loss = 1.537283\n",
      "2024-12-04 17:56:37.889000: I runner.py:310] Step = 42400 ; steps/s = 1.63, tokens/s = 82596 (39164 source, 43432 target) ; Learning rate = 0.000429 ; Loss = 1.531859\n",
      "2024-12-04 17:57:39.407000: I runner.py:310] Step = 42500 ; steps/s = 1.63, tokens/s = 82583 (39143 source, 43440 target) ; Learning rate = 0.000429 ; Loss = 1.530526\n",
      "2024-12-04 17:58:40.880000: I runner.py:310] Step = 42600 ; steps/s = 1.63, tokens/s = 82653 (39190 source, 43463 target) ; Learning rate = 0.000428 ; Loss = 1.528502\n",
      "2024-12-04 17:59:41.782000: I runner.py:310] Step = 42700 ; steps/s = 1.64, tokens/s = 81848 (38784 source, 43064 target) ; Learning rate = 0.000428 ; Loss = 1.545534\n",
      "2024-12-04 18:00:43.268000: I runner.py:310] Step = 42800 ; steps/s = 1.63, tokens/s = 82593 (39177 source, 43416 target) ; Learning rate = 0.000427 ; Loss = 1.527676\n",
      "2024-12-04 18:01:44.746000: I runner.py:310] Step = 42900 ; steps/s = 1.63, tokens/s = 82611 (39137 source, 43474 target) ; Learning rate = 0.000427 ; Loss = 1.540690\n",
      "2024-12-04 18:02:46.260000: I runner.py:310] Step = 43000 ; steps/s = 1.63, tokens/s = 82601 (39174 source, 43427 target) ; Learning rate = 0.000426 ; Loss = 1.538904\n",
      "2024-12-04 18:03:47.198000: I runner.py:310] Step = 43100 ; steps/s = 1.64, tokens/s = 81793 (38803 source, 42990 target) ; Learning rate = 0.000426 ; Loss = 1.538171\n",
      "2024-12-04 18:04:48.737000: I runner.py:310] Step = 43200 ; steps/s = 1.63, tokens/s = 82583 (39160 source, 43423 target) ; Learning rate = 0.000425 ; Loss = 1.526598\n",
      "2024-12-04 18:05:50.201000: I runner.py:310] Step = 43300 ; steps/s = 1.63, tokens/s = 82627 (39139 source, 43488 target) ; Learning rate = 0.000425 ; Loss = 1.531155\n",
      "2024-12-04 18:06:51.334000: I runner.py:310] Step = 43400 ; steps/s = 1.64, tokens/s = 82040 (38898 source, 43142 target) ; Learning rate = 0.000424 ; Loss = 1.535544\n",
      "2024-12-04 18:07:52.635000: I runner.py:310] Step = 43500 ; steps/s = 1.63, tokens/s = 82316 (39036 source, 43280 target) ; Learning rate = 0.000424 ; Loss = 1.543170\n",
      "2024-12-04 18:08:54.116000: I runner.py:310] Step = 43600 ; steps/s = 1.63, tokens/s = 82624 (39146 source, 43478 target) ; Learning rate = 0.000423 ; Loss = 1.526224\n",
      "2024-12-04 18:09:55.653000: I runner.py:310] Step = 43700 ; steps/s = 1.63, tokens/s = 82554 (39170 source, 43384 target) ; Learning rate = 0.000423 ; Loss = 1.528095\n",
      "2024-12-04 18:10:56.528000: I runner.py:310] Step = 43800 ; steps/s = 1.64, tokens/s = 81887 (38805 source, 43082 target) ; Learning rate = 0.000422 ; Loss = 1.542255\n",
      "2024-12-04 18:11:58.035000: I runner.py:310] Step = 43900 ; steps/s = 1.63, tokens/s = 82583 (39136 source, 43447 target) ; Learning rate = 0.000422 ; Loss = 1.533269\n",
      "2024-12-04 18:12:59.554000: I runner.py:310] Step = 44000 ; steps/s = 1.63, tokens/s = 82566 (39137 source, 43429 target) ; Learning rate = 0.000421 ; Loss = 1.529299\n",
      "2024-12-04 18:14:01.048000: I runner.py:310] Step = 44100 ; steps/s = 1.63, tokens/s = 82627 (39174 source, 43453 target) ; Learning rate = 0.000421 ; Loss = 1.538856\n",
      "2024-12-04 18:15:01.940000: I runner.py:310] Step = 44200 ; steps/s = 1.64, tokens/s = 81853 (38823 source, 43030 target) ; Learning rate = 0.000420 ; Loss = 1.525780\n",
      "2024-12-04 18:16:03.397000: I runner.py:310] Step = 44300 ; steps/s = 1.63, tokens/s = 82669 (39187 source, 43482 target) ; Learning rate = 0.000420 ; Loss = 1.521568\n",
      "2024-12-04 18:17:04.888000: I runner.py:310] Step = 44400 ; steps/s = 1.63, tokens/s = 82640 (39176 source, 43464 target) ; Learning rate = 0.000419 ; Loss = 1.538667\n",
      "2024-12-04 18:18:06.305000: I runner.py:310] Step = 44500 ; steps/s = 1.63, tokens/s = 82702 (39228 source, 43474 target) ; Learning rate = 0.000419 ; Loss = 1.539940\n",
      "2024-12-04 18:19:07.211000: I runner.py:310] Step = 44600 ; steps/s = 1.64, tokens/s = 81799 (38775 source, 43024 target) ; Learning rate = 0.000419 ; Loss = 1.526378\n",
      "2024-12-04 18:20:08.694000: I runner.py:310] Step = 44700 ; steps/s = 1.63, tokens/s = 82659 (39203 source, 43456 target) ; Learning rate = 0.000418 ; Loss = 1.528087\n",
      "2024-12-04 18:21:10.210000: I runner.py:310] Step = 44800 ; steps/s = 1.63, tokens/s = 82594 (39164 source, 43430 target) ; Learning rate = 0.000418 ; Loss = 1.531943\n",
      "2024-12-04 18:22:11.728000: I runner.py:310] Step = 44900 ; steps/s = 1.63, tokens/s = 82554 (39124 source, 43430 target) ; Learning rate = 0.000417 ; Loss = 1.544774\n",
      "2024-12-04 18:23:12.641000: I runner.py:310] Step = 45000 ; steps/s = 1.64, tokens/s = 81824 (38785 source, 43039 target) ; Learning rate = 0.000417 ; Loss = 1.516731\n",
      "2024-12-04 18:23:12.643000: I training.py:192] Running evaluation for step 45000\n",
      "2024-12-04 18:27:42.022000: I training.py:192] Evaluation result for step 45000: loss = 1.201167 ; perplexity = 3.323993\n",
      "2024-12-04 18:28:43.383000: I runner.py:310] Step = 45100 ; steps/s = 1.63, tokens/s = 82775 (39237 source, 43538 target) ; Learning rate = 0.000416 ; Loss = 1.533884\n",
      "2024-12-04 18:29:44.880000: I runner.py:310] Step = 45200 ; steps/s = 1.63, tokens/s = 82601 (39161 source, 43440 target) ; Learning rate = 0.000416 ; Loss = 1.536284\n",
      "2024-12-04 18:30:46.443000: I runner.py:310] Step = 45300 ; steps/s = 1.62, tokens/s = 82540 (39154 source, 43386 target) ; Learning rate = 0.000415 ; Loss = 1.537062\n",
      "2024-12-04 18:31:47.337000: I runner.py:310] Step = 45400 ; steps/s = 1.64, tokens/s = 81850 (38796 source, 43054 target) ; Learning rate = 0.000415 ; Loss = 1.540941\n",
      "2024-12-04 18:32:48.823000: I runner.py:310] Step = 45500 ; steps/s = 1.63, tokens/s = 82623 (39161 source, 43462 target) ; Learning rate = 0.000414 ; Loss = 1.534629\n",
      "2024-12-04 18:33:50.242000: I runner.py:310] Step = 45600 ; steps/s = 1.63, tokens/s = 82691 (39199 source, 43492 target) ; Learning rate = 0.000414 ; Loss = 1.524129\n",
      "2024-12-04 18:34:51.718000: I runner.py:310] Step = 45700 ; steps/s = 1.63, tokens/s = 82648 (39201 source, 43447 target) ; Learning rate = 0.000413 ; Loss = 1.530655\n",
      "2024-12-04 18:35:52.619000: I runner.py:310] Step = 45800 ; steps/s = 1.64, tokens/s = 81862 (38812 source, 43050 target) ; Learning rate = 0.000413 ; Loss = 1.529121\n",
      "2024-12-04 18:36:54.127000: I runner.py:310] Step = 45900 ; steps/s = 1.63, tokens/s = 82593 (39138 source, 43455 target) ; Learning rate = 0.000413 ; Loss = 1.524732\n",
      "2024-12-04 18:37:55.657000: I runner.py:310] Step = 46000 ; steps/s = 1.63, tokens/s = 82556 (39151 source, 43405 target) ; Learning rate = 0.000412 ; Loss = 1.535690\n",
      "2024-12-04 18:38:57.069000: I runner.py:310] Step = 46100 ; steps/s = 1.63, tokens/s = 82724 (39228 source, 43496 target) ; Learning rate = 0.000412 ; Loss = 1.533436\n",
      "2024-12-04 18:39:58.012000: I runner.py:310] Step = 46200 ; steps/s = 1.64, tokens/s = 81772 (38756 source, 43016 target) ; Learning rate = 0.000411 ; Loss = 1.544010\n",
      "2024-12-04 18:40:59.600000: I runner.py:310] Step = 46300 ; steps/s = 1.62, tokens/s = 82458 (39104 source, 43354 target) ; Learning rate = 0.000411 ; Loss = 1.522215\n",
      "2024-12-04 18:42:01.012000: I runner.py:310] Step = 46400 ; steps/s = 1.63, tokens/s = 82726 (39228 source, 43498 target) ; Learning rate = 0.000410 ; Loss = 1.523197\n",
      "2024-12-04 18:43:02.420000: I runner.py:310] Step = 46500 ; steps/s = 1.63, tokens/s = 82734 (39220 source, 43514 target) ; Learning rate = 0.000410 ; Loss = 1.525790\n",
      "2024-12-04 18:44:03.337000: I runner.py:310] Step = 46600 ; steps/s = 1.64, tokens/s = 81799 (38759 source, 43040 target) ; Learning rate = 0.000409 ; Loss = 1.519683\n",
      "2024-12-04 18:45:04.847000: I runner.py:310] Step = 46700 ; steps/s = 1.63, tokens/s = 82589 (39150 source, 43439 target) ; Learning rate = 0.000409 ; Loss = 1.537035\n",
      "2024-12-04 18:46:06.316000: I runner.py:310] Step = 46800 ; steps/s = 1.63, tokens/s = 82659 (39191 source, 43468 target) ; Learning rate = 0.000409 ; Loss = 1.535304\n",
      "2024-12-04 18:47:07.778000: I runner.py:310] Step = 46900 ; steps/s = 1.63, tokens/s = 82666 (39216 source, 43450 target) ; Learning rate = 0.000408 ; Loss = 1.534942\n",
      "2024-12-04 18:48:08.663000: I runner.py:310] Step = 47000 ; steps/s = 1.64, tokens/s = 81837 (38769 source, 43068 target) ; Learning rate = 0.000408 ; Loss = 1.532986\n",
      "2024-12-04 18:49:10.149000: I runner.py:310] Step = 47100 ; steps/s = 1.63, tokens/s = 82605 (39155 source, 43450 target) ; Learning rate = 0.000407 ; Loss = 1.527179\n",
      "2024-12-04 18:50:11.641000: I runner.py:310] Step = 47200 ; steps/s = 1.63, tokens/s = 82630 (39171 source, 43459 target) ; Learning rate = 0.000407 ; Loss = 1.523082\n",
      "2024-12-04 18:51:13.160000: I runner.py:310] Step = 47300 ; steps/s = 1.63, tokens/s = 82585 (39180 source, 43405 target) ; Learning rate = 0.000406 ; Loss = 1.524573\n",
      "2024-12-04 18:52:14.085000: I runner.py:310] Step = 47400 ; steps/s = 1.64, tokens/s = 81785 (38791 source, 42994 target) ; Learning rate = 0.000406 ; Loss = 1.518571\n",
      "2024-12-04 18:53:15.580000: I runner.py:310] Step = 47500 ; steps/s = 1.63, tokens/s = 82630 (39152 source, 43478 target) ; Learning rate = 0.000406 ; Loss = 1.525317\n",
      "2024-12-04 18:54:17.058000: I runner.py:310] Step = 47600 ; steps/s = 1.63, tokens/s = 82640 (39191 source, 43449 target) ; Learning rate = 0.000405 ; Loss = 1.531211\n",
      "2024-12-04 18:55:18.321000: I runner.py:310] Step = 47700 ; steps/s = 1.63, tokens/s = 82306 (39012 source, 43294 target) ; Learning rate = 0.000405 ; Loss = 1.533069\n",
      "2024-12-04 18:56:19.444000: I runner.py:310] Step = 47800 ; steps/s = 1.64, tokens/s = 82164 (38975 source, 43189 target) ; Learning rate = 0.000404 ; Loss = 1.520117\n",
      "2024-12-04 18:57:20.972000: I runner.py:310] Step = 47900 ; steps/s = 1.63, tokens/s = 82528 (39140 source, 43388 target) ; Learning rate = 0.000404 ; Loss = 1.525808\n",
      "2024-12-04 18:58:22.437000: I runner.py:310] Step = 48000 ; steps/s = 1.63, tokens/s = 82632 (39160 source, 43472 target) ; Learning rate = 0.000403 ; Loss = 1.531201\n",
      "2024-12-04 18:59:23.399000: I runner.py:310] Step = 48100 ; steps/s = 1.64, tokens/s = 81777 (38758 source, 43019 target) ; Learning rate = 0.000403 ; Loss = 1.525062\n",
      "2024-12-04 19:00:24.838000: I runner.py:310] Step = 48200 ; steps/s = 1.63, tokens/s = 82668 (39192 source, 43476 target) ; Learning rate = 0.000403 ; Loss = 1.515789\n",
      "2024-12-04 19:01:26.384000: I runner.py:310] Step = 48300 ; steps/s = 1.63, tokens/s = 82551 (39127 source, 43424 target) ; Learning rate = 0.000402 ; Loss = 1.527169\n",
      "2024-12-04 19:02:27.868000: I runner.py:310] Step = 48400 ; steps/s = 1.63, tokens/s = 82646 (39185 source, 43461 target) ; Learning rate = 0.000402 ; Loss = 1.531477\n",
      "2024-12-04 19:03:28.837000: I runner.py:310] Step = 48500 ; steps/s = 1.64, tokens/s = 81741 (38765 source, 42976 target) ; Learning rate = 0.000401 ; Loss = 1.524486\n",
      "2024-12-04 19:04:30.273000: I runner.py:310] Step = 48600 ; steps/s = 1.63, tokens/s = 82731 (39214 source, 43517 target) ; Learning rate = 0.000401 ; Loss = 1.531886\n",
      "2024-12-04 19:05:31.769000: I runner.py:310] Step = 48700 ; steps/s = 1.63, tokens/s = 82591 (39161 source, 43430 target) ; Learning rate = 0.000401 ; Loss = 1.520826\n",
      "2024-12-04 19:06:33.261000: I runner.py:310] Step = 48800 ; steps/s = 1.63, tokens/s = 82612 (39179 source, 43433 target) ; Learning rate = 0.000400 ; Loss = 1.520897\n",
      "2024-12-04 19:07:34.217000: I runner.py:310] Step = 48900 ; steps/s = 1.64, tokens/s = 81754 (38739 source, 43015 target) ; Learning rate = 0.000400 ; Loss = 1.518896\n",
      "2024-12-04 19:08:35.682000: I runner.py:310] Step = 49000 ; steps/s = 1.63, tokens/s = 82643 (39162 source, 43481 target) ; Learning rate = 0.000399 ; Loss = 1.518472\n",
      "2024-12-04 19:09:37.138000: I runner.py:310] Step = 49100 ; steps/s = 1.63, tokens/s = 82688 (39184 source, 43504 target) ; Learning rate = 0.000399 ; Loss = 1.528036\n",
      "2024-12-04 19:10:38.650000: I runner.py:310] Step = 49200 ; steps/s = 1.63, tokens/s = 82575 (39182 source, 43393 target) ; Learning rate = 0.000398 ; Loss = 1.520956\n",
      "2024-12-04 19:11:39.577000: I runner.py:310] Step = 49300 ; steps/s = 1.64, tokens/s = 81771 (38765 source, 43006 target) ; Learning rate = 0.000398 ; Loss = 1.520078\n",
      "2024-12-04 19:12:41.016000: I runner.py:310] Step = 49400 ; steps/s = 1.63, tokens/s = 82684 (39171 source, 43513 target) ; Learning rate = 0.000398 ; Loss = 1.517913\n",
      "2024-12-04 19:13:42.496000: I runner.py:310] Step = 49500 ; steps/s = 1.63, tokens/s = 82626 (39190 source, 43436 target) ; Learning rate = 0.000397 ; Loss = 1.526975\n",
      "2024-12-04 19:14:44.028000: I runner.py:310] Step = 49600 ; steps/s = 1.63, tokens/s = 82563 (39167 source, 43396 target) ; Learning rate = 0.000397 ; Loss = 1.538534\n",
      "2024-12-04 19:15:44.965000: I runner.py:310] Step = 49700 ; steps/s = 1.64, tokens/s = 81784 (38764 source, 43020 target) ; Learning rate = 0.000396 ; Loss = 1.529420\n",
      "2024-12-04 19:16:46.511000: I runner.py:310] Step = 49800 ; steps/s = 1.62, tokens/s = 82544 (39099 source, 43445 target) ; Learning rate = 0.000396 ; Loss = 1.511615\n",
      "2024-12-04 19:17:47.988000: I runner.py:310] Step = 49900 ; steps/s = 1.63, tokens/s = 82647 (39177 source, 43470 target) ; Learning rate = 0.000396 ; Loss = 1.519870\n",
      "2024-12-04 19:18:49.435000: I runner.py:310] Step = 50000 ; steps/s = 1.63, tokens/s = 82682 (39220 source, 43462 target) ; Learning rate = 0.000395 ; Loss = 1.520285\n",
      "2024-12-04 19:18:50.978000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-50000\n",
      "2024-12-04 19:18:50.978000: I training.py:192] Running evaluation for step 50000\n",
      "2024-12-04 19:23:20.047000: I training.py:192] Evaluation result for step 50000: loss = 1.212186 ; perplexity = 3.360822\n",
      "2024-12-04 19:24:20.919000: I runner.py:310] Step = 50100 ; steps/s = 1.64, tokens/s = 81872 (38821 source, 43051 target) ; Learning rate = 0.000395 ; Loss = 1.515303\n",
      "2024-12-04 19:25:22.406000: I runner.py:310] Step = 50200 ; steps/s = 1.63, tokens/s = 82634 (39197 source, 43437 target) ; Learning rate = 0.000394 ; Loss = 1.529475\n",
      "2024-12-04 19:26:23.931000: I runner.py:310] Step = 50300 ; steps/s = 1.63, tokens/s = 82583 (39145 source, 43438 target) ; Learning rate = 0.000394 ; Loss = 1.523211\n",
      "2024-12-04 19:27:25.389000: I runner.py:310] Step = 50400 ; steps/s = 1.63, tokens/s = 82629 (39174 source, 43455 target) ; Learning rate = 0.000394 ; Loss = 1.529641\n",
      "2024-12-04 19:28:26.297000: I runner.py:310] Step = 50500 ; steps/s = 1.64, tokens/s = 81832 (38801 source, 43031 target) ; Learning rate = 0.000393 ; Loss = 1.528365\n",
      "2024-12-04 19:29:27.761000: I runner.py:310] Step = 50600 ; steps/s = 1.63, tokens/s = 82661 (39188 source, 43473 target) ; Learning rate = 0.000393 ; Loss = 1.516914\n",
      "2024-12-04 19:30:29.221000: I runner.py:310] Step = 50700 ; steps/s = 1.63, tokens/s = 82640 (39185 source, 43455 target) ; Learning rate = 0.000393 ; Loss = 1.519409\n",
      "2024-12-04 19:31:30.733000: I runner.py:310] Step = 50800 ; steps/s = 1.63, tokens/s = 82610 (39165 source, 43445 target) ; Learning rate = 0.000392 ; Loss = 1.518010\n",
      "2024-12-04 19:32:31.662000: I runner.py:310] Step = 50900 ; steps/s = 1.64, tokens/s = 81780 (38767 source, 43013 target) ; Learning rate = 0.000392 ; Loss = 1.511755\n",
      "2024-12-04 19:33:33.181000: I runner.py:310] Step = 51000 ; steps/s = 1.63, tokens/s = 82575 (39114 source, 43461 target) ; Learning rate = 0.000391 ; Loss = 1.517874\n",
      "2024-12-04 19:34:34.663000: I runner.py:310] Step = 51100 ; steps/s = 1.63, tokens/s = 82631 (39207 source, 43424 target) ; Learning rate = 0.000391 ; Loss = 1.520723\n",
      "2024-12-04 19:35:36.124000: I runner.py:310] Step = 51200 ; steps/s = 1.63, tokens/s = 82644 (39183 source, 43461 target) ; Learning rate = 0.000391 ; Loss = 1.523430\n",
      "2024-12-04 19:36:37.057000: I runner.py:310] Step = 51300 ; steps/s = 1.64, tokens/s = 81815 (38786 source, 43029 target) ; Learning rate = 0.000390 ; Loss = 1.522950\n",
      "2024-12-04 19:37:38.457000: I runner.py:310] Step = 51400 ; steps/s = 1.63, tokens/s = 82757 (39230 source, 43527 target) ; Learning rate = 0.000390 ; Loss = 1.511371\n",
      "2024-12-04 19:38:39.902000: I runner.py:310] Step = 51500 ; steps/s = 1.63, tokens/s = 82661 (39203 source, 43458 target) ; Learning rate = 0.000389 ; Loss = 1.509075\n",
      "2024-12-04 19:39:41.399000: I runner.py:310] Step = 51600 ; steps/s = 1.63, tokens/s = 82622 (39161 source, 43461 target) ; Learning rate = 0.000389 ; Loss = 1.520697\n",
      "2024-12-04 19:40:42.356000: I runner.py:310] Step = 51700 ; steps/s = 1.64, tokens/s = 81712 (38723 source, 42989 target) ; Learning rate = 0.000389 ; Loss = 1.515241\n",
      "2024-12-04 19:41:43.886000: I runner.py:310] Step = 51800 ; steps/s = 1.63, tokens/s = 82553 (39120 source, 43433 target) ; Learning rate = 0.000388 ; Loss = 1.517338\n",
      "2024-12-04 19:42:45.340000: I runner.py:310] Step = 51900 ; steps/s = 1.63, tokens/s = 82679 (39221 source, 43458 target) ; Learning rate = 0.000388 ; Loss = 1.529710\n",
      "2024-12-04 19:43:46.762000: I runner.py:310] Step = 52000 ; steps/s = 1.63, tokens/s = 82498 (39126 source, 43372 target) ; Learning rate = 0.000388 ; Loss = 1.533206\n",
      "2024-12-04 19:44:47.760000: I runner.py:310] Step = 52100 ; steps/s = 1.64, tokens/s = 81910 (38834 source, 43076 target) ; Learning rate = 0.000387 ; Loss = 1.512518\n",
      "2024-12-04 19:45:49.336000: I runner.py:310] Step = 52200 ; steps/s = 1.62, tokens/s = 82534 (39124 source, 43410 target) ; Learning rate = 0.000387 ; Loss = 1.521290\n",
      "2024-12-04 19:46:50.794000: I runner.py:310] Step = 52300 ; steps/s = 1.63, tokens/s = 82646 (39180 source, 43466 target) ; Learning rate = 0.000386 ; Loss = 1.524810\n",
      "2024-12-04 19:47:51.700000: I runner.py:310] Step = 52400 ; steps/s = 1.64, tokens/s = 81810 (38792 source, 43018 target) ; Learning rate = 0.000386 ; Loss = 1.518621\n",
      "2024-12-04 19:48:53.162000: I runner.py:310] Step = 52500 ; steps/s = 1.63, tokens/s = 82636 (39176 source, 43460 target) ; Learning rate = 0.000386 ; Loss = 1.503728\n",
      "2024-12-04 19:49:54.650000: I runner.py:310] Step = 52600 ; steps/s = 1.63, tokens/s = 82627 (39159 source, 43468 target) ; Learning rate = 0.000385 ; Loss = 1.526540\n",
      "2024-12-04 19:50:56.075000: I runner.py:310] Step = 52700 ; steps/s = 1.63, tokens/s = 82687 (39211 source, 43476 target) ; Learning rate = 0.000385 ; Loss = 1.534015\n",
      "2024-12-04 19:51:56.946000: I runner.py:310] Step = 52800 ; steps/s = 1.64, tokens/s = 81907 (38834 source, 43073 target) ; Learning rate = 0.000385 ; Loss = 1.516498\n",
      "2024-12-04 19:52:58.433000: I runner.py:310] Step = 52900 ; steps/s = 1.63, tokens/s = 82649 (39185 source, 43464 target) ; Learning rate = 0.000384 ; Loss = 1.507125\n",
      "2024-12-04 19:53:59.951000: I runner.py:310] Step = 53000 ; steps/s = 1.63, tokens/s = 82562 (39142 source, 43420 target) ; Learning rate = 0.000384 ; Loss = 1.513882\n",
      "2024-12-04 19:55:01.422000: I runner.py:310] Step = 53100 ; steps/s = 1.63, tokens/s = 82642 (39180 source, 43462 target) ; Learning rate = 0.000384 ; Loss = 1.519311\n",
      "2024-12-04 19:56:02.368000: I runner.py:310] Step = 53200 ; steps/s = 1.64, tokens/s = 81769 (38760 source, 43009 target) ; Learning rate = 0.000383 ; Loss = 1.513801\n",
      "2024-12-04 19:57:03.828000: I runner.py:310] Step = 53300 ; steps/s = 1.63, tokens/s = 82623 (39171 source, 43452 target) ; Learning rate = 0.000383 ; Loss = 1.509251\n",
      "2024-12-04 19:58:05.303000: I runner.py:310] Step = 53400 ; steps/s = 1.63, tokens/s = 82662 (39182 source, 43480 target) ; Learning rate = 0.000382 ; Loss = 1.516621\n",
      "2024-12-04 19:59:06.798000: I runner.py:310] Step = 53500 ; steps/s = 1.63, tokens/s = 82625 (39190 source, 43435 target) ; Learning rate = 0.000382 ; Loss = 1.524143\n",
      "2024-12-04 20:00:07.719000: I runner.py:310] Step = 53600 ; steps/s = 1.64, tokens/s = 81791 (38759 source, 43032 target) ; Learning rate = 0.000382 ; Loss = 1.512178\n",
      "2024-12-04 20:01:09.165000: I runner.py:310] Step = 53700 ; steps/s = 1.63, tokens/s = 82674 (39194 source, 43480 target) ; Learning rate = 0.000381 ; Loss = 1.518281\n",
      "2024-12-04 20:02:10.588000: I runner.py:310] Step = 53800 ; steps/s = 1.63, tokens/s = 82733 (39225 source, 43508 target) ; Learning rate = 0.000381 ; Loss = 1.519573\n",
      "2024-12-04 20:03:12.107000: I runner.py:310] Step = 53900 ; steps/s = 1.63, tokens/s = 82571 (39165 source, 43406 target) ; Learning rate = 0.000381 ; Loss = 1.520427\n",
      "2024-12-04 20:04:13.018000: I runner.py:310] Step = 54000 ; steps/s = 1.64, tokens/s = 81818 (38799 source, 43019 target) ; Learning rate = 0.000380 ; Loss = 1.515694\n",
      "2024-12-04 20:05:14.511000: I runner.py:310] Step = 54100 ; steps/s = 1.63, tokens/s = 82646 (39183 source, 43463 target) ; Learning rate = 0.000380 ; Loss = 1.522288\n",
      "2024-12-04 20:06:15.984000: I runner.py:310] Step = 54200 ; steps/s = 1.63, tokens/s = 82646 (39175 source, 43471 target) ; Learning rate = 0.000380 ; Loss = 1.514532\n",
      "2024-12-04 20:07:17.470000: I runner.py:310] Step = 54300 ; steps/s = 1.63, tokens/s = 82613 (39148 source, 43465 target) ; Learning rate = 0.000379 ; Loss = 1.515302\n",
      "2024-12-04 20:08:18.335000: I runner.py:310] Step = 54400 ; steps/s = 1.64, tokens/s = 81862 (38821 source, 43041 target) ; Learning rate = 0.000379 ; Loss = 1.514631\n",
      "2024-12-04 20:09:19.843000: I runner.py:310] Step = 54500 ; steps/s = 1.63, tokens/s = 82544 (39127 source, 43417 target) ; Learning rate = 0.000379 ; Loss = 1.514861\n",
      "2024-12-04 20:10:21.316000: I runner.py:310] Step = 54600 ; steps/s = 1.63, tokens/s = 82650 (39190 source, 43460 target) ; Learning rate = 0.000378 ; Loss = 1.517044\n",
      "2024-12-04 20:11:22.812000: I runner.py:310] Step = 54700 ; steps/s = 1.63, tokens/s = 82636 (39177 source, 43459 target) ; Learning rate = 0.000378 ; Loss = 1.523023\n",
      "2024-12-04 20:12:23.793000: I runner.py:310] Step = 54800 ; steps/s = 1.64, tokens/s = 81742 (38755 source, 42987 target) ; Learning rate = 0.000378 ; Loss = 1.521806\n",
      "2024-12-04 20:13:25.326000: I runner.py:310] Step = 54900 ; steps/s = 1.63, tokens/s = 82571 (39153 source, 43418 target) ; Learning rate = 0.000377 ; Loss = 1.515425\n",
      "2024-12-04 20:14:26.846000: I runner.py:310] Step = 55000 ; steps/s = 1.63, tokens/s = 82568 (39135 source, 43433 target) ; Learning rate = 0.000377 ; Loss = 1.513370\n",
      "2024-12-04 20:14:26.848000: I training.py:192] Running evaluation for step 55000\n",
      "2024-12-04 20:18:50.217000: I training.py:192] Evaluation result for step 55000: loss = 1.220562 ; perplexity = 3.389091\n",
      "2024-12-04 20:19:51.630000: I runner.py:310] Step = 55100 ; steps/s = 1.63, tokens/s = 82738 (39225 source, 43513 target) ; Learning rate = 0.000377 ; Loss = 1.515550\n",
      "2024-12-04 20:20:52.540000: I runner.py:310] Step = 55200 ; steps/s = 1.64, tokens/s = 81823 (38782 source, 43041 target) ; Learning rate = 0.000376 ; Loss = 1.516149\n",
      "2024-12-04 20:21:54.030000: I runner.py:310] Step = 55300 ; steps/s = 1.63, tokens/s = 82646 (39179 source, 43467 target) ; Learning rate = 0.000376 ; Loss = 1.513479\n",
      "2024-12-04 20:22:55.480000: I runner.py:310] Step = 55400 ; steps/s = 1.63, tokens/s = 82687 (39212 source, 43475 target) ; Learning rate = 0.000376 ; Loss = 1.522075\n",
      "2024-12-04 20:23:56.892000: I runner.py:310] Step = 55500 ; steps/s = 1.63, tokens/s = 82700 (39215 source, 43485 target) ; Learning rate = 0.000375 ; Loss = 1.512616\n",
      "2024-12-04 20:24:57.819000: I runner.py:310] Step = 55600 ; steps/s = 1.64, tokens/s = 81760 (38758 source, 43002 target) ; Learning rate = 0.000375 ; Loss = 1.513150\n",
      "2024-12-04 20:25:59.324000: I runner.py:310] Step = 55700 ; steps/s = 1.63, tokens/s = 82610 (39158 source, 43452 target) ; Learning rate = 0.000375 ; Loss = 1.512558\n",
      "2024-12-04 20:27:00.852000: I runner.py:310] Step = 55800 ; steps/s = 1.63, tokens/s = 82583 (39142 source, 43441 target) ; Learning rate = 0.000374 ; Loss = 1.516195\n",
      "2024-12-04 20:28:02.315000: I runner.py:310] Step = 55900 ; steps/s = 1.63, tokens/s = 82637 (39199 source, 43438 target) ; Learning rate = 0.000374 ; Loss = 1.510744\n",
      "2024-12-04 20:29:03.162000: I runner.py:310] Step = 56000 ; steps/s = 1.64, tokens/s = 81914 (38833 source, 43081 target) ; Learning rate = 0.000374 ; Loss = 1.509166\n",
      "2024-12-04 20:30:04.651000: I runner.py:310] Step = 56100 ; steps/s = 1.63, tokens/s = 82618 (39169 source, 43449 target) ; Learning rate = 0.000373 ; Loss = 1.509886\n",
      "2024-12-04 20:31:06.127000: I runner.py:310] Step = 56200 ; steps/s = 1.63, tokens/s = 82643 (39176 source, 43467 target) ; Learning rate = 0.000373 ; Loss = 1.514453\n",
      "2024-12-04 20:32:07.636000: I runner.py:310] Step = 56300 ; steps/s = 1.63, tokens/s = 82566 (39151 source, 43415 target) ; Learning rate = 0.000373 ; Loss = 1.508738\n",
      "2024-12-04 20:33:08.527000: I runner.py:310] Step = 56400 ; steps/s = 1.64, tokens/s = 81847 (38802 source, 43045 target) ; Learning rate = 0.000372 ; Loss = 1.514916\n",
      "2024-12-04 20:34:10.014000: I runner.py:310] Step = 56500 ; steps/s = 1.63, tokens/s = 82608 (39154 source, 43454 target) ; Learning rate = 0.000372 ; Loss = 1.510989\n",
      "2024-12-04 20:35:11.461000: I runner.py:310] Step = 56600 ; steps/s = 1.63, tokens/s = 82666 (39214 source, 43452 target) ; Learning rate = 0.000372 ; Loss = 1.512987\n",
      "2024-12-04 20:36:12.348000: I runner.py:310] Step = 56700 ; steps/s = 1.64, tokens/s = 81859 (38797 source, 43062 target) ; Learning rate = 0.000371 ; Loss = 1.505318\n",
      "2024-12-04 20:37:13.791000: I runner.py:310] Step = 56800 ; steps/s = 1.63, tokens/s = 82682 (39173 source, 43509 target) ; Learning rate = 0.000371 ; Loss = 1.501646\n",
      "2024-12-04 20:38:15.282000: I runner.py:310] Step = 56900 ; steps/s = 1.63, tokens/s = 82642 (39171 source, 43471 target) ; Learning rate = 0.000371 ; Loss = 1.511634\n",
      "2024-12-04 20:39:16.737000: I runner.py:310] Step = 57000 ; steps/s = 1.63, tokens/s = 82645 (39193 source, 43452 target) ; Learning rate = 0.000370 ; Loss = 1.512544\n",
      "2024-12-04 20:40:17.577000: I runner.py:310] Step = 57100 ; steps/s = 1.64, tokens/s = 81921 (38854 source, 43067 target) ; Learning rate = 0.000370 ; Loss = 1.511840\n",
      "2024-12-04 20:41:19.052000: I runner.py:310] Step = 57200 ; steps/s = 1.63, tokens/s = 82656 (39183 source, 43473 target) ; Learning rate = 0.000370 ; Loss = 1.512409\n",
      "2024-12-04 20:42:20.530000: I runner.py:310] Step = 57300 ; steps/s = 1.63, tokens/s = 82646 (39180 source, 43466 target) ; Learning rate = 0.000369 ; Loss = 1.513979\n",
      "2024-12-04 20:43:22.003000: I runner.py:310] Step = 57400 ; steps/s = 1.63, tokens/s = 82601 (39173 source, 43428 target) ; Learning rate = 0.000369 ; Loss = 1.512691\n",
      "2024-12-04 20:44:22.899000: I runner.py:310] Step = 57500 ; steps/s = 1.64, tokens/s = 81857 (38801 source, 43056 target) ; Learning rate = 0.000369 ; Loss = 1.514372\n",
      "2024-12-04 20:45:24.339000: I runner.py:310] Step = 57600 ; steps/s = 1.63, tokens/s = 82677 (39197 source, 43480 target) ; Learning rate = 0.000368 ; Loss = 1.507541\n",
      "2024-12-04 20:46:25.829000: I runner.py:310] Step = 57700 ; steps/s = 1.63, tokens/s = 82630 (39189 source, 43441 target) ; Learning rate = 0.000368 ; Loss = 1.509834\n",
      "2024-12-04 20:47:27.288000: I runner.py:310] Step = 57800 ; steps/s = 1.63, tokens/s = 82656 (39173 source, 43483 target) ; Learning rate = 0.000368 ; Loss = 1.505849\n",
      "2024-12-04 20:48:28.183000: I runner.py:310] Step = 57900 ; steps/s = 1.64, tokens/s = 81813 (38789 source, 43024 target) ; Learning rate = 0.000367 ; Loss = 1.497414\n",
      "2024-12-04 20:49:29.661000: I runner.py:310] Step = 58000 ; steps/s = 1.63, tokens/s = 82660 (39206 source, 43454 target) ; Learning rate = 0.000367 ; Loss = 1.512303\n",
      "2024-12-04 20:50:31.118000: I runner.py:310] Step = 58100 ; steps/s = 1.63, tokens/s = 82681 (39171 source, 43510 target) ; Learning rate = 0.000367 ; Loss = 1.504694\n",
      "2024-12-04 20:51:32.595000: I runner.py:310] Step = 58200 ; steps/s = 1.63, tokens/s = 82617 (39174 source, 43443 target) ; Learning rate = 0.000366 ; Loss = 1.506958\n",
      "2024-12-04 20:52:33.472000: I runner.py:310] Step = 58300 ; steps/s = 1.64, tokens/s = 81850 (38804 source, 43046 target) ; Learning rate = 0.000366 ; Loss = 1.514662\n",
      "2024-12-04 20:53:34.967000: I runner.py:310] Step = 58400 ; steps/s = 1.63, tokens/s = 82623 (39157 source, 43466 target) ; Learning rate = 0.000366 ; Loss = 1.508032\n",
      "2024-12-04 20:54:36.422000: I runner.py:310] Step = 58500 ; steps/s = 1.63, tokens/s = 82634 (39196 source, 43438 target) ; Learning rate = 0.000365 ; Loss = 1.509058\n",
      "2024-12-04 20:55:37.822000: I runner.py:310] Step = 58600 ; steps/s = 1.63, tokens/s = 82743 (39229 source, 43514 target) ; Learning rate = 0.000365 ; Loss = 1.508228\n",
      "2024-12-04 20:56:38.810000: I runner.py:310] Step = 58700 ; steps/s = 1.64, tokens/s = 81721 (38740 source, 42981 target) ; Learning rate = 0.000365 ; Loss = 1.501685\n",
      "2024-12-04 20:57:40.249000: I runner.py:310] Step = 58800 ; steps/s = 1.63, tokens/s = 82690 (39193 source, 43497 target) ; Learning rate = 0.000365 ; Loss = 1.515181\n",
      "2024-12-04 20:58:41.733000: I runner.py:310] Step = 58900 ; steps/s = 1.63, tokens/s = 82622 (39168 source, 43454 target) ; Learning rate = 0.000364 ; Loss = 1.511646\n",
      "2024-12-04 20:59:43.240000: I runner.py:310] Step = 59000 ; steps/s = 1.63, tokens/s = 82586 (39161 source, 43425 target) ; Learning rate = 0.000364 ; Loss = 1.507851\n",
      "2024-12-04 21:00:44.150000: I runner.py:310] Step = 59100 ; steps/s = 1.64, tokens/s = 81816 (38787 source, 43029 target) ; Learning rate = 0.000364 ; Loss = 1.500877\n",
      "2024-12-04 21:01:45.642000: I runner.py:310] Step = 59200 ; steps/s = 1.63, tokens/s = 82616 (39183 source, 43433 target) ; Learning rate = 0.000363 ; Loss = 1.517307\n",
      "2024-12-04 21:02:47.047000: I runner.py:310] Step = 59300 ; steps/s = 1.63, tokens/s = 82723 (39209 source, 43514 target) ; Learning rate = 0.000363 ; Loss = 1.519309\n",
      "2024-12-04 21:03:48.506000: I runner.py:310] Step = 59400 ; steps/s = 1.63, tokens/s = 82660 (39194 source, 43466 target) ; Learning rate = 0.000363 ; Loss = 1.515626\n",
      "2024-12-04 21:04:49.401000: I runner.py:310] Step = 59500 ; steps/s = 1.64, tokens/s = 81835 (38786 source, 43049 target) ; Learning rate = 0.000362 ; Loss = 1.503785\n",
      "2024-12-04 21:05:50.863000: I runner.py:310] Step = 59600 ; steps/s = 1.63, tokens/s = 82674 (39227 source, 43447 target) ; Learning rate = 0.000362 ; Loss = 1.514279\n",
      "2024-12-04 21:06:52.342000: I runner.py:310] Step = 59700 ; steps/s = 1.63, tokens/s = 82620 (39147 source, 43473 target) ; Learning rate = 0.000362 ; Loss = 1.513779\n",
      "2024-12-04 21:07:53.794000: I runner.py:310] Step = 59800 ; steps/s = 1.63, tokens/s = 82669 (39203 source, 43466 target) ; Learning rate = 0.000361 ; Loss = 1.518221\n",
      "2024-12-04 21:08:54.704000: I runner.py:310] Step = 59900 ; steps/s = 1.64, tokens/s = 81863 (38786 source, 43077 target) ; Learning rate = 0.000361 ; Loss = 1.512035\n",
      "2024-12-04 21:09:56.216000: I runner.py:310] Step = 60000 ; steps/s = 1.63, tokens/s = 82596 (39167 source, 43429 target) ; Learning rate = 0.000361 ; Loss = 1.500767\n",
      "2024-12-04 21:09:57.822000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-60000\n",
      "2024-12-04 21:09:57.822000: I training.py:192] Running evaluation for step 60000\n",
      "2024-12-04 21:14:20.100000: I training.py:192] Evaluation result for step 60000: loss = 1.224677 ; perplexity = 3.403068\n",
      "2024-12-04 21:15:21.503000: I runner.py:310] Step = 60100 ; steps/s = 1.63, tokens/s = 82741 (39234 source, 43507 target) ; Learning rate = 0.000361 ; Loss = 1.508235\n",
      "2024-12-04 21:16:23.006000: I runner.py:310] Step = 60200 ; steps/s = 1.63, tokens/s = 82588 (39157 source, 43431 target) ; Learning rate = 0.000360 ; Loss = 1.508911\n",
      "2024-12-04 21:17:24.006000: I runner.py:310] Step = 60300 ; steps/s = 1.64, tokens/s = 81684 (38727 source, 42957 target) ; Learning rate = 0.000360 ; Loss = 1.511039\n",
      "2024-12-04 21:18:25.479000: I runner.py:310] Step = 60400 ; steps/s = 1.63, tokens/s = 82653 (39191 source, 43462 target) ; Learning rate = 0.000360 ; Loss = 1.506424\n",
      "2024-12-04 21:19:26.983000: I runner.py:310] Step = 60500 ; steps/s = 1.63, tokens/s = 82584 (39148 source, 43436 target) ; Learning rate = 0.000359 ; Loss = 1.510155\n",
      "2024-12-04 21:20:28.420000: I runner.py:310] Step = 60600 ; steps/s = 1.63, tokens/s = 82688 (39209 source, 43479 target) ; Learning rate = 0.000359 ; Loss = 1.516495\n",
      "2024-12-04 21:21:29.347000: I runner.py:310] Step = 60700 ; steps/s = 1.64, tokens/s = 81815 (38767 source, 43048 target) ; Learning rate = 0.000359 ; Loss = 1.508249\n",
      "2024-12-04 21:22:30.874000: I runner.py:310] Step = 60800 ; steps/s = 1.63, tokens/s = 82551 (39131 source, 43420 target) ; Learning rate = 0.000358 ; Loss = 1.503594\n",
      "2024-12-04 21:23:32.415000: I runner.py:310] Step = 60900 ; steps/s = 1.63, tokens/s = 82571 (39171 source, 43400 target) ; Learning rate = 0.000358 ; Loss = 1.511205\n",
      "2024-12-04 21:24:33.403000: I runner.py:310] Step = 61000 ; steps/s = 1.64, tokens/s = 81685 (38728 source, 42957 target) ; Learning rate = 0.000358 ; Loss = 1.506317\n",
      "2024-12-04 21:25:34.858000: I runner.py:310] Step = 61100 ; steps/s = 1.63, tokens/s = 82685 (39155 source, 43530 target) ; Learning rate = 0.000358 ; Loss = 1.496971\n",
      "2024-12-04 21:26:36.316000: I runner.py:310] Step = 61200 ; steps/s = 1.63, tokens/s = 82653 (39190 source, 43463 target) ; Learning rate = 0.000357 ; Loss = 1.503177\n",
      "2024-12-04 21:27:37.822000: I runner.py:310] Step = 61300 ; steps/s = 1.63, tokens/s = 82584 (39177 source, 43407 target) ; Learning rate = 0.000357 ; Loss = 1.507981\n",
      "2024-12-04 21:28:38.760000: I runner.py:310] Step = 61400 ; steps/s = 1.64, tokens/s = 81801 (38787 source, 43014 target) ; Learning rate = 0.000357 ; Loss = 1.509148\n",
      "2024-12-04 21:29:40.287000: I runner.py:310] Step = 61500 ; steps/s = 1.63, tokens/s = 82573 (39148 source, 43425 target) ; Learning rate = 0.000356 ; Loss = 1.515414\n",
      "2024-12-04 21:30:41.778000: I runner.py:310] Step = 61600 ; steps/s = 1.63, tokens/s = 82604 (39163 source, 43441 target) ; Learning rate = 0.000356 ; Loss = 1.502868\n",
      "2024-12-04 21:31:43.236000: I runner.py:310] Step = 61700 ; steps/s = 1.63, tokens/s = 82684 (39208 source, 43476 target) ; Learning rate = 0.000356 ; Loss = 1.500665\n",
      "2024-12-04 21:32:44.115000: I runner.py:310] Step = 61800 ; steps/s = 1.64, tokens/s = 81826 (38783 source, 43043 target) ; Learning rate = 0.000356 ; Loss = 1.515111\n",
      "2024-12-04 21:33:45.611000: I runner.py:310] Step = 61900 ; steps/s = 1.63, tokens/s = 82628 (39168 source, 43460 target) ; Learning rate = 0.000355 ; Loss = 1.504182\n",
      "2024-12-04 21:34:47.135000: I runner.py:310] Step = 62000 ; steps/s = 1.63, tokens/s = 82567 (39160 source, 43407 target) ; Learning rate = 0.000355 ; Loss = 1.508734\n",
      "2024-12-04 21:35:48.631000: I runner.py:310] Step = 62100 ; steps/s = 1.63, tokens/s = 82629 (39177 source, 43452 target) ; Learning rate = 0.000355 ; Loss = 1.507593\n",
      "2024-12-04 21:36:49.573000: I runner.py:310] Step = 62200 ; steps/s = 1.64, tokens/s = 81760 (38766 source, 42994 target) ; Learning rate = 0.000354 ; Loss = 1.501395\n",
      "2024-12-04 21:37:51.001000: I runner.py:310] Step = 62300 ; steps/s = 1.63, tokens/s = 82696 (39170 source, 43526 target) ; Learning rate = 0.000354 ; Loss = 1.505225\n",
      "2024-12-04 21:38:52.533000: I runner.py:310] Step = 62400 ; steps/s = 1.63, tokens/s = 82578 (39174 source, 43404 target) ; Learning rate = 0.000354 ; Loss = 1.505081\n",
      "2024-12-04 21:39:54.003000: I runner.py:310] Step = 62500 ; steps/s = 1.63, tokens/s = 82648 (39186 source, 43462 target) ; Learning rate = 0.000354 ; Loss = 1.507672\n",
      "2024-12-04 21:40:54.937000: I runner.py:310] Step = 62600 ; steps/s = 1.64, tokens/s = 81781 (38780 source, 43001 target) ; Learning rate = 0.000353 ; Loss = 1.518500\n",
      "2024-12-04 21:41:56.390000: I runner.py:310] Step = 62700 ; steps/s = 1.63, tokens/s = 82653 (39176 source, 43477 target) ; Learning rate = 0.000353 ; Loss = 1.501152\n",
      "2024-12-04 21:42:57.874000: I runner.py:310] Step = 62800 ; steps/s = 1.63, tokens/s = 82648 (39168 source, 43480 target) ; Learning rate = 0.000353 ; Loss = 1.499575\n",
      "2024-12-04 21:43:59.360000: I runner.py:310] Step = 62900 ; steps/s = 1.63, tokens/s = 82622 (39196 source, 43426 target) ; Learning rate = 0.000352 ; Loss = 1.508363\n",
      "2024-12-04 21:45:00.308000: I runner.py:310] Step = 63000 ; steps/s = 1.64, tokens/s = 81758 (38756 source, 43002 target) ; Learning rate = 0.000352 ; Loss = 1.511338\n",
      "2024-12-04 21:46:01.756000: I runner.py:310] Step = 63100 ; steps/s = 1.63, tokens/s = 82693 (39176 source, 43517 target) ; Learning rate = 0.000352 ; Loss = 1.505551\n",
      "2024-12-04 21:47:03.239000: I runner.py:310] Step = 63200 ; steps/s = 1.63, tokens/s = 82605 (39173 source, 43432 target) ; Learning rate = 0.000352 ; Loss = 1.510983\n",
      "2024-12-04 21:48:04.609000: I runner.py:310] Step = 63300 ; steps/s = 1.63, tokens/s = 82786 (39254 source, 43532 target) ; Learning rate = 0.000351 ; Loss = 1.505547\n",
      "2024-12-04 21:49:05.523000: I runner.py:310] Step = 63400 ; steps/s = 1.64, tokens/s = 81793 (38782 source, 43011 target) ; Learning rate = 0.000351 ; Loss = 1.514893\n",
      "2024-12-04 21:50:07.025000: I runner.py:310] Step = 63500 ; steps/s = 1.63, tokens/s = 82622 (39173 source, 43449 target) ; Learning rate = 0.000351 ; Loss = 1.495457\n",
      "2024-12-04 21:51:08.552000: I runner.py:310] Step = 63600 ; steps/s = 1.63, tokens/s = 82583 (39159 source, 43424 target) ; Learning rate = 0.000350 ; Loss = 1.505966\n",
      "2024-12-04 21:52:10.002000: I runner.py:310] Step = 63700 ; steps/s = 1.63, tokens/s = 82635 (39157 source, 43478 target) ; Learning rate = 0.000350 ; Loss = 1.501419\n",
      "2024-12-04 21:53:10.893000: I runner.py:310] Step = 63800 ; steps/s = 1.64, tokens/s = 81846 (38798 source, 43048 target) ; Learning rate = 0.000350 ; Loss = 1.503401\n",
      "2024-12-04 21:54:12.340000: I runner.py:310] Step = 63900 ; steps/s = 1.63, tokens/s = 82681 (39192 source, 43489 target) ; Learning rate = 0.000350 ; Loss = 1.503998\n",
      "2024-12-04 21:55:13.804000: I runner.py:310] Step = 64000 ; steps/s = 1.63, tokens/s = 82657 (39213 source, 43444 target) ; Learning rate = 0.000349 ; Loss = 1.508339\n",
      "2024-12-04 21:56:15.250000: I runner.py:310] Step = 64100 ; steps/s = 1.63, tokens/s = 82670 (39201 source, 43469 target) ; Learning rate = 0.000349 ; Loss = 1.513413\n",
      "2024-12-04 21:57:16.099000: I runner.py:310] Step = 64200 ; steps/s = 1.64, tokens/s = 81923 (38833 source, 43090 target) ; Learning rate = 0.000349 ; Loss = 1.507219\n",
      "2024-12-04 21:58:17.606000: I runner.py:310] Step = 64300 ; steps/s = 1.63, tokens/s = 82615 (39197 source, 43418 target) ; Learning rate = 0.000349 ; Loss = 1.502733\n",
      "2024-12-04 21:59:19.072000: I runner.py:310] Step = 64400 ; steps/s = 1.63, tokens/s = 82621 (39145 source, 43476 target) ; Learning rate = 0.000348 ; Loss = 1.505821\n",
      "2024-12-04 22:00:20.556000: I runner.py:310] Step = 64500 ; steps/s = 1.63, tokens/s = 82625 (39178 source, 43447 target) ; Learning rate = 0.000348 ; Loss = 1.499792\n",
      "2024-12-04 22:01:21.433000: I runner.py:310] Step = 64600 ; steps/s = 1.64, tokens/s = 81855 (38789 source, 43066 target) ; Learning rate = 0.000348 ; Loss = 1.496159\n",
      "2024-12-04 22:02:22.951000: I runner.py:310] Step = 64700 ; steps/s = 1.63, tokens/s = 82573 (39182 source, 43391 target) ; Learning rate = 0.000347 ; Loss = 1.501760\n",
      "2024-12-04 22:03:24.416000: I runner.py:310] Step = 64800 ; steps/s = 1.63, tokens/s = 82644 (39154 source, 43490 target) ; Learning rate = 0.000347 ; Loss = 1.504053\n",
      "2024-12-04 22:04:25.877000: I runner.py:310] Step = 64900 ; steps/s = 1.63, tokens/s = 82678 (39191 source, 43487 target) ; Learning rate = 0.000347 ; Loss = 1.519621\n",
      "2024-12-04 22:05:26.795000: I runner.py:310] Step = 65000 ; steps/s = 1.64, tokens/s = 81806 (38785 source, 43021 target) ; Learning rate = 0.000347 ; Loss = 1.494736\n",
      "2024-12-04 22:05:26.797000: I training.py:192] Running evaluation for step 65000\n",
      "2024-12-04 22:09:44.900000: I training.py:192] Evaluation result for step 65000: loss = 1.236132 ; perplexity = 3.442271\n",
      "2024-12-04 22:10:46.254000: I runner.py:310] Step = 65100 ; steps/s = 1.63, tokens/s = 82818 (39262 source, 43556 target) ; Learning rate = 0.000346 ; Loss = 1.503729\n",
      "2024-12-04 22:11:47.739000: I runner.py:310] Step = 65200 ; steps/s = 1.63, tokens/s = 82614 (39156 source, 43458 target) ; Learning rate = 0.000346 ; Loss = 1.505493\n",
      "2024-12-04 22:12:48.633000: I runner.py:310] Step = 65300 ; steps/s = 1.64, tokens/s = 81838 (38825 source, 43013 target) ; Learning rate = 0.000346 ; Loss = 1.502838\n",
      "2024-12-04 22:13:50.123000: I runner.py:310] Step = 65400 ; steps/s = 1.63, tokens/s = 82627 (39161 source, 43466 target) ; Learning rate = 0.000346 ; Loss = 1.494378\n",
      "2024-12-04 22:14:51.483000: I runner.py:310] Step = 65500 ; steps/s = 1.63, tokens/s = 82783 (39230 source, 43553 target) ; Learning rate = 0.000345 ; Loss = 1.512740\n",
      "2024-12-04 22:15:52.966000: I runner.py:310] Step = 65600 ; steps/s = 1.63, tokens/s = 82641 (39201 source, 43440 target) ; Learning rate = 0.000345 ; Loss = 1.515499\n",
      "2024-12-04 22:16:53.933000: I runner.py:310] Step = 65700 ; steps/s = 1.64, tokens/s = 81738 (38744 source, 42994 target) ; Learning rate = 0.000345 ; Loss = 1.505772\n",
      "2024-12-04 22:17:55.464000: I runner.py:310] Step = 65800 ; steps/s = 1.63, tokens/s = 82589 (39146 source, 43443 target) ; Learning rate = 0.000345 ; Loss = 1.512468\n",
      "2024-12-04 22:18:56.899000: I runner.py:310] Step = 65900 ; steps/s = 1.63, tokens/s = 82695 (39216 source, 43479 target) ; Learning rate = 0.000344 ; Loss = 1.506578\n",
      "2024-12-04 22:19:58.414000: I runner.py:310] Step = 66000 ; steps/s = 1.63, tokens/s = 82561 (39139 source, 43422 target) ; Learning rate = 0.000344 ; Loss = 1.499375\n",
      "2024-12-04 22:20:59.303000: I runner.py:310] Step = 66100 ; steps/s = 1.64, tokens/s = 81823 (38786 source, 43037 target) ; Learning rate = 0.000344 ; Loss = 1.500751\n",
      "2024-12-04 22:22:00.794000: I runner.py:310] Step = 66200 ; steps/s = 1.63, tokens/s = 82642 (39181 source, 43461 target) ; Learning rate = 0.000344 ; Loss = 1.504783\n",
      "2024-12-04 22:23:02.253000: I runner.py:310] Step = 66300 ; steps/s = 1.63, tokens/s = 82671 (39187 source, 43484 target) ; Learning rate = 0.000343 ; Loss = 1.505303\n",
      "2024-12-04 22:24:03.746000: I runner.py:310] Step = 66400 ; steps/s = 1.63, tokens/s = 82557 (39160 source, 43397 target) ; Learning rate = 0.000343 ; Loss = 1.505309\n",
      "2024-12-04 22:25:04.648000: I runner.py:310] Step = 66500 ; steps/s = 1.64, tokens/s = 81887 (38809 source, 43078 target) ; Learning rate = 0.000343 ; Loss = 1.499846\n",
      "2024-12-04 22:26:06.112000: I runner.py:310] Step = 66600 ; steps/s = 1.63, tokens/s = 82627 (39152 source, 43475 target) ; Learning rate = 0.000342 ; Loss = 1.499982\n",
      "2024-12-04 22:27:07.577000: I runner.py:310] Step = 66700 ; steps/s = 1.63, tokens/s = 82649 (39178 source, 43471 target) ; Learning rate = 0.000342 ; Loss = 1.501856\n",
      "2024-12-04 22:28:09.120000: I runner.py:310] Step = 66800 ; steps/s = 1.63, tokens/s = 82553 (39159 source, 43394 target) ; Learning rate = 0.000342 ; Loss = 1.502512\n",
      "2024-12-04 22:29:10.022000: I runner.py:310] Step = 66900 ; steps/s = 1.64, tokens/s = 81824 (38814 source, 43010 target) ; Learning rate = 0.000342 ; Loss = 1.503796\n",
      "2024-12-04 22:30:11.455000: I runner.py:310] Step = 67000 ; steps/s = 1.63, tokens/s = 82732 (39210 source, 43522 target) ; Learning rate = 0.000341 ; Loss = 1.498123\n",
      "2024-12-04 22:31:12.902000: I runner.py:310] Step = 67100 ; steps/s = 1.63, tokens/s = 82664 (39217 source, 43447 target) ; Learning rate = 0.000341 ; Loss = 1.496788\n",
      "2024-12-04 22:32:14.350000: I runner.py:310] Step = 67200 ; steps/s = 1.63, tokens/s = 82676 (39181 source, 43495 target) ; Learning rate = 0.000341 ; Loss = 1.498664\n",
      "2024-12-04 22:33:15.218000: I runner.py:310] Step = 67300 ; steps/s = 1.64, tokens/s = 81851 (38799 source, 43052 target) ; Learning rate = 0.000341 ; Loss = 1.507822\n",
      "2024-12-04 22:34:16.651000: I runner.py:310] Step = 67400 ; steps/s = 1.63, tokens/s = 82703 (39204 source, 43499 target) ; Learning rate = 0.000340 ; Loss = 1.497276\n",
      "2024-12-04 22:35:18.115000: I runner.py:310] Step = 67500 ; steps/s = 1.63, tokens/s = 82638 (39193 source, 43445 target) ; Learning rate = 0.000340 ; Loss = 1.505406\n",
      "2024-12-04 22:36:19.543000: I runner.py:310] Step = 67600 ; steps/s = 1.63, tokens/s = 82700 (39203 source, 43497 target) ; Learning rate = 0.000340 ; Loss = 1.505870\n",
      "2024-12-04 22:37:20.420000: I runner.py:310] Step = 67700 ; steps/s = 1.64, tokens/s = 81887 (38808 source, 43079 target) ; Learning rate = 0.000340 ; Loss = 1.504577\n",
      "2024-12-04 22:38:21.821000: I runner.py:310] Step = 67800 ; steps/s = 1.63, tokens/s = 82735 (39218 source, 43517 target) ; Learning rate = 0.000339 ; Loss = 1.502589\n",
      "2024-12-04 22:39:23.356000: I runner.py:310] Step = 67900 ; steps/s = 1.63, tokens/s = 82563 (39157 source, 43406 target) ; Learning rate = 0.000339 ; Loss = 1.505541\n",
      "2024-12-04 22:40:24.813000: I runner.py:310] Step = 68000 ; steps/s = 1.63, tokens/s = 82665 (39194 source, 43471 target) ; Learning rate = 0.000339 ; Loss = 1.503961\n",
      "2024-12-04 22:41:25.777000: I runner.py:310] Step = 68100 ; steps/s = 1.64, tokens/s = 81730 (38738 source, 42992 target) ; Learning rate = 0.000339 ; Loss = 1.493083\n",
      "2024-12-04 22:42:27.246000: I runner.py:310] Step = 68200 ; steps/s = 1.63, tokens/s = 82658 (39168 source, 43490 target) ; Learning rate = 0.000338 ; Loss = 1.498834\n",
      "2024-12-04 22:43:28.703000: I runner.py:310] Step = 68300 ; steps/s = 1.63, tokens/s = 82654 (39183 source, 43471 target) ; Learning rate = 0.000338 ; Loss = 1.508014\n",
      "2024-12-04 22:44:30.206000: I runner.py:310] Step = 68400 ; steps/s = 1.63, tokens/s = 82616 (39187 source, 43429 target) ; Learning rate = 0.000338 ; Loss = 1.508980\n",
      "2024-12-04 22:45:31.049000: I runner.py:310] Step = 68500 ; steps/s = 1.64, tokens/s = 81913 (38849 source, 43064 target) ; Learning rate = 0.000338 ; Loss = 1.489769\n",
      "2024-12-04 22:46:32.494000: I runner.py:310] Step = 68600 ; steps/s = 1.63, tokens/s = 82688 (39199 source, 43489 target) ; Learning rate = 0.000337 ; Loss = 1.497991\n",
      "2024-12-04 22:47:33.977000: I runner.py:310] Step = 68700 ; steps/s = 1.63, tokens/s = 82609 (39162 source, 43447 target) ; Learning rate = 0.000337 ; Loss = 1.500058\n",
      "2024-12-04 22:48:35.462000: I runner.py:310] Step = 68800 ; steps/s = 1.63, tokens/s = 82618 (39174 source, 43444 target) ; Learning rate = 0.000337 ; Loss = 1.506954\n",
      "2024-12-04 22:49:36.341000: I runner.py:310] Step = 68900 ; steps/s = 1.64, tokens/s = 81817 (38760 source, 43057 target) ; Learning rate = 0.000337 ; Loss = 1.498185\n",
      "2024-12-04 22:50:37.792000: I runner.py:310] Step = 69000 ; steps/s = 1.63, tokens/s = 82670 (39203 source, 43467 target) ; Learning rate = 0.000336 ; Loss = 1.498074\n",
      "2024-12-04 22:51:39.262000: I runner.py:310] Step = 69100 ; steps/s = 1.63, tokens/s = 82673 (39215 source, 43458 target) ; Learning rate = 0.000336 ; Loss = 1.506001\n",
      "2024-12-04 22:52:40.743000: I runner.py:310] Step = 69200 ; steps/s = 1.63, tokens/s = 82663 (39188 source, 43475 target) ; Learning rate = 0.000336 ; Loss = 1.500265\n",
      "2024-12-04 22:53:41.639000: I runner.py:310] Step = 69300 ; steps/s = 1.64, tokens/s = 81830 (38786 source, 43044 target) ; Learning rate = 0.000336 ; Loss = 1.487755\n",
      "2024-12-04 22:54:43.168000: I runner.py:310] Step = 69400 ; steps/s = 1.63, tokens/s = 82552 (39154 source, 43398 target) ; Learning rate = 0.000336 ; Loss = 1.502536\n",
      "2024-12-04 22:55:44.711000: I runner.py:310] Step = 69500 ; steps/s = 1.63, tokens/s = 82582 (39143 source, 43439 target) ; Learning rate = 0.000335 ; Loss = 1.498247\n",
      "2024-12-04 22:56:45.694000: I runner.py:310] Step = 69600 ; steps/s = 1.64, tokens/s = 81681 (38731 source, 42950 target) ; Learning rate = 0.000335 ; Loss = 1.498555\n",
      "2024-12-04 22:57:47.153000: I runner.py:310] Step = 69700 ; steps/s = 1.63, tokens/s = 82673 (39190 source, 43483 target) ; Learning rate = 0.000335 ; Loss = 1.495192\n",
      "2024-12-04 22:58:48.691000: I runner.py:310] Step = 69800 ; steps/s = 1.63, tokens/s = 82557 (39140 source, 43417 target) ; Learning rate = 0.000335 ; Loss = 1.500573\n",
      "2024-12-04 22:59:50.103000: I runner.py:310] Step = 69900 ; steps/s = 1.63, tokens/s = 82709 (39198 source, 43511 target) ; Learning rate = 0.000334 ; Loss = 1.501719\n",
      "2024-12-04 23:00:50.982000: I runner.py:310] Step = 70000 ; steps/s = 1.64, tokens/s = 81883 (38827 source, 43056 target) ; Learning rate = 0.000334 ; Loss = 1.495764\n",
      "2024-12-04 23:00:52.672000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-70000\n",
      "2024-12-04 23:00:52.672000: I training.py:192] Running evaluation for step 70000\n",
      "2024-12-04 23:05:10.415000: I training.py:192] Evaluation result for step 70000: loss = 1.241202 ; perplexity = 3.459768\n",
      "2024-12-04 23:06:11.788000: I runner.py:310] Step = 70100 ; steps/s = 1.63, tokens/s = 82761 (39227 source, 43534 target) ; Learning rate = 0.000334 ; Loss = 1.496848\n",
      "2024-12-04 23:07:13.337000: I runner.py:310] Step = 70200 ; steps/s = 1.62, tokens/s = 82544 (39136 source, 43408 target) ; Learning rate = 0.000334 ; Loss = 1.500127\n",
      "2024-12-04 23:08:14.776000: I runner.py:310] Step = 70300 ; steps/s = 1.63, tokens/s = 82691 (39205 source, 43486 target) ; Learning rate = 0.000333 ; Loss = 1.504833\n",
      "2024-12-04 23:09:15.717000: I runner.py:310] Step = 70400 ; steps/s = 1.64, tokens/s = 81779 (38771 source, 43008 target) ; Learning rate = 0.000333 ; Loss = 1.492963\n",
      "2024-12-04 23:10:17.220000: I runner.py:310] Step = 70500 ; steps/s = 1.63, tokens/s = 82579 (39137 source, 43442 target) ; Learning rate = 0.000333 ; Loss = 1.496523\n",
      "2024-12-04 23:11:18.700000: I runner.py:310] Step = 70600 ; steps/s = 1.63, tokens/s = 82656 (39204 source, 43452 target) ; Learning rate = 0.000333 ; Loss = 1.497512\n",
      "2024-12-04 23:12:20.128000: I runner.py:310] Step = 70700 ; steps/s = 1.63, tokens/s = 82731 (39223 source, 43508 target) ; Learning rate = 0.000332 ; Loss = 1.494527\n",
      "2024-12-04 23:13:21.052000: I runner.py:310] Step = 70800 ; steps/s = 1.64, tokens/s = 81795 (38774 source, 43021 target) ; Learning rate = 0.000332 ; Loss = 1.501982\n",
      "2024-12-04 23:14:22.496000: I runner.py:310] Step = 70900 ; steps/s = 1.63, tokens/s = 82672 (39212 source, 43460 target) ; Learning rate = 0.000332 ; Loss = 1.495502\n",
      "2024-12-04 23:15:23.946000: I runner.py:310] Step = 71000 ; steps/s = 1.63, tokens/s = 82673 (39180 source, 43493 target) ; Learning rate = 0.000332 ; Loss = 1.505441\n",
      "2024-12-04 23:16:25.517000: I runner.py:310] Step = 71100 ; steps/s = 1.62, tokens/s = 82482 (39104 source, 43378 target) ; Learning rate = 0.000331 ; Loss = 1.490946\n",
      "2024-12-04 23:17:26.503000: I runner.py:310] Step = 71200 ; steps/s = 1.64, tokens/s = 81717 (38730 source, 42987 target) ; Learning rate = 0.000331 ; Loss = 1.498157\n",
      "2024-12-04 23:18:27.941000: I runner.py:310] Step = 71300 ; steps/s = 1.63, tokens/s = 82698 (39187 source, 43511 target) ; Learning rate = 0.000331 ; Loss = 1.491623\n",
      "2024-12-04 23:19:29.372000: I runner.py:310] Step = 71400 ; steps/s = 1.63, tokens/s = 82697 (39225 source, 43472 target) ; Learning rate = 0.000331 ; Loss = 1.499922\n",
      "2024-12-04 23:20:30.846000: I runner.py:310] Step = 71500 ; steps/s = 1.63, tokens/s = 82662 (39204 source, 43458 target) ; Learning rate = 0.000331 ; Loss = 1.500090\n",
      "2024-12-04 23:21:31.767000: I runner.py:310] Step = 71600 ; steps/s = 1.64, tokens/s = 81767 (38785 source, 42982 target) ; Learning rate = 0.000330 ; Loss = 1.494724\n",
      "2024-12-04 23:22:33.239000: I runner.py:310] Step = 71700 ; steps/s = 1.63, tokens/s = 82636 (39195 source, 43441 target) ; Learning rate = 0.000330 ; Loss = 1.503222\n",
      "2024-12-04 23:23:34.673000: I runner.py:310] Step = 71800 ; steps/s = 1.63, tokens/s = 82707 (39200 source, 43507 target) ; Learning rate = 0.000330 ; Loss = 1.488290\n",
      "2024-12-04 23:24:36.175000: I runner.py:310] Step = 71900 ; steps/s = 1.63, tokens/s = 82599 (39150 source, 43449 target) ; Learning rate = 0.000330 ; Loss = 1.502603\n",
      "2024-12-04 23:25:37.124000: I runner.py:310] Step = 72000 ; steps/s = 1.64, tokens/s = 81795 (38774 source, 43021 target) ; Learning rate = 0.000329 ; Loss = 1.487179\n",
      "2024-12-04 23:26:38.604000: I runner.py:310] Step = 72100 ; steps/s = 1.63, tokens/s = 82614 (39147 source, 43467 target) ; Learning rate = 0.000329 ; Loss = 1.494151\n",
      "2024-12-04 23:27:40.164000: I runner.py:310] Step = 72200 ; steps/s = 1.62, tokens/s = 82504 (39125 source, 43379 target) ; Learning rate = 0.000329 ; Loss = 1.499126\n",
      "2024-12-04 23:28:41.711000: I runner.py:310] Step = 72300 ; steps/s = 1.62, tokens/s = 82560 (39136 source, 43424 target) ; Learning rate = 0.000329 ; Loss = 1.506258\n",
      "2024-12-04 23:29:42.577000: I runner.py:310] Step = 72400 ; steps/s = 1.64, tokens/s = 81869 (38824 source, 43045 target) ; Learning rate = 0.000328 ; Loss = 1.499593\n",
      "2024-12-04 23:30:43.987000: I runner.py:310] Step = 72500 ; steps/s = 1.63, tokens/s = 82748 (39220 source, 43528 target) ; Learning rate = 0.000328 ; Loss = 1.501815\n",
      "2024-12-04 23:31:45.490000: I runner.py:310] Step = 72600 ; steps/s = 1.63, tokens/s = 82588 (39153 source, 43435 target) ; Learning rate = 0.000328 ; Loss = 1.497819\n",
      "2024-12-04 23:32:46.907000: I runner.py:310] Step = 72700 ; steps/s = 1.63, tokens/s = 82730 (39226 source, 43504 target) ; Learning rate = 0.000328 ; Loss = 1.500845\n",
      "2024-12-04 23:33:47.813000: I runner.py:310] Step = 72800 ; steps/s = 1.64, tokens/s = 81819 (38783 source, 43036 target) ; Learning rate = 0.000328 ; Loss = 1.504572\n",
      "2024-12-04 23:34:49.316000: I runner.py:310] Step = 72900 ; steps/s = 1.63, tokens/s = 82565 (39139 source, 43426 target) ; Learning rate = 0.000327 ; Loss = 1.495931\n",
      "2024-12-04 23:35:50.799000: I runner.py:310] Step = 73000 ; steps/s = 1.63, tokens/s = 82648 (39218 source, 43430 target) ; Learning rate = 0.000327 ; Loss = 1.505162\n",
      "2024-12-04 23:36:52.257000: I runner.py:310] Step = 73100 ; steps/s = 1.63, tokens/s = 82688 (39190 source, 43498 target) ; Learning rate = 0.000327 ; Loss = 1.501983\n",
      "2024-12-04 23:37:53.133000: I runner.py:310] Step = 73200 ; steps/s = 1.64, tokens/s = 81857 (38777 source, 43080 target) ; Learning rate = 0.000327 ; Loss = 1.503133\n",
      "2024-12-04 23:38:54.623000: I runner.py:310] Step = 73300 ; steps/s = 1.63, tokens/s = 82625 (39169 source, 43456 target) ; Learning rate = 0.000326 ; Loss = 1.492950\n",
      "2024-12-04 23:39:56.170000: I runner.py:310] Step = 73400 ; steps/s = 1.63, tokens/s = 82533 (39162 source, 43371 target) ; Learning rate = 0.000326 ; Loss = 1.495174\n",
      "2024-12-04 23:40:57.704000: I runner.py:310] Step = 73500 ; steps/s = 1.63, tokens/s = 82550 (39138 source, 43412 target) ; Learning rate = 0.000326 ; Loss = 1.497638\n",
      "2024-12-04 23:41:58.614000: I runner.py:310] Step = 73600 ; steps/s = 1.64, tokens/s = 81797 (38772 source, 43025 target) ; Learning rate = 0.000326 ; Loss = 1.495214\n",
      "2024-12-04 23:43:00.168000: I runner.py:310] Step = 73700 ; steps/s = 1.62, tokens/s = 82547 (39130 source, 43417 target) ; Learning rate = 0.000326 ; Loss = 1.493506\n",
      "2024-12-04 23:44:01.675000: I runner.py:310] Step = 73800 ; steps/s = 1.63, tokens/s = 82595 (39170 source, 43425 target) ; Learning rate = 0.000325 ; Loss = 1.500329\n",
      "2024-12-04 23:45:02.645000: I runner.py:310] Step = 73900 ; steps/s = 1.64, tokens/s = 81731 (38753 source, 42978 target) ; Learning rate = 0.000325 ; Loss = 1.490275\n",
      "2024-12-04 23:46:04.118000: I runner.py:310] Step = 74000 ; steps/s = 1.63, tokens/s = 82653 (39141 source, 43512 target) ; Learning rate = 0.000325 ; Loss = 1.496842\n",
      "2024-12-04 23:47:05.608000: I runner.py:310] Step = 74100 ; steps/s = 1.63, tokens/s = 82671 (39220 source, 43451 target) ; Learning rate = 0.000325 ; Loss = 1.489135\n",
      "2024-12-04 23:48:07.014000: I runner.py:310] Step = 74200 ; steps/s = 1.63, tokens/s = 82731 (39244 source, 43487 target) ; Learning rate = 0.000324 ; Loss = 1.492217\n",
      "2024-12-04 23:49:07.906000: I runner.py:310] Step = 74300 ; steps/s = 1.64, tokens/s = 81787 (38762 source, 43025 target) ; Learning rate = 0.000324 ; Loss = 1.498597\n",
      "2024-12-04 23:50:09.461000: I runner.py:310] Step = 74400 ; steps/s = 1.62, tokens/s = 82518 (39099 source, 43419 target) ; Learning rate = 0.000324 ; Loss = 1.500433\n",
      "2024-12-04 23:51:10.928000: I runner.py:310] Step = 74500 ; steps/s = 1.63, tokens/s = 82656 (39180 source, 43476 target) ; Learning rate = 0.000324 ; Loss = 1.493651\n",
      "2024-12-04 23:52:12.392000: I runner.py:310] Step = 74600 ; steps/s = 1.63, tokens/s = 82696 (39232 source, 43464 target) ; Learning rate = 0.000324 ; Loss = 1.493955\n",
      "2024-12-04 23:53:13.361000: I runner.py:310] Step = 74700 ; steps/s = 1.64, tokens/s = 81708 (38743 source, 42965 target) ; Learning rate = 0.000323 ; Loss = 1.498612\n",
      "2024-12-04 23:54:14.819000: I runner.py:310] Step = 74800 ; steps/s = 1.63, tokens/s = 82649 (39170 source, 43479 target) ; Learning rate = 0.000323 ; Loss = 1.494665\n",
      "2024-12-04 23:55:16.303000: I runner.py:310] Step = 74900 ; steps/s = 1.63, tokens/s = 82631 (39186 source, 43445 target) ; Learning rate = 0.000323 ; Loss = 1.494574\n",
      "2024-12-04 23:56:17.815000: I runner.py:310] Step = 75000 ; steps/s = 1.63, tokens/s = 82607 (39162 source, 43445 target) ; Learning rate = 0.000323 ; Loss = 1.496106\n",
      "2024-12-04 23:56:17.817000: I training.py:192] Running evaluation for step 75000\n",
      "2024-12-05 00:00:43.565000: I training.py:192] Evaluation result for step 75000: loss = 1.245895 ; perplexity = 3.476045\n",
      "2024-12-05 00:01:44.342000: I runner.py:310] Step = 75100 ; steps/s = 1.65, tokens/s = 82030 (38879 source, 43151 target) ; Learning rate = 0.000323 ; Loss = 1.488001\n",
      "2024-12-05 00:02:45.827000: I runner.py:310] Step = 75200 ; steps/s = 1.63, tokens/s = 82606 (39153 source, 43453 target) ; Learning rate = 0.000322 ; Loss = 1.491643\n",
      "2024-12-05 00:03:47.348000: I runner.py:310] Step = 75300 ; steps/s = 1.63, tokens/s = 82591 (39171 source, 43420 target) ; Learning rate = 0.000322 ; Loss = 1.491971\n",
      "2024-12-05 00:04:48.814000: I runner.py:310] Step = 75400 ; steps/s = 1.63, tokens/s = 82623 (39169 source, 43454 target) ; Learning rate = 0.000322 ; Loss = 1.500399\n",
      "2024-12-05 00:05:49.783000: I runner.py:310] Step = 75500 ; steps/s = 1.64, tokens/s = 81763 (38769 source, 42994 target) ; Learning rate = 0.000322 ; Loss = 1.494030\n",
      "2024-12-05 00:06:51.317000: I runner.py:310] Step = 75600 ; steps/s = 1.63, tokens/s = 82536 (39136 source, 43400 target) ; Learning rate = 0.000321 ; Loss = 1.495714\n",
      "2024-12-05 00:07:52.840000: I runner.py:310] Step = 75700 ; steps/s = 1.63, tokens/s = 82569 (39156 source, 43413 target) ; Learning rate = 0.000321 ; Loss = 1.492011\n",
      "2024-12-05 00:08:54.379000: I runner.py:310] Step = 75800 ; steps/s = 1.63, tokens/s = 82572 (39148 source, 43424 target) ; Learning rate = 0.000321 ; Loss = 1.496001\n",
      "2024-12-05 00:09:55.289000: I runner.py:310] Step = 75900 ; steps/s = 1.64, tokens/s = 81817 (38766 source, 43051 target) ; Learning rate = 0.000321 ; Loss = 1.491173\n",
      "2024-12-05 00:10:56.807000: I runner.py:310] Step = 76000 ; steps/s = 1.63, tokens/s = 82574 (39137 source, 43437 target) ; Learning rate = 0.000321 ; Loss = 1.487265\n",
      "2024-12-05 00:11:58.288000: I runner.py:310] Step = 76100 ; steps/s = 1.63, tokens/s = 82644 (39177 source, 43467 target) ; Learning rate = 0.000320 ; Loss = 1.493834\n",
      "2024-12-05 00:12:59.800000: I runner.py:310] Step = 76200 ; steps/s = 1.63, tokens/s = 82555 (39170 source, 43385 target) ; Learning rate = 0.000320 ; Loss = 1.496511\n",
      "2024-12-05 00:14:00.784000: I runner.py:310] Step = 76300 ; steps/s = 1.64, tokens/s = 81740 (38736 source, 43004 target) ; Learning rate = 0.000320 ; Loss = 1.489649\n",
      "2024-12-05 00:15:02.317000: I runner.py:310] Step = 76400 ; steps/s = 1.63, tokens/s = 82557 (39119 source, 43438 target) ; Learning rate = 0.000320 ; Loss = 1.491495\n",
      "2024-12-05 00:16:03.801000: I runner.py:310] Step = 76500 ; steps/s = 1.63, tokens/s = 82613 (39187 source, 43426 target) ; Learning rate = 0.000320 ; Loss = 1.497357\n",
      "2024-12-05 00:17:05.270000: I runner.py:310] Step = 76600 ; steps/s = 1.63, tokens/s = 82652 (39203 source, 43449 target) ; Learning rate = 0.000319 ; Loss = 1.496121\n",
      "2024-12-05 00:18:06.547000: I runner.py:310] Step = 76700 ; steps/s = 1.63, tokens/s = 81325 (38561 source, 42764 target) ; Learning rate = 0.000319 ; Loss = 1.490199\n",
      "2024-12-05 00:19:08.474000: I runner.py:310] Step = 76800 ; steps/s = 1.61, tokens/s = 82026 (38868 source, 43158 target) ; Learning rate = 0.000319 ; Loss = 1.497045\n",
      "2024-12-05 00:20:09.948000: I runner.py:310] Step = 76900 ; steps/s = 1.63, tokens/s = 82648 (39197 source, 43451 target) ; Learning rate = 0.000319 ; Loss = 1.497751\n",
      "2024-12-05 00:21:11.491000: I runner.py:310] Step = 77000 ; steps/s = 1.63, tokens/s = 82530 (39120 source, 43410 target) ; Learning rate = 0.000319 ; Loss = 1.495040\n",
      "2024-12-05 00:22:12.421000: I runner.py:310] Step = 77100 ; steps/s = 1.64, tokens/s = 81792 (38778 source, 43014 target) ; Learning rate = 0.000318 ; Loss = 1.490696\n",
      "2024-12-05 00:23:13.968000: I runner.py:310] Step = 77200 ; steps/s = 1.63, tokens/s = 82564 (39148 source, 43416 target) ; Learning rate = 0.000318 ; Loss = 1.488822\n",
      "2024-12-05 00:24:15.476000: I runner.py:310] Step = 77300 ; steps/s = 1.63, tokens/s = 82617 (39152 source, 43465 target) ; Learning rate = 0.000318 ; Loss = 1.494459\n",
      "2024-12-05 00:25:16.961000: I runner.py:310] Step = 77400 ; steps/s = 1.63, tokens/s = 82613 (39192 source, 43421 target) ; Learning rate = 0.000318 ; Loss = 1.494180\n",
      "2024-12-05 00:26:17.902000: I runner.py:310] Step = 77500 ; steps/s = 1.64, tokens/s = 81780 (38776 source, 43004 target) ; Learning rate = 0.000317 ; Loss = 1.485749\n",
      "2024-12-05 00:27:19.402000: I runner.py:310] Step = 77600 ; steps/s = 1.63, tokens/s = 82593 (39182 source, 43411 target) ; Learning rate = 0.000317 ; Loss = 1.496445\n",
      "2024-12-05 00:28:20.895000: I runner.py:310] Step = 77700 ; steps/s = 1.63, tokens/s = 82623 (39154 source, 43469 target) ; Learning rate = 0.000317 ; Loss = 1.493738\n",
      "2024-12-05 00:29:22.465000: I runner.py:310] Step = 77800 ; steps/s = 1.62, tokens/s = 82484 (39076 source, 43408 target) ; Learning rate = 0.000317 ; Loss = 1.492054\n",
      "2024-12-05 00:30:23.496000: I runner.py:310] Step = 77900 ; steps/s = 1.64, tokens/s = 81663 (38722 source, 42941 target) ; Learning rate = 0.000317 ; Loss = 1.501740\n",
      "2024-12-05 00:31:25.030000: I runner.py:310] Step = 78000 ; steps/s = 1.63, tokens/s = 82525 (39129 source, 43396 target) ; Learning rate = 0.000316 ; Loss = 1.489116\n",
      "2024-12-05 00:32:26.583000: I runner.py:310] Step = 78100 ; steps/s = 1.62, tokens/s = 82558 (39144 source, 43414 target) ; Learning rate = 0.000316 ; Loss = 1.488662\n",
      "2024-12-05 00:33:27.482000: I runner.py:310] Step = 78200 ; steps/s = 1.64, tokens/s = 81831 (38798 source, 43033 target) ; Learning rate = 0.000316 ; Loss = 1.511191\n",
      "2024-12-05 00:34:28.996000: I runner.py:310] Step = 78300 ; steps/s = 1.63, tokens/s = 82584 (39154 source, 43430 target) ; Learning rate = 0.000316 ; Loss = 1.488809\n",
      "2024-12-05 00:35:30.465000: I runner.py:310] Step = 78400 ; steps/s = 1.63, tokens/s = 82658 (39168 source, 43490 target) ; Learning rate = 0.000316 ; Loss = 1.497157\n",
      "2024-12-05 00:36:32.000000: I runner.py:310] Step = 78500 ; steps/s = 1.63, tokens/s = 82570 (39141 source, 43429 target) ; Learning rate = 0.000315 ; Loss = 1.493589\n",
      "2024-12-05 00:37:32.926000: I runner.py:310] Step = 78600 ; steps/s = 1.64, tokens/s = 81791 (38792 source, 42999 target) ; Learning rate = 0.000315 ; Loss = 1.494366\n",
      "2024-12-05 00:38:34.449000: I runner.py:310] Step = 78700 ; steps/s = 1.63, tokens/s = 82566 (39126 source, 43440 target) ; Learning rate = 0.000315 ; Loss = 1.487854\n",
      "2024-12-05 00:39:35.939000: I runner.py:310] Step = 78800 ; steps/s = 1.63, tokens/s = 82591 (39164 source, 43427 target) ; Learning rate = 0.000315 ; Loss = 1.491124\n",
      "2024-12-05 00:40:37.506000: I runner.py:310] Step = 78900 ; steps/s = 1.62, tokens/s = 82528 (39133 source, 43395 target) ; Learning rate = 0.000315 ; Loss = 1.501932\n",
      "2024-12-05 00:41:38.487000: I runner.py:310] Step = 79000 ; steps/s = 1.64, tokens/s = 81737 (38740 source, 42997 target) ; Learning rate = 0.000314 ; Loss = 1.491432\n",
      "2024-12-05 00:42:40.027000: I runner.py:310] Step = 79100 ; steps/s = 1.63, tokens/s = 82544 (39134 source, 43410 target) ; Learning rate = 0.000314 ; Loss = 1.487701\n",
      "2024-12-05 00:43:41.613000: I runner.py:310] Step = 79200 ; steps/s = 1.62, tokens/s = 82492 (39108 source, 43384 target) ; Learning rate = 0.000314 ; Loss = 1.490480\n",
      "2024-12-05 00:44:43.138000: I runner.py:310] Step = 79300 ; steps/s = 1.63, tokens/s = 82594 (39165 source, 43429 target) ; Learning rate = 0.000314 ; Loss = 1.498854\n",
      "2024-12-05 00:45:44.102000: I runner.py:310] Step = 79400 ; steps/s = 1.64, tokens/s = 81729 (38747 source, 42982 target) ; Learning rate = 0.000314 ; Loss = 1.491746\n",
      "2024-12-05 00:46:45.628000: I runner.py:310] Step = 79500 ; steps/s = 1.63, tokens/s = 82577 (39145 source, 43432 target) ; Learning rate = 0.000313 ; Loss = 1.498250\n",
      "2024-12-05 00:47:47.111000: I runner.py:310] Step = 79600 ; steps/s = 1.63, tokens/s = 82625 (39183 source, 43442 target) ; Learning rate = 0.000313 ; Loss = 1.487089\n",
      "2024-12-05 00:48:48.598000: I runner.py:310] Step = 79700 ; steps/s = 1.63, tokens/s = 82595 (39162 source, 43433 target) ; Learning rate = 0.000313 ; Loss = 1.493863\n",
      "2024-12-05 00:49:49.595000: I runner.py:310] Step = 79800 ; steps/s = 1.64, tokens/s = 81732 (38731 source, 43001 target) ; Learning rate = 0.000313 ; Loss = 1.487855\n",
      "2024-12-05 00:50:51.092000: I runner.py:310] Step = 79900 ; steps/s = 1.63, tokens/s = 82630 (39166 source, 43464 target) ; Learning rate = 0.000313 ; Loss = 1.485987\n",
      "2024-12-05 00:51:52.555000: I runner.py:310] Step = 80000 ; steps/s = 1.63, tokens/s = 82635 (39188 source, 43447 target) ; Learning rate = 0.000312 ; Loss = 1.489878\n",
      "2024-12-05 00:51:54.188000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-80000\n",
      "2024-12-05 00:51:54.188000: I training.py:192] Running evaluation for step 80000\n",
      "2024-12-05 00:56:10.341000: I training.py:192] Evaluation result for step 80000: loss = 1.250842 ; perplexity = 3.493283\n",
      "2024-12-05 00:57:11.744000: I runner.py:310] Step = 80100 ; steps/s = 1.63, tokens/s = 82743 (39232 source, 43511 target) ; Learning rate = 0.000312 ; Loss = 1.494296\n",
      "2024-12-05 00:58:12.701000: I runner.py:310] Step = 80200 ; steps/s = 1.64, tokens/s = 81753 (38764 source, 42989 target) ; Learning rate = 0.000312 ; Loss = 1.491266\n",
      "2024-12-05 00:59:14.188000: I runner.py:310] Step = 80300 ; steps/s = 1.63, tokens/s = 82623 (39170 source, 43453 target) ; Learning rate = 0.000312 ; Loss = 1.490659\n",
      "2024-12-05 01:00:15.765000: I runner.py:310] Step = 80400 ; steps/s = 1.62, tokens/s = 82465 (39098 source, 43367 target) ; Learning rate = 0.000312 ; Loss = 1.494969\n",
      "2024-12-05 01:01:17.319000: I runner.py:310] Step = 80500 ; steps/s = 1.62, tokens/s = 82571 (39156 source, 43415 target) ; Learning rate = 0.000312 ; Loss = 1.497348\n",
      "2024-12-05 01:02:18.267000: I runner.py:310] Step = 80600 ; steps/s = 1.64, tokens/s = 81753 (38741 source, 43012 target) ; Learning rate = 0.000311 ; Loss = 1.480680\n",
      "2024-12-05 01:03:19.810000: I runner.py:310] Step = 80700 ; steps/s = 1.63, tokens/s = 82527 (39131 source, 43396 target) ; Learning rate = 0.000311 ; Loss = 1.492477\n",
      "2024-12-05 01:04:21.287000: I runner.py:310] Step = 80800 ; steps/s = 1.63, tokens/s = 82641 (39188 source, 43453 target) ; Learning rate = 0.000311 ; Loss = 1.493150\n",
      "2024-12-05 01:05:22.850000: I runner.py:310] Step = 80900 ; steps/s = 1.62, tokens/s = 82535 (39112 source, 43423 target) ; Learning rate = 0.000311 ; Loss = 1.494032\n",
      "2024-12-05 01:06:23.756000: I runner.py:310] Step = 81000 ; steps/s = 1.64, tokens/s = 81820 (38793 source, 43027 target) ; Learning rate = 0.000311 ; Loss = 1.488253\n",
      "2024-12-05 01:07:25.354000: I runner.py:310] Step = 81100 ; steps/s = 1.62, tokens/s = 82508 (39122 source, 43386 target) ; Learning rate = 0.000310 ; Loss = 1.489859\n",
      "2024-12-05 01:08:26.874000: I runner.py:310] Step = 81200 ; steps/s = 1.63, tokens/s = 82582 (39148 source, 43434 target) ; Learning rate = 0.000310 ; Loss = 1.486190\n",
      "2024-12-05 01:09:28.412000: I runner.py:310] Step = 81300 ; steps/s = 1.63, tokens/s = 82538 (39112 source, 43426 target) ; Learning rate = 0.000310 ; Loss = 1.496874\n",
      "2024-12-05 01:10:29.321000: I runner.py:310] Step = 81400 ; steps/s = 1.64, tokens/s = 81826 (38807 source, 43019 target) ; Learning rate = 0.000310 ; Loss = 1.486979\n",
      "2024-12-05 01:11:30.928000: I runner.py:310] Step = 81500 ; steps/s = 1.62, tokens/s = 82461 (39120 source, 43341 target) ; Learning rate = 0.000310 ; Loss = 1.485139\n",
      "2024-12-05 01:12:32.465000: I runner.py:310] Step = 81600 ; steps/s = 1.63, tokens/s = 82565 (39139 source, 43426 target) ; Learning rate = 0.000309 ; Loss = 1.502029\n",
      "2024-12-05 01:13:33.945000: I runner.py:310] Step = 81700 ; steps/s = 1.63, tokens/s = 82600 (39151 source, 43449 target) ; Learning rate = 0.000309 ; Loss = 1.499425\n",
      "2024-12-05 01:14:34.871000: I runner.py:310] Step = 81800 ; steps/s = 1.64, tokens/s = 81806 (38770 source, 43036 target) ; Learning rate = 0.000309 ; Loss = 1.499224\n",
      "2024-12-05 01:15:36.380000: I runner.py:310] Step = 81900 ; steps/s = 1.63, tokens/s = 82601 (39156 source, 43445 target) ; Learning rate = 0.000309 ; Loss = 1.484233\n",
      "2024-12-05 01:16:37.901000: I runner.py:310] Step = 82000 ; steps/s = 1.63, tokens/s = 82595 (39169 source, 43426 target) ; Learning rate = 0.000309 ; Loss = 1.498201\n",
      "2024-12-05 01:17:39.472000: I runner.py:310] Step = 82100 ; steps/s = 1.62, tokens/s = 82496 (39127 source, 43369 target) ; Learning rate = 0.000308 ; Loss = 1.490031\n",
      "2024-12-05 01:18:40.496000: I runner.py:310] Step = 82200 ; steps/s = 1.64, tokens/s = 81680 (38729 source, 42951 target) ; Learning rate = 0.000308 ; Loss = 1.487383\n",
      "2024-12-05 01:19:42.042000: I runner.py:310] Step = 82300 ; steps/s = 1.62, tokens/s = 82526 (39107 source, 43419 target) ; Learning rate = 0.000308 ; Loss = 1.482575\n",
      "2024-12-05 01:20:43.587000: I runner.py:310] Step = 82400 ; steps/s = 1.62, tokens/s = 82514 (39124 source, 43390 target) ; Learning rate = 0.000308 ; Loss = 1.493593\n",
      "2024-12-05 01:21:44.692000: I runner.py:310] Step = 82500 ; steps/s = 1.64, tokens/s = 82101 (38927 source, 43174 target) ; Learning rate = 0.000308 ; Loss = 1.487577\n",
      "2024-12-05 01:22:45.994000: I runner.py:310] Step = 82600 ; steps/s = 1.63, tokens/s = 82319 (39009 source, 43310 target) ; Learning rate = 0.000308 ; Loss = 1.489751\n",
      "2024-12-05 01:23:47.548000: I runner.py:310] Step = 82700 ; steps/s = 1.62, tokens/s = 82556 (39136 source, 43420 target) ; Learning rate = 0.000307 ; Loss = 1.488144\n",
      "2024-12-05 01:24:49.091000: I runner.py:310] Step = 82800 ; steps/s = 1.63, tokens/s = 82563 (39163 source, 43400 target) ; Learning rate = 0.000307 ; Loss = 1.492505\n",
      "2024-12-05 01:25:50.082000: I runner.py:310] Step = 82900 ; steps/s = 1.64, tokens/s = 81693 (38733 source, 42960 target) ; Learning rate = 0.000307 ; Loss = 1.492650\n",
      "2024-12-05 01:26:51.575000: I runner.py:310] Step = 83000 ; steps/s = 1.63, tokens/s = 82644 (39165 source, 43479 target) ; Learning rate = 0.000307 ; Loss = 1.481317\n",
      "2024-12-05 01:27:53.094000: I runner.py:310] Step = 83100 ; steps/s = 1.63, tokens/s = 82544 (39122 source, 43422 target) ; Learning rate = 0.000307 ; Loss = 1.492060\n",
      "2024-12-05 01:28:54.651000: I runner.py:310] Step = 83200 ; steps/s = 1.62, tokens/s = 82549 (39132 source, 43417 target) ; Learning rate = 0.000306 ; Loss = 1.488806\n",
      "2024-12-05 01:29:55.618000: I runner.py:310] Step = 83300 ; steps/s = 1.64, tokens/s = 81723 (38760 source, 42963 target) ; Learning rate = 0.000306 ; Loss = 1.485722\n",
      "2024-12-05 01:30:57.222000: I runner.py:310] Step = 83400 ; steps/s = 1.62, tokens/s = 82491 (39099 source, 43392 target) ; Learning rate = 0.000306 ; Loss = 1.489351\n",
      "2024-12-05 01:31:58.698000: I runner.py:310] Step = 83500 ; steps/s = 1.63, tokens/s = 82593 (39159 source, 43434 target) ; Learning rate = 0.000306 ; Loss = 1.485763\n",
      "2024-12-05 01:33:00.294000: I runner.py:310] Step = 83600 ; steps/s = 1.62, tokens/s = 82491 (39111 source, 43380 target) ; Learning rate = 0.000306 ; Loss = 1.484905\n",
      "2024-12-05 01:34:01.244000: I runner.py:310] Step = 83700 ; steps/s = 1.64, tokens/s = 81772 (38760 source, 43012 target) ; Learning rate = 0.000306 ; Loss = 1.488365\n",
      "2024-12-05 01:35:02.810000: I runner.py:310] Step = 83800 ; steps/s = 1.62, tokens/s = 82514 (39135 source, 43379 target) ; Learning rate = 0.000305 ; Loss = 1.482405\n",
      "2024-12-05 01:36:04.360000: I runner.py:310] Step = 83900 ; steps/s = 1.62, tokens/s = 82536 (39156 source, 43380 target) ; Learning rate = 0.000305 ; Loss = 1.490451\n",
      "2024-12-05 01:37:05.900000: I runner.py:310] Step = 84000 ; steps/s = 1.63, tokens/s = 82546 (39119 source, 43427 target) ; Learning rate = 0.000305 ; Loss = 1.485149\n",
      "2024-12-05 01:38:06.851000: I runner.py:310] Step = 84100 ; steps/s = 1.64, tokens/s = 81773 (38755 source, 43018 target) ; Learning rate = 0.000305 ; Loss = 1.495065\n",
      "2024-12-05 01:39:08.395000: I runner.py:310] Step = 84200 ; steps/s = 1.63, tokens/s = 82543 (39149 source, 43394 target) ; Learning rate = 0.000305 ; Loss = 1.488078\n",
      "2024-12-05 01:40:09.892000: I runner.py:310] Step = 84300 ; steps/s = 1.63, tokens/s = 82633 (39202 source, 43431 target) ; Learning rate = 0.000304 ; Loss = 1.490075\n",
      "2024-12-05 01:41:11.403000: I runner.py:310] Step = 84400 ; steps/s = 1.63, tokens/s = 82604 (39150 source, 43454 target) ; Learning rate = 0.000304 ; Loss = 1.491919\n",
      "2024-12-05 01:42:12.345000: I runner.py:310] Step = 84500 ; steps/s = 1.64, tokens/s = 81766 (38739 source, 43027 target) ; Learning rate = 0.000304 ; Loss = 1.485562\n",
      "2024-12-05 01:43:13.970000: I runner.py:310] Step = 84600 ; steps/s = 1.62, tokens/s = 82446 (39094 source, 43352 target) ; Learning rate = 0.000304 ; Loss = 1.494396\n",
      "2024-12-05 01:44:15.485000: I runner.py:310] Step = 84700 ; steps/s = 1.63, tokens/s = 82580 (39168 source, 43412 target) ; Learning rate = 0.000304 ; Loss = 1.492374\n",
      "2024-12-05 01:45:17.018000: I runner.py:310] Step = 84800 ; steps/s = 1.63, tokens/s = 82534 (39133 source, 43401 target) ; Learning rate = 0.000304 ; Loss = 1.491945\n",
      "2024-12-05 01:46:17.971000: I runner.py:310] Step = 84900 ; steps/s = 1.64, tokens/s = 81744 (38707 source, 43037 target) ; Learning rate = 0.000303 ; Loss = 1.497906\n",
      "2024-12-05 01:47:19.513000: I runner.py:310] Step = 85000 ; steps/s = 1.63, tokens/s = 82530 (39132 source, 43398 target) ; Learning rate = 0.000303 ; Loss = 1.487061\n",
      "2024-12-05 01:47:19.514000: I training.py:192] Running evaluation for step 85000\n",
      "2024-12-05 01:51:41.705000: I training.py:192] Evaluation result for step 85000: loss = 1.254644 ; perplexity = 3.506591\n",
      "2024-12-05 01:52:43.050000: I runner.py:310] Step = 85100 ; steps/s = 1.63, tokens/s = 82842 (39285 source, 43557 target) ; Learning rate = 0.000303 ; Loss = 1.486239\n",
      "2024-12-05 01:53:44.625000: I runner.py:310] Step = 85200 ; steps/s = 1.62, tokens/s = 82510 (39124 source, 43386 target) ; Learning rate = 0.000303 ; Loss = 1.488594\n",
      "2024-12-05 01:54:45.638000: I runner.py:310] Step = 85300 ; steps/s = 1.64, tokens/s = 81701 (38709 source, 42992 target) ; Learning rate = 0.000303 ; Loss = 1.489922\n",
      "2024-12-05 01:55:47.133000: I runner.py:310] Step = 85400 ; steps/s = 1.63, tokens/s = 82609 (39172 source, 43437 target) ; Learning rate = 0.000302 ; Loss = 1.485960\n",
      "2024-12-05 01:56:48.718000: I runner.py:310] Step = 85500 ; steps/s = 1.62, tokens/s = 82486 (39115 source, 43371 target) ; Learning rate = 0.000302 ; Loss = 1.494284\n",
      "2024-12-05 01:57:50.253000: I runner.py:310] Step = 85600 ; steps/s = 1.63, tokens/s = 82558 (39151 source, 43407 target) ; Learning rate = 0.000302 ; Loss = 1.488138\n",
      "2024-12-05 01:58:51.258000: I runner.py:310] Step = 85700 ; steps/s = 1.64, tokens/s = 81662 (38707 source, 42955 target) ; Learning rate = 0.000302 ; Loss = 1.483663\n",
      "2024-12-05 01:59:52.750000: I runner.py:310] Step = 85800 ; steps/s = 1.63, tokens/s = 82626 (39168 source, 43458 target) ; Learning rate = 0.000302 ; Loss = 1.484672\n",
      "2024-12-05 02:00:54.309000: I runner.py:310] Step = 85900 ; steps/s = 1.62, tokens/s = 82533 (39148 source, 43385 target) ; Learning rate = 0.000302 ; Loss = 1.491770\n",
      "2024-12-05 02:01:55.879000: I runner.py:310] Step = 86000 ; steps/s = 1.62, tokens/s = 82525 (39132 source, 43393 target) ; Learning rate = 0.000301 ; Loss = 1.486384\n",
      "2024-12-05 02:02:56.858000: I runner.py:310] Step = 86100 ; steps/s = 1.64, tokens/s = 81702 (38738 source, 42964 target) ; Learning rate = 0.000301 ; Loss = 1.480708\n",
      "2024-12-05 02:03:58.418000: I runner.py:310] Step = 86200 ; steps/s = 1.62, tokens/s = 82537 (39140 source, 43397 target) ; Learning rate = 0.000301 ; Loss = 1.482644\n",
      "2024-12-05 02:04:59.955000: I runner.py:310] Step = 86300 ; steps/s = 1.63, tokens/s = 82571 (39139 source, 43432 target) ; Learning rate = 0.000301 ; Loss = 1.483279\n",
      "2024-12-05 02:06:01.502000: I runner.py:310] Step = 86400 ; steps/s = 1.63, tokens/s = 82527 (39115 source, 43412 target) ; Learning rate = 0.000301 ; Loss = 1.487546\n",
      "2024-12-05 02:07:02.456000: I runner.py:310] Step = 86500 ; steps/s = 1.64, tokens/s = 81739 (38722 source, 43017 target) ; Learning rate = 0.000301 ; Loss = 1.486228\n",
      "2024-12-05 02:08:04.008000: I runner.py:310] Step = 86600 ; steps/s = 1.62, tokens/s = 82558 (39144 source, 43414 target) ; Learning rate = 0.000300 ; Loss = 1.483392\n",
      "2024-12-05 02:09:05.507000: I runner.py:310] Step = 86700 ; steps/s = 1.63, tokens/s = 82584 (39147 source, 43437 target) ; Learning rate = 0.000300 ; Loss = 1.489320\n",
      "2024-12-05 02:10:06.875000: I runner.py:310] Step = 86800 ; steps/s = 1.63, tokens/s = 82191 (38995 source, 43196 target) ; Learning rate = 0.000300 ; Loss = 1.483155\n",
      "2024-12-05 02:11:08.033000: I runner.py:310] Step = 86900 ; steps/s = 1.64, tokens/s = 82119 (38905 source, 43214 target) ; Learning rate = 0.000300 ; Loss = 1.475749\n",
      "2024-12-05 02:12:09.573000: I runner.py:310] Step = 87000 ; steps/s = 1.63, tokens/s = 82526 (39128 source, 43398 target) ; Learning rate = 0.000300 ; Loss = 1.490541\n",
      "2024-12-05 02:13:11.107000: I runner.py:310] Step = 87100 ; steps/s = 1.63, tokens/s = 82536 (39141 source, 43395 target) ; Learning rate = 0.000299 ; Loss = 1.491318\n",
      "2024-12-05 02:14:12.011000: I runner.py:310] Step = 87200 ; steps/s = 1.64, tokens/s = 81846 (38826 source, 43020 target) ; Learning rate = 0.000299 ; Loss = 1.487178\n",
      "2024-12-05 02:15:13.478000: I runner.py:310] Step = 87300 ; steps/s = 1.63, tokens/s = 82645 (39182 source, 43463 target) ; Learning rate = 0.000299 ; Loss = 1.488045\n",
      "2024-12-05 02:16:14.980000: I runner.py:310] Step = 87400 ; steps/s = 1.63, tokens/s = 82578 (39140 source, 43438 target) ; Learning rate = 0.000299 ; Loss = 1.490023\n",
      "2024-12-05 02:17:16.590000: I runner.py:310] Step = 87500 ; steps/s = 1.62, tokens/s = 82493 (39111 source, 43382 target) ; Learning rate = 0.000299 ; Loss = 1.485096\n",
      "2024-12-05 02:18:17.544000: I runner.py:310] Step = 87600 ; steps/s = 1.64, tokens/s = 81751 (38754 source, 42997 target) ; Learning rate = 0.000299 ; Loss = 1.487272\n",
      "2024-12-05 02:19:19.135000: I runner.py:310] Step = 87700 ; steps/s = 1.62, tokens/s = 82501 (39101 source, 43400 target) ; Learning rate = 0.000298 ; Loss = 1.488550\n",
      "2024-12-05 02:20:20.640000: I runner.py:310] Step = 87800 ; steps/s = 1.63, tokens/s = 82602 (39164 source, 43438 target) ; Learning rate = 0.000298 ; Loss = 1.483769\n",
      "2024-12-05 02:21:22.192000: I runner.py:310] Step = 87900 ; steps/s = 1.62, tokens/s = 82526 (39138 source, 43388 target) ; Learning rate = 0.000298 ; Loss = 1.481529\n",
      "2024-12-05 02:22:23.172000: I runner.py:310] Step = 88000 ; steps/s = 1.64, tokens/s = 81723 (38733 source, 42990 target) ; Learning rate = 0.000298 ; Loss = 1.487464\n",
      "2024-12-05 02:23:24.649000: I runner.py:310] Step = 88100 ; steps/s = 1.63, tokens/s = 82644 (39158 source, 43486 target) ; Learning rate = 0.000298 ; Loss = 1.484633\n",
      "2024-12-05 02:24:26.178000: I runner.py:310] Step = 88200 ; steps/s = 1.63, tokens/s = 82612 (39186 source, 43426 target) ; Learning rate = 0.000298 ; Loss = 1.488882\n",
      "2024-12-05 02:25:27.710000: I runner.py:310] Step = 88300 ; steps/s = 1.63, tokens/s = 82511 (39123 source, 43388 target) ; Learning rate = 0.000297 ; Loss = 1.477747\n",
      "2024-12-05 02:26:28.681000: I runner.py:310] Step = 88400 ; steps/s = 1.64, tokens/s = 81736 (38751 source, 42985 target) ; Learning rate = 0.000297 ; Loss = 1.480107\n",
      "2024-12-05 02:27:30.301000: I runner.py:310] Step = 88500 ; steps/s = 1.62, tokens/s = 82415 (39063 source, 43352 target) ; Learning rate = 0.000297 ; Loss = 1.477935\n",
      "2024-12-05 02:28:31.801000: I runner.py:310] Step = 88600 ; steps/s = 1.63, tokens/s = 82587 (39168 source, 43419 target) ; Learning rate = 0.000297 ; Loss = 1.488669\n",
      "2024-12-05 02:29:33.281000: I runner.py:310] Step = 88700 ; steps/s = 1.63, tokens/s = 82671 (39202 source, 43469 target) ; Learning rate = 0.000297 ; Loss = 1.490278\n",
      "2024-12-05 02:30:34.259000: I runner.py:310] Step = 88800 ; steps/s = 1.64, tokens/s = 81736 (38746 source, 42990 target) ; Learning rate = 0.000297 ; Loss = 1.481949\n",
      "2024-12-05 02:31:35.857000: I runner.py:310] Step = 88900 ; steps/s = 1.62, tokens/s = 82476 (39121 source, 43355 target) ; Learning rate = 0.000296 ; Loss = 1.482192\n",
      "2024-12-05 02:32:37.410000: I runner.py:310] Step = 89000 ; steps/s = 1.62, tokens/s = 82524 (39133 source, 43391 target) ; Learning rate = 0.000296 ; Loss = 1.480893\n",
      "2024-12-05 02:33:39.016000: I runner.py:310] Step = 89100 ; steps/s = 1.62, tokens/s = 82460 (39072 source, 43388 target) ; Learning rate = 0.000296 ; Loss = 1.494027\n",
      "2024-12-05 02:34:39.991000: I runner.py:310] Step = 89200 ; steps/s = 1.64, tokens/s = 81708 (38721 source, 42987 target) ; Learning rate = 0.000296 ; Loss = 1.493383\n",
      "2024-12-05 02:35:41.501000: I runner.py:310] Step = 89300 ; steps/s = 1.63, tokens/s = 82632 (39183 source, 43449 target) ; Learning rate = 0.000296 ; Loss = 1.487830\n",
      "2024-12-05 02:36:43.081000: I runner.py:310] Step = 89400 ; steps/s = 1.62, tokens/s = 82485 (39104 source, 43381 target) ; Learning rate = 0.000296 ; Loss = 1.481889\n",
      "2024-12-05 02:37:44.632000: I runner.py:310] Step = 89500 ; steps/s = 1.62, tokens/s = 82520 (39114 source, 43406 target) ; Learning rate = 0.000295 ; Loss = 1.486214\n",
      "2024-12-05 02:38:45.616000: I runner.py:310] Step = 89600 ; steps/s = 1.64, tokens/s = 81719 (38749 source, 42970 target) ; Learning rate = 0.000295 ; Loss = 1.492927\n",
      "2024-12-05 02:39:47.174000: I runner.py:310] Step = 89700 ; steps/s = 1.62, tokens/s = 82520 (39099 source, 43421 target) ; Learning rate = 0.000295 ; Loss = 1.479063\n",
      "2024-12-05 02:40:48.704000: I runner.py:310] Step = 89800 ; steps/s = 1.63, tokens/s = 82596 (39174 source, 43422 target) ; Learning rate = 0.000295 ; Loss = 1.480325\n",
      "2024-12-05 02:41:50.274000: I runner.py:310] Step = 89900 ; steps/s = 1.62, tokens/s = 82495 (39133 source, 43362 target) ; Learning rate = 0.000295 ; Loss = 1.489861\n",
      "2024-12-05 02:42:51.231000: I runner.py:310] Step = 90000 ; steps/s = 1.64, tokens/s = 81759 (38755 source, 43004 target) ; Learning rate = 0.000295 ; Loss = 1.475105\n",
      "2024-12-05 02:42:52.926000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-90000\n",
      "2024-12-05 02:42:52.926000: I training.py:192] Running evaluation for step 90000\n",
      "2024-12-05 02:47:16.499000: I training.py:192] Evaluation result for step 90000: loss = 1.258475 ; perplexity = 3.520049\n",
      "2024-12-05 02:48:17.853000: I runner.py:310] Step = 90100 ; steps/s = 1.63, tokens/s = 82822 (39248 source, 43574 target) ; Learning rate = 0.000294 ; Loss = 1.482176\n",
      "2024-12-05 02:49:19.400000: I runner.py:310] Step = 90200 ; steps/s = 1.62, tokens/s = 82510 (39100 source, 43410 target) ; Learning rate = 0.000294 ; Loss = 1.483890\n",
      "2024-12-05 02:50:20.945000: I runner.py:310] Step = 90300 ; steps/s = 1.63, tokens/s = 82563 (39171 source, 43392 target) ; Learning rate = 0.000294 ; Loss = 1.494183\n",
      "2024-12-05 02:51:21.922000: I runner.py:310] Step = 90400 ; steps/s = 1.64, tokens/s = 81690 (38719 source, 42971 target) ; Learning rate = 0.000294 ; Loss = 1.489802\n",
      "2024-12-05 02:52:23.429000: I runner.py:310] Step = 90500 ; steps/s = 1.63, tokens/s = 82597 (39168 source, 43429 target) ; Learning rate = 0.000294 ; Loss = 1.482954\n",
      "2024-12-05 02:53:24.923000: I runner.py:310] Step = 90600 ; steps/s = 1.63, tokens/s = 82649 (39165 source, 43484 target) ; Learning rate = 0.000294 ; Loss = 1.483601\n",
      "2024-12-05 02:54:26.393000: I runner.py:310] Step = 90700 ; steps/s = 1.63, tokens/s = 82649 (39203 source, 43446 target) ; Learning rate = 0.000293 ; Loss = 1.486861\n",
      "2024-12-05 02:55:27.362000: I runner.py:310] Step = 90800 ; steps/s = 1.64, tokens/s = 81704 (38734 source, 42970 target) ; Learning rate = 0.000293 ; Loss = 1.478649\n",
      "2024-12-05 02:56:28.825000: I runner.py:310] Step = 90900 ; steps/s = 1.63, tokens/s = 82678 (39177 source, 43501 target) ; Learning rate = 0.000293 ; Loss = 1.477477\n",
      "2024-12-05 02:57:30.331000: I runner.py:310] Step = 91000 ; steps/s = 1.63, tokens/s = 82592 (39169 source, 43423 target) ; Learning rate = 0.000293 ; Loss = 1.489061\n",
      "2024-12-05 02:58:31.740000: I runner.py:310] Step = 91100 ; steps/s = 1.63, tokens/s = 82506 (39129 source, 43377 target) ; Learning rate = 0.000293 ; Loss = 1.491067\n",
      "2024-12-05 02:59:32.745000: I runner.py:310] Step = 91200 ; steps/s = 1.64, tokens/s = 81914 (38824 source, 43090 target) ; Learning rate = 0.000293 ; Loss = 1.481011\n",
      "2024-12-05 03:00:34.255000: I runner.py:310] Step = 91300 ; steps/s = 1.63, tokens/s = 82621 (39176 source, 43445 target) ; Learning rate = 0.000293 ; Loss = 1.488447\n",
      "2024-12-05 03:01:35.726000: I runner.py:310] Step = 91400 ; steps/s = 1.63, tokens/s = 82660 (39203 source, 43457 target) ; Learning rate = 0.000292 ; Loss = 1.488656\n",
      "2024-12-05 03:02:36.665000: I runner.py:310] Step = 91500 ; steps/s = 1.64, tokens/s = 81729 (38743 source, 42986 target) ; Learning rate = 0.000292 ; Loss = 1.491216\n",
      "2024-12-05 03:03:38.237000: I runner.py:310] Step = 91600 ; steps/s = 1.62, tokens/s = 82508 (39130 source, 43378 target) ; Learning rate = 0.000292 ; Loss = 1.486453\n",
      "2024-12-05 03:04:39.771000: I runner.py:310] Step = 91700 ; steps/s = 1.63, tokens/s = 82539 (39119 source, 43420 target) ; Learning rate = 0.000292 ; Loss = 1.483759\n",
      "2024-12-05 03:05:41.318000: I runner.py:310] Step = 91800 ; steps/s = 1.62, tokens/s = 82560 (39127 source, 43433 target) ; Learning rate = 0.000292 ; Loss = 1.486905\n",
      "2024-12-05 03:06:42.290000: I runner.py:310] Step = 91900 ; steps/s = 1.64, tokens/s = 81754 (38768 source, 42986 target) ; Learning rate = 0.000292 ; Loss = 1.485613\n",
      "2024-12-05 03:07:43.735000: I runner.py:310] Step = 92000 ; steps/s = 1.63, tokens/s = 82706 (39188 source, 43518 target) ; Learning rate = 0.000291 ; Loss = 1.486005\n",
      "2024-12-05 03:08:45.247000: I runner.py:310] Step = 92100 ; steps/s = 1.63, tokens/s = 82559 (39161 source, 43398 target) ; Learning rate = 0.000291 ; Loss = 1.484055\n",
      "2024-12-05 03:09:46.791000: I runner.py:310] Step = 92200 ; steps/s = 1.63, tokens/s = 82513 (39123 source, 43390 target) ; Learning rate = 0.000291 ; Loss = 1.487426\n",
      "2024-12-05 03:10:47.756000: I runner.py:310] Step = 92300 ; steps/s = 1.64, tokens/s = 81778 (38762 source, 43016 target) ; Learning rate = 0.000291 ; Loss = 1.482150\n",
      "2024-12-05 03:11:49.272000: I runner.py:310] Step = 92400 ; steps/s = 1.63, tokens/s = 82584 (39162 source, 43422 target) ; Learning rate = 0.000291 ; Loss = 1.482387\n",
      "2024-12-05 03:12:50.836000: I runner.py:310] Step = 92500 ; steps/s = 1.62, tokens/s = 82548 (39128 source, 43420 target) ; Learning rate = 0.000291 ; Loss = 1.487417\n",
      "2024-12-05 03:13:52.322000: I runner.py:310] Step = 92600 ; steps/s = 1.63, tokens/s = 82580 (39147 source, 43433 target) ; Learning rate = 0.000290 ; Loss = 1.492234\n",
      "2024-12-05 03:14:53.277000: I runner.py:310] Step = 92700 ; steps/s = 1.64, tokens/s = 81770 (38764 source, 43006 target) ; Learning rate = 0.000290 ; Loss = 1.486617\n",
      "2024-12-05 03:15:54.852000: I runner.py:310] Step = 92800 ; steps/s = 1.62, tokens/s = 82504 (39118 source, 43386 target) ; Learning rate = 0.000290 ; Loss = 1.478265\n",
      "2024-12-05 03:16:56.392000: I runner.py:310] Step = 92900 ; steps/s = 1.63, tokens/s = 82557 (39165 source, 43392 target) ; Learning rate = 0.000290 ; Loss = 1.484128\n",
      "2024-12-05 03:17:57.905000: I runner.py:310] Step = 93000 ; steps/s = 1.63, tokens/s = 82601 (39155 source, 43446 target) ; Learning rate = 0.000290 ; Loss = 1.480451\n",
      "2024-12-05 03:18:58.868000: I runner.py:310] Step = 93100 ; steps/s = 1.64, tokens/s = 81731 (38723 source, 43008 target) ; Learning rate = 0.000290 ; Loss = 1.489129\n",
      "2024-12-05 03:20:00.373000: I runner.py:310] Step = 93200 ; steps/s = 1.63, tokens/s = 82599 (39157 source, 43442 target) ; Learning rate = 0.000290 ; Loss = 1.480903\n",
      "2024-12-05 03:21:01.952000: I runner.py:310] Step = 93300 ; steps/s = 1.62, tokens/s = 82472 (39104 source, 43368 target) ; Learning rate = 0.000289 ; Loss = 1.483730\n",
      "2024-12-05 03:22:03.451000: I runner.py:310] Step = 93400 ; steps/s = 1.63, tokens/s = 82618 (39158 source, 43460 target) ; Learning rate = 0.000289 ; Loss = 1.476705\n",
      "2024-12-05 03:23:04.465000: I runner.py:310] Step = 93500 ; steps/s = 1.64, tokens/s = 81674 (38736 source, 42938 target) ; Learning rate = 0.000289 ; Loss = 1.485449\n",
      "2024-12-05 03:24:05.983000: I runner.py:310] Step = 93600 ; steps/s = 1.63, tokens/s = 82623 (39180 source, 43443 target) ; Learning rate = 0.000289 ; Loss = 1.479242\n",
      "2024-12-05 03:25:07.541000: I runner.py:310] Step = 93700 ; steps/s = 1.62, tokens/s = 82530 (39127 source, 43403 target) ; Learning rate = 0.000289 ; Loss = 1.485044\n",
      "2024-12-05 03:26:09.084000: I runner.py:310] Step = 93800 ; steps/s = 1.63, tokens/s = 82525 (39117 source, 43408 target) ; Learning rate = 0.000289 ; Loss = 1.480678\n",
      "2024-12-05 03:27:10.067000: I runner.py:310] Step = 93900 ; steps/s = 1.64, tokens/s = 81682 (38708 source, 42974 target) ; Learning rate = 0.000288 ; Loss = 1.482764\n",
      "2024-12-05 03:28:11.620000: I runner.py:310] Step = 94000 ; steps/s = 1.62, tokens/s = 82519 (39123 source, 43396 target) ; Learning rate = 0.000288 ; Loss = 1.479515\n",
      "2024-12-05 03:29:13.145000: I runner.py:310] Step = 94100 ; steps/s = 1.63, tokens/s = 82594 (39166 source, 43428 target) ; Learning rate = 0.000288 ; Loss = 1.485482\n",
      "2024-12-05 03:30:14.676000: I runner.py:310] Step = 94200 ; steps/s = 1.63, tokens/s = 82557 (39137 source, 43420 target) ; Learning rate = 0.000288 ; Loss = 1.489606\n",
      "2024-12-05 03:31:15.656000: I runner.py:310] Step = 94300 ; steps/s = 1.64, tokens/s = 81741 (38746 source, 42995 target) ; Learning rate = 0.000288 ; Loss = 1.491020\n",
      "2024-12-05 03:32:17.220000: I runner.py:310] Step = 94400 ; steps/s = 1.62, tokens/s = 82547 (39147 source, 43400 target) ; Learning rate = 0.000288 ; Loss = 1.481596\n",
      "2024-12-05 03:33:18.753000: I runner.py:310] Step = 94500 ; steps/s = 1.63, tokens/s = 82547 (39137 source, 43410 target) ; Learning rate = 0.000288 ; Loss = 1.480604\n",
      "2024-12-05 03:34:20.336000: I runner.py:310] Step = 94600 ; steps/s = 1.62, tokens/s = 82460 (39088 source, 43372 target) ; Learning rate = 0.000287 ; Loss = 1.484769\n",
      "2024-12-05 03:35:21.349000: I runner.py:310] Step = 94700 ; steps/s = 1.64, tokens/s = 81698 (38760 source, 42938 target) ; Learning rate = 0.000287 ; Loss = 1.485124\n",
      "2024-12-05 03:36:22.914000: I runner.py:310] Step = 94800 ; steps/s = 1.62, tokens/s = 82521 (39117 source, 43404 target) ; Learning rate = 0.000287 ; Loss = 1.480486\n",
      "2024-12-05 03:37:24.439000: I runner.py:310] Step = 94900 ; steps/s = 1.63, tokens/s = 82574 (39144 source, 43430 target) ; Learning rate = 0.000287 ; Loss = 1.482183\n",
      "2024-12-05 03:38:25.918000: I runner.py:310] Step = 95000 ; steps/s = 1.63, tokens/s = 82647 (39178 source, 43469 target) ; Learning rate = 0.000287 ; Loss = 1.480960\n",
      "2024-12-05 03:38:25.920000: I training.py:192] Running evaluation for step 95000\n",
      "2024-12-05 03:42:43.935000: I training.py:192] Evaluation result for step 95000: loss = 1.264109 ; perplexity = 3.539939\n",
      "2024-12-05 03:43:44.797000: I runner.py:310] Step = 95100 ; steps/s = 1.64, tokens/s = 81869 (38813 source, 43056 target) ; Learning rate = 0.000287 ; Loss = 1.478469\n",
      "2024-12-05 03:44:46.251000: I runner.py:310] Step = 95200 ; steps/s = 1.63, tokens/s = 82682 (39212 source, 43470 target) ; Learning rate = 0.000286 ; Loss = 1.479936\n",
      "2024-12-05 03:45:47.787000: I runner.py:310] Step = 95300 ; steps/s = 1.63, tokens/s = 82564 (39138 source, 43426 target) ; Learning rate = 0.000286 ; Loss = 1.480610\n",
      "2024-12-05 03:46:49.315000: I runner.py:310] Step = 95400 ; steps/s = 1.63, tokens/s = 82562 (39150 source, 43412 target) ; Learning rate = 0.000286 ; Loss = 1.481350\n",
      "2024-12-05 03:47:50.240000: I runner.py:310] Step = 95500 ; steps/s = 1.64, tokens/s = 81834 (38790 source, 43044 target) ; Learning rate = 0.000286 ; Loss = 1.481428\n",
      "2024-12-05 03:48:51.898000: I runner.py:310] Step = 95600 ; steps/s = 1.62, tokens/s = 82378 (39038 source, 43340 target) ; Learning rate = 0.000286 ; Loss = 1.476377\n",
      "2024-12-05 03:49:53.449000: I runner.py:310] Step = 95700 ; steps/s = 1.62, tokens/s = 82525 (39121 source, 43404 target) ; Learning rate = 0.000286 ; Loss = 1.476807\n",
      "2024-12-05 03:50:54.460000: I runner.py:310] Step = 95800 ; steps/s = 1.64, tokens/s = 81657 (38732 source, 42925 target) ; Learning rate = 0.000286 ; Loss = 1.478661\n",
      "2024-12-05 03:51:56.024000: I runner.py:310] Step = 95900 ; steps/s = 1.62, tokens/s = 82553 (39121 source, 43432 target) ; Learning rate = 0.000285 ; Loss = 1.481820\n",
      "2024-12-05 03:52:57.600000: I runner.py:310] Step = 96000 ; steps/s = 1.62, tokens/s = 82483 (39108 source, 43375 target) ; Learning rate = 0.000285 ; Loss = 1.477786\n",
      "2024-12-05 03:53:59.099000: I runner.py:310] Step = 96100 ; steps/s = 1.63, tokens/s = 82628 (39206 source, 43422 target) ; Learning rate = 0.000285 ; Loss = 1.480333\n",
      "2024-12-05 03:55:00.078000: I runner.py:310] Step = 96200 ; steps/s = 1.64, tokens/s = 81691 (38702 source, 42989 target) ; Learning rate = 0.000285 ; Loss = 1.490404\n",
      "2024-12-05 03:56:01.619000: I runner.py:310] Step = 96300 ; steps/s = 1.63, tokens/s = 82576 (39163 source, 43413 target) ; Learning rate = 0.000285 ; Loss = 1.478688\n",
      "2024-12-05 03:57:03.221000: I runner.py:310] Step = 96400 ; steps/s = 1.62, tokens/s = 82499 (39114 source, 43385 target) ; Learning rate = 0.000285 ; Loss = 1.475768\n",
      "2024-12-05 03:58:04.749000: I runner.py:310] Step = 96500 ; steps/s = 1.63, tokens/s = 82549 (39115 source, 43434 target) ; Learning rate = 0.000285 ; Loss = 1.478730\n",
      "2024-12-05 03:59:05.709000: I runner.py:310] Step = 96600 ; steps/s = 1.64, tokens/s = 81721 (38765 source, 42956 target) ; Learning rate = 0.000284 ; Loss = 1.486855\n",
      "2024-12-05 04:00:07.256000: I runner.py:310] Step = 96700 ; steps/s = 1.63, tokens/s = 82548 (39134 source, 43414 target) ; Learning rate = 0.000284 ; Loss = 1.484363\n",
      "2024-12-05 04:01:08.784000: I runner.py:310] Step = 96800 ; steps/s = 1.63, tokens/s = 82573 (39161 source, 43412 target) ; Learning rate = 0.000284 ; Loss = 1.482312\n",
      "2024-12-05 04:02:10.375000: I runner.py:310] Step = 96900 ; steps/s = 1.62, tokens/s = 82483 (39108 source, 43375 target) ; Learning rate = 0.000284 ; Loss = 1.478954\n",
      "2024-12-05 04:03:11.359000: I runner.py:310] Step = 97000 ; steps/s = 1.64, tokens/s = 81714 (38704 source, 43010 target) ; Learning rate = 0.000284 ; Loss = 1.482573\n",
      "2024-12-05 04:04:12.938000: I runner.py:310] Step = 97100 ; steps/s = 1.62, tokens/s = 82503 (39128 source, 43375 target) ; Learning rate = 0.000284 ; Loss = 1.479469\n",
      "2024-12-05 04:05:14.516000: I runner.py:310] Step = 97200 ; steps/s = 1.62, tokens/s = 82530 (39138 source, 43392 target) ; Learning rate = 0.000284 ; Loss = 1.479800\n",
      "2024-12-05 04:06:15.991000: I runner.py:310] Step = 97300 ; steps/s = 1.63, tokens/s = 82621 (39163 source, 43458 target) ; Learning rate = 0.000283 ; Loss = 1.482181\n",
      "2024-12-05 04:07:17.036000: I runner.py:310] Step = 97400 ; steps/s = 1.64, tokens/s = 81613 (38679 source, 42934 target) ; Learning rate = 0.000283 ; Loss = 1.478564\n",
      "2024-12-05 04:08:18.528000: I runner.py:310] Step = 97500 ; steps/s = 1.63, tokens/s = 82640 (39181 source, 43459 target) ; Learning rate = 0.000283 ; Loss = 1.487455\n",
      "2024-12-05 04:09:20.085000: I runner.py:310] Step = 97600 ; steps/s = 1.62, tokens/s = 82536 (39112 source, 43424 target) ; Learning rate = 0.000283 ; Loss = 1.482326\n",
      "2024-12-05 04:10:21.681000: I runner.py:310] Step = 97700 ; steps/s = 1.62, tokens/s = 82461 (39110 source, 43351 target) ; Learning rate = 0.000283 ; Loss = 1.490691\n",
      "2024-12-05 04:11:22.641000: I runner.py:310] Step = 97800 ; steps/s = 1.64, tokens/s = 81743 (38765 source, 42978 target) ; Learning rate = 0.000283 ; Loss = 1.486354\n",
      "2024-12-05 04:12:24.149000: I runner.py:310] Step = 97900 ; steps/s = 1.63, tokens/s = 82606 (39149 source, 43457 target) ; Learning rate = 0.000282 ; Loss = 1.477935\n",
      "2024-12-05 04:13:25.780000: I runner.py:310] Step = 98000 ; steps/s = 1.62, tokens/s = 82424 (39070 source, 43354 target) ; Learning rate = 0.000282 ; Loss = 1.480835\n",
      "2024-12-05 04:14:27.318000: I runner.py:310] Step = 98100 ; steps/s = 1.63, tokens/s = 82555 (39145 source, 43410 target) ; Learning rate = 0.000282 ; Loss = 1.477229\n",
      "2024-12-05 04:15:28.331000: I runner.py:310] Step = 98200 ; steps/s = 1.64, tokens/s = 81677 (38736 source, 42941 target) ; Learning rate = 0.000282 ; Loss = 1.486870\n",
      "2024-12-05 04:16:29.914000: I runner.py:310] Step = 98300 ; steps/s = 1.62, tokens/s = 82481 (39082 source, 43399 target) ; Learning rate = 0.000282 ; Loss = 1.476996\n",
      "2024-12-05 04:17:31.589000: I runner.py:310] Step = 98400 ; steps/s = 1.62, tokens/s = 82374 (39067 source, 43307 target) ; Learning rate = 0.000282 ; Loss = 1.478174\n",
      "2024-12-05 04:18:33.084000: I runner.py:310] Step = 98500 ; steps/s = 1.63, tokens/s = 82634 (39185 source, 43449 target) ; Learning rate = 0.000282 ; Loss = 1.475348\n",
      "2024-12-05 04:19:34.016000: I runner.py:310] Step = 98600 ; steps/s = 1.64, tokens/s = 81777 (38775 source, 43002 target) ; Learning rate = 0.000281 ; Loss = 1.478735\n",
      "2024-12-05 04:20:35.574000: I runner.py:310] Step = 98700 ; steps/s = 1.62, tokens/s = 82518 (39108 source, 43410 target) ; Learning rate = 0.000281 ; Loss = 1.486391\n",
      "2024-12-05 04:21:37.154000: I runner.py:310] Step = 98800 ; steps/s = 1.62, tokens/s = 82523 (39138 source, 43385 target) ; Learning rate = 0.000281 ; Loss = 1.483611\n",
      "2024-12-05 04:22:38.706000: I runner.py:310] Step = 98900 ; steps/s = 1.62, tokens/s = 82520 (39094 source, 43426 target) ; Learning rate = 0.000281 ; Loss = 1.485657\n",
      "2024-12-05 04:23:39.720000: I runner.py:310] Step = 99000 ; steps/s = 1.64, tokens/s = 81680 (38712 source, 42968 target) ; Learning rate = 0.000281 ; Loss = 1.482388\n",
      "2024-12-05 04:24:41.225000: I runner.py:310] Step = 99100 ; steps/s = 1.63, tokens/s = 82576 (39167 source, 43409 target) ; Learning rate = 0.000281 ; Loss = 1.480592\n",
      "2024-12-05 04:25:42.767000: I runner.py:310] Step = 99200 ; steps/s = 1.63, tokens/s = 82558 (39137 source, 43421 target) ; Learning rate = 0.000281 ; Loss = 1.478671\n",
      "2024-12-05 04:26:44.355000: I runner.py:310] Step = 99300 ; steps/s = 1.62, tokens/s = 82492 (39124 source, 43368 target) ; Learning rate = 0.000280 ; Loss = 1.481357\n",
      "2024-12-05 04:27:45.409000: I runner.py:310] Step = 99400 ; steps/s = 1.64, tokens/s = 81619 (38696 source, 42923 target) ; Learning rate = 0.000280 ; Loss = 1.476114\n",
      "2024-12-05 04:28:46.944000: I runner.py:310] Step = 99500 ; steps/s = 1.63, tokens/s = 82542 (39149 source, 43393 target) ; Learning rate = 0.000280 ; Loss = 1.478051\n",
      "2024-12-05 04:29:48.517000: I runner.py:310] Step = 99600 ; steps/s = 1.62, tokens/s = 82551 (39130 source, 43421 target) ; Learning rate = 0.000280 ; Loss = 1.479203\n",
      "2024-12-05 04:30:50.100000: I runner.py:310] Step = 99700 ; steps/s = 1.62, tokens/s = 82472 (39093 source, 43379 target) ; Learning rate = 0.000280 ; Loss = 1.482152\n",
      "2024-12-05 04:31:51.047000: I runner.py:310] Step = 99800 ; steps/s = 1.64, tokens/s = 81787 (38774 source, 43013 target) ; Learning rate = 0.000280 ; Loss = 1.485520\n",
      "2024-12-05 04:32:52.553000: I runner.py:310] Step = 99900 ; steps/s = 1.63, tokens/s = 82591 (39168 source, 43423 target) ; Learning rate = 0.000280 ; Loss = 1.484872\n",
      "2024-12-05 04:33:54.064000: I runner.py:310] Step = 100000 ; steps/s = 1.63, tokens/s = 82601 (39156 source, 43445 target) ; Learning rate = 0.000280 ; Loss = 1.476376\n",
      "2024-12-05 04:33:55.723000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-100000\n",
      "2024-12-05 04:33:55.723000: I training.py:192] Running evaluation for step 100000\n",
      "2024-12-05 04:38:13.535000: I training.py:192] Evaluation result for step 100000: loss = 1.262311 ; perplexity = 3.533577\n",
      "2024-12-05 04:39:14.476000: I runner.py:310] Step = 100100 ; steps/s = 1.64, tokens/s = 81758 (38757 source, 43001 target) ; Learning rate = 0.000279 ; Loss = 1.483017\n",
      "2024-12-05 04:40:16.002000: I runner.py:310] Step = 100200 ; steps/s = 1.63, tokens/s = 82562 (39134 source, 43428 target) ; Learning rate = 0.000279 ; Loss = 1.483298\n",
      "2024-12-05 04:41:17.586000: I runner.py:310] Step = 100300 ; steps/s = 1.62, tokens/s = 82509 (39104 source, 43405 target) ; Learning rate = 0.000279 ; Loss = 1.476155\n",
      "2024-12-05 04:42:19.053000: I runner.py:310] Step = 100400 ; steps/s = 1.63, tokens/s = 82653 (39184 source, 43469 target) ; Learning rate = 0.000279 ; Loss = 1.478905\n",
      "2024-12-05 04:43:20.048000: I runner.py:310] Step = 100500 ; steps/s = 1.64, tokens/s = 81697 (38753 source, 42944 target) ; Learning rate = 0.000279 ; Loss = 1.488304\n",
      "2024-12-05 04:44:21.599000: I runner.py:310] Step = 100600 ; steps/s = 1.62, tokens/s = 82548 (39126 source, 43422 target) ; Learning rate = 0.000279 ; Loss = 1.480270\n",
      "2024-12-05 04:45:23.132000: I runner.py:310] Step = 100700 ; steps/s = 1.63, tokens/s = 82555 (39146 source, 43409 target) ; Learning rate = 0.000279 ; Loss = 1.478273\n",
      "2024-12-05 04:46:24.692000: I runner.py:310] Step = 100800 ; steps/s = 1.62, tokens/s = 82529 (39129 source, 43400 target) ; Learning rate = 0.000278 ; Loss = 1.474857\n",
      "2024-12-05 04:47:25.584000: I runner.py:310] Step = 100900 ; steps/s = 1.64, tokens/s = 81814 (38790 source, 43024 target) ; Learning rate = 0.000278 ; Loss = 1.480215\n",
      "2024-12-05 04:48:27.088000: I runner.py:310] Step = 101000 ; steps/s = 1.63, tokens/s = 82604 (39166 source, 43438 target) ; Learning rate = 0.000278 ; Loss = 1.480428\n",
      "2024-12-05 04:49:28.602000: I runner.py:310] Step = 101100 ; steps/s = 1.63, tokens/s = 82585 (39163 source, 43422 target) ; Learning rate = 0.000278 ; Loss = 1.478759\n",
      "2024-12-05 04:50:30.166000: I runner.py:310] Step = 101200 ; steps/s = 1.62, tokens/s = 82524 (39128 source, 43396 target) ; Learning rate = 0.000278 ; Loss = 1.483647\n",
      "2024-12-05 04:51:31.073000: I runner.py:310] Step = 101300 ; steps/s = 1.64, tokens/s = 81826 (38763 source, 43063 target) ; Learning rate = 0.000278 ; Loss = 1.470811\n",
      "2024-12-05 04:52:32.564000: I runner.py:310] Step = 101400 ; steps/s = 1.63, tokens/s = 82594 (39136 source, 43458 target) ; Learning rate = 0.000278 ; Loss = 1.478931\n",
      "2024-12-05 04:53:34.097000: I runner.py:310] Step = 101500 ; steps/s = 1.63, tokens/s = 82594 (39157 source, 43437 target) ; Learning rate = 0.000277 ; Loss = 1.480113\n",
      "2024-12-05 04:54:35.614000: I runner.py:310] Step = 101600 ; steps/s = 1.63, tokens/s = 82588 (39198 source, 43390 target) ; Learning rate = 0.000277 ; Loss = 1.483057\n",
      "2024-12-05 04:55:36.579000: I runner.py:310] Step = 101700 ; steps/s = 1.64, tokens/s = 81724 (38723 source, 43001 target) ; Learning rate = 0.000277 ; Loss = 1.481392\n",
      "2024-12-05 04:56:38.094000: I runner.py:310] Step = 101800 ; steps/s = 1.63, tokens/s = 82569 (39148 source, 43421 target) ; Learning rate = 0.000277 ; Loss = 1.488272\n",
      "2024-12-05 04:57:39.638000: I runner.py:310] Step = 101900 ; steps/s = 1.63, tokens/s = 82551 (39165 source, 43386 target) ; Learning rate = 0.000277 ; Loss = 1.482535\n",
      "2024-12-05 04:58:41.114000: I runner.py:310] Step = 102000 ; steps/s = 1.63, tokens/s = 82622 (39150 source, 43472 target) ; Learning rate = 0.000277 ; Loss = 1.478289\n",
      "2024-12-05 04:59:42.102000: I runner.py:310] Step = 102100 ; steps/s = 1.64, tokens/s = 81746 (38746 source, 43000 target) ; Learning rate = 0.000277 ; Loss = 1.476366\n",
      "2024-12-05 05:00:43.658000: I runner.py:310] Step = 102200 ; steps/s = 1.62, tokens/s = 82509 (39136 source, 43373 target) ; Learning rate = 0.000276 ; Loss = 1.478027\n",
      "2024-12-05 05:01:45.189000: I runner.py:310] Step = 102300 ; steps/s = 1.63, tokens/s = 82566 (39127 source, 43439 target) ; Learning rate = 0.000276 ; Loss = 1.477438\n",
      "2024-12-05 05:02:46.783000: I runner.py:310] Step = 102400 ; steps/s = 1.62, tokens/s = 82477 (39103 source, 43374 target) ; Learning rate = 0.000276 ; Loss = 1.484190\n",
      "2024-12-05 05:03:47.800000: I runner.py:310] Step = 102500 ; steps/s = 1.64, tokens/s = 81690 (38727 source, 42963 target) ; Learning rate = 0.000276 ; Loss = 1.481248\n",
      "2024-12-05 05:04:49.306000: I runner.py:310] Step = 102600 ; steps/s = 1.63, tokens/s = 82596 (39177 source, 43419 target) ; Learning rate = 0.000276 ; Loss = 1.476644\n",
      "2024-12-05 05:05:50.850000: I runner.py:310] Step = 102700 ; steps/s = 1.63, tokens/s = 82540 (39140 source, 43400 target) ; Learning rate = 0.000276 ; Loss = 1.472472\n",
      "2024-12-05 05:06:52.397000: I runner.py:310] Step = 102800 ; steps/s = 1.63, tokens/s = 82553 (39118 source, 43435 target) ; Learning rate = 0.000276 ; Loss = 1.477282\n",
      "2024-12-05 05:07:53.339000: I runner.py:310] Step = 102900 ; steps/s = 1.64, tokens/s = 81792 (38760 source, 43032 target) ; Learning rate = 0.000276 ; Loss = 1.487896\n",
      "2024-12-05 05:08:54.829000: I runner.py:310] Step = 103000 ; steps/s = 1.63, tokens/s = 82615 (39151 source, 43464 target) ; Learning rate = 0.000275 ; Loss = 1.475808\n",
      "2024-12-05 05:09:56.396000: I runner.py:310] Step = 103100 ; steps/s = 1.62, tokens/s = 82508 (39139 source, 43369 target) ; Learning rate = 0.000275 ; Loss = 1.482944\n",
      "2024-12-05 05:10:57.912000: I runner.py:310] Step = 103200 ; steps/s = 1.63, tokens/s = 82582 (39164 source, 43418 target) ; Learning rate = 0.000275 ; Loss = 1.479103\n",
      "2024-12-05 05:11:58.882000: I runner.py:310] Step = 103300 ; steps/s = 1.64, tokens/s = 81742 (38750 source, 42992 target) ; Learning rate = 0.000275 ; Loss = 1.483505\n",
      "2024-12-05 05:13:00.420000: I runner.py:310] Step = 103400 ; steps/s = 1.63, tokens/s = 82576 (39156 source, 43420 target) ; Learning rate = 0.000275 ; Loss = 1.481383\n",
      "2024-12-05 05:14:01.961000: I runner.py:310] Step = 103500 ; steps/s = 1.63, tokens/s = 82530 (39112 source, 43418 target) ; Learning rate = 0.000275 ; Loss = 1.477315\n",
      "2024-12-05 05:15:03.502000: I runner.py:310] Step = 103600 ; steps/s = 1.63, tokens/s = 82545 (39142 source, 43403 target) ; Learning rate = 0.000275 ; Loss = 1.476041\n",
      "2024-12-05 05:16:04.467000: I runner.py:310] Step = 103700 ; steps/s = 1.64, tokens/s = 81721 (38725 source, 42996 target) ; Learning rate = 0.000274 ; Loss = 1.480653\n",
      "2024-12-05 05:17:06.018000: I runner.py:310] Step = 103800 ; steps/s = 1.62, tokens/s = 82529 (39143 source, 43386 target) ; Learning rate = 0.000274 ; Loss = 1.474583\n",
      "2024-12-05 05:18:07.520000: I runner.py:310] Step = 103900 ; steps/s = 1.63, tokens/s = 82605 (39159 source, 43446 target) ; Learning rate = 0.000274 ; Loss = 1.483588\n",
      "2024-12-05 05:19:09.108000: I runner.py:310] Step = 104000 ; steps/s = 1.62, tokens/s = 82501 (39127 source, 43374 target) ; Learning rate = 0.000274 ; Loss = 1.486103\n",
      "2024-12-05 05:20:10.061000: I runner.py:310] Step = 104100 ; steps/s = 1.64, tokens/s = 81788 (38758 source, 43030 target) ; Learning rate = 0.000274 ; Loss = 1.471471\n",
      "2024-12-05 05:21:11.584000: I runner.py:310] Step = 104200 ; steps/s = 1.63, tokens/s = 82529 (39139 source, 43390 target) ; Learning rate = 0.000274 ; Loss = 1.477505\n",
      "2024-12-05 05:22:13.069000: I runner.py:310] Step = 104300 ; steps/s = 1.63, tokens/s = 82649 (39204 source, 43445 target) ; Learning rate = 0.000274 ; Loss = 1.479962\n",
      "2024-12-05 05:23:14.033000: I runner.py:310] Step = 104400 ; steps/s = 1.64, tokens/s = 81721 (38729 source, 42992 target) ; Learning rate = 0.000274 ; Loss = 1.476750\n",
      "2024-12-05 05:24:15.597000: I runner.py:310] Step = 104500 ; steps/s = 1.62, tokens/s = 82552 (39122 source, 43430 target) ; Learning rate = 0.000273 ; Loss = 1.476630\n",
      "2024-12-05 05:25:17.106000: I runner.py:310] Step = 104600 ; steps/s = 1.63, tokens/s = 82579 (39145 source, 43434 target) ; Learning rate = 0.000273 ; Loss = 1.478486\n",
      "2024-12-05 05:26:18.565000: I runner.py:310] Step = 104700 ; steps/s = 1.63, tokens/s = 82650 (39216 source, 43434 target) ; Learning rate = 0.000273 ; Loss = 1.472395\n",
      "2024-12-05 05:27:19.554000: I runner.py:310] Step = 104800 ; steps/s = 1.64, tokens/s = 81707 (38730 source, 42977 target) ; Learning rate = 0.000273 ; Loss = 1.476634\n",
      "2024-12-05 05:28:21.115000: I runner.py:310] Step = 104900 ; steps/s = 1.62, tokens/s = 82532 (39111 source, 43421 target) ; Learning rate = 0.000273 ; Loss = 1.474614\n",
      "2024-12-05 05:29:22.676000: I runner.py:310] Step = 105000 ; steps/s = 1.62, tokens/s = 82535 (39139 source, 43396 target) ; Learning rate = 0.000273 ; Loss = 1.476665\n",
      "2024-12-05 05:29:22.677000: I training.py:192] Running evaluation for step 105000\n",
      "2024-12-05 05:33:44.290000: I training.py:192] Evaluation result for step 105000: loss = 1.265298 ; perplexity = 3.544147\n",
      "2024-12-05 05:34:45.645000: I runner.py:310] Step = 105100 ; steps/s = 1.63, tokens/s = 82815 (39263 source, 43552 target) ; Learning rate = 0.000273 ; Loss = 1.479060\n",
      "2024-12-05 05:35:46.627000: I runner.py:310] Step = 105200 ; steps/s = 1.64, tokens/s = 81697 (38714 source, 42983 target) ; Learning rate = 0.000273 ; Loss = 1.478275\n",
      "2024-12-05 05:36:48.145000: I runner.py:310] Step = 105300 ; steps/s = 1.63, tokens/s = 82577 (39137 source, 43440 target) ; Learning rate = 0.000272 ; Loss = 1.475776\n",
      "2024-12-05 05:37:49.621000: I runner.py:310] Step = 105400 ; steps/s = 1.63, tokens/s = 82652 (39202 source, 43450 target) ; Learning rate = 0.000272 ; Loss = 1.478551\n",
      "2024-12-05 05:38:51.125000: I runner.py:310] Step = 105500 ; steps/s = 1.63, tokens/s = 82602 (39168 source, 43434 target) ; Learning rate = 0.000272 ; Loss = 1.489498\n",
      "2024-12-05 05:39:52.087000: I runner.py:310] Step = 105600 ; steps/s = 1.64, tokens/s = 81747 (38751 source, 42996 target) ; Learning rate = 0.000272 ; Loss = 1.480647\n",
      "2024-12-05 05:40:53.666000: I runner.py:310] Step = 105700 ; steps/s = 1.62, tokens/s = 82499 (39086 source, 43413 target) ; Learning rate = 0.000272 ; Loss = 1.480048\n",
      "2024-12-05 05:41:55.200000: I runner.py:310] Step = 105800 ; steps/s = 1.63, tokens/s = 82549 (39172 source, 43377 target) ; Learning rate = 0.000272 ; Loss = 1.476591\n",
      "2024-12-05 05:42:56.776000: I runner.py:310] Step = 105900 ; steps/s = 1.62, tokens/s = 82535 (39145 source, 43390 target) ; Learning rate = 0.000272 ; Loss = 1.473849\n",
      "2024-12-05 05:43:57.742000: I runner.py:310] Step = 106000 ; steps/s = 1.64, tokens/s = 81717 (38717 source, 43000 target) ; Learning rate = 0.000271 ; Loss = 1.470298\n",
      "2024-12-05 05:44:59.270000: I runner.py:310] Step = 106100 ; steps/s = 1.63, tokens/s = 82568 (39145 source, 43423 target) ; Learning rate = 0.000271 ; Loss = 1.477003\n",
      "2024-12-05 05:46:00.857000: I runner.py:310] Step = 106200 ; steps/s = 1.62, tokens/s = 82491 (39114 source, 43377 target) ; Learning rate = 0.000271 ; Loss = 1.476031\n",
      "2024-12-05 05:47:02.463000: I runner.py:310] Step = 106300 ; steps/s = 1.62, tokens/s = 82489 (39121 source, 43368 target) ; Learning rate = 0.000271 ; Loss = 1.479860\n",
      "2024-12-05 05:48:03.457000: I runner.py:310] Step = 106400 ; steps/s = 1.64, tokens/s = 81685 (38707 source, 42978 target) ; Learning rate = 0.000271 ; Loss = 1.484804\n",
      "2024-12-05 05:49:04.996000: I runner.py:310] Step = 106500 ; steps/s = 1.63, tokens/s = 82579 (39152 source, 43427 target) ; Learning rate = 0.000271 ; Loss = 1.475696\n",
      "2024-12-05 05:50:06.514000: I runner.py:310] Step = 106600 ; steps/s = 1.63, tokens/s = 82561 (39141 source, 43420 target) ; Learning rate = 0.000271 ; Loss = 1.472875\n",
      "2024-12-05 05:51:08.029000: I runner.py:310] Step = 106700 ; steps/s = 1.63, tokens/s = 82581 (39168 source, 43413 target) ; Learning rate = 0.000271 ; Loss = 1.480786\n",
      "2024-12-05 05:52:08.994000: I runner.py:310] Step = 106800 ; steps/s = 1.64, tokens/s = 81732 (38712 source, 43020 target) ; Learning rate = 0.000270 ; Loss = 1.476022\n",
      "2024-12-05 05:53:10.490000: I runner.py:310] Step = 106900 ; steps/s = 1.63, tokens/s = 82636 (39179 source, 43457 target) ; Learning rate = 0.000270 ; Loss = 1.475510\n",
      "2024-12-05 05:54:12.014000: I runner.py:310] Step = 107000 ; steps/s = 1.63, tokens/s = 82571 (39147 source, 43424 target) ; Learning rate = 0.000270 ; Loss = 1.483482\n",
      "2024-12-05 05:55:13.599000: I runner.py:310] Step = 107100 ; steps/s = 1.62, tokens/s = 82494 (39122 source, 43372 target) ; Learning rate = 0.000270 ; Loss = 1.482915\n",
      "2024-12-05 05:56:14.561000: I runner.py:310] Step = 107200 ; steps/s = 1.64, tokens/s = 81718 (38746 source, 42972 target) ; Learning rate = 0.000270 ; Loss = 1.472433\n",
      "2024-12-05 05:57:16.044000: I runner.py:310] Step = 107300 ; steps/s = 1.63, tokens/s = 82634 (39188 source, 43446 target) ; Learning rate = 0.000270 ; Loss = 1.475772\n",
      "2024-12-05 05:58:17.537000: I runner.py:310] Step = 107400 ; steps/s = 1.63, tokens/s = 82637 (39179 source, 43458 target) ; Learning rate = 0.000270 ; Loss = 1.482101\n",
      "2024-12-05 05:59:19.070000: I runner.py:310] Step = 107500 ; steps/s = 1.63, tokens/s = 82564 (39143 source, 43421 target) ; Learning rate = 0.000270 ; Loss = 1.484362\n",
      "2024-12-05 06:00:20.061000: I runner.py:310] Step = 107600 ; steps/s = 1.64, tokens/s = 81679 (38720 source, 42959 target) ; Learning rate = 0.000269 ; Loss = 1.467730\n",
      "2024-12-05 06:01:21.628000: I runner.py:310] Step = 107700 ; steps/s = 1.62, tokens/s = 82504 (39122 source, 43382 target) ; Learning rate = 0.000269 ; Loss = 1.476175\n",
      "2024-12-05 06:02:23.164000: I runner.py:310] Step = 107800 ; steps/s = 1.63, tokens/s = 82571 (39150 source, 43421 target) ; Learning rate = 0.000269 ; Loss = 1.477656\n",
      "2024-12-05 06:03:24.638000: I runner.py:310] Step = 107900 ; steps/s = 1.63, tokens/s = 82657 (39187 source, 43470 target) ; Learning rate = 0.000269 ; Loss = 1.480599\n",
      "2024-12-05 06:04:25.583000: I runner.py:310] Step = 108000 ; steps/s = 1.64, tokens/s = 81794 (38738 source, 43056 target) ; Learning rate = 0.000269 ; Loss = 1.471411\n",
      "2024-12-05 06:05:27.134000: I runner.py:310] Step = 108100 ; steps/s = 1.62, tokens/s = 82532 (39131 source, 43401 target) ; Learning rate = 0.000269 ; Loss = 1.478358\n",
      "2024-12-05 06:06:28.649000: I runner.py:310] Step = 108200 ; steps/s = 1.63, tokens/s = 82582 (39160 source, 43422 target) ; Learning rate = 0.000269 ; Loss = 1.475977\n",
      "2024-12-05 06:07:30.228000: I runner.py:310] Step = 108300 ; steps/s = 1.62, tokens/s = 82483 (39124 source, 43359 target) ; Learning rate = 0.000269 ; Loss = 1.486534\n",
      "2024-12-05 06:08:31.156000: I runner.py:310] Step = 108400 ; steps/s = 1.64, tokens/s = 81811 (38774 source, 43037 target) ; Learning rate = 0.000268 ; Loss = 1.480373\n",
      "2024-12-05 06:09:32.728000: I runner.py:310] Step = 108500 ; steps/s = 1.62, tokens/s = 82521 (39106 source, 43415 target) ; Learning rate = 0.000268 ; Loss = 1.483283\n",
      "2024-12-05 06:10:34.277000: I runner.py:310] Step = 108600 ; steps/s = 1.62, tokens/s = 82520 (39122 source, 43398 target) ; Learning rate = 0.000268 ; Loss = 1.470938\n",
      "2024-12-05 06:11:35.279000: I runner.py:310] Step = 108700 ; steps/s = 1.64, tokens/s = 81677 (38758 source, 42919 target) ; Learning rate = 0.000268 ; Loss = 1.477552\n",
      "2024-12-05 06:12:36.779000: I runner.py:310] Step = 108800 ; steps/s = 1.63, tokens/s = 82612 (39126 source, 43486 target) ; Learning rate = 0.000268 ; Loss = 1.478427\n",
      "2024-12-05 06:13:38.242000: I runner.py:310] Step = 108900 ; steps/s = 1.63, tokens/s = 82692 (39210 source, 43482 target) ; Learning rate = 0.000268 ; Loss = 1.468359\n",
      "2024-12-05 06:14:39.721000: I runner.py:310] Step = 109000 ; steps/s = 1.63, tokens/s = 82622 (39172 source, 43450 target) ; Learning rate = 0.000268 ; Loss = 1.476945\n",
      "2024-12-05 06:15:40.727000: I runner.py:310] Step = 109100 ; steps/s = 1.64, tokens/s = 81668 (38736 source, 42932 target) ; Learning rate = 0.000268 ; Loss = 1.479190\n",
      "2024-12-05 06:16:42.216000: I runner.py:310] Step = 109200 ; steps/s = 1.63, tokens/s = 82610 (39166 source, 43444 target) ; Learning rate = 0.000267 ; Loss = 1.474519\n",
      "2024-12-05 06:17:43.777000: I runner.py:310] Step = 109300 ; steps/s = 1.62, tokens/s = 82524 (39134 source, 43390 target) ; Learning rate = 0.000267 ; Loss = 1.472376\n",
      "2024-12-05 06:18:45.274000: I runner.py:310] Step = 109400 ; steps/s = 1.63, tokens/s = 82637 (39188 source, 43449 target) ; Learning rate = 0.000267 ; Loss = 1.475487\n",
      "2024-12-05 06:19:46.184000: I runner.py:310] Step = 109500 ; steps/s = 1.64, tokens/s = 81791 (38758 source, 43033 target) ; Learning rate = 0.000267 ; Loss = 1.472511\n",
      "2024-12-05 06:20:47.698000: I runner.py:310] Step = 109600 ; steps/s = 1.63, tokens/s = 82629 (39197 source, 43432 target) ; Learning rate = 0.000267 ; Loss = 1.473727\n",
      "2024-12-05 06:21:49.175000: I runner.py:310] Step = 109700 ; steps/s = 1.63, tokens/s = 82641 (39174 source, 43467 target) ; Learning rate = 0.000267 ; Loss = 1.472930\n",
      "2024-12-05 06:22:50.661000: I runner.py:310] Step = 109800 ; steps/s = 1.63, tokens/s = 82585 (39143 source, 43442 target) ; Learning rate = 0.000267 ; Loss = 1.478169\n",
      "2024-12-05 06:23:51.654000: I runner.py:310] Step = 109900 ; steps/s = 1.64, tokens/s = 81701 (38714 source, 42987 target) ; Learning rate = 0.000267 ; Loss = 1.479630\n",
      "2024-12-05 06:24:53.200000: I runner.py:310] Step = 110000 ; steps/s = 1.63, tokens/s = 82582 (39169 source, 43413 target) ; Learning rate = 0.000266 ; Loss = 1.473435\n",
      "2024-12-05 06:24:55.279000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-110000\n",
      "2024-12-05 06:24:55.280000: I training.py:192] Running evaluation for step 110000\n",
      "2024-12-05 06:29:23.741000: I training.py:192] Evaluation result for step 110000: loss = 1.274603 ; perplexity = 3.577280\n",
      "2024-12-05 06:30:25.158000: I runner.py:310] Step = 110100 ; steps/s = 1.63, tokens/s = 82711 (39212 source, 43499 target) ; Learning rate = 0.000266 ; Loss = 1.478290\n",
      "2024-12-05 06:31:26.641000: I runner.py:310] Step = 110200 ; steps/s = 1.63, tokens/s = 82608 (39155 source, 43453 target) ; Learning rate = 0.000266 ; Loss = 1.475363\n",
      "2024-12-05 06:32:27.533000: I runner.py:310] Step = 110300 ; steps/s = 1.64, tokens/s = 81827 (38791 source, 43036 target) ; Learning rate = 0.000266 ; Loss = 1.472029\n",
      "2024-12-05 06:33:29.030000: I runner.py:310] Step = 110400 ; steps/s = 1.63, tokens/s = 82592 (39157 source, 43435 target) ; Learning rate = 0.000266 ; Loss = 1.475952\n",
      "2024-12-05 06:34:30.604000: I runner.py:310] Step = 110500 ; steps/s = 1.62, tokens/s = 82518 (39115 source, 43403 target) ; Learning rate = 0.000266 ; Loss = 1.473761\n",
      "2024-12-05 06:35:32.164000: I runner.py:310] Step = 110600 ; steps/s = 1.62, tokens/s = 82538 (39145 source, 43393 target) ; Learning rate = 0.000266 ; Loss = 1.482173\n",
      "2024-12-05 06:36:33.138000: I runner.py:310] Step = 110700 ; steps/s = 1.64, tokens/s = 81763 (38759 source, 43004 target) ; Learning rate = 0.000266 ; Loss = 1.477457\n",
      "2024-12-05 06:37:34.676000: I runner.py:310] Step = 110800 ; steps/s = 1.63, tokens/s = 82545 (39150 source, 43395 target) ; Learning rate = 0.000266 ; Loss = 1.473500\n",
      "2024-12-05 06:38:36.202000: I runner.py:310] Step = 110900 ; steps/s = 1.63, tokens/s = 82562 (39124 source, 43438 target) ; Learning rate = 0.000265 ; Loss = 1.480087\n",
      "2024-12-05 06:39:37.773000: I runner.py:310] Step = 111000 ; steps/s = 1.62, tokens/s = 82527 (39126 source, 43401 target) ; Learning rate = 0.000265 ; Loss = 1.480754\n",
      "2024-12-05 06:40:38.667000: I runner.py:310] Step = 111100 ; steps/s = 1.64, tokens/s = 81818 (38793 source, 43025 target) ; Learning rate = 0.000265 ; Loss = 1.476114\n",
      "2024-12-05 06:41:40.164000: I runner.py:310] Step = 111200 ; steps/s = 1.63, tokens/s = 82607 (39156 source, 43451 target) ; Learning rate = 0.000265 ; Loss = 1.478835\n",
      "2024-12-05 06:42:41.653000: I runner.py:310] Step = 111300 ; steps/s = 1.63, tokens/s = 82615 (39162 source, 43453 target) ; Learning rate = 0.000265 ; Loss = 1.481661\n",
      "2024-12-05 06:43:43.138000: I runner.py:310] Step = 111400 ; steps/s = 1.63, tokens/s = 82635 (39187 source, 43448 target) ; Learning rate = 0.000265 ; Loss = 1.473318\n",
      "2024-12-05 06:44:44.147000: I runner.py:310] Step = 111500 ; steps/s = 1.64, tokens/s = 81650 (38711 source, 42939 target) ; Learning rate = 0.000265 ; Loss = 1.472223\n",
      "2024-12-05 06:45:45.713000: I runner.py:310] Step = 111600 ; steps/s = 1.62, tokens/s = 82532 (39117 source, 43415 target) ; Learning rate = 0.000265 ; Loss = 1.473081\n",
      "2024-12-05 06:46:47.216000: I runner.py:310] Step = 111700 ; steps/s = 1.63, tokens/s = 82646 (39185 source, 43461 target) ; Learning rate = 0.000264 ; Loss = 1.472678\n",
      "2024-12-05 06:47:48.736000: I runner.py:310] Step = 111800 ; steps/s = 1.63, tokens/s = 82557 (39151 source, 43406 target) ; Learning rate = 0.000264 ; Loss = 1.483075\n",
      "2024-12-05 06:48:49.682000: I runner.py:310] Step = 111900 ; steps/s = 1.64, tokens/s = 81698 (38721 source, 42977 target) ; Learning rate = 0.000264 ; Loss = 1.475209\n",
      "2024-12-05 06:49:51.233000: I runner.py:310] Step = 112000 ; steps/s = 1.62, tokens/s = 82563 (39150 source, 43413 target) ; Learning rate = 0.000264 ; Loss = 1.472521\n",
      "2024-12-05 06:50:52.759000: I runner.py:310] Step = 112100 ; steps/s = 1.63, tokens/s = 82583 (39149 source, 43434 target) ; Learning rate = 0.000264 ; Loss = 1.480220\n",
      "2024-12-05 06:51:54.331000: I runner.py:310] Step = 112200 ; steps/s = 1.62, tokens/s = 82537 (39143 source, 43394 target) ; Learning rate = 0.000264 ; Loss = 1.482064\n",
      "2024-12-05 06:52:55.252000: I runner.py:310] Step = 112300 ; steps/s = 1.64, tokens/s = 81781 (38765 source, 43016 target) ; Learning rate = 0.000264 ; Loss = 1.471230\n",
      "2024-12-05 06:53:56.777000: I runner.py:310] Step = 112400 ; steps/s = 1.63, tokens/s = 82574 (39137 source, 43437 target) ; Learning rate = 0.000264 ; Loss = 1.475534\n",
      "2024-12-05 06:54:58.351000: I runner.py:310] Step = 112500 ; steps/s = 1.62, tokens/s = 82494 (39112 source, 43382 target) ; Learning rate = 0.000264 ; Loss = 1.478072\n",
      "2024-12-05 06:55:59.784000: I runner.py:310] Step = 112600 ; steps/s = 1.63, tokens/s = 82709 (39228 source, 43481 target) ; Learning rate = 0.000263 ; Loss = 1.478146\n",
      "2024-12-05 06:57:00.658000: I runner.py:310] Step = 112700 ; steps/s = 1.64, tokens/s = 81827 (38769 source, 43058 target) ; Learning rate = 0.000263 ; Loss = 1.475824\n",
      "2024-12-05 06:58:02.194000: I runner.py:310] Step = 112800 ; steps/s = 1.63, tokens/s = 82551 (39153 source, 43398 target) ; Learning rate = 0.000263 ; Loss = 1.479803\n",
      "2024-12-05 06:59:03.700000: I runner.py:310] Step = 112900 ; steps/s = 1.63, tokens/s = 82630 (39189 source, 43441 target) ; Learning rate = 0.000263 ; Loss = 1.475817\n",
      "2024-12-05 07:00:04.662000: I runner.py:310] Step = 113000 ; steps/s = 1.64, tokens/s = 81744 (38758 source, 42986 target) ; Learning rate = 0.000263 ; Loss = 1.479619\n",
      "2024-12-05 07:01:06.184000: I runner.py:310] Step = 113100 ; steps/s = 1.63, tokens/s = 82633 (39153 source, 43480 target) ; Learning rate = 0.000263 ; Loss = 1.468278\n",
      "2024-12-05 07:02:07.722000: I runner.py:310] Step = 113200 ; steps/s = 1.63, tokens/s = 82550 (39143 source, 43407 target) ; Learning rate = 0.000263 ; Loss = 1.478536\n",
      "2024-12-05 07:03:09.215000: I runner.py:310] Step = 113300 ; steps/s = 1.63, tokens/s = 82614 (39177 source, 43437 target) ; Learning rate = 0.000263 ; Loss = 1.477659\n",
      "2024-12-05 07:04:10.190000: I runner.py:310] Step = 113400 ; steps/s = 1.64, tokens/s = 81683 (38721 source, 42962 target) ; Learning rate = 0.000262 ; Loss = 1.478235\n",
      "2024-12-05 07:05:11.782000: I runner.py:310] Step = 113500 ; steps/s = 1.62, tokens/s = 82487 (39104 source, 43383 target) ; Learning rate = 0.000262 ; Loss = 1.476151\n",
      "2024-12-05 07:06:13.279000: I runner.py:310] Step = 113600 ; steps/s = 1.63, tokens/s = 82624 (39178 source, 43446 target) ; Learning rate = 0.000262 ; Loss = 1.470658\n",
      "2024-12-05 07:07:14.776000: I runner.py:310] Step = 113700 ; steps/s = 1.63, tokens/s = 82627 (39178 source, 43449 target) ; Learning rate = 0.000262 ; Loss = 1.468548\n",
      "2024-12-05 07:08:15.700000: I runner.py:310] Step = 113800 ; steps/s = 1.64, tokens/s = 81770 (38749 source, 43021 target) ; Learning rate = 0.000262 ; Loss = 1.471397\n",
      "2024-12-05 07:09:17.259000: I runner.py:310] Step = 113900 ; steps/s = 1.62, tokens/s = 82531 (39137 source, 43394 target) ; Learning rate = 0.000262 ; Loss = 1.471301\n",
      "2024-12-05 07:10:18.830000: I runner.py:310] Step = 114000 ; steps/s = 1.62, tokens/s = 82542 (39122 source, 43420 target) ; Learning rate = 0.000262 ; Loss = 1.479544\n",
      "2024-12-05 07:11:20.314000: I runner.py:310] Step = 114100 ; steps/s = 1.63, tokens/s = 82612 (39159 source, 43453 target) ; Learning rate = 0.000262 ; Loss = 1.472060\n",
      "2024-12-05 07:12:21.299000: I runner.py:310] Step = 114200 ; steps/s = 1.64, tokens/s = 81699 (38742 source, 42957 target) ; Learning rate = 0.000262 ; Loss = 1.471376\n",
      "2024-12-05 07:13:22.837000: I runner.py:310] Step = 114300 ; steps/s = 1.63, tokens/s = 82530 (39118 source, 43412 target) ; Learning rate = 0.000261 ; Loss = 1.476428\n",
      "2024-12-05 07:14:24.381000: I runner.py:310] Step = 114400 ; steps/s = 1.63, tokens/s = 82563 (39140 source, 43423 target) ; Learning rate = 0.000261 ; Loss = 1.476123\n",
      "2024-12-05 07:15:25.964000: I runner.py:310] Step = 114500 ; steps/s = 1.62, tokens/s = 82504 (39137 source, 43367 target) ; Learning rate = 0.000261 ; Loss = 1.472419\n",
      "2024-12-05 07:16:26.984000: I runner.py:310] Step = 114600 ; steps/s = 1.64, tokens/s = 81670 (38719 source, 42951 target) ; Learning rate = 0.000261 ; Loss = 1.484551\n",
      "2024-12-05 07:17:28.611000: I runner.py:310] Step = 114700 ; steps/s = 1.62, tokens/s = 82454 (39088 source, 43366 target) ; Learning rate = 0.000261 ; Loss = 1.477687\n",
      "2024-12-05 07:18:30.087000: I runner.py:310] Step = 114800 ; steps/s = 1.63, tokens/s = 82628 (39181 source, 43447 target) ; Learning rate = 0.000261 ; Loss = 1.475832\n",
      "2024-12-05 07:19:31.545000: I runner.py:310] Step = 114900 ; steps/s = 1.63, tokens/s = 82660 (39192 source, 43468 target) ; Learning rate = 0.000261 ; Loss = 1.470168\n",
      "2024-12-05 07:20:32.465000: I runner.py:310] Step = 115000 ; steps/s = 1.64, tokens/s = 81805 (38743 source, 43062 target) ; Learning rate = 0.000261 ; Loss = 1.478155\n",
      "2024-12-05 07:20:32.466000: I training.py:192] Running evaluation for step 115000\n",
      "2024-12-05 07:24:49.404000: I training.py:192] Evaluation result for step 115000: loss = 1.274259 ; perplexity = 3.576051\n",
      "2024-12-05 07:25:50.852000: I runner.py:310] Step = 115100 ; steps/s = 1.63, tokens/s = 82697 (39246 source, 43451 target) ; Learning rate = 0.000261 ; Loss = 1.473419\n",
      "2024-12-05 07:26:52.308000: I runner.py:310] Step = 115200 ; steps/s = 1.63, tokens/s = 82664 (39180 source, 43484 target) ; Learning rate = 0.000260 ; Loss = 1.474954\n",
      "2024-12-05 07:27:53.838000: I runner.py:310] Step = 115300 ; steps/s = 1.63, tokens/s = 82573 (39150 source, 43423 target) ; Learning rate = 0.000260 ; Loss = 1.472133\n",
      "2024-12-05 07:28:54.828000: I runner.py:310] Step = 115400 ; steps/s = 1.64, tokens/s = 81708 (38743 source, 42965 target) ; Learning rate = 0.000260 ; Loss = 1.472672\n",
      "2024-12-05 07:29:56.409000: I runner.py:310] Step = 115500 ; steps/s = 1.62, tokens/s = 82507 (39111 source, 43396 target) ; Learning rate = 0.000260 ; Loss = 1.471588\n",
      "2024-12-05 07:30:57.928000: I runner.py:310] Step = 115600 ; steps/s = 1.63, tokens/s = 82528 (39126 source, 43402 target) ; Learning rate = 0.000260 ; Loss = 1.472244\n",
      "2024-12-05 07:31:59.528000: I runner.py:310] Step = 115700 ; steps/s = 1.62, tokens/s = 82466 (39102 source, 43364 target) ; Learning rate = 0.000260 ; Loss = 1.482781\n",
      "2024-12-05 07:33:00.499000: I runner.py:310] Step = 115800 ; steps/s = 1.64, tokens/s = 81764 (38751 source, 43013 target) ; Learning rate = 0.000260 ; Loss = 1.475290\n",
      "2024-12-05 07:34:02.068000: I runner.py:310] Step = 115900 ; steps/s = 1.62, tokens/s = 82518 (39135 source, 43383 target) ; Learning rate = 0.000260 ; Loss = 1.472017\n",
      "2024-12-05 07:35:03.678000: I runner.py:310] Step = 116000 ; steps/s = 1.62, tokens/s = 82443 (39093 source, 43350 target) ; Learning rate = 0.000260 ; Loss = 1.470502\n",
      "2024-12-05 07:36:05.168000: I runner.py:310] Step = 116100 ; steps/s = 1.63, tokens/s = 82651 (39165 source, 43486 target) ; Learning rate = 0.000259 ; Loss = 1.469864\n",
      "2024-12-05 07:37:06.168000: I runner.py:310] Step = 116200 ; steps/s = 1.64, tokens/s = 81697 (38729 source, 42968 target) ; Learning rate = 0.000259 ; Loss = 1.482470\n",
      "2024-12-05 07:38:07.678000: I runner.py:310] Step = 116300 ; steps/s = 1.63, tokens/s = 82569 (39158 source, 43411 target) ; Learning rate = 0.000259 ; Loss = 1.466212\n",
      "2024-12-05 07:39:09.133000: I runner.py:310] Step = 116400 ; steps/s = 1.63, tokens/s = 82651 (39190 source, 43461 target) ; Learning rate = 0.000259 ; Loss = 1.479476\n",
      "2024-12-05 07:40:10.692000: I runner.py:310] Step = 116500 ; steps/s = 1.62, tokens/s = 82545 (39129 source, 43416 target) ; Learning rate = 0.000259 ; Loss = 1.474511\n",
      "2024-12-05 07:41:11.642000: I runner.py:310] Step = 116600 ; steps/s = 1.64, tokens/s = 81732 (38744 source, 42988 target) ; Learning rate = 0.000259 ; Loss = 1.471878\n",
      "2024-12-05 07:42:13.152000: I runner.py:310] Step = 116700 ; steps/s = 1.63, tokens/s = 82639 (39173 source, 43466 target) ; Learning rate = 0.000259 ; Loss = 1.473310\n",
      "2024-12-05 07:43:14.725000: I runner.py:310] Step = 116800 ; steps/s = 1.62, tokens/s = 82496 (39110 source, 43386 target) ; Learning rate = 0.000259 ; Loss = 1.477216\n",
      "2024-12-05 07:44:16.235000: I runner.py:310] Step = 116900 ; steps/s = 1.63, tokens/s = 82586 (39170 source, 43416 target) ; Learning rate = 0.000259 ; Loss = 1.476265\n",
      "2024-12-05 07:45:17.243000: I runner.py:310] Step = 117000 ; steps/s = 1.64, tokens/s = 81653 (38706 source, 42947 target) ; Learning rate = 0.000258 ; Loss = 1.476232\n",
      "2024-12-05 07:46:18.748000: I runner.py:310] Step = 117100 ; steps/s = 1.63, tokens/s = 82616 (39180 source, 43436 target) ; Learning rate = 0.000258 ; Loss = 1.472901\n",
      "2024-12-05 07:47:20.311000: I runner.py:310] Step = 117200 ; steps/s = 1.62, tokens/s = 82518 (39129 source, 43389 target) ; Learning rate = 0.000258 ; Loss = 1.480986\n",
      "2024-12-05 07:48:21.300000: I runner.py:310] Step = 117300 ; steps/s = 1.64, tokens/s = 81694 (38720 source, 42974 target) ; Learning rate = 0.000258 ; Loss = 1.497463\n",
      "2024-12-05 07:49:22.881000: I runner.py:310] Step = 117400 ; steps/s = 1.62, tokens/s = 82549 (39114 source, 43435 target) ; Learning rate = 0.000258 ; Loss = 1.468692\n",
      "2024-12-05 07:50:24.390000: I runner.py:310] Step = 117500 ; steps/s = 1.63, tokens/s = 82559 (39154 source, 43405 target) ; Learning rate = 0.000258 ; Loss = 1.471151\n",
      "2024-12-05 07:51:25.956000: I runner.py:310] Step = 117600 ; steps/s = 1.62, tokens/s = 82546 (39111 source, 43435 target) ; Learning rate = 0.000258 ; Loss = 1.473266\n",
      "2024-12-05 07:52:26.942000: I runner.py:310] Step = 117700 ; steps/s = 1.64, tokens/s = 81678 (38736 source, 42942 target) ; Learning rate = 0.000258 ; Loss = 1.476726\n",
      "2024-12-05 07:53:28.507000: I runner.py:310] Step = 117800 ; steps/s = 1.62, tokens/s = 82564 (39147 source, 43417 target) ; Learning rate = 0.000258 ; Loss = 1.477141\n",
      "2024-12-05 07:54:30.025000: I runner.py:310] Step = 117900 ; steps/s = 1.63, tokens/s = 82570 (39151 source, 43419 target) ; Learning rate = 0.000257 ; Loss = 1.472455\n",
      "2024-12-05 07:55:31.545000: I runner.py:310] Step = 118000 ; steps/s = 1.63, tokens/s = 82559 (39143 source, 43416 target) ; Learning rate = 0.000257 ; Loss = 1.477662\n",
      "2024-12-05 07:56:32.510000: I runner.py:310] Step = 118100 ; steps/s = 1.64, tokens/s = 81738 (38742 source, 42996 target) ; Learning rate = 0.000257 ; Loss = 1.471775\n",
      "2024-12-05 07:57:34.102000: I runner.py:310] Step = 118200 ; steps/s = 1.62, tokens/s = 82488 (39091 source, 43397 target) ; Learning rate = 0.000257 ; Loss = 1.470655\n",
      "2024-12-05 07:58:35.656000: I runner.py:310] Step = 118300 ; steps/s = 1.62, tokens/s = 82529 (39156 source, 43373 target) ; Learning rate = 0.000257 ; Loss = 1.474573\n",
      "2024-12-05 07:59:37.132000: I runner.py:310] Step = 118400 ; steps/s = 1.63, tokens/s = 82645 (39172 source, 43473 target) ; Learning rate = 0.000257 ; Loss = 1.479032\n",
      "2024-12-05 08:00:38.117000: I runner.py:310] Step = 118500 ; steps/s = 1.64, tokens/s = 81707 (38729 source, 42978 target) ; Learning rate = 0.000257 ; Loss = 1.477321\n",
      "2024-12-05 08:01:39.653000: I runner.py:310] Step = 118600 ; steps/s = 1.63, tokens/s = 82525 (39112 source, 43413 target) ; Learning rate = 0.000257 ; Loss = 1.478088\n",
      "2024-12-05 08:02:41.127000: I runner.py:310] Step = 118700 ; steps/s = 1.63, tokens/s = 82680 (39233 source, 43447 target) ; Learning rate = 0.000257 ; Loss = 1.473521\n",
      "2024-12-05 08:03:42.624000: I runner.py:310] Step = 118800 ; steps/s = 1.63, tokens/s = 82603 (39145 source, 43458 target) ; Learning rate = 0.000256 ; Loss = 1.472619\n",
      "2024-12-05 08:04:43.645000: I runner.py:310] Step = 118900 ; steps/s = 1.64, tokens/s = 81647 (38707 source, 42940 target) ; Learning rate = 0.000256 ; Loss = 1.481547\n",
      "2024-12-05 08:05:45.178000: I runner.py:310] Step = 119000 ; steps/s = 1.63, tokens/s = 82571 (39125 source, 43446 target) ; Learning rate = 0.000256 ; Loss = 1.469361\n",
      "2024-12-05 08:06:46.711000: I runner.py:310] Step = 119100 ; steps/s = 1.63, tokens/s = 82574 (39153 source, 43421 target) ; Learning rate = 0.000256 ; Loss = 1.468757\n",
      "2024-12-05 08:07:48.270000: I runner.py:310] Step = 119200 ; steps/s = 1.62, tokens/s = 82531 (39168 source, 43363 target) ; Learning rate = 0.000256 ; Loss = 1.475845\n",
      "2024-12-05 08:08:49.212000: I runner.py:310] Step = 119300 ; steps/s = 1.64, tokens/s = 81764 (38738 source, 43026 target) ; Learning rate = 0.000256 ; Loss = 1.480750\n",
      "2024-12-05 08:09:50.735000: I runner.py:310] Step = 119400 ; steps/s = 1.63, tokens/s = 82608 (39178 source, 43430 target) ; Learning rate = 0.000256 ; Loss = 1.467619\n",
      "2024-12-05 08:10:52.250000: I runner.py:310] Step = 119500 ; steps/s = 1.63, tokens/s = 82588 (39144 source, 43444 target) ; Learning rate = 0.000256 ; Loss = 1.470352\n",
      "2024-12-05 08:11:53.696000: I runner.py:310] Step = 119600 ; steps/s = 1.63, tokens/s = 82672 (39205 source, 43467 target) ; Learning rate = 0.000256 ; Loss = 1.471549\n",
      "2024-12-05 08:12:54.563000: I runner.py:310] Step = 119700 ; steps/s = 1.64, tokens/s = 81829 (38786 source, 43043 target) ; Learning rate = 0.000255 ; Loss = 1.476200\n",
      "2024-12-05 08:13:56.113000: I runner.py:310] Step = 119800 ; steps/s = 1.62, tokens/s = 82572 (39124 source, 43448 target) ; Learning rate = 0.000255 ; Loss = 1.467921\n",
      "2024-12-05 08:14:57.622000: I runner.py:310] Step = 119900 ; steps/s = 1.63, tokens/s = 82588 (39180 source, 43408 target) ; Learning rate = 0.000255 ; Loss = 1.471762\n",
      "2024-12-05 08:15:59.120000: I runner.py:310] Step = 120000 ; steps/s = 1.63, tokens/s = 82595 (39153 source, 43442 target) ; Learning rate = 0.000255 ; Loss = 1.470695\n",
      "2024-12-05 08:16:00.731000: I training.py:176] Saved checkpoint TR-KK-EN-Shared-vocab/ckpt-120000\n",
      "2024-12-05 08:16:00.732000: I training.py:192] Running evaluation for step 120000\n",
      "2024-12-05 08:20:23.366000: I training.py:192] Evaluation result for step 120000: loss = 1.278829 ; perplexity = 3.592432\n",
      "2024-12-05 08:21:24.223000: I runner.py:310] Step = 120100 ; steps/s = 1.64, tokens/s = 81902 (38826 source, 43076 target) ; Learning rate = 0.000255 ; Loss = 1.471994\n",
      "2024-12-05 08:22:25.655000: I runner.py:310] Step = 120200 ; steps/s = 1.63, tokens/s = 82720 (39185 source, 43535 target) ; Learning rate = 0.000255 ; Loss = 1.471935\n",
      "2024-12-05 08:23:27.288000: I runner.py:310] Step = 120300 ; steps/s = 1.62, tokens/s = 82460 (39106 source, 43354 target) ; Learning rate = 0.000255 ; Loss = 1.474917\n",
      "2024-12-05 08:24:28.847000: I runner.py:310] Step = 120400 ; steps/s = 1.62, tokens/s = 82487 (39137 source, 43350 target) ; Learning rate = 0.000255 ; Loss = 1.473446\n",
      "2024-12-05 08:25:29.835000: I runner.py:310] Step = 120500 ; steps/s = 1.64, tokens/s = 81707 (38724 source, 42983 target) ; Learning rate = 0.000255 ; Loss = 1.477599\n",
      "2024-12-05 08:26:31.397000: I runner.py:310] Step = 120600 ; steps/s = 1.62, tokens/s = 82522 (39121 source, 43401 target) ; Learning rate = 0.000255 ; Loss = 1.468915\n",
      "2024-12-05 08:27:32.925000: I runner.py:310] Step = 120700 ; steps/s = 1.63, tokens/s = 82573 (39145 source, 43428 target) ; Learning rate = 0.000254 ; Loss = 1.473156\n",
      "2024-12-05 08:28:34.457000: I runner.py:310] Step = 120800 ; steps/s = 1.63, tokens/s = 82555 (39166 source, 43389 target) ; Learning rate = 0.000254 ; Loss = 1.475222\n",
      "2024-12-05 08:29:35.390000: I runner.py:310] Step = 120900 ; steps/s = 1.64, tokens/s = 81749 (38730 source, 43019 target) ; Learning rate = 0.000254 ; Loss = 1.467264\n",
      "2024-12-05 08:30:36.845000: I runner.py:310] Step = 121000 ; steps/s = 1.63, tokens/s = 82672 (39193 source, 43479 target) ; Learning rate = 0.000254 ; Loss = 1.473728\n",
      "2024-12-05 08:31:38.412000: I runner.py:310] Step = 121100 ; steps/s = 1.62, tokens/s = 82522 (39148 source, 43374 target) ; Learning rate = 0.000254 ; Loss = 1.475418\n",
      "2024-12-05 08:32:39.971000: I runner.py:310] Step = 121200 ; steps/s = 1.62, tokens/s = 82530 (39118 source, 43412 target) ; Learning rate = 0.000254 ; Loss = 1.476762\n",
      "2024-12-05 08:33:40.965000: I runner.py:310] Step = 121300 ; steps/s = 1.64, tokens/s = 81707 (38690 source, 43017 target) ; Learning rate = 0.000254 ; Loss = 1.471859\n",
      "2024-12-05 08:34:42.563000: I runner.py:310] Step = 121400 ; steps/s = 1.62, tokens/s = 82503 (39145 source, 43358 target) ; Learning rate = 0.000254 ; Loss = 1.470407\n",
      "2024-12-05 08:35:44.049000: I runner.py:310] Step = 121500 ; steps/s = 1.63, tokens/s = 82627 (39182 source, 43445 target) ; Learning rate = 0.000254 ; Loss = 1.469508\n",
      "2024-12-05 08:36:45.261000: I runner.py:310] Step = 121600 ; steps/s = 1.63, tokens/s = 81922 (38846 source, 43076 target) ; Learning rate = 0.000253 ; Loss = 1.464545\n",
      "2024-12-05 08:37:46.618000: I runner.py:310] Step = 121700 ; steps/s = 1.63, tokens/s = 82266 (38986 source, 43280 target) ; Learning rate = 0.000253 ; Loss = 1.470372\n",
      "2024-12-05 08:38:48.164000: I runner.py:310] Step = 121800 ; steps/s = 1.62, tokens/s = 82559 (39133 source, 43426 target) ; Learning rate = 0.000253 ; Loss = 1.473425\n",
      "2024-12-05 08:39:49.667000: I runner.py:310] Step = 121900 ; steps/s = 1.63, tokens/s = 82612 (39202 source, 43410 target) ; Learning rate = 0.000253 ; Loss = 1.477028\n",
      "2024-12-05 08:40:50.625000: I runner.py:310] Step = 122000 ; steps/s = 1.64, tokens/s = 81723 (38732 source, 42991 target) ; Learning rate = 0.000253 ; Loss = 1.470080\n",
      "2024-12-05 08:41:52.140000: I runner.py:310] Step = 122100 ; steps/s = 1.63, tokens/s = 82591 (39146 source, 43445 target) ; Learning rate = 0.000253 ; Loss = 1.474558\n",
      "2024-12-05 08:42:53.660000: I runner.py:310] Step = 122200 ; steps/s = 1.63, tokens/s = 82571 (39148 source, 43423 target) ; Learning rate = 0.000253 ; Loss = 1.472649\n",
      "2024-12-05 08:43:55.277000: I runner.py:310] Step = 122300 ; steps/s = 1.62, tokens/s = 82430 (39077 source, 43353 target) ; Learning rate = 0.000253 ; Loss = 1.478238\n",
      "2024-12-05 08:44:56.242000: I runner.py:310] Step = 122400 ; steps/s = 1.64, tokens/s = 81760 (38768 source, 42992 target) ; Learning rate = 0.000253 ; Loss = 1.474495\n",
      "2024-12-05 08:45:57.766000: I runner.py:310] Step = 122500 ; steps/s = 1.63, tokens/s = 82613 (39183 source, 43430 target) ; Learning rate = 0.000253 ; Loss = 1.467399\n",
      "2024-12-05 08:46:59.314000: I runner.py:310] Step = 122600 ; steps/s = 1.62, tokens/s = 82524 (39110 source, 43414 target) ; Learning rate = 0.000252 ; Loss = 1.471778\n",
      "2024-12-05 08:48:00.886000: I runner.py:310] Step = 122700 ; steps/s = 1.62, tokens/s = 82507 (39113 source, 43394 target) ; Learning rate = 0.000252 ; Loss = 1.479115\n",
      "2024-12-05 08:49:01.875000: I runner.py:310] Step = 122800 ; steps/s = 1.64, tokens/s = 81689 (38724 source, 42965 target) ; Learning rate = 0.000252 ; Loss = 1.477631\n",
      "2024-12-05 08:50:03.427000: I runner.py:310] Step = 122900 ; steps/s = 1.62, tokens/s = 82516 (39101 source, 43415 target) ; Learning rate = 0.000252 ; Loss = 1.470271\n",
      "2024-12-05 08:51:04.857000: I runner.py:310] Step = 123000 ; steps/s = 1.63, tokens/s = 82710 (39223 source, 43487 target) ; Learning rate = 0.000252 ; Loss = 1.466698\n",
      "2024-12-05 08:52:06.358000: I runner.py:310] Step = 123100 ; steps/s = 1.63, tokens/s = 82624 (39179 source, 43445 target) ; Learning rate = 0.000252 ; Loss = 1.470753\n",
      "2024-12-05 08:53:07.416000: I runner.py:310] Step = 123200 ; steps/s = 1.64, tokens/s = 81621 (38702 source, 42919 target) ; Learning rate = 0.000252 ; Loss = 1.470973\n",
      "2024-12-05 08:54:08.932000: I runner.py:310] Step = 123300 ; steps/s = 1.63, tokens/s = 82632 (39187 source, 43445 target) ; Learning rate = 0.000252 ; Loss = 1.467981\n",
      "2024-12-05 08:55:10.519000: I runner.py:310] Step = 123400 ; steps/s = 1.62, tokens/s = 82457 (39059 source, 43398 target) ; Learning rate = 0.000252 ; Loss = 1.474011\n",
      "2024-12-05 08:56:12.091000: I runner.py:310] Step = 123500 ; steps/s = 1.62, tokens/s = 82508 (39129 source, 43379 target) ; Learning rate = 0.000252 ; Loss = 1.471284\n",
      "2024-12-05 08:57:13.079000: I runner.py:310] Step = 123600 ; steps/s = 1.64, tokens/s = 81723 (38740 source, 42983 target) ; Learning rate = 0.000251 ; Loss = 1.471481\n",
      "2024-12-05 08:58:14.597000: I runner.py:310] Step = 123700 ; steps/s = 1.63, tokens/s = 82587 (39139 source, 43448 target) ; Learning rate = 0.000251 ; Loss = 1.473196\n",
      "2024-12-05 08:59:16.078000: I runner.py:310] Step = 123800 ; steps/s = 1.63, tokens/s = 82590 (39181 source, 43409 target) ; Learning rate = 0.000251 ; Loss = 1.469155\n",
      "2024-12-05 09:00:17.589000: I runner.py:310] Step = 123900 ; steps/s = 1.63, tokens/s = 82608 (39162 source, 43446 target) ; Learning rate = 0.000251 ; Loss = 1.471343\n",
      "2024-12-05 09:01:18.539000: I runner.py:310] Step = 124000 ; steps/s = 1.64, tokens/s = 81759 (38772 source, 42987 target) ; Learning rate = 0.000251 ; Loss = 1.474542\n",
      "2024-12-05 09:02:20.067000: I runner.py:310] Step = 124100 ; steps/s = 1.63, tokens/s = 82587 (39164 source, 43423 target) ; Learning rate = 0.000251 ; Loss = 1.476688\n",
      "2024-12-05 09:03:21.593000: I runner.py:310] Step = 124200 ; steps/s = 1.63, tokens/s = 82598 (39145 source, 43453 target) ; Learning rate = 0.000251 ; Loss = 1.467074\n",
      "2024-12-05 09:04:23.154000: I runner.py:310] Step = 124300 ; steps/s = 1.62, tokens/s = 82506 (39129 source, 43377 target) ; Learning rate = 0.000251 ; Loss = 1.471672\n",
      "2024-12-05 09:05:24.121000: I runner.py:310] Step = 124400 ; steps/s = 1.64, tokens/s = 81737 (38725 source, 43012 target) ; Learning rate = 0.000251 ; Loss = 1.478440\n",
      "2024-12-05 09:06:25.639000: I runner.py:310] Step = 124500 ; steps/s = 1.63, tokens/s = 82582 (39154 source, 43428 target) ; Learning rate = 0.000251 ; Loss = 1.470421\n",
      "2024-12-05 09:07:27.174000: I runner.py:310] Step = 124600 ; steps/s = 1.63, tokens/s = 82554 (39156 source, 43398 target) ; Learning rate = 0.000250 ; Loss = 1.462785\n",
      "2024-12-05 09:08:28.683000: I runner.py:310] Step = 124700 ; steps/s = 1.63, tokens/s = 82590 (39143 source, 43447 target) ; Learning rate = 0.000250 ; Loss = 1.467143\n",
      "2024-12-05 09:09:29.648000: I runner.py:310] Step = 124800 ; steps/s = 1.64, tokens/s = 81754 (38740 source, 43014 target) ; Learning rate = 0.000250 ; Loss = 1.473957\n",
      "2024-12-05 09:10:31.237000: I runner.py:310] Step = 124900 ; steps/s = 1.62, tokens/s = 82480 (39115 source, 43365 target) ; Learning rate = 0.000250 ; Loss = 1.467381\n",
      "2024-12-05 09:11:32.709000: I runner.py:310] Step = 125000 ; steps/s = 1.63, tokens/s = 82667 (39212 source, 43455 target) ; Learning rate = 0.000250 ; Loss = 1.472542\n",
      "2024-12-05 09:11:32.710000: I training.py:192] Running evaluation for step 125000\n",
      "2024-12-05 09:15:55.088000: I training.py:192] Evaluation result for step 125000: loss = 1.283387 ; perplexity = 3.608843\n",
      "2024-12-05 09:16:56.447000: I runner.py:310] Step = 125100 ; steps/s = 1.63, tokens/s = 82781 (39248 source, 43533 target) ; Learning rate = 0.000250 ; Loss = 1.472549\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Tr-En (Tatoeba) -> Kk-En\n",
    "!onmt-main --model kk-tr-en-shared.py --config Tr-Kk-En_shared_vocab.yml --auto_config --checkpoint_path TR-EN-Shared-vocab/ckpt-100000 train --with_eval --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75754458-7b11-479c-a054-521684ccb045",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 09:19:05.616199: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-05 09:19:06.385567: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-05 09:19:06.385633: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-05 09:19:06.385640: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-05 09:19:07.340000: I main.py:308] Loading model description from TR-KK-EN-Shared-vocab/model_description.py\n",
      "2024-12-05 09:19:07.530000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-05 09:19:07.530000: I main.py:315] Using model:\n",
      "(model): MyCustomTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): WordEmbedder()\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): WordEmbedder()\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-05 09:19:07.536000: I main.py:340] Using parameters:\n",
      "data:\n",
      "  eval_features_file: KK_tokens_valid_shared\n",
      "  eval_labels_file: KK_valid_target_tokens_shared.txt\n",
      "  source_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file: KK_tokens_train_shared\n",
      "  train_labels_file: KK_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: TR-KK-EN-Shared-vocab\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 250000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-05 09:19:07.721299: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-05 09:19:08.312424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-05 09:19:08.467000: I inputter.py:316] Initialized source input layer:\n",
      "2024-12-05 09:19:08.467000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-05 09:19:08.467000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-05 09:19:08.540000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-05 09:19:08.540000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-05 09:19:08.540000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-05 09:19:08.558000: I runner.py:462] Restored checkpoint TR-KK-EN-Shared-vocab/ckpt-100000\n",
      "2024-12-05 09:19:08.596000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-05 09:19:09.096815: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-05 09:19:09.204000: I runner.py:471] Tracing and optimizing the inference graph...\n",
      "2024-12-05 09:19:22.172562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-05 09:19:23.065046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-05 09:19:34.249000: I runner.py:471] 991 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-05 09:19:44.584000: I runner.py:471] 1951 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-05 09:19:54.665000: I runner.py:471] 2719 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-05 09:20:05.073000: I runner.py:471] 3615 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-05 09:20:15.227000: I runner.py:471] 4511 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-05 09:20:25.480000: I runner.py:471] 5375 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-05 09:20:35.846000: I runner.py:471] 6207 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-05 09:20:45.960000: I runner.py:471] 6975 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-05 09:20:56.191000: I runner.py:471] 7903 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-05 09:21:06.668000: I runner.py:471] 8767 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-05 09:21:16.698000: I runner.py:471] 9535 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-05 09:21:27.550000: I runner.py:471] 10495 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-12-05 09:21:46.211000: I runner.py:471] 11950 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-05 09:21:56.437000: I runner.py:471] 12654 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-05 09:22:06.772000: I runner.py:471] 13486 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-05 09:22:16.849000: I runner.py:471] 14382 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-05 09:22:26.920000: I runner.py:471] 15214 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-05 09:22:37.066000: I runner.py:471] 16046 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-05 09:22:47.338000: I runner.py:471] 16942 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-05 09:22:57.504000: I runner.py:471] 17772 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-12-05 09:23:20.370000: I runner.py:471] 17797 predictions are buffered, but waiting for the prediction of queued line 281 to advance the output...\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 onmt-main --config Tr-Kk-En_shared_vocab.yml --auto_config --checkpoint_path TR-KK-EN-Shared-vocab/ckpt-100000 infer --features_file KK_tokens_test_shared --predictions_file output_tr_kk_en_shared_vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1489a64b-ead7-4450-af70-98498e1dc87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 MT-Preparation/subwording/3-desubword.py en_shared_vocab.model output_tr_kk_en_shared_vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b2f1805-8965-40b6-a22c-6a7349d0e166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference first sentence: In the developed world, this figure is 35 25%\n",
      "Translated first sentence: In the developed countries of the world , this figure is 35 25%\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "BLEU:  BLEU = 51.49 72.0/55.8/45.7/38.3 (BP = 1.000 ratio = 1.120 hyp_len = 464044 ref_len = 414303)\n",
      "CHRF:  chrF2 = 76.25\n"
     ]
    }
   ],
   "source": [
    "# BLEU and chrF scores\n",
    "!python3 compute-bleu.py en_test_shuffled.txt-filtered.en output_tr_kk_en_shared_vocab.txt.desubword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3924e665-2243-477c-bb99-015a1d1707a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama METEOR Puanı: 0.7583990764796427\n"
     ]
    }
   ],
   "source": [
    "# Average METEOR score (Ortalama METEOR Puanı)\n",
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "def read_and_tokenize_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return [nltk.word_tokenize(line.strip()) for line in lines]\n",
    "\n",
    "def calculate_meteor(reference_file, hypothesis_file):\n",
    "    references = read_and_tokenize_file(reference_file)\n",
    "    hypotheses = read_and_tokenize_file(hypothesis_file)\n",
    "    \n",
    "    if len(references) != len(hypotheses):\n",
    "        raise ValueError(\"Dosyaların satır sayıları eşleşmiyor\")\n",
    "\n",
    "    total_meteor_score = 0.0\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        total_meteor_score += meteor_score([ref], hyp)\n",
    "\n",
    "    average_meteor_score = total_meteor_score / len(references)\n",
    "    return average_meteor_score\n",
    "\n",
    "reference_file = 'en_test_shuffled.txt-filtered.en'\n",
    "hypothesis_file = 'output_tr_kk_en_shared_vocab.txt.desubword'\n",
    "\n",
    "score = calculate_meteor(reference_file, hypothesis_file)\n",
    "print(f\"Ortalama METEOR Puanı: {score}\") #Average METEOR score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8c141-eb00-4b73-98ad-7becdf52d0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
