{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2c7cb96-8477-40ad-a314-0e9931805f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turkish POS Tagging\n",
    "import re\n",
    "from zemberek.morphology import TurkishMorphology\n",
    "from zemberek.tokenization import TurkishTokenizer\n",
    "\n",
    "def pos_tagger(input, output):\n",
    "    tokenizer = TurkishTokenizer.DEFAULT\n",
    "    morphology = TurkishMorphology.create_with_defaults()\n",
    "    \n",
    "    input_file_path = input\n",
    "    output_file_path = output\n",
    "    \n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        with open(input_file_path, \"r\") as input_file:\n",
    "            line_count = 0\n",
    "            error_count = 0\n",
    "            \n",
    "            for line in input_file:\n",
    "                line_count += 1\n",
    "                tags = []\n",
    "                data = line.strip()\n",
    "                tokens = tokenizer.tokenize(data)\n",
    "                \n",
    "                for token in tokens:\n",
    "                    analysis = morphology.analyze(token.normalized)\n",
    "                    if analysis.analysis_results:\n",
    "                        match = re.search(r\"\\[(.*?):(.*?)(?=[\\]\\.,;!?]|$)\", str(analysis.analysis_results[0]))\n",
    "                        if match:\n",
    "                            tag = match.group(2)\n",
    "                            tag = re.sub(r'[^\\w\\s]', '', str(tag))\n",
    "                            tags.append(str(tag) + \" \")\n",
    "                    else:\n",
    "                        tags.append(\"X \")\n",
    "                \n",
    "                if len(tokens) != len(tags):\n",
    "                    error_count += 1\n",
    "                    print(f\"Error in line {line_count}: Token count ({len(tokens)}) does not match tag count ({len(tags)})\")\n",
    "    \n",
    "                output_file.write(\"\".join(tags) + \"\\n\")\n",
    "    \n",
    "            print(f\"Total lines processed: {line_count}\")\n",
    "            print(f\"Total errors found: {error_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a78bad4-7b3d-477b-ba43-d7bfda973847",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagger(\"Tatoeba.en-tr.tr-filtered.tr.train\",\"Tatoeba_pos_tags_train\")\n",
    "pos_tagger(\"Tatoeba.en-tr.tr-filtered.tr.dev\",\"Tatoeba_pos_tags_dev\")\n",
    "pos_tagger(\"Tatoeba.en-tr.tr-filtered.tr.test\",\"Tatoeba_pos_tags_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1aacb1-da1e-4311-b069-5f1545fee11b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Tatoeba tr_vocab creation\n",
    "!onmt-build-vocab --sentencepiece model_type=bpe --size 32000 --save_vocab tr_vocab Tatoeba.en-tr.tr-filtered.tr.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017418b3-992f-4229-886b-245a975e0838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Tatoeba en_vocab creation\n",
    "!onmt-build-vocab --sentencepiece model_type=bpe --size 32000 --save_vocab en_vocab Tatoeba.en-tr.en-filtered.en.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55fcebbf-54aa-4522-bf41-7a9068a6650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tatoeba Tr side shared_vocab subwording and subword units POS tags\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "\n",
    "def preprocess_text(text, pos_tags):\n",
    "\n",
    "    text = re.sub(r\"([.,!?;:()\\\"'])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    new_pos_tags = []\n",
    "    i = 0  # pos_tags index'i\n",
    "    for word in words:\n",
    "        #if re.match(r\"[.,!?;:()\\\"']\", word):\n",
    "            #new_pos_tags.append('Punc') \n",
    "            #pass\n",
    "        #else:\n",
    "            if i < len(pos_tags):\n",
    "                new_pos_tags.append(pos_tags[i]) \n",
    "                i += 1\n",
    "\n",
    "    return words, new_pos_tags\n",
    "\n",
    "def tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path):\n",
    "\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(model_path)\n",
    "    \n",
    "    with open(input_text_path, 'r', encoding='utf-8') as text_file, \\\n",
    "         open(input_pos_path, 'r', encoding='utf-8') as pos_file, \\\n",
    "         open(output_token_path, 'w', encoding='utf-8') as token_file, \\\n",
    "         open(output_pos_path, 'w', encoding='utf-8') as pos_file_out:\n",
    "        \n",
    "        for text_line, pos_line in zip(text_file, pos_file):\n",
    "            pos_tags = pos_line.strip().split()\n",
    "            preprocessed_words, adjusted_pos_tags = preprocess_text(text_line, pos_tags)\n",
    "            \n",
    "            tokenized_text = []\n",
    "            tokenized_tags = []\n",
    "            \n",
    "            for word, tag in zip(preprocessed_words, adjusted_pos_tags):\n",
    "                tokens = sp.EncodeAsPieces(word)\n",
    "                tokenized_text.extend(tokens)\n",
    "                tokenized_tags.extend([tag] * len(tokens))\n",
    "            \n",
    "            token_file.write(\" \".join(tokenized_text) + \"\\n\")\n",
    "            pos_file_out.write(\" \".join(tokenized_tags) + \"\\n\")\n",
    "\n",
    "\n",
    "input_text_path = \"Tatoeba.en-tr.tr-filtered.tr.train\" \n",
    "input_pos_path = \"Tatoeba_pos_tags_train\" \n",
    "output_token_path = \"Tatoeba_tokens_train_shared\"\n",
    "output_pos_path = \"Tatoeba_pos_tags_train_shared.txt\" \n",
    "model_path = \"kk_tr_shared_vocab.model\" \n",
    "\n",
    "tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3b6d19c-17d1-4d0a-b876-43288e872e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_path = \"Tatoeba.en-tr.tr-filtered.tr.dev\"\n",
    "input_pos_path = \"Tatoeba_pos_tags_dev\" \n",
    "output_token_path = \"Tatoeba_tokens_dev_shared\" \n",
    "output_pos_path = \"Tatoeba_pos_tags_dev_shared.txt\" \n",
    "model_path = \"kk_tr_shared_vocab.model\" \n",
    "\n",
    "tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path)\n",
    "\n",
    "input_text_path = \"Tatoeba.en-tr.tr-filtered.tr.test\"\n",
    "input_pos_path = \"Tatoeba_pos_tags_test\" \n",
    "output_token_path = \"Tatoeba_tokens_test_shared\" \n",
    "output_pos_path = \"Tatoeba_pos_tags_test_shared.txt\" \n",
    "model_path = \"kk_tr_shared_vocab.model\" \n",
    "\n",
    "tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dfe233-9ae6-4040-9316-43ef57eb6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tatoeba tr_vocab subwording and subword POS tags\n",
    "input_text_path = \"Tatoeba.en-tr.tr-filtered.tr.train\" \n",
    "input_pos_path = \"Tatoeba_pos_tags_train\" \n",
    "output_token_path = \"Tatoeba_tokens_train\" \n",
    "output_pos_path = \"Tatoeba_pos_tags_train.txt\" \n",
    "model_path = \"kk_tr_shared_vocab.model\" \n",
    "\n",
    "tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path)\n",
    "\n",
    "input_text_path = \"Tatoeba.en-tr.tr-filtered.tr.dev\" \n",
    "input_pos_path = \"Tatoeba_pos_tags_dev\" \n",
    "output_token_path = \"Tatoeba_tokens_dev\" \n",
    "output_pos_path = \"Tatoeba_pos_tags_dev.txt\" \n",
    "model_path = \"kk_tr_shared_vocab.model\"\n",
    "\n",
    "tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path)\n",
    "\n",
    "input_text_path = \"Tatoeba.en-tr.tr-filtered.tr.test\"\n",
    "input_pos_path = \"Tatoeba_pos_tags_test\" \n",
    "output_token_path = \"Tatoeba_tokens_test\" \n",
    "output_pos_path = \"Tatoeba_pos_tags_test.txt\" \n",
    "model_path = \"kk_tr_shared_vocab.model\" \n",
    "\n",
    "tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54900b24-9ce0-438f-b1c4-4c25ceaa1865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kazakh shared_vocab subwording and subword units POS tags\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "\n",
    "def preprocess_text(text, pos_tags):\n",
    "    \n",
    "    text = re.sub(r\"([.,!?;:()\\\"'])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    new_pos_tags = []\n",
    "    i = 0  # pos_tags index'i\n",
    "    for word in words:\n",
    "        #if re.match(r\"[.,!?;:()\\\"']\", word):\n",
    "            #new_pos_tags.append('Punc') \n",
    "            #pass\n",
    "        #else:\n",
    "            if i < len(pos_tags):\n",
    "                new_pos_tags.append(pos_tags[i]) \n",
    "                i += 1\n",
    "\n",
    "    return words, new_pos_tags\n",
    "\n",
    "def tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path):\n",
    "\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(model_path)\n",
    "    \n",
    "    with open(input_text_path, 'r', encoding='utf-8') as text_file, \\\n",
    "         open(input_pos_path, 'r', encoding='utf-8') as pos_file, \\\n",
    "         open(output_token_path, 'w', encoding='utf-8') as token_file, \\\n",
    "         open(output_pos_path, 'w', encoding='utf-8') as pos_file_out:\n",
    "        \n",
    "        for text_line, pos_line in zip(text_file, pos_file):\n",
    "            pos_tags = pos_line.strip().split()\n",
    "            preprocessed_words, adjusted_pos_tags = preprocess_text(text_line, pos_tags)\n",
    "            \n",
    "            tokenized_text = []\n",
    "            tokenized_tags = []\n",
    "            \n",
    "            for word, tag in zip(preprocessed_words, adjusted_pos_tags):\n",
    "                tokens = sp.EncodeAsPieces(word)\n",
    "                tokenized_text.extend(tokens)\n",
    "                tokenized_tags.extend([tag] * len(tokens))\n",
    "            \n",
    "            token_file.write(\" \".join(tokenized_text) + \"\\n\")\n",
    "            pos_file_out.write(\" \".join(tokenized_tags) + \"\\n\")\n",
    "\n",
    "\n",
    "input_text_path = \"kk_train_shuffled.txt-filtered.kk\"\n",
    "input_pos_path = \"RoBERTa_KK_POS_train\" \n",
    "output_token_path = \"KK_tokens_train_shared\"\n",
    "output_pos_path = \"KK_pos_tags_train_shared.txt\" \n",
    "model_path = \"kk_tr_shared_vocab.model\"\n",
    "\n",
    "tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53963021-c3c3-42cc-ad36-b90101cda41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_path = \"kk_valid_shuffled.txt-filtered.kk\"\n",
    "input_pos_path = \"RoBERTa_KK_POS_valid\" \n",
    "output_token_path = \"KK_tokens_valid_shared\"\n",
    "output_pos_path = \"KK_pos_tags_valid_shared.txt\" \n",
    "model_path = \"kk_tr_shared_vocab.model\" \n",
    "\n",
    "tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path)\n",
    "\n",
    "input_text_path = \"kk_test_shuffled.txt-filtered.kk\" \n",
    "input_pos_path = \"RoBERTa_KK_POS_test\" \n",
    "output_token_path = \"KK_tokens_test_shared\" \n",
    "output_pos_path = \"KK_pos_tags_test_shared.txt\" \n",
    "model_path = \"kk_tr_shared_vocab.model\" \n",
    "\n",
    "tokenize_and_label_separate_files(input_text_path, input_pos_path, output_token_path, output_pos_path, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0e5c68d-b515-43b1-939d-32503258786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tatoeba corpus English side subwording\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "\n",
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.Load(\"en_vocab.model\")\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    text = re.sub(r'([.,!?;:()])', r' \\1 ', text)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def tokenize_and_save(input_file, model, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f, open(output_file, 'w', encoding='utf-8') as fo:\n",
    "        for line in f:\n",
    "            preprocessed_line = preprocess(line.strip())\n",
    "            tokens = model.encode_as_pieces(preprocessed_line)\n",
    "            fo.write(\" \".join(tokens) + \"\\n\")\n",
    "\n",
    "tokenize_and_save(\"Tatoeba.en-tr.en-filtered.en.train\", sp_model, \"Tatoeba_train_target_tokens.txt\")\n",
    "tokenize_and_save(\"Tatoeba.en-tr.en-filtered.en.dev\", sp_model, \"Tatoeba_dev_target_tokens.txt\")\n",
    "tokenize_and_save(\"Tatoeba.en-tr.en-filtered.en.test\", sp_model, \"Tatoeba_test_target_tokens.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87d36226-54c1-4fd1-9d9e-71c28eba5872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tatoeba corpus English side shared vocab subwording\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "\n",
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.Load(\"en_shared_vocab.model\")\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    text = re.sub(r'([.,!?;:()])', r' \\1 ', text)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def tokenize_and_save(input_file, model, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f, open(output_file, 'w', encoding='utf-8') as fo:\n",
    "        for line in f:\n",
    "            preprocessed_line = preprocess(line.strip())\n",
    "            tokens = model.encode_as_pieces(preprocessed_line)\n",
    "            fo.write(\" \".join(tokens) + \"\\n\")\n",
    "\n",
    "tokenize_and_save(\"Tatoeba.en-tr.en-filtered.en.train\", sp_model, \"Tatoeba_train_target_tokens_shared.txt\")\n",
    "tokenize_and_save(\"Tatoeba.en-tr.en-filtered.en.dev\", sp_model, \"Tatoeba_dev_target_tokens_shared.txt\")\n",
    "tokenize_and_save(\"Tatoeba.en-tr.en-filtered.en.test\", sp_model, \"Tatoeba_test_target_tokens_shared.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de390a3b-7d13-40f9-8ebe-71cfecb07f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kazakh corpus English side shared vocab subwording\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "\n",
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.Load(\"en_shared_vocab.model\")\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    text = re.sub(r'([.,!?;:()])', r' \\1 ', text)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def tokenize_and_save(input_file, model, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f, open(output_file, 'w', encoding='utf-8') as fo:\n",
    "        for line in f:\n",
    "            preprocessed_line = preprocess(line.strip())\n",
    "            tokens = model.encode_as_pieces(preprocessed_line)\n",
    "            fo.write(\" \".join(tokens) + \"\\n\")\n",
    "\n",
    "tokenize_and_save(\"en_train_shuffled.txt-filtered.en\", sp_model, \"KK_train_target_tokens_shared.txt\")\n",
    "tokenize_and_save(\"en_valid_shuffled.txt-filtered.en\", sp_model, \"KK_valid_target_tokens_shared.txt\")\n",
    "tokenize_and_save(\"en_test_shuffled.txt-filtered.en\", sp_model, \"KK_test_target_tokens_shared.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3765a643-c0e4-4ced-ac42-9bddd3b07fe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-28 13:42:38.609568: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-28 13:42:39.408065: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-28 13:42:39.408129: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-28 13:42:39.408136: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-11-28 13:42:40.363000: I onmt-main:8] Creating model directory POS_TR_KK_EN\n",
      "2024-11-28 13:42:40.565000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-11-28 13:42:40.565000: I main.py:315] Using model:\n",
      "(model): MultiFeaturesTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): ParallelInputter(\n",
      "      (inputters): ListWrapper(\n",
      "        (0): WordEmbedder()\n",
      "        (1): WordEmbedder()\n",
      "      )\n",
      "      (reducer): ConcatReducer()\n",
      "    )\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): ParallelInputter(\n",
      "        (inputters): ListWrapper(\n",
      "          (0): WordEmbedder()\n",
      "          (1): WordEmbedder()\n",
      "        )\n",
      "        (reducer): ConcatReducer()\n",
      "      )\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-11-28 13:42:40.568531: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-28 13:42:42.117719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-11-28 13:42:42.118428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7715 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-11-28 13:42:42.118862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 6099 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "2024-11-28 13:42:42.122000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file:\n",
      "  - Tatoeba_tokens_dev_shared\n",
      "  - Tatoeba_pos_tags_dev_shared.txt\n",
      "  eval_labels_file: Tatoeba_dev_target_tokens_shared.txt\n",
      "  source_1_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  source_2_vocabulary: Zemberek_TR_unique_pos\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file:\n",
      "  - Tatoeba_tokens_train_shared\n",
      "  - Tatoeba_pos_tags_train_shared.txt\n",
      "  train_labels_file: Tatoeba_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: POS_TR_KK_EN\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 205000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-11-28 13:42:42.450000: I inputter.py:316] Initialized source_1 input layer:\n",
      "2024-11-28 13:42:42.450000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-11-28 13:42:42.450000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-11-28 13:42:42.453000: I inputter.py:316] Initialized source_2 input layer:\n",
      "2024-11-28 13:42:42.454000: I inputter.py:316]  - vocabulary size: 21\n",
      "2024-11-28 13:42:42.454000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-11-28 13:42:42.527000: I inputter.py:316] Initialized target input layer:\n",
      "2024-11-28 13:42:42.528000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-11-28 13:42:42.528000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-11-28 13:42:42.532000: W runner.py:269] No checkpoint to restore in POS_TR_KK_EN\n",
      "2024-11-28 13:42:42.534000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "2024-11-28 13:42:42.587000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-11-28 13:42:43.627742: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-11-28 13:42:43.752000: I main.py:325] Accumulate gradients of 7 iterations to reach effective batch size of 25000\n",
      "2024-11-28 13:42:43.874000: I mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2024-11-28 13:42:44.103000: I dataset_ops.py:2542] Training on 647485 examples\n",
      "2024-11-28 13:43:52.412293: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-11-28 13:43:53.531186: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-11-28 13:43:53.693582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-11-28 13:44:03.265000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-28 13:44:03.288000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-28 13:44:04.850000: I cross_device_ops.py:897] batch_all_reduce: 261 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-11-28 13:44:09.870000: I cross_device_ops.py:897] batch_all_reduce: 261 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-11-28 13:44:16.645000: I runner.py:310] Number of model parameters: 93357261\n",
      "2024-11-28 13:44:16.649000: I runner.py:310] Number of model weights: 261 (trainable = 261, non trainable = 0)\n",
      "2024-11-28 13:44:16.683000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-28 13:44:18.793000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-1\n",
      "2024-11-28 13:44:19.553000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-28 13:44:19.575000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-28 13:44:20.269000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-28 13:44:20.292000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-28 13:44:20.957000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-28 13:44:20.981000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-28 13:44:21.583000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-28 13:45:19.875000: I runner.py:310] Step = 100 ; steps/s = 1.63, tokens/s = 43455 (43455 target) ; Learning rate = 0.000009 ; Loss = 9.589040\n",
      "2024-11-28 13:46:20.920000: I runner.py:310] Step = 200 ; steps/s = 1.64, tokens/s = 43684 (43684 target) ; Learning rate = 0.000018 ; Loss = 8.501582\n",
      "2024-11-28 13:47:23.670000: I runner.py:310] Step = 300 ; steps/s = 1.59, tokens/s = 41706 (41706 target) ; Learning rate = 0.000027 ; Loss = 7.160185\n",
      "2024-11-28 13:48:24.816000: I runner.py:310] Step = 400 ; steps/s = 1.64, tokens/s = 43614 (43614 target) ; Learning rate = 0.000035 ; Loss = 6.183506\n",
      "2024-11-28 13:49:26.026000: I runner.py:310] Step = 500 ; steps/s = 1.63, tokens/s = 43559 (43559 target) ; Learning rate = 0.000044 ; Loss = 5.844463\n",
      "2024-11-28 13:50:26.735000: I runner.py:310] Step = 600 ; steps/s = 1.65, tokens/s = 43104 (43104 target) ; Learning rate = 0.000053 ; Loss = 5.263995\n",
      "2024-11-28 13:51:27.825000: I runner.py:310] Step = 700 ; steps/s = 1.64, tokens/s = 43651 (43651 target) ; Learning rate = 0.000062 ; Loss = 4.978475\n",
      "2024-11-28 13:52:28.448000: I runner.py:310] Step = 800 ; steps/s = 1.65, tokens/s = 43182 (43182 target) ; Learning rate = 0.000071 ; Loss = 4.785862\n",
      "2024-11-28 13:53:29.474000: I runner.py:310] Step = 900 ; steps/s = 1.64, tokens/s = 43693 (43693 target) ; Learning rate = 0.000080 ; Loss = 4.562232\n",
      "2024-11-28 13:54:30.523000: I runner.py:310] Step = 1000 ; steps/s = 1.64, tokens/s = 43678 (43678 target) ; Learning rate = 0.000088 ; Loss = 4.450618\n",
      "2024-11-28 13:55:31.363000: I runner.py:310] Step = 1100 ; steps/s = 1.64, tokens/s = 43020 (43020 target) ; Learning rate = 0.000097 ; Loss = 4.299417\n",
      "2024-11-28 13:56:32.393000: I runner.py:310] Step = 1200 ; steps/s = 1.64, tokens/s = 43697 (43697 target) ; Learning rate = 0.000106 ; Loss = 3.965107\n",
      "2024-11-28 13:57:32.968000: I runner.py:310] Step = 1300 ; steps/s = 1.65, tokens/s = 43191 (43191 target) ; Learning rate = 0.000115 ; Loss = 4.548756\n",
      "2024-11-28 13:58:34.009000: I runner.py:310] Step = 1400 ; steps/s = 1.64, tokens/s = 43695 (43695 target) ; Learning rate = 0.000124 ; Loss = 3.775358\n",
      "2024-11-28 13:59:35.067000: I runner.py:310] Step = 1500 ; steps/s = 1.64, tokens/s = 43671 (43671 target) ; Learning rate = 0.000133 ; Loss = 3.598196\n",
      "2024-11-28 14:00:35.575000: I runner.py:310] Step = 1600 ; steps/s = 1.65, tokens/s = 43249 (43249 target) ; Learning rate = 0.000142 ; Loss = 3.614839\n",
      "2024-11-28 14:01:36.648000: I runner.py:310] Step = 1700 ; steps/s = 1.64, tokens/s = 43663 (43663 target) ; Learning rate = 0.000150 ; Loss = 3.415058\n",
      "2024-11-28 14:02:37.738000: I runner.py:310] Step = 1800 ; steps/s = 1.64, tokens/s = 43654 (43654 target) ; Learning rate = 0.000159 ; Loss = 3.110598\n",
      "2024-11-28 14:03:38.341000: I runner.py:310] Step = 1900 ; steps/s = 1.65, tokens/s = 43191 (43191 target) ; Learning rate = 0.000168 ; Loss = 3.086717\n",
      "2024-11-28 14:04:39.329000: I runner.py:310] Step = 2000 ; steps/s = 1.64, tokens/s = 43715 (43715 target) ; Learning rate = 0.000177 ; Loss = 2.924254\n",
      "2024-11-28 14:05:39.894000: I runner.py:310] Step = 2100 ; steps/s = 1.65, tokens/s = 43211 (43211 target) ; Learning rate = 0.000186 ; Loss = 2.914441\n",
      "2024-11-28 14:06:40.923000: I runner.py:310] Step = 2200 ; steps/s = 1.64, tokens/s = 43699 (43699 target) ; Learning rate = 0.000195 ; Loss = 2.819425\n",
      "2024-11-28 14:07:41.997000: I runner.py:310] Step = 2300 ; steps/s = 1.64, tokens/s = 43655 (43655 target) ; Learning rate = 0.000203 ; Loss = 2.573839\n",
      "2024-11-28 14:08:42.557000: I runner.py:310] Step = 2400 ; steps/s = 1.65, tokens/s = 43214 (43214 target) ; Learning rate = 0.000212 ; Loss = 2.668379\n",
      "2024-11-28 14:09:44.825000: I runner.py:310] Step = 2500 ; steps/s = 1.61, tokens/s = 42825 (42825 target) ; Learning rate = 0.000221 ; Loss = 2.500884\n",
      "2024-11-28 14:10:46.892000: I runner.py:310] Step = 2600 ; steps/s = 1.61, tokens/s = 42168 (42168 target) ; Learning rate = 0.000230 ; Loss = 3.540845\n",
      "2024-11-28 14:11:48.591000: I runner.py:310] Step = 2700 ; steps/s = 1.62, tokens/s = 43216 (43216 target) ; Learning rate = 0.000239 ; Loss = 2.440482\n",
      "2024-11-28 14:12:49.594000: I runner.py:310] Step = 2800 ; steps/s = 1.64, tokens/s = 43713 (43713 target) ; Learning rate = 0.000248 ; Loss = 2.384397\n",
      "2024-11-28 14:13:50.597000: I runner.py:310] Step = 2900 ; steps/s = 1.64, tokens/s = 42914 (42914 target) ; Learning rate = 0.000256 ; Loss = 2.474072\n",
      "2024-11-28 14:14:51.629000: I runner.py:310] Step = 3000 ; steps/s = 1.64, tokens/s = 43694 (43694 target) ; Learning rate = 0.000265 ; Loss = 2.281133\n",
      "2024-11-28 14:15:52.785000: I runner.py:310] Step = 3100 ; steps/s = 1.64, tokens/s = 43600 (43600 target) ; Learning rate = 0.000274 ; Loss = 2.347102\n",
      "2024-11-28 14:16:53.471000: I runner.py:310] Step = 3200 ; steps/s = 1.65, tokens/s = 43121 (43121 target) ; Learning rate = 0.000283 ; Loss = 2.285836\n",
      "2024-11-28 14:17:54.602000: I runner.py:310] Step = 3300 ; steps/s = 1.64, tokens/s = 43635 (43635 target) ; Learning rate = 0.000292 ; Loss = 2.396050\n",
      "2024-11-28 14:18:55.262000: I runner.py:310] Step = 3400 ; steps/s = 1.65, tokens/s = 43137 (43137 target) ; Learning rate = 0.000301 ; Loss = 2.244017\n",
      "2024-11-28 14:19:56.311000: I runner.py:310] Step = 3500 ; steps/s = 1.64, tokens/s = 43678 (43678 target) ; Learning rate = 0.000309 ; Loss = 2.170656\n",
      "2024-11-28 14:20:57.462000: I runner.py:310] Step = 3600 ; steps/s = 1.64, tokens/s = 43609 (43609 target) ; Learning rate = 0.000318 ; Loss = 2.338153\n",
      "2024-11-28 14:21:58.088000: I runner.py:310] Step = 3700 ; steps/s = 1.65, tokens/s = 43162 (43162 target) ; Learning rate = 0.000327 ; Loss = 2.288167\n",
      "2024-11-28 14:22:59.164000: I runner.py:310] Step = 3800 ; steps/s = 1.64, tokens/s = 43659 (43659 target) ; Learning rate = 0.000336 ; Loss = 2.280324\n",
      "2024-11-28 14:23:59.869000: I runner.py:310] Step = 3900 ; steps/s = 1.65, tokens/s = 43109 (43109 target) ; Learning rate = 0.000345 ; Loss = 2.370127\n",
      "2024-11-28 14:25:01.017000: I runner.py:310] Step = 4000 ; steps/s = 1.64, tokens/s = 43615 (43615 target) ; Learning rate = 0.000354 ; Loss = 2.133036\n",
      "2024-11-28 14:26:02.060000: I runner.py:310] Step = 4100 ; steps/s = 1.64, tokens/s = 43687 (43687 target) ; Learning rate = 0.000362 ; Loss = 2.207056\n",
      "2024-11-28 14:27:02.663000: I runner.py:310] Step = 4200 ; steps/s = 1.65, tokens/s = 43175 (43175 target) ; Learning rate = 0.000371 ; Loss = 2.245334\n",
      "2024-11-28 14:28:03.692000: I runner.py:310] Step = 4300 ; steps/s = 1.64, tokens/s = 43697 (43697 target) ; Learning rate = 0.000380 ; Loss = 2.160534\n",
      "2024-11-28 14:29:04.702000: I runner.py:310] Step = 4400 ; steps/s = 1.64, tokens/s = 43706 (43706 target) ; Learning rate = 0.000389 ; Loss = 2.152627\n",
      "2024-11-28 14:30:05.449000: I runner.py:310] Step = 4500 ; steps/s = 1.65, tokens/s = 43084 (43084 target) ; Learning rate = 0.000398 ; Loss = 2.062755\n",
      "2024-11-28 14:31:07.882000: I runner.py:310] Step = 4600 ; steps/s = 1.60, tokens/s = 42711 (42711 target) ; Learning rate = 0.000407 ; Loss = 2.110066\n",
      "2024-11-28 14:32:08.465000: I runner.py:310] Step = 4700 ; steps/s = 1.65, tokens/s = 43195 (43195 target) ; Learning rate = 0.000416 ; Loss = 2.029034\n",
      "2024-11-28 14:33:09.492000: I runner.py:310] Step = 4800 ; steps/s = 1.64, tokens/s = 43697 (43697 target) ; Learning rate = 0.000424 ; Loss = 2.042502\n",
      "2024-11-28 14:34:10.703000: I runner.py:310] Step = 4900 ; steps/s = 1.63, tokens/s = 43570 (43570 target) ; Learning rate = 0.000433 ; Loss = 2.040673\n",
      "2024-11-28 14:35:11.305000: I runner.py:310] Step = 5000 ; steps/s = 1.65, tokens/s = 43182 (43182 target) ; Learning rate = 0.000442 ; Loss = 2.094405\n",
      "2024-11-28 14:35:11.306000: I training.py:192] Running evaluation for step 5000\n",
      "2024-11-28 14:36:19.518000: I training.py:192] Evaluation result for step 5000: loss = 0.851772 ; perplexity = 2.343797\n",
      "2024-11-28 14:37:20.607000: I runner.py:310] Step = 5100 ; steps/s = 1.64, tokens/s = 43662 (43662 target) ; Learning rate = 0.000451 ; Loss = 2.151003\n",
      "2024-11-28 14:38:21.438000: I runner.py:310] Step = 5200 ; steps/s = 1.64, tokens/s = 43023 (43023 target) ; Learning rate = 0.000460 ; Loss = 2.535046\n",
      "2024-11-28 14:39:22.723000: I runner.py:310] Step = 5300 ; steps/s = 1.63, tokens/s = 43514 (43514 target) ; Learning rate = 0.000469 ; Loss = 1.959447\n",
      "2024-11-28 14:40:23.918000: I runner.py:310] Step = 5400 ; steps/s = 1.63, tokens/s = 43576 (43576 target) ; Learning rate = 0.000477 ; Loss = 1.987696\n",
      "2024-11-28 14:41:24.827000: I runner.py:310] Step = 5500 ; steps/s = 1.64, tokens/s = 42961 (42961 target) ; Learning rate = 0.000486 ; Loss = 2.011844\n",
      "2024-11-28 14:42:26.090000: I runner.py:310] Step = 5600 ; steps/s = 1.63, tokens/s = 43519 (43519 target) ; Learning rate = 0.000495 ; Loss = 1.965406\n",
      "2024-11-28 14:43:27.348000: I runner.py:310] Step = 5700 ; steps/s = 1.63, tokens/s = 43541 (43541 target) ; Learning rate = 0.000504 ; Loss = 2.054862\n",
      "2024-11-28 14:44:28.148000: I runner.py:310] Step = 5800 ; steps/s = 1.64, tokens/s = 43032 (43032 target) ; Learning rate = 0.000513 ; Loss = 1.920819\n",
      "2024-11-28 14:45:29.461000: I runner.py:310] Step = 5900 ; steps/s = 1.63, tokens/s = 43495 (43495 target) ; Learning rate = 0.000522 ; Loss = 1.949010\n",
      "2024-11-28 14:46:30.615000: I runner.py:310] Step = 6000 ; steps/s = 1.64, tokens/s = 42800 (42800 target) ; Learning rate = 0.000530 ; Loss = 1.966135\n",
      "2024-11-28 14:47:31.771000: I runner.py:310] Step = 6100 ; steps/s = 1.64, tokens/s = 43612 (43612 target) ; Learning rate = 0.000539 ; Loss = 1.956537\n",
      "2024-11-28 14:48:32.869000: I runner.py:310] Step = 6200 ; steps/s = 1.64, tokens/s = 43645 (43645 target) ; Learning rate = 0.000548 ; Loss = 1.904018\n",
      "2024-11-28 14:49:33.517000: I runner.py:310] Step = 6300 ; steps/s = 1.65, tokens/s = 43143 (43143 target) ; Learning rate = 0.000557 ; Loss = 1.983572\n",
      "2024-11-28 14:50:34.554000: I runner.py:310] Step = 6400 ; steps/s = 1.64, tokens/s = 43697 (43697 target) ; Learning rate = 0.000566 ; Loss = 1.969247\n",
      "2024-11-28 14:51:35.279000: I runner.py:310] Step = 6500 ; steps/s = 1.65, tokens/s = 43094 (43094 target) ; Learning rate = 0.000575 ; Loss = 1.880513\n",
      "2024-11-28 14:52:36.309000: I runner.py:310] Step = 6600 ; steps/s = 1.64, tokens/s = 43695 (43695 target) ; Learning rate = 0.000583 ; Loss = 1.842121\n",
      "2024-11-28 14:53:37.416000: I runner.py:310] Step = 6700 ; steps/s = 1.64, tokens/s = 43640 (43640 target) ; Learning rate = 0.000592 ; Loss = 1.937931\n",
      "2024-11-28 14:54:37.976000: I runner.py:310] Step = 6800 ; steps/s = 1.65, tokens/s = 43211 (43211 target) ; Learning rate = 0.000601 ; Loss = 1.863464\n",
      "2024-11-28 14:55:39.056000: I runner.py:310] Step = 6900 ; steps/s = 1.64, tokens/s = 43663 (43663 target) ; Learning rate = 0.000610 ; Loss = 1.882996\n",
      "2024-11-28 14:56:40.027000: I runner.py:310] Step = 7000 ; steps/s = 1.64, tokens/s = 43736 (43736 target) ; Learning rate = 0.000619 ; Loss = 1.873964\n",
      "2024-11-28 14:57:40.510000: I runner.py:310] Step = 7100 ; steps/s = 1.65, tokens/s = 43270 (43270 target) ; Learning rate = 0.000628 ; Loss = 1.846085\n",
      "2024-11-28 14:58:41.518000: I runner.py:310] Step = 7200 ; steps/s = 1.64, tokens/s = 43711 (43711 target) ; Learning rate = 0.000636 ; Loss = 1.847597\n",
      "2024-11-28 14:59:42.061000: I runner.py:310] Step = 7300 ; steps/s = 1.65, tokens/s = 43226 (43226 target) ; Learning rate = 0.000645 ; Loss = 1.836661\n",
      "2024-11-28 15:00:43.058000: I runner.py:310] Step = 7400 ; steps/s = 1.64, tokens/s = 43707 (43707 target) ; Learning rate = 0.000654 ; Loss = 1.851675\n",
      "2024-11-28 15:01:44.076000: I runner.py:310] Step = 7500 ; steps/s = 1.64, tokens/s = 43712 (43712 target) ; Learning rate = 0.000663 ; Loss = 1.835032\n",
      "2024-11-28 15:02:44.643000: I runner.py:310] Step = 7600 ; steps/s = 1.65, tokens/s = 43204 (43204 target) ; Learning rate = 0.000672 ; Loss = 1.830501\n",
      "2024-11-28 15:03:45.608000: I runner.py:310] Step = 7700 ; steps/s = 1.64, tokens/s = 43741 (43741 target) ; Learning rate = 0.000681 ; Loss = 1.830368\n",
      "2024-11-28 15:04:46.163000: I runner.py:310] Step = 7800 ; steps/s = 1.65, tokens/s = 43222 (43222 target) ; Learning rate = 0.000690 ; Loss = 1.866068\n",
      "2024-11-28 15:05:47.130000: I runner.py:310] Step = 7900 ; steps/s = 1.64, tokens/s = 43741 (43741 target) ; Learning rate = 0.000698 ; Loss = 1.782622\n",
      "2024-11-28 15:06:48.104000: I runner.py:310] Step = 8000 ; steps/s = 1.64, tokens/s = 43734 (43734 target) ; Learning rate = 0.000707 ; Loss = 1.836697\n",
      "2024-11-28 15:07:48.652000: I runner.py:310] Step = 8100 ; steps/s = 1.65, tokens/s = 43224 (43224 target) ; Learning rate = 0.000716 ; Loss = 1.783995\n",
      "2024-11-28 15:08:49.636000: I runner.py:310] Step = 8200 ; steps/s = 1.64, tokens/s = 43732 (43732 target) ; Learning rate = 0.000725 ; Loss = 1.824971\n",
      "2024-11-28 15:09:50.595000: I runner.py:310] Step = 8300 ; steps/s = 1.64, tokens/s = 43740 (43740 target) ; Learning rate = 0.000734 ; Loss = 1.776678\n",
      "2024-11-28 15:10:51.113000: I runner.py:310] Step = 8400 ; steps/s = 1.65, tokens/s = 43241 (43241 target) ; Learning rate = 0.000743 ; Loss = 1.788861\n",
      "2024-11-28 15:11:52.046000: I runner.py:310] Step = 8500 ; steps/s = 1.64, tokens/s = 43772 (43772 target) ; Learning rate = 0.000751 ; Loss = 1.824938\n",
      "2024-11-28 15:12:52.576000: I runner.py:310] Step = 8600 ; steps/s = 1.65, tokens/s = 43226 (43226 target) ; Learning rate = 0.000760 ; Loss = 1.764644\n",
      "2024-11-28 15:13:53.639000: I runner.py:310] Step = 8700 ; steps/s = 1.64, tokens/s = 43671 (43671 target) ; Learning rate = 0.000769 ; Loss = 1.798963\n",
      "2024-11-28 15:14:54.655000: I runner.py:310] Step = 8800 ; steps/s = 1.64, tokens/s = 43705 (43705 target) ; Learning rate = 0.000778 ; Loss = 1.797861\n",
      "2024-11-28 15:15:55.182000: I runner.py:310] Step = 8900 ; steps/s = 1.65, tokens/s = 43236 (43236 target) ; Learning rate = 0.000787 ; Loss = 1.802888\n",
      "2024-11-28 15:16:56.215000: I runner.py:310] Step = 9000 ; steps/s = 1.64, tokens/s = 43696 (43696 target) ; Learning rate = 0.000796 ; Loss = 1.792070\n",
      "2024-11-28 15:17:56.714000: I runner.py:310] Step = 9100 ; steps/s = 1.65, tokens/s = 43249 (43249 target) ; Learning rate = 0.000804 ; Loss = 1.762877\n",
      "2024-11-28 15:18:57.741000: I runner.py:310] Step = 9200 ; steps/s = 1.64, tokens/s = 43703 (43703 target) ; Learning rate = 0.000813 ; Loss = 1.755849\n",
      "2024-11-28 15:19:58.808000: I runner.py:310] Step = 9300 ; steps/s = 1.64, tokens/s = 43667 (43667 target) ; Learning rate = 0.000822 ; Loss = 1.768993\n",
      "2024-11-28 15:20:59.383000: I runner.py:310] Step = 9400 ; steps/s = 1.65, tokens/s = 43199 (43199 target) ; Learning rate = 0.000831 ; Loss = 1.741849\n",
      "2024-11-28 15:22:00.418000: I runner.py:310] Step = 9500 ; steps/s = 1.64, tokens/s = 43703 (43703 target) ; Learning rate = 0.000840 ; Loss = 1.743573\n",
      "2024-11-28 15:23:01.464000: I runner.py:310] Step = 9600 ; steps/s = 1.64, tokens/s = 43666 (43666 target) ; Learning rate = 0.000849 ; Loss = 1.782779\n",
      "2024-11-28 15:24:02.078000: I runner.py:310] Step = 9700 ; steps/s = 1.65, tokens/s = 43178 (43178 target) ; Learning rate = 0.000857 ; Loss = 1.717973\n",
      "2024-11-28 15:25:03.196000: I runner.py:310] Step = 9800 ; steps/s = 1.64, tokens/s = 43627 (43627 target) ; Learning rate = 0.000866 ; Loss = 1.737174\n",
      "2024-11-28 15:26:03.765000: I runner.py:310] Step = 9900 ; steps/s = 1.65, tokens/s = 43208 (43208 target) ; Learning rate = 0.000875 ; Loss = 1.718158\n",
      "2024-11-28 15:27:04.843000: I runner.py:310] Step = 10000 ; steps/s = 1.64, tokens/s = 43650 (43650 target) ; Learning rate = 0.000884 ; Loss = 1.738257\n",
      "2024-11-28 15:27:06.567000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-10000\n",
      "2024-11-28 15:27:06.567000: I training.py:192] Running evaluation for step 10000\n",
      "2024-11-28 15:28:08.461000: I training.py:192] Evaluation result for step 10000: loss = 0.730037 ; perplexity = 2.075158\n",
      "2024-11-28 15:29:09.426000: I runner.py:310] Step = 10100 ; steps/s = 1.64, tokens/s = 43761 (43761 target) ; Learning rate = 0.000879 ; Loss = 1.786584\n",
      "2024-11-28 15:30:09.989000: I runner.py:310] Step = 10200 ; steps/s = 1.65, tokens/s = 43221 (43221 target) ; Learning rate = 0.000875 ; Loss = 1.724465\n",
      "2024-11-28 15:31:11.093000: I runner.py:310] Step = 10300 ; steps/s = 1.64, tokens/s = 43639 (43639 target) ; Learning rate = 0.000871 ; Loss = 1.729391\n",
      "2024-11-28 15:32:11.717000: I runner.py:310] Step = 10400 ; steps/s = 1.65, tokens/s = 43163 (43163 target) ; Learning rate = 0.000867 ; Loss = 1.732738\n",
      "2024-11-28 15:33:12.741000: I runner.py:310] Step = 10500 ; steps/s = 1.64, tokens/s = 43714 (43714 target) ; Learning rate = 0.000863 ; Loss = 1.728722\n",
      "2024-11-28 15:34:13.795000: I runner.py:310] Step = 10600 ; steps/s = 1.64, tokens/s = 43665 (43665 target) ; Learning rate = 0.000858 ; Loss = 1.696999\n",
      "2024-11-28 15:35:14.500000: I runner.py:310] Step = 10700 ; steps/s = 1.65, tokens/s = 43108 (43108 target) ; Learning rate = 0.000854 ; Loss = 1.708963\n",
      "2024-11-28 15:36:15.519000: I runner.py:310] Step = 10800 ; steps/s = 1.64, tokens/s = 43702 (43702 target) ; Learning rate = 0.000850 ; Loss = 1.690710\n",
      "2024-11-28 15:37:16.624000: I runner.py:310] Step = 10900 ; steps/s = 1.64, tokens/s = 43640 (43640 target) ; Learning rate = 0.000847 ; Loss = 1.692708\n",
      "2024-11-28 15:38:17.195000: I runner.py:310] Step = 11000 ; steps/s = 1.65, tokens/s = 43213 (43213 target) ; Learning rate = 0.000843 ; Loss = 1.708560\n",
      "2024-11-28 15:39:18.281000: I runner.py:310] Step = 11100 ; steps/s = 1.64, tokens/s = 43652 (43652 target) ; Learning rate = 0.000839 ; Loss = 1.691545\n",
      "2024-11-28 15:40:18.855000: I runner.py:310] Step = 11200 ; steps/s = 1.65, tokens/s = 43203 (43203 target) ; Learning rate = 0.000835 ; Loss = 1.678839\n",
      "2024-11-28 15:41:19.938000: I runner.py:310] Step = 11300 ; steps/s = 1.64, tokens/s = 43664 (43664 target) ; Learning rate = 0.000831 ; Loss = 1.702497\n",
      "2024-11-28 15:42:21.011000: I runner.py:310] Step = 11400 ; steps/s = 1.64, tokens/s = 43652 (43652 target) ; Learning rate = 0.000828 ; Loss = 1.672099\n",
      "2024-11-28 15:43:21.616000: I runner.py:310] Step = 11500 ; steps/s = 1.65, tokens/s = 43187 (43187 target) ; Learning rate = 0.000824 ; Loss = 1.665481\n",
      "2024-11-28 15:44:22.653000: I runner.py:310] Step = 11600 ; steps/s = 1.64, tokens/s = 43690 (43690 target) ; Learning rate = 0.000821 ; Loss = 1.675784\n",
      "2024-11-28 15:45:23.201000: I runner.py:310] Step = 11700 ; steps/s = 1.65, tokens/s = 43215 (43215 target) ; Learning rate = 0.000817 ; Loss = 1.663165\n",
      "2024-11-28 15:46:24.216000: I runner.py:310] Step = 11800 ; steps/s = 1.64, tokens/s = 43719 (43719 target) ; Learning rate = 0.000814 ; Loss = 1.655374\n",
      "2024-11-28 15:47:25.218000: I runner.py:310] Step = 11900 ; steps/s = 1.64, tokens/s = 43710 (43710 target) ; Learning rate = 0.000810 ; Loss = 1.660939\n",
      "2024-11-28 15:48:25.755000: I runner.py:310] Step = 12000 ; steps/s = 1.65, tokens/s = 43225 (43225 target) ; Learning rate = 0.000807 ; Loss = 1.672411\n",
      "2024-11-28 15:49:26.805000: I runner.py:310] Step = 12100 ; steps/s = 1.64, tokens/s = 43678 (43678 target) ; Learning rate = 0.000803 ; Loss = 1.655419\n",
      "2024-11-28 15:50:27.804000: I runner.py:310] Step = 12200 ; steps/s = 1.64, tokens/s = 43715 (43715 target) ; Learning rate = 0.000800 ; Loss = 1.659085\n",
      "2024-11-28 15:51:28.427000: I runner.py:310] Step = 12300 ; steps/s = 1.65, tokens/s = 43178 (43178 target) ; Learning rate = 0.000797 ; Loss = 1.637204\n",
      "2024-11-28 15:52:29.484000: I runner.py:310] Step = 12400 ; steps/s = 1.64, tokens/s = 43667 (43667 target) ; Learning rate = 0.000794 ; Loss = 1.660634\n",
      "2024-11-28 15:53:30.103000: I runner.py:310] Step = 12500 ; steps/s = 1.65, tokens/s = 43175 (43175 target) ; Learning rate = 0.000791 ; Loss = 1.628632\n",
      "2024-11-28 15:54:31.113000: I runner.py:310] Step = 12600 ; steps/s = 1.64, tokens/s = 43711 (43711 target) ; Learning rate = 0.000787 ; Loss = 1.661130\n",
      "2024-11-28 15:55:32.205000: I runner.py:310] Step = 12700 ; steps/s = 1.64, tokens/s = 43648 (43648 target) ; Learning rate = 0.000784 ; Loss = 1.651446\n",
      "2024-11-28 15:56:32.748000: I runner.py:310] Step = 12800 ; steps/s = 1.65, tokens/s = 43233 (43233 target) ; Learning rate = 0.000781 ; Loss = 1.643891\n",
      "2024-11-28 15:57:33.732000: I runner.py:310] Step = 12900 ; steps/s = 1.64, tokens/s = 43723 (43723 target) ; Learning rate = 0.000778 ; Loss = 1.654509\n",
      "2024-11-28 15:58:34.287000: I runner.py:310] Step = 13000 ; steps/s = 1.65, tokens/s = 43210 (43210 target) ; Learning rate = 0.000775 ; Loss = 1.626425\n",
      "2024-11-28 15:59:35.326000: I runner.py:310] Step = 13100 ; steps/s = 1.64, tokens/s = 43704 (43704 target) ; Learning rate = 0.000772 ; Loss = 1.633253\n",
      "2024-11-28 16:00:36.351000: I runner.py:310] Step = 13200 ; steps/s = 1.64, tokens/s = 43696 (43696 target) ; Learning rate = 0.000769 ; Loss = 1.637012\n",
      "2024-11-28 16:01:36.962000: I runner.py:310] Step = 13300 ; steps/s = 1.65, tokens/s = 43180 (43180 target) ; Learning rate = 0.000766 ; Loss = 1.630298\n",
      "2024-11-28 16:02:38.005000: I runner.py:310] Step = 13400 ; steps/s = 1.64, tokens/s = 43676 (43676 target) ; Learning rate = 0.000764 ; Loss = 1.635311\n",
      "2024-11-28 16:03:39.041000: I runner.py:310] Step = 13500 ; steps/s = 1.64, tokens/s = 43686 (43686 target) ; Learning rate = 0.000761 ; Loss = 1.629047\n",
      "2024-11-28 16:04:39.591000: I runner.py:310] Step = 13600 ; steps/s = 1.65, tokens/s = 43214 (43214 target) ; Learning rate = 0.000758 ; Loss = 1.617937\n",
      "2024-11-28 16:05:40.605000: I runner.py:310] Step = 13700 ; steps/s = 1.64, tokens/s = 43696 (43696 target) ; Learning rate = 0.000755 ; Loss = 1.628274\n",
      "2024-11-28 16:06:41.151000: I runner.py:310] Step = 13800 ; steps/s = 1.65, tokens/s = 43240 (43240 target) ; Learning rate = 0.000752 ; Loss = 1.611430\n",
      "2024-11-28 16:07:42.166000: I runner.py:310] Step = 13900 ; steps/s = 1.64, tokens/s = 43710 (43710 target) ; Learning rate = 0.000750 ; Loss = 1.621515\n",
      "2024-11-28 16:08:43.199000: I runner.py:310] Step = 14000 ; steps/s = 1.64, tokens/s = 43689 (43689 target) ; Learning rate = 0.000747 ; Loss = 1.623090\n",
      "2024-11-28 16:09:43.794000: I runner.py:310] Step = 14100 ; steps/s = 1.65, tokens/s = 43198 (43198 target) ; Learning rate = 0.000744 ; Loss = 1.614386\n",
      "2024-11-28 16:10:44.799000: I runner.py:310] Step = 14200 ; steps/s = 1.64, tokens/s = 43707 (43707 target) ; Learning rate = 0.000742 ; Loss = 1.615229\n",
      "2024-11-28 16:11:45.447000: I runner.py:310] Step = 14300 ; steps/s = 1.65, tokens/s = 43141 (43141 target) ; Learning rate = 0.000739 ; Loss = 1.600285\n",
      "2024-11-28 16:12:46.481000: I runner.py:310] Step = 14400 ; steps/s = 1.64, tokens/s = 43701 (43701 target) ; Learning rate = 0.000737 ; Loss = 1.603617\n",
      "2024-11-28 16:13:47.558000: I runner.py:310] Step = 14500 ; steps/s = 1.64, tokens/s = 43655 (43655 target) ; Learning rate = 0.000734 ; Loss = 1.617859\n",
      "2024-11-28 16:14:48.087000: I runner.py:310] Step = 14600 ; steps/s = 1.65, tokens/s = 43235 (43235 target) ; Learning rate = 0.000731 ; Loss = 1.616730\n",
      "2024-11-28 16:15:49.105000: I runner.py:310] Step = 14700 ; steps/s = 1.64, tokens/s = 43697 (43697 target) ; Learning rate = 0.000729 ; Loss = 1.615903\n",
      "2024-11-28 16:16:50.066000: I runner.py:310] Step = 14800 ; steps/s = 1.64, tokens/s = 43754 (43754 target) ; Learning rate = 0.000727 ; Loss = 1.624302\n",
      "2024-11-28 16:17:50.607000: I runner.py:310] Step = 14900 ; steps/s = 1.65, tokens/s = 43233 (43233 target) ; Learning rate = 0.000724 ; Loss = 1.600614\n",
      "2024-11-28 16:18:51.578000: I runner.py:310] Step = 15000 ; steps/s = 1.64, tokens/s = 43738 (43738 target) ; Learning rate = 0.000722 ; Loss = 1.603829\n",
      "2024-11-28 16:18:51.579000: I training.py:192] Running evaluation for step 15000\n",
      "2024-11-28 16:19:47.335000: I training.py:192] Evaluation result for step 15000: loss = 0.718322 ; perplexity = 2.050989\n",
      "2024-11-28 16:20:48.236000: I runner.py:310] Step = 15100 ; steps/s = 1.64, tokens/s = 42972 (42972 target) ; Learning rate = 0.000719 ; Loss = 1.597987\n",
      "2024-11-28 16:21:50.014000: I runner.py:310] Step = 15200 ; steps/s = 1.62, tokens/s = 43171 (43171 target) ; Learning rate = 0.000717 ; Loss = 1.591471\n",
      "2024-11-28 16:22:51.237000: I runner.py:310] Step = 15300 ; steps/s = 1.63, tokens/s = 43552 (43552 target) ; Learning rate = 0.000715 ; Loss = 1.612263\n",
      "2024-11-28 16:23:51.836000: I runner.py:310] Step = 15400 ; steps/s = 1.65, tokens/s = 43192 (43192 target) ; Learning rate = 0.000712 ; Loss = 1.585138\n",
      "2024-11-28 16:24:52.842000: I runner.py:310] Step = 15500 ; steps/s = 1.64, tokens/s = 43705 (43705 target) ; Learning rate = 0.000710 ; Loss = 1.594373\n",
      "2024-11-28 16:25:53.433000: I runner.py:310] Step = 15600 ; steps/s = 1.65, tokens/s = 43187 (43187 target) ; Learning rate = 0.000708 ; Loss = 1.578854\n",
      "2024-11-28 16:26:54.421000: I runner.py:310] Step = 15700 ; steps/s = 1.64, tokens/s = 43732 (43732 target) ; Learning rate = 0.000705 ; Loss = 1.595427\n",
      "2024-11-28 16:27:55.452000: I runner.py:310] Step = 15800 ; steps/s = 1.64, tokens/s = 43693 (43693 target) ; Learning rate = 0.000703 ; Loss = 1.596686\n",
      "2024-11-28 16:28:55.992000: I runner.py:310] Step = 15900 ; steps/s = 1.65, tokens/s = 43234 (43234 target) ; Learning rate = 0.000701 ; Loss = 1.592687\n",
      "2024-11-28 16:29:57.043000: I runner.py:310] Step = 16000 ; steps/s = 1.64, tokens/s = 43687 (43687 target) ; Learning rate = 0.000699 ; Loss = 1.589486\n",
      "2024-11-28 16:30:57.964000: I runner.py:310] Step = 16100 ; steps/s = 1.64, tokens/s = 43570 (43570 target) ; Learning rate = 0.000697 ; Loss = 1.630678\n",
      "2024-11-28 16:31:58.656000: I runner.py:310] Step = 16200 ; steps/s = 1.65, tokens/s = 43310 (43310 target) ; Learning rate = 0.000694 ; Loss = 1.579730\n",
      "2024-11-28 16:32:59.691000: I runner.py:310] Step = 16300 ; steps/s = 1.64, tokens/s = 43676 (43676 target) ; Learning rate = 0.000692 ; Loss = 1.586067\n",
      "2024-11-28 16:34:00.265000: I runner.py:310] Step = 16400 ; steps/s = 1.65, tokens/s = 43224 (43224 target) ; Learning rate = 0.000690 ; Loss = 1.587456\n",
      "2024-11-28 16:35:01.261000: I runner.py:310] Step = 16500 ; steps/s = 1.64, tokens/s = 43711 (43711 target) ; Learning rate = 0.000688 ; Loss = 1.585831\n",
      "2024-11-28 16:36:02.291000: I runner.py:310] Step = 16600 ; steps/s = 1.64, tokens/s = 43697 (43697 target) ; Learning rate = 0.000686 ; Loss = 1.584832\n",
      "2024-11-28 16:37:02.850000: I runner.py:310] Step = 16700 ; steps/s = 1.65, tokens/s = 43221 (43221 target) ; Learning rate = 0.000684 ; Loss = 1.567858\n",
      "2024-11-28 16:38:03.956000: I runner.py:310] Step = 16800 ; steps/s = 1.64, tokens/s = 43639 (43639 target) ; Learning rate = 0.000682 ; Loss = 1.586114\n",
      "2024-11-28 16:39:04.557000: I runner.py:310] Step = 16900 ; steps/s = 1.65, tokens/s = 43174 (43174 target) ; Learning rate = 0.000680 ; Loss = 1.560308\n",
      "2024-11-28 16:40:05.488000: I runner.py:310] Step = 17000 ; steps/s = 1.64, tokens/s = 43768 (43768 target) ; Learning rate = 0.000678 ; Loss = 1.569406\n",
      "2024-11-28 16:41:06.488000: I runner.py:310] Step = 17100 ; steps/s = 1.64, tokens/s = 43714 (43714 target) ; Learning rate = 0.000676 ; Loss = 1.587686\n",
      "2024-11-28 16:42:07.064000: I runner.py:310] Step = 17200 ; steps/s = 1.65, tokens/s = 43211 (43211 target) ; Learning rate = 0.000674 ; Loss = 1.582209\n",
      "2024-11-28 16:43:07.972000: I runner.py:310] Step = 17300 ; steps/s = 1.64, tokens/s = 43779 (43779 target) ; Learning rate = 0.000672 ; Loss = 1.594566\n",
      "2024-11-28 16:44:08.619000: I runner.py:310] Step = 17400 ; steps/s = 1.65, tokens/s = 43486 (43486 target) ; Learning rate = 0.000670 ; Loss = 1.728927\n",
      "2024-11-28 16:45:09.440000: I runner.py:310] Step = 17500 ; steps/s = 1.64, tokens/s = 43516 (43516 target) ; Learning rate = 0.000668 ; Loss = 1.562204\n",
      "2024-11-28 16:46:10.460000: I runner.py:310] Step = 17600 ; steps/s = 1.64, tokens/s = 43693 (43693 target) ; Learning rate = 0.000666 ; Loss = 1.583401\n",
      "2024-11-28 16:47:11.045000: I runner.py:310] Step = 17700 ; steps/s = 1.65, tokens/s = 43207 (43207 target) ; Learning rate = 0.000664 ; Loss = 1.561440\n",
      "2024-11-28 16:48:12.107000: I runner.py:310] Step = 17800 ; steps/s = 1.64, tokens/s = 43671 (43671 target) ; Learning rate = 0.000662 ; Loss = 1.579284\n",
      "2024-11-28 16:49:13.111000: I runner.py:310] Step = 17900 ; steps/s = 1.64, tokens/s = 43709 (43709 target) ; Learning rate = 0.000661 ; Loss = 1.583702\n",
      "2024-11-28 16:50:13.616000: I runner.py:310] Step = 18000 ; steps/s = 1.65, tokens/s = 43254 (43254 target) ; Learning rate = 0.000659 ; Loss = 1.560783\n",
      "2024-11-28 16:51:14.629000: I runner.py:310] Step = 18100 ; steps/s = 1.64, tokens/s = 43694 (43694 target) ; Learning rate = 0.000657 ; Loss = 1.561287\n",
      "2024-11-28 16:52:15.123000: I runner.py:310] Step = 18200 ; steps/s = 1.65, tokens/s = 43269 (43269 target) ; Learning rate = 0.000655 ; Loss = 1.556028\n",
      "2024-11-28 16:53:16.134000: I runner.py:310] Step = 18300 ; steps/s = 1.64, tokens/s = 43716 (43716 target) ; Learning rate = 0.000653 ; Loss = 1.558040\n",
      "2024-11-28 16:54:17.153000: I runner.py:310] Step = 18400 ; steps/s = 1.64, tokens/s = 43696 (43696 target) ; Learning rate = 0.000652 ; Loss = 1.567744\n",
      "2024-11-28 16:55:17.661000: I runner.py:310] Step = 18500 ; steps/s = 1.65, tokens/s = 43260 (43260 target) ; Learning rate = 0.000650 ; Loss = 1.559130\n",
      "2024-11-28 16:56:18.609000: I runner.py:310] Step = 18600 ; steps/s = 1.64, tokens/s = 43755 (43755 target) ; Learning rate = 0.000648 ; Loss = 1.561063\n",
      "2024-11-28 16:57:19.109000: I runner.py:310] Step = 18700 ; steps/s = 1.65, tokens/s = 43242 (43242 target) ; Learning rate = 0.000646 ; Loss = 1.553099\n",
      "2024-11-28 16:58:20.108000: I runner.py:310] Step = 18800 ; steps/s = 1.64, tokens/s = 43728 (43728 target) ; Learning rate = 0.000645 ; Loss = 1.550898\n",
      "2024-11-28 16:59:21.052000: I runner.py:310] Step = 18900 ; steps/s = 1.64, tokens/s = 43752 (43752 target) ; Learning rate = 0.000643 ; Loss = 1.562610\n",
      "2024-11-28 17:00:21.638000: I runner.py:310] Step = 19000 ; steps/s = 1.65, tokens/s = 43190 (43190 target) ; Learning rate = 0.000641 ; Loss = 1.549656\n",
      "2024-11-28 17:01:22.693000: I runner.py:310] Step = 19100 ; steps/s = 1.64, tokens/s = 43682 (43682 target) ; Learning rate = 0.000640 ; Loss = 1.559919\n",
      "2024-11-28 17:02:23.766000: I runner.py:310] Step = 19200 ; steps/s = 1.64, tokens/s = 43660 (43660 target) ; Learning rate = 0.000638 ; Loss = 1.569124\n",
      "2024-11-28 17:03:24.359000: I runner.py:310] Step = 19300 ; steps/s = 1.65, tokens/s = 43192 (43192 target) ; Learning rate = 0.000636 ; Loss = 1.552296\n",
      "2024-11-28 17:04:25.406000: I runner.py:310] Step = 19400 ; steps/s = 1.64, tokens/s = 43686 (43686 target) ; Learning rate = 0.000635 ; Loss = 1.557130\n",
      "2024-11-28 17:05:26.008000: I runner.py:310] Step = 19500 ; steps/s = 1.65, tokens/s = 43175 (43175 target) ; Learning rate = 0.000633 ; Loss = 1.539856\n",
      "2024-11-28 17:06:27.003000: I runner.py:310] Step = 19600 ; steps/s = 1.64, tokens/s = 43720 (43720 target) ; Learning rate = 0.000631 ; Loss = 1.550818\n",
      "2024-11-28 17:07:28.094000: I runner.py:310] Step = 19700 ; steps/s = 1.64, tokens/s = 43657 (43657 target) ; Learning rate = 0.000630 ; Loss = 1.559089\n",
      "2024-11-28 17:08:28.640000: I runner.py:310] Step = 19800 ; steps/s = 1.65, tokens/s = 43214 (43214 target) ; Learning rate = 0.000628 ; Loss = 1.550395\n",
      "2024-11-28 17:09:29.713000: I runner.py:310] Step = 19900 ; steps/s = 1.64, tokens/s = 43671 (43671 target) ; Learning rate = 0.000627 ; Loss = 1.555599\n",
      "2024-11-28 17:10:30.255000: I runner.py:310] Step = 20000 ; steps/s = 1.65, tokens/s = 43221 (43221 target) ; Learning rate = 0.000625 ; Loss = 1.538273\n",
      "2024-11-28 17:10:31.906000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-20000\n",
      "2024-11-28 17:10:31.907000: I training.py:192] Running evaluation for step 20000\n",
      "2024-11-28 17:11:44.556000: I training.py:192] Evaluation result for step 20000: loss = 0.737920 ; perplexity = 2.091579\n",
      "2024-11-28 17:12:45.459000: I runner.py:310] Step = 20100 ; steps/s = 1.64, tokens/s = 43804 (43804 target) ; Learning rate = 0.000623 ; Loss = 1.545747\n",
      "2024-11-28 17:13:46.553000: I runner.py:310] Step = 20200 ; steps/s = 1.64, tokens/s = 43637 (43637 target) ; Learning rate = 0.000622 ; Loss = 1.549841\n",
      "2024-11-28 17:14:47.188000: I runner.py:310] Step = 20300 ; steps/s = 1.65, tokens/s = 43163 (43163 target) ; Learning rate = 0.000620 ; Loss = 1.536069\n",
      "2024-11-28 17:15:48.304000: I runner.py:310] Step = 20400 ; steps/s = 1.64, tokens/s = 43643 (43643 target) ; Learning rate = 0.000619 ; Loss = 1.553968\n",
      "2024-11-28 17:16:49.383000: I runner.py:310] Step = 20500 ; steps/s = 1.64, tokens/s = 43659 (43659 target) ; Learning rate = 0.000617 ; Loss = 1.555694\n",
      "2024-11-28 17:17:49.952000: I runner.py:310] Step = 20600 ; steps/s = 1.65, tokens/s = 43202 (43202 target) ; Learning rate = 0.000616 ; Loss = 1.536035\n",
      "2024-11-28 17:18:50.972000: I runner.py:310] Step = 20700 ; steps/s = 1.64, tokens/s = 43693 (43693 target) ; Learning rate = 0.000614 ; Loss = 1.544411\n",
      "2024-11-28 17:19:51.510000: I runner.py:310] Step = 20800 ; steps/s = 1.65, tokens/s = 43244 (43244 target) ; Learning rate = 0.000613 ; Loss = 1.528902\n",
      "2024-11-28 17:20:52.590000: I runner.py:310] Step = 20900 ; steps/s = 1.64, tokens/s = 43658 (43658 target) ; Learning rate = 0.000611 ; Loss = 1.536842\n",
      "2024-11-28 17:21:53.683000: I runner.py:310] Step = 21000 ; steps/s = 1.64, tokens/s = 43652 (43652 target) ; Learning rate = 0.000610 ; Loss = 1.541549\n",
      "2024-11-28 17:22:54.262000: I runner.py:310] Step = 21100 ; steps/s = 1.65, tokens/s = 43191 (43191 target) ; Learning rate = 0.000608 ; Loss = 1.540744\n",
      "2024-11-28 17:23:55.286000: I runner.py:310] Step = 21200 ; steps/s = 1.64, tokens/s = 43705 (43705 target) ; Learning rate = 0.000607 ; Loss = 1.543420\n",
      "2024-11-28 17:24:55.877000: I runner.py:310] Step = 21300 ; steps/s = 1.65, tokens/s = 43195 (43195 target) ; Learning rate = 0.000606 ; Loss = 1.531611\n",
      "2024-11-28 17:25:56.968000: I runner.py:310] Step = 21400 ; steps/s = 1.64, tokens/s = 43654 (43654 target) ; Learning rate = 0.000604 ; Loss = 1.529229\n",
      "2024-11-28 17:26:58.027000: I runner.py:310] Step = 21500 ; steps/s = 1.64, tokens/s = 43665 (43665 target) ; Learning rate = 0.000603 ; Loss = 1.536564\n",
      "2024-11-28 17:27:58.641000: I runner.py:310] Step = 21600 ; steps/s = 1.65, tokens/s = 43179 (43179 target) ; Learning rate = 0.000601 ; Loss = 1.531190\n",
      "2024-11-28 17:28:59.690000: I runner.py:310] Step = 21700 ; steps/s = 1.64, tokens/s = 43681 (43681 target) ; Learning rate = 0.000600 ; Loss = 1.539964\n",
      "2024-11-28 17:30:00.732000: I runner.py:310] Step = 21800 ; steps/s = 1.64, tokens/s = 43692 (43692 target) ; Learning rate = 0.000599 ; Loss = 1.540333\n",
      "2024-11-28 17:31:01.256000: I runner.py:310] Step = 21900 ; steps/s = 1.65, tokens/s = 43241 (43241 target) ; Learning rate = 0.000597 ; Loss = 1.534449\n",
      "2024-11-28 17:32:02.319000: I runner.py:310] Step = 22000 ; steps/s = 1.64, tokens/s = 43655 (43655 target) ; Learning rate = 0.000596 ; Loss = 1.540633\n",
      "2024-11-28 17:33:03.003000: I runner.py:310] Step = 22100 ; steps/s = 1.65, tokens/s = 43143 (43143 target) ; Learning rate = 0.000595 ; Loss = 1.533357\n",
      "2024-11-28 17:34:04.040000: I runner.py:310] Step = 22200 ; steps/s = 1.64, tokens/s = 43684 (43684 target) ; Learning rate = 0.000593 ; Loss = 1.540256\n",
      "2024-11-28 17:35:05.117000: I runner.py:310] Step = 22300 ; steps/s = 1.64, tokens/s = 43667 (43667 target) ; Learning rate = 0.000592 ; Loss = 1.536394\n",
      "2024-11-28 17:36:05.737000: I runner.py:310] Step = 22400 ; steps/s = 1.65, tokens/s = 43162 (43162 target) ; Learning rate = 0.000591 ; Loss = 1.530123\n",
      "2024-11-28 17:37:06.819000: I runner.py:310] Step = 22500 ; steps/s = 1.64, tokens/s = 43673 (43673 target) ; Learning rate = 0.000589 ; Loss = 1.536468\n",
      "2024-11-28 17:38:07.433000: I runner.py:310] Step = 22600 ; steps/s = 1.65, tokens/s = 43166 (43166 target) ; Learning rate = 0.000588 ; Loss = 1.527610\n",
      "2024-11-28 17:39:08.520000: I runner.py:310] Step = 22700 ; steps/s = 1.64, tokens/s = 43648 (43648 target) ; Learning rate = 0.000587 ; Loss = 1.518202\n",
      "2024-11-28 17:40:09.578000: I runner.py:310] Step = 22800 ; steps/s = 1.64, tokens/s = 43678 (43678 target) ; Learning rate = 0.000585 ; Loss = 1.538014\n",
      "2024-11-28 17:41:10.108000: I runner.py:310] Step = 22900 ; steps/s = 1.65, tokens/s = 43249 (43249 target) ; Learning rate = 0.000584 ; Loss = 1.520343\n",
      "2024-11-28 17:42:11.188000: I runner.py:310] Step = 23000 ; steps/s = 1.64, tokens/s = 43648 (43648 target) ; Learning rate = 0.000583 ; Loss = 1.546258\n",
      "2024-11-28 17:43:12.197000: I runner.py:310] Step = 23100 ; steps/s = 1.64, tokens/s = 43710 (43710 target) ; Learning rate = 0.000582 ; Loss = 1.530607\n",
      "2024-11-28 17:44:12.797000: I runner.py:310] Step = 23200 ; steps/s = 1.65, tokens/s = 43190 (43190 target) ; Learning rate = 0.000580 ; Loss = 1.521610\n",
      "2024-11-28 17:45:13.811000: I runner.py:310] Step = 23300 ; steps/s = 1.64, tokens/s = 43703 (43703 target) ; Learning rate = 0.000579 ; Loss = 1.535068\n",
      "2024-11-28 17:46:14.452000: I runner.py:310] Step = 23400 ; steps/s = 1.65, tokens/s = 43155 (43155 target) ; Learning rate = 0.000578 ; Loss = 1.524145\n",
      "2024-11-28 17:47:15.530000: I runner.py:310] Step = 23500 ; steps/s = 1.64, tokens/s = 43654 (43654 target) ; Learning rate = 0.000577 ; Loss = 1.520224\n",
      "2024-11-28 17:48:16.564000: I runner.py:310] Step = 23600 ; steps/s = 1.64, tokens/s = 43693 (43693 target) ; Learning rate = 0.000575 ; Loss = 1.528510\n",
      "2024-11-28 17:49:17.119000: I runner.py:310] Step = 23700 ; steps/s = 1.65, tokens/s = 43221 (43221 target) ; Learning rate = 0.000574 ; Loss = 1.521204\n",
      "2024-11-28 17:50:18.127000: I runner.py:310] Step = 23800 ; steps/s = 1.64, tokens/s = 43715 (43715 target) ; Learning rate = 0.000573 ; Loss = 1.528105\n",
      "2024-11-28 17:51:18.743000: I runner.py:310] Step = 23900 ; steps/s = 1.65, tokens/s = 43168 (43168 target) ; Learning rate = 0.000572 ; Loss = 1.514946\n",
      "2024-11-28 17:52:19.806000: I runner.py:310] Step = 24000 ; steps/s = 1.64, tokens/s = 43669 (43669 target) ; Learning rate = 0.000571 ; Loss = 1.534198\n",
      "2024-11-28 17:53:20.870000: I runner.py:310] Step = 24100 ; steps/s = 1.64, tokens/s = 43667 (43667 target) ; Learning rate = 0.000569 ; Loss = 1.526883\n",
      "2024-11-28 17:54:21.477000: I runner.py:310] Step = 24200 ; steps/s = 1.65, tokens/s = 43192 (43192 target) ; Learning rate = 0.000568 ; Loss = 1.527817\n",
      "2024-11-28 17:55:22.570000: I runner.py:310] Step = 24300 ; steps/s = 1.64, tokens/s = 43646 (43646 target) ; Learning rate = 0.000567 ; Loss = 1.528432\n",
      "2024-11-28 17:56:23.587000: I runner.py:310] Step = 24400 ; steps/s = 1.64, tokens/s = 43701 (43701 target) ; Learning rate = 0.000566 ; Loss = 1.521734\n",
      "2024-11-28 17:57:24.160000: I runner.py:310] Step = 24500 ; steps/s = 1.65, tokens/s = 43214 (43214 target) ; Learning rate = 0.000565 ; Loss = 1.516161\n",
      "2024-11-28 17:58:25.229000: I runner.py:310] Step = 24600 ; steps/s = 1.64, tokens/s = 43660 (43660 target) ; Learning rate = 0.000564 ; Loss = 1.524237\n",
      "2024-11-28 17:59:25.842000: I runner.py:310] Step = 24700 ; steps/s = 1.65, tokens/s = 43185 (43185 target) ; Learning rate = 0.000562 ; Loss = 1.519251\n",
      "2024-11-28 18:00:26.881000: I runner.py:310] Step = 24800 ; steps/s = 1.64, tokens/s = 43689 (43689 target) ; Learning rate = 0.000561 ; Loss = 1.528400\n",
      "2024-11-28 18:01:27.922000: I runner.py:310] Step = 24900 ; steps/s = 1.64, tokens/s = 43674 (43674 target) ; Learning rate = 0.000560 ; Loss = 1.531650\n",
      "2024-11-28 18:02:28.545000: I runner.py:310] Step = 25000 ; steps/s = 1.65, tokens/s = 43177 (43177 target) ; Learning rate = 0.000559 ; Loss = 1.520275\n",
      "2024-11-28 18:02:28.547000: I training.py:192] Running evaluation for step 25000\n",
      "2024-11-28 18:03:21.236000: I training.py:192] Evaluation result for step 25000: loss = 0.751825 ; perplexity = 2.120866\n",
      "2024-11-28 18:04:22.212000: I runner.py:310] Step = 25100 ; steps/s = 1.64, tokens/s = 43733 (43733 target) ; Learning rate = 0.000558 ; Loss = 1.524366\n",
      "2024-11-28 18:05:22.886000: I runner.py:310] Step = 25200 ; steps/s = 1.65, tokens/s = 43136 (43136 target) ; Learning rate = 0.000557 ; Loss = 1.515503\n",
      "2024-11-28 18:06:23.943000: I runner.py:310] Step = 25300 ; steps/s = 1.64, tokens/s = 43675 (43675 target) ; Learning rate = 0.000556 ; Loss = 1.518099\n",
      "2024-11-28 18:07:25.038000: I runner.py:310] Step = 25400 ; steps/s = 1.64, tokens/s = 43636 (43636 target) ; Learning rate = 0.000555 ; Loss = 1.523100\n",
      "2024-11-28 18:08:25.570000: I runner.py:310] Step = 25500 ; steps/s = 1.65, tokens/s = 43243 (43243 target) ; Learning rate = 0.000553 ; Loss = 1.514997\n",
      "2024-11-28 18:09:26.657000: I runner.py:310] Step = 25600 ; steps/s = 1.64, tokens/s = 43649 (43649 target) ; Learning rate = 0.000552 ; Loss = 1.526559\n",
      "2024-11-28 18:10:27.733000: I runner.py:310] Step = 25700 ; steps/s = 1.64, tokens/s = 43664 (43664 target) ; Learning rate = 0.000551 ; Loss = 1.519117\n",
      "2024-11-28 18:11:28.557000: I runner.py:310] Step = 25800 ; steps/s = 1.64, tokens/s = 43026 (43026 target) ; Learning rate = 0.000550 ; Loss = 1.512873\n",
      "2024-11-28 18:12:29.618000: I runner.py:310] Step = 25900 ; steps/s = 1.64, tokens/s = 43665 (43665 target) ; Learning rate = 0.000549 ; Loss = 1.517689\n",
      "2024-11-28 18:13:30.223000: I runner.py:310] Step = 26000 ; steps/s = 1.65, tokens/s = 43189 (43189 target) ; Learning rate = 0.000548 ; Loss = 1.509336\n",
      "2024-11-28 18:14:31.251000: I runner.py:310] Step = 26100 ; steps/s = 1.64, tokens/s = 43695 (43695 target) ; Learning rate = 0.000547 ; Loss = 1.513102\n",
      "2024-11-28 18:15:32.290000: I runner.py:310] Step = 26200 ; steps/s = 1.64, tokens/s = 43690 (43690 target) ; Learning rate = 0.000546 ; Loss = 1.523726\n",
      "2024-11-28 18:16:32.906000: I runner.py:310] Step = 26300 ; steps/s = 1.65, tokens/s = 43180 (43180 target) ; Learning rate = 0.000545 ; Loss = 1.512260\n",
      "2024-11-28 18:17:33.982000: I runner.py:310] Step = 26400 ; steps/s = 1.64, tokens/s = 43659 (43659 target) ; Learning rate = 0.000544 ; Loss = 1.520133\n",
      "2024-11-28 18:18:34.665000: I runner.py:310] Step = 26500 ; steps/s = 1.65, tokens/s = 43121 (43121 target) ; Learning rate = 0.000543 ; Loss = 1.500933\n",
      "2024-11-28 18:19:35.692000: I runner.py:310] Step = 26600 ; steps/s = 1.64, tokens/s = 43694 (43694 target) ; Learning rate = 0.000542 ; Loss = 1.503013\n",
      "2024-11-28 18:20:36.768000: I runner.py:310] Step = 26700 ; steps/s = 1.64, tokens/s = 43658 (43658 target) ; Learning rate = 0.000541 ; Loss = 1.518752\n",
      "2024-11-28 18:21:37.385000: I runner.py:310] Step = 26800 ; steps/s = 1.65, tokens/s = 43177 (43177 target) ; Learning rate = 0.000540 ; Loss = 1.508981\n",
      "2024-11-28 18:22:38.483000: I runner.py:310] Step = 26900 ; steps/s = 1.64, tokens/s = 43643 (43643 target) ; Learning rate = 0.000539 ; Loss = 1.522884\n",
      "2024-11-28 18:23:39.534000: I runner.py:310] Step = 27000 ; steps/s = 1.64, tokens/s = 43683 (43683 target) ; Learning rate = 0.000538 ; Loss = 1.514786\n",
      "2024-11-28 18:24:40.102000: I runner.py:310] Step = 27100 ; steps/s = 1.65, tokens/s = 43216 (43216 target) ; Learning rate = 0.000537 ; Loss = 1.502499\n",
      "2024-11-28 18:25:41.113000: I runner.py:310] Step = 27200 ; steps/s = 1.64, tokens/s = 43699 (43699 target) ; Learning rate = 0.000536 ; Loss = 1.512193\n",
      "2024-11-28 18:26:41.722000: I runner.py:310] Step = 27300 ; steps/s = 1.65, tokens/s = 43182 (43182 target) ; Learning rate = 0.000535 ; Loss = 1.502781\n",
      "2024-11-28 18:27:42.808000: I runner.py:310] Step = 27400 ; steps/s = 1.64, tokens/s = 43650 (43650 target) ; Learning rate = 0.000534 ; Loss = 1.511276\n",
      "2024-11-28 18:28:43.892000: I runner.py:310] Step = 27500 ; steps/s = 1.64, tokens/s = 43655 (43655 target) ; Learning rate = 0.000533 ; Loss = 1.513219\n",
      "2024-11-28 18:29:44.526000: I runner.py:310] Step = 27600 ; steps/s = 1.65, tokens/s = 43171 (43171 target) ; Learning rate = 0.000532 ; Loss = 1.506436\n",
      "2024-11-28 18:30:45.563000: I runner.py:310] Step = 27700 ; steps/s = 1.64, tokens/s = 43694 (43694 target) ; Learning rate = 0.000531 ; Loss = 1.513531\n",
      "2024-11-28 18:31:46.264000: I runner.py:310] Step = 27800 ; steps/s = 1.65, tokens/s = 43102 (43102 target) ; Learning rate = 0.000530 ; Loss = 1.502909\n",
      "2024-11-28 18:32:47.277000: I runner.py:310] Step = 27900 ; steps/s = 1.64, tokens/s = 43709 (43709 target) ; Learning rate = 0.000529 ; Loss = 1.502374\n",
      "2024-11-28 18:33:48.325000: I runner.py:310] Step = 28000 ; steps/s = 1.64, tokens/s = 43675 (43675 target) ; Learning rate = 0.000528 ; Loss = 1.511876\n",
      "2024-11-28 18:34:48.931000: I runner.py:310] Step = 28100 ; steps/s = 1.65, tokens/s = 43183 (43183 target) ; Learning rate = 0.000527 ; Loss = 1.505286\n",
      "2024-11-28 18:35:49.964000: I runner.py:310] Step = 28200 ; steps/s = 1.64, tokens/s = 43699 (43699 target) ; Learning rate = 0.000526 ; Loss = 1.506929\n",
      "2024-11-28 18:36:51.042000: I runner.py:310] Step = 28300 ; steps/s = 1.64, tokens/s = 43655 (43655 target) ; Learning rate = 0.000525 ; Loss = 1.517366\n",
      "2024-11-28 18:37:51.628000: I runner.py:310] Step = 28400 ; steps/s = 1.65, tokens/s = 43190 (43190 target) ; Learning rate = 0.000524 ; Loss = 1.504832\n",
      "2024-11-28 18:38:52.711000: I runner.py:310] Step = 28500 ; steps/s = 1.64, tokens/s = 43665 (43665 target) ; Learning rate = 0.000524 ; Loss = 1.509176\n",
      "2024-11-28 18:39:53.353000: I runner.py:310] Step = 28600 ; steps/s = 1.65, tokens/s = 43159 (43159 target) ; Learning rate = 0.000523 ; Loss = 1.499657\n",
      "2024-11-28 18:40:54.376000: I runner.py:310] Step = 28700 ; steps/s = 1.64, tokens/s = 43705 (43705 target) ; Learning rate = 0.000522 ; Loss = 1.507011\n",
      "2024-11-28 18:41:55.457000: I runner.py:310] Step = 28800 ; steps/s = 1.64, tokens/s = 43647 (43647 target) ; Learning rate = 0.000521 ; Loss = 1.509485\n",
      "2024-11-28 18:42:56.047000: I runner.py:310] Step = 28900 ; steps/s = 1.65, tokens/s = 43204 (43204 target) ; Learning rate = 0.000520 ; Loss = 1.501077\n",
      "2024-11-28 18:43:57.189000: I runner.py:310] Step = 29000 ; steps/s = 1.64, tokens/s = 43603 (43603 target) ; Learning rate = 0.000519 ; Loss = 1.507143\n",
      "2024-11-28 18:44:57.820000: I runner.py:310] Step = 29100 ; steps/s = 1.65, tokens/s = 43167 (43167 target) ; Learning rate = 0.000518 ; Loss = 1.495071\n",
      "2024-11-28 18:45:58.850000: I runner.py:310] Step = 29200 ; steps/s = 1.64, tokens/s = 43692 (43692 target) ; Learning rate = 0.000517 ; Loss = 1.497866\n",
      "2024-11-28 18:46:59.904000: I runner.py:310] Step = 29300 ; steps/s = 1.64, tokens/s = 43676 (43676 target) ; Learning rate = 0.000516 ; Loss = 1.515917\n",
      "2024-11-28 18:48:00.513000: I runner.py:310] Step = 29400 ; steps/s = 1.65, tokens/s = 43194 (43194 target) ; Learning rate = 0.000515 ; Loss = 1.504159\n",
      "2024-11-28 18:49:01.577000: I runner.py:310] Step = 29500 ; steps/s = 1.64, tokens/s = 43664 (43664 target) ; Learning rate = 0.000515 ; Loss = 1.501381\n",
      "2024-11-28 18:50:02.570000: I runner.py:310] Step = 29600 ; steps/s = 1.64, tokens/s = 43717 (43717 target) ; Learning rate = 0.000514 ; Loss = 1.504836\n",
      "2024-11-28 18:51:03.215000: I runner.py:310] Step = 29700 ; steps/s = 1.65, tokens/s = 43155 (43155 target) ; Learning rate = 0.000513 ; Loss = 1.496883\n",
      "2024-11-28 18:52:04.298000: I runner.py:310] Step = 29800 ; steps/s = 1.64, tokens/s = 43651 (43651 target) ; Learning rate = 0.000512 ; Loss = 1.508711\n",
      "2024-11-28 18:53:04.913000: I runner.py:310] Step = 29900 ; steps/s = 1.65, tokens/s = 43183 (43183 target) ; Learning rate = 0.000511 ; Loss = 1.498303\n",
      "2024-11-28 18:54:05.993000: I runner.py:310] Step = 30000 ; steps/s = 1.64, tokens/s = 43657 (43657 target) ; Learning rate = 0.000510 ; Loss = 1.498403\n",
      "2024-11-28 18:54:07.726000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-30000\n",
      "2024-11-28 18:54:07.726000: I training.py:192] Running evaluation for step 30000\n",
      "2024-11-28 18:54:57.106000: I training.py:192] Evaluation result for step 30000: loss = 0.764479 ; perplexity = 2.147875\n",
      "2024-11-28 18:55:58.104000: I runner.py:310] Step = 30100 ; steps/s = 1.64, tokens/s = 43720 (43720 target) ; Learning rate = 0.000509 ; Loss = 1.507679\n",
      "2024-11-28 18:56:58.637000: I runner.py:310] Step = 30200 ; steps/s = 1.65, tokens/s = 43239 (43239 target) ; Learning rate = 0.000509 ; Loss = 1.495719\n",
      "2024-11-28 18:57:59.683000: I runner.py:310] Step = 30300 ; steps/s = 1.64, tokens/s = 43686 (43686 target) ; Learning rate = 0.000508 ; Loss = 1.497595\n",
      "2024-11-28 18:59:00.355000: I runner.py:310] Step = 30400 ; steps/s = 1.65, tokens/s = 43134 (43134 target) ; Learning rate = 0.000507 ; Loss = 1.494352\n",
      "2024-11-28 19:00:01.466000: I runner.py:310] Step = 30500 ; steps/s = 1.64, tokens/s = 43637 (43637 target) ; Learning rate = 0.000506 ; Loss = 1.502924\n",
      "2024-11-28 19:01:02.590000: I runner.py:310] Step = 30600 ; steps/s = 1.64, tokens/s = 43623 (43623 target) ; Learning rate = 0.000505 ; Loss = 1.498073\n",
      "2024-11-28 19:02:03.291000: I runner.py:310] Step = 30700 ; steps/s = 1.65, tokens/s = 43116 (43116 target) ; Learning rate = 0.000504 ; Loss = 1.495144\n",
      "2024-11-28 19:03:04.402000: I runner.py:310] Step = 30800 ; steps/s = 1.64, tokens/s = 43632 (43632 target) ; Learning rate = 0.000504 ; Loss = 1.498882\n",
      "2024-11-28 19:04:05.571000: I runner.py:310] Step = 30900 ; steps/s = 1.64, tokens/s = 43602 (43602 target) ; Learning rate = 0.000503 ; Loss = 1.501011\n",
      "2024-11-28 19:05:06.110000: I runner.py:310] Step = 31000 ; steps/s = 1.65, tokens/s = 43232 (43232 target) ; Learning rate = 0.000502 ; Loss = 1.493332\n",
      "2024-11-28 19:06:07.207000: I runner.py:310] Step = 31100 ; steps/s = 1.64, tokens/s = 43639 (43639 target) ; Learning rate = 0.000501 ; Loss = 1.496790\n",
      "2024-11-28 19:07:07.780000: I runner.py:310] Step = 31200 ; steps/s = 1.65, tokens/s = 43204 (43204 target) ; Learning rate = 0.000500 ; Loss = 1.490577\n",
      "2024-11-28 19:08:08.856000: I runner.py:310] Step = 31300 ; steps/s = 1.64, tokens/s = 43652 (43652 target) ; Learning rate = 0.000500 ; Loss = 1.490530\n",
      "2024-11-28 19:09:09.997000: I runner.py:310] Step = 31400 ; steps/s = 1.64, tokens/s = 43626 (43626 target) ; Learning rate = 0.000499 ; Loss = 1.504130\n",
      "2024-11-28 19:10:10.633000: I runner.py:310] Step = 31500 ; steps/s = 1.65, tokens/s = 43162 (43162 target) ; Learning rate = 0.000498 ; Loss = 1.498607\n",
      "2024-11-28 19:11:11.695000: I runner.py:310] Step = 31600 ; steps/s = 1.64, tokens/s = 43662 (43662 target) ; Learning rate = 0.000497 ; Loss = 1.495787\n",
      "2024-11-28 19:12:12.282000: I runner.py:310] Step = 31700 ; steps/s = 1.65, tokens/s = 43197 (43197 target) ; Learning rate = 0.000496 ; Loss = 1.481091\n",
      "2024-11-28 19:13:13.343000: I runner.py:310] Step = 31800 ; steps/s = 1.64, tokens/s = 43677 (43677 target) ; Learning rate = 0.000496 ; Loss = 1.505450\n",
      "2024-11-28 19:14:14.400000: I runner.py:310] Step = 31900 ; steps/s = 1.64, tokens/s = 43679 (43679 target) ; Learning rate = 0.000495 ; Loss = 1.500002\n",
      "2024-11-28 19:15:15.067000: I runner.py:310] Step = 32000 ; steps/s = 1.65, tokens/s = 43128 (43128 target) ; Learning rate = 0.000494 ; Loss = 1.494819\n",
      "2024-11-28 19:16:16.132000: I runner.py:310] Step = 32100 ; steps/s = 1.64, tokens/s = 43672 (43672 target) ; Learning rate = 0.000493 ; Loss = 1.497665\n",
      "2024-11-28 19:17:17.196000: I runner.py:310] Step = 32200 ; steps/s = 1.64, tokens/s = 43669 (43669 target) ; Learning rate = 0.000493 ; Loss = 1.503851\n",
      "2024-11-28 19:18:17.834000: I runner.py:310] Step = 32300 ; steps/s = 1.65, tokens/s = 43162 (43162 target) ; Learning rate = 0.000492 ; Loss = 1.490781\n",
      "2024-11-28 19:19:18.911000: I runner.py:310] Step = 32400 ; steps/s = 1.64, tokens/s = 43654 (43654 target) ; Learning rate = 0.000491 ; Loss = 1.494615\n",
      "2024-11-28 19:20:19.483000: I runner.py:310] Step = 32500 ; steps/s = 1.65, tokens/s = 43205 (43205 target) ; Learning rate = 0.000490 ; Loss = 1.486808\n",
      "2024-11-28 19:21:20.540000: I runner.py:310] Step = 32600 ; steps/s = 1.64, tokens/s = 43680 (43680 target) ; Learning rate = 0.000490 ; Loss = 1.499454\n",
      "2024-11-28 19:22:21.652000: I runner.py:310] Step = 32700 ; steps/s = 1.64, tokens/s = 43643 (43643 target) ; Learning rate = 0.000489 ; Loss = 1.499853\n",
      "2024-11-28 19:23:22.299000: I runner.py:310] Step = 32800 ; steps/s = 1.65, tokens/s = 43139 (43139 target) ; Learning rate = 0.000488 ; Loss = 1.501820\n",
      "2024-11-28 19:24:23.355000: I runner.py:310] Step = 32900 ; steps/s = 1.64, tokens/s = 43675 (43675 target) ; Learning rate = 0.000487 ; Loss = 1.496609\n",
      "2024-11-28 19:25:23.937000: I runner.py:310] Step = 33000 ; steps/s = 1.65, tokens/s = 43200 (43200 target) ; Learning rate = 0.000487 ; Loss = 1.485819\n",
      "2024-11-28 19:26:25.053000: I runner.py:310] Step = 33100 ; steps/s = 1.64, tokens/s = 43647 (43647 target) ; Learning rate = 0.000486 ; Loss = 1.494819\n",
      "2024-11-28 19:27:26.149000: I runner.py:310] Step = 33200 ; steps/s = 1.64, tokens/s = 43646 (43646 target) ; Learning rate = 0.000485 ; Loss = 1.491472\n",
      "2024-11-28 19:28:26.758000: I runner.py:310] Step = 33300 ; steps/s = 1.65, tokens/s = 43172 (43172 target) ; Learning rate = 0.000484 ; Loss = 1.489902\n",
      "2024-11-28 19:29:27.824000: I runner.py:310] Step = 33400 ; steps/s = 1.64, tokens/s = 43672 (43672 target) ; Learning rate = 0.000484 ; Loss = 1.496055\n",
      "2024-11-28 19:30:28.886000: I runner.py:310] Step = 33500 ; steps/s = 1.64, tokens/s = 43625 (43625 target) ; Learning rate = 0.000483 ; Loss = 1.500780\n",
      "2024-11-28 19:31:29.539000: I runner.py:310] Step = 33600 ; steps/s = 1.65, tokens/s = 43199 (43199 target) ; Learning rate = 0.000482 ; Loss = 1.487398\n",
      "2024-11-28 19:32:30.598000: I runner.py:310] Step = 33700 ; steps/s = 1.64, tokens/s = 43667 (43667 target) ; Learning rate = 0.000481 ; Loss = 1.483331\n",
      "2024-11-28 19:33:31.207000: I runner.py:310] Step = 33800 ; steps/s = 1.65, tokens/s = 43180 (43180 target) ; Learning rate = 0.000481 ; Loss = 1.490160\n",
      "2024-11-28 19:34:32.320000: I runner.py:310] Step = 33900 ; steps/s = 1.64, tokens/s = 43640 (43640 target) ; Learning rate = 0.000480 ; Loss = 1.497638\n",
      "2024-11-28 19:35:33.417000: I runner.py:310] Step = 34000 ; steps/s = 1.64, tokens/s = 43644 (43644 target) ; Learning rate = 0.000479 ; Loss = 1.491949\n",
      "2024-11-28 19:36:34.032000: I runner.py:310] Step = 34100 ; steps/s = 1.65, tokens/s = 43166 (43166 target) ; Learning rate = 0.000479 ; Loss = 1.491619\n",
      "2024-11-28 19:37:35.114000: I runner.py:310] Step = 34200 ; steps/s = 1.64, tokens/s = 43657 (43657 target) ; Learning rate = 0.000478 ; Loss = 1.491976\n",
      "2024-11-28 19:38:35.740000: I runner.py:310] Step = 34300 ; steps/s = 1.65, tokens/s = 43175 (43175 target) ; Learning rate = 0.000477 ; Loss = 1.486272\n",
      "2024-11-28 19:39:36.840000: I runner.py:310] Step = 34400 ; steps/s = 1.64, tokens/s = 43647 (43647 target) ; Learning rate = 0.000477 ; Loss = 1.488835\n",
      "2024-11-28 19:40:37.976000: I runner.py:310] Step = 34500 ; steps/s = 1.64, tokens/s = 43616 (43616 target) ; Learning rate = 0.000476 ; Loss = 1.492010\n",
      "2024-11-28 19:41:38.508000: I runner.py:310] Step = 34600 ; steps/s = 1.65, tokens/s = 43228 (43228 target) ; Learning rate = 0.000475 ; Loss = 1.491092\n",
      "2024-11-28 19:42:39.620000: I runner.py:310] Step = 34700 ; steps/s = 1.64, tokens/s = 43632 (43632 target) ; Learning rate = 0.000474 ; Loss = 1.496400\n",
      "2024-11-28 19:43:40.591000: I runner.py:310] Step = 34800 ; steps/s = 1.64, tokens/s = 43496 (43496 target) ; Learning rate = 0.000474 ; Loss = 1.555432\n",
      "2024-11-28 19:44:41.290000: I runner.py:310] Step = 34900 ; steps/s = 1.65, tokens/s = 43357 (43357 target) ; Learning rate = 0.000473 ; Loss = 1.482721\n",
      "2024-11-28 19:45:42.432000: I runner.py:310] Step = 35000 ; steps/s = 1.64, tokens/s = 43617 (43617 target) ; Learning rate = 0.000472 ; Loss = 1.489745\n",
      "2024-11-28 19:45:42.434000: I training.py:192] Running evaluation for step 35000\n",
      "2024-11-28 19:46:30.575000: I training.py:192] Evaluation result for step 35000: loss = 0.773513 ; perplexity = 2.167367\n",
      "2024-11-28 19:47:31.141000: I runner.py:310] Step = 35100 ; steps/s = 1.65, tokens/s = 43221 (43221 target) ; Learning rate = 0.000472 ; Loss = 1.483322\n",
      "2024-11-28 19:48:32.220000: I runner.py:310] Step = 35200 ; steps/s = 1.64, tokens/s = 43656 (43656 target) ; Learning rate = 0.000471 ; Loss = 1.486166\n",
      "2024-11-28 19:49:33.293000: I runner.py:310] Step = 35300 ; steps/s = 1.64, tokens/s = 43666 (43666 target) ; Learning rate = 0.000470 ; Loss = 1.489066\n",
      "2024-11-28 19:50:33.881000: I runner.py:310] Step = 35400 ; steps/s = 1.65, tokens/s = 43195 (43195 target) ; Learning rate = 0.000470 ; Loss = 1.485562\n",
      "2024-11-28 19:51:34.978000: I runner.py:310] Step = 35500 ; steps/s = 1.64, tokens/s = 43635 (43635 target) ; Learning rate = 0.000469 ; Loss = 1.489011\n",
      "2024-11-28 19:52:35.572000: I runner.py:310] Step = 35600 ; steps/s = 1.65, tokens/s = 43201 (43201 target) ; Learning rate = 0.000468 ; Loss = 1.483422\n",
      "2024-11-28 19:53:36.657000: I runner.py:310] Step = 35700 ; steps/s = 1.64, tokens/s = 43657 (43657 target) ; Learning rate = 0.000468 ; Loss = 1.489956\n",
      "2024-11-28 19:54:37.735000: I runner.py:310] Step = 35800 ; steps/s = 1.64, tokens/s = 43658 (43658 target) ; Learning rate = 0.000467 ; Loss = 1.490395\n",
      "2024-11-28 19:55:38.374000: I runner.py:310] Step = 35900 ; steps/s = 1.65, tokens/s = 43157 (43157 target) ; Learning rate = 0.000466 ; Loss = 1.484885\n",
      "2024-11-28 19:56:39.390000: I runner.py:310] Step = 36000 ; steps/s = 1.64, tokens/s = 43703 (43703 target) ; Learning rate = 0.000466 ; Loss = 1.490123\n",
      "2024-11-28 19:57:40.071000: I runner.py:310] Step = 36100 ; steps/s = 1.65, tokens/s = 43250 (43250 target) ; Learning rate = 0.000465 ; Loss = 1.526787\n",
      "2024-11-28 19:58:41.037000: I runner.py:310] Step = 36200 ; steps/s = 1.64, tokens/s = 43633 (43633 target) ; Learning rate = 0.000465 ; Loss = 1.482540\n",
      "2024-11-28 19:59:42.148000: I runner.py:310] Step = 36300 ; steps/s = 1.64, tokens/s = 43636 (43636 target) ; Learning rate = 0.000464 ; Loss = 1.483038\n",
      "2024-11-28 20:00:42.733000: I runner.py:310] Step = 36400 ; steps/s = 1.65, tokens/s = 43178 (43178 target) ; Learning rate = 0.000463 ; Loss = 1.478660\n",
      "2024-11-28 20:01:43.804000: I runner.py:310] Step = 36500 ; steps/s = 1.64, tokens/s = 43673 (43673 target) ; Learning rate = 0.000463 ; Loss = 1.497829\n",
      "2024-11-28 20:02:44.865000: I runner.py:310] Step = 36600 ; steps/s = 1.64, tokens/s = 43671 (43671 target) ; Learning rate = 0.000462 ; Loss = 1.493638\n",
      "2024-11-28 20:03:45.499000: I runner.py:310] Step = 36700 ; steps/s = 1.65, tokens/s = 43167 (43167 target) ; Learning rate = 0.000461 ; Loss = 1.483289\n",
      "2024-11-28 20:04:46.568000: I runner.py:310] Step = 36800 ; steps/s = 1.64, tokens/s = 43661 (43661 target) ; Learning rate = 0.000461 ; Loss = 1.490792\n",
      "2024-11-28 20:05:47.239000: I runner.py:310] Step = 36900 ; steps/s = 1.65, tokens/s = 43127 (43127 target) ; Learning rate = 0.000460 ; Loss = 1.482381\n",
      "2024-11-28 20:06:48.342000: I runner.py:310] Step = 37000 ; steps/s = 1.64, tokens/s = 43649 (43649 target) ; Learning rate = 0.000460 ; Loss = 1.485656\n",
      "2024-11-28 20:07:49.421000: I runner.py:310] Step = 37100 ; steps/s = 1.64, tokens/s = 43660 (43660 target) ; Learning rate = 0.000459 ; Loss = 1.482537\n",
      "2024-11-28 20:08:50.067000: I runner.py:310] Step = 37200 ; steps/s = 1.65, tokens/s = 43153 (43153 target) ; Learning rate = 0.000458 ; Loss = 1.481841\n",
      "2024-11-28 20:09:51.143000: I runner.py:310] Step = 37300 ; steps/s = 1.64, tokens/s = 43661 (43661 target) ; Learning rate = 0.000458 ; Loss = 1.486038\n",
      "2024-11-28 20:10:51.755000: I runner.py:310] Step = 37400 ; steps/s = 1.65, tokens/s = 43177 (43177 target) ; Learning rate = 0.000457 ; Loss = 1.477669\n",
      "2024-11-28 20:11:52.861000: I runner.py:310] Step = 37500 ; steps/s = 1.64, tokens/s = 43640 (43640 target) ; Learning rate = 0.000456 ; Loss = 1.479443\n",
      "2024-11-28 20:12:53.935000: I runner.py:310] Step = 37600 ; steps/s = 1.64, tokens/s = 43663 (43663 target) ; Learning rate = 0.000456 ; Loss = 1.483493\n",
      "2024-11-28 20:13:54.531000: I runner.py:310] Step = 37700 ; steps/s = 1.65, tokens/s = 43197 (43197 target) ; Learning rate = 0.000455 ; Loss = 1.481943\n",
      "2024-11-28 20:14:55.567000: I runner.py:310] Step = 37800 ; steps/s = 1.64, tokens/s = 43688 (43688 target) ; Learning rate = 0.000455 ; Loss = 1.490373\n",
      "2024-11-28 20:15:56.650000: I runner.py:310] Step = 37900 ; steps/s = 1.64, tokens/s = 43655 (43655 target) ; Learning rate = 0.000454 ; Loss = 1.481674\n",
      "2024-11-28 20:16:57.256000: I runner.py:310] Step = 38000 ; steps/s = 1.65, tokens/s = 43181 (43181 target) ; Learning rate = 0.000453 ; Loss = 1.483603\n",
      "2024-11-28 20:17:58.336000: I runner.py:310] Step = 38100 ; steps/s = 1.64, tokens/s = 43667 (43667 target) ; Learning rate = 0.000453 ; Loss = 1.484936\n",
      "2024-11-28 20:18:58.971000: I runner.py:310] Step = 38200 ; steps/s = 1.65, tokens/s = 43153 (43153 target) ; Learning rate = 0.000452 ; Loss = 1.477365\n",
      "2024-11-28 20:20:00.022000: I runner.py:310] Step = 38300 ; steps/s = 1.64, tokens/s = 43680 (43680 target) ; Learning rate = 0.000452 ; Loss = 1.481536\n",
      "2024-11-28 20:21:01.141000: I runner.py:310] Step = 38400 ; steps/s = 1.64, tokens/s = 43625 (43625 target) ; Learning rate = 0.000451 ; Loss = 1.488653\n",
      "2024-11-28 20:22:01.793000: I runner.py:310] Step = 38500 ; steps/s = 1.65, tokens/s = 43155 (43155 target) ; Learning rate = 0.000450 ; Loss = 1.475335\n",
      "2024-11-28 20:23:02.843000: I runner.py:310] Step = 38600 ; steps/s = 1.64, tokens/s = 43673 (43673 target) ; Learning rate = 0.000450 ; Loss = 1.488768\n",
      "2024-11-28 20:24:03.436000: I runner.py:310] Step = 38700 ; steps/s = 1.65, tokens/s = 43193 (43193 target) ; Learning rate = 0.000449 ; Loss = 1.476696\n",
      "2024-11-28 20:25:04.532000: I runner.py:310] Step = 38800 ; steps/s = 1.64, tokens/s = 43653 (43653 target) ; Learning rate = 0.000449 ; Loss = 1.480001\n",
      "2024-11-28 20:26:05.628000: I runner.py:310] Step = 38900 ; steps/s = 1.64, tokens/s = 43649 (43649 target) ; Learning rate = 0.000448 ; Loss = 1.481901\n",
      "2024-11-28 20:27:06.242000: I runner.py:310] Step = 39000 ; steps/s = 1.65, tokens/s = 43168 (43168 target) ; Learning rate = 0.000448 ; Loss = 1.476337\n",
      "2024-11-28 20:28:07.298000: I runner.py:310] Step = 39100 ; steps/s = 1.64, tokens/s = 43686 (43686 target) ; Learning rate = 0.000447 ; Loss = 1.483273\n",
      "2024-11-28 20:29:08.430000: I runner.py:310] Step = 39200 ; steps/s = 1.64, tokens/s = 43619 (43619 target) ; Learning rate = 0.000446 ; Loss = 1.482213\n",
      "2024-11-28 20:30:09.024000: I runner.py:310] Step = 39300 ; steps/s = 1.65, tokens/s = 43196 (43196 target) ; Learning rate = 0.000446 ; Loss = 1.477821\n",
      "2024-11-28 20:31:10.178000: I runner.py:310] Step = 39400 ; steps/s = 1.64, tokens/s = 43597 (43597 target) ; Learning rate = 0.000445 ; Loss = 1.479919\n",
      "2024-11-28 20:32:10.838000: I runner.py:310] Step = 39500 ; steps/s = 1.65, tokens/s = 43144 (43144 target) ; Learning rate = 0.000445 ; Loss = 1.471806\n",
      "2024-11-28 20:33:11.956000: I runner.py:310] Step = 39600 ; steps/s = 1.64, tokens/s = 43643 (43643 target) ; Learning rate = 0.000444 ; Loss = 1.486568\n",
      "2024-11-28 20:34:13.022000: I runner.py:310] Step = 39700 ; steps/s = 1.64, tokens/s = 43667 (43667 target) ; Learning rate = 0.000444 ; Loss = 1.483120\n",
      "2024-11-28 20:35:13.646000: I runner.py:310] Step = 39800 ; steps/s = 1.65, tokens/s = 43156 (43156 target) ; Learning rate = 0.000443 ; Loss = 1.476640\n",
      "2024-11-28 20:36:14.732000: I runner.py:310] Step = 39900 ; steps/s = 1.64, tokens/s = 43650 (43650 target) ; Learning rate = 0.000442 ; Loss = 1.473462\n",
      "2024-11-28 20:37:15.332000: I runner.py:310] Step = 40000 ; steps/s = 1.65, tokens/s = 43193 (43193 target) ; Learning rate = 0.000442 ; Loss = 1.479516\n",
      "2024-11-28 20:37:17.115000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-40000\n",
      "2024-11-28 20:37:17.116000: I training.py:192] Running evaluation for step 40000\n",
      "2024-11-28 20:38:12.015000: I training.py:192] Evaluation result for step 40000: loss = 0.792272 ; perplexity = 2.208407\n",
      "2024-11-28 20:39:13.030000: I runner.py:310] Step = 40100 ; steps/s = 1.64, tokens/s = 43716 (43716 target) ; Learning rate = 0.000441 ; Loss = 1.478243\n",
      "2024-11-28 20:40:14.064000: I runner.py:310] Step = 40200 ; steps/s = 1.64, tokens/s = 43681 (43681 target) ; Learning rate = 0.000441 ; Loss = 1.479139\n",
      "2024-11-28 20:41:14.687000: I runner.py:310] Step = 40300 ; steps/s = 1.65, tokens/s = 43177 (43177 target) ; Learning rate = 0.000440 ; Loss = 1.471754\n",
      "2024-11-28 20:42:15.769000: I runner.py:310] Step = 40400 ; steps/s = 1.64, tokens/s = 43657 (43657 target) ; Learning rate = 0.000440 ; Loss = 1.489392\n",
      "2024-11-28 20:43:16.902000: I runner.py:310] Step = 40500 ; steps/s = 1.64, tokens/s = 43619 (43619 target) ; Learning rate = 0.000439 ; Loss = 1.484691\n",
      "2024-11-28 20:44:17.672000: I runner.py:310] Step = 40600 ; steps/s = 1.65, tokens/s = 43069 (43069 target) ; Learning rate = 0.000439 ; Loss = 1.474024\n",
      "2024-11-28 20:45:18.820000: I runner.py:310] Step = 40700 ; steps/s = 1.64, tokens/s = 43606 (43606 target) ; Learning rate = 0.000438 ; Loss = 1.481575\n",
      "2024-11-28 20:46:19.482000: I runner.py:310] Step = 40800 ; steps/s = 1.65, tokens/s = 43138 (43138 target) ; Learning rate = 0.000438 ; Loss = 1.476363\n",
      "2024-11-28 20:47:20.569000: I runner.py:310] Step = 40900 ; steps/s = 1.64, tokens/s = 43659 (43659 target) ; Learning rate = 0.000437 ; Loss = 1.476125\n",
      "2024-11-28 20:48:21.644000: I runner.py:310] Step = 41000 ; steps/s = 1.64, tokens/s = 43663 (43663 target) ; Learning rate = 0.000437 ; Loss = 1.480450\n",
      "2024-11-28 20:49:22.266000: I runner.py:310] Step = 41100 ; steps/s = 1.65, tokens/s = 43170 (43170 target) ; Learning rate = 0.000436 ; Loss = 1.475698\n",
      "2024-11-28 20:50:23.391000: I runner.py:310] Step = 41200 ; steps/s = 1.64, tokens/s = 43626 (43626 target) ; Learning rate = 0.000435 ; Loss = 1.479704\n",
      "2024-11-28 20:51:24.088000: I runner.py:310] Step = 41300 ; steps/s = 1.65, tokens/s = 43112 (43112 target) ; Learning rate = 0.000435 ; Loss = 1.474440\n",
      "2024-11-28 20:52:25.168000: I runner.py:310] Step = 41400 ; steps/s = 1.64, tokens/s = 43660 (43660 target) ; Learning rate = 0.000434 ; Loss = 1.474476\n",
      "2024-11-28 20:53:26.207000: I runner.py:310] Step = 41500 ; steps/s = 1.64, tokens/s = 43679 (43679 target) ; Learning rate = 0.000434 ; Loss = 1.476142\n",
      "2024-11-28 20:54:26.839000: I runner.py:310] Step = 41600 ; steps/s = 1.65, tokens/s = 43176 (43176 target) ; Learning rate = 0.000433 ; Loss = 1.470763\n",
      "2024-11-28 20:55:27.946000: I runner.py:310] Step = 41700 ; steps/s = 1.64, tokens/s = 43634 (43634 target) ; Learning rate = 0.000433 ; Loss = 1.483105\n",
      "2024-11-28 20:56:29.013000: I runner.py:310] Step = 41800 ; steps/s = 1.64, tokens/s = 43667 (43667 target) ; Learning rate = 0.000432 ; Loss = 1.474374\n",
      "2024-11-28 20:57:29.613000: I runner.py:310] Step = 41900 ; steps/s = 1.65, tokens/s = 43188 (43188 target) ; Learning rate = 0.000432 ; Loss = 1.471477\n",
      "2024-11-28 20:58:30.686000: I runner.py:310] Step = 42000 ; steps/s = 1.64, tokens/s = 43660 (43660 target) ; Learning rate = 0.000431 ; Loss = 1.475655\n",
      "2024-11-28 20:59:31.345000: I runner.py:310] Step = 42100 ; steps/s = 1.65, tokens/s = 43140 (43140 target) ; Learning rate = 0.000431 ; Loss = 1.466111\n",
      "2024-11-28 21:00:32.435000: I runner.py:310] Step = 42200 ; steps/s = 1.64, tokens/s = 43654 (43654 target) ; Learning rate = 0.000430 ; Loss = 1.477762\n",
      "2024-11-28 21:01:33.519000: I runner.py:310] Step = 42300 ; steps/s = 1.64, tokens/s = 43652 (43652 target) ; Learning rate = 0.000430 ; Loss = 1.479602\n",
      "2024-11-28 21:02:34.082000: I runner.py:310] Step = 42400 ; steps/s = 1.65, tokens/s = 43218 (43218 target) ; Learning rate = 0.000429 ; Loss = 1.477396\n",
      "2024-11-28 21:03:35.157000: I runner.py:310] Step = 42500 ; steps/s = 1.64, tokens/s = 43657 (43657 target) ; Learning rate = 0.000429 ; Loss = 1.476932\n",
      "2024-11-28 21:04:35.807000: I runner.py:310] Step = 42600 ; steps/s = 1.65, tokens/s = 43156 (43156 target) ; Learning rate = 0.000428 ; Loss = 1.469153\n",
      "2024-11-28 21:05:36.927000: I runner.py:310] Step = 42700 ; steps/s = 1.64, tokens/s = 43630 (43630 target) ; Learning rate = 0.000428 ; Loss = 1.476453\n",
      "2024-11-28 21:06:37.923000: I runner.py:310] Step = 42800 ; steps/s = 1.64, tokens/s = 43723 (43723 target) ; Learning rate = 0.000427 ; Loss = 1.477467\n",
      "2024-11-28 21:07:38.564000: I runner.py:310] Step = 42900 ; steps/s = 1.65, tokens/s = 43159 (43159 target) ; Learning rate = 0.000427 ; Loss = 1.475556\n",
      "2024-11-28 21:08:39.623000: I runner.py:310] Step = 43000 ; steps/s = 1.64, tokens/s = 43672 (43672 target) ; Learning rate = 0.000426 ; Loss = 1.479684\n",
      "2024-11-28 21:09:40.745000: I runner.py:310] Step = 43100 ; steps/s = 1.64, tokens/s = 43627 (43627 target) ; Learning rate = 0.000426 ; Loss = 1.474416\n",
      "2024-11-28 21:10:41.335000: I runner.py:310] Step = 43200 ; steps/s = 1.65, tokens/s = 43186 (43186 target) ; Learning rate = 0.000425 ; Loss = 1.467510\n",
      "2024-11-28 21:11:42.376000: I runner.py:310] Step = 43300 ; steps/s = 1.64, tokens/s = 43679 (43679 target) ; Learning rate = 0.000425 ; Loss = 1.479178\n",
      "2024-11-28 21:12:43.019000: I runner.py:310] Step = 43400 ; steps/s = 1.65, tokens/s = 43162 (43162 target) ; Learning rate = 0.000424 ; Loss = 1.470239\n",
      "2024-11-28 21:13:44.112000: I runner.py:310] Step = 43500 ; steps/s = 1.64, tokens/s = 43650 (43650 target) ; Learning rate = 0.000424 ; Loss = 1.470863\n",
      "2024-11-28 21:14:45.126000: I runner.py:310] Step = 43600 ; steps/s = 1.64, tokens/s = 43714 (43714 target) ; Learning rate = 0.000423 ; Loss = 1.474862\n",
      "2024-11-28 21:15:45.732000: I runner.py:310] Step = 43700 ; steps/s = 1.65, tokens/s = 43173 (43173 target) ; Learning rate = 0.000423 ; Loss = 1.472626\n",
      "2024-11-28 21:16:46.804000: I runner.py:310] Step = 43800 ; steps/s = 1.64, tokens/s = 43666 (43666 target) ; Learning rate = 0.000422 ; Loss = 1.482106\n",
      "2024-11-28 21:17:47.483000: I runner.py:310] Step = 43900 ; steps/s = 1.65, tokens/s = 43134 (43134 target) ; Learning rate = 0.000422 ; Loss = 1.470886\n",
      "2024-11-28 21:18:48.587000: I runner.py:310] Step = 44000 ; steps/s = 1.64, tokens/s = 43638 (43638 target) ; Learning rate = 0.000421 ; Loss = 1.469599\n",
      "2024-11-28 21:19:49.652000: I runner.py:310] Step = 44100 ; steps/s = 1.64, tokens/s = 43668 (43668 target) ; Learning rate = 0.000421 ; Loss = 1.478227\n",
      "2024-11-28 21:20:50.301000: I runner.py:310] Step = 44200 ; steps/s = 1.65, tokens/s = 43156 (43156 target) ; Learning rate = 0.000420 ; Loss = 1.474729\n",
      "2024-11-28 21:21:51.336000: I runner.py:310] Step = 44300 ; steps/s = 1.64, tokens/s = 43683 (43683 target) ; Learning rate = 0.000420 ; Loss = 1.475025\n",
      "2024-11-28 21:22:52.437000: I runner.py:310] Step = 44400 ; steps/s = 1.64, tokens/s = 43647 (43647 target) ; Learning rate = 0.000419 ; Loss = 1.476127\n",
      "2024-11-28 21:23:53.107000: I runner.py:310] Step = 44500 ; steps/s = 1.65, tokens/s = 43135 (43135 target) ; Learning rate = 0.000419 ; Loss = 1.472374\n",
      "2024-11-28 21:24:54.248000: I runner.py:310] Step = 44600 ; steps/s = 1.64, tokens/s = 43604 (43604 target) ; Learning rate = 0.000419 ; Loss = 1.472614\n",
      "2024-11-28 21:25:54.916000: I runner.py:310] Step = 44700 ; steps/s = 1.65, tokens/s = 43150 (43150 target) ; Learning rate = 0.000418 ; Loss = 1.467287\n",
      "2024-11-28 21:26:55.874000: I runner.py:310] Step = 44800 ; steps/s = 1.64, tokens/s = 43749 (43749 target) ; Learning rate = 0.000418 ; Loss = 1.474804\n",
      "2024-11-28 21:27:56.983000: I runner.py:310] Step = 44900 ; steps/s = 1.64, tokens/s = 43637 (43637 target) ; Learning rate = 0.000417 ; Loss = 1.472039\n",
      "2024-11-28 21:28:57.630000: I runner.py:310] Step = 45000 ; steps/s = 1.65, tokens/s = 43149 (43149 target) ; Learning rate = 0.000417 ; Loss = 1.473047\n",
      "2024-11-28 21:28:57.632000: I training.py:192] Running evaluation for step 45000\n",
      "2024-11-28 21:29:46.772000: I training.py:192] Evaluation result for step 45000: loss = 0.796928 ; perplexity = 2.218714\n",
      "2024-11-28 21:30:47.765000: I runner.py:310] Step = 45100 ; steps/s = 1.64, tokens/s = 43721 (43721 target) ; Learning rate = 0.000416 ; Loss = 1.473537\n",
      "2024-11-28 21:31:48.426000: I runner.py:310] Step = 45200 ; steps/s = 1.65, tokens/s = 43151 (43151 target) ; Learning rate = 0.000416 ; Loss = 1.468995\n",
      "2024-11-28 21:32:49.502000: I runner.py:310] Step = 45300 ; steps/s = 1.64, tokens/s = 43672 (43672 target) ; Learning rate = 0.000415 ; Loss = 1.467649\n",
      "2024-11-28 21:33:50.631000: I runner.py:310] Step = 45400 ; steps/s = 1.64, tokens/s = 43608 (43608 target) ; Learning rate = 0.000415 ; Loss = 1.468806\n",
      "2024-11-28 21:34:51.302000: I runner.py:310] Step = 45500 ; steps/s = 1.65, tokens/s = 43136 (43136 target) ; Learning rate = 0.000414 ; Loss = 1.468481\n",
      "2024-11-28 21:35:52.430000: I runner.py:310] Step = 45600 ; steps/s = 1.64, tokens/s = 43613 (43613 target) ; Learning rate = 0.000414 ; Loss = 1.469387\n",
      "2024-11-28 21:36:53.529000: I runner.py:310] Step = 45700 ; steps/s = 1.64, tokens/s = 43657 (43657 target) ; Learning rate = 0.000413 ; Loss = 1.474313\n",
      "2024-11-28 21:37:54.200000: I runner.py:310] Step = 45800 ; steps/s = 1.65, tokens/s = 43145 (43145 target) ; Learning rate = 0.000413 ; Loss = 1.471386\n",
      "2024-11-28 21:38:55.372000: I runner.py:310] Step = 45900 ; steps/s = 1.63, tokens/s = 43594 (43594 target) ; Learning rate = 0.000413 ; Loss = 1.472168\n",
      "2024-11-28 21:39:56.055000: I runner.py:310] Step = 46000 ; steps/s = 1.65, tokens/s = 43123 (43123 target) ; Learning rate = 0.000412 ; Loss = 1.463002\n",
      "2024-11-28 21:40:57.059000: I runner.py:310] Step = 46100 ; steps/s = 1.64, tokens/s = 43714 (43714 target) ; Learning rate = 0.000412 ; Loss = 1.472628\n",
      "2024-11-28 21:41:58.216000: I runner.py:310] Step = 46200 ; steps/s = 1.64, tokens/s = 43600 (43600 target) ; Learning rate = 0.000411 ; Loss = 1.468424\n",
      "2024-11-28 21:42:58.757000: I runner.py:310] Step = 46300 ; steps/s = 1.65, tokens/s = 43230 (43230 target) ; Learning rate = 0.000411 ; Loss = 1.477378\n",
      "2024-11-28 21:43:59.870000: I runner.py:310] Step = 46400 ; steps/s = 1.64, tokens/s = 43636 (43636 target) ; Learning rate = 0.000410 ; Loss = 1.467201\n",
      "2024-11-28 21:45:00.536000: I runner.py:310] Step = 46500 ; steps/s = 1.65, tokens/s = 43131 (43131 target) ; Learning rate = 0.000410 ; Loss = 1.473842\n",
      "2024-11-28 21:46:01.596000: I runner.py:310] Step = 46600 ; steps/s = 1.64, tokens/s = 43676 (43676 target) ; Learning rate = 0.000409 ; Loss = 1.463852\n",
      "2024-11-28 21:47:02.681000: I runner.py:310] Step = 46700 ; steps/s = 1.64, tokens/s = 43652 (43652 target) ; Learning rate = 0.000409 ; Loss = 1.469187\n",
      "2024-11-28 21:48:03.353000: I runner.py:310] Step = 46800 ; steps/s = 1.65, tokens/s = 43131 (43131 target) ; Learning rate = 0.000409 ; Loss = 1.473836\n",
      "2024-11-28 21:49:04.451000: I runner.py:310] Step = 46900 ; steps/s = 1.64, tokens/s = 43646 (43646 target) ; Learning rate = 0.000408 ; Loss = 1.468810\n",
      "2024-11-28 21:50:05.582000: I runner.py:310] Step = 47000 ; steps/s = 1.64, tokens/s = 43628 (43628 target) ; Learning rate = 0.000408 ; Loss = 1.472098\n",
      "2024-11-28 21:51:06.231000: I runner.py:310] Step = 47100 ; steps/s = 1.65, tokens/s = 43154 (43154 target) ; Learning rate = 0.000407 ; Loss = 1.469024\n",
      "2024-11-28 21:52:07.369000: I runner.py:310] Step = 47200 ; steps/s = 1.64, tokens/s = 43612 (43612 target) ; Learning rate = 0.000407 ; Loss = 1.474169\n",
      "2024-11-28 21:53:07.965000: I runner.py:310] Step = 47300 ; steps/s = 1.65, tokens/s = 43190 (43190 target) ; Learning rate = 0.000406 ; Loss = 1.467776\n",
      "2024-11-28 21:54:09.080000: I runner.py:310] Step = 47400 ; steps/s = 1.64, tokens/s = 43632 (43632 target) ; Learning rate = 0.000406 ; Loss = 1.469236\n",
      "2024-11-28 21:55:10.167000: I runner.py:310] Step = 47500 ; steps/s = 1.64, tokens/s = 43656 (43656 target) ; Learning rate = 0.000406 ; Loss = 1.469387\n",
      "2024-11-28 21:56:10.782000: I runner.py:310] Step = 47600 ; steps/s = 1.65, tokens/s = 43182 (43182 target) ; Learning rate = 0.000405 ; Loss = 1.470660\n",
      "2024-11-28 21:57:11.857000: I runner.py:310] Step = 47700 ; steps/s = 1.64, tokens/s = 43665 (43665 target) ; Learning rate = 0.000405 ; Loss = 1.467413\n",
      "2024-11-28 21:58:12.465000: I runner.py:310] Step = 47800 ; steps/s = 1.65, tokens/s = 43168 (43168 target) ; Learning rate = 0.000404 ; Loss = 1.472433\n",
      "2024-11-28 21:59:13.498000: I runner.py:310] Step = 47900 ; steps/s = 1.64, tokens/s = 43699 (43699 target) ; Learning rate = 0.000404 ; Loss = 1.462379\n",
      "2024-11-28 22:00:14.597000: I runner.py:310] Step = 48000 ; steps/s = 1.64, tokens/s = 43642 (43642 target) ; Learning rate = 0.000403 ; Loss = 1.468602\n",
      "2024-11-28 22:01:15.171000: I runner.py:310] Step = 48100 ; steps/s = 1.65, tokens/s = 43195 (43195 target) ; Learning rate = 0.000403 ; Loss = 1.468226\n",
      "2024-11-28 22:02:16.279000: I runner.py:310] Step = 48200 ; steps/s = 1.64, tokens/s = 43648 (43648 target) ; Learning rate = 0.000403 ; Loss = 1.478256\n",
      "2024-11-28 22:03:17.357000: I runner.py:310] Step = 48300 ; steps/s = 1.64, tokens/s = 43655 (43655 target) ; Learning rate = 0.000402 ; Loss = 1.469634\n",
      "2024-11-28 22:04:17.982000: I runner.py:310] Step = 48400 ; steps/s = 1.65, tokens/s = 43173 (43173 target) ; Learning rate = 0.000402 ; Loss = 1.462281\n",
      "2024-11-28 22:05:19.110000: I runner.py:310] Step = 48500 ; steps/s = 1.64, tokens/s = 43629 (43629 target) ; Learning rate = 0.000401 ; Loss = 1.468295\n",
      "2024-11-28 22:06:19.775000: I runner.py:310] Step = 48600 ; steps/s = 1.65, tokens/s = 43134 (43134 target) ; Learning rate = 0.000401 ; Loss = 1.470278\n",
      "2024-11-28 22:07:20.777000: I runner.py:310] Step = 48700 ; steps/s = 1.64, tokens/s = 43725 (43725 target) ; Learning rate = 0.000401 ; Loss = 1.467272\n",
      "2024-11-28 22:08:21.909000: I runner.py:310] Step = 48800 ; steps/s = 1.64, tokens/s = 43610 (43610 target) ; Learning rate = 0.000400 ; Loss = 1.472128\n",
      "2024-11-28 22:09:22.503000: I runner.py:310] Step = 48900 ; steps/s = 1.65, tokens/s = 43186 (43186 target) ; Learning rate = 0.000400 ; Loss = 1.473441\n",
      "2024-11-28 22:10:23.602000: I runner.py:310] Step = 49000 ; steps/s = 1.64, tokens/s = 43640 (43640 target) ; Learning rate = 0.000399 ; Loss = 1.472429\n",
      "2024-11-28 22:11:24.200000: I runner.py:310] Step = 49100 ; steps/s = 1.65, tokens/s = 43199 (43199 target) ; Learning rate = 0.000399 ; Loss = 1.462247\n",
      "2024-11-28 22:12:25.261000: I runner.py:310] Step = 49200 ; steps/s = 1.64, tokens/s = 43679 (43679 target) ; Learning rate = 0.000398 ; Loss = 1.462957\n",
      "2024-11-28 22:13:26.319000: I runner.py:310] Step = 49300 ; steps/s = 1.64, tokens/s = 43667 (43667 target) ; Learning rate = 0.000398 ; Loss = 1.471341\n",
      "2024-11-28 22:14:26.957000: I runner.py:310] Step = 49400 ; steps/s = 1.65, tokens/s = 43161 (43161 target) ; Learning rate = 0.000398 ; Loss = 1.465512\n",
      "2024-11-28 22:15:28.023000: I runner.py:310] Step = 49500 ; steps/s = 1.64, tokens/s = 43673 (43673 target) ; Learning rate = 0.000397 ; Loss = 1.471204\n",
      "2024-11-28 22:16:29.108000: I runner.py:310] Step = 49600 ; steps/s = 1.64, tokens/s = 43653 (43653 target) ; Learning rate = 0.000397 ; Loss = 1.470035\n",
      "2024-11-28 22:17:29.747000: I runner.py:310] Step = 49700 ; steps/s = 1.65, tokens/s = 43153 (43153 target) ; Learning rate = 0.000396 ; Loss = 1.459590\n",
      "2024-11-28 22:18:30.848000: I runner.py:310] Step = 49800 ; steps/s = 1.64, tokens/s = 43645 (43645 target) ; Learning rate = 0.000396 ; Loss = 1.470790\n",
      "2024-11-28 22:19:31.447000: I runner.py:310] Step = 49900 ; steps/s = 1.65, tokens/s = 43189 (43189 target) ; Learning rate = 0.000396 ; Loss = 1.463566\n",
      "2024-11-28 22:20:32.549000: I runner.py:310] Step = 50000 ; steps/s = 1.64, tokens/s = 43641 (43641 target) ; Learning rate = 0.000395 ; Loss = 1.464844\n",
      "2024-11-28 22:20:34.418000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-50000\n",
      "2024-11-28 22:20:34.418000: I training.py:192] Running evaluation for step 50000\n",
      "2024-11-28 22:21:25.352000: I training.py:192] Evaluation result for step 50000: loss = 0.802626 ; perplexity = 2.231392\n",
      "2024-11-28 22:22:26.410000: I runner.py:310] Step = 50100 ; steps/s = 1.64, tokens/s = 43677 (43677 target) ; Learning rate = 0.000395 ; Loss = 1.466138\n",
      "2024-11-28 22:23:27.124000: I runner.py:310] Step = 50200 ; steps/s = 1.65, tokens/s = 43116 (43116 target) ; Learning rate = 0.000394 ; Loss = 1.464173\n",
      "2024-11-28 22:24:28.192000: I runner.py:310] Step = 50300 ; steps/s = 1.64, tokens/s = 43673 (43673 target) ; Learning rate = 0.000394 ; Loss = 1.467666\n",
      "2024-11-28 22:25:28.835000: I runner.py:310] Step = 50400 ; steps/s = 1.65, tokens/s = 43138 (43138 target) ; Learning rate = 0.000394 ; Loss = 1.459941\n",
      "2024-11-28 22:26:29.833000: I runner.py:310] Step = 50500 ; steps/s = 1.64, tokens/s = 43722 (43722 target) ; Learning rate = 0.000393 ; Loss = 1.462139\n",
      "2024-11-28 22:27:30.929000: I runner.py:310] Step = 50600 ; steps/s = 1.64, tokens/s = 43641 (43641 target) ; Learning rate = 0.000393 ; Loss = 1.463044\n",
      "2024-11-28 22:28:31.581000: I runner.py:310] Step = 50700 ; steps/s = 1.65, tokens/s = 43152 (43152 target) ; Learning rate = 0.000393 ; Loss = 1.465388\n",
      "2024-11-28 22:29:32.693000: I runner.py:310] Step = 50800 ; steps/s = 1.64, tokens/s = 43636 (43636 target) ; Learning rate = 0.000392 ; Loss = 1.469990\n",
      "2024-11-28 22:30:33.751000: I runner.py:310] Step = 50900 ; steps/s = 1.64, tokens/s = 43676 (43676 target) ; Learning rate = 0.000392 ; Loss = 1.469272\n",
      "2024-11-28 22:31:34.400000: I runner.py:310] Step = 51000 ; steps/s = 1.65, tokens/s = 43148 (43148 target) ; Learning rate = 0.000391 ; Loss = 1.464000\n",
      "2024-11-28 22:32:35.485000: I runner.py:310] Step = 51100 ; steps/s = 1.64, tokens/s = 43652 (43652 target) ; Learning rate = 0.000391 ; Loss = 1.462905\n",
      "2024-11-28 22:33:36.084000: I runner.py:310] Step = 51200 ; steps/s = 1.65, tokens/s = 43195 (43195 target) ; Learning rate = 0.000391 ; Loss = 1.461514\n",
      "2024-11-28 22:34:37.140000: I runner.py:310] Step = 51300 ; steps/s = 1.64, tokens/s = 43677 (43677 target) ; Learning rate = 0.000390 ; Loss = 1.466109\n",
      "2024-11-28 22:35:38.219000: I runner.py:310] Step = 51400 ; steps/s = 1.64, tokens/s = 43651 (43651 target) ; Learning rate = 0.000390 ; Loss = 1.464935\n",
      "2024-11-28 22:36:38.820000: I runner.py:310] Step = 51500 ; steps/s = 1.65, tokens/s = 43201 (43201 target) ; Learning rate = 0.000389 ; Loss = 1.464338\n",
      "2024-11-28 22:37:39.895000: I runner.py:310] Step = 51600 ; steps/s = 1.64, tokens/s = 43649 (43649 target) ; Learning rate = 0.000389 ; Loss = 1.468338\n",
      "2024-11-28 22:38:40.383000: I runner.py:310] Step = 51700 ; steps/s = 1.65, tokens/s = 43268 (43268 target) ; Learning rate = 0.000389 ; Loss = 1.460234\n",
      "2024-11-28 22:39:41.450000: I runner.py:310] Step = 51800 ; steps/s = 1.64, tokens/s = 43661 (43661 target) ; Learning rate = 0.000388 ; Loss = 1.460048\n",
      "2024-11-28 22:40:42.539000: I runner.py:310] Step = 51900 ; steps/s = 1.64, tokens/s = 43654 (43654 target) ; Learning rate = 0.000388 ; Loss = 1.465709\n",
      "2024-11-28 22:41:43.147000: I runner.py:310] Step = 52000 ; steps/s = 1.65, tokens/s = 43190 (43190 target) ; Learning rate = 0.000388 ; Loss = 1.458311\n",
      "2024-11-28 22:42:44.219000: I runner.py:310] Step = 52100 ; steps/s = 1.64, tokens/s = 43667 (43667 target) ; Learning rate = 0.000387 ; Loss = 1.465514\n",
      "2024-11-28 22:43:45.272000: I runner.py:310] Step = 52200 ; steps/s = 1.64, tokens/s = 43558 (43558 target) ; Learning rate = 0.000387 ; Loss = 1.483524\n",
      "2024-11-28 22:44:45.947000: I runner.py:310] Step = 52300 ; steps/s = 1.65, tokens/s = 43251 (43251 target) ; Learning rate = 0.000386 ; Loss = 1.460629\n",
      "2024-11-28 22:45:47.055000: I runner.py:310] Step = 52400 ; steps/s = 1.64, tokens/s = 43634 (43634 target) ; Learning rate = 0.000386 ; Loss = 1.464291\n",
      "2024-11-28 22:46:47.684000: I runner.py:310] Step = 52500 ; steps/s = 1.65, tokens/s = 43168 (43168 target) ; Learning rate = 0.000386 ; Loss = 1.462363\n",
      "2024-11-28 22:47:48.682000: I runner.py:310] Step = 52600 ; steps/s = 1.64, tokens/s = 43714 (43714 target) ; Learning rate = 0.000385 ; Loss = 1.460215\n",
      "2024-11-28 22:48:49.796000: I runner.py:310] Step = 52700 ; steps/s = 1.64, tokens/s = 43628 (43628 target) ; Learning rate = 0.000385 ; Loss = 1.461922\n",
      "2024-11-28 22:49:50.425000: I runner.py:310] Step = 52800 ; steps/s = 1.65, tokens/s = 43169 (43169 target) ; Learning rate = 0.000385 ; Loss = 1.468936\n",
      "2024-11-28 22:50:51.516000: I runner.py:310] Step = 52900 ; steps/s = 1.64, tokens/s = 43641 (43641 target) ; Learning rate = 0.000384 ; Loss = 1.466193\n",
      "2024-11-28 22:51:52.162000: I runner.py:310] Step = 53000 ; steps/s = 1.65, tokens/s = 43168 (43168 target) ; Learning rate = 0.000384 ; Loss = 1.456431\n",
      "2024-11-28 22:52:53.279000: I runner.py:310] Step = 53100 ; steps/s = 1.64, tokens/s = 43633 (43633 target) ; Learning rate = 0.000384 ; Loss = 1.457239\n",
      "2024-11-28 22:53:54.356000: I runner.py:310] Step = 53200 ; steps/s = 1.64, tokens/s = 43659 (43659 target) ; Learning rate = 0.000383 ; Loss = 1.468170\n",
      "2024-11-28 22:54:54.988000: I runner.py:310] Step = 53300 ; steps/s = 1.65, tokens/s = 43162 (43162 target) ; Learning rate = 0.000383 ; Loss = 1.459727\n",
      "2024-11-28 22:55:56.088000: I runner.py:310] Step = 53400 ; steps/s = 1.64, tokens/s = 43651 (43651 target) ; Learning rate = 0.000382 ; Loss = 1.462936\n",
      "2024-11-28 22:56:56.945000: I runner.py:310] Step = 53500 ; steps/s = 1.64, tokens/s = 43426 (43426 target) ; Learning rate = 0.000382 ; Loss = 1.518871\n",
      "2024-11-28 22:57:57.796000: I runner.py:310] Step = 53600 ; steps/s = 1.64, tokens/s = 43394 (43394 target) ; Learning rate = 0.000382 ; Loss = 1.460326\n",
      "2024-11-28 22:58:58.888000: I runner.py:310] Step = 53700 ; steps/s = 1.64, tokens/s = 43642 (43642 target) ; Learning rate = 0.000381 ; Loss = 1.469645\n",
      "2024-11-28 22:59:59.552000: I runner.py:310] Step = 53800 ; steps/s = 1.65, tokens/s = 43141 (43141 target) ; Learning rate = 0.000381 ; Loss = 1.456766\n",
      "2024-11-28 23:01:00.571000: I runner.py:310] Step = 53900 ; steps/s = 1.64, tokens/s = 43709 (43709 target) ; Learning rate = 0.000381 ; Loss = 1.461622\n",
      "2024-11-28 23:02:01.724000: I runner.py:310] Step = 54000 ; steps/s = 1.64, tokens/s = 43611 (43611 target) ; Learning rate = 0.000380 ; Loss = 1.464804\n",
      "2024-11-28 23:03:02.336000: I runner.py:310] Step = 54100 ; steps/s = 1.65, tokens/s = 43169 (43169 target) ; Learning rate = 0.000380 ; Loss = 1.467429\n",
      "2024-11-28 23:04:03.442000: I runner.py:310] Step = 54200 ; steps/s = 1.64, tokens/s = 43640 (43640 target) ; Learning rate = 0.000380 ; Loss = 1.464193\n",
      "2024-11-28 23:05:04.060000: I runner.py:310] Step = 54300 ; steps/s = 1.65, tokens/s = 43169 (43169 target) ; Learning rate = 0.000379 ; Loss = 1.460972\n",
      "2024-11-28 23:06:05.163000: I runner.py:310] Step = 54400 ; steps/s = 1.64, tokens/s = 43643 (43643 target) ; Learning rate = 0.000379 ; Loss = 1.466101\n",
      "2024-11-28 23:07:06.254000: I runner.py:310] Step = 54500 ; steps/s = 1.64, tokens/s = 43651 (43651 target) ; Learning rate = 0.000379 ; Loss = 1.463251\n",
      "2024-11-28 23:08:06.857000: I runner.py:310] Step = 54600 ; steps/s = 1.65, tokens/s = 43191 (43191 target) ; Learning rate = 0.000378 ; Loss = 1.463782\n",
      "2024-11-28 23:09:07.983000: I runner.py:310] Step = 54700 ; steps/s = 1.64, tokens/s = 43629 (43629 target) ; Learning rate = 0.000378 ; Loss = 1.461639\n",
      "2024-11-28 23:10:08.598000: I runner.py:310] Step = 54800 ; steps/s = 1.65, tokens/s = 43165 (43165 target) ; Learning rate = 0.000378 ; Loss = 1.465280\n",
      "2024-11-28 23:11:09.642000: I runner.py:310] Step = 54900 ; steps/s = 1.64, tokens/s = 43692 (43692 target) ; Learning rate = 0.000377 ; Loss = 1.457648\n",
      "2024-11-28 23:12:10.690000: I runner.py:310] Step = 55000 ; steps/s = 1.64, tokens/s = 43678 (43678 target) ; Learning rate = 0.000377 ; Loss = 1.460524\n",
      "2024-11-28 23:12:10.691000: I training.py:192] Running evaluation for step 55000\n",
      "2024-11-28 23:12:59.964000: I training.py:192] Evaluation result for step 55000: loss = 0.810264 ; perplexity = 2.248502\n",
      "2024-11-28 23:14:00.555000: I runner.py:310] Step = 55100 ; steps/s = 1.65, tokens/s = 43198 (43198 target) ; Learning rate = 0.000377 ; Loss = 1.456972\n",
      "2024-11-28 23:15:01.633000: I runner.py:310] Step = 55200 ; steps/s = 1.64, tokens/s = 43671 (43671 target) ; Learning rate = 0.000376 ; Loss = 1.470902\n",
      "2024-11-28 23:16:02.658000: I runner.py:310] Step = 55300 ; steps/s = 1.64, tokens/s = 43692 (43692 target) ; Learning rate = 0.000376 ; Loss = 1.463455\n",
      "2024-11-28 23:17:03.213000: I runner.py:310] Step = 55400 ; steps/s = 1.65, tokens/s = 43220 (43220 target) ; Learning rate = 0.000376 ; Loss = 1.461049\n",
      "2024-11-28 23:18:04.267000: I runner.py:310] Step = 55500 ; steps/s = 1.64, tokens/s = 43685 (43685 target) ; Learning rate = 0.000375 ; Loss = 1.465340\n",
      "2024-11-28 23:19:04.821000: I runner.py:310] Step = 55600 ; steps/s = 1.65, tokens/s = 43206 (43206 target) ; Learning rate = 0.000375 ; Loss = 1.454983\n",
      "2024-11-28 23:20:05.910000: I runner.py:310] Step = 55700 ; steps/s = 1.64, tokens/s = 43642 (43642 target) ; Learning rate = 0.000375 ; Loss = 1.458145\n",
      "2024-11-28 23:21:06.940000: I runner.py:310] Step = 55800 ; steps/s = 1.64, tokens/s = 43699 (43699 target) ; Learning rate = 0.000374 ; Loss = 1.468474\n",
      "2024-11-28 23:22:07.574000: I runner.py:310] Step = 55900 ; steps/s = 1.65, tokens/s = 43168 (43168 target) ; Learning rate = 0.000374 ; Loss = 1.459834\n",
      "2024-11-28 23:23:08.649000: I runner.py:310] Step = 56000 ; steps/s = 1.64, tokens/s = 43663 (43663 target) ; Learning rate = 0.000374 ; Loss = 1.464756\n",
      "2024-11-28 23:24:09.292000: I runner.py:310] Step = 56100 ; steps/s = 1.65, tokens/s = 43152 (43152 target) ; Learning rate = 0.000373 ; Loss = 1.460110\n",
      "2024-11-28 23:25:10.349000: I runner.py:310] Step = 56200 ; steps/s = 1.64, tokens/s = 43681 (43681 target) ; Learning rate = 0.000373 ; Loss = 1.457754\n",
      "2024-11-28 23:26:11.432000: I runner.py:310] Step = 56300 ; steps/s = 1.64, tokens/s = 43641 (43641 target) ; Learning rate = 0.000373 ; Loss = 1.464156\n",
      "2024-11-28 23:27:12.063000: I runner.py:310] Step = 56400 ; steps/s = 1.65, tokens/s = 43169 (43169 target) ; Learning rate = 0.000372 ; Loss = 1.453145\n",
      "2024-11-28 23:28:13.148000: I runner.py:310] Step = 56500 ; steps/s = 1.64, tokens/s = 43652 (43652 target) ; Learning rate = 0.000372 ; Loss = 1.465585\n",
      "2024-11-28 23:29:14.262000: I runner.py:310] Step = 56600 ; steps/s = 1.64, tokens/s = 43637 (43637 target) ; Learning rate = 0.000372 ; Loss = 1.468855\n",
      "2024-11-28 23:30:14.960000: I runner.py:310] Step = 56700 ; steps/s = 1.65, tokens/s = 43127 (43127 target) ; Learning rate = 0.000371 ; Loss = 1.456817\n",
      "2024-11-28 23:31:16.052000: I runner.py:310] Step = 56800 ; steps/s = 1.64, tokens/s = 43646 (43646 target) ; Learning rate = 0.000371 ; Loss = 1.464795\n",
      "2024-11-28 23:32:16.736000: I runner.py:310] Step = 56900 ; steps/s = 1.65, tokens/s = 43126 (43126 target) ; Learning rate = 0.000371 ; Loss = 1.453452\n",
      "2024-11-28 23:33:17.767000: I runner.py:310] Step = 57000 ; steps/s = 1.64, tokens/s = 43696 (43696 target) ; Learning rate = 0.000370 ; Loss = 1.463661\n",
      "2024-11-28 23:34:18.872000: I runner.py:310] Step = 57100 ; steps/s = 1.64, tokens/s = 43641 (43641 target) ; Learning rate = 0.000370 ; Loss = 1.465131\n",
      "2024-11-28 23:35:19.464000: I runner.py:310] Step = 57200 ; steps/s = 1.65, tokens/s = 43191 (43191 target) ; Learning rate = 0.000370 ; Loss = 1.461690\n",
      "2024-11-28 23:36:20.502000: I runner.py:310] Step = 57300 ; steps/s = 1.64, tokens/s = 43691 (43691 target) ; Learning rate = 0.000369 ; Loss = 1.460878\n",
      "2024-11-28 23:37:21.109000: I runner.py:310] Step = 57400 ; steps/s = 1.65, tokens/s = 43172 (43172 target) ; Learning rate = 0.000369 ; Loss = 1.455585\n",
      "2024-11-28 23:38:22.141000: I runner.py:310] Step = 57500 ; steps/s = 1.64, tokens/s = 43690 (43690 target) ; Learning rate = 0.000369 ; Loss = 1.448539\n",
      "2024-11-28 23:39:23.239000: I runner.py:310] Step = 57600 ; steps/s = 1.64, tokens/s = 43646 (43646 target) ; Learning rate = 0.000368 ; Loss = 1.461331\n",
      "2024-11-28 23:40:23.867000: I runner.py:310] Step = 57700 ; steps/s = 1.65, tokens/s = 43165 (43165 target) ; Learning rate = 0.000368 ; Loss = 1.455537\n",
      "2024-11-28 23:41:24.950000: I runner.py:310] Step = 57800 ; steps/s = 1.64, tokens/s = 43659 (43659 target) ; Learning rate = 0.000368 ; Loss = 1.460193\n",
      "2024-11-28 23:42:26.042000: I runner.py:310] Step = 57900 ; steps/s = 1.64, tokens/s = 43648 (43648 target) ; Learning rate = 0.000367 ; Loss = 1.464778\n",
      "2024-11-28 23:43:26.650000: I runner.py:310] Step = 58000 ; steps/s = 1.65, tokens/s = 43186 (43186 target) ; Learning rate = 0.000367 ; Loss = 1.460668\n",
      "2024-11-28 23:44:27.766000: I runner.py:310] Step = 58100 ; steps/s = 1.64, tokens/s = 43621 (43621 target) ; Learning rate = 0.000367 ; Loss = 1.464101\n",
      "2024-11-28 23:45:28.391000: I runner.py:310] Step = 58200 ; steps/s = 1.65, tokens/s = 43175 (43175 target) ; Learning rate = 0.000366 ; Loss = 1.455062\n",
      "2024-11-28 23:46:29.470000: I runner.py:310] Step = 58300 ; steps/s = 1.64, tokens/s = 43655 (43655 target) ; Learning rate = 0.000366 ; Loss = 1.455941\n",
      "2024-11-28 23:47:30.584000: I runner.py:310] Step = 58400 ; steps/s = 1.64, tokens/s = 43635 (43635 target) ; Learning rate = 0.000366 ; Loss = 1.465936\n",
      "2024-11-28 23:48:31.332000: I runner.py:310] Step = 58500 ; steps/s = 1.65, tokens/s = 43083 (43083 target) ; Learning rate = 0.000365 ; Loss = 1.458433\n",
      "2024-11-28 23:49:32.364000: I runner.py:310] Step = 58600 ; steps/s = 1.64, tokens/s = 43697 (43697 target) ; Learning rate = 0.000365 ; Loss = 1.458695\n",
      "2024-11-28 23:50:33.057000: I runner.py:310] Step = 58700 ; steps/s = 1.65, tokens/s = 43118 (43118 target) ; Learning rate = 0.000365 ; Loss = 1.447665\n",
      "2024-11-28 23:51:34.183000: I runner.py:310] Step = 58800 ; steps/s = 1.64, tokens/s = 43619 (43619 target) ; Learning rate = 0.000365 ; Loss = 1.454724\n",
      "2024-11-28 23:52:35.280000: I runner.py:310] Step = 58900 ; steps/s = 1.64, tokens/s = 43652 (43652 target) ; Learning rate = 0.000364 ; Loss = 1.461175\n",
      "2024-11-28 23:53:35.916000: I runner.py:310] Step = 59000 ; steps/s = 1.65, tokens/s = 43162 (43162 target) ; Learning rate = 0.000364 ; Loss = 1.450841\n",
      "2024-11-28 23:54:36.986000: I runner.py:310] Step = 59100 ; steps/s = 1.64, tokens/s = 43675 (43675 target) ; Learning rate = 0.000364 ; Loss = 1.464582\n",
      "2024-11-28 23:55:38.107000: I runner.py:310] Step = 59200 ; steps/s = 1.64, tokens/s = 43626 (43626 target) ; Learning rate = 0.000363 ; Loss = 1.465702\n",
      "2024-11-28 23:56:38.709000: I runner.py:310] Step = 59300 ; steps/s = 1.65, tokens/s = 43181 (43181 target) ; Learning rate = 0.000363 ; Loss = 1.452204\n",
      "2024-11-28 23:57:39.830000: I runner.py:310] Step = 59400 ; steps/s = 1.64, tokens/s = 43623 (43623 target) ; Learning rate = 0.000363 ; Loss = 1.461936\n",
      "2024-11-28 23:58:40.415000: I runner.py:310] Step = 59500 ; steps/s = 1.65, tokens/s = 43204 (43204 target) ; Learning rate = 0.000362 ; Loss = 1.457855\n",
      "2024-11-28 23:59:41.459000: I runner.py:310] Step = 59600 ; steps/s = 1.64, tokens/s = 43687 (43687 target) ; Learning rate = 0.000362 ; Loss = 1.455520\n",
      "2024-11-29 00:00:42.550000: I runner.py:310] Step = 59700 ; steps/s = 1.64, tokens/s = 43637 (43637 target) ; Learning rate = 0.000362 ; Loss = 1.458471\n",
      "2024-11-29 00:01:43.174000: I runner.py:310] Step = 59800 ; steps/s = 1.65, tokens/s = 43173 (43173 target) ; Learning rate = 0.000361 ; Loss = 1.452808\n",
      "2024-11-29 00:02:44.222000: I runner.py:310] Step = 59900 ; steps/s = 1.64, tokens/s = 43690 (43690 target) ; Learning rate = 0.000361 ; Loss = 1.459356\n",
      "2024-11-29 00:03:44.897000: I runner.py:310] Step = 60000 ; steps/s = 1.65, tokens/s = 43124 (43124 target) ; Learning rate = 0.000361 ; Loss = 1.454197\n",
      "2024-11-29 00:03:47.313000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-60000\n",
      "2024-11-29 00:03:47.313000: I training.py:192] Running evaluation for step 60000\n",
      "2024-11-29 00:04:37.778000: I training.py:192] Evaluation result for step 60000: loss = 0.819265 ; perplexity = 2.268831\n",
      "2024-11-29 00:05:38.758000: I runner.py:310] Step = 60100 ; steps/s = 1.64, tokens/s = 43745 (43745 target) ; Learning rate = 0.000361 ; Loss = 1.461191\n",
      "2024-11-29 00:06:39.841000: I runner.py:310] Step = 60200 ; steps/s = 1.64, tokens/s = 43653 (43653 target) ; Learning rate = 0.000360 ; Loss = 1.460852\n",
      "2024-11-29 00:07:40.536000: I runner.py:310] Step = 60300 ; steps/s = 1.65, tokens/s = 43110 (43110 target) ; Learning rate = 0.000360 ; Loss = 1.452265\n",
      "2024-11-29 00:08:41.657000: I runner.py:310] Step = 60400 ; steps/s = 1.64, tokens/s = 43617 (43617 target) ; Learning rate = 0.000360 ; Loss = 1.456015\n",
      "2024-11-29 00:09:42.773000: I runner.py:310] Step = 60500 ; steps/s = 1.64, tokens/s = 43643 (43643 target) ; Learning rate = 0.000359 ; Loss = 1.461751\n",
      "2024-11-29 00:10:43.433000: I runner.py:310] Step = 60600 ; steps/s = 1.65, tokens/s = 43144 (43144 target) ; Learning rate = 0.000359 ; Loss = 1.455289\n",
      "2024-11-29 00:11:44.571000: I runner.py:310] Step = 60700 ; steps/s = 1.64, tokens/s = 43605 (43605 target) ; Learning rate = 0.000359 ; Loss = 1.456217\n",
      "2024-11-29 00:12:45.181000: I runner.py:310] Step = 60800 ; steps/s = 1.65, tokens/s = 43194 (43194 target) ; Learning rate = 0.000358 ; Loss = 1.452153\n",
      "2024-11-29 00:13:46.306000: I runner.py:310] Step = 60900 ; steps/s = 1.64, tokens/s = 43622 (43622 target) ; Learning rate = 0.000358 ; Loss = 1.454234\n",
      "2024-11-29 00:14:47.424000: I runner.py:310] Step = 61000 ; steps/s = 1.64, tokens/s = 43629 (43629 target) ; Learning rate = 0.000358 ; Loss = 1.460553\n",
      "2024-11-29 00:15:48.081000: I runner.py:310] Step = 61100 ; steps/s = 1.65, tokens/s = 43152 (43152 target) ; Learning rate = 0.000358 ; Loss = 1.454143\n",
      "2024-11-29 00:16:49.155000: I runner.py:310] Step = 61200 ; steps/s = 1.64, tokens/s = 43657 (43657 target) ; Learning rate = 0.000357 ; Loss = 1.461257\n",
      "2024-11-29 00:17:49.794000: I runner.py:310] Step = 61300 ; steps/s = 1.65, tokens/s = 43161 (43161 target) ; Learning rate = 0.000357 ; Loss = 1.456330\n",
      "2024-11-29 00:18:50.914000: I runner.py:310] Step = 61400 ; steps/s = 1.64, tokens/s = 43627 (43627 target) ; Learning rate = 0.000357 ; Loss = 1.453978\n",
      "2024-11-29 00:19:52.060000: I runner.py:310] Step = 61500 ; steps/s = 1.64, tokens/s = 43612 (43612 target) ; Learning rate = 0.000356 ; Loss = 1.452103\n",
      "2024-11-29 00:20:52.700000: I runner.py:310] Step = 61600 ; steps/s = 1.65, tokens/s = 43162 (43162 target) ; Learning rate = 0.000356 ; Loss = 1.452157\n",
      "2024-11-29 00:21:53.752000: I runner.py:310] Step = 61700 ; steps/s = 1.64, tokens/s = 43684 (43684 target) ; Learning rate = 0.000356 ; Loss = 1.459749\n",
      "2024-11-29 00:22:54.829000: I runner.py:310] Step = 61800 ; steps/s = 1.64, tokens/s = 43654 (43654 target) ; Learning rate = 0.000356 ; Loss = 1.459337\n",
      "2024-11-29 00:23:55.387000: I runner.py:310] Step = 61900 ; steps/s = 1.65, tokens/s = 43220 (43220 target) ; Learning rate = 0.000355 ; Loss = 1.456753\n",
      "2024-11-29 00:24:56.513000: I runner.py:310] Step = 62000 ; steps/s = 1.64, tokens/s = 43622 (43622 target) ; Learning rate = 0.000355 ; Loss = 1.460179\n",
      "2024-11-29 00:25:57.234000: I runner.py:310] Step = 62100 ; steps/s = 1.65, tokens/s = 43101 (43101 target) ; Learning rate = 0.000355 ; Loss = 1.451784\n",
      "2024-11-29 00:26:58.335000: I runner.py:310] Step = 62200 ; steps/s = 1.64, tokens/s = 43644 (43644 target) ; Learning rate = 0.000354 ; Loss = 1.452345\n",
      "2024-11-29 00:27:59.459000: I runner.py:310] Step = 62300 ; steps/s = 1.64, tokens/s = 43631 (43631 target) ; Learning rate = 0.000354 ; Loss = 1.459284\n",
      "2024-11-29 00:29:00.132000: I runner.py:310] Step = 62400 ; steps/s = 1.65, tokens/s = 43146 (43146 target) ; Learning rate = 0.000354 ; Loss = 1.453697\n",
      "2024-11-29 00:30:01.231000: I runner.py:310] Step = 62500 ; steps/s = 1.64, tokens/s = 43638 (43638 target) ; Learning rate = 0.000354 ; Loss = 1.455668\n",
      "2024-11-29 00:31:01.841000: I runner.py:310] Step = 62600 ; steps/s = 1.65, tokens/s = 43171 (43171 target) ; Learning rate = 0.000353 ; Loss = 1.452465\n",
      "2024-11-29 00:32:02.939000: I runner.py:310] Step = 62700 ; steps/s = 1.64, tokens/s = 43647 (43647 target) ; Learning rate = 0.000353 ; Loss = 1.453424\n",
      "2024-11-29 00:33:04.007000: I runner.py:310] Step = 62800 ; steps/s = 1.64, tokens/s = 43672 (43672 target) ; Learning rate = 0.000353 ; Loss = 1.454885\n",
      "2024-11-29 00:34:04.707000: I runner.py:310] Step = 62900 ; steps/s = 1.65, tokens/s = 43109 (43109 target) ; Learning rate = 0.000352 ; Loss = 1.458551\n",
      "2024-11-29 00:35:05.837000: I runner.py:310] Step = 63000 ; steps/s = 1.64, tokens/s = 43631 (43631 target) ; Learning rate = 0.000352 ; Loss = 1.457412\n",
      "2024-11-29 00:36:06.994000: I runner.py:310] Step = 63100 ; steps/s = 1.64, tokens/s = 43601 (43601 target) ; Learning rate = 0.000352 ; Loss = 1.457587\n",
      "2024-11-29 00:37:07.636000: I runner.py:310] Step = 63200 ; steps/s = 1.65, tokens/s = 43148 (43148 target) ; Learning rate = 0.000352 ; Loss = 1.455512\n",
      "2024-11-29 00:38:08.747000: I runner.py:310] Step = 63300 ; steps/s = 1.64, tokens/s = 43637 (43637 target) ; Learning rate = 0.000351 ; Loss = 1.457759\n",
      "2024-11-29 00:39:09.406000: I runner.py:310] Step = 63400 ; steps/s = 1.65, tokens/s = 43147 (43147 target) ; Learning rate = 0.000351 ; Loss = 1.449503\n",
      "2024-11-29 00:40:10.483000: I runner.py:310] Step = 63500 ; steps/s = 1.64, tokens/s = 43649 (43649 target) ; Learning rate = 0.000351 ; Loss = 1.452962\n",
      "2024-11-29 00:41:11.643000: I runner.py:310] Step = 63600 ; steps/s = 1.64, tokens/s = 43609 (43609 target) ; Learning rate = 0.000350 ; Loss = 1.461336\n",
      "2024-11-29 00:42:12.226000: I runner.py:310] Step = 63700 ; steps/s = 1.65, tokens/s = 43196 (43196 target) ; Learning rate = 0.000350 ; Loss = 1.455079\n",
      "2024-11-29 00:43:13.357000: I runner.py:310] Step = 63800 ; steps/s = 1.64, tokens/s = 43619 (43619 target) ; Learning rate = 0.000350 ; Loss = 1.454076\n",
      "2024-11-29 00:44:13.961000: I runner.py:310] Step = 63900 ; steps/s = 1.65, tokens/s = 43188 (43188 target) ; Learning rate = 0.000350 ; Loss = 1.450763\n",
      "2024-11-29 00:45:15.070000: I runner.py:310] Step = 64000 ; steps/s = 1.64, tokens/s = 43643 (43643 target) ; Learning rate = 0.000349 ; Loss = 1.450695\n",
      "2024-11-29 00:46:16.126000: I runner.py:310] Step = 64100 ; steps/s = 1.64, tokens/s = 43675 (43675 target) ; Learning rate = 0.000349 ; Loss = 1.465939\n",
      "2024-11-29 00:47:16.774000: I runner.py:310] Step = 64200 ; steps/s = 1.65, tokens/s = 43150 (43150 target) ; Learning rate = 0.000349 ; Loss = 1.450639\n",
      "2024-11-29 00:48:17.848000: I runner.py:310] Step = 64300 ; steps/s = 1.64, tokens/s = 43655 (43655 target) ; Learning rate = 0.000349 ; Loss = 1.459602\n",
      "2024-11-29 00:49:18.942000: I runner.py:310] Step = 64400 ; steps/s = 1.64, tokens/s = 43647 (43647 target) ; Learning rate = 0.000348 ; Loss = 1.453892\n",
      "2024-11-29 00:50:19.588000: I runner.py:310] Step = 64500 ; steps/s = 1.65, tokens/s = 43152 (43152 target) ; Learning rate = 0.000348 ; Loss = 1.452075\n",
      "2024-11-29 00:51:20.640000: I runner.py:310] Step = 64600 ; steps/s = 1.64, tokens/s = 43674 (43674 target) ; Learning rate = 0.000348 ; Loss = 1.456272\n",
      "2024-11-29 00:52:21.214000: I runner.py:310] Step = 64700 ; steps/s = 1.65, tokens/s = 43211 (43211 target) ; Learning rate = 0.000347 ; Loss = 1.443799\n",
      "2024-11-29 00:53:22.335000: I runner.py:310] Step = 64800 ; steps/s = 1.64, tokens/s = 43624 (43624 target) ; Learning rate = 0.000347 ; Loss = 1.452307\n",
      "2024-11-29 00:54:23.449000: I runner.py:310] Step = 64900 ; steps/s = 1.64, tokens/s = 43641 (43641 target) ; Learning rate = 0.000347 ; Loss = 1.457670\n",
      "2024-11-29 00:55:24.091000: I runner.py:310] Step = 65000 ; steps/s = 1.65, tokens/s = 43153 (43153 target) ; Learning rate = 0.000347 ; Loss = 1.450286\n",
      "2024-11-29 00:55:24.092000: I training.py:192] Running evaluation for step 65000\n",
      "2024-11-29 00:56:13.290000: I training.py:192] Evaluation result for step 65000: loss = 0.823200 ; perplexity = 2.277776\n",
      "2024-11-29 00:57:14.266000: I runner.py:310] Step = 65100 ; steps/s = 1.64, tokens/s = 43741 (43741 target) ; Learning rate = 0.000346 ; Loss = 1.458713\n",
      "2024-11-29 00:58:14.817000: I runner.py:310] Step = 65200 ; steps/s = 1.65, tokens/s = 43217 (43217 target) ; Learning rate = 0.000346 ; Loss = 1.449091\n",
      "2024-11-29 00:59:15.902000: I runner.py:310] Step = 65300 ; steps/s = 1.64, tokens/s = 43654 (43654 target) ; Learning rate = 0.000346 ; Loss = 1.449924\n",
      "2024-11-29 01:00:17.005000: I runner.py:310] Step = 65400 ; steps/s = 1.64, tokens/s = 43633 (43633 target) ; Learning rate = 0.000346 ; Loss = 1.456423\n",
      "2024-11-29 01:01:17.588000: I runner.py:310] Step = 65500 ; steps/s = 1.65, tokens/s = 43201 (43201 target) ; Learning rate = 0.000345 ; Loss = 1.450246\n",
      "2024-11-29 01:02:18.640000: I runner.py:310] Step = 65600 ; steps/s = 1.64, tokens/s = 43675 (43675 target) ; Learning rate = 0.000345 ; Loss = 1.452569\n",
      "2024-11-29 01:03:19.652000: I runner.py:310] Step = 65700 ; steps/s = 1.64, tokens/s = 43717 (43717 target) ; Learning rate = 0.000345 ; Loss = 1.456639\n",
      "2024-11-29 01:04:20.310000: I runner.py:310] Step = 65800 ; steps/s = 1.65, tokens/s = 43145 (43145 target) ; Learning rate = 0.000345 ; Loss = 1.452515\n",
      "2024-11-29 01:05:21.380000: I runner.py:310] Step = 65900 ; steps/s = 1.64, tokens/s = 43669 (43669 target) ; Learning rate = 0.000344 ; Loss = 1.453572\n",
      "2024-11-29 01:06:22.000000: I runner.py:310] Step = 66000 ; steps/s = 1.65, tokens/s = 43169 (43169 target) ; Learning rate = 0.000344 ; Loss = 1.445948\n",
      "2024-11-29 01:07:23.063000: I runner.py:310] Step = 66100 ; steps/s = 1.64, tokens/s = 43672 (43672 target) ; Learning rate = 0.000344 ; Loss = 1.459799\n",
      "2024-11-29 01:08:24.199000: I runner.py:310] Step = 66200 ; steps/s = 1.64, tokens/s = 43625 (43625 target) ; Learning rate = 0.000344 ; Loss = 1.455129\n",
      "2024-11-29 01:09:24.781000: I runner.py:310] Step = 66300 ; steps/s = 1.65, tokens/s = 43205 (43205 target) ; Learning rate = 0.000343 ; Loss = 1.458403\n",
      "2024-11-29 01:10:25.828000: I runner.py:310] Step = 66400 ; steps/s = 1.64, tokens/s = 43675 (43675 target) ; Learning rate = 0.000343 ; Loss = 1.454972\n",
      "2024-11-29 01:11:26.442000: I runner.py:310] Step = 66500 ; steps/s = 1.65, tokens/s = 43173 (43173 target) ; Learning rate = 0.000343 ; Loss = 1.448570\n",
      "2024-11-29 01:12:27.533000: I runner.py:310] Step = 66600 ; steps/s = 1.64, tokens/s = 43656 (43656 target) ; Learning rate = 0.000342 ; Loss = 1.456977\n",
      "2024-11-29 01:13:28.618000: I runner.py:310] Step = 66700 ; steps/s = 1.64, tokens/s = 43652 (43652 target) ; Learning rate = 0.000342 ; Loss = 1.456620\n",
      "2024-11-29 01:14:29.214000: I runner.py:310] Step = 66800 ; steps/s = 1.65, tokens/s = 43190 (43190 target) ; Learning rate = 0.000342 ; Loss = 1.450695\n",
      "2024-11-29 01:15:30.235000: I runner.py:310] Step = 66900 ; steps/s = 1.64, tokens/s = 43694 (43694 target) ; Learning rate = 0.000342 ; Loss = 1.448650\n",
      "2024-11-29 01:16:31.320000: I runner.py:310] Step = 67000 ; steps/s = 1.64, tokens/s = 43662 (43662 target) ; Learning rate = 0.000341 ; Loss = 1.453444\n",
      "2024-11-29 01:17:31.951000: I runner.py:310] Step = 67100 ; steps/s = 1.65, tokens/s = 43166 (43166 target) ; Learning rate = 0.000341 ; Loss = 1.449888\n",
      "2024-11-29 01:18:33.019000: I runner.py:310] Step = 67200 ; steps/s = 1.64, tokens/s = 43661 (43661 target) ; Learning rate = 0.000341 ; Loss = 1.453511\n",
      "2024-11-29 01:19:33.575000: I runner.py:310] Step = 67300 ; steps/s = 1.65, tokens/s = 43228 (43228 target) ; Learning rate = 0.000341 ; Loss = 1.450903\n",
      "2024-11-29 01:20:34.590000: I runner.py:310] Step = 67400 ; steps/s = 1.64, tokens/s = 43711 (43711 target) ; Learning rate = 0.000340 ; Loss = 1.453956\n",
      "2024-11-29 01:21:35.651000: I runner.py:310] Step = 67500 ; steps/s = 1.64, tokens/s = 43664 (43664 target) ; Learning rate = 0.000340 ; Loss = 1.449240\n",
      "2024-11-29 01:22:36.221000: I runner.py:310] Step = 67600 ; steps/s = 1.65, tokens/s = 43209 (43209 target) ; Learning rate = 0.000340 ; Loss = 1.456255\n",
      "2024-11-29 01:23:37.214000: I runner.py:310] Step = 67700 ; steps/s = 1.64, tokens/s = 43724 (43724 target) ; Learning rate = 0.000340 ; Loss = 1.455662\n",
      "2024-11-29 01:24:37.812000: I runner.py:310] Step = 67800 ; steps/s = 1.65, tokens/s = 43182 (43182 target) ; Learning rate = 0.000339 ; Loss = 1.449570\n",
      "2024-11-29 01:25:38.890000: I runner.py:310] Step = 67900 ; steps/s = 1.64, tokens/s = 43652 (43652 target) ; Learning rate = 0.000339 ; Loss = 1.449525\n",
      "2024-11-29 01:26:39.867000: I runner.py:310] Step = 68000 ; steps/s = 1.64, tokens/s = 43733 (43733 target) ; Learning rate = 0.000339 ; Loss = 1.454133\n",
      "2024-11-29 01:27:40.487000: I runner.py:310] Step = 68100 ; steps/s = 1.65, tokens/s = 43183 (43183 target) ; Learning rate = 0.000339 ; Loss = 1.451878\n",
      "2024-11-29 01:28:41.515000: I runner.py:310] Step = 68200 ; steps/s = 1.64, tokens/s = 43691 (43691 target) ; Learning rate = 0.000338 ; Loss = 1.451755\n",
      "2024-11-29 01:29:42.521000: I runner.py:310] Step = 68300 ; steps/s = 1.64, tokens/s = 43708 (43708 target) ; Learning rate = 0.000338 ; Loss = 1.449645\n",
      "2024-11-29 01:30:43.067000: I runner.py:310] Step = 68400 ; steps/s = 1.65, tokens/s = 43217 (43217 target) ; Learning rate = 0.000338 ; Loss = 1.449670\n",
      "2024-11-29 01:31:44.133000: I runner.py:310] Step = 68500 ; steps/s = 1.64, tokens/s = 43671 (43671 target) ; Learning rate = 0.000338 ; Loss = 1.450397\n",
      "2024-11-29 01:32:44.733000: I runner.py:310] Step = 68600 ; steps/s = 1.65, tokens/s = 43198 (43198 target) ; Learning rate = 0.000337 ; Loss = 1.450071\n",
      "2024-11-29 01:33:45.821000: I runner.py:310] Step = 68700 ; steps/s = 1.64, tokens/s = 43653 (43653 target) ; Learning rate = 0.000337 ; Loss = 1.452712\n",
      "2024-11-29 01:34:46.842000: I runner.py:310] Step = 68800 ; steps/s = 1.64, tokens/s = 43695 (43695 target) ; Learning rate = 0.000337 ; Loss = 1.452303\n",
      "2024-11-29 01:35:47.420000: I runner.py:310] Step = 68900 ; steps/s = 1.65, tokens/s = 43192 (43192 target) ; Learning rate = 0.000337 ; Loss = 1.453214\n",
      "2024-11-29 01:36:48.549000: I runner.py:310] Step = 69000 ; steps/s = 1.64, tokens/s = 43620 (43620 target) ; Learning rate = 0.000336 ; Loss = 1.454412\n",
      "2024-11-29 01:37:49.085000: I runner.py:310] Step = 69100 ; steps/s = 1.65, tokens/s = 43237 (43237 target) ; Learning rate = 0.000336 ; Loss = 1.447982\n",
      "2024-11-29 01:38:50.168000: I runner.py:310] Step = 69200 ; steps/s = 1.64, tokens/s = 43661 (43661 target) ; Learning rate = 0.000336 ; Loss = 1.453131\n",
      "2024-11-29 01:39:51.254000: I runner.py:310] Step = 69300 ; steps/s = 1.64, tokens/s = 43654 (43654 target) ; Learning rate = 0.000336 ; Loss = 1.456760\n",
      "2024-11-29 01:40:51.851000: I runner.py:310] Step = 69400 ; steps/s = 1.65, tokens/s = 43193 (43193 target) ; Learning rate = 0.000336 ; Loss = 1.455774\n",
      "2024-11-29 01:41:52.954000: I runner.py:310] Step = 69500 ; steps/s = 1.64, tokens/s = 43641 (43641 target) ; Learning rate = 0.000335 ; Loss = 1.454017\n",
      "2024-11-29 01:42:53.992000: I runner.py:310] Step = 69600 ; steps/s = 1.64, tokens/s = 43684 (43684 target) ; Learning rate = 0.000335 ; Loss = 1.455104\n",
      "2024-11-29 01:43:54.630000: I runner.py:310] Step = 69700 ; steps/s = 1.65, tokens/s = 43159 (43159 target) ; Learning rate = 0.000335 ; Loss = 1.456824\n",
      "2024-11-29 01:44:55.715000: I runner.py:310] Step = 69800 ; steps/s = 1.64, tokens/s = 43645 (43645 target) ; Learning rate = 0.000335 ; Loss = 1.454133\n",
      "2024-11-29 01:45:56.270000: I runner.py:310] Step = 69900 ; steps/s = 1.65, tokens/s = 43231 (43231 target) ; Learning rate = 0.000334 ; Loss = 1.451646\n",
      "2024-11-29 01:46:57.350000: I runner.py:310] Step = 70000 ; steps/s = 1.64, tokens/s = 43652 (43652 target) ; Learning rate = 0.000334 ; Loss = 1.448066\n",
      "2024-11-29 01:46:59.352000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-70000\n",
      "2024-11-29 01:46:59.352000: I training.py:192] Running evaluation for step 70000\n",
      "2024-11-29 01:47:48.776000: I training.py:192] Evaluation result for step 70000: loss = 0.827714 ; perplexity = 2.288081\n",
      "2024-11-29 01:48:49.789000: I runner.py:310] Step = 70100 ; steps/s = 1.64, tokens/s = 43722 (43722 target) ; Learning rate = 0.000334 ; Loss = 1.455256\n",
      "2024-11-29 01:49:50.438000: I runner.py:310] Step = 70200 ; steps/s = 1.65, tokens/s = 43153 (43153 target) ; Learning rate = 0.000334 ; Loss = 1.450941\n",
      "2024-11-29 01:50:51.551000: I runner.py:310] Step = 70300 ; steps/s = 1.64, tokens/s = 43629 (43629 target) ; Learning rate = 0.000333 ; Loss = 1.450278\n",
      "2024-11-29 01:51:52.204000: I runner.py:310] Step = 70400 ; steps/s = 1.65, tokens/s = 43149 (43149 target) ; Learning rate = 0.000333 ; Loss = 1.447070\n",
      "2024-11-29 01:52:53.241000: I runner.py:310] Step = 70500 ; steps/s = 1.64, tokens/s = 43689 (43689 target) ; Learning rate = 0.000333 ; Loss = 1.447708\n",
      "2024-11-29 01:53:54.365000: I runner.py:310] Step = 70600 ; steps/s = 1.64, tokens/s = 43628 (43628 target) ; Learning rate = 0.000333 ; Loss = 1.457954\n",
      "2024-11-29 01:54:54.900000: I runner.py:310] Step = 70700 ; steps/s = 1.65, tokens/s = 43227 (43227 target) ; Learning rate = 0.000332 ; Loss = 1.449319\n",
      "2024-11-29 01:55:56.031000: I runner.py:310] Step = 70800 ; steps/s = 1.64, tokens/s = 43629 (43629 target) ; Learning rate = 0.000332 ; Loss = 1.452201\n",
      "2024-11-29 01:56:56.972000: I runner.py:310] Step = 70900 ; steps/s = 1.64, tokens/s = 43538 (43538 target) ; Learning rate = 0.000332 ; Loss = 1.474703\n",
      "2024-11-29 01:57:57.683000: I runner.py:310] Step = 71000 ; steps/s = 1.65, tokens/s = 43327 (43327 target) ; Learning rate = 0.000332 ; Loss = 1.449396\n",
      "2024-11-29 01:58:58.780000: I runner.py:310] Step = 71100 ; steps/s = 1.64, tokens/s = 43645 (43645 target) ; Learning rate = 0.000331 ; Loss = 1.449322\n",
      "2024-11-29 01:59:59.431000: I runner.py:310] Step = 71200 ; steps/s = 1.65, tokens/s = 43149 (43149 target) ; Learning rate = 0.000331 ; Loss = 1.445649\n",
      "2024-11-29 02:01:00.462000: I runner.py:310] Step = 71300 ; steps/s = 1.64, tokens/s = 43691 (43691 target) ; Learning rate = 0.000331 ; Loss = 1.452894\n",
      "2024-11-29 02:02:01.523000: I runner.py:310] Step = 71400 ; steps/s = 1.64, tokens/s = 43673 (43673 target) ; Learning rate = 0.000331 ; Loss = 1.454566\n",
      "2024-11-29 02:03:02.209000: I runner.py:310] Step = 71500 ; steps/s = 1.65, tokens/s = 43126 (43126 target) ; Learning rate = 0.000331 ; Loss = 1.453320\n",
      "2024-11-29 02:04:03.247000: I runner.py:310] Step = 71600 ; steps/s = 1.64, tokens/s = 43688 (43688 target) ; Learning rate = 0.000330 ; Loss = 1.452706\n",
      "2024-11-29 02:05:03.870000: I runner.py:310] Step = 71700 ; steps/s = 1.65, tokens/s = 43165 (43165 target) ; Learning rate = 0.000330 ; Loss = 1.445540\n",
      "2024-11-29 02:06:05.000000: I runner.py:310] Step = 71800 ; steps/s = 1.64, tokens/s = 43621 (43621 target) ; Learning rate = 0.000330 ; Loss = 1.454533\n",
      "2024-11-29 02:07:06.045000: I runner.py:310] Step = 71900 ; steps/s = 1.64, tokens/s = 43683 (43683 target) ; Learning rate = 0.000330 ; Loss = 1.448630\n",
      "2024-11-29 02:08:06.643000: I runner.py:310] Step = 72000 ; steps/s = 1.65, tokens/s = 43191 (43191 target) ; Learning rate = 0.000329 ; Loss = 1.442754\n",
      "2024-11-29 02:09:07.766000: I runner.py:310] Step = 72100 ; steps/s = 1.64, tokens/s = 43627 (43627 target) ; Learning rate = 0.000329 ; Loss = 1.451485\n",
      "2024-11-29 02:10:08.554000: I runner.py:310] Step = 72200 ; steps/s = 1.65, tokens/s = 43288 (43288 target) ; Learning rate = 0.000329 ; Loss = 1.475410\n",
      "2024-11-29 02:11:09.492000: I runner.py:310] Step = 72300 ; steps/s = 1.64, tokens/s = 43531 (43531 target) ; Learning rate = 0.000329 ; Loss = 1.456866\n",
      "2024-11-29 02:12:10.662000: I runner.py:310] Step = 72400 ; steps/s = 1.63, tokens/s = 43594 (43594 target) ; Learning rate = 0.000328 ; Loss = 1.450832\n",
      "2024-11-29 02:13:11.271000: I runner.py:310] Step = 72500 ; steps/s = 1.65, tokens/s = 43172 (43172 target) ; Learning rate = 0.000328 ; Loss = 1.450590\n",
      "2024-11-29 02:14:12.336000: I runner.py:310] Step = 72600 ; steps/s = 1.64, tokens/s = 43673 (43673 target) ; Learning rate = 0.000328 ; Loss = 1.453088\n",
      "2024-11-29 02:15:13.429000: I runner.py:310] Step = 72700 ; steps/s = 1.64, tokens/s = 43644 (43644 target) ; Learning rate = 0.000328 ; Loss = 1.459034\n",
      "2024-11-29 02:16:14.009000: I runner.py:310] Step = 72800 ; steps/s = 1.65, tokens/s = 43201 (43201 target) ; Learning rate = 0.000328 ; Loss = 1.450970\n",
      "2024-11-29 02:17:15.113000: I runner.py:310] Step = 72900 ; steps/s = 1.64, tokens/s = 43638 (43638 target) ; Learning rate = 0.000327 ; Loss = 1.450235\n",
      "2024-11-29 02:18:15.783000: I runner.py:310] Step = 73000 ; steps/s = 1.65, tokens/s = 43145 (43145 target) ; Learning rate = 0.000327 ; Loss = 1.444653\n",
      "2024-11-29 02:19:16.852000: I runner.py:310] Step = 73100 ; steps/s = 1.64, tokens/s = 43667 (43667 target) ; Learning rate = 0.000327 ; Loss = 1.450915\n",
      "2024-11-29 02:20:17.964000: I runner.py:310] Step = 73200 ; steps/s = 1.64, tokens/s = 43633 (43633 target) ; Learning rate = 0.000327 ; Loss = 1.449768\n",
      "2024-11-29 02:21:18.550000: I runner.py:310] Step = 73300 ; steps/s = 1.65, tokens/s = 43193 (43193 target) ; Learning rate = 0.000326 ; Loss = 1.452224\n",
      "2024-11-29 02:22:19.650000: I runner.py:310] Step = 73400 ; steps/s = 1.64, tokens/s = 43643 (43643 target) ; Learning rate = 0.000326 ; Loss = 1.456152\n",
      "2024-11-29 02:23:20.292000: I runner.py:310] Step = 73500 ; steps/s = 1.65, tokens/s = 43153 (43153 target) ; Learning rate = 0.000326 ; Loss = 1.440943\n",
      "2024-11-29 02:24:21.359000: I runner.py:310] Step = 73600 ; steps/s = 1.64, tokens/s = 43678 (43678 target) ; Learning rate = 0.000326 ; Loss = 1.448434\n",
      "2024-11-29 02:25:22.447000: I runner.py:310] Step = 73700 ; steps/s = 1.64, tokens/s = 43647 (43647 target) ; Learning rate = 0.000326 ; Loss = 1.449839\n",
      "2024-11-29 02:26:23.102000: I runner.py:310] Step = 73800 ; steps/s = 1.65, tokens/s = 43143 (43143 target) ; Learning rate = 0.000325 ; Loss = 1.450510\n",
      "2024-11-29 02:27:24.161000: I runner.py:310] Step = 73900 ; steps/s = 1.64, tokens/s = 43680 (43680 target) ; Learning rate = 0.000325 ; Loss = 1.451476\n",
      "2024-11-29 02:28:25.241000: I runner.py:310] Step = 74000 ; steps/s = 1.64, tokens/s = 43657 (43657 target) ; Learning rate = 0.000325 ; Loss = 1.449351\n",
      "2024-11-29 02:29:25.901000: I runner.py:310] Step = 74100 ; steps/s = 1.65, tokens/s = 43147 (43147 target) ; Learning rate = 0.000325 ; Loss = 1.448050\n",
      "2024-11-29 02:30:26.987000: I runner.py:310] Step = 74200 ; steps/s = 1.64, tokens/s = 43642 (43642 target) ; Learning rate = 0.000324 ; Loss = 1.452800\n",
      "2024-11-29 02:31:27.627000: I runner.py:310] Step = 74300 ; steps/s = 1.65, tokens/s = 43166 (43166 target) ; Learning rate = 0.000324 ; Loss = 1.447716\n",
      "2024-11-29 02:32:28.723000: I runner.py:310] Step = 74400 ; steps/s = 1.64, tokens/s = 43656 (43656 target) ; Learning rate = 0.000324 ; Loss = 1.449342\n",
      "2024-11-29 02:33:29.760000: I runner.py:310] Step = 74500 ; steps/s = 1.64, tokens/s = 43684 (43684 target) ; Learning rate = 0.000324 ; Loss = 1.445752\n",
      "2024-11-29 02:34:30.378000: I runner.py:310] Step = 74600 ; steps/s = 1.65, tokens/s = 43161 (43161 target) ; Learning rate = 0.000324 ; Loss = 1.446670\n",
      "2024-11-29 02:35:31.423000: I runner.py:310] Step = 74700 ; steps/s = 1.64, tokens/s = 43679 (43679 target) ; Learning rate = 0.000323 ; Loss = 1.450160\n",
      "2024-11-29 02:36:32.108000: I runner.py:310] Step = 74800 ; steps/s = 1.65, tokens/s = 43132 (43132 target) ; Learning rate = 0.000323 ; Loss = 1.443739\n",
      "2024-11-29 02:37:33.158000: I runner.py:310] Step = 74900 ; steps/s = 1.64, tokens/s = 43691 (43691 target) ; Learning rate = 0.000323 ; Loss = 1.445967\n",
      "2024-11-29 02:38:34.233000: I runner.py:310] Step = 75000 ; steps/s = 1.64, tokens/s = 43658 (43658 target) ; Learning rate = 0.000323 ; Loss = 1.450648\n",
      "2024-11-29 02:38:34.234000: I training.py:192] Running evaluation for step 75000\n",
      "2024-11-29 02:39:23.442000: I training.py:192] Evaluation result for step 75000: loss = 0.833135 ; perplexity = 2.300520\n",
      "2024-11-29 02:40:24.062000: I runner.py:310] Step = 75100 ; steps/s = 1.65, tokens/s = 43170 (43170 target) ; Learning rate = 0.000323 ; Loss = 1.444442\n",
      "2024-11-29 02:41:25.207000: I runner.py:310] Step = 75200 ; steps/s = 1.64, tokens/s = 43617 (43617 target) ; Learning rate = 0.000322 ; Loss = 1.450036\n",
      "2024-11-29 02:42:26.323000: I runner.py:310] Step = 75300 ; steps/s = 1.64, tokens/s = 43634 (43634 target) ; Learning rate = 0.000322 ; Loss = 1.451839\n",
      "2024-11-29 02:43:27.016000: I runner.py:310] Step = 75400 ; steps/s = 1.65, tokens/s = 43130 (43130 target) ; Learning rate = 0.000322 ; Loss = 1.446973\n",
      "2024-11-29 02:44:28.167000: I runner.py:310] Step = 75500 ; steps/s = 1.64, tokens/s = 43598 (43598 target) ; Learning rate = 0.000322 ; Loss = 1.448973\n",
      "2024-11-29 02:45:28.887000: I runner.py:310] Step = 75600 ; steps/s = 1.65, tokens/s = 43097 (43097 target) ; Learning rate = 0.000321 ; Loss = 1.443705\n",
      "2024-11-29 02:46:30.015000: I runner.py:310] Step = 75700 ; steps/s = 1.64, tokens/s = 43636 (43636 target) ; Learning rate = 0.000321 ; Loss = 1.452376\n",
      "2024-11-29 02:47:31.162000: I runner.py:310] Step = 75800 ; steps/s = 1.64, tokens/s = 43612 (43612 target) ; Learning rate = 0.000321 ; Loss = 1.449563\n",
      "2024-11-29 02:48:31.864000: I runner.py:310] Step = 75900 ; steps/s = 1.65, tokens/s = 43102 (43102 target) ; Learning rate = 0.000321 ; Loss = 1.449254\n",
      "2024-11-29 02:49:33.033000: I runner.py:310] Step = 76000 ; steps/s = 1.64, tokens/s = 43591 (43591 target) ; Learning rate = 0.000321 ; Loss = 1.450090\n",
      "2024-11-29 02:50:33.984000: I runner.py:310] Step = 76100 ; steps/s = 1.64, tokens/s = 42939 (42939 target) ; Learning rate = 0.000320 ; Loss = 1.444585\n",
      "2024-11-29 02:51:35.122000: I runner.py:310] Step = 76200 ; steps/s = 1.64, tokens/s = 43626 (43626 target) ; Learning rate = 0.000320 ; Loss = 1.451137\n",
      "2024-11-29 02:52:36.273000: I runner.py:310] Step = 76300 ; steps/s = 1.64, tokens/s = 43600 (43600 target) ; Learning rate = 0.000320 ; Loss = 1.450892\n",
      "2024-11-29 02:53:36.954000: I runner.py:310] Step = 76400 ; steps/s = 1.65, tokens/s = 43124 (43124 target) ; Learning rate = 0.000320 ; Loss = 1.444735\n",
      "2024-11-29 02:54:38.119000: I runner.py:310] Step = 76500 ; steps/s = 1.64, tokens/s = 43598 (43598 target) ; Learning rate = 0.000320 ; Loss = 1.442716\n",
      "2024-11-29 02:55:39.312000: I runner.py:310] Step = 76600 ; steps/s = 1.63, tokens/s = 43573 (43573 target) ; Learning rate = 0.000319 ; Loss = 1.451177\n",
      "2024-11-29 02:56:39.996000: I runner.py:310] Step = 76700 ; steps/s = 1.65, tokens/s = 43128 (43128 target) ; Learning rate = 0.000319 ; Loss = 1.447041\n",
      "2024-11-29 02:57:41.104000: I runner.py:310] Step = 76800 ; steps/s = 1.64, tokens/s = 43647 (43647 target) ; Learning rate = 0.000319 ; Loss = 1.450424\n",
      "2024-11-29 02:58:41.887000: I runner.py:310] Step = 76900 ; steps/s = 1.65, tokens/s = 43047 (43047 target) ; Learning rate = 0.000319 ; Loss = 1.446792\n",
      "2024-11-29 02:59:42.990000: I runner.py:310] Step = 77000 ; steps/s = 1.64, tokens/s = 43648 (43648 target) ; Learning rate = 0.000319 ; Loss = 1.447625\n",
      "2024-11-29 03:00:44.207000: I runner.py:310] Step = 77100 ; steps/s = 1.63, tokens/s = 43562 (43562 target) ; Learning rate = 0.000318 ; Loss = 1.447940\n",
      "2024-11-29 03:01:44.956000: I runner.py:310] Step = 77200 ; steps/s = 1.65, tokens/s = 43076 (43076 target) ; Learning rate = 0.000318 ; Loss = 1.446575\n",
      "2024-11-29 03:02:46.069000: I runner.py:310] Step = 77300 ; steps/s = 1.64, tokens/s = 43637 (43637 target) ; Learning rate = 0.000318 ; Loss = 1.447194\n",
      "2024-11-29 03:03:46.763000: I runner.py:310] Step = 77400 ; steps/s = 1.65, tokens/s = 43109 (43109 target) ; Learning rate = 0.000318 ; Loss = 1.446607\n",
      "2024-11-29 03:04:47.854000: I runner.py:310] Step = 77500 ; steps/s = 1.64, tokens/s = 43649 (43649 target) ; Learning rate = 0.000317 ; Loss = 1.441695\n",
      "2024-11-29 03:05:48.946000: I runner.py:310] Step = 77600 ; steps/s = 1.64, tokens/s = 43655 (43655 target) ; Learning rate = 0.000317 ; Loss = 1.448072\n",
      "2024-11-29 03:06:49.614000: I runner.py:310] Step = 77700 ; steps/s = 1.65, tokens/s = 43146 (43146 target) ; Learning rate = 0.000317 ; Loss = 1.449595\n",
      "2024-11-29 03:07:50.773000: I runner.py:310] Step = 77800 ; steps/s = 1.64, tokens/s = 43602 (43602 target) ; Learning rate = 0.000317 ; Loss = 1.449374\n",
      "2024-11-29 03:08:51.923000: I runner.py:310] Step = 77900 ; steps/s = 1.64, tokens/s = 43607 (43607 target) ; Learning rate = 0.000317 ; Loss = 1.447392\n",
      "2024-11-29 03:09:52.639000: I runner.py:310] Step = 78000 ; steps/s = 1.65, tokens/s = 43090 (43090 target) ; Learning rate = 0.000316 ; Loss = 1.449117\n",
      "2024-11-29 03:10:53.777000: I runner.py:310] Step = 78100 ; steps/s = 1.64, tokens/s = 43622 (43622 target) ; Learning rate = 0.000316 ; Loss = 1.449303\n",
      "2024-11-29 03:11:54.486000: I runner.py:310] Step = 78200 ; steps/s = 1.65, tokens/s = 43108 (43108 target) ; Learning rate = 0.000316 ; Loss = 1.443704\n",
      "2024-11-29 03:12:55.615000: I runner.py:310] Step = 78300 ; steps/s = 1.64, tokens/s = 43638 (43638 target) ; Learning rate = 0.000316 ; Loss = 1.448284\n",
      "2024-11-29 03:13:56.769000: I runner.py:310] Step = 78400 ; steps/s = 1.64, tokens/s = 43600 (43600 target) ; Learning rate = 0.000316 ; Loss = 1.450355\n",
      "2024-11-29 03:14:57.524000: I runner.py:310] Step = 78500 ; steps/s = 1.65, tokens/s = 43065 (43065 target) ; Learning rate = 0.000315 ; Loss = 1.448474\n",
      "2024-11-29 03:15:58.599000: I runner.py:310] Step = 78600 ; steps/s = 1.64, tokens/s = 43663 (43663 target) ; Learning rate = 0.000315 ; Loss = 1.445725\n",
      "2024-11-29 03:16:59.315000: I runner.py:310] Step = 78700 ; steps/s = 1.65, tokens/s = 43110 (43110 target) ; Learning rate = 0.000315 ; Loss = 1.443125\n",
      "2024-11-29 03:18:00.491000: I runner.py:310] Step = 78800 ; steps/s = 1.63, tokens/s = 43597 (43597 target) ; Learning rate = 0.000315 ; Loss = 1.448817\n",
      "2024-11-29 03:19:01.586000: I runner.py:310] Step = 78900 ; steps/s = 1.64, tokens/s = 43643 (43643 target) ; Learning rate = 0.000315 ; Loss = 1.449977\n",
      "2024-11-29 03:20:02.228000: I runner.py:310] Step = 79000 ; steps/s = 1.65, tokens/s = 43150 (43150 target) ; Learning rate = 0.000314 ; Loss = 1.447485\n",
      "2024-11-29 03:21:03.338000: I runner.py:310] Step = 79100 ; steps/s = 1.64, tokens/s = 43634 (43634 target) ; Learning rate = 0.000314 ; Loss = 1.452441\n",
      "2024-11-29 03:22:04.501000: I runner.py:310] Step = 79200 ; steps/s = 1.64, tokens/s = 43602 (43602 target) ; Learning rate = 0.000314 ; Loss = 1.452592\n",
      "2024-11-29 03:23:05.122000: I runner.py:310] Step = 79300 ; steps/s = 1.65, tokens/s = 43178 (43178 target) ; Learning rate = 0.000314 ; Loss = 1.443911\n",
      "2024-11-29 03:24:06.225000: I runner.py:310] Step = 79400 ; steps/s = 1.64, tokens/s = 43642 (43642 target) ; Learning rate = 0.000314 ; Loss = 1.449904\n",
      "2024-11-29 03:25:06.977000: I runner.py:310] Step = 79500 ; steps/s = 1.65, tokens/s = 43073 (43073 target) ; Learning rate = 0.000313 ; Loss = 1.440981\n",
      "2024-11-29 03:26:08.134000: I runner.py:310] Step = 79600 ; steps/s = 1.64, tokens/s = 43604 (43604 target) ; Learning rate = 0.000313 ; Loss = 1.448411\n",
      "2024-11-29 03:27:09.296000: I runner.py:310] Step = 79700 ; steps/s = 1.64, tokens/s = 43602 (43602 target) ; Learning rate = 0.000313 ; Loss = 1.449819\n",
      "2024-11-29 03:28:09.980000: I runner.py:310] Step = 79800 ; steps/s = 1.65, tokens/s = 43113 (43113 target) ; Learning rate = 0.000313 ; Loss = 1.462000\n",
      "2024-11-29 03:29:11.137000: I runner.py:310] Step = 79900 ; steps/s = 1.64, tokens/s = 43610 (43610 target) ; Learning rate = 0.000313 ; Loss = 1.449860\n",
      "2024-11-29 03:30:11.801000: I runner.py:310] Step = 80000 ; steps/s = 1.65, tokens/s = 43150 (43150 target) ; Learning rate = 0.000312 ; Loss = 1.442983\n",
      "2024-11-29 03:30:14.053000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-80000\n",
      "2024-11-29 03:30:14.054000: I training.py:192] Running evaluation for step 80000\n",
      "2024-11-29 03:31:02.365000: I training.py:192] Evaluation result for step 80000: loss = 0.842137 ; perplexity = 2.321321\n",
      "2024-11-29 03:32:03.447000: I runner.py:310] Step = 80100 ; steps/s = 1.64, tokens/s = 43677 (43677 target) ; Learning rate = 0.000312 ; Loss = 1.449952\n",
      "2024-11-29 03:33:04.533000: I runner.py:310] Step = 80200 ; steps/s = 1.64, tokens/s = 43642 (43642 target) ; Learning rate = 0.000312 ; Loss = 1.447714\n",
      "2024-11-29 03:34:05.227000: I runner.py:310] Step = 80300 ; steps/s = 1.65, tokens/s = 43122 (43122 target) ; Learning rate = 0.000312 ; Loss = 1.445496\n",
      "2024-11-29 03:35:06.457000: I runner.py:310] Step = 80400 ; steps/s = 1.63, tokens/s = 43564 (43564 target) ; Learning rate = 0.000312 ; Loss = 1.446575\n",
      "2024-11-29 03:36:07.610000: I runner.py:310] Step = 80500 ; steps/s = 1.64, tokens/s = 43591 (43591 target) ; Learning rate = 0.000312 ; Loss = 1.445937\n",
      "2024-11-29 03:37:08.310000: I runner.py:310] Step = 80600 ; steps/s = 1.65, tokens/s = 43111 (43111 target) ; Learning rate = 0.000311 ; Loss = 1.439981\n",
      "2024-11-29 03:38:09.469000: I runner.py:310] Step = 80700 ; steps/s = 1.64, tokens/s = 43621 (43621 target) ; Learning rate = 0.000311 ; Loss = 1.444244\n",
      "2024-11-29 03:39:10.182000: I runner.py:310] Step = 80800 ; steps/s = 1.65, tokens/s = 43092 (43092 target) ; Learning rate = 0.000311 ; Loss = 1.443486\n",
      "2024-11-29 03:40:11.250000: I runner.py:310] Step = 80900 ; steps/s = 1.64, tokens/s = 43662 (43662 target) ; Learning rate = 0.000311 ; Loss = 1.442896\n",
      "2024-11-29 03:41:12.391000: I runner.py:310] Step = 81000 ; steps/s = 1.64, tokens/s = 43624 (43624 target) ; Learning rate = 0.000311 ; Loss = 1.447120\n",
      "2024-11-29 03:42:13.076000: I runner.py:310] Step = 81100 ; steps/s = 1.65, tokens/s = 43115 (43115 target) ; Learning rate = 0.000310 ; Loss = 1.451156\n",
      "2024-11-29 03:43:14.289000: I runner.py:310] Step = 81200 ; steps/s = 1.63, tokens/s = 43567 (43567 target) ; Learning rate = 0.000310 ; Loss = 1.443753\n",
      "2024-11-29 03:44:15.043000: I runner.py:310] Step = 81300 ; steps/s = 1.65, tokens/s = 43084 (43084 target) ; Learning rate = 0.000310 ; Loss = 1.440747\n",
      "2024-11-29 03:45:16.200000: I runner.py:310] Step = 81400 ; steps/s = 1.64, tokens/s = 43605 (43605 target) ; Learning rate = 0.000310 ; Loss = 1.442343\n",
      "2024-11-29 03:46:17.351000: I runner.py:310] Step = 81500 ; steps/s = 1.64, tokens/s = 43599 (43599 target) ; Learning rate = 0.000310 ; Loss = 1.444824\n",
      "2024-11-29 03:47:18.017000: I runner.py:310] Step = 81600 ; steps/s = 1.65, tokens/s = 43141 (43141 target) ; Learning rate = 0.000309 ; Loss = 1.441246\n",
      "2024-11-29 03:48:19.170000: I runner.py:310] Step = 81700 ; steps/s = 1.64, tokens/s = 43613 (43613 target) ; Learning rate = 0.000309 ; Loss = 1.447734\n",
      "2024-11-29 03:49:20.320000: I runner.py:310] Step = 81800 ; steps/s = 1.64, tokens/s = 43603 (43603 target) ; Learning rate = 0.000309 ; Loss = 1.448033\n",
      "2024-11-29 03:50:20.938000: I runner.py:310] Step = 81900 ; steps/s = 1.65, tokens/s = 43169 (43169 target) ; Learning rate = 0.000309 ; Loss = 1.444696\n",
      "2024-11-29 03:51:22.092000: I runner.py:310] Step = 82000 ; steps/s = 1.64, tokens/s = 43605 (43605 target) ; Learning rate = 0.000309 ; Loss = 1.447784\n",
      "2024-11-29 03:52:22.744000: I runner.py:310] Step = 82100 ; steps/s = 1.65, tokens/s = 43153 (43153 target) ; Learning rate = 0.000308 ; Loss = 1.446117\n",
      "2024-11-29 03:53:23.927000: I runner.py:310] Step = 82200 ; steps/s = 1.63, tokens/s = 43601 (43601 target) ; Learning rate = 0.000308 ; Loss = 1.448037\n",
      "2024-11-29 03:54:25.064000: I runner.py:310] Step = 82300 ; steps/s = 1.64, tokens/s = 43604 (43604 target) ; Learning rate = 0.000308 ; Loss = 1.443458\n",
      "2024-11-29 03:55:25.778000: I runner.py:310] Step = 82400 ; steps/s = 1.65, tokens/s = 43101 (43101 target) ; Learning rate = 0.000308 ; Loss = 1.453885\n",
      "2024-11-29 03:56:26.965000: I runner.py:310] Step = 82500 ; steps/s = 1.63, tokens/s = 43585 (43585 target) ; Learning rate = 0.000308 ; Loss = 1.442460\n",
      "2024-11-29 03:57:27.599000: I runner.py:310] Step = 82600 ; steps/s = 1.65, tokens/s = 43161 (43161 target) ; Learning rate = 0.000308 ; Loss = 1.444491\n",
      "2024-11-29 03:58:28.771000: I runner.py:310] Step = 82700 ; steps/s = 1.64, tokens/s = 43596 (43596 target) ; Learning rate = 0.000307 ; Loss = 1.438244\n",
      "2024-11-29 03:59:29.887000: I runner.py:310] Step = 82800 ; steps/s = 1.64, tokens/s = 43621 (43621 target) ; Learning rate = 0.000307 ; Loss = 1.448162\n",
      "2024-11-29 04:00:30.490000: I runner.py:310] Step = 82900 ; steps/s = 1.65, tokens/s = 43185 (43185 target) ; Learning rate = 0.000307 ; Loss = 1.446187\n",
      "2024-11-29 04:01:31.629000: I runner.py:310] Step = 83000 ; steps/s = 1.64, tokens/s = 43629 (43629 target) ; Learning rate = 0.000307 ; Loss = 1.446907\n",
      "2024-11-29 04:02:32.795000: I runner.py:310] Step = 83100 ; steps/s = 1.64, tokens/s = 43592 (43592 target) ; Learning rate = 0.000307 ; Loss = 1.446793\n",
      "2024-11-29 04:03:33.411000: I runner.py:310] Step = 83200 ; steps/s = 1.65, tokens/s = 43181 (43181 target) ; Learning rate = 0.000306 ; Loss = 1.443315\n",
      "2024-11-29 04:04:34.533000: I runner.py:310] Step = 83300 ; steps/s = 1.64, tokens/s = 43624 (43624 target) ; Learning rate = 0.000306 ; Loss = 1.443137\n",
      "2024-11-29 04:05:35.178000: I runner.py:310] Step = 83400 ; steps/s = 1.65, tokens/s = 43152 (43152 target) ; Learning rate = 0.000306 ; Loss = 1.446374\n",
      "2024-11-29 04:06:36.317000: I runner.py:310] Step = 83500 ; steps/s = 1.64, tokens/s = 43607 (43607 target) ; Learning rate = 0.000306 ; Loss = 1.442016\n",
      "2024-11-29 04:07:37.457000: I runner.py:310] Step = 83600 ; steps/s = 1.64, tokens/s = 43625 (43625 target) ; Learning rate = 0.000306 ; Loss = 1.448179\n",
      "2024-11-29 04:08:38.095000: I runner.py:310] Step = 83700 ; steps/s = 1.65, tokens/s = 43157 (43157 target) ; Learning rate = 0.000306 ; Loss = 1.445116\n",
      "2024-11-29 04:09:39.214000: I runner.py:310] Step = 83800 ; steps/s = 1.64, tokens/s = 43631 (43631 target) ; Learning rate = 0.000305 ; Loss = 1.446945\n",
      "2024-11-29 04:10:39.898000: I runner.py:310] Step = 83900 ; steps/s = 1.65, tokens/s = 43131 (43131 target) ; Learning rate = 0.000305 ; Loss = 1.442873\n",
      "2024-11-29 04:11:40.990000: I runner.py:310] Step = 84000 ; steps/s = 1.64, tokens/s = 43648 (43648 target) ; Learning rate = 0.000305 ; Loss = 1.445739\n",
      "2024-11-29 04:12:42.214000: I runner.py:310] Step = 84100 ; steps/s = 1.63, tokens/s = 43554 (43554 target) ; Learning rate = 0.000305 ; Loss = 1.448542\n",
      "2024-11-29 04:13:42.946000: I runner.py:310] Step = 84200 ; steps/s = 1.65, tokens/s = 43096 (43096 target) ; Learning rate = 0.000305 ; Loss = 1.444714\n",
      "2024-11-29 04:14:44.121000: I runner.py:310] Step = 84300 ; steps/s = 1.63, tokens/s = 43589 (43589 target) ; Learning rate = 0.000304 ; Loss = 1.451739\n",
      "2024-11-29 04:15:45.253000: I runner.py:310] Step = 84400 ; steps/s = 1.64, tokens/s = 43620 (43620 target) ; Learning rate = 0.000304 ; Loss = 1.441433\n",
      "2024-11-29 04:16:45.870000: I runner.py:310] Step = 84500 ; steps/s = 1.65, tokens/s = 43175 (43175 target) ; Learning rate = 0.000304 ; Loss = 1.441652\n",
      "2024-11-29 04:17:46.991000: I runner.py:310] Step = 84600 ; steps/s = 1.64, tokens/s = 43626 (43626 target) ; Learning rate = 0.000304 ; Loss = 1.447892\n",
      "2024-11-29 04:18:47.733000: I runner.py:310] Step = 84700 ; steps/s = 1.65, tokens/s = 43086 (43086 target) ; Learning rate = 0.000304 ; Loss = 1.440110\n",
      "2024-11-29 04:19:48.872000: I runner.py:310] Step = 84800 ; steps/s = 1.64, tokens/s = 43631 (43631 target) ; Learning rate = 0.000304 ; Loss = 1.443673\n",
      "2024-11-29 04:20:50.058000: I runner.py:310] Step = 84900 ; steps/s = 1.63, tokens/s = 43571 (43571 target) ; Learning rate = 0.000303 ; Loss = 1.446131\n",
      "2024-11-29 04:21:50.837000: I runner.py:310] Step = 85000 ; steps/s = 1.65, tokens/s = 43057 (43057 target) ; Learning rate = 0.000303 ; Loss = 1.447678\n",
      "2024-11-29 04:21:50.839000: I training.py:192] Running evaluation for step 85000\n",
      "2024-11-29 04:22:39.357000: I training.py:192] Evaluation result for step 85000: loss = 0.846234 ; perplexity = 2.330853\n",
      "2024-11-29 04:23:40.330000: I runner.py:310] Step = 85100 ; steps/s = 1.64, tokens/s = 43742 (43742 target) ; Learning rate = 0.000303 ; Loss = 1.448531\n",
      "2024-11-29 04:24:40.995000: I runner.py:310] Step = 85200 ; steps/s = 1.65, tokens/s = 43140 (43140 target) ; Learning rate = 0.000303 ; Loss = 1.440294\n",
      "2024-11-29 04:25:42.256000: I runner.py:310] Step = 85300 ; steps/s = 1.63, tokens/s = 43530 (43530 target) ; Learning rate = 0.000303 ; Loss = 1.443301\n",
      "2024-11-29 04:26:43.460000: I runner.py:310] Step = 85400 ; steps/s = 1.63, tokens/s = 43574 (43574 target) ; Learning rate = 0.000302 ; Loss = 1.447997\n",
      "2024-11-29 04:27:44.116000: I runner.py:310] Step = 85500 ; steps/s = 1.65, tokens/s = 43137 (43137 target) ; Learning rate = 0.000302 ; Loss = 1.444056\n",
      "2024-11-29 04:28:45.278000: I runner.py:310] Step = 85600 ; steps/s = 1.64, tokens/s = 43591 (43591 target) ; Learning rate = 0.000302 ; Loss = 1.443801\n",
      "2024-11-29 04:29:46.466000: I runner.py:310] Step = 85700 ; steps/s = 1.63, tokens/s = 43591 (43591 target) ; Learning rate = 0.000302 ; Loss = 1.448935\n",
      "2024-11-29 04:30:47.242000: I runner.py:310] Step = 85800 ; steps/s = 1.65, tokens/s = 43061 (43061 target) ; Learning rate = 0.000302 ; Loss = 1.446731\n",
      "2024-11-29 04:31:48.388000: I runner.py:310] Step = 85900 ; steps/s = 1.64, tokens/s = 43619 (43619 target) ; Learning rate = 0.000302 ; Loss = 1.447396\n",
      "2024-11-29 04:32:49.062000: I runner.py:310] Step = 86000 ; steps/s = 1.65, tokens/s = 43128 (43128 target) ; Learning rate = 0.000301 ; Loss = 1.441413\n",
      "2024-11-29 04:33:50.207000: I runner.py:310] Step = 86100 ; steps/s = 1.64, tokens/s = 43619 (43619 target) ; Learning rate = 0.000301 ; Loss = 1.441574\n",
      "2024-11-29 04:34:51.398000: I runner.py:310] Step = 86200 ; steps/s = 1.63, tokens/s = 43571 (43571 target) ; Learning rate = 0.000301 ; Loss = 1.446187\n",
      "2024-11-29 04:35:52.034000: I runner.py:310] Step = 86300 ; steps/s = 1.65, tokens/s = 43152 (43152 target) ; Learning rate = 0.000301 ; Loss = 1.444260\n",
      "2024-11-29 04:36:53.115000: I runner.py:310] Step = 86400 ; steps/s = 1.64, tokens/s = 43664 (43664 target) ; Learning rate = 0.000301 ; Loss = 1.443325\n",
      "2024-11-29 04:37:53.866000: I runner.py:310] Step = 86500 ; steps/s = 1.65, tokens/s = 43073 (43073 target) ; Learning rate = 0.000301 ; Loss = 1.442259\n",
      "2024-11-29 04:38:54.960000: I runner.py:310] Step = 86600 ; steps/s = 1.64, tokens/s = 43659 (43659 target) ; Learning rate = 0.000300 ; Loss = 1.442271\n",
      "2024-11-29 04:39:56.067000: I runner.py:310] Step = 86700 ; steps/s = 1.64, tokens/s = 43623 (43623 target) ; Learning rate = 0.000300 ; Loss = 1.445270\n",
      "2024-11-29 04:40:56.796000: I runner.py:310] Step = 86800 ; steps/s = 1.65, tokens/s = 43102 (43102 target) ; Learning rate = 0.000300 ; Loss = 1.441622\n",
      "2024-11-29 04:41:57.876000: I runner.py:310] Step = 86900 ; steps/s = 1.64, tokens/s = 43648 (43648 target) ; Learning rate = 0.000300 ; Loss = 1.445608\n",
      "2024-11-29 04:42:59.027000: I runner.py:310] Step = 87000 ; steps/s = 1.64, tokens/s = 43619 (43619 target) ; Learning rate = 0.000300 ; Loss = 1.449726\n",
      "2024-11-29 04:43:59.718000: I runner.py:310] Step = 87100 ; steps/s = 1.65, tokens/s = 43112 (43112 target) ; Learning rate = 0.000299 ; Loss = 1.438729\n",
      "2024-11-29 04:45:00.875000: I runner.py:310] Step = 87200 ; steps/s = 1.64, tokens/s = 43597 (43597 target) ; Learning rate = 0.000299 ; Loss = 1.447603\n",
      "2024-11-29 04:46:01.639000: I runner.py:310] Step = 87300 ; steps/s = 1.65, tokens/s = 43084 (43084 target) ; Learning rate = 0.000299 ; Loss = 1.441384\n",
      "2024-11-29 04:47:02.806000: I runner.py:310] Step = 87400 ; steps/s = 1.64, tokens/s = 43586 (43586 target) ; Learning rate = 0.000299 ; Loss = 1.444694\n",
      "2024-11-29 04:48:03.945000: I runner.py:310] Step = 87500 ; steps/s = 1.64, tokens/s = 43628 (43628 target) ; Learning rate = 0.000299 ; Loss = 1.447618\n",
      "2024-11-29 04:49:04.634000: I runner.py:310] Step = 87600 ; steps/s = 1.65, tokens/s = 43124 (43124 target) ; Learning rate = 0.000299 ; Loss = 1.446079\n",
      "2024-11-29 04:50:05.818000: I runner.py:310] Step = 87700 ; steps/s = 1.63, tokens/s = 43584 (43584 target) ; Learning rate = 0.000298 ; Loss = 1.444396\n",
      "2024-11-29 04:51:06.511000: I runner.py:310] Step = 87800 ; steps/s = 1.65, tokens/s = 43117 (43117 target) ; Learning rate = 0.000298 ; Loss = 1.440768\n",
      "2024-11-29 04:52:07.674000: I runner.py:310] Step = 87900 ; steps/s = 1.64, tokens/s = 43597 (43597 target) ; Learning rate = 0.000298 ; Loss = 1.443758\n",
      "2024-11-29 04:53:08.797000: I runner.py:310] Step = 88000 ; steps/s = 1.64, tokens/s = 43628 (43628 target) ; Learning rate = 0.000298 ; Loss = 1.449298\n",
      "2024-11-29 04:54:09.498000: I runner.py:310] Step = 88100 ; steps/s = 1.65, tokens/s = 43124 (43124 target) ; Learning rate = 0.000298 ; Loss = 1.444580\n",
      "2024-11-29 04:55:10.641000: I runner.py:310] Step = 88200 ; steps/s = 1.64, tokens/s = 43615 (43615 target) ; Learning rate = 0.000298 ; Loss = 1.444520\n",
      "2024-11-29 04:56:11.766000: I runner.py:310] Step = 88300 ; steps/s = 1.64, tokens/s = 43549 (43549 target) ; Learning rate = 0.000297 ; Loss = 1.452602\n",
      "2024-11-29 04:57:12.445000: I runner.py:310] Step = 88400 ; steps/s = 1.65, tokens/s = 43198 (43198 target) ; Learning rate = 0.000297 ; Loss = 1.443020\n",
      "2024-11-29 04:58:13.643000: I runner.py:310] Step = 88500 ; steps/s = 1.63, tokens/s = 43576 (43576 target) ; Learning rate = 0.000297 ; Loss = 1.446168\n",
      "2024-11-29 04:59:14.380000: I runner.py:310] Step = 88600 ; steps/s = 1.65, tokens/s = 43095 (43095 target) ; Learning rate = 0.000297 ; Loss = 1.439512\n",
      "2024-11-29 05:00:15.503000: I runner.py:310] Step = 88700 ; steps/s = 1.64, tokens/s = 43616 (43616 target) ; Learning rate = 0.000297 ; Loss = 1.437986\n",
      "2024-11-29 05:01:16.637000: I runner.py:310] Step = 88800 ; steps/s = 1.64, tokens/s = 43622 (43622 target) ; Learning rate = 0.000297 ; Loss = 1.449692\n",
      "2024-11-29 05:02:17.369000: I runner.py:310] Step = 88900 ; steps/s = 1.65, tokens/s = 43091 (43091 target) ; Learning rate = 0.000296 ; Loss = 1.446296\n",
      "2024-11-29 05:03:18.514000: I runner.py:310] Step = 89000 ; steps/s = 1.64, tokens/s = 43607 (43607 target) ; Learning rate = 0.000296 ; Loss = 1.445338\n",
      "2024-11-29 05:04:19.178000: I runner.py:310] Step = 89100 ; steps/s = 1.65, tokens/s = 43140 (43140 target) ; Learning rate = 0.000296 ; Loss = 1.443386\n",
      "2024-11-29 05:05:20.364000: I runner.py:310] Step = 89200 ; steps/s = 1.63, tokens/s = 43589 (43589 target) ; Learning rate = 0.000296 ; Loss = 1.442134\n",
      "2024-11-29 05:06:21.556000: I runner.py:310] Step = 89300 ; steps/s = 1.63, tokens/s = 43581 (43581 target) ; Learning rate = 0.000296 ; Loss = 1.446798\n",
      "2024-11-29 05:07:22.306000: I runner.py:310] Step = 89400 ; steps/s = 1.65, tokens/s = 43075 (43075 target) ; Learning rate = 0.000296 ; Loss = 1.439226\n",
      "2024-11-29 05:08:23.418000: I runner.py:310] Step = 89500 ; steps/s = 1.64, tokens/s = 43638 (43638 target) ; Learning rate = 0.000295 ; Loss = 1.447904\n",
      "2024-11-29 05:09:24.430000: I runner.py:310] Step = 89600 ; steps/s = 1.64, tokens/s = 43415 (43415 target) ; Learning rate = 0.000295 ; Loss = 1.484553\n",
      "2024-11-29 05:10:25.285000: I runner.py:310] Step = 89700 ; steps/s = 1.64, tokens/s = 43300 (43300 target) ; Learning rate = 0.000295 ; Loss = 1.442155\n",
      "2024-11-29 05:11:26.399000: I runner.py:310] Step = 89800 ; steps/s = 1.64, tokens/s = 43629 (43629 target) ; Learning rate = 0.000295 ; Loss = 1.446515\n",
      "2024-11-29 05:12:27.101000: I runner.py:310] Step = 89900 ; steps/s = 1.65, tokens/s = 43124 (43124 target) ; Learning rate = 0.000295 ; Loss = 1.439417\n",
      "2024-11-29 05:13:28.206000: I runner.py:310] Step = 90000 ; steps/s = 1.64, tokens/s = 43640 (43640 target) ; Learning rate = 0.000295 ; Loss = 1.445401\n",
      "2024-11-29 05:13:30.365000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-90000\n",
      "2024-11-29 05:13:30.365000: I training.py:192] Running evaluation for step 90000\n",
      "2024-11-29 05:14:20.093000: I training.py:192] Evaluation result for step 90000: loss = 0.848670 ; perplexity = 2.336538\n",
      "2024-11-29 05:15:21.161000: I runner.py:310] Step = 90100 ; steps/s = 1.64, tokens/s = 43668 (43668 target) ; Learning rate = 0.000294 ; Loss = 1.445479\n",
      "2024-11-29 05:16:21.877000: I runner.py:310] Step = 90200 ; steps/s = 1.65, tokens/s = 43104 (43104 target) ; Learning rate = 0.000294 ; Loss = 1.444292\n",
      "2024-11-29 05:17:23.011000: I runner.py:310] Step = 90300 ; steps/s = 1.64, tokens/s = 43614 (43614 target) ; Learning rate = 0.000294 ; Loss = 1.444794\n",
      "2024-11-29 05:18:23.734000: I runner.py:310] Step = 90400 ; steps/s = 1.65, tokens/s = 43102 (43102 target) ; Learning rate = 0.000294 ; Loss = 1.445824\n",
      "2024-11-29 05:19:24.854000: I runner.py:310] Step = 90500 ; steps/s = 1.64, tokens/s = 43621 (43621 target) ; Learning rate = 0.000294 ; Loss = 1.441764\n",
      "2024-11-29 05:20:26.022000: I runner.py:310] Step = 90600 ; steps/s = 1.64, tokens/s = 43598 (43598 target) ; Learning rate = 0.000294 ; Loss = 1.444398\n",
      "2024-11-29 05:21:26.694000: I runner.py:310] Step = 90700 ; steps/s = 1.65, tokens/s = 43141 (43141 target) ; Learning rate = 0.000293 ; Loss = 1.440984\n",
      "2024-11-29 05:22:27.830000: I runner.py:310] Step = 90800 ; steps/s = 1.64, tokens/s = 43626 (43626 target) ; Learning rate = 0.000293 ; Loss = 1.442758\n",
      "2024-11-29 05:23:28.406000: I runner.py:310] Step = 90900 ; steps/s = 1.65, tokens/s = 43193 (43193 target) ; Learning rate = 0.000293 ; Loss = 1.459405\n",
      "2024-11-29 05:24:29.588000: I runner.py:310] Step = 91000 ; steps/s = 1.63, tokens/s = 43589 (43589 target) ; Learning rate = 0.000293 ; Loss = 1.443099\n",
      "2024-11-29 05:25:30.752000: I runner.py:310] Step = 91100 ; steps/s = 1.64, tokens/s = 43594 (43594 target) ; Learning rate = 0.000293 ; Loss = 1.441875\n",
      "2024-11-29 05:26:31.542000: I runner.py:310] Step = 91200 ; steps/s = 1.65, tokens/s = 43057 (43057 target) ; Learning rate = 0.000293 ; Loss = 1.438380\n",
      "2024-11-29 05:27:32.683000: I runner.py:310] Step = 91300 ; steps/s = 1.64, tokens/s = 43619 (43619 target) ; Learning rate = 0.000293 ; Loss = 1.443065\n",
      "2024-11-29 05:28:33.869000: I runner.py:310] Step = 91400 ; steps/s = 1.63, tokens/s = 43572 (43572 target) ; Learning rate = 0.000292 ; Loss = 1.443923\n",
      "2024-11-29 05:29:34.556000: I runner.py:310] Step = 91500 ; steps/s = 1.65, tokens/s = 43119 (43119 target) ; Learning rate = 0.000292 ; Loss = 1.443092\n",
      "2024-11-29 05:30:35.654000: I runner.py:310] Step = 91600 ; steps/s = 1.64, tokens/s = 43637 (43637 target) ; Learning rate = 0.000292 ; Loss = 1.442327\n",
      "2024-11-29 05:31:36.333000: I runner.py:310] Step = 91700 ; steps/s = 1.65, tokens/s = 43139 (43139 target) ; Learning rate = 0.000292 ; Loss = 1.442525\n",
      "2024-11-29 05:32:37.433000: I runner.py:310] Step = 91800 ; steps/s = 1.64, tokens/s = 43639 (43639 target) ; Learning rate = 0.000292 ; Loss = 1.440682\n",
      "2024-11-29 05:33:38.512000: I runner.py:310] Step = 91900 ; steps/s = 1.64, tokens/s = 43670 (43670 target) ; Learning rate = 0.000292 ; Loss = 1.443242\n",
      "2024-11-29 05:34:39.273000: I runner.py:310] Step = 92000 ; steps/s = 1.65, tokens/s = 43070 (43070 target) ; Learning rate = 0.000291 ; Loss = 1.441684\n",
      "2024-11-29 05:35:40.384000: I runner.py:310] Step = 92100 ; steps/s = 1.64, tokens/s = 43637 (43637 target) ; Learning rate = 0.000291 ; Loss = 1.446617\n",
      "2024-11-29 05:36:41.107000: I runner.py:310] Step = 92200 ; steps/s = 1.65, tokens/s = 43094 (43094 target) ; Learning rate = 0.000291 ; Loss = 1.443502\n",
      "2024-11-29 05:37:42.289000: I runner.py:310] Step = 92300 ; steps/s = 1.63, tokens/s = 43587 (43587 target) ; Learning rate = 0.000291 ; Loss = 1.435759\n",
      "2024-11-29 05:38:43.407000: I runner.py:310] Step = 92400 ; steps/s = 1.64, tokens/s = 43635 (43635 target) ; Learning rate = 0.000291 ; Loss = 1.442367\n",
      "2024-11-29 05:39:44.081000: I runner.py:310] Step = 92500 ; steps/s = 1.65, tokens/s = 43131 (43131 target) ; Learning rate = 0.000291 ; Loss = 1.438418\n",
      "2024-11-29 05:40:45.290000: I runner.py:310] Step = 92600 ; steps/s = 1.63, tokens/s = 43575 (43575 target) ; Learning rate = 0.000290 ; Loss = 1.446508\n",
      "2024-11-29 05:41:46.482000: I runner.py:310] Step = 92700 ; steps/s = 1.63, tokens/s = 43573 (43573 target) ; Learning rate = 0.000290 ; Loss = 1.442989\n",
      "2024-11-29 05:42:47.100000: I runner.py:310] Step = 92800 ; steps/s = 1.65, tokens/s = 43172 (43172 target) ; Learning rate = 0.000290 ; Loss = 1.442808\n",
      "2024-11-29 05:43:48.246000: I runner.py:310] Step = 92900 ; steps/s = 1.64, tokens/s = 43612 (43612 target) ; Learning rate = 0.000290 ; Loss = 1.443882\n",
      "2024-11-29 05:44:48.970000: I runner.py:310] Step = 93000 ; steps/s = 1.65, tokens/s = 43094 (43094 target) ; Learning rate = 0.000290 ; Loss = 1.444750\n",
      "2024-11-29 05:45:50.107000: I runner.py:310] Step = 93100 ; steps/s = 1.64, tokens/s = 43610 (43610 target) ; Learning rate = 0.000290 ; Loss = 1.439220\n",
      "2024-11-29 05:46:51.289000: I runner.py:310] Step = 93200 ; steps/s = 1.63, tokens/s = 43590 (43590 target) ; Learning rate = 0.000290 ; Loss = 1.446398\n",
      "2024-11-29 05:47:52.051000: I runner.py:310] Step = 93300 ; steps/s = 1.65, tokens/s = 43066 (43066 target) ; Learning rate = 0.000289 ; Loss = 1.445289\n",
      "2024-11-29 05:48:53.210000: I runner.py:310] Step = 93400 ; steps/s = 1.64, tokens/s = 43597 (43597 target) ; Learning rate = 0.000289 ; Loss = 1.441197\n",
      "2024-11-29 05:49:53.890000: I runner.py:310] Step = 93500 ; steps/s = 1.65, tokens/s = 43137 (43137 target) ; Learning rate = 0.000289 ; Loss = 1.440356\n",
      "2024-11-29 05:50:55.069000: I runner.py:310] Step = 93600 ; steps/s = 1.63, tokens/s = 43590 (43590 target) ; Learning rate = 0.000289 ; Loss = 1.438144\n",
      "2024-11-29 05:51:56.256000: I runner.py:310] Step = 93700 ; steps/s = 1.63, tokens/s = 43572 (43572 target) ; Learning rate = 0.000289 ; Loss = 1.442059\n",
      "2024-11-29 05:52:56.939000: I runner.py:310] Step = 93800 ; steps/s = 1.65, tokens/s = 43136 (43136 target) ; Learning rate = 0.000289 ; Loss = 1.434291\n",
      "2024-11-29 05:53:58.126000: I runner.py:310] Step = 93900 ; steps/s = 1.63, tokens/s = 43575 (43575 target) ; Learning rate = 0.000288 ; Loss = 1.440844\n",
      "2024-11-29 05:54:59.336000: I runner.py:310] Step = 94000 ; steps/s = 1.63, tokens/s = 43573 (43573 target) ; Learning rate = 0.000288 ; Loss = 1.444232\n",
      "2024-11-29 05:56:00.033000: I runner.py:310] Step = 94100 ; steps/s = 1.65, tokens/s = 43127 (43127 target) ; Learning rate = 0.000288 ; Loss = 1.438180\n",
      "2024-11-29 05:57:01.261000: I runner.py:310] Step = 94200 ; steps/s = 1.63, tokens/s = 43547 (43547 target) ; Learning rate = 0.000288 ; Loss = 1.438296\n",
      "2024-11-29 05:58:02.020000: I runner.py:310] Step = 94300 ; steps/s = 1.65, tokens/s = 43066 (43066 target) ; Learning rate = 0.000288 ; Loss = 1.443785\n",
      "2024-11-29 05:59:03.151000: I runner.py:310] Step = 94400 ; steps/s = 1.64, tokens/s = 43628 (43628 target) ; Learning rate = 0.000288 ; Loss = 1.444887\n",
      "2024-11-29 06:00:04.293000: I runner.py:310] Step = 94500 ; steps/s = 1.64, tokens/s = 43609 (43609 target) ; Learning rate = 0.000288 ; Loss = 1.440015\n",
      "2024-11-29 06:01:04.967000: I runner.py:310] Step = 94600 ; steps/s = 1.65, tokens/s = 43130 (43130 target) ; Learning rate = 0.000287 ; Loss = 1.442435\n",
      "2024-11-29 06:02:06.096000: I runner.py:310] Step = 94700 ; steps/s = 1.64, tokens/s = 43622 (43622 target) ; Learning rate = 0.000287 ; Loss = 1.440586\n",
      "2024-11-29 06:03:06.786000: I runner.py:310] Step = 94800 ; steps/s = 1.65, tokens/s = 43122 (43122 target) ; Learning rate = 0.000287 ; Loss = 1.440327\n",
      "2024-11-29 06:04:07.964000: I runner.py:310] Step = 94900 ; steps/s = 1.63, tokens/s = 43590 (43590 target) ; Learning rate = 0.000287 ; Loss = 1.438824\n",
      "2024-11-29 06:05:09.107000: I runner.py:310] Step = 95000 ; steps/s = 1.64, tokens/s = 43613 (43613 target) ; Learning rate = 0.000287 ; Loss = 1.444850\n",
      "2024-11-29 06:05:09.108000: I training.py:192] Running evaluation for step 95000\n",
      "2024-11-29 06:05:58.124000: I training.py:192] Evaluation result for step 95000: loss = 0.848138 ; perplexity = 2.335294\n",
      "2024-11-29 06:06:58.701000: I runner.py:310] Step = 95100 ; steps/s = 1.65, tokens/s = 43212 (43212 target) ; Learning rate = 0.000287 ; Loss = 1.438387\n",
      "2024-11-29 06:07:59.830000: I runner.py:310] Step = 95200 ; steps/s = 1.64, tokens/s = 43617 (43617 target) ; Learning rate = 0.000286 ; Loss = 1.438250\n",
      "2024-11-29 06:09:01.025000: I runner.py:310] Step = 95300 ; steps/s = 1.63, tokens/s = 43581 (43581 target) ; Learning rate = 0.000286 ; Loss = 1.443290\n",
      "2024-11-29 06:10:01.663000: I runner.py:310] Step = 95400 ; steps/s = 1.65, tokens/s = 43163 (43163 target) ; Learning rate = 0.000286 ; Loss = 1.440859\n",
      "2024-11-29 06:11:02.806000: I runner.py:310] Step = 95500 ; steps/s = 1.64, tokens/s = 43610 (43610 target) ; Learning rate = 0.000286 ; Loss = 1.446969\n",
      "2024-11-29 06:12:03.482000: I runner.py:310] Step = 95600 ; steps/s = 1.65, tokens/s = 43132 (43132 target) ; Learning rate = 0.000286 ; Loss = 1.435957\n",
      "2024-11-29 06:13:04.629000: I runner.py:310] Step = 95700 ; steps/s = 1.64, tokens/s = 43614 (43614 target) ; Learning rate = 0.000286 ; Loss = 1.443683\n",
      "2024-11-29 06:14:05.809000: I runner.py:310] Step = 95800 ; steps/s = 1.63, tokens/s = 43589 (43589 target) ; Learning rate = 0.000286 ; Loss = 1.440703\n",
      "2024-11-29 06:15:06.525000: I runner.py:310] Step = 95900 ; steps/s = 1.65, tokens/s = 43095 (43095 target) ; Learning rate = 0.000285 ; Loss = 1.439975\n",
      "2024-11-29 06:16:07.673000: I runner.py:310] Step = 96000 ; steps/s = 1.64, tokens/s = 43608 (43608 target) ; Learning rate = 0.000285 ; Loss = 1.446248\n",
      "2024-11-29 06:17:08.399000: I runner.py:310] Step = 96100 ; steps/s = 1.65, tokens/s = 43104 (43104 target) ; Learning rate = 0.000285 ; Loss = 1.437528\n",
      "2024-11-29 06:18:09.534000: I runner.py:310] Step = 96200 ; steps/s = 1.64, tokens/s = 43621 (43621 target) ; Learning rate = 0.000285 ; Loss = 1.441304\n",
      "2024-11-29 06:19:10.654000: I runner.py:310] Step = 96300 ; steps/s = 1.64, tokens/s = 43631 (43631 target) ; Learning rate = 0.000285 ; Loss = 1.440041\n",
      "2024-11-29 06:20:11.283000: I runner.py:310] Step = 96400 ; steps/s = 1.65, tokens/s = 43164 (43164 target) ; Learning rate = 0.000285 ; Loss = 1.439077\n",
      "2024-11-29 06:21:12.452000: I runner.py:310] Step = 96500 ; steps/s = 1.63, tokens/s = 43574 (43574 target) ; Learning rate = 0.000285 ; Loss = 1.439389\n",
      "2024-11-29 06:22:13.620000: I runner.py:310] Step = 96600 ; steps/s = 1.63, tokens/s = 43611 (43611 target) ; Learning rate = 0.000284 ; Loss = 1.444887\n",
      "2024-11-29 06:23:14.324000: I runner.py:310] Step = 96700 ; steps/s = 1.65, tokens/s = 43115 (43115 target) ; Learning rate = 0.000284 ; Loss = 1.441395\n",
      "2024-11-29 06:24:15.491000: I runner.py:310] Step = 96800 ; steps/s = 1.64, tokens/s = 43596 (43596 target) ; Learning rate = 0.000284 ; Loss = 1.442826\n",
      "2024-11-29 06:25:16.245000: I runner.py:310] Step = 96900 ; steps/s = 1.65, tokens/s = 43077 (43077 target) ; Learning rate = 0.000284 ; Loss = 1.434047\n",
      "2024-11-29 06:26:17.383000: I runner.py:310] Step = 97000 ; steps/s = 1.64, tokens/s = 43622 (43622 target) ; Learning rate = 0.000284 ; Loss = 1.443343\n",
      "2024-11-29 06:27:18.548000: I runner.py:310] Step = 97100 ; steps/s = 1.64, tokens/s = 43595 (43595 target) ; Learning rate = 0.000284 ; Loss = 1.442705\n",
      "2024-11-29 06:28:19.234000: I runner.py:310] Step = 97200 ; steps/s = 1.65, tokens/s = 43126 (43126 target) ; Learning rate = 0.000284 ; Loss = 1.436743\n",
      "2024-11-29 06:29:20.381000: I runner.py:310] Step = 97300 ; steps/s = 1.64, tokens/s = 43606 (43606 target) ; Learning rate = 0.000283 ; Loss = 1.440159\n",
      "2024-11-29 06:30:21.132000: I runner.py:310] Step = 97400 ; steps/s = 1.65, tokens/s = 43074 (43074 target) ; Learning rate = 0.000283 ; Loss = 1.439916\n",
      "2024-11-29 06:31:22.229000: I runner.py:310] Step = 97500 ; steps/s = 1.64, tokens/s = 43650 (43650 target) ; Learning rate = 0.000283 ; Loss = 1.435539\n",
      "2024-11-29 06:32:23.364000: I runner.py:310] Step = 97600 ; steps/s = 1.64, tokens/s = 43610 (43610 target) ; Learning rate = 0.000283 ; Loss = 1.442873\n",
      "2024-11-29 06:33:24.055000: I runner.py:310] Step = 97700 ; steps/s = 1.65, tokens/s = 43120 (43120 target) ; Learning rate = 0.000283 ; Loss = 1.439116\n",
      "2024-11-29 06:34:25.218000: I runner.py:310] Step = 97800 ; steps/s = 1.64, tokens/s = 43600 (43600 target) ; Learning rate = 0.000283 ; Loss = 1.442623\n",
      "2024-11-29 06:35:26.399000: I runner.py:310] Step = 97900 ; steps/s = 1.63, tokens/s = 43592 (43592 target) ; Learning rate = 0.000282 ; Loss = 1.443457\n",
      "2024-11-29 06:36:27.117000: I runner.py:310] Step = 98000 ; steps/s = 1.65, tokens/s = 43109 (43109 target) ; Learning rate = 0.000282 ; Loss = 1.437446\n",
      "2024-11-29 06:37:28.279000: I runner.py:310] Step = 98100 ; steps/s = 1.64, tokens/s = 43588 (43588 target) ; Learning rate = 0.000282 ; Loss = 1.439355\n",
      "2024-11-29 06:38:28.987000: I runner.py:310] Step = 98200 ; steps/s = 1.65, tokens/s = 43114 (43114 target) ; Learning rate = 0.000282 ; Loss = 1.441513\n",
      "2024-11-29 06:39:30.091000: I runner.py:310] Step = 98300 ; steps/s = 1.64, tokens/s = 43647 (43647 target) ; Learning rate = 0.000282 ; Loss = 1.445169\n",
      "2024-11-29 06:40:31.230000: I runner.py:310] Step = 98400 ; steps/s = 1.64, tokens/s = 43609 (43609 target) ; Learning rate = 0.000282 ; Loss = 1.442699\n",
      "2024-11-29 06:41:32.053000: I runner.py:310] Step = 98500 ; steps/s = 1.64, tokens/s = 43024 (43024 target) ; Learning rate = 0.000282 ; Loss = 1.435080\n",
      "2024-11-29 06:42:33.224000: I runner.py:310] Step = 98600 ; steps/s = 1.63, tokens/s = 43594 (43594 target) ; Learning rate = 0.000281 ; Loss = 1.438403\n",
      "2024-11-29 06:43:33.870000: I runner.py:310] Step = 98700 ; steps/s = 1.65, tokens/s = 43152 (43152 target) ; Learning rate = 0.000281 ; Loss = 1.440448\n",
      "2024-11-29 06:44:35.046000: I runner.py:310] Step = 98800 ; steps/s = 1.63, tokens/s = 43604 (43604 target) ; Learning rate = 0.000281 ; Loss = 1.439072\n",
      "2024-11-29 06:45:36.100000: I runner.py:310] Step = 98900 ; steps/s = 1.64, tokens/s = 43663 (43663 target) ; Learning rate = 0.000281 ; Loss = 1.452227\n",
      "2024-11-29 06:46:36.790000: I runner.py:310] Step = 99000 ; steps/s = 1.65, tokens/s = 43127 (43127 target) ; Learning rate = 0.000281 ; Loss = 1.435991\n",
      "2024-11-29 06:47:37.916000: I runner.py:310] Step = 99100 ; steps/s = 1.64, tokens/s = 43620 (43620 target) ; Learning rate = 0.000281 ; Loss = 1.441083\n",
      "2024-11-29 06:48:39.131000: I runner.py:310] Step = 99200 ; steps/s = 1.63, tokens/s = 43565 (43565 target) ; Learning rate = 0.000281 ; Loss = 1.441344\n",
      "2024-11-29 06:49:39.786000: I runner.py:310] Step = 99300 ; steps/s = 1.65, tokens/s = 43157 (43157 target) ; Learning rate = 0.000280 ; Loss = 1.438652\n",
      "2024-11-29 06:50:40.953000: I runner.py:310] Step = 99400 ; steps/s = 1.64, tokens/s = 43584 (43584 target) ; Learning rate = 0.000280 ; Loss = 1.438176\n",
      "2024-11-29 06:51:41.682000: I runner.py:310] Step = 99500 ; steps/s = 1.65, tokens/s = 43097 (43097 target) ; Learning rate = 0.000280 ; Loss = 1.438015\n",
      "2024-11-29 06:52:42.856000: I runner.py:310] Step = 99600 ; steps/s = 1.63, tokens/s = 43590 (43590 target) ; Learning rate = 0.000280 ; Loss = 1.441824\n",
      "2024-11-29 06:53:44.061000: I runner.py:310] Step = 99700 ; steps/s = 1.63, tokens/s = 43570 (43570 target) ; Learning rate = 0.000280 ; Loss = 1.440636\n",
      "2024-11-29 06:54:44.745000: I runner.py:310] Step = 99800 ; steps/s = 1.65, tokens/s = 43125 (43125 target) ; Learning rate = 0.000280 ; Loss = 1.443125\n",
      "2024-11-29 06:55:45.920000: I runner.py:310] Step = 99900 ; steps/s = 1.63, tokens/s = 43587 (43587 target) ; Learning rate = 0.000280 ; Loss = 1.438827\n",
      "2024-11-29 06:56:46.592000: I runner.py:310] Step = 100000 ; steps/s = 1.65, tokens/s = 43130 (43130 target) ; Learning rate = 0.000280 ; Loss = 1.438177\n",
      "2024-11-29 06:56:48.783000: I training.py:176] Saved checkpoint POS_TR_KK_EN/ckpt-100000\n",
      "2024-11-29 06:56:48.783000: I training.py:192] Running evaluation for step 100000\n",
      "2024-11-29 06:57:36.197000: I training.py:192] Evaluation result for step 100000: loss = 0.855738 ; perplexity = 2.353110\n",
      "2024-11-29 06:58:37.213000: I runner.py:310] Step = 100100 ; steps/s = 1.64, tokens/s = 43706 (43706 target) ; Learning rate = 0.000279 ; Loss = 1.440658\n",
      "2024-11-29 06:59:38.395000: I runner.py:310] Step = 100200 ; steps/s = 1.63, tokens/s = 43590 (43590 target) ; Learning rate = 0.000279 ; Loss = 1.443297\n",
      "2024-11-29 07:00:39.139000: I runner.py:310] Step = 100300 ; steps/s = 1.65, tokens/s = 43087 (43087 target) ; Learning rate = 0.000279 ; Loss = 1.439122\n",
      "2024-11-29 07:01:40.237000: I runner.py:310] Step = 100400 ; steps/s = 1.64, tokens/s = 43641 (43641 target) ; Learning rate = 0.000279 ; Loss = 1.436783\n",
      "2024-11-29 07:02:41.416000: I runner.py:310] Step = 100500 ; steps/s = 1.63, tokens/s = 43589 (43589 target) ; Learning rate = 0.000279 ; Loss = 1.443290\n",
      "2024-11-29 07:03:42.114000: I runner.py:310] Step = 100600 ; steps/s = 1.65, tokens/s = 43119 (43119 target) ; Learning rate = 0.000279 ; Loss = 1.438655\n",
      "2024-11-29 07:04:43.257000: I runner.py:310] Step = 100700 ; steps/s = 1.64, tokens/s = 43609 (43609 target) ; Learning rate = 0.000279 ; Loss = 1.435024\n",
      "2024-11-29 07:05:43.941000: I runner.py:310] Step = 100800 ; steps/s = 1.65, tokens/s = 43123 (43123 target) ; Learning rate = 0.000278 ; Loss = 1.439114\n",
      "2024-11-29 07:06:45.064000: I runner.py:310] Step = 100900 ; steps/s = 1.64, tokens/s = 43637 (43637 target) ; Learning rate = 0.000278 ; Loss = 1.444757\n",
      "2024-11-29 07:07:46.134000: I runner.py:310] Step = 101000 ; steps/s = 1.64, tokens/s = 43668 (43668 target) ; Learning rate = 0.000278 ; Loss = 1.439410\n",
      "2024-11-29 07:08:46.778000: I runner.py:310] Step = 101100 ; steps/s = 1.65, tokens/s = 43159 (43159 target) ; Learning rate = 0.000278 ; Loss = 1.439154\n",
      "2024-11-29 07:09:47.940000: I runner.py:310] Step = 101200 ; steps/s = 1.64, tokens/s = 43590 (43590 target) ; Learning rate = 0.000278 ; Loss = 1.442765\n",
      "2024-11-29 07:10:48.611000: I runner.py:310] Step = 101300 ; steps/s = 1.65, tokens/s = 43132 (43132 target) ; Learning rate = 0.000278 ; Loss = 1.432765\n",
      "2024-11-29 07:11:49.704000: I runner.py:310] Step = 101400 ; steps/s = 1.64, tokens/s = 43655 (43655 target) ; Learning rate = 0.000278 ; Loss = 1.441541\n",
      "2024-11-29 07:12:50.880000: I runner.py:310] Step = 101500 ; steps/s = 1.63, tokens/s = 43583 (43583 target) ; Learning rate = 0.000277 ; Loss = 1.450364\n",
      "2024-11-29 07:13:51.611000: I runner.py:310] Step = 101600 ; steps/s = 1.65, tokens/s = 43089 (43089 target) ; Learning rate = 0.000277 ; Loss = 1.441876\n",
      "2024-11-29 07:14:52.781000: I runner.py:310] Step = 101700 ; steps/s = 1.64, tokens/s = 43596 (43596 target) ; Learning rate = 0.000277 ; Loss = 1.442193\n",
      "2024-11-29 07:15:53.912000: I runner.py:310] Step = 101800 ; steps/s = 1.64, tokens/s = 43630 (43630 target) ; Learning rate = 0.000277 ; Loss = 1.440621\n",
      "2024-11-29 07:16:54.566000: I runner.py:310] Step = 101900 ; steps/s = 1.65, tokens/s = 43143 (43143 target) ; Learning rate = 0.000277 ; Loss = 1.435031\n",
      "2024-11-29 07:17:55.673000: I runner.py:310] Step = 102000 ; steps/s = 1.64, tokens/s = 43640 (43640 target) ; Learning rate = 0.000277 ; Loss = 1.438992\n",
      "2024-11-29 07:18:56.373000: I runner.py:310] Step = 102100 ; steps/s = 1.65, tokens/s = 43116 (43116 target) ; Learning rate = 0.000277 ; Loss = 1.434523\n",
      "2024-11-29 07:19:57.528000: I runner.py:310] Step = 102200 ; steps/s = 1.64, tokens/s = 43605 (43605 target) ; Learning rate = 0.000276 ; Loss = 1.440522\n",
      "2024-11-29 07:20:58.684000: I runner.py:310] Step = 102300 ; steps/s = 1.64, tokens/s = 43596 (43596 target) ; Learning rate = 0.000276 ; Loss = 1.443261\n",
      "2024-11-29 07:21:59.392000: I runner.py:310] Step = 102400 ; steps/s = 1.65, tokens/s = 43120 (43120 target) ; Learning rate = 0.000276 ; Loss = 1.436629\n",
      "2024-11-29 07:23:00.513000: I runner.py:310] Step = 102500 ; steps/s = 1.64, tokens/s = 43617 (43617 target) ; Learning rate = 0.000276 ; Loss = 1.439839\n",
      "2024-11-29 07:24:01.159000: I runner.py:310] Step = 102600 ; steps/s = 1.65, tokens/s = 43155 (43155 target) ; Learning rate = 0.000276 ; Loss = 1.434471\n",
      "2024-11-29 07:25:02.267000: I runner.py:310] Step = 102700 ; steps/s = 1.64, tokens/s = 43643 (43643 target) ; Learning rate = 0.000276 ; Loss = 1.443480\n",
      "2024-11-29 07:26:03.381000: I runner.py:310] Step = 102800 ; steps/s = 1.64, tokens/s = 43636 (43636 target) ; Learning rate = 0.000276 ; Loss = 1.444314\n",
      "2024-11-29 07:27:04.082000: I runner.py:310] Step = 102900 ; steps/s = 1.65, tokens/s = 43109 (43109 target) ; Learning rate = 0.000276 ; Loss = 1.434691\n",
      "2024-11-29 07:28:05.153000: I runner.py:310] Step = 103000 ; steps/s = 1.64, tokens/s = 43665 (43665 target) ; Learning rate = 0.000275 ; Loss = 1.441485\n",
      "2024-11-29 07:29:06.281000: I runner.py:310] Step = 103100 ; steps/s = 1.64, tokens/s = 43625 (43625 target) ; Learning rate = 0.000275 ; Loss = 1.438208\n",
      "2024-11-29 07:30:07.035000: I runner.py:310] Step = 103200 ; steps/s = 1.65, tokens/s = 43068 (43068 target) ; Learning rate = 0.000275 ; Loss = 1.439653\n",
      "2024-11-29 07:31:08.135000: I runner.py:310] Step = 103300 ; steps/s = 1.64, tokens/s = 43645 (43645 target) ; Learning rate = 0.000275 ; Loss = 1.440994\n",
      "2024-11-29 07:32:08.848000: I runner.py:310] Step = 103400 ; steps/s = 1.65, tokens/s = 43106 (43106 target) ; Learning rate = 0.000275 ; Loss = 1.437104\n",
      "2024-11-29 07:33:09.964000: I runner.py:310] Step = 103500 ; steps/s = 1.64, tokens/s = 43635 (43635 target) ; Learning rate = 0.000275 ; Loss = 1.439038\n",
      "2024-11-29 07:34:11.092000: I runner.py:310] Step = 103600 ; steps/s = 1.64, tokens/s = 43629 (43629 target) ; Learning rate = 0.000275 ; Loss = 1.436297\n",
      "2024-11-29 07:35:11.736000: I runner.py:310] Step = 103700 ; steps/s = 1.65, tokens/s = 43157 (43157 target) ; Learning rate = 0.000274 ; Loss = 1.434028\n",
      "2024-11-29 07:36:12.860000: I runner.py:310] Step = 103800 ; steps/s = 1.64, tokens/s = 43626 (43626 target) ; Learning rate = 0.000274 ; Loss = 1.438068\n",
      "2024-11-29 07:37:13.592000: I runner.py:310] Step = 103900 ; steps/s = 1.65, tokens/s = 43094 (43094 target) ; Learning rate = 0.000274 ; Loss = 1.439527\n",
      "2024-11-29 07:38:14.690000: I runner.py:310] Step = 104000 ; steps/s = 1.64, tokens/s = 43653 (43653 target) ; Learning rate = 0.000274 ; Loss = 1.439466\n",
      "2024-11-29 07:39:15.794000: I runner.py:310] Step = 104100 ; steps/s = 1.64, tokens/s = 43636 (43636 target) ; Learning rate = 0.000274 ; Loss = 1.438793\n",
      "2024-11-29 07:40:16.518000: I runner.py:310] Step = 104200 ; steps/s = 1.65, tokens/s = 43099 (43099 target) ; Learning rate = 0.000274 ; Loss = 1.439077\n",
      "2024-11-29 07:41:17.628000: I runner.py:310] Step = 104300 ; steps/s = 1.64, tokens/s = 43640 (43640 target) ; Learning rate = 0.000274 ; Loss = 1.442675\n",
      "2024-11-29 07:42:18.784000: I runner.py:310] Step = 104400 ; steps/s = 1.64, tokens/s = 43593 (43593 target) ; Learning rate = 0.000274 ; Loss = 1.440181\n",
      "2024-11-29 07:43:19.499000: I runner.py:310] Step = 104500 ; steps/s = 1.65, tokens/s = 43106 (43106 target) ; Learning rate = 0.000273 ; Loss = 1.439357\n",
      "2024-11-29 07:44:20.630000: I runner.py:310] Step = 104600 ; steps/s = 1.64, tokens/s = 43623 (43623 target) ; Learning rate = 0.000273 ; Loss = 1.439593\n",
      "2024-11-29 07:45:21.304000: I runner.py:310] Step = 104700 ; steps/s = 1.65, tokens/s = 43128 (43128 target) ; Learning rate = 0.000273 ; Loss = 1.435435\n",
      "2024-11-29 07:46:22.503000: I runner.py:310] Step = 104800 ; steps/s = 1.63, tokens/s = 43573 (43573 target) ; Learning rate = 0.000273 ; Loss = 1.437759\n",
      "2024-11-29 07:47:23.651000: I runner.py:310] Step = 104900 ; steps/s = 1.64, tokens/s = 43617 (43617 target) ; Learning rate = 0.000273 ; Loss = 1.437989\n",
      "2024-11-29 07:48:24.305000: I runner.py:310] Step = 105000 ; steps/s = 1.65, tokens/s = 43147 (43147 target) ; Learning rate = 0.000273 ; Loss = 1.440455\n",
      "2024-11-29 07:48:24.306000: I training.py:192] Running evaluation for step 105000\n",
      "2024-11-29 07:49:12.057000: I training.py:192] Evaluation result for step 105000: loss = 0.858633 ; perplexity = 2.359932\n",
      "2024-11-29 07:50:13.081000: I runner.py:310] Step = 105100 ; steps/s = 1.64, tokens/s = 43700 (43700 target) ; Learning rate = 0.000273 ; Loss = 1.439018\n",
      "2024-11-29 07:51:13.847000: I runner.py:310] Step = 105200 ; steps/s = 1.65, tokens/s = 43074 (43074 target) ; Learning rate = 0.000273 ; Loss = 1.436184\n",
      "2024-11-29 07:52:15.062000: I runner.py:310] Step = 105300 ; steps/s = 1.63, tokens/s = 43559 (43559 target) ; Learning rate = 0.000272 ; Loss = 1.438358\n",
      "2024-11-29 07:53:16.259000: I runner.py:310] Step = 105400 ; steps/s = 1.63, tokens/s = 43573 (43573 target) ; Learning rate = 0.000272 ; Loss = 1.437147\n",
      "2024-11-29 07:54:16.988000: I runner.py:310] Step = 105500 ; steps/s = 1.65, tokens/s = 43101 (43101 target) ; Learning rate = 0.000272 ; Loss = 1.440043\n",
      "2024-11-29 07:55:18.121000: I runner.py:310] Step = 105600 ; steps/s = 1.64, tokens/s = 43615 (43615 target) ; Learning rate = 0.000272 ; Loss = 1.435339\n",
      "2024-11-29 07:56:19.308000: I runner.py:310] Step = 105700 ; steps/s = 1.63, tokens/s = 43581 (43581 target) ; Learning rate = 0.000272 ; Loss = 1.439834\n",
      "2024-11-29 07:57:20.101000: I runner.py:310] Step = 105800 ; steps/s = 1.65, tokens/s = 43059 (43059 target) ; Learning rate = 0.000272 ; Loss = 1.437027\n",
      "2024-11-29 07:58:21.251000: I runner.py:310] Step = 105900 ; steps/s = 1.64, tokens/s = 43610 (43610 target) ; Learning rate = 0.000272 ; Loss = 1.442360\n",
      "2024-11-29 07:59:22.001000: I runner.py:310] Step = 106000 ; steps/s = 1.65, tokens/s = 43066 (43066 target) ; Learning rate = 0.000271 ; Loss = 1.439618\n",
      "2024-11-29 08:00:23.156000: I runner.py:310] Step = 106100 ; steps/s = 1.64, tokens/s = 43598 (43598 target) ; Learning rate = 0.000271 ; Loss = 1.436571\n",
      "2024-11-29 08:01:24.260000: I runner.py:310] Step = 106200 ; steps/s = 1.64, tokens/s = 43640 (43640 target) ; Learning rate = 0.000271 ; Loss = 1.439167\n",
      "2024-11-29 08:02:24.918000: I runner.py:310] Step = 106300 ; steps/s = 1.65, tokens/s = 43156 (43156 target) ; Learning rate = 0.000271 ; Loss = 1.436800\n",
      "2024-11-29 08:03:26.065000: I runner.py:310] Step = 106400 ; steps/s = 1.64, tokens/s = 43605 (43605 target) ; Learning rate = 0.000271 ; Loss = 1.437343\n",
      "2024-11-29 08:04:26.743000: I runner.py:310] Step = 106500 ; steps/s = 1.65, tokens/s = 43125 (43125 target) ; Learning rate = 0.000271 ; Loss = 1.436740\n",
      "2024-11-29 08:05:27.900000: I runner.py:310] Step = 106600 ; steps/s = 1.64, tokens/s = 43612 (43612 target) ; Learning rate = 0.000271 ; Loss = 1.441293\n",
      "2024-11-29 08:06:29.099000: I runner.py:310] Step = 106700 ; steps/s = 1.63, tokens/s = 43568 (43568 target) ; Learning rate = 0.000271 ; Loss = 1.441900\n",
      "2024-11-29 08:07:29.832000: I runner.py:310] Step = 106800 ; steps/s = 1.65, tokens/s = 43095 (43095 target) ; Learning rate = 0.000270 ; Loss = 1.438331\n",
      "2024-11-29 08:08:31.019000: I runner.py:310] Step = 106900 ; steps/s = 1.63, tokens/s = 43577 (43577 target) ; Learning rate = 0.000270 ; Loss = 1.437172\n",
      "2024-11-29 08:09:32.060000: I runner.py:310] Step = 107000 ; steps/s = 1.64, tokens/s = 43508 (43508 target) ; Learning rate = 0.000270 ; Loss = 1.444455\n",
      "2024-11-29 08:10:32.867000: I runner.py:310] Step = 107100 ; steps/s = 1.64, tokens/s = 43217 (43217 target) ; Learning rate = 0.000270 ; Loss = 1.436737\n",
      "2024-11-29 08:11:34.025000: I runner.py:310] Step = 107200 ; steps/s = 1.64, tokens/s = 43605 (43605 target) ; Learning rate = 0.000270 ; Loss = 1.439732\n",
      "2024-11-29 08:12:34.758000: I runner.py:310] Step = 107300 ; steps/s = 1.65, tokens/s = 43092 (43092 target) ; Learning rate = 0.000270 ; Loss = 1.437736\n",
      "2024-11-29 08:13:35.889000: I runner.py:310] Step = 107400 ; steps/s = 1.64, tokens/s = 43614 (43614 target) ; Learning rate = 0.000270 ; Loss = 1.441446\n",
      "2024-11-29 08:14:37.029000: I runner.py:310] Step = 107500 ; steps/s = 1.64, tokens/s = 43630 (43630 target) ; Learning rate = 0.000270 ; Loss = 1.438300\n",
      "2024-11-29 08:15:37.709000: I runner.py:310] Step = 107600 ; steps/s = 1.65, tokens/s = 43122 (43122 target) ; Learning rate = 0.000269 ; Loss = 1.439210\n",
      "2024-11-29 08:16:38.884000: I runner.py:310] Step = 107700 ; steps/s = 1.63, tokens/s = 43592 (43592 target) ; Learning rate = 0.000269 ; Loss = 1.439695\n",
      "2024-11-29 08:17:39.587000: I runner.py:310] Step = 107800 ; steps/s = 1.65, tokens/s = 43119 (43119 target) ; Learning rate = 0.000269 ; Loss = 1.434676\n",
      "2024-11-29 08:18:40.721000: I runner.py:310] Step = 107900 ; steps/s = 1.64, tokens/s = 43626 (43626 target) ; Learning rate = 0.000269 ; Loss = 1.443248\n",
      "2024-11-29 08:19:41.889000: I runner.py:310] Step = 108000 ; steps/s = 1.63, tokens/s = 43595 (43595 target) ; Learning rate = 0.000269 ; Loss = 1.433562\n",
      "2024-11-29 08:20:42.598000: I runner.py:310] Step = 108100 ; steps/s = 1.65, tokens/s = 43099 (43099 target) ; Learning rate = 0.000269 ; Loss = 1.437723\n",
      "2024-11-29 08:21:43.725000: I runner.py:310] Step = 108200 ; steps/s = 1.64, tokens/s = 43620 (43620 target) ; Learning rate = 0.000269 ; Loss = 1.440311\n",
      "2024-11-29 08:22:44.608000: I runner.py:310] Step = 108300 ; steps/s = 1.64, tokens/s = 43327 (43327 target) ; Learning rate = 0.000269 ; Loss = 1.469942\n",
      "2024-11-29 08:23:45.576000: I runner.py:310] Step = 108400 ; steps/s = 1.64, tokens/s = 43409 (43409 target) ; Learning rate = 0.000268 ; Loss = 1.433432\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Tr-En(Tatoeba)(POS Tags) -> Kk-En(POS Tags)\n",
    "!onmt-main --model kk-tr-en-modelim.py --config kk-tr-en-pos.yml --auto_config train --with_eval --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18b6183a-35a3-451a-93c8-7c84bf96ccc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-29 08:24:55.914844: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-29 08:24:56.706744: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-29 08:24:56.706810: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-29 08:24:56.706818: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-11-29 08:24:57.674000: I onmt-main:8] Creating model directory POS_TR_KK_EN-2\n",
      "2024-11-29 08:24:57.873000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-11-29 08:24:57.874000: I main.py:315] Using model:\n",
      "(model): MultiFeaturesTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): ParallelInputter(\n",
      "      (inputters): ListWrapper(\n",
      "        (0): WordEmbedder()\n",
      "        (1): WordEmbedder()\n",
      "      )\n",
      "      (reducer): ConcatReducer()\n",
      "    )\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): ParallelInputter(\n",
      "        (inputters): ListWrapper(\n",
      "          (0): WordEmbedder()\n",
      "          (1): WordEmbedder()\n",
      "        )\n",
      "        (reducer): ConcatReducer()\n",
      "      )\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-11-29 08:24:57.877181: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-29 08:24:59.416039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-11-29 08:24:59.416738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7767 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-11-29 08:24:59.417152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 6099 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "2024-11-29 08:24:59.421000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file:\n",
      "  - KK_tokens_valid_shared\n",
      "  - KK_pos_tags_valid_shared.txt\n",
      "  eval_labels_file: KK_valid_target_tokens_shared.txt\n",
      "  source_1_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  source_2_vocabulary: RoBERTa_KK_unique_pos\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file:\n",
      "  - KK_tokens_train_shared\n",
      "  - KK_pos_tags_train_shared.txt\n",
      "  train_labels_file: KK_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: POS_TR_KK_EN-2\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 205000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-11-29 08:24:59.759000: I inputter.py:316] Initialized source_1 input layer:\n",
      "2024-11-29 08:24:59.759000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-11-29 08:24:59.759000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-11-29 08:24:59.764000: I inputter.py:316] Initialized source_2 input layer:\n",
      "2024-11-29 08:24:59.764000: I inputter.py:316]  - vocabulary size: 21\n",
      "2024-11-29 08:24:59.764000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-11-29 08:24:59.839000: I inputter.py:316] Initialized target input layer:\n",
      "2024-11-29 08:24:59.839000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-11-29 08:24:59.839000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-11-29 08:24:59.863000: I runner.py:269] Restored checkpoint POS_TR_KK_EN/ckpt-100000\n",
      "2024-11-29 08:24:59.865000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "2024-11-29 08:24:59.915000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-11-29 08:25:00.953288: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-11-29 08:25:01.177000: I main.py:325] Accumulate gradients of 7 iterations to reach effective batch size of 25000\n",
      "2024-11-29 08:25:01.299000: I mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2024-11-29 08:25:01.778000: I dataset_ops.py:2542] Training on 318032 examples\n",
      "2024-11-29 08:26:09.110888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-11-29 08:26:10.053277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-11-29 08:26:10.462412: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-11-29 08:26:19.422000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-29 08:26:19.446000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-29 08:26:21.003000: I cross_device_ops.py:897] batch_all_reduce: 261 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-11-29 08:26:25.993000: I cross_device_ops.py:897] batch_all_reduce: 261 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-11-29 08:26:32.713000: I runner.py:310] Number of model parameters: 93357261\n",
      "2024-11-29 08:26:32.717000: I runner.py:310] Number of model weights: 261 (trainable = 261, non trainable = 0)\n",
      "2024-11-29 08:26:32.751000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-29 08:26:34.815000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-1\n",
      "2024-11-29 08:26:35.363000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-29 08:26:35.384000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-29 08:26:36.043000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-29 08:26:36.064000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-29 08:26:36.660000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-29 08:26:36.683000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-29 08:26:37.297000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-29 08:27:36.062000: I runner.py:310] Step = 100 ; steps/s = 1.62, tokens/s = 43315 (43315 target) ; Learning rate = 0.000009 ; Loss = 8.555531\n",
      "2024-11-29 08:28:37.886000: I runner.py:310] Step = 200 ; steps/s = 1.62, tokens/s = 43189 (43189 target) ; Learning rate = 0.000018 ; Loss = 7.368733\n",
      "2024-11-29 08:29:40.086000: I runner.py:310] Step = 300 ; steps/s = 1.61, tokens/s = 42960 (42960 target) ; Learning rate = 0.000027 ; Loss = 6.890476\n",
      "2024-11-29 08:30:42.343000: I runner.py:310] Step = 400 ; steps/s = 1.61, tokens/s = 42083 (42083 target) ; Learning rate = 0.000035 ; Loss = 6.604114\n",
      "2024-11-29 08:31:44.334000: I runner.py:310] Step = 500 ; steps/s = 1.61, tokens/s = 43112 (43112 target) ; Learning rate = 0.000044 ; Loss = 6.314523\n",
      "2024-11-29 08:32:46.289000: I runner.py:310] Step = 600 ; steps/s = 1.61, tokens/s = 43111 (43111 target) ; Learning rate = 0.000053 ; Loss = 6.144961\n",
      "2024-11-29 08:33:48.252000: I runner.py:310] Step = 700 ; steps/s = 1.61, tokens/s = 43113 (43113 target) ; Learning rate = 0.000062 ; Loss = 6.033874\n",
      "2024-11-29 08:34:50.071000: I runner.py:310] Step = 800 ; steps/s = 1.62, tokens/s = 42411 (42411 target) ; Learning rate = 0.000071 ; Loss = 5.852361\n",
      "2024-11-29 08:35:52.016000: I runner.py:310] Step = 900 ; steps/s = 1.61, tokens/s = 43137 (43137 target) ; Learning rate = 0.000080 ; Loss = 5.737930\n",
      "2024-11-29 08:36:53.881000: I runner.py:310] Step = 1000 ; steps/s = 1.62, tokens/s = 43163 (43163 target) ; Learning rate = 0.000088 ; Loss = 5.484520\n",
      "2024-11-29 08:37:55.869000: I runner.py:310] Step = 1100 ; steps/s = 1.61, tokens/s = 43081 (43081 target) ; Learning rate = 0.000097 ; Loss = 5.262038\n",
      "2024-11-29 08:38:57.244000: I runner.py:310] Step = 1200 ; steps/s = 1.63, tokens/s = 42738 (42738 target) ; Learning rate = 0.000106 ; Loss = 4.815976\n",
      "2024-11-29 08:39:59.163000: I runner.py:310] Step = 1300 ; steps/s = 1.62, tokens/s = 43115 (43115 target) ; Learning rate = 0.000115 ; Loss = 4.583746\n",
      "2024-11-29 08:41:01.089000: I runner.py:310] Step = 1400 ; steps/s = 1.61, tokens/s = 43150 (43150 target) ; Learning rate = 0.000124 ; Loss = 4.339175\n",
      "2024-11-29 08:42:02.954000: I runner.py:310] Step = 1500 ; steps/s = 1.62, tokens/s = 43199 (43199 target) ; Learning rate = 0.000133 ; Loss = 4.137869\n",
      "2024-11-29 08:43:04.371000: I runner.py:310] Step = 1600 ; steps/s = 1.63, tokens/s = 42682 (42682 target) ; Learning rate = 0.000142 ; Loss = 3.908195\n",
      "2024-11-29 08:44:06.283000: I runner.py:310] Step = 1700 ; steps/s = 1.62, tokens/s = 43178 (43178 target) ; Learning rate = 0.000150 ; Loss = 3.891455\n",
      "2024-11-29 08:45:08.251000: I runner.py:310] Step = 1800 ; steps/s = 1.61, tokens/s = 43106 (43106 target) ; Learning rate = 0.000159 ; Loss = 3.792375\n",
      "2024-11-29 08:46:10.186000: I runner.py:310] Step = 1900 ; steps/s = 1.61, tokens/s = 43144 (43144 target) ; Learning rate = 0.000168 ; Loss = 3.485242\n",
      "2024-11-29 08:47:11.487000: I runner.py:310] Step = 2000 ; steps/s = 1.63, tokens/s = 42718 (42718 target) ; Learning rate = 0.000177 ; Loss = 3.496889\n",
      "2024-11-29 08:48:13.418000: I runner.py:310] Step = 2100 ; steps/s = 1.61, tokens/s = 43149 (43149 target) ; Learning rate = 0.000186 ; Loss = 3.389226\n",
      "2024-11-29 08:49:15.371000: I runner.py:310] Step = 2200 ; steps/s = 1.61, tokens/s = 43117 (43117 target) ; Learning rate = 0.000195 ; Loss = 3.366253\n",
      "2024-11-29 08:50:17.294000: I runner.py:310] Step = 2300 ; steps/s = 1.62, tokens/s = 43140 (43140 target) ; Learning rate = 0.000203 ; Loss = 3.327302\n",
      "2024-11-29 08:51:18.702000: I runner.py:310] Step = 2400 ; steps/s = 1.63, tokens/s = 42683 (42683 target) ; Learning rate = 0.000212 ; Loss = 3.190249\n",
      "2024-11-29 08:52:20.577000: I runner.py:310] Step = 2500 ; steps/s = 1.62, tokens/s = 43206 (43206 target) ; Learning rate = 0.000221 ; Loss = 3.170935\n",
      "2024-11-29 08:53:22.499000: I runner.py:310] Step = 2600 ; steps/s = 1.62, tokens/s = 43114 (43114 target) ; Learning rate = 0.000230 ; Loss = 3.086926\n",
      "2024-11-29 08:54:24.518000: I runner.py:310] Step = 2700 ; steps/s = 1.61, tokens/s = 43067 (43067 target) ; Learning rate = 0.000239 ; Loss = 3.071659\n",
      "2024-11-29 08:55:25.952000: I runner.py:310] Step = 2800 ; steps/s = 1.63, tokens/s = 42666 (42666 target) ; Learning rate = 0.000248 ; Loss = 3.099308\n",
      "2024-11-29 08:56:27.851000: I runner.py:310] Step = 2900 ; steps/s = 1.62, tokens/s = 43175 (43175 target) ; Learning rate = 0.000256 ; Loss = 2.950888\n",
      "2024-11-29 08:57:29.748000: I runner.py:310] Step = 3000 ; steps/s = 1.62, tokens/s = 43170 (43170 target) ; Learning rate = 0.000265 ; Loss = 2.880610\n",
      "2024-11-29 08:58:31.734000: I runner.py:310] Step = 3100 ; steps/s = 1.61, tokens/s = 43086 (43086 target) ; Learning rate = 0.000274 ; Loss = 2.836422\n",
      "2024-11-29 08:59:32.983000: I runner.py:310] Step = 3200 ; steps/s = 1.63, tokens/s = 42783 (42783 target) ; Learning rate = 0.000283 ; Loss = 2.676149\n",
      "2024-11-29 09:00:34.924000: I runner.py:310] Step = 3300 ; steps/s = 1.61, tokens/s = 43154 (43154 target) ; Learning rate = 0.000292 ; Loss = 2.784650\n",
      "2024-11-29 09:01:36.942000: I runner.py:310] Step = 3400 ; steps/s = 1.61, tokens/s = 43067 (43067 target) ; Learning rate = 0.000301 ; Loss = 2.803945\n",
      "2024-11-29 09:02:38.832000: I runner.py:310] Step = 3500 ; steps/s = 1.62, tokens/s = 43162 (43162 target) ; Learning rate = 0.000309 ; Loss = 2.733466\n",
      "2024-11-29 09:03:40.266000: I runner.py:310] Step = 3600 ; steps/s = 1.63, tokens/s = 42674 (42674 target) ; Learning rate = 0.000318 ; Loss = 2.698065\n",
      "2024-11-29 09:04:42.207000: I runner.py:310] Step = 3700 ; steps/s = 1.61, tokens/s = 43139 (43139 target) ; Learning rate = 0.000327 ; Loss = 2.766361\n",
      "2024-11-29 09:05:44.128000: I runner.py:310] Step = 3800 ; steps/s = 1.62, tokens/s = 43129 (43129 target) ; Learning rate = 0.000336 ; Loss = 2.682529\n",
      "2024-11-29 09:06:46.070000: I runner.py:310] Step = 3900 ; steps/s = 1.61, tokens/s = 43141 (43141 target) ; Learning rate = 0.000345 ; Loss = 2.619702\n",
      "2024-11-29 09:07:47.440000: I runner.py:310] Step = 4000 ; steps/s = 1.63, tokens/s = 42734 (42734 target) ; Learning rate = 0.000354 ; Loss = 2.609403\n",
      "2024-11-29 09:08:49.316000: I runner.py:310] Step = 4100 ; steps/s = 1.62, tokens/s = 43144 (43144 target) ; Learning rate = 0.000362 ; Loss = 2.532206\n",
      "2024-11-29 09:09:51.297000: I runner.py:310] Step = 4200 ; steps/s = 1.61, tokens/s = 43091 (43091 target) ; Learning rate = 0.000371 ; Loss = 2.533978\n",
      "2024-11-29 09:10:52.911000: I runner.py:310] Step = 4300 ; steps/s = 1.62, tokens/s = 42817 (42817 target) ; Learning rate = 0.000380 ; Loss = 2.701729\n",
      "2024-11-29 09:11:54.651000: I runner.py:310] Step = 4400 ; steps/s = 1.62, tokens/s = 43026 (43026 target) ; Learning rate = 0.000389 ; Loss = 2.496529\n",
      "2024-11-29 09:12:56.630000: I runner.py:310] Step = 4500 ; steps/s = 1.61, tokens/s = 43109 (43109 target) ; Learning rate = 0.000398 ; Loss = 2.557127\n",
      "2024-11-29 09:13:58.532000: I runner.py:310] Step = 4600 ; steps/s = 1.62, tokens/s = 43157 (43157 target) ; Learning rate = 0.000407 ; Loss = 2.456682\n",
      "2024-11-29 09:14:59.926000: I runner.py:310] Step = 4700 ; steps/s = 1.63, tokens/s = 42643 (42643 target) ; Learning rate = 0.000416 ; Loss = 2.433151\n",
      "2024-11-29 09:16:01.855000: I runner.py:310] Step = 4800 ; steps/s = 1.61, tokens/s = 43186 (43186 target) ; Learning rate = 0.000424 ; Loss = 2.387786\n",
      "2024-11-29 09:17:03.801000: I runner.py:310] Step = 4900 ; steps/s = 1.61, tokens/s = 43126 (43126 target) ; Learning rate = 0.000433 ; Loss = 2.351695\n",
      "2024-11-29 09:18:05.757000: I runner.py:310] Step = 5000 ; steps/s = 1.61, tokens/s = 43095 (43095 target) ; Learning rate = 0.000442 ; Loss = 2.388178\n",
      "2024-11-29 09:18:05.759000: I training.py:192] Running evaluation for step 5000\n",
      "2024-11-29 09:23:41.892000: I training.py:192] Evaluation result for step 5000: loss = 1.253231 ; perplexity = 3.501638\n",
      "2024-11-29 09:24:43.188000: I runner.py:310] Step = 5100 ; steps/s = 1.63, tokens/s = 42759 (42759 target) ; Learning rate = 0.000451 ; Loss = 2.392715\n",
      "2024-11-29 09:25:45.204000: I runner.py:310] Step = 5200 ; steps/s = 1.61, tokens/s = 43093 (43093 target) ; Learning rate = 0.000460 ; Loss = 2.321635\n",
      "2024-11-29 09:26:47.283000: I runner.py:310] Step = 5300 ; steps/s = 1.61, tokens/s = 43029 (43029 target) ; Learning rate = 0.000469 ; Loss = 2.361568\n",
      "2024-11-29 09:27:49.420000: I runner.py:310] Step = 5400 ; steps/s = 1.61, tokens/s = 42993 (42993 target) ; Learning rate = 0.000477 ; Loss = 2.354467\n",
      "2024-11-29 09:28:50.900000: I runner.py:310] Step = 5500 ; steps/s = 1.63, tokens/s = 42639 (42639 target) ; Learning rate = 0.000486 ; Loss = 2.329261\n",
      "2024-11-29 09:29:52.957000: I runner.py:310] Step = 5600 ; steps/s = 1.61, tokens/s = 43082 (43082 target) ; Learning rate = 0.000495 ; Loss = 2.336605\n",
      "2024-11-29 09:30:55.040000: I runner.py:310] Step = 5700 ; steps/s = 1.61, tokens/s = 43009 (43009 target) ; Learning rate = 0.000504 ; Loss = 2.308675\n",
      "2024-11-29 09:31:57.102000: I runner.py:310] Step = 5800 ; steps/s = 1.61, tokens/s = 43038 (43038 target) ; Learning rate = 0.000513 ; Loss = 2.309017\n",
      "2024-11-29 09:32:58.578000: I runner.py:310] Step = 5900 ; steps/s = 1.63, tokens/s = 42612 (42612 target) ; Learning rate = 0.000522 ; Loss = 2.187013\n",
      "2024-11-29 09:34:00.663000: I runner.py:310] Step = 6000 ; steps/s = 1.61, tokens/s = 43044 (43044 target) ; Learning rate = 0.000530 ; Loss = 2.259240\n",
      "2024-11-29 09:35:02.700000: I runner.py:310] Step = 6100 ; steps/s = 1.61, tokens/s = 43099 (43099 target) ; Learning rate = 0.000539 ; Loss = 2.207962\n",
      "2024-11-29 09:36:04.716000: I runner.py:310] Step = 6200 ; steps/s = 1.61, tokens/s = 43014 (43014 target) ; Learning rate = 0.000548 ; Loss = 2.234062\n",
      "2024-11-29 09:37:06.276000: I runner.py:310] Step = 6300 ; steps/s = 1.62, tokens/s = 42613 (42613 target) ; Learning rate = 0.000557 ; Loss = 2.208628\n",
      "2024-11-29 09:38:08.310000: I runner.py:310] Step = 6400 ; steps/s = 1.61, tokens/s = 43096 (43096 target) ; Learning rate = 0.000566 ; Loss = 2.198086\n",
      "2024-11-29 09:39:10.360000: I runner.py:310] Step = 6500 ; steps/s = 1.61, tokens/s = 43038 (43038 target) ; Learning rate = 0.000575 ; Loss = 2.221332\n",
      "2024-11-29 09:40:12.379000: I runner.py:310] Step = 6600 ; steps/s = 1.61, tokens/s = 43074 (43074 target) ; Learning rate = 0.000583 ; Loss = 2.219258\n",
      "2024-11-29 09:41:13.782000: I runner.py:310] Step = 6700 ; steps/s = 1.63, tokens/s = 42663 (42663 target) ; Learning rate = 0.000592 ; Loss = 2.102130\n",
      "2024-11-29 09:42:15.861000: I runner.py:310] Step = 6800 ; steps/s = 1.61, tokens/s = 43041 (43041 target) ; Learning rate = 0.000601 ; Loss = 2.141861\n",
      "2024-11-29 09:43:17.816000: I runner.py:310] Step = 6900 ; steps/s = 1.61, tokens/s = 43117 (43117 target) ; Learning rate = 0.000610 ; Loss = 2.201963\n",
      "2024-11-29 09:44:19.801000: I runner.py:310] Step = 7000 ; steps/s = 1.61, tokens/s = 43101 (43101 target) ; Learning rate = 0.000619 ; Loss = 2.215377\n",
      "2024-11-29 09:45:21.097000: I runner.py:310] Step = 7100 ; steps/s = 1.63, tokens/s = 42767 (42767 target) ; Learning rate = 0.000628 ; Loss = 2.097666\n",
      "2024-11-29 09:46:23.005000: I runner.py:310] Step = 7200 ; steps/s = 1.62, tokens/s = 43138 (43138 target) ; Learning rate = 0.000636 ; Loss = 2.126013\n",
      "2024-11-29 09:47:24.999000: I runner.py:310] Step = 7300 ; steps/s = 1.61, tokens/s = 43108 (43108 target) ; Learning rate = 0.000645 ; Loss = 2.084882\n",
      "2024-11-29 09:48:26.982000: I runner.py:310] Step = 7400 ; steps/s = 1.61, tokens/s = 43106 (43106 target) ; Learning rate = 0.000654 ; Loss = 2.133555\n",
      "2024-11-29 09:49:28.363000: I runner.py:310] Step = 7500 ; steps/s = 1.63, tokens/s = 42701 (42701 target) ; Learning rate = 0.000663 ; Loss = 2.094709\n",
      "2024-11-29 09:50:30.335000: I runner.py:310] Step = 7600 ; steps/s = 1.61, tokens/s = 43109 (43109 target) ; Learning rate = 0.000672 ; Loss = 2.069944\n",
      "2024-11-29 09:51:32.329000: I runner.py:310] Step = 7700 ; steps/s = 1.61, tokens/s = 43085 (43085 target) ; Learning rate = 0.000681 ; Loss = 2.079847\n",
      "2024-11-29 09:52:34.336000: I runner.py:310] Step = 7800 ; steps/s = 1.61, tokens/s = 43102 (43102 target) ; Learning rate = 0.000690 ; Loss = 2.067211\n",
      "2024-11-29 09:53:35.812000: I runner.py:310] Step = 7900 ; steps/s = 1.63, tokens/s = 42667 (42667 target) ; Learning rate = 0.000698 ; Loss = 2.047722\n",
      "2024-11-29 09:54:37.828000: I runner.py:310] Step = 8000 ; steps/s = 1.61, tokens/s = 43073 (43073 target) ; Learning rate = 0.000707 ; Loss = 2.051817\n",
      "2024-11-29 09:55:39.826000: I runner.py:310] Step = 8100 ; steps/s = 1.61, tokens/s = 43073 (43073 target) ; Learning rate = 0.000716 ; Loss = 2.093824\n",
      "2024-11-29 09:56:41.803000: I runner.py:310] Step = 8200 ; steps/s = 1.61, tokens/s = 43094 (43094 target) ; Learning rate = 0.000725 ; Loss = 2.071214\n",
      "2024-11-29 09:57:43.183000: I runner.py:310] Step = 8300 ; steps/s = 1.63, tokens/s = 42688 (42688 target) ; Learning rate = 0.000734 ; Loss = 1.989212\n",
      "2024-11-29 09:58:45.243000: I runner.py:310] Step = 8400 ; steps/s = 1.61, tokens/s = 43070 (43070 target) ; Learning rate = 0.000743 ; Loss = 2.047566\n",
      "2024-11-29 09:59:47.260000: I runner.py:310] Step = 8500 ; steps/s = 1.61, tokens/s = 43086 (43086 target) ; Learning rate = 0.000751 ; Loss = 2.041743\n",
      "2024-11-29 10:00:49.036000: I runner.py:310] Step = 8600 ; steps/s = 1.62, tokens/s = 42910 (42910 target) ; Learning rate = 0.000760 ; Loss = 2.040171\n",
      "2024-11-29 10:01:50.609000: I runner.py:310] Step = 8700 ; steps/s = 1.62, tokens/s = 42901 (42901 target) ; Learning rate = 0.000769 ; Loss = 1.988767\n",
      "2024-11-29 10:02:52.602000: I runner.py:310] Step = 8800 ; steps/s = 1.61, tokens/s = 43095 (43095 target) ; Learning rate = 0.000778 ; Loss = 1.966153\n",
      "2024-11-29 10:03:54.562000: I runner.py:310] Step = 8900 ; steps/s = 1.61, tokens/s = 43133 (43133 target) ; Learning rate = 0.000787 ; Loss = 2.017843\n",
      "2024-11-29 10:04:55.925000: I runner.py:310] Step = 9000 ; steps/s = 1.63, tokens/s = 42682 (42682 target) ; Learning rate = 0.000796 ; Loss = 1.972927\n",
      "2024-11-29 10:05:57.845000: I runner.py:310] Step = 9100 ; steps/s = 1.62, tokens/s = 43161 (43161 target) ; Learning rate = 0.000804 ; Loss = 1.977463\n",
      "2024-11-29 10:06:59.857000: I runner.py:310] Step = 9200 ; steps/s = 1.61, tokens/s = 43058 (43058 target) ; Learning rate = 0.000813 ; Loss = 1.979059\n",
      "2024-11-29 10:08:01.843000: I runner.py:310] Step = 9300 ; steps/s = 1.61, tokens/s = 43114 (43114 target) ; Learning rate = 0.000822 ; Loss = 1.997253\n",
      "2024-11-29 10:09:03.312000: I runner.py:310] Step = 9400 ; steps/s = 1.63, tokens/s = 42634 (42634 target) ; Learning rate = 0.000831 ; Loss = 1.957170\n",
      "2024-11-29 10:10:05.298000: I runner.py:310] Step = 9500 ; steps/s = 1.61, tokens/s = 43086 (43086 target) ; Learning rate = 0.000840 ; Loss = 1.965535\n",
      "2024-11-29 10:11:07.337000: I runner.py:310] Step = 9600 ; steps/s = 1.61, tokens/s = 43067 (43067 target) ; Learning rate = 0.000849 ; Loss = 1.996191\n",
      "2024-11-29 10:12:09.350000: I runner.py:310] Step = 9700 ; steps/s = 1.61, tokens/s = 43093 (43093 target) ; Learning rate = 0.000857 ; Loss = 1.977664\n",
      "2024-11-29 10:13:10.784000: I runner.py:310] Step = 9800 ; steps/s = 1.63, tokens/s = 42647 (42647 target) ; Learning rate = 0.000866 ; Loss = 1.945257\n",
      "2024-11-29 10:14:12.812000: I runner.py:310] Step = 9900 ; steps/s = 1.61, tokens/s = 43076 (43076 target) ; Learning rate = 0.000875 ; Loss = 1.916932\n",
      "2024-11-29 10:15:14.698000: I runner.py:310] Step = 10000 ; steps/s = 1.62, tokens/s = 43181 (43181 target) ; Learning rate = 0.000884 ; Loss = 1.954122\n",
      "2024-11-29 10:15:16.458000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-10000\n",
      "2024-11-29 10:15:16.459000: I training.py:192] Running evaluation for step 10000\n",
      "2024-11-29 10:19:56.434000: I training.py:192] Evaluation result for step 10000: loss = 1.063639 ; perplexity = 2.896893\n",
      "2024-11-29 10:20:58.196000: I runner.py:310] Step = 10100 ; steps/s = 1.62, tokens/s = 43257 (43257 target) ; Learning rate = 0.000879 ; Loss = 1.970175\n",
      "2024-11-29 10:21:59.605000: I runner.py:310] Step = 10200 ; steps/s = 1.63, tokens/s = 42663 (42663 target) ; Learning rate = 0.000875 ; Loss = 1.934208\n",
      "2024-11-29 10:23:01.519000: I runner.py:310] Step = 10300 ; steps/s = 1.62, tokens/s = 43161 (43161 target) ; Learning rate = 0.000871 ; Loss = 1.882940\n",
      "2024-11-29 10:24:03.480000: I runner.py:310] Step = 10400 ; steps/s = 1.61, tokens/s = 43134 (43134 target) ; Learning rate = 0.000867 ; Loss = 1.956186\n",
      "2024-11-29 10:25:05.368000: I runner.py:310] Step = 10500 ; steps/s = 1.62, tokens/s = 43152 (43152 target) ; Learning rate = 0.000863 ; Loss = 1.931288\n",
      "2024-11-29 10:26:06.750000: I runner.py:310] Step = 10600 ; steps/s = 1.63, tokens/s = 42732 (42732 target) ; Learning rate = 0.000858 ; Loss = 1.868897\n",
      "2024-11-29 10:27:08.668000: I runner.py:310] Step = 10700 ; steps/s = 1.62, tokens/s = 43123 (43123 target) ; Learning rate = 0.000854 ; Loss = 1.896602\n",
      "2024-11-29 10:28:10.622000: I runner.py:310] Step = 10800 ; steps/s = 1.61, tokens/s = 43104 (43104 target) ; Learning rate = 0.000850 ; Loss = 1.933114\n",
      "2024-11-29 10:29:12.584000: I runner.py:310] Step = 10900 ; steps/s = 1.61, tokens/s = 43154 (43154 target) ; Learning rate = 0.000847 ; Loss = 1.900637\n",
      "2024-11-29 10:30:13.997000: I runner.py:310] Step = 11000 ; steps/s = 1.63, tokens/s = 42652 (42652 target) ; Learning rate = 0.000843 ; Loss = 1.914299\n",
      "2024-11-29 10:31:15.954000: I runner.py:310] Step = 11100 ; steps/s = 1.61, tokens/s = 43169 (43169 target) ; Learning rate = 0.000839 ; Loss = 1.890018\n",
      "2024-11-29 10:32:17.910000: I runner.py:310] Step = 11200 ; steps/s = 1.61, tokens/s = 43089 (43089 target) ; Learning rate = 0.000835 ; Loss = 1.884482\n",
      "2024-11-29 10:33:19.871000: I runner.py:310] Step = 11300 ; steps/s = 1.61, tokens/s = 43140 (43140 target) ; Learning rate = 0.000831 ; Loss = 1.887393\n",
      "2024-11-29 10:34:21.255000: I runner.py:310] Step = 11400 ; steps/s = 1.63, tokens/s = 42678 (42678 target) ; Learning rate = 0.000828 ; Loss = 1.839156\n",
      "2024-11-29 10:35:23.219000: I runner.py:310] Step = 11500 ; steps/s = 1.61, tokens/s = 43130 (43130 target) ; Learning rate = 0.000824 ; Loss = 1.887716\n",
      "2024-11-29 10:36:25.240000: I runner.py:310] Step = 11600 ; steps/s = 1.61, tokens/s = 43042 (43042 target) ; Learning rate = 0.000821 ; Loss = 1.874115\n",
      "2024-11-29 10:37:27.222000: I runner.py:310] Step = 11700 ; steps/s = 1.61, tokens/s = 43111 (43111 target) ; Learning rate = 0.000817 ; Loss = 1.886378\n",
      "2024-11-29 10:38:28.557000: I runner.py:310] Step = 11800 ; steps/s = 1.63, tokens/s = 42733 (42733 target) ; Learning rate = 0.000814 ; Loss = 1.840646\n",
      "2024-11-29 10:39:30.536000: I runner.py:310] Step = 11900 ; steps/s = 1.61, tokens/s = 43097 (43097 target) ; Learning rate = 0.000810 ; Loss = 1.856528\n",
      "2024-11-29 10:40:32.548000: I runner.py:310] Step = 12000 ; steps/s = 1.61, tokens/s = 43075 (43075 target) ; Learning rate = 0.000807 ; Loss = 1.886186\n",
      "2024-11-29 10:41:34.502000: I runner.py:310] Step = 12100 ; steps/s = 1.61, tokens/s = 43137 (43137 target) ; Learning rate = 0.000803 ; Loss = 1.890438\n",
      "2024-11-29 10:42:35.907000: I runner.py:310] Step = 12200 ; steps/s = 1.63, tokens/s = 42698 (42698 target) ; Learning rate = 0.000800 ; Loss = 1.818017\n",
      "2024-11-29 10:43:37.840000: I runner.py:310] Step = 12300 ; steps/s = 1.61, tokens/s = 43144 (43144 target) ; Learning rate = 0.000797 ; Loss = 1.845103\n",
      "2024-11-29 10:44:39.842000: I runner.py:310] Step = 12400 ; steps/s = 1.61, tokens/s = 43073 (43073 target) ; Learning rate = 0.000794 ; Loss = 1.840858\n",
      "2024-11-29 10:45:41.784000: I runner.py:310] Step = 12500 ; steps/s = 1.61, tokens/s = 43130 (43130 target) ; Learning rate = 0.000791 ; Loss = 1.874025\n",
      "2024-11-29 10:46:43.198000: I runner.py:310] Step = 12600 ; steps/s = 1.63, tokens/s = 42692 (42692 target) ; Learning rate = 0.000787 ; Loss = 1.845431\n",
      "2024-11-29 10:47:45.168000: I runner.py:310] Step = 12700 ; steps/s = 1.61, tokens/s = 43094 (43094 target) ; Learning rate = 0.000784 ; Loss = 1.819616\n",
      "2024-11-29 10:48:47.169000: I runner.py:310] Step = 12800 ; steps/s = 1.61, tokens/s = 43082 (43082 target) ; Learning rate = 0.000781 ; Loss = 1.816050\n",
      "2024-11-29 10:49:49.011000: I runner.py:310] Step = 12900 ; steps/s = 1.62, tokens/s = 43090 (43090 target) ; Learning rate = 0.000778 ; Loss = 1.844024\n",
      "2024-11-29 10:50:50.451000: I runner.py:310] Step = 13000 ; steps/s = 1.63, tokens/s = 42782 (42782 target) ; Learning rate = 0.000775 ; Loss = 1.785507\n",
      "2024-11-29 10:51:52.387000: I runner.py:310] Step = 13100 ; steps/s = 1.61, tokens/s = 43108 (43108 target) ; Learning rate = 0.000772 ; Loss = 1.809223\n",
      "2024-11-29 10:52:54.351000: I runner.py:310] Step = 13200 ; steps/s = 1.61, tokens/s = 43120 (43120 target) ; Learning rate = 0.000769 ; Loss = 1.834410\n",
      "2024-11-29 10:53:55.692000: I runner.py:310] Step = 13300 ; steps/s = 1.63, tokens/s = 42737 (42737 target) ; Learning rate = 0.000766 ; Loss = 1.796249\n",
      "2024-11-29 10:54:57.641000: I runner.py:310] Step = 13400 ; steps/s = 1.61, tokens/s = 43158 (43158 target) ; Learning rate = 0.000764 ; Loss = 1.806415\n",
      "2024-11-29 10:55:59.584000: I runner.py:310] Step = 13500 ; steps/s = 1.61, tokens/s = 43122 (43122 target) ; Learning rate = 0.000761 ; Loss = 1.804761\n",
      "2024-11-29 10:57:01.488000: I runner.py:310] Step = 13600 ; steps/s = 1.62, tokens/s = 43185 (43185 target) ; Learning rate = 0.000758 ; Loss = 1.802268\n",
      "2024-11-29 10:58:02.820000: I runner.py:310] Step = 13700 ; steps/s = 1.63, tokens/s = 42679 (42679 target) ; Learning rate = 0.000755 ; Loss = 1.798064\n",
      "2024-11-29 10:59:04.814000: I runner.py:310] Step = 13800 ; steps/s = 1.61, tokens/s = 43124 (43124 target) ; Learning rate = 0.000752 ; Loss = 1.787250\n",
      "2024-11-29 11:00:06.729000: I runner.py:310] Step = 13900 ; steps/s = 1.62, tokens/s = 43146 (43146 target) ; Learning rate = 0.000750 ; Loss = 1.778270\n",
      "2024-11-29 11:01:08.641000: I runner.py:310] Step = 14000 ; steps/s = 1.62, tokens/s = 43144 (43144 target) ; Learning rate = 0.000747 ; Loss = 1.801814\n",
      "2024-11-29 11:02:10.128000: I runner.py:310] Step = 14100 ; steps/s = 1.63, tokens/s = 42626 (42626 target) ; Learning rate = 0.000744 ; Loss = 1.785713\n",
      "2024-11-29 11:03:12.068000: I runner.py:310] Step = 14200 ; steps/s = 1.61, tokens/s = 43158 (43158 target) ; Learning rate = 0.000742 ; Loss = 1.782262\n",
      "2024-11-29 11:04:14.096000: I runner.py:310] Step = 14300 ; steps/s = 1.61, tokens/s = 43070 (43070 target) ; Learning rate = 0.000739 ; Loss = 1.776450\n",
      "2024-11-29 11:05:16.099000: I runner.py:310] Step = 14400 ; steps/s = 1.61, tokens/s = 43072 (43072 target) ; Learning rate = 0.000737 ; Loss = 1.764874\n",
      "2024-11-29 11:06:17.491000: I runner.py:310] Step = 14500 ; steps/s = 1.63, tokens/s = 42674 (42674 target) ; Learning rate = 0.000734 ; Loss = 1.789725\n",
      "2024-11-29 11:07:19.491000: I runner.py:310] Step = 14600 ; steps/s = 1.61, tokens/s = 43094 (43094 target) ; Learning rate = 0.000731 ; Loss = 1.765705\n",
      "2024-11-29 11:08:21.417000: I runner.py:310] Step = 14700 ; steps/s = 1.61, tokens/s = 43119 (43119 target) ; Learning rate = 0.000729 ; Loss = 1.777772\n",
      "2024-11-29 11:09:23.366000: I runner.py:310] Step = 14800 ; steps/s = 1.61, tokens/s = 43138 (43138 target) ; Learning rate = 0.000727 ; Loss = 1.772245\n",
      "2024-11-29 11:10:24.751000: I runner.py:310] Step = 14900 ; steps/s = 1.63, tokens/s = 42688 (42688 target) ; Learning rate = 0.000724 ; Loss = 1.742037\n",
      "2024-11-29 11:11:26.658000: I runner.py:310] Step = 15000 ; steps/s = 1.62, tokens/s = 43167 (43167 target) ; Learning rate = 0.000722 ; Loss = 1.768743\n",
      "2024-11-29 11:11:26.660000: I training.py:192] Running evaluation for step 15000\n",
      "2024-11-29 11:15:51.644000: I training.py:192] Evaluation result for step 15000: loss = 1.062366 ; perplexity = 2.893207\n",
      "2024-11-29 11:16:53.433000: I runner.py:310] Step = 15100 ; steps/s = 1.62, tokens/s = 43231 (43231 target) ; Learning rate = 0.000719 ; Loss = 1.772684\n",
      "2024-11-29 11:17:55.328000: I runner.py:310] Step = 15200 ; steps/s = 1.62, tokens/s = 43181 (43181 target) ; Learning rate = 0.000717 ; Loss = 1.780436\n",
      "2024-11-29 11:18:56.682000: I runner.py:310] Step = 15300 ; steps/s = 1.63, tokens/s = 42723 (42723 target) ; Learning rate = 0.000715 ; Loss = 1.746953\n",
      "2024-11-29 11:19:58.683000: I runner.py:310] Step = 15400 ; steps/s = 1.61, tokens/s = 43082 (43082 target) ; Learning rate = 0.000712 ; Loss = 1.763066\n",
      "2024-11-29 11:21:00.753000: I runner.py:310] Step = 15500 ; steps/s = 1.61, tokens/s = 43052 (43052 target) ; Learning rate = 0.000710 ; Loss = 1.749132\n",
      "2024-11-29 11:22:02.740000: I runner.py:310] Step = 15600 ; steps/s = 1.61, tokens/s = 43111 (43111 target) ; Learning rate = 0.000708 ; Loss = 1.785873\n",
      "2024-11-29 11:23:04.137000: I runner.py:310] Step = 15700 ; steps/s = 1.63, tokens/s = 42661 (42661 target) ; Learning rate = 0.000705 ; Loss = 1.746129\n",
      "2024-11-29 11:24:06.115000: I runner.py:310] Step = 15800 ; steps/s = 1.61, tokens/s = 43122 (43122 target) ; Learning rate = 0.000703 ; Loss = 1.737597\n",
      "2024-11-29 11:25:08.042000: I runner.py:310] Step = 15900 ; steps/s = 1.61, tokens/s = 43136 (43136 target) ; Learning rate = 0.000701 ; Loss = 1.746054\n",
      "2024-11-29 11:26:09.994000: I runner.py:310] Step = 16000 ; steps/s = 1.61, tokens/s = 43100 (43100 target) ; Learning rate = 0.000699 ; Loss = 1.759274\n",
      "2024-11-29 11:27:11.354000: I runner.py:310] Step = 16100 ; steps/s = 1.63, tokens/s = 42716 (42716 target) ; Learning rate = 0.000697 ; Loss = 1.760192\n",
      "2024-11-29 11:28:13.303000: I runner.py:310] Step = 16200 ; steps/s = 1.61, tokens/s = 43159 (43159 target) ; Learning rate = 0.000694 ; Loss = 1.726413\n",
      "2024-11-29 11:29:15.248000: I runner.py:310] Step = 16300 ; steps/s = 1.61, tokens/s = 43112 (43112 target) ; Learning rate = 0.000692 ; Loss = 1.740199\n",
      "2024-11-29 11:30:17.154000: I runner.py:310] Step = 16400 ; steps/s = 1.62, tokens/s = 43161 (43161 target) ; Learning rate = 0.000690 ; Loss = 1.758473\n",
      "2024-11-29 11:31:18.477000: I runner.py:310] Step = 16500 ; steps/s = 1.63, tokens/s = 42736 (42736 target) ; Learning rate = 0.000688 ; Loss = 1.716896\n",
      "2024-11-29 11:32:20.389000: I runner.py:310] Step = 16600 ; steps/s = 1.62, tokens/s = 43151 (43151 target) ; Learning rate = 0.000686 ; Loss = 1.737261\n",
      "2024-11-29 11:33:22.299000: I runner.py:310] Step = 16700 ; steps/s = 1.62, tokens/s = 43174 (43174 target) ; Learning rate = 0.000684 ; Loss = 1.736048\n",
      "2024-11-29 11:34:24.236000: I runner.py:310] Step = 16800 ; steps/s = 1.61, tokens/s = 43117 (43117 target) ; Learning rate = 0.000682 ; Loss = 1.753228\n",
      "2024-11-29 11:35:25.586000: I runner.py:310] Step = 16900 ; steps/s = 1.63, tokens/s = 42732 (42732 target) ; Learning rate = 0.000680 ; Loss = 1.728365\n",
      "2024-11-29 11:36:27.452000: I runner.py:310] Step = 17000 ; steps/s = 1.62, tokens/s = 43188 (43188 target) ; Learning rate = 0.000678 ; Loss = 1.721693\n",
      "2024-11-29 11:37:29.388000: I runner.py:310] Step = 17100 ; steps/s = 1.61, tokens/s = 43124 (43124 target) ; Learning rate = 0.000676 ; Loss = 1.734325\n",
      "2024-11-29 11:38:31.349000: I runner.py:310] Step = 17200 ; steps/s = 1.61, tokens/s = 43102 (43102 target) ; Learning rate = 0.000674 ; Loss = 1.741271\n",
      "2024-11-29 11:39:32.596000: I runner.py:310] Step = 17300 ; steps/s = 1.63, tokens/s = 42792 (42792 target) ; Learning rate = 0.000672 ; Loss = 1.744702\n",
      "2024-11-29 11:40:34.517000: I runner.py:310] Step = 17400 ; steps/s = 1.62, tokens/s = 43169 (43169 target) ; Learning rate = 0.000670 ; Loss = 1.717090\n",
      "2024-11-29 11:41:36.412000: I runner.py:310] Step = 17500 ; steps/s = 1.62, tokens/s = 43165 (43165 target) ; Learning rate = 0.000668 ; Loss = 1.723141\n",
      "2024-11-29 11:42:37.763000: I runner.py:310] Step = 17600 ; steps/s = 1.63, tokens/s = 42697 (42697 target) ; Learning rate = 0.000666 ; Loss = 1.718825\n",
      "2024-11-29 11:43:39.712000: I runner.py:310] Step = 17700 ; steps/s = 1.61, tokens/s = 43170 (43170 target) ; Learning rate = 0.000664 ; Loss = 1.729824\n",
      "2024-11-29 11:44:41.614000: I runner.py:310] Step = 17800 ; steps/s = 1.62, tokens/s = 43166 (43166 target) ; Learning rate = 0.000662 ; Loss = 1.721009\n",
      "2024-11-29 11:45:43.560000: I runner.py:310] Step = 17900 ; steps/s = 1.61, tokens/s = 43102 (43102 target) ; Learning rate = 0.000661 ; Loss = 1.729296\n",
      "2024-11-29 11:46:44.965000: I runner.py:310] Step = 18000 ; steps/s = 1.63, tokens/s = 42668 (42668 target) ; Learning rate = 0.000659 ; Loss = 1.705314\n",
      "2024-11-29 11:47:46.912000: I runner.py:310] Step = 18100 ; steps/s = 1.61, tokens/s = 43122 (43122 target) ; Learning rate = 0.000657 ; Loss = 1.699870\n",
      "2024-11-29 11:48:48.854000: I runner.py:310] Step = 18200 ; steps/s = 1.61, tokens/s = 43125 (43125 target) ; Learning rate = 0.000655 ; Loss = 1.709519\n",
      "2024-11-29 11:49:50.804000: I runner.py:310] Step = 18300 ; steps/s = 1.61, tokens/s = 43128 (43128 target) ; Learning rate = 0.000653 ; Loss = 1.722564\n",
      "2024-11-29 11:50:52.154000: I runner.py:310] Step = 18400 ; steps/s = 1.63, tokens/s = 42722 (42722 target) ; Learning rate = 0.000652 ; Loss = 1.711379\n",
      "2024-11-29 11:51:54.053000: I runner.py:310] Step = 18500 ; steps/s = 1.62, tokens/s = 43137 (43137 target) ; Learning rate = 0.000650 ; Loss = 1.704619\n",
      "2024-11-29 11:52:56.015000: I runner.py:310] Step = 18600 ; steps/s = 1.61, tokens/s = 43142 (43142 target) ; Learning rate = 0.000648 ; Loss = 1.710546\n",
      "2024-11-29 11:53:57.918000: I runner.py:310] Step = 18700 ; steps/s = 1.62, tokens/s = 43151 (43151 target) ; Learning rate = 0.000646 ; Loss = 1.708525\n",
      "2024-11-29 11:54:59.301000: I runner.py:310] Step = 18800 ; steps/s = 1.63, tokens/s = 42726 (42726 target) ; Learning rate = 0.000645 ; Loss = 1.693022\n",
      "2024-11-29 11:56:01.262000: I runner.py:310] Step = 18900 ; steps/s = 1.61, tokens/s = 43120 (43120 target) ; Learning rate = 0.000643 ; Loss = 1.688842\n",
      "2024-11-29 11:57:03.197000: I runner.py:310] Step = 19000 ; steps/s = 1.61, tokens/s = 43113 (43113 target) ; Learning rate = 0.000641 ; Loss = 1.710475\n",
      "2024-11-29 11:58:05.054000: I runner.py:310] Step = 19100 ; steps/s = 1.62, tokens/s = 43182 (43182 target) ; Learning rate = 0.000640 ; Loss = 1.718434\n",
      "2024-11-29 11:59:06.382000: I runner.py:310] Step = 19200 ; steps/s = 1.63, tokens/s = 42741 (42741 target) ; Learning rate = 0.000638 ; Loss = 1.704260\n",
      "2024-11-29 12:00:08.260000: I runner.py:310] Step = 19300 ; steps/s = 1.62, tokens/s = 43187 (43187 target) ; Learning rate = 0.000636 ; Loss = 1.696687\n",
      "2024-11-29 12:01:10.186000: I runner.py:310] Step = 19400 ; steps/s = 1.61, tokens/s = 43139 (43139 target) ; Learning rate = 0.000635 ; Loss = 1.698091\n",
      "2024-11-29 12:02:12.136000: I runner.py:310] Step = 19500 ; steps/s = 1.61, tokens/s = 43108 (43108 target) ; Learning rate = 0.000633 ; Loss = 1.721279\n",
      "2024-11-29 12:03:13.534000: I runner.py:310] Step = 19600 ; steps/s = 1.63, tokens/s = 42711 (42711 target) ; Learning rate = 0.000631 ; Loss = 1.705103\n",
      "2024-11-29 12:04:15.456000: I runner.py:310] Step = 19700 ; steps/s = 1.62, tokens/s = 43156 (43156 target) ; Learning rate = 0.000630 ; Loss = 1.693795\n",
      "2024-11-29 12:05:17.389000: I runner.py:310] Step = 19800 ; steps/s = 1.61, tokens/s = 43137 (43137 target) ; Learning rate = 0.000628 ; Loss = 1.676112\n",
      "2024-11-29 12:06:19.365000: I runner.py:310] Step = 19900 ; steps/s = 1.61, tokens/s = 43109 (43109 target) ; Learning rate = 0.000627 ; Loss = 1.717994\n",
      "2024-11-29 12:07:20.716000: I runner.py:310] Step = 20000 ; steps/s = 1.63, tokens/s = 42699 (42699 target) ; Learning rate = 0.000625 ; Loss = 1.710141\n",
      "2024-11-29 12:07:22.466000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-20000\n",
      "2024-11-29 12:07:22.466000: I training.py:192] Running evaluation for step 20000\n",
      "2024-11-29 12:11:56.317000: I training.py:192] Evaluation result for step 20000: loss = 1.074953 ; perplexity = 2.929855\n",
      "2024-11-29 12:12:58.197000: I runner.py:310] Step = 20100 ; steps/s = 1.62, tokens/s = 43203 (43203 target) ; Learning rate = 0.000623 ; Loss = 1.674917\n",
      "2024-11-29 12:14:00.113000: I runner.py:310] Step = 20200 ; steps/s = 1.62, tokens/s = 43158 (43158 target) ; Learning rate = 0.000622 ; Loss = 1.679443\n",
      "2024-11-29 12:15:02.071000: I runner.py:310] Step = 20300 ; steps/s = 1.61, tokens/s = 43108 (43108 target) ; Learning rate = 0.000620 ; Loss = 1.693333\n",
      "2024-11-29 12:16:03.443000: I runner.py:310] Step = 20400 ; steps/s = 1.63, tokens/s = 42692 (42692 target) ; Learning rate = 0.000619 ; Loss = 1.673400\n",
      "2024-11-29 12:17:05.330000: I runner.py:310] Step = 20500 ; steps/s = 1.62, tokens/s = 43200 (43200 target) ; Learning rate = 0.000617 ; Loss = 1.690089\n",
      "2024-11-29 12:18:07.291000: I runner.py:310] Step = 20600 ; steps/s = 1.61, tokens/s = 43106 (43106 target) ; Learning rate = 0.000616 ; Loss = 1.690522\n",
      "2024-11-29 12:19:09.309000: I runner.py:310] Step = 20700 ; steps/s = 1.61, tokens/s = 43075 (43075 target) ; Learning rate = 0.000614 ; Loss = 1.696423\n",
      "2024-11-29 12:20:10.727000: I runner.py:310] Step = 20800 ; steps/s = 1.63, tokens/s = 42687 (42687 target) ; Learning rate = 0.000613 ; Loss = 1.650278\n",
      "2024-11-29 12:21:12.620000: I runner.py:310] Step = 20900 ; steps/s = 1.62, tokens/s = 43185 (43185 target) ; Learning rate = 0.000611 ; Loss = 1.685435\n",
      "2024-11-29 12:22:14.541000: I runner.py:310] Step = 21000 ; steps/s = 1.62, tokens/s = 43129 (43129 target) ; Learning rate = 0.000610 ; Loss = 1.683619\n",
      "2024-11-29 12:23:16.456000: I runner.py:310] Step = 21100 ; steps/s = 1.62, tokens/s = 43128 (43128 target) ; Learning rate = 0.000608 ; Loss = 1.694108\n",
      "2024-11-29 12:24:17.826000: I runner.py:310] Step = 21200 ; steps/s = 1.63, tokens/s = 42703 (42703 target) ; Learning rate = 0.000607 ; Loss = 1.663954\n",
      "2024-11-29 12:25:19.779000: I runner.py:310] Step = 21300 ; steps/s = 1.61, tokens/s = 43131 (43131 target) ; Learning rate = 0.000606 ; Loss = 1.686147\n",
      "2024-11-29 12:26:21.779000: I runner.py:310] Step = 21400 ; steps/s = 1.61, tokens/s = 43091 (43091 target) ; Learning rate = 0.000604 ; Loss = 1.693130\n",
      "2024-11-29 12:27:23.698000: I runner.py:310] Step = 21500 ; steps/s = 1.62, tokens/s = 43125 (43125 target) ; Learning rate = 0.000603 ; Loss = 1.704761\n",
      "2024-11-29 12:28:25.008000: I runner.py:310] Step = 21600 ; steps/s = 1.63, tokens/s = 42777 (42777 target) ; Learning rate = 0.000601 ; Loss = 1.649689\n",
      "2024-11-29 12:29:26.954000: I runner.py:310] Step = 21700 ; steps/s = 1.61, tokens/s = 43101 (43101 target) ; Learning rate = 0.000600 ; Loss = 1.675313\n",
      "2024-11-29 12:30:28.878000: I runner.py:310] Step = 21800 ; steps/s = 1.62, tokens/s = 43127 (43127 target) ; Learning rate = 0.000599 ; Loss = 1.688278\n",
      "2024-11-29 12:31:30.276000: I runner.py:310] Step = 21900 ; steps/s = 1.63, tokens/s = 42689 (42689 target) ; Learning rate = 0.000597 ; Loss = 1.678317\n",
      "2024-11-29 12:32:32.164000: I runner.py:310] Step = 22000 ; steps/s = 1.62, tokens/s = 43204 (43204 target) ; Learning rate = 0.000596 ; Loss = 1.662019\n",
      "2024-11-29 12:33:34.167000: I runner.py:310] Step = 22100 ; steps/s = 1.61, tokens/s = 43061 (43061 target) ; Learning rate = 0.000595 ; Loss = 1.672996\n",
      "2024-11-29 12:34:36.087000: I runner.py:310] Step = 22200 ; steps/s = 1.62, tokens/s = 43147 (43147 target) ; Learning rate = 0.000593 ; Loss = 1.666928\n",
      "2024-11-29 12:35:37.476000: I runner.py:310] Step = 22300 ; steps/s = 1.63, tokens/s = 42700 (42700 target) ; Learning rate = 0.000592 ; Loss = 1.657349\n",
      "2024-11-29 12:36:39.455000: I runner.py:310] Step = 22400 ; steps/s = 1.61, tokens/s = 43121 (43121 target) ; Learning rate = 0.000591 ; Loss = 1.641521\n",
      "2024-11-29 12:37:41.391000: I runner.py:310] Step = 22500 ; steps/s = 1.61, tokens/s = 43147 (43147 target) ; Learning rate = 0.000589 ; Loss = 1.661841\n",
      "2024-11-29 12:38:43.343000: I runner.py:310] Step = 22600 ; steps/s = 1.61, tokens/s = 43096 (43096 target) ; Learning rate = 0.000588 ; Loss = 1.669816\n",
      "2024-11-29 12:39:44.771000: I runner.py:310] Step = 22700 ; steps/s = 1.63, tokens/s = 42676 (42676 target) ; Learning rate = 0.000587 ; Loss = 1.669242\n",
      "2024-11-29 12:40:46.736000: I runner.py:310] Step = 22800 ; steps/s = 1.61, tokens/s = 43136 (43136 target) ; Learning rate = 0.000585 ; Loss = 1.674325\n",
      "2024-11-29 12:41:48.634000: I runner.py:310] Step = 22900 ; steps/s = 1.62, tokens/s = 43163 (43163 target) ; Learning rate = 0.000584 ; Loss = 1.644470\n",
      "2024-11-29 12:42:50.555000: I runner.py:310] Step = 23000 ; steps/s = 1.62, tokens/s = 43146 (43146 target) ; Learning rate = 0.000583 ; Loss = 1.666703\n",
      "2024-11-29 12:43:51.918000: I runner.py:310] Step = 23100 ; steps/s = 1.63, tokens/s = 42672 (42672 target) ; Learning rate = 0.000582 ; Loss = 1.645338\n",
      "2024-11-29 12:44:53.869000: I runner.py:310] Step = 23200 ; steps/s = 1.61, tokens/s = 43140 (43140 target) ; Learning rate = 0.000580 ; Loss = 1.657681\n",
      "2024-11-29 12:45:55.780000: I runner.py:310] Step = 23300 ; steps/s = 1.62, tokens/s = 43161 (43161 target) ; Learning rate = 0.000579 ; Loss = 1.658415\n",
      "2024-11-29 12:46:57.768000: I runner.py:310] Step = 23400 ; steps/s = 1.61, tokens/s = 43095 (43095 target) ; Learning rate = 0.000578 ; Loss = 1.683490\n",
      "2024-11-29 12:47:59.169000: I runner.py:310] Step = 23500 ; steps/s = 1.63, tokens/s = 42699 (42699 target) ; Learning rate = 0.000577 ; Loss = 1.659810\n",
      "2024-11-29 12:49:01.118000: I runner.py:310] Step = 23600 ; steps/s = 1.61, tokens/s = 43123 (43123 target) ; Learning rate = 0.000575 ; Loss = 1.661860\n",
      "2024-11-29 12:50:03.025000: I runner.py:310] Step = 23700 ; steps/s = 1.62, tokens/s = 43166 (43166 target) ; Learning rate = 0.000574 ; Loss = 1.665340\n",
      "2024-11-29 12:51:04.943000: I runner.py:310] Step = 23800 ; steps/s = 1.62, tokens/s = 43113 (43113 target) ; Learning rate = 0.000573 ; Loss = 1.647281\n",
      "2024-11-29 12:52:06.342000: I runner.py:310] Step = 23900 ; steps/s = 1.63, tokens/s = 42698 (42698 target) ; Learning rate = 0.000572 ; Loss = 1.645874\n",
      "2024-11-29 12:53:08.221000: I runner.py:310] Step = 24000 ; steps/s = 1.62, tokens/s = 43208 (43208 target) ; Learning rate = 0.000571 ; Loss = 1.646113\n",
      "2024-11-29 12:54:10.153000: I runner.py:310] Step = 24100 ; steps/s = 1.61, tokens/s = 43098 (43098 target) ; Learning rate = 0.000569 ; Loss = 1.659381\n",
      "2024-11-29 12:55:12.069000: I runner.py:310] Step = 24200 ; steps/s = 1.62, tokens/s = 43166 (43166 target) ; Learning rate = 0.000568 ; Loss = 1.657808\n",
      "2024-11-29 12:56:13.476000: I runner.py:310] Step = 24300 ; steps/s = 1.63, tokens/s = 42677 (42677 target) ; Learning rate = 0.000567 ; Loss = 1.660913\n",
      "2024-11-29 12:57:15.410000: I runner.py:310] Step = 24400 ; steps/s = 1.61, tokens/s = 43146 (43146 target) ; Learning rate = 0.000566 ; Loss = 1.642588\n",
      "2024-11-29 12:58:17.361000: I runner.py:310] Step = 24500 ; steps/s = 1.61, tokens/s = 43132 (43132 target) ; Learning rate = 0.000565 ; Loss = 1.641798\n",
      "2024-11-29 12:59:19.268000: I runner.py:310] Step = 24600 ; steps/s = 1.62, tokens/s = 43131 (43131 target) ; Learning rate = 0.000564 ; Loss = 1.645499\n",
      "2024-11-29 13:00:20.626000: I runner.py:310] Step = 24700 ; steps/s = 1.63, tokens/s = 42724 (42724 target) ; Learning rate = 0.000562 ; Loss = 1.654028\n",
      "2024-11-29 13:01:22.551000: I runner.py:310] Step = 24800 ; steps/s = 1.62, tokens/s = 43144 (43144 target) ; Learning rate = 0.000561 ; Loss = 1.644309\n",
      "2024-11-29 13:02:24.670000: I runner.py:310] Step = 24900 ; steps/s = 1.61, tokens/s = 43007 (43007 target) ; Learning rate = 0.000560 ; Loss = 1.649141\n",
      "2024-11-29 13:03:26.576000: I runner.py:310] Step = 25000 ; steps/s = 1.62, tokens/s = 43159 (43159 target) ; Learning rate = 0.000559 ; Loss = 1.660651\n",
      "2024-11-29 13:03:26.579000: I training.py:192] Running evaluation for step 25000\n",
      "2024-11-29 13:07:56.667000: I training.py:192] Evaluation result for step 25000: loss = 1.093679 ; perplexity = 2.985237\n",
      "2024-11-29 13:08:57.953000: I runner.py:310] Step = 25100 ; steps/s = 1.63, tokens/s = 42750 (42750 target) ; Learning rate = 0.000558 ; Loss = 1.625442\n",
      "2024-11-29 13:09:59.870000: I runner.py:310] Step = 25200 ; steps/s = 1.62, tokens/s = 43155 (43155 target) ; Learning rate = 0.000557 ; Loss = 1.655608\n",
      "2024-11-29 13:11:01.811000: I runner.py:310] Step = 25300 ; steps/s = 1.61, tokens/s = 43123 (43123 target) ; Learning rate = 0.000556 ; Loss = 1.641467\n",
      "2024-11-29 13:12:03.785000: I runner.py:310] Step = 25400 ; steps/s = 1.61, tokens/s = 43106 (43106 target) ; Learning rate = 0.000555 ; Loss = 1.648268\n",
      "2024-11-29 13:13:05.135000: I runner.py:310] Step = 25500 ; steps/s = 1.63, tokens/s = 42736 (42736 target) ; Learning rate = 0.000553 ; Loss = 1.652380\n",
      "2024-11-29 13:14:07.010000: I runner.py:310] Step = 25600 ; steps/s = 1.62, tokens/s = 43173 (43173 target) ; Learning rate = 0.000552 ; Loss = 1.636650\n",
      "2024-11-29 13:15:08.977000: I runner.py:310] Step = 25700 ; steps/s = 1.61, tokens/s = 43114 (43114 target) ; Learning rate = 0.000551 ; Loss = 1.628773\n",
      "2024-11-29 13:16:10.981000: I runner.py:310] Step = 25800 ; steps/s = 1.61, tokens/s = 43081 (43081 target) ; Learning rate = 0.000550 ; Loss = 1.646235\n",
      "2024-11-29 13:17:12.355000: I runner.py:310] Step = 25900 ; steps/s = 1.63, tokens/s = 42714 (42714 target) ; Learning rate = 0.000549 ; Loss = 1.626744\n",
      "2024-11-29 13:18:14.262000: I runner.py:310] Step = 26000 ; steps/s = 1.62, tokens/s = 43177 (43177 target) ; Learning rate = 0.000548 ; Loss = 1.652378\n",
      "2024-11-29 13:19:16.236000: I runner.py:310] Step = 26100 ; steps/s = 1.61, tokens/s = 43075 (43075 target) ; Learning rate = 0.000547 ; Loss = 1.638841\n",
      "2024-11-29 13:20:17.662000: I runner.py:310] Step = 26200 ; steps/s = 1.63, tokens/s = 42669 (42669 target) ; Learning rate = 0.000546 ; Loss = 1.633964\n",
      "2024-11-29 13:21:19.638000: I runner.py:310] Step = 26300 ; steps/s = 1.61, tokens/s = 43115 (43115 target) ; Learning rate = 0.000545 ; Loss = 1.634103\n",
      "2024-11-29 13:22:21.598000: I runner.py:310] Step = 26400 ; steps/s = 1.61, tokens/s = 43097 (43097 target) ; Learning rate = 0.000544 ; Loss = 1.633054\n",
      "2024-11-29 13:23:23.564000: I runner.py:310] Step = 26500 ; steps/s = 1.61, tokens/s = 43107 (43107 target) ; Learning rate = 0.000543 ; Loss = 1.625427\n",
      "2024-11-29 13:24:24.931000: I runner.py:310] Step = 26600 ; steps/s = 1.63, tokens/s = 42738 (42738 target) ; Learning rate = 0.000542 ; Loss = 1.624449\n",
      "2024-11-29 13:25:26.878000: I runner.py:310] Step = 26700 ; steps/s = 1.61, tokens/s = 43110 (43110 target) ; Learning rate = 0.000541 ; Loss = 1.615827\n",
      "2024-11-29 13:26:28.806000: I runner.py:310] Step = 26800 ; steps/s = 1.61, tokens/s = 43142 (43142 target) ; Learning rate = 0.000540 ; Loss = 1.631443\n",
      "2024-11-29 13:27:30.893000: I runner.py:310] Step = 26900 ; steps/s = 1.61, tokens/s = 43032 (43032 target) ; Learning rate = 0.000539 ; Loss = 1.652850\n",
      "2024-11-29 13:28:32.286000: I runner.py:310] Step = 27000 ; steps/s = 1.63, tokens/s = 42708 (42708 target) ; Learning rate = 0.000538 ; Loss = 1.628595\n",
      "2024-11-29 13:29:34.315000: I runner.py:310] Step = 27100 ; steps/s = 1.61, tokens/s = 43080 (43080 target) ; Learning rate = 0.000537 ; Loss = 1.623059\n",
      "2024-11-29 13:30:36.320000: I runner.py:310] Step = 27200 ; steps/s = 1.61, tokens/s = 43099 (43099 target) ; Learning rate = 0.000536 ; Loss = 1.636379\n",
      "2024-11-29 13:31:38.233000: I runner.py:310] Step = 27300 ; steps/s = 1.62, tokens/s = 43146 (43146 target) ; Learning rate = 0.000535 ; Loss = 1.637752\n",
      "2024-11-29 13:32:39.683000: I runner.py:310] Step = 27400 ; steps/s = 1.63, tokens/s = 42649 (42649 target) ; Learning rate = 0.000534 ; Loss = 1.626432\n",
      "2024-11-29 13:33:41.656000: I runner.py:310] Step = 27500 ; steps/s = 1.61, tokens/s = 43103 (43103 target) ; Learning rate = 0.000533 ; Loss = 1.612308\n",
      "2024-11-29 13:34:43.645000: I runner.py:310] Step = 27600 ; steps/s = 1.61, tokens/s = 43091 (43091 target) ; Learning rate = 0.000532 ; Loss = 1.630168\n",
      "2024-11-29 13:35:45.581000: I runner.py:310] Step = 27700 ; steps/s = 1.61, tokens/s = 43152 (43152 target) ; Learning rate = 0.000531 ; Loss = 1.643353\n",
      "2024-11-29 13:36:46.974000: I runner.py:310] Step = 27800 ; steps/s = 1.63, tokens/s = 42690 (42690 target) ; Learning rate = 0.000530 ; Loss = 1.611362\n",
      "2024-11-29 13:37:48.938000: I runner.py:310] Step = 27900 ; steps/s = 1.61, tokens/s = 43123 (43123 target) ; Learning rate = 0.000529 ; Loss = 1.614157\n",
      "2024-11-29 13:38:50.854000: I runner.py:310] Step = 28000 ; steps/s = 1.62, tokens/s = 43128 (43128 target) ; Learning rate = 0.000528 ; Loss = 1.638165\n",
      "2024-11-29 13:39:52.882000: I runner.py:310] Step = 28100 ; steps/s = 1.61, tokens/s = 43071 (43071 target) ; Learning rate = 0.000527 ; Loss = 1.637552\n",
      "2024-11-29 13:40:54.326000: I runner.py:310] Step = 28200 ; steps/s = 1.63, tokens/s = 42683 (42683 target) ; Learning rate = 0.000526 ; Loss = 1.621320\n",
      "2024-11-29 13:41:56.269000: I runner.py:310] Step = 28300 ; steps/s = 1.61, tokens/s = 43147 (43147 target) ; Learning rate = 0.000525 ; Loss = 1.622513\n",
      "2024-11-29 13:42:58.254000: I runner.py:310] Step = 28400 ; steps/s = 1.61, tokens/s = 43089 (43089 target) ; Learning rate = 0.000524 ; Loss = 1.624135\n",
      "2024-11-29 13:44:00.287000: I runner.py:310] Step = 28500 ; steps/s = 1.61, tokens/s = 43048 (43048 target) ; Learning rate = 0.000524 ; Loss = 1.625760\n",
      "2024-11-29 13:45:01.679000: I runner.py:310] Step = 28600 ; steps/s = 1.63, tokens/s = 42698 (42698 target) ; Learning rate = 0.000523 ; Loss = 1.630407\n",
      "2024-11-29 13:46:03.627000: I runner.py:310] Step = 28700 ; steps/s = 1.61, tokens/s = 43134 (43134 target) ; Learning rate = 0.000522 ; Loss = 1.621140\n",
      "2024-11-29 13:47:05.633000: I runner.py:310] Step = 28800 ; steps/s = 1.61, tokens/s = 43099 (43099 target) ; Learning rate = 0.000521 ; Loss = 1.618763\n",
      "2024-11-29 13:48:07.606000: I runner.py:310] Step = 28900 ; steps/s = 1.61, tokens/s = 43081 (43081 target) ; Learning rate = 0.000520 ; Loss = 1.621539\n",
      "2024-11-29 13:49:09.047000: I runner.py:310] Step = 29000 ; steps/s = 1.63, tokens/s = 42668 (42668 target) ; Learning rate = 0.000519 ; Loss = 1.639776\n",
      "2024-11-29 13:50:10.963000: I runner.py:310] Step = 29100 ; steps/s = 1.62, tokens/s = 43147 (43147 target) ; Learning rate = 0.000518 ; Loss = 1.610621\n",
      "2024-11-29 13:51:12.943000: I runner.py:310] Step = 29200 ; steps/s = 1.61, tokens/s = 43099 (43099 target) ; Learning rate = 0.000517 ; Loss = 1.613640\n",
      "2024-11-29 13:52:14.919000: I runner.py:310] Step = 29300 ; steps/s = 1.61, tokens/s = 43093 (43093 target) ; Learning rate = 0.000516 ; Loss = 1.625538\n",
      "2024-11-29 13:53:16.331000: I runner.py:310] Step = 29400 ; steps/s = 1.63, tokens/s = 42684 (42684 target) ; Learning rate = 0.000515 ; Loss = 1.625878\n",
      "2024-11-29 13:54:18.271000: I runner.py:310] Step = 29500 ; steps/s = 1.61, tokens/s = 43132 (43132 target) ; Learning rate = 0.000515 ; Loss = 1.609419\n",
      "2024-11-29 13:55:20.209000: I runner.py:310] Step = 29600 ; steps/s = 1.61, tokens/s = 43148 (43148 target) ; Learning rate = 0.000514 ; Loss = 1.608487\n",
      "2024-11-29 13:56:22.136000: I runner.py:310] Step = 29700 ; steps/s = 1.61, tokens/s = 43128 (43128 target) ; Learning rate = 0.000513 ; Loss = 1.622807\n",
      "2024-11-29 13:57:23.450000: I runner.py:310] Step = 29800 ; steps/s = 1.63, tokens/s = 42747 (42747 target) ; Learning rate = 0.000512 ; Loss = 1.600954\n",
      "2024-11-29 13:58:25.460000: I runner.py:310] Step = 29900 ; steps/s = 1.61, tokens/s = 43082 (43082 target) ; Learning rate = 0.000511 ; Loss = 1.616882\n",
      "2024-11-29 13:59:27.428000: I runner.py:310] Step = 30000 ; steps/s = 1.61, tokens/s = 43110 (43110 target) ; Learning rate = 0.000510 ; Loss = 1.633082\n",
      "2024-11-29 13:59:29.245000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-30000\n",
      "2024-11-29 13:59:29.245000: I training.py:192] Running evaluation for step 30000\n",
      "2024-11-29 14:03:48.363000: I training.py:192] Evaluation result for step 30000: loss = 1.109827 ; perplexity = 3.033833\n",
      "2024-11-29 14:04:50.210000: I runner.py:310] Step = 30100 ; steps/s = 1.62, tokens/s = 43203 (43203 target) ; Learning rate = 0.000509 ; Loss = 1.631913\n",
      "2024-11-29 14:05:51.573000: I runner.py:310] Step = 30200 ; steps/s = 1.63, tokens/s = 42726 (42726 target) ; Learning rate = 0.000509 ; Loss = 1.596265\n",
      "2024-11-29 14:06:53.553000: I runner.py:310] Step = 30300 ; steps/s = 1.61, tokens/s = 43099 (43099 target) ; Learning rate = 0.000508 ; Loss = 1.618123\n",
      "2024-11-29 14:07:55.535000: I runner.py:310] Step = 30400 ; steps/s = 1.61, tokens/s = 43105 (43105 target) ; Learning rate = 0.000507 ; Loss = 1.625877\n",
      "2024-11-29 14:08:56.912000: I runner.py:310] Step = 30500 ; steps/s = 1.63, tokens/s = 42681 (42681 target) ; Learning rate = 0.000506 ; Loss = 1.610671\n",
      "2024-11-29 14:09:58.872000: I runner.py:310] Step = 30600 ; steps/s = 1.61, tokens/s = 43157 (43157 target) ; Learning rate = 0.000505 ; Loss = 1.611617\n",
      "2024-11-29 14:11:00.799000: I runner.py:310] Step = 30700 ; steps/s = 1.62, tokens/s = 43166 (43166 target) ; Learning rate = 0.000504 ; Loss = 1.615197\n",
      "2024-11-29 14:12:02.732000: I runner.py:310] Step = 30800 ; steps/s = 1.61, tokens/s = 43089 (43089 target) ; Learning rate = 0.000504 ; Loss = 1.612948\n",
      "2024-11-29 14:13:04.151000: I runner.py:310] Step = 30900 ; steps/s = 1.63, tokens/s = 42665 (42665 target) ; Learning rate = 0.000503 ; Loss = 1.610660\n",
      "2024-11-29 14:14:06.096000: I runner.py:310] Step = 31000 ; steps/s = 1.61, tokens/s = 43152 (43152 target) ; Learning rate = 0.000502 ; Loss = 1.604877\n",
      "2024-11-29 14:15:08.054000: I runner.py:310] Step = 31100 ; steps/s = 1.61, tokens/s = 43112 (43112 target) ; Learning rate = 0.000501 ; Loss = 1.611350\n",
      "2024-11-29 14:16:10.028000: I runner.py:310] Step = 31200 ; steps/s = 1.61, tokens/s = 43085 (43085 target) ; Learning rate = 0.000500 ; Loss = 1.615473\n",
      "2024-11-29 14:17:11.488000: I runner.py:310] Step = 31300 ; steps/s = 1.63, tokens/s = 42659 (42659 target) ; Learning rate = 0.000500 ; Loss = 1.606884\n",
      "2024-11-29 14:18:13.389000: I runner.py:310] Step = 31400 ; steps/s = 1.62, tokens/s = 43196 (43196 target) ; Learning rate = 0.000499 ; Loss = 1.604065\n",
      "2024-11-29 14:19:15.391000: I runner.py:310] Step = 31500 ; steps/s = 1.61, tokens/s = 43064 (43064 target) ; Learning rate = 0.000498 ; Loss = 1.605334\n",
      "2024-11-29 14:20:17.396000: I runner.py:310] Step = 31600 ; steps/s = 1.61, tokens/s = 43077 (43077 target) ; Learning rate = 0.000497 ; Loss = 1.608062\n",
      "2024-11-29 14:21:18.856000: I runner.py:310] Step = 31700 ; steps/s = 1.63, tokens/s = 42645 (42645 target) ; Learning rate = 0.000496 ; Loss = 1.607457\n",
      "2024-11-29 14:22:20.759000: I runner.py:310] Step = 31800 ; steps/s = 1.62, tokens/s = 43156 (43156 target) ; Learning rate = 0.000496 ; Loss = 1.612893\n",
      "2024-11-29 14:23:22.693000: I runner.py:310] Step = 31900 ; steps/s = 1.61, tokens/s = 43136 (43136 target) ; Learning rate = 0.000495 ; Loss = 1.607285\n",
      "2024-11-29 14:24:24.642000: I runner.py:310] Step = 32000 ; steps/s = 1.61, tokens/s = 43146 (43146 target) ; Learning rate = 0.000494 ; Loss = 1.607450\n",
      "2024-11-29 14:25:26.030000: I runner.py:310] Step = 32100 ; steps/s = 1.63, tokens/s = 42689 (42689 target) ; Learning rate = 0.000493 ; Loss = 1.606956\n",
      "2024-11-29 14:26:28.002000: I runner.py:310] Step = 32200 ; steps/s = 1.61, tokens/s = 43126 (43126 target) ; Learning rate = 0.000493 ; Loss = 1.602733\n",
      "2024-11-29 14:27:29.996000: I runner.py:310] Step = 32300 ; steps/s = 1.61, tokens/s = 43070 (43070 target) ; Learning rate = 0.000492 ; Loss = 1.601454\n",
      "2024-11-29 14:28:31.899000: I runner.py:310] Step = 32400 ; steps/s = 1.62, tokens/s = 43149 (43149 target) ; Learning rate = 0.000491 ; Loss = 1.610224\n",
      "2024-11-29 14:29:33.248000: I runner.py:310] Step = 32500 ; steps/s = 1.63, tokens/s = 42715 (42715 target) ; Learning rate = 0.000490 ; Loss = 1.591261\n",
      "2024-11-29 14:30:35.228000: I runner.py:310] Step = 32600 ; steps/s = 1.61, tokens/s = 43093 (43093 target) ; Learning rate = 0.000490 ; Loss = 1.600313\n",
      "2024-11-29 14:31:37.114000: I runner.py:310] Step = 32700 ; steps/s = 1.62, tokens/s = 43165 (43165 target) ; Learning rate = 0.000489 ; Loss = 1.612713\n",
      "2024-11-29 14:32:39.050000: I runner.py:310] Step = 32800 ; steps/s = 1.61, tokens/s = 43147 (43147 target) ; Learning rate = 0.000488 ; Loss = 1.612865\n",
      "2024-11-29 14:33:40.456000: I runner.py:310] Step = 32900 ; steps/s = 1.63, tokens/s = 42698 (42698 target) ; Learning rate = 0.000487 ; Loss = 1.613959\n",
      "2024-11-29 14:34:42.340000: I runner.py:310] Step = 33000 ; steps/s = 1.62, tokens/s = 43191 (43191 target) ; Learning rate = 0.000487 ; Loss = 1.592319\n",
      "2024-11-29 14:35:44.241000: I runner.py:310] Step = 33100 ; steps/s = 1.62, tokens/s = 43150 (43150 target) ; Learning rate = 0.000486 ; Loss = 1.599267\n",
      "2024-11-29 14:36:46.191000: I runner.py:310] Step = 33200 ; steps/s = 1.61, tokens/s = 43124 (43124 target) ; Learning rate = 0.000485 ; Loss = 1.596650\n",
      "2024-11-29 14:37:47.502000: I runner.py:310] Step = 33300 ; steps/s = 1.63, tokens/s = 42715 (42715 target) ; Learning rate = 0.000484 ; Loss = 1.614127\n",
      "2024-11-29 14:38:49.433000: I runner.py:310] Step = 33400 ; steps/s = 1.61, tokens/s = 43166 (43166 target) ; Learning rate = 0.000484 ; Loss = 1.594179\n",
      "2024-11-29 14:39:51.365000: I runner.py:310] Step = 33500 ; steps/s = 1.61, tokens/s = 43138 (43138 target) ; Learning rate = 0.000483 ; Loss = 1.595678\n",
      "2024-11-29 14:40:53.345000: I runner.py:310] Step = 33600 ; steps/s = 1.61, tokens/s = 43108 (43108 target) ; Learning rate = 0.000482 ; Loss = 1.602918\n",
      "2024-11-29 14:41:54.767000: I runner.py:310] Step = 33700 ; steps/s = 1.63, tokens/s = 42646 (42646 target) ; Learning rate = 0.000481 ; Loss = 1.589750\n",
      "2024-11-29 14:42:56.759000: I runner.py:310] Step = 33800 ; steps/s = 1.61, tokens/s = 43099 (43099 target) ; Learning rate = 0.000481 ; Loss = 1.594888\n",
      "2024-11-29 14:43:58.684000: I runner.py:310] Step = 33900 ; steps/s = 1.61, tokens/s = 43124 (43124 target) ; Learning rate = 0.000480 ; Loss = 1.614910\n",
      "2024-11-29 14:45:00.632000: I runner.py:310] Step = 34000 ; steps/s = 1.61, tokens/s = 43150 (43150 target) ; Learning rate = 0.000479 ; Loss = 1.616860\n",
      "2024-11-29 14:46:02.001000: I runner.py:310] Step = 34100 ; steps/s = 1.63, tokens/s = 42727 (42727 target) ; Learning rate = 0.000479 ; Loss = 1.583605\n",
      "2024-11-29 14:47:03.913000: I runner.py:310] Step = 34200 ; steps/s = 1.62, tokens/s = 43137 (43137 target) ; Learning rate = 0.000478 ; Loss = 1.596464\n",
      "2024-11-29 14:48:05.899000: I runner.py:310] Step = 34300 ; steps/s = 1.61, tokens/s = 43100 (43100 target) ; Learning rate = 0.000477 ; Loss = 1.609157\n",
      "2024-11-29 14:49:07.838000: I runner.py:310] Step = 34400 ; steps/s = 1.61, tokens/s = 43138 (43138 target) ; Learning rate = 0.000477 ; Loss = 1.603653\n",
      "2024-11-29 14:50:09.287000: I runner.py:310] Step = 34500 ; steps/s = 1.63, tokens/s = 42654 (42654 target) ; Learning rate = 0.000476 ; Loss = 1.586297\n",
      "2024-11-29 14:51:11.193000: I runner.py:310] Step = 34600 ; steps/s = 1.62, tokens/s = 43174 (43174 target) ; Learning rate = 0.000475 ; Loss = 1.600679\n",
      "2024-11-29 14:52:13.043000: I runner.py:310] Step = 34700 ; steps/s = 1.62, tokens/s = 43170 (43170 target) ; Learning rate = 0.000474 ; Loss = 1.618528\n",
      "2024-11-29 14:53:14.416000: I runner.py:310] Step = 34800 ; steps/s = 1.63, tokens/s = 42686 (42686 target) ; Learning rate = 0.000474 ; Loss = 1.594144\n",
      "2024-11-29 14:54:16.385000: I runner.py:310] Step = 34900 ; steps/s = 1.61, tokens/s = 43131 (43131 target) ; Learning rate = 0.000473 ; Loss = 1.585154\n",
      "2024-11-29 14:55:18.286000: I runner.py:310] Step = 35000 ; steps/s = 1.62, tokens/s = 43153 (43153 target) ; Learning rate = 0.000472 ; Loss = 1.595484\n",
      "2024-11-29 14:55:18.287000: I training.py:192] Running evaluation for step 35000\n",
      "2024-11-29 14:59:30.883000: I training.py:192] Evaluation result for step 35000: loss = 1.121616 ; perplexity = 3.069811\n",
      "2024-11-29 15:00:32.671000: I runner.py:310] Step = 35100 ; steps/s = 1.62, tokens/s = 43267 (43267 target) ; Learning rate = 0.000472 ; Loss = 1.607360\n",
      "2024-11-29 15:01:34.018000: I runner.py:310] Step = 35200 ; steps/s = 1.63, tokens/s = 42691 (42691 target) ; Learning rate = 0.000471 ; Loss = 1.592969\n",
      "2024-11-29 15:02:35.976000: I runner.py:310] Step = 35300 ; steps/s = 1.61, tokens/s = 43106 (43106 target) ; Learning rate = 0.000470 ; Loss = 1.581511\n",
      "2024-11-29 15:03:37.910000: I runner.py:310] Step = 35400 ; steps/s = 1.61, tokens/s = 43157 (43157 target) ; Learning rate = 0.000470 ; Loss = 1.602392\n",
      "2024-11-29 15:04:39.859000: I runner.py:310] Step = 35500 ; steps/s = 1.61, tokens/s = 43129 (43129 target) ; Learning rate = 0.000469 ; Loss = 1.590495\n",
      "2024-11-29 15:05:41.257000: I runner.py:310] Step = 35600 ; steps/s = 1.63, tokens/s = 42701 (42701 target) ; Learning rate = 0.000468 ; Loss = 1.593930\n",
      "2024-11-29 15:06:43.168000: I runner.py:310] Step = 35700 ; steps/s = 1.62, tokens/s = 43192 (43192 target) ; Learning rate = 0.000468 ; Loss = 1.591914\n",
      "2024-11-29 15:07:45.152000: I runner.py:310] Step = 35800 ; steps/s = 1.61, tokens/s = 43124 (43124 target) ; Learning rate = 0.000467 ; Loss = 1.591927\n",
      "2024-11-29 15:08:47.146000: I runner.py:310] Step = 35900 ; steps/s = 1.61, tokens/s = 43058 (43058 target) ; Learning rate = 0.000466 ; Loss = 1.595911\n",
      "2024-11-29 15:09:48.483000: I runner.py:310] Step = 36000 ; steps/s = 1.63, tokens/s = 42694 (42694 target) ; Learning rate = 0.000466 ; Loss = 1.578624\n",
      "2024-11-29 15:10:50.409000: I runner.py:310] Step = 36100 ; steps/s = 1.61, tokens/s = 43129 (43129 target) ; Learning rate = 0.000465 ; Loss = 1.585123\n",
      "2024-11-29 15:11:52.332000: I runner.py:310] Step = 36200 ; steps/s = 1.62, tokens/s = 43156 (43156 target) ; Learning rate = 0.000465 ; Loss = 1.581637\n",
      "2024-11-29 15:12:54.267000: I runner.py:310] Step = 36300 ; steps/s = 1.61, tokens/s = 43138 (43138 target) ; Learning rate = 0.000464 ; Loss = 1.607595\n",
      "2024-11-29 15:13:55.678000: I runner.py:310] Step = 36400 ; steps/s = 1.63, tokens/s = 42687 (42687 target) ; Learning rate = 0.000463 ; Loss = 1.600170\n",
      "2024-11-29 15:14:57.676000: I runner.py:310] Step = 36500 ; steps/s = 1.61, tokens/s = 43094 (43094 target) ; Learning rate = 0.000463 ; Loss = 1.586864\n",
      "2024-11-29 15:15:59.563000: I runner.py:310] Step = 36600 ; steps/s = 1.62, tokens/s = 43193 (43193 target) ; Learning rate = 0.000462 ; Loss = 1.587843\n",
      "2024-11-29 15:17:01.489000: I runner.py:310] Step = 36700 ; steps/s = 1.61, tokens/s = 43126 (43126 target) ; Learning rate = 0.000461 ; Loss = 1.603291\n",
      "2024-11-29 15:18:02.889000: I runner.py:310] Step = 36800 ; steps/s = 1.63, tokens/s = 42663 (42663 target) ; Learning rate = 0.000461 ; Loss = 1.588993\n",
      "2024-11-29 15:19:04.822000: I runner.py:310] Step = 36900 ; steps/s = 1.61, tokens/s = 43175 (43175 target) ; Learning rate = 0.000460 ; Loss = 1.586699\n",
      "2024-11-29 15:20:06.752000: I runner.py:310] Step = 37000 ; steps/s = 1.61, tokens/s = 43121 (43121 target) ; Learning rate = 0.000460 ; Loss = 1.574766\n",
      "2024-11-29 15:21:08.721000: I runner.py:310] Step = 37100 ; steps/s = 1.61, tokens/s = 43086 (43086 target) ; Learning rate = 0.000459 ; Loss = 1.596657\n",
      "2024-11-29 15:22:10.060000: I runner.py:310] Step = 37200 ; steps/s = 1.63, tokens/s = 42739 (42739 target) ; Learning rate = 0.000458 ; Loss = 1.593713\n",
      "2024-11-29 15:23:11.951000: I runner.py:310] Step = 37300 ; steps/s = 1.62, tokens/s = 43181 (43181 target) ; Learning rate = 0.000458 ; Loss = 1.576943\n",
      "2024-11-29 15:24:13.936000: I runner.py:310] Step = 37400 ; steps/s = 1.61, tokens/s = 43076 (43076 target) ; Learning rate = 0.000457 ; Loss = 1.576214\n",
      "2024-11-29 15:25:15.834000: I runner.py:310] Step = 37500 ; steps/s = 1.62, tokens/s = 43166 (43166 target) ; Learning rate = 0.000456 ; Loss = 1.578750\n",
      "2024-11-29 15:26:17.241000: I runner.py:310] Step = 37600 ; steps/s = 1.63, tokens/s = 42710 (42710 target) ; Learning rate = 0.000456 ; Loss = 1.593669\n",
      "2024-11-29 15:27:19.178000: I runner.py:310] Step = 37700 ; steps/s = 1.61, tokens/s = 43127 (43127 target) ; Learning rate = 0.000455 ; Loss = 1.578303\n",
      "2024-11-29 15:28:21.130000: I runner.py:310] Step = 37800 ; steps/s = 1.61, tokens/s = 43106 (43106 target) ; Learning rate = 0.000455 ; Loss = 1.588170\n",
      "2024-11-29 15:29:23.055000: I runner.py:310] Step = 37900 ; steps/s = 1.62, tokens/s = 43131 (43131 target) ; Learning rate = 0.000454 ; Loss = 1.583632\n",
      "2024-11-29 15:30:24.450000: I runner.py:310] Step = 38000 ; steps/s = 1.63, tokens/s = 42694 (42694 target) ; Learning rate = 0.000453 ; Loss = 1.574003\n",
      "2024-11-29 15:31:26.462000: I runner.py:310] Step = 38100 ; steps/s = 1.61, tokens/s = 43091 (43091 target) ; Learning rate = 0.000453 ; Loss = 1.589967\n",
      "2024-11-29 15:32:28.388000: I runner.py:310] Step = 38200 ; steps/s = 1.62, tokens/s = 43141 (43141 target) ; Learning rate = 0.000452 ; Loss = 1.598954\n",
      "2024-11-29 15:33:30.363000: I runner.py:310] Step = 38300 ; steps/s = 1.61, tokens/s = 43109 (43109 target) ; Learning rate = 0.000452 ; Loss = 1.583893\n",
      "2024-11-29 15:34:31.777000: I runner.py:310] Step = 38400 ; steps/s = 1.63, tokens/s = 42695 (42695 target) ; Learning rate = 0.000451 ; Loss = 1.592272\n",
      "2024-11-29 15:35:33.704000: I runner.py:310] Step = 38500 ; steps/s = 1.61, tokens/s = 43140 (43140 target) ; Learning rate = 0.000450 ; Loss = 1.576459\n",
      "2024-11-29 15:36:35.631000: I runner.py:310] Step = 38600 ; steps/s = 1.62, tokens/s = 43146 (43146 target) ; Learning rate = 0.000450 ; Loss = 1.581159\n",
      "2024-11-29 15:37:37.562000: I runner.py:310] Step = 38700 ; steps/s = 1.61, tokens/s = 43126 (43126 target) ; Learning rate = 0.000449 ; Loss = 1.594668\n",
      "2024-11-29 15:38:38.946000: I runner.py:310] Step = 38800 ; steps/s = 1.63, tokens/s = 42729 (42729 target) ; Learning rate = 0.000449 ; Loss = 1.580776\n",
      "2024-11-29 15:39:40.890000: I runner.py:310] Step = 38900 ; steps/s = 1.61, tokens/s = 43130 (43130 target) ; Learning rate = 0.000448 ; Loss = 1.577085\n",
      "2024-11-29 15:40:42.874000: I runner.py:310] Step = 39000 ; steps/s = 1.61, tokens/s = 43078 (43078 target) ; Learning rate = 0.000448 ; Loss = 1.578732\n",
      "2024-11-29 15:41:44.294000: I runner.py:310] Step = 39100 ; steps/s = 1.63, tokens/s = 42641 (42641 target) ; Learning rate = 0.000447 ; Loss = 1.609859\n",
      "2024-11-29 15:42:46.197000: I runner.py:310] Step = 39200 ; steps/s = 1.62, tokens/s = 43172 (43172 target) ; Learning rate = 0.000446 ; Loss = 1.590799\n",
      "2024-11-29 15:43:48.119000: I runner.py:310] Step = 39300 ; steps/s = 1.62, tokens/s = 43139 (43139 target) ; Learning rate = 0.000446 ; Loss = 1.579153\n",
      "2024-11-29 15:44:50.135000: I runner.py:310] Step = 39400 ; steps/s = 1.61, tokens/s = 43097 (43097 target) ; Learning rate = 0.000445 ; Loss = 1.576076\n",
      "2024-11-29 15:45:51.481000: I runner.py:310] Step = 39500 ; steps/s = 1.63, tokens/s = 42723 (42723 target) ; Learning rate = 0.000445 ; Loss = 1.576669\n",
      "2024-11-29 15:46:53.434000: I runner.py:310] Step = 39600 ; steps/s = 1.61, tokens/s = 43153 (43153 target) ; Learning rate = 0.000444 ; Loss = 1.578304\n",
      "2024-11-29 15:47:55.388000: I runner.py:310] Step = 39700 ; steps/s = 1.61, tokens/s = 43088 (43088 target) ; Learning rate = 0.000444 ; Loss = 1.567289\n",
      "2024-11-29 15:48:57.344000: I runner.py:310] Step = 39800 ; steps/s = 1.61, tokens/s = 43118 (43118 target) ; Learning rate = 0.000443 ; Loss = 1.584151\n",
      "2024-11-29 15:49:58.678000: I runner.py:310] Step = 39900 ; steps/s = 1.63, tokens/s = 42740 (42740 target) ; Learning rate = 0.000442 ; Loss = 1.586255\n",
      "2024-11-29 15:51:00.670000: I runner.py:310] Step = 40000 ; steps/s = 1.61, tokens/s = 43099 (43099 target) ; Learning rate = 0.000442 ; Loss = 1.583722\n",
      "2024-11-29 15:51:02.599000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-40000\n",
      "2024-11-29 15:51:02.599000: I training.py:192] Running evaluation for step 40000\n",
      "2024-11-29 15:55:21.957000: I training.py:192] Evaluation result for step 40000: loss = 1.137972 ; perplexity = 3.120433\n",
      "2024-11-29 15:56:23.778000: I runner.py:310] Step = 40100 ; steps/s = 1.62, tokens/s = 43229 (43229 target) ; Learning rate = 0.000441 ; Loss = 1.576013\n",
      "2024-11-29 15:57:25.724000: I runner.py:310] Step = 40200 ; steps/s = 1.61, tokens/s = 43125 (43125 target) ; Learning rate = 0.000441 ; Loss = 1.589549\n",
      "2024-11-29 15:58:27.120000: I runner.py:310] Step = 40300 ; steps/s = 1.63, tokens/s = 42688 (42688 target) ; Learning rate = 0.000440 ; Loss = 1.579072\n",
      "2024-11-29 15:59:29.077000: I runner.py:310] Step = 40400 ; steps/s = 1.61, tokens/s = 43123 (43123 target) ; Learning rate = 0.000440 ; Loss = 1.571671\n",
      "2024-11-29 16:00:31.016000: I runner.py:310] Step = 40500 ; steps/s = 1.61, tokens/s = 43149 (43149 target) ; Learning rate = 0.000439 ; Loss = 1.585534\n",
      "2024-11-29 16:01:33.017000: I runner.py:310] Step = 40600 ; steps/s = 1.61, tokens/s = 43065 (43065 target) ; Learning rate = 0.000439 ; Loss = 1.582855\n",
      "2024-11-29 16:02:34.469000: I runner.py:310] Step = 40700 ; steps/s = 1.63, tokens/s = 42654 (42654 target) ; Learning rate = 0.000438 ; Loss = 1.579780\n",
      "2024-11-29 16:03:36.421000: I runner.py:310] Step = 40800 ; steps/s = 1.61, tokens/s = 43116 (43116 target) ; Learning rate = 0.000438 ; Loss = 1.577676\n",
      "2024-11-29 16:04:38.329000: I runner.py:310] Step = 40900 ; steps/s = 1.62, tokens/s = 43159 (43159 target) ; Learning rate = 0.000437 ; Loss = 1.579430\n",
      "2024-11-29 16:05:40.282000: I runner.py:310] Step = 41000 ; steps/s = 1.61, tokens/s = 43128 (43128 target) ; Learning rate = 0.000437 ; Loss = 1.572059\n",
      "2024-11-29 16:06:41.712000: I runner.py:310] Step = 41100 ; steps/s = 1.63, tokens/s = 42659 (42659 target) ; Learning rate = 0.000436 ; Loss = 1.564280\n",
      "2024-11-29 16:07:43.647000: I runner.py:310] Step = 41200 ; steps/s = 1.61, tokens/s = 43138 (43138 target) ; Learning rate = 0.000435 ; Loss = 1.575874\n",
      "2024-11-29 16:08:45.612000: I runner.py:310] Step = 41300 ; steps/s = 1.61, tokens/s = 43126 (43126 target) ; Learning rate = 0.000435 ; Loss = 1.578208\n",
      "2024-11-29 16:09:47.597000: I runner.py:310] Step = 41400 ; steps/s = 1.61, tokens/s = 43089 (43089 target) ; Learning rate = 0.000434 ; Loss = 1.586883\n",
      "2024-11-29 16:10:48.996000: I runner.py:310] Step = 41500 ; steps/s = 1.63, tokens/s = 42682 (42682 target) ; Learning rate = 0.000434 ; Loss = 1.570555\n",
      "2024-11-29 16:11:50.997000: I runner.py:310] Step = 41600 ; steps/s = 1.61, tokens/s = 43095 (43095 target) ; Learning rate = 0.000433 ; Loss = 1.566320\n",
      "2024-11-29 16:12:52.954000: I runner.py:310] Step = 41700 ; steps/s = 1.61, tokens/s = 43143 (43143 target) ; Learning rate = 0.000433 ; Loss = 1.576449\n",
      "2024-11-29 16:13:54.982000: I runner.py:310] Step = 41800 ; steps/s = 1.61, tokens/s = 43086 (43086 target) ; Learning rate = 0.000432 ; Loss = 1.579581\n",
      "2024-11-29 16:14:56.393000: I runner.py:310] Step = 41900 ; steps/s = 1.63, tokens/s = 42638 (42638 target) ; Learning rate = 0.000432 ; Loss = 1.568539\n",
      "2024-11-29 16:15:58.333000: I runner.py:310] Step = 42000 ; steps/s = 1.61, tokens/s = 43120 (43120 target) ; Learning rate = 0.000431 ; Loss = 1.577265\n",
      "2024-11-29 16:17:00.285000: I runner.py:310] Step = 42100 ; steps/s = 1.61, tokens/s = 43135 (43135 target) ; Learning rate = 0.000431 ; Loss = 1.584598\n",
      "2024-11-29 16:18:02.272000: I runner.py:310] Step = 42200 ; steps/s = 1.61, tokens/s = 43096 (43096 target) ; Learning rate = 0.000430 ; Loss = 1.579288\n",
      "2024-11-29 16:19:03.620000: I runner.py:310] Step = 42300 ; steps/s = 1.63, tokens/s = 42737 (42737 target) ; Learning rate = 0.000430 ; Loss = 1.585468\n",
      "2024-11-29 16:20:05.537000: I runner.py:310] Step = 42400 ; steps/s = 1.62, tokens/s = 43135 (43135 target) ; Learning rate = 0.000429 ; Loss = 1.564979\n",
      "2024-11-29 16:21:07.496000: I runner.py:310] Step = 42500 ; steps/s = 1.61, tokens/s = 43090 (43090 target) ; Learning rate = 0.000429 ; Loss = 1.571086\n",
      "2024-11-29 16:22:09.466000: I runner.py:310] Step = 42600 ; steps/s = 1.61, tokens/s = 43143 (43143 target) ; Learning rate = 0.000428 ; Loss = 1.567985\n",
      "2024-11-29 16:23:10.946000: I runner.py:310] Step = 42700 ; steps/s = 1.63, tokens/s = 42622 (42622 target) ; Learning rate = 0.000428 ; Loss = 1.558159\n",
      "2024-11-29 16:24:12.850000: I runner.py:310] Step = 42800 ; steps/s = 1.62, tokens/s = 43147 (43147 target) ; Learning rate = 0.000427 ; Loss = 1.578774\n",
      "2024-11-29 16:25:14.758000: I runner.py:310] Step = 42900 ; steps/s = 1.62, tokens/s = 43134 (43134 target) ; Learning rate = 0.000427 ; Loss = 1.567519\n",
      "2024-11-29 16:26:16.692000: I runner.py:310] Step = 43000 ; steps/s = 1.61, tokens/s = 43183 (43183 target) ; Learning rate = 0.000426 ; Loss = 1.580662\n",
      "2024-11-29 16:27:18.087000: I runner.py:310] Step = 43100 ; steps/s = 1.63, tokens/s = 42661 (42661 target) ; Learning rate = 0.000426 ; Loss = 1.563018\n",
      "2024-11-29 16:28:20.057000: I runner.py:310] Step = 43200 ; steps/s = 1.61, tokens/s = 43125 (43125 target) ; Learning rate = 0.000425 ; Loss = 1.579917\n",
      "2024-11-29 16:29:22.027000: I runner.py:310] Step = 43300 ; steps/s = 1.61, tokens/s = 43119 (43119 target) ; Learning rate = 0.000425 ; Loss = 1.580710\n",
      "2024-11-29 16:30:23.634000: I runner.py:310] Step = 43400 ; steps/s = 1.62, tokens/s = 42815 (42815 target) ; Learning rate = 0.000424 ; Loss = 1.574083\n",
      "2024-11-29 16:31:25.348000: I runner.py:310] Step = 43500 ; steps/s = 1.62, tokens/s = 43015 (43015 target) ; Learning rate = 0.000424 ; Loss = 1.573907\n",
      "2024-11-29 16:32:27.294000: I runner.py:310] Step = 43600 ; steps/s = 1.61, tokens/s = 43135 (43135 target) ; Learning rate = 0.000423 ; Loss = 1.563075\n",
      "2024-11-29 16:33:29.283000: I runner.py:310] Step = 43700 ; steps/s = 1.61, tokens/s = 43067 (43067 target) ; Learning rate = 0.000423 ; Loss = 1.574552\n",
      "2024-11-29 16:34:30.679000: I runner.py:310] Step = 43800 ; steps/s = 1.63, tokens/s = 42713 (42713 target) ; Learning rate = 0.000422 ; Loss = 1.567154\n",
      "2024-11-29 16:35:32.641000: I runner.py:310] Step = 43900 ; steps/s = 1.61, tokens/s = 43121 (43121 target) ; Learning rate = 0.000422 ; Loss = 1.576722\n",
      "2024-11-29 16:36:34.625000: I runner.py:310] Step = 44000 ; steps/s = 1.61, tokens/s = 43121 (43121 target) ; Learning rate = 0.000421 ; Loss = 1.562949\n",
      "2024-11-29 16:37:36.602000: I runner.py:310] Step = 44100 ; steps/s = 1.61, tokens/s = 43102 (43102 target) ; Learning rate = 0.000421 ; Loss = 1.572990\n",
      "2024-11-29 16:38:37.928000: I runner.py:310] Step = 44200 ; steps/s = 1.63, tokens/s = 42723 (42723 target) ; Learning rate = 0.000420 ; Loss = 1.568017\n",
      "2024-11-29 16:39:39.818000: I runner.py:310] Step = 44300 ; steps/s = 1.62, tokens/s = 43142 (43142 target) ; Learning rate = 0.000420 ; Loss = 1.564963\n",
      "2024-11-29 16:40:41.737000: I runner.py:310] Step = 44400 ; steps/s = 1.62, tokens/s = 43161 (43161 target) ; Learning rate = 0.000419 ; Loss = 1.555792\n",
      "2024-11-29 16:41:43.657000: I runner.py:310] Step = 44500 ; steps/s = 1.62, tokens/s = 43173 (43173 target) ; Learning rate = 0.000419 ; Loss = 1.566327\n",
      "2024-11-29 16:42:45.040000: I runner.py:310] Step = 44600 ; steps/s = 1.63, tokens/s = 42667 (42667 target) ; Learning rate = 0.000419 ; Loss = 1.561474\n",
      "2024-11-29 16:43:46.990000: I runner.py:310] Step = 44700 ; steps/s = 1.61, tokens/s = 43167 (43167 target) ; Learning rate = 0.000418 ; Loss = 1.565639\n",
      "2024-11-29 16:44:48.911000: I runner.py:310] Step = 44800 ; steps/s = 1.62, tokens/s = 43114 (43114 target) ; Learning rate = 0.000418 ; Loss = 1.564944\n",
      "2024-11-29 16:45:50.934000: I runner.py:310] Step = 44900 ; steps/s = 1.61, tokens/s = 43113 (43113 target) ; Learning rate = 0.000417 ; Loss = 1.572659\n",
      "2024-11-29 16:46:52.361000: I runner.py:310] Step = 45000 ; steps/s = 1.63, tokens/s = 42635 (42635 target) ; Learning rate = 0.000417 ; Loss = 1.572726\n",
      "2024-11-29 16:46:52.362000: I training.py:192] Running evaluation for step 45000\n",
      "2024-11-29 16:51:00.395000: I training.py:192] Evaluation result for step 45000: loss = 1.140445 ; perplexity = 3.128161\n",
      "2024-11-29 16:52:02.163000: I runner.py:310] Step = 45100 ; steps/s = 1.62, tokens/s = 43296 (43296 target) ; Learning rate = 0.000416 ; Loss = 1.561065\n",
      "2024-11-29 16:53:04.101000: I runner.py:310] Step = 45200 ; steps/s = 1.61, tokens/s = 43109 (43109 target) ; Learning rate = 0.000416 ; Loss = 1.568194\n",
      "2024-11-29 16:54:06.106000: I runner.py:310] Step = 45300 ; steps/s = 1.61, tokens/s = 43075 (43075 target) ; Learning rate = 0.000415 ; Loss = 1.566985\n",
      "2024-11-29 16:55:07.534000: I runner.py:310] Step = 45400 ; steps/s = 1.63, tokens/s = 42687 (42687 target) ; Learning rate = 0.000415 ; Loss = 1.567322\n",
      "2024-11-29 16:56:09.500000: I runner.py:310] Step = 45500 ; steps/s = 1.61, tokens/s = 43122 (43122 target) ; Learning rate = 0.000414 ; Loss = 1.556540\n",
      "2024-11-29 16:57:11.487000: I runner.py:310] Step = 45600 ; steps/s = 1.61, tokens/s = 43110 (43110 target) ; Learning rate = 0.000414 ; Loss = 1.562528\n",
      "2024-11-29 16:58:13.476000: I runner.py:310] Step = 45700 ; steps/s = 1.61, tokens/s = 43068 (43068 target) ; Learning rate = 0.000413 ; Loss = 1.569245\n",
      "2024-11-29 16:59:14.846000: I runner.py:310] Step = 45800 ; steps/s = 1.63, tokens/s = 42695 (42695 target) ; Learning rate = 0.000413 ; Loss = 1.562565\n",
      "2024-11-29 17:00:16.816000: I runner.py:310] Step = 45900 ; steps/s = 1.61, tokens/s = 43089 (43089 target) ; Learning rate = 0.000413 ; Loss = 1.568617\n",
      "2024-11-29 17:01:18.816000: I runner.py:310] Step = 46000 ; steps/s = 1.61, tokens/s = 43112 (43112 target) ; Learning rate = 0.000412 ; Loss = 1.569206\n",
      "2024-11-29 17:02:20.721000: I runner.py:310] Step = 46100 ; steps/s = 1.62, tokens/s = 43149 (43149 target) ; Learning rate = 0.000412 ; Loss = 1.571920\n",
      "2024-11-29 17:03:22.119000: I runner.py:310] Step = 46200 ; steps/s = 1.63, tokens/s = 42709 (42709 target) ; Learning rate = 0.000411 ; Loss = 1.557813\n",
      "2024-11-29 17:04:24.028000: I runner.py:310] Step = 46300 ; steps/s = 1.62, tokens/s = 43159 (43159 target) ; Learning rate = 0.000411 ; Loss = 1.564266\n",
      "2024-11-29 17:05:25.995000: I runner.py:310] Step = 46400 ; steps/s = 1.61, tokens/s = 43099 (43099 target) ; Learning rate = 0.000410 ; Loss = 1.568846\n",
      "2024-11-29 17:06:27.909000: I runner.py:310] Step = 46500 ; steps/s = 1.62, tokens/s = 43148 (43148 target) ; Learning rate = 0.000410 ; Loss = 1.568925\n",
      "2024-11-29 17:07:29.397000: I runner.py:310] Step = 46600 ; steps/s = 1.63, tokens/s = 42638 (42638 target) ; Learning rate = 0.000409 ; Loss = 1.549754\n",
      "2024-11-29 17:08:31.418000: I runner.py:310] Step = 46700 ; steps/s = 1.61, tokens/s = 43083 (43083 target) ; Learning rate = 0.000409 ; Loss = 1.557471\n",
      "2024-11-29 17:09:33.364000: I runner.py:310] Step = 46800 ; steps/s = 1.61, tokens/s = 43124 (43124 target) ; Learning rate = 0.000409 ; Loss = 1.566585\n",
      "2024-11-29 17:10:35.328000: I runner.py:310] Step = 46900 ; steps/s = 1.61, tokens/s = 43104 (43104 target) ; Learning rate = 0.000408 ; Loss = 1.568674\n",
      "2024-11-29 17:11:36.744000: I runner.py:310] Step = 47000 ; steps/s = 1.63, tokens/s = 42669 (42669 target) ; Learning rate = 0.000408 ; Loss = 1.577005\n",
      "2024-11-29 17:12:38.717000: I runner.py:310] Step = 47100 ; steps/s = 1.61, tokens/s = 43109 (43109 target) ; Learning rate = 0.000407 ; Loss = 1.557539\n",
      "2024-11-29 17:13:40.691000: I runner.py:310] Step = 47200 ; steps/s = 1.61, tokens/s = 43106 (43106 target) ; Learning rate = 0.000407 ; Loss = 1.558041\n",
      "2024-11-29 17:14:42.697000: I runner.py:310] Step = 47300 ; steps/s = 1.61, tokens/s = 43088 (43088 target) ; Learning rate = 0.000406 ; Loss = 1.559525\n",
      "2024-11-29 17:15:44.149000: I runner.py:310] Step = 47400 ; steps/s = 1.63, tokens/s = 42662 (42662 target) ; Learning rate = 0.000406 ; Loss = 1.546543\n",
      "2024-11-29 17:16:46.114000: I runner.py:310] Step = 47500 ; steps/s = 1.61, tokens/s = 43096 (43096 target) ; Learning rate = 0.000406 ; Loss = 1.561441\n",
      "2024-11-29 17:17:48.090000: I runner.py:310] Step = 47600 ; steps/s = 1.61, tokens/s = 43125 (43125 target) ; Learning rate = 0.000405 ; Loss = 1.567567\n",
      "2024-11-29 17:18:49.858000: I runner.py:310] Step = 47700 ; steps/s = 1.62, tokens/s = 42928 (42928 target) ; Learning rate = 0.000405 ; Loss = 1.567170\n",
      "2024-11-29 17:19:51.472000: I runner.py:310] Step = 47800 ; steps/s = 1.62, tokens/s = 42858 (42858 target) ; Learning rate = 0.000404 ; Loss = 1.564466\n",
      "2024-11-29 17:20:53.439000: I runner.py:310] Step = 47900 ; steps/s = 1.61, tokens/s = 43110 (43110 target) ; Learning rate = 0.000404 ; Loss = 1.561061\n",
      "2024-11-29 17:21:55.384000: I runner.py:310] Step = 48000 ; steps/s = 1.61, tokens/s = 43129 (43129 target) ; Learning rate = 0.000403 ; Loss = 1.561946\n",
      "2024-11-29 17:22:56.785000: I runner.py:310] Step = 48100 ; steps/s = 1.63, tokens/s = 42680 (42680 target) ; Learning rate = 0.000403 ; Loss = 1.562275\n",
      "2024-11-29 17:23:58.763000: I runner.py:310] Step = 48200 ; steps/s = 1.61, tokens/s = 43131 (43131 target) ; Learning rate = 0.000403 ; Loss = 1.562191\n",
      "2024-11-29 17:25:00.686000: I runner.py:310] Step = 48300 ; steps/s = 1.62, tokens/s = 43153 (43153 target) ; Learning rate = 0.000402 ; Loss = 1.560283\n",
      "2024-11-29 17:26:02.670000: I runner.py:310] Step = 48400 ; steps/s = 1.61, tokens/s = 43083 (43083 target) ; Learning rate = 0.000402 ; Loss = 1.557517\n",
      "2024-11-29 17:27:04.101000: I runner.py:310] Step = 48500 ; steps/s = 1.63, tokens/s = 42648 (42648 target) ; Learning rate = 0.000401 ; Loss = 1.555653\n",
      "2024-11-29 17:28:06.129000: I runner.py:310] Step = 48600 ; steps/s = 1.61, tokens/s = 43040 (43040 target) ; Learning rate = 0.000401 ; Loss = 1.552422\n",
      "2024-11-29 17:29:08.128000: I runner.py:310] Step = 48700 ; steps/s = 1.61, tokens/s = 43090 (43090 target) ; Learning rate = 0.000401 ; Loss = 1.562867\n",
      "2024-11-29 17:30:10.125000: I runner.py:310] Step = 48800 ; steps/s = 1.61, tokens/s = 43108 (43108 target) ; Learning rate = 0.000400 ; Loss = 1.562322\n",
      "2024-11-29 17:31:11.600000: I runner.py:310] Step = 48900 ; steps/s = 1.63, tokens/s = 42666 (42666 target) ; Learning rate = 0.000400 ; Loss = 1.556971\n",
      "2024-11-29 17:32:13.525000: I runner.py:310] Step = 49000 ; steps/s = 1.62, tokens/s = 43124 (43124 target) ; Learning rate = 0.000399 ; Loss = 1.552296\n",
      "2024-11-29 17:33:15.529000: I runner.py:310] Step = 49100 ; steps/s = 1.61, tokens/s = 43114 (43114 target) ; Learning rate = 0.000399 ; Loss = 1.572106\n",
      "2024-11-29 17:34:17.459000: I runner.py:310] Step = 49200 ; steps/s = 1.61, tokens/s = 43106 (43106 target) ; Learning rate = 0.000398 ; Loss = 1.564640\n",
      "2024-11-29 17:35:18.874000: I runner.py:310] Step = 49300 ; steps/s = 1.63, tokens/s = 42692 (42692 target) ; Learning rate = 0.000398 ; Loss = 1.553009\n",
      "2024-11-29 17:36:20.768000: I runner.py:310] Step = 49400 ; steps/s = 1.62, tokens/s = 43164 (43164 target) ; Learning rate = 0.000398 ; Loss = 1.559383\n",
      "2024-11-29 17:37:22.767000: I runner.py:310] Step = 49500 ; steps/s = 1.61, tokens/s = 43116 (43116 target) ; Learning rate = 0.000397 ; Loss = 1.565469\n",
      "2024-11-29 17:38:24.706000: I runner.py:310] Step = 49600 ; steps/s = 1.61, tokens/s = 43121 (43121 target) ; Learning rate = 0.000397 ; Loss = 1.556005\n",
      "2024-11-29 17:39:26.090000: I runner.py:310] Step = 49700 ; steps/s = 1.63, tokens/s = 42697 (42697 target) ; Learning rate = 0.000396 ; Loss = 1.552704\n",
      "2024-11-29 17:40:28.060000: I runner.py:310] Step = 49800 ; steps/s = 1.61, tokens/s = 43142 (43142 target) ; Learning rate = 0.000396 ; Loss = 1.554184\n",
      "2024-11-29 17:41:30.053000: I runner.py:310] Step = 49900 ; steps/s = 1.61, tokens/s = 43081 (43081 target) ; Learning rate = 0.000396 ; Loss = 1.556763\n",
      "2024-11-29 17:42:31.974000: I runner.py:310] Step = 50000 ; steps/s = 1.62, tokens/s = 43121 (43121 target) ; Learning rate = 0.000395 ; Loss = 1.556681\n",
      "2024-11-29 17:42:33.966000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-50000\n",
      "2024-11-29 17:42:33.966000: I training.py:192] Running evaluation for step 50000\n",
      "2024-11-29 17:46:55.942000: I training.py:192] Evaluation result for step 50000: loss = 1.149526 ; perplexity = 3.156695\n",
      "2024-11-29 17:47:57.177000: I runner.py:310] Step = 50100 ; steps/s = 1.63, tokens/s = 42805 (42805 target) ; Learning rate = 0.000395 ; Loss = 1.542546\n",
      "2024-11-29 17:48:59.154000: I runner.py:310] Step = 50200 ; steps/s = 1.61, tokens/s = 43112 (43112 target) ; Learning rate = 0.000394 ; Loss = 1.554909\n",
      "2024-11-29 17:50:01.190000: I runner.py:310] Step = 50300 ; steps/s = 1.61, tokens/s = 43060 (43060 target) ; Learning rate = 0.000394 ; Loss = 1.557362\n",
      "2024-11-29 17:51:03.128000: I runner.py:310] Step = 50400 ; steps/s = 1.61, tokens/s = 43117 (43117 target) ; Learning rate = 0.000394 ; Loss = 1.559109\n",
      "2024-11-29 17:52:04.511000: I runner.py:310] Step = 50500 ; steps/s = 1.63, tokens/s = 42718 (42718 target) ; Learning rate = 0.000393 ; Loss = 1.555408\n",
      "2024-11-29 17:53:06.469000: I runner.py:310] Step = 50600 ; steps/s = 1.61, tokens/s = 43108 (43108 target) ; Learning rate = 0.000393 ; Loss = 1.555976\n",
      "2024-11-29 17:54:08.477000: I runner.py:310] Step = 50700 ; steps/s = 1.61, tokens/s = 43067 (43067 target) ; Learning rate = 0.000393 ; Loss = 1.559491\n",
      "2024-11-29 17:55:10.489000: I runner.py:310] Step = 50800 ; steps/s = 1.61, tokens/s = 43090 (43090 target) ; Learning rate = 0.000392 ; Loss = 1.565811\n",
      "2024-11-29 17:56:11.947000: I runner.py:310] Step = 50900 ; steps/s = 1.63, tokens/s = 42682 (42682 target) ; Learning rate = 0.000392 ; Loss = 1.566369\n",
      "2024-11-29 17:57:13.934000: I runner.py:310] Step = 51000 ; steps/s = 1.61, tokens/s = 43094 (43094 target) ; Learning rate = 0.000391 ; Loss = 1.552129\n",
      "2024-11-29 17:58:15.943000: I runner.py:310] Step = 51100 ; steps/s = 1.61, tokens/s = 43073 (43073 target) ; Learning rate = 0.000391 ; Loss = 1.550660\n",
      "2024-11-29 17:59:17.911000: I runner.py:310] Step = 51200 ; steps/s = 1.61, tokens/s = 43116 (43116 target) ; Learning rate = 0.000391 ; Loss = 1.551717\n",
      "2024-11-29 18:00:19.294000: I runner.py:310] Step = 51300 ; steps/s = 1.63, tokens/s = 42679 (42679 target) ; Learning rate = 0.000390 ; Loss = 1.547034\n",
      "2024-11-29 18:01:21.265000: I runner.py:310] Step = 51400 ; steps/s = 1.61, tokens/s = 43125 (43125 target) ; Learning rate = 0.000390 ; Loss = 1.555299\n",
      "2024-11-29 18:02:23.301000: I runner.py:310] Step = 51500 ; steps/s = 1.61, tokens/s = 43074 (43074 target) ; Learning rate = 0.000389 ; Loss = 1.560505\n",
      "2024-11-29 18:03:25.327000: I runner.py:310] Step = 51600 ; steps/s = 1.61, tokens/s = 43057 (43057 target) ; Learning rate = 0.000389 ; Loss = 1.563466\n",
      "2024-11-29 18:04:26.758000: I runner.py:310] Step = 51700 ; steps/s = 1.63, tokens/s = 42659 (42659 target) ; Learning rate = 0.000389 ; Loss = 1.542629\n",
      "2024-11-29 18:05:28.743000: I runner.py:310] Step = 51800 ; steps/s = 1.61, tokens/s = 43134 (43134 target) ; Learning rate = 0.000388 ; Loss = 1.549719\n",
      "2024-11-29 18:06:30.786000: I runner.py:310] Step = 51900 ; steps/s = 1.61, tokens/s = 43048 (43048 target) ; Learning rate = 0.000388 ; Loss = 1.551920\n",
      "2024-11-29 18:07:32.755000: I runner.py:310] Step = 52000 ; steps/s = 1.61, tokens/s = 42989 (42989 target) ; Learning rate = 0.000388 ; Loss = 1.569074\n",
      "2024-11-29 18:08:34.251000: I runner.py:310] Step = 52100 ; steps/s = 1.63, tokens/s = 42700 (42700 target) ; Learning rate = 0.000387 ; Loss = 1.544339\n",
      "2024-11-29 18:09:36.228000: I runner.py:310] Step = 52200 ; steps/s = 1.61, tokens/s = 43086 (43086 target) ; Learning rate = 0.000387 ; Loss = 1.567624\n",
      "2024-11-29 18:10:38.263000: I runner.py:310] Step = 52300 ; steps/s = 1.61, tokens/s = 43112 (43112 target) ; Learning rate = 0.000386 ; Loss = 1.562226\n",
      "2024-11-29 18:11:39.727000: I runner.py:310] Step = 52400 ; steps/s = 1.63, tokens/s = 42642 (42642 target) ; Learning rate = 0.000386 ; Loss = 1.549707\n",
      "2024-11-29 18:12:41.709000: I runner.py:310] Step = 52500 ; steps/s = 1.61, tokens/s = 43107 (43107 target) ; Learning rate = 0.000386 ; Loss = 1.551288\n",
      "2024-11-29 18:13:43.716000: I runner.py:310] Step = 52600 ; steps/s = 1.61, tokens/s = 43100 (43100 target) ; Learning rate = 0.000385 ; Loss = 1.553086\n",
      "2024-11-29 18:14:45.707000: I runner.py:310] Step = 52700 ; steps/s = 1.61, tokens/s = 43091 (43091 target) ; Learning rate = 0.000385 ; Loss = 1.557012\n",
      "2024-11-29 18:15:47.151000: I runner.py:310] Step = 52800 ; steps/s = 1.63, tokens/s = 42639 (42639 target) ; Learning rate = 0.000385 ; Loss = 1.552503\n",
      "2024-11-29 18:16:49.125000: I runner.py:310] Step = 52900 ; steps/s = 1.61, tokens/s = 43118 (43118 target) ; Learning rate = 0.000384 ; Loss = 1.556159\n",
      "2024-11-29 18:17:51.141000: I runner.py:310] Step = 53000 ; steps/s = 1.61, tokens/s = 43109 (43109 target) ; Learning rate = 0.000384 ; Loss = 1.545163\n",
      "2024-11-29 18:18:53.173000: I runner.py:310] Step = 53100 ; steps/s = 1.61, tokens/s = 43059 (43059 target) ; Learning rate = 0.000384 ; Loss = 1.554245\n",
      "2024-11-29 18:19:54.584000: I runner.py:310] Step = 53200 ; steps/s = 1.63, tokens/s = 42666 (42666 target) ; Learning rate = 0.000383 ; Loss = 1.553214\n",
      "2024-11-29 18:20:56.545000: I runner.py:310] Step = 53300 ; steps/s = 1.61, tokens/s = 43147 (43147 target) ; Learning rate = 0.000383 ; Loss = 1.547109\n",
      "2024-11-29 18:21:58.544000: I runner.py:310] Step = 53400 ; steps/s = 1.61, tokens/s = 43105 (43105 target) ; Learning rate = 0.000382 ; Loss = 1.546960\n",
      "2024-11-29 18:23:00.523000: I runner.py:310] Step = 53500 ; steps/s = 1.61, tokens/s = 43089 (43089 target) ; Learning rate = 0.000382 ; Loss = 1.549534\n",
      "2024-11-29 18:24:01.910000: I runner.py:310] Step = 53600 ; steps/s = 1.63, tokens/s = 42661 (42661 target) ; Learning rate = 0.000382 ; Loss = 1.548426\n",
      "2024-11-29 18:25:03.921000: I runner.py:310] Step = 53700 ; steps/s = 1.61, tokens/s = 43091 (43091 target) ; Learning rate = 0.000381 ; Loss = 1.550027\n",
      "2024-11-29 18:26:05.911000: I runner.py:310] Step = 53800 ; steps/s = 1.61, tokens/s = 43106 (43106 target) ; Learning rate = 0.000381 ; Loss = 1.543883\n",
      "2024-11-29 18:27:07.832000: I runner.py:310] Step = 53900 ; steps/s = 1.62, tokens/s = 43132 (43132 target) ; Learning rate = 0.000381 ; Loss = 1.547184\n",
      "2024-11-29 18:28:09.274000: I runner.py:310] Step = 54000 ; steps/s = 1.63, tokens/s = 42659 (42659 target) ; Learning rate = 0.000380 ; Loss = 1.546405\n",
      "2024-11-29 18:29:11.180000: I runner.py:310] Step = 54100 ; steps/s = 1.62, tokens/s = 43170 (43170 target) ; Learning rate = 0.000380 ; Loss = 1.553044\n",
      "2024-11-29 18:30:13.126000: I runner.py:310] Step = 54200 ; steps/s = 1.61, tokens/s = 43119 (43119 target) ; Learning rate = 0.000380 ; Loss = 1.554453\n",
      "2024-11-29 18:31:15.057000: I runner.py:310] Step = 54300 ; steps/s = 1.61, tokens/s = 43132 (43132 target) ; Learning rate = 0.000379 ; Loss = 1.546149\n",
      "2024-11-29 18:32:16.460000: I runner.py:310] Step = 54400 ; steps/s = 1.63, tokens/s = 42644 (42644 target) ; Learning rate = 0.000379 ; Loss = 1.544771\n",
      "2024-11-29 18:33:18.449000: I runner.py:310] Step = 54500 ; steps/s = 1.61, tokens/s = 43128 (43128 target) ; Learning rate = 0.000379 ; Loss = 1.555815\n",
      "2024-11-29 18:34:20.403000: I runner.py:310] Step = 54600 ; steps/s = 1.61, tokens/s = 43097 (43097 target) ; Learning rate = 0.000378 ; Loss = 1.556310\n",
      "2024-11-29 18:35:22.399000: I runner.py:310] Step = 54700 ; steps/s = 1.61, tokens/s = 43124 (43124 target) ; Learning rate = 0.000378 ; Loss = 1.553682\n",
      "2024-11-29 18:36:23.797000: I runner.py:310] Step = 54800 ; steps/s = 1.63, tokens/s = 42674 (42674 target) ; Learning rate = 0.000378 ; Loss = 1.533760\n",
      "2024-11-29 18:37:25.794000: I runner.py:310] Step = 54900 ; steps/s = 1.61, tokens/s = 43099 (43099 target) ; Learning rate = 0.000377 ; Loss = 1.546678\n",
      "2024-11-29 18:38:27.814000: I runner.py:310] Step = 55000 ; steps/s = 1.61, tokens/s = 43076 (43076 target) ; Learning rate = 0.000377 ; Loss = 1.556905\n",
      "2024-11-29 18:38:27.816000: I training.py:192] Running evaluation for step 55000\n",
      "2024-11-29 18:42:38.186000: I training.py:192] Evaluation result for step 55000: loss = 1.159899 ; perplexity = 3.189610\n",
      "2024-11-29 18:43:39.994000: I runner.py:310] Step = 55100 ; steps/s = 1.62, tokens/s = 43222 (43222 target) ; Learning rate = 0.000377 ; Loss = 1.555152\n",
      "2024-11-29 18:44:41.404000: I runner.py:310] Step = 55200 ; steps/s = 1.63, tokens/s = 42697 (42697 target) ; Learning rate = 0.000376 ; Loss = 1.539310\n",
      "2024-11-29 18:45:43.368000: I runner.py:310] Step = 55300 ; steps/s = 1.61, tokens/s = 43147 (43147 target) ; Learning rate = 0.000376 ; Loss = 1.537114\n",
      "2024-11-29 18:46:45.299000: I runner.py:310] Step = 55400 ; steps/s = 1.61, tokens/s = 43120 (43120 target) ; Learning rate = 0.000376 ; Loss = 1.541631\n",
      "2024-11-29 18:47:47.283000: I runner.py:310] Step = 55500 ; steps/s = 1.61, tokens/s = 43081 (43081 target) ; Learning rate = 0.000375 ; Loss = 1.551715\n",
      "2024-11-29 18:48:48.643000: I runner.py:310] Step = 55600 ; steps/s = 1.63, tokens/s = 42717 (42717 target) ; Learning rate = 0.000375 ; Loss = 1.539118\n",
      "2024-11-29 18:49:50.535000: I runner.py:310] Step = 55700 ; steps/s = 1.62, tokens/s = 43156 (43156 target) ; Learning rate = 0.000375 ; Loss = 1.552050\n",
      "2024-11-29 18:50:52.523000: I runner.py:310] Step = 55800 ; steps/s = 1.61, tokens/s = 43097 (43097 target) ; Learning rate = 0.000374 ; Loss = 1.547287\n",
      "2024-11-29 18:51:54.487000: I runner.py:310] Step = 55900 ; steps/s = 1.61, tokens/s = 43129 (43129 target) ; Learning rate = 0.000374 ; Loss = 1.546970\n",
      "2024-11-29 18:52:55.819000: I runner.py:310] Step = 56000 ; steps/s = 1.63, tokens/s = 42734 (42734 target) ; Learning rate = 0.000374 ; Loss = 1.545104\n",
      "2024-11-29 18:53:57.739000: I runner.py:310] Step = 56100 ; steps/s = 1.62, tokens/s = 43130 (43130 target) ; Learning rate = 0.000373 ; Loss = 1.538424\n",
      "2024-11-29 18:54:59.681000: I runner.py:310] Step = 56200 ; steps/s = 1.61, tokens/s = 43112 (43112 target) ; Learning rate = 0.000373 ; Loss = 1.548956\n",
      "2024-11-29 18:56:01.648000: I runner.py:310] Step = 56300 ; steps/s = 1.61, tokens/s = 43136 (43136 target) ; Learning rate = 0.000373 ; Loss = 1.545871\n",
      "2024-11-29 18:57:02.960000: I runner.py:310] Step = 56400 ; steps/s = 1.63, tokens/s = 42760 (42760 target) ; Learning rate = 0.000372 ; Loss = 1.536896\n",
      "2024-11-29 18:58:04.885000: I runner.py:310] Step = 56500 ; steps/s = 1.61, tokens/s = 43153 (43153 target) ; Learning rate = 0.000372 ; Loss = 1.545585\n",
      "2024-11-29 18:59:06.805000: I runner.py:310] Step = 56600 ; steps/s = 1.62, tokens/s = 43111 (43111 target) ; Learning rate = 0.000372 ; Loss = 1.544832\n",
      "2024-11-29 19:00:08.182000: I runner.py:310] Step = 56700 ; steps/s = 1.63, tokens/s = 42709 (42709 target) ; Learning rate = 0.000371 ; Loss = 1.542754\n",
      "2024-11-29 19:01:10.031000: I runner.py:310] Step = 56800 ; steps/s = 1.62, tokens/s = 43209 (43209 target) ; Learning rate = 0.000371 ; Loss = 1.540103\n",
      "2024-11-29 19:02:11.937000: I runner.py:310] Step = 56900 ; steps/s = 1.62, tokens/s = 43163 (43163 target) ; Learning rate = 0.000371 ; Loss = 1.543758\n",
      "2024-11-29 19:03:13.809000: I runner.py:310] Step = 57000 ; steps/s = 1.62, tokens/s = 43217 (43217 target) ; Learning rate = 0.000370 ; Loss = 1.542808\n",
      "2024-11-29 19:04:15.196000: I runner.py:310] Step = 57100 ; steps/s = 1.63, tokens/s = 42655 (42655 target) ; Learning rate = 0.000370 ; Loss = 1.548070\n",
      "2024-11-29 19:05:17.145000: I runner.py:310] Step = 57200 ; steps/s = 1.61, tokens/s = 43124 (43124 target) ; Learning rate = 0.000370 ; Loss = 1.544230\n",
      "2024-11-29 19:06:19.083000: I runner.py:310] Step = 57300 ; steps/s = 1.61, tokens/s = 43134 (43134 target) ; Learning rate = 0.000369 ; Loss = 1.537744\n",
      "2024-11-29 19:07:21.000000: I runner.py:310] Step = 57400 ; steps/s = 1.62, tokens/s = 43152 (43152 target) ; Learning rate = 0.000369 ; Loss = 1.547053\n",
      "2024-11-29 19:08:22.400000: I runner.py:310] Step = 57500 ; steps/s = 1.63, tokens/s = 42709 (42709 target) ; Learning rate = 0.000369 ; Loss = 1.543302\n",
      "2024-11-29 19:09:24.454000: I runner.py:310] Step = 57600 ; steps/s = 1.61, tokens/s = 43057 (43057 target) ; Learning rate = 0.000368 ; Loss = 1.542292\n",
      "2024-11-29 19:10:26.418000: I runner.py:310] Step = 57700 ; steps/s = 1.61, tokens/s = 43136 (43136 target) ; Learning rate = 0.000368 ; Loss = 1.544161\n",
      "2024-11-29 19:11:28.375000: I runner.py:310] Step = 57800 ; steps/s = 1.61, tokens/s = 43084 (43084 target) ; Learning rate = 0.000368 ; Loss = 1.550601\n",
      "2024-11-29 19:12:29.760000: I runner.py:310] Step = 57900 ; steps/s = 1.63, tokens/s = 42679 (42679 target) ; Learning rate = 0.000367 ; Loss = 1.540796\n",
      "2024-11-29 19:13:31.705000: I runner.py:310] Step = 58000 ; steps/s = 1.61, tokens/s = 43137 (43137 target) ; Learning rate = 0.000367 ; Loss = 1.539581\n",
      "2024-11-29 19:14:33.691000: I runner.py:310] Step = 58100 ; steps/s = 1.61, tokens/s = 43107 (43107 target) ; Learning rate = 0.000367 ; Loss = 1.545876\n",
      "2024-11-29 19:15:35.640000: I runner.py:310] Step = 58200 ; steps/s = 1.61, tokens/s = 43113 (43113 target) ; Learning rate = 0.000366 ; Loss = 1.550043\n",
      "2024-11-29 19:16:36.938000: I runner.py:310] Step = 58300 ; steps/s = 1.63, tokens/s = 42770 (42770 target) ; Learning rate = 0.000366 ; Loss = 1.549291\n",
      "2024-11-29 19:17:38.889000: I runner.py:310] Step = 58400 ; steps/s = 1.61, tokens/s = 43113 (43113 target) ; Learning rate = 0.000366 ; Loss = 1.541981\n",
      "2024-11-29 19:18:40.913000: I runner.py:310] Step = 58500 ; steps/s = 1.61, tokens/s = 43095 (43095 target) ; Learning rate = 0.000365 ; Loss = 1.540181\n",
      "2024-11-29 19:19:42.911000: I runner.py:310] Step = 58600 ; steps/s = 1.61, tokens/s = 43080 (43080 target) ; Learning rate = 0.000365 ; Loss = 1.544000\n",
      "2024-11-29 19:20:44.280000: I runner.py:310] Step = 58700 ; steps/s = 1.63, tokens/s = 42701 (42701 target) ; Learning rate = 0.000365 ; Loss = 1.549446\n",
      "2024-11-29 19:21:46.236000: I runner.py:310] Step = 58800 ; steps/s = 1.61, tokens/s = 43139 (43139 target) ; Learning rate = 0.000365 ; Loss = 1.530026\n",
      "2024-11-29 19:22:48.178000: I runner.py:310] Step = 58900 ; steps/s = 1.61, tokens/s = 43126 (43126 target) ; Learning rate = 0.000364 ; Loss = 1.541747\n",
      "2024-11-29 19:23:50.128000: I runner.py:310] Step = 59000 ; steps/s = 1.61, tokens/s = 43104 (43104 target) ; Learning rate = 0.000364 ; Loss = 1.546838\n",
      "2024-11-29 19:24:51.528000: I runner.py:310] Step = 59100 ; steps/s = 1.63, tokens/s = 42693 (42693 target) ; Learning rate = 0.000364 ; Loss = 1.544719\n",
      "2024-11-29 19:25:53.470000: I runner.py:310] Step = 59200 ; steps/s = 1.61, tokens/s = 43141 (43141 target) ; Learning rate = 0.000363 ; Loss = 1.546208\n",
      "2024-11-29 19:26:55.406000: I runner.py:310] Step = 59300 ; steps/s = 1.61, tokens/s = 43146 (43146 target) ; Learning rate = 0.000363 ; Loss = 1.537009\n",
      "2024-11-29 19:27:57.354000: I runner.py:310] Step = 59400 ; steps/s = 1.61, tokens/s = 43100 (43100 target) ; Learning rate = 0.000363 ; Loss = 1.533807\n",
      "2024-11-29 19:28:58.707000: I runner.py:310] Step = 59500 ; steps/s = 1.63, tokens/s = 42723 (42723 target) ; Learning rate = 0.000362 ; Loss = 1.543083\n",
      "2024-11-29 19:30:00.622000: I runner.py:310] Step = 59600 ; steps/s = 1.62, tokens/s = 43115 (43115 target) ; Learning rate = 0.000362 ; Loss = 1.536826\n",
      "2024-11-29 19:31:02.497000: I runner.py:310] Step = 59700 ; steps/s = 1.62, tokens/s = 43211 (43211 target) ; Learning rate = 0.000362 ; Loss = 1.538575\n",
      "2024-11-29 19:32:04.384000: I runner.py:310] Step = 59800 ; steps/s = 1.62, tokens/s = 43179 (43179 target) ; Learning rate = 0.000361 ; Loss = 1.551899\n",
      "2024-11-29 19:33:05.815000: I runner.py:310] Step = 59900 ; steps/s = 1.63, tokens/s = 42671 (42671 target) ; Learning rate = 0.000361 ; Loss = 1.557401\n",
      "2024-11-29 19:34:07.759000: I runner.py:310] Step = 60000 ; steps/s = 1.61, tokens/s = 43093 (43093 target) ; Learning rate = 0.000361 ; Loss = 1.533663\n",
      "2024-11-29 19:34:09.838000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-60000\n",
      "2024-11-29 19:34:09.838000: I training.py:192] Running evaluation for step 60000\n",
      "2024-11-29 19:38:23.811000: I training.py:192] Evaluation result for step 60000: loss = 1.166031 ; perplexity = 3.209229\n",
      "2024-11-29 19:39:25.623000: I runner.py:310] Step = 60100 ; steps/s = 1.62, tokens/s = 43266 (43266 target) ; Learning rate = 0.000361 ; Loss = 1.537898\n",
      "2024-11-29 19:40:27.547000: I runner.py:310] Step = 60200 ; steps/s = 1.62, tokens/s = 43142 (43142 target) ; Learning rate = 0.000360 ; Loss = 1.540675\n",
      "2024-11-29 19:41:28.964000: I runner.py:310] Step = 60300 ; steps/s = 1.63, tokens/s = 42659 (42659 target) ; Learning rate = 0.000360 ; Loss = 1.545427\n",
      "2024-11-29 19:42:30.794000: I runner.py:310] Step = 60400 ; steps/s = 1.62, tokens/s = 43209 (43209 target) ; Learning rate = 0.000360 ; Loss = 1.540465\n",
      "2024-11-29 19:43:32.755000: I runner.py:310] Step = 60500 ; steps/s = 1.61, tokens/s = 43121 (43121 target) ; Learning rate = 0.000359 ; Loss = 1.540528\n",
      "2024-11-29 19:44:34.735000: I runner.py:310] Step = 60600 ; steps/s = 1.61, tokens/s = 43099 (43099 target) ; Learning rate = 0.000359 ; Loss = 1.542433\n",
      "2024-11-29 19:45:36.171000: I runner.py:310] Step = 60700 ; steps/s = 1.63, tokens/s = 42680 (42680 target) ; Learning rate = 0.000359 ; Loss = 1.538142\n",
      "2024-11-29 19:46:38.148000: I runner.py:310] Step = 60800 ; steps/s = 1.61, tokens/s = 43111 (43111 target) ; Learning rate = 0.000358 ; Loss = 1.539192\n",
      "2024-11-29 19:47:40.059000: I runner.py:310] Step = 60900 ; steps/s = 1.62, tokens/s = 43151 (43151 target) ; Learning rate = 0.000358 ; Loss = 1.542506\n",
      "2024-11-29 19:48:41.467000: I runner.py:310] Step = 61000 ; steps/s = 1.63, tokens/s = 42656 (42656 target) ; Learning rate = 0.000358 ; Loss = 1.536537\n",
      "2024-11-29 19:49:43.435000: I runner.py:310] Step = 61100 ; steps/s = 1.61, tokens/s = 43129 (43129 target) ; Learning rate = 0.000358 ; Loss = 1.540930\n",
      "2024-11-29 19:50:45.392000: I runner.py:310] Step = 61200 ; steps/s = 1.61, tokens/s = 43094 (43094 target) ; Learning rate = 0.000357 ; Loss = 1.542227\n",
      "2024-11-29 19:51:47.300000: I runner.py:310] Step = 61300 ; steps/s = 1.62, tokens/s = 43156 (43156 target) ; Learning rate = 0.000357 ; Loss = 1.550901\n",
      "2024-11-29 19:52:48.689000: I runner.py:310] Step = 61400 ; steps/s = 1.63, tokens/s = 42718 (42718 target) ; Learning rate = 0.000357 ; Loss = 1.541053\n",
      "2024-11-29 19:53:50.543000: I runner.py:310] Step = 61500 ; steps/s = 1.62, tokens/s = 43188 (43188 target) ; Learning rate = 0.000356 ; Loss = 1.536207\n",
      "2024-11-29 19:54:52.506000: I runner.py:310] Step = 61600 ; steps/s = 1.61, tokens/s = 43128 (43128 target) ; Learning rate = 0.000356 ; Loss = 1.551565\n",
      "2024-11-29 19:55:54.454000: I runner.py:310] Step = 61700 ; steps/s = 1.61, tokens/s = 43131 (43131 target) ; Learning rate = 0.000356 ; Loss = 1.538912\n",
      "2024-11-29 19:56:55.872000: I runner.py:310] Step = 61800 ; steps/s = 1.63, tokens/s = 42651 (42651 target) ; Learning rate = 0.000356 ; Loss = 1.535981\n",
      "2024-11-29 19:57:57.803000: I runner.py:310] Step = 61900 ; steps/s = 1.61, tokens/s = 43157 (43157 target) ; Learning rate = 0.000355 ; Loss = 1.533601\n",
      "2024-11-29 19:58:59.797000: I runner.py:310] Step = 62000 ; steps/s = 1.61, tokens/s = 43092 (43092 target) ; Learning rate = 0.000355 ; Loss = 1.529679\n",
      "2024-11-29 20:00:01.789000: I runner.py:310] Step = 62100 ; steps/s = 1.61, tokens/s = 43112 (43112 target) ; Learning rate = 0.000355 ; Loss = 1.535202\n",
      "2024-11-29 20:01:03.180000: I runner.py:310] Step = 62200 ; steps/s = 1.63, tokens/s = 42655 (42655 target) ; Learning rate = 0.000354 ; Loss = 1.539801\n",
      "2024-11-29 20:02:05.091000: I runner.py:310] Step = 62300 ; steps/s = 1.62, tokens/s = 43143 (43143 target) ; Learning rate = 0.000354 ; Loss = 1.545990\n",
      "2024-11-29 20:03:07.008000: I runner.py:310] Step = 62400 ; steps/s = 1.62, tokens/s = 43139 (43139 target) ; Learning rate = 0.000354 ; Loss = 1.532871\n",
      "2024-11-29 20:04:08.920000: I runner.py:310] Step = 62500 ; steps/s = 1.62, tokens/s = 43172 (43172 target) ; Learning rate = 0.000354 ; Loss = 1.533431\n",
      "2024-11-29 20:05:10.254000: I runner.py:310] Step = 62600 ; steps/s = 1.63, tokens/s = 42715 (42715 target) ; Learning rate = 0.000353 ; Loss = 1.544999\n",
      "2024-11-29 20:06:12.102000: I runner.py:310] Step = 62700 ; steps/s = 1.62, tokens/s = 43212 (43212 target) ; Learning rate = 0.000353 ; Loss = 1.533373\n",
      "2024-11-29 20:07:14.011000: I runner.py:310] Step = 62800 ; steps/s = 1.62, tokens/s = 43140 (43140 target) ; Learning rate = 0.000353 ; Loss = 1.532254\n",
      "2024-11-29 20:08:15.954000: I runner.py:310] Step = 62900 ; steps/s = 1.61, tokens/s = 43171 (43171 target) ; Learning rate = 0.000352 ; Loss = 1.542116\n",
      "2024-11-29 20:09:17.404000: I runner.py:310] Step = 63000 ; steps/s = 1.63, tokens/s = 42636 (42636 target) ; Learning rate = 0.000352 ; Loss = 1.554683\n",
      "2024-11-29 20:10:19.363000: I runner.py:310] Step = 63100 ; steps/s = 1.61, tokens/s = 43129 (43129 target) ; Learning rate = 0.000352 ; Loss = 1.542158\n",
      "2024-11-29 20:11:21.293000: I runner.py:310] Step = 63200 ; steps/s = 1.61, tokens/s = 43135 (43135 target) ; Learning rate = 0.000352 ; Loss = 1.529988\n",
      "2024-11-29 20:12:23.289000: I runner.py:310] Step = 63300 ; steps/s = 1.61, tokens/s = 43067 (43067 target) ; Learning rate = 0.000351 ; Loss = 1.533279\n",
      "2024-11-29 20:13:24.668000: I runner.py:310] Step = 63400 ; steps/s = 1.63, tokens/s = 42726 (42726 target) ; Learning rate = 0.000351 ; Loss = 1.530732\n",
      "2024-11-29 20:14:26.589000: I runner.py:310] Step = 63500 ; steps/s = 1.62, tokens/s = 43139 (43139 target) ; Learning rate = 0.000351 ; Loss = 1.538731\n",
      "2024-11-29 20:15:28.531000: I runner.py:310] Step = 63600 ; steps/s = 1.61, tokens/s = 43098 (43098 target) ; Learning rate = 0.000350 ; Loss = 1.543506\n",
      "2024-11-29 20:16:30.463000: I runner.py:310] Step = 63700 ; steps/s = 1.61, tokens/s = 43170 (43170 target) ; Learning rate = 0.000350 ; Loss = 1.542169\n",
      "2024-11-29 20:17:31.814000: I runner.py:310] Step = 63800 ; steps/s = 1.63, tokens/s = 42724 (42724 target) ; Learning rate = 0.000350 ; Loss = 1.548215\n",
      "2024-11-29 20:18:33.734000: I runner.py:310] Step = 63900 ; steps/s = 1.62, tokens/s = 43134 (43134 target) ; Learning rate = 0.000350 ; Loss = 1.539092\n",
      "2024-11-29 20:19:35.588000: I runner.py:310] Step = 64000 ; steps/s = 1.62, tokens/s = 43234 (43234 target) ; Learning rate = 0.000349 ; Loss = 1.536509\n",
      "2024-11-29 20:20:37.559000: I runner.py:310] Step = 64100 ; steps/s = 1.61, tokens/s = 43084 (43084 target) ; Learning rate = 0.000349 ; Loss = 1.533327\n",
      "2024-11-29 20:21:38.931000: I runner.py:310] Step = 64200 ; steps/s = 1.63, tokens/s = 42732 (42732 target) ; Learning rate = 0.000349 ; Loss = 1.541390\n",
      "2024-11-29 20:22:40.868000: I runner.py:310] Step = 64300 ; steps/s = 1.61, tokens/s = 43108 (43108 target) ; Learning rate = 0.000349 ; Loss = 1.535873\n",
      "2024-11-29 20:23:42.835000: I runner.py:310] Step = 64400 ; steps/s = 1.61, tokens/s = 43089 (43089 target) ; Learning rate = 0.000348 ; Loss = 1.533470\n",
      "2024-11-29 20:24:44.773000: I runner.py:310] Step = 64500 ; steps/s = 1.61, tokens/s = 43155 (43155 target) ; Learning rate = 0.000348 ; Loss = 1.537660\n",
      "2024-11-29 20:25:46.162000: I runner.py:310] Step = 64600 ; steps/s = 1.63, tokens/s = 42698 (42698 target) ; Learning rate = 0.000348 ; Loss = 1.529609\n",
      "2024-11-29 20:26:48.067000: I runner.py:310] Step = 64700 ; steps/s = 1.62, tokens/s = 43150 (43150 target) ; Learning rate = 0.000347 ; Loss = 1.535979\n",
      "2024-11-29 20:27:49.960000: I runner.py:310] Step = 64800 ; steps/s = 1.62, tokens/s = 43178 (43178 target) ; Learning rate = 0.000347 ; Loss = 1.536256\n",
      "2024-11-29 20:28:51.886000: I runner.py:310] Step = 64900 ; steps/s = 1.61, tokens/s = 43118 (43118 target) ; Learning rate = 0.000347 ; Loss = 1.545859\n",
      "2024-11-29 20:29:53.281000: I runner.py:310] Step = 65000 ; steps/s = 1.63, tokens/s = 42695 (42695 target) ; Learning rate = 0.000347 ; Loss = 1.528878\n",
      "2024-11-29 20:29:53.283000: I training.py:192] Running evaluation for step 65000\n",
      "2024-11-29 20:34:02.397000: I training.py:192] Evaluation result for step 65000: loss = 1.173148 ; perplexity = 3.232152\n",
      "2024-11-29 20:35:04.205000: I runner.py:310] Step = 65100 ; steps/s = 1.62, tokens/s = 43216 (43216 target) ; Learning rate = 0.000346 ; Loss = 1.534043\n",
      "2024-11-29 20:36:06.163000: I runner.py:310] Step = 65200 ; steps/s = 1.61, tokens/s = 43133 (43133 target) ; Learning rate = 0.000346 ; Loss = 1.540659\n",
      "2024-11-29 20:37:07.600000: I runner.py:310] Step = 65300 ; steps/s = 1.63, tokens/s = 42649 (42649 target) ; Learning rate = 0.000346 ; Loss = 1.536110\n",
      "2024-11-29 20:38:09.522000: I runner.py:310] Step = 65400 ; steps/s = 1.62, tokens/s = 43146 (43146 target) ; Learning rate = 0.000346 ; Loss = 1.537725\n",
      "2024-11-29 20:39:11.503000: I runner.py:310] Step = 65500 ; steps/s = 1.61, tokens/s = 43118 (43118 target) ; Learning rate = 0.000345 ; Loss = 1.523495\n",
      "2024-11-29 20:40:13.450000: I runner.py:310] Step = 65600 ; steps/s = 1.61, tokens/s = 43123 (43123 target) ; Learning rate = 0.000345 ; Loss = 1.536195\n",
      "2024-11-29 20:41:14.867000: I runner.py:310] Step = 65700 ; steps/s = 1.63, tokens/s = 42673 (42673 target) ; Learning rate = 0.000345 ; Loss = 1.528685\n",
      "2024-11-29 20:42:16.732000: I runner.py:310] Step = 65800 ; steps/s = 1.62, tokens/s = 43204 (43204 target) ; Learning rate = 0.000345 ; Loss = 1.537186\n",
      "2024-11-29 20:43:18.758000: I runner.py:310] Step = 65900 ; steps/s = 1.61, tokens/s = 43035 (43035 target) ; Learning rate = 0.000344 ; Loss = 1.529197\n",
      "2024-11-29 20:44:20.701000: I runner.py:310] Step = 66000 ; steps/s = 1.61, tokens/s = 43152 (43152 target) ; Learning rate = 0.000344 ; Loss = 1.537096\n",
      "2024-11-29 20:45:22.196000: I runner.py:310] Step = 66100 ; steps/s = 1.63, tokens/s = 42635 (42635 target) ; Learning rate = 0.000344 ; Loss = 1.539805\n",
      "2024-11-29 20:46:24.092000: I runner.py:310] Step = 66200 ; steps/s = 1.62, tokens/s = 43139 (43139 target) ; Learning rate = 0.000344 ; Loss = 1.528229\n",
      "2024-11-29 20:47:26.036000: I runner.py:310] Step = 66300 ; steps/s = 1.61, tokens/s = 43142 (43142 target) ; Learning rate = 0.000343 ; Loss = 1.524532\n",
      "2024-11-29 20:48:28.032000: I runner.py:310] Step = 66400 ; steps/s = 1.61, tokens/s = 43098 (43098 target) ; Learning rate = 0.000343 ; Loss = 1.535737\n",
      "2024-11-29 20:49:29.417000: I runner.py:310] Step = 66500 ; steps/s = 1.63, tokens/s = 42693 (42693 target) ; Learning rate = 0.000343 ; Loss = 1.528239\n",
      "2024-11-29 20:50:31.405000: I runner.py:310] Step = 66600 ; steps/s = 1.61, tokens/s = 43090 (43090 target) ; Learning rate = 0.000342 ; Loss = 1.539020\n",
      "2024-11-29 20:51:33.353000: I runner.py:310] Step = 66700 ; steps/s = 1.61, tokens/s = 43135 (43135 target) ; Learning rate = 0.000342 ; Loss = 1.536488\n",
      "2024-11-29 20:52:35.339000: I runner.py:310] Step = 66800 ; steps/s = 1.61, tokens/s = 43103 (43103 target) ; Learning rate = 0.000342 ; Loss = 1.535590\n",
      "2024-11-29 20:53:36.704000: I runner.py:310] Step = 66900 ; steps/s = 1.63, tokens/s = 42718 (42718 target) ; Learning rate = 0.000342 ; Loss = 1.533676\n",
      "2024-11-29 20:54:38.739000: I runner.py:310] Step = 67000 ; steps/s = 1.61, tokens/s = 43078 (43078 target) ; Learning rate = 0.000341 ; Loss = 1.531600\n",
      "2024-11-29 20:55:40.744000: I runner.py:310] Step = 67100 ; steps/s = 1.61, tokens/s = 43089 (43089 target) ; Learning rate = 0.000341 ; Loss = 1.538155\n",
      "2024-11-29 20:56:42.728000: I runner.py:310] Step = 67200 ; steps/s = 1.61, tokens/s = 43087 (43087 target) ; Learning rate = 0.000341 ; Loss = 1.529128\n",
      "2024-11-29 20:57:44.165000: I runner.py:310] Step = 67300 ; steps/s = 1.63, tokens/s = 42660 (42660 target) ; Learning rate = 0.000341 ; Loss = 1.538249\n",
      "2024-11-29 20:58:46.132000: I runner.py:310] Step = 67400 ; steps/s = 1.61, tokens/s = 43135 (43135 target) ; Learning rate = 0.000340 ; Loss = 1.529571\n",
      "2024-11-29 20:59:48.078000: I runner.py:310] Step = 67500 ; steps/s = 1.61, tokens/s = 43107 (43107 target) ; Learning rate = 0.000340 ; Loss = 1.526557\n",
      "2024-11-29 21:00:50.050000: I runner.py:310] Step = 67600 ; steps/s = 1.61, tokens/s = 43113 (43113 target) ; Learning rate = 0.000340 ; Loss = 1.535748\n",
      "2024-11-29 21:01:51.455000: I runner.py:310] Step = 67700 ; steps/s = 1.63, tokens/s = 42642 (42642 target) ; Learning rate = 0.000340 ; Loss = 1.548707\n",
      "2024-11-29 21:02:53.416000: I runner.py:310] Step = 67800 ; steps/s = 1.61, tokens/s = 43129 (43129 target) ; Learning rate = 0.000339 ; Loss = 1.530993\n",
      "2024-11-29 21:03:55.366000: I runner.py:310] Step = 67900 ; steps/s = 1.61, tokens/s = 43124 (43124 target) ; Learning rate = 0.000339 ; Loss = 1.526617\n",
      "2024-11-29 21:04:57.346000: I runner.py:310] Step = 68000 ; steps/s = 1.61, tokens/s = 43100 (43100 target) ; Learning rate = 0.000339 ; Loss = 1.530611\n",
      "2024-11-29 21:05:58.767000: I runner.py:310] Step = 68100 ; steps/s = 1.63, tokens/s = 42682 (42682 target) ; Learning rate = 0.000339 ; Loss = 1.543500\n",
      "2024-11-29 21:07:00.747000: I runner.py:310] Step = 68200 ; steps/s = 1.61, tokens/s = 43129 (43129 target) ; Learning rate = 0.000338 ; Loss = 1.529616\n",
      "2024-11-29 21:08:02.733000: I runner.py:310] Step = 68300 ; steps/s = 1.61, tokens/s = 43103 (43103 target) ; Learning rate = 0.000338 ; Loss = 1.534316\n",
      "2024-11-29 21:09:04.706000: I runner.py:310] Step = 68400 ; steps/s = 1.61, tokens/s = 43077 (43077 target) ; Learning rate = 0.000338 ; Loss = 1.540544\n",
      "2024-11-29 21:10:06.084000: I runner.py:310] Step = 68500 ; steps/s = 1.63, tokens/s = 42716 (42716 target) ; Learning rate = 0.000338 ; Loss = 1.519512\n",
      "2024-11-29 21:11:08.058000: I runner.py:310] Step = 68600 ; steps/s = 1.61, tokens/s = 43141 (43141 target) ; Learning rate = 0.000337 ; Loss = 1.534874\n",
      "2024-11-29 21:12:09.943000: I runner.py:310] Step = 68700 ; steps/s = 1.62, tokens/s = 43173 (43173 target) ; Learning rate = 0.000337 ; Loss = 1.537807\n",
      "2024-11-29 21:13:11.886000: I runner.py:310] Step = 68800 ; steps/s = 1.61, tokens/s = 43090 (43090 target) ; Learning rate = 0.000337 ; Loss = 1.534604\n",
      "2024-11-29 21:14:13.206000: I runner.py:310] Step = 68900 ; steps/s = 1.63, tokens/s = 42721 (42721 target) ; Learning rate = 0.000337 ; Loss = 1.539249\n",
      "2024-11-29 21:15:15.200000: I runner.py:310] Step = 69000 ; steps/s = 1.61, tokens/s = 43144 (43144 target) ; Learning rate = 0.000336 ; Loss = 1.531281\n",
      "2024-11-29 21:16:17.120000: I runner.py:310] Step = 69100 ; steps/s = 1.62, tokens/s = 43140 (43140 target) ; Learning rate = 0.000336 ; Loss = 1.526078\n",
      "2024-11-29 21:17:19.078000: I runner.py:310] Step = 69200 ; steps/s = 1.61, tokens/s = 43110 (43110 target) ; Learning rate = 0.000336 ; Loss = 1.537811\n",
      "2024-11-29 21:18:20.470000: I runner.py:310] Step = 69300 ; steps/s = 1.63, tokens/s = 42683 (42683 target) ; Learning rate = 0.000336 ; Loss = 1.521805\n",
      "2024-11-29 21:19:22.499000: I runner.py:310] Step = 69400 ; steps/s = 1.61, tokens/s = 43047 (43047 target) ; Learning rate = 0.000336 ; Loss = 1.536473\n",
      "2024-11-29 21:20:24.451000: I runner.py:310] Step = 69500 ; steps/s = 1.61, tokens/s = 43150 (43150 target) ; Learning rate = 0.000335 ; Loss = 1.527717\n",
      "2024-11-29 21:21:25.884000: I runner.py:310] Step = 69600 ; steps/s = 1.63, tokens/s = 42641 (42641 target) ; Learning rate = 0.000335 ; Loss = 1.531353\n",
      "2024-11-29 21:22:27.868000: I runner.py:310] Step = 69700 ; steps/s = 1.61, tokens/s = 43178 (43178 target) ; Learning rate = 0.000335 ; Loss = 1.532527\n",
      "2024-11-29 21:23:29.842000: I runner.py:310] Step = 69800 ; steps/s = 1.61, tokens/s = 43098 (43098 target) ; Learning rate = 0.000335 ; Loss = 1.528726\n",
      "2024-11-29 21:24:31.826000: I runner.py:310] Step = 69900 ; steps/s = 1.61, tokens/s = 43083 (43083 target) ; Learning rate = 0.000334 ; Loss = 1.525412\n",
      "2024-11-29 21:25:33.225000: I runner.py:310] Step = 70000 ; steps/s = 1.63, tokens/s = 42656 (42656 target) ; Learning rate = 0.000334 ; Loss = 1.530939\n",
      "2024-11-29 21:25:35.873000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-70000\n",
      "2024-11-29 21:25:35.873000: I training.py:192] Running evaluation for step 70000\n",
      "2024-11-29 21:29:50.385000: I training.py:192] Evaluation result for step 70000: loss = 1.173081 ; perplexity = 3.231936\n",
      "2024-11-29 21:30:52.175000: I runner.py:310] Step = 70100 ; steps/s = 1.62, tokens/s = 43264 (43264 target) ; Learning rate = 0.000334 ; Loss = 1.534040\n",
      "2024-11-29 21:31:54.092000: I runner.py:310] Step = 70200 ; steps/s = 1.62, tokens/s = 43149 (43149 target) ; Learning rate = 0.000334 ; Loss = 1.522208\n",
      "2024-11-29 21:32:56.051000: I runner.py:310] Step = 70300 ; steps/s = 1.61, tokens/s = 43109 (43109 target) ; Learning rate = 0.000333 ; Loss = 1.521288\n",
      "2024-11-29 21:33:57.474000: I runner.py:310] Step = 70400 ; steps/s = 1.63, tokens/s = 42666 (42666 target) ; Learning rate = 0.000333 ; Loss = 1.527725\n",
      "2024-11-29 21:34:59.422000: I runner.py:310] Step = 70500 ; steps/s = 1.61, tokens/s = 43147 (43147 target) ; Learning rate = 0.000333 ; Loss = 1.525353\n",
      "2024-11-29 21:36:01.444000: I runner.py:310] Step = 70600 ; steps/s = 1.61, tokens/s = 43047 (43047 target) ; Learning rate = 0.000333 ; Loss = 1.528065\n",
      "2024-11-29 21:37:03.411000: I runner.py:310] Step = 70700 ; steps/s = 1.61, tokens/s = 43104 (43104 target) ; Learning rate = 0.000332 ; Loss = 1.530243\n",
      "2024-11-29 21:38:04.797000: I runner.py:310] Step = 70800 ; steps/s = 1.63, tokens/s = 42721 (42721 target) ; Learning rate = 0.000332 ; Loss = 1.525788\n",
      "2024-11-29 21:39:06.731000: I runner.py:310] Step = 70900 ; steps/s = 1.61, tokens/s = 43126 (43126 target) ; Learning rate = 0.000332 ; Loss = 1.534091\n",
      "2024-11-29 21:40:08.686000: I runner.py:310] Step = 71000 ; steps/s = 1.61, tokens/s = 43120 (43120 target) ; Learning rate = 0.000332 ; Loss = 1.528041\n",
      "2024-11-29 21:41:10.649000: I runner.py:310] Step = 71100 ; steps/s = 1.61, tokens/s = 43095 (43095 target) ; Learning rate = 0.000331 ; Loss = 1.532421\n",
      "2024-11-29 21:42:12.050000: I runner.py:310] Step = 71200 ; steps/s = 1.63, tokens/s = 42710 (42710 target) ; Learning rate = 0.000331 ; Loss = 1.528813\n",
      "2024-11-29 21:43:14.062000: I runner.py:310] Step = 71300 ; steps/s = 1.61, tokens/s = 43113 (43113 target) ; Learning rate = 0.000331 ; Loss = 1.531155\n",
      "2024-11-29 21:44:16.058000: I runner.py:310] Step = 71400 ; steps/s = 1.61, tokens/s = 43113 (43113 target) ; Learning rate = 0.000331 ; Loss = 1.531144\n",
      "2024-11-29 21:45:18.023000: I runner.py:310] Step = 71500 ; steps/s = 1.61, tokens/s = 43051 (43051 target) ; Learning rate = 0.000331 ; Loss = 1.533248\n",
      "2024-11-29 21:46:19.437000: I runner.py:310] Step = 71600 ; steps/s = 1.63, tokens/s = 42659 (42659 target) ; Learning rate = 0.000330 ; Loss = 1.527981\n",
      "2024-11-29 21:47:21.426000: I runner.py:310] Step = 71700 ; steps/s = 1.61, tokens/s = 43080 (43080 target) ; Learning rate = 0.000330 ; Loss = 1.533035\n",
      "2024-11-29 21:48:23.432000: I runner.py:310] Step = 71800 ; steps/s = 1.61, tokens/s = 43127 (43127 target) ; Learning rate = 0.000330 ; Loss = 1.534541\n",
      "2024-11-29 21:49:25.384000: I runner.py:310] Step = 71900 ; steps/s = 1.61, tokens/s = 43120 (43120 target) ; Learning rate = 0.000330 ; Loss = 1.534291\n",
      "2024-11-29 21:50:26.744000: I runner.py:310] Step = 72000 ; steps/s = 1.63, tokens/s = 42752 (42752 target) ; Learning rate = 0.000329 ; Loss = 1.520368\n",
      "2024-11-29 21:51:28.688000: I runner.py:310] Step = 72100 ; steps/s = 1.61, tokens/s = 43124 (43124 target) ; Learning rate = 0.000329 ; Loss = 1.532624\n",
      "2024-11-29 21:52:30.669000: I runner.py:310] Step = 72200 ; steps/s = 1.61, tokens/s = 43092 (43092 target) ; Learning rate = 0.000329 ; Loss = 1.529334\n",
      "2024-11-29 21:53:32.671000: I runner.py:310] Step = 72300 ; steps/s = 1.61, tokens/s = 43069 (43069 target) ; Learning rate = 0.000329 ; Loss = 1.533496\n",
      "2024-11-29 21:54:34.078000: I runner.py:310] Step = 72400 ; steps/s = 1.63, tokens/s = 42669 (42669 target) ; Learning rate = 0.000328 ; Loss = 1.529108\n",
      "2024-11-29 21:55:36.052000: I runner.py:310] Step = 72500 ; steps/s = 1.61, tokens/s = 43114 (43114 target) ; Learning rate = 0.000328 ; Loss = 1.521543\n",
      "2024-11-29 21:56:38.007000: I runner.py:310] Step = 72600 ; steps/s = 1.61, tokens/s = 43110 (43110 target) ; Learning rate = 0.000328 ; Loss = 1.525579\n",
      "2024-11-29 21:57:39.958000: I runner.py:310] Step = 72700 ; steps/s = 1.61, tokens/s = 43148 (43148 target) ; Learning rate = 0.000328 ; Loss = 1.524946\n",
      "2024-11-29 21:58:41.335000: I runner.py:310] Step = 72800 ; steps/s = 1.63, tokens/s = 42708 (42708 target) ; Learning rate = 0.000328 ; Loss = 1.533776\n",
      "2024-11-29 21:59:43.254000: I runner.py:310] Step = 72900 ; steps/s = 1.62, tokens/s = 43149 (43149 target) ; Learning rate = 0.000327 ; Loss = 1.519996\n",
      "2024-11-29 22:00:45.246000: I runner.py:310] Step = 73000 ; steps/s = 1.61, tokens/s = 43092 (43092 target) ; Learning rate = 0.000327 ; Loss = 1.524565\n",
      "2024-11-29 22:01:47.210000: I runner.py:310] Step = 73100 ; steps/s = 1.61, tokens/s = 43123 (43123 target) ; Learning rate = 0.000327 ; Loss = 1.526457\n",
      "2024-11-29 22:02:48.604000: I runner.py:310] Step = 73200 ; steps/s = 1.63, tokens/s = 42706 (42706 target) ; Learning rate = 0.000327 ; Loss = 1.525007\n",
      "2024-11-29 22:03:50.549000: I runner.py:310] Step = 73300 ; steps/s = 1.61, tokens/s = 43133 (43133 target) ; Learning rate = 0.000326 ; Loss = 1.528022\n",
      "2024-11-29 22:04:52.591000: I runner.py:310] Step = 73400 ; steps/s = 1.61, tokens/s = 43024 (43024 target) ; Learning rate = 0.000326 ; Loss = 1.528316\n",
      "2024-11-29 22:05:54.528000: I runner.py:310] Step = 73500 ; steps/s = 1.61, tokens/s = 43156 (43156 target) ; Learning rate = 0.000326 ; Loss = 1.529152\n",
      "2024-11-29 22:06:55.848000: I runner.py:310] Step = 73600 ; steps/s = 1.63, tokens/s = 42733 (42733 target) ; Learning rate = 0.000326 ; Loss = 1.529137\n",
      "2024-11-29 22:07:57.789000: I runner.py:310] Step = 73700 ; steps/s = 1.61, tokens/s = 43117 (43117 target) ; Learning rate = 0.000326 ; Loss = 1.515693\n",
      "2024-11-29 22:08:59.801000: I runner.py:310] Step = 73800 ; steps/s = 1.61, tokens/s = 43087 (43087 target) ; Learning rate = 0.000325 ; Loss = 1.534362\n",
      "2024-11-29 22:10:01.215000: I runner.py:310] Step = 73900 ; steps/s = 1.63, tokens/s = 42672 (42672 target) ; Learning rate = 0.000325 ; Loss = 1.527683\n",
      "2024-11-29 22:11:03.196000: I runner.py:310] Step = 74000 ; steps/s = 1.61, tokens/s = 43137 (43137 target) ; Learning rate = 0.000325 ; Loss = 1.520969\n",
      "2024-11-29 22:12:05.151000: I runner.py:310] Step = 74100 ; steps/s = 1.61, tokens/s = 43118 (43118 target) ; Learning rate = 0.000325 ; Loss = 1.527532\n",
      "2024-11-29 22:13:07.123000: I runner.py:310] Step = 74200 ; steps/s = 1.61, tokens/s = 43083 (43083 target) ; Learning rate = 0.000324 ; Loss = 1.531620\n",
      "2024-11-29 22:14:08.550000: I runner.py:310] Step = 74300 ; steps/s = 1.63, tokens/s = 42676 (42676 target) ; Learning rate = 0.000324 ; Loss = 1.524236\n",
      "2024-11-29 22:15:10.599000: I runner.py:310] Step = 74400 ; steps/s = 1.61, tokens/s = 43069 (43069 target) ; Learning rate = 0.000324 ; Loss = 1.519368\n",
      "2024-11-29 22:16:12.514000: I runner.py:310] Step = 74500 ; steps/s = 1.62, tokens/s = 43142 (43142 target) ; Learning rate = 0.000324 ; Loss = 1.522803\n",
      "2024-11-29 22:17:14.456000: I runner.py:310] Step = 74600 ; steps/s = 1.61, tokens/s = 43134 (43134 target) ; Learning rate = 0.000324 ; Loss = 1.526701\n",
      "2024-11-29 22:18:15.872000: I runner.py:310] Step = 74700 ; steps/s = 1.63, tokens/s = 42663 (42663 target) ; Learning rate = 0.000323 ; Loss = 1.529409\n",
      "2024-11-29 22:19:17.803000: I runner.py:310] Step = 74800 ; steps/s = 1.61, tokens/s = 43121 (43121 target) ; Learning rate = 0.000323 ; Loss = 1.526757\n",
      "2024-11-29 22:20:19.772000: I runner.py:310] Step = 74900 ; steps/s = 1.61, tokens/s = 43134 (43134 target) ; Learning rate = 0.000323 ; Loss = 1.525210\n",
      "2024-11-29 22:21:21.784000: I runner.py:310] Step = 75000 ; steps/s = 1.61, tokens/s = 43099 (43099 target) ; Learning rate = 0.000323 ; Loss = 1.527350\n",
      "2024-11-29 22:21:21.786000: I training.py:192] Running evaluation for step 75000\n",
      "2024-11-29 22:25:36.607000: I training.py:192] Evaluation result for step 75000: loss = 1.183109 ; perplexity = 3.264508\n",
      "2024-11-29 22:26:37.821000: I runner.py:310] Step = 75100 ; steps/s = 1.63, tokens/s = 42803 (42803 target) ; Learning rate = 0.000323 ; Loss = 1.528845\n",
      "2024-11-29 22:27:39.679000: I runner.py:310] Step = 75200 ; steps/s = 1.62, tokens/s = 43179 (43179 target) ; Learning rate = 0.000322 ; Loss = 1.530865\n",
      "2024-11-29 22:28:41.630000: I runner.py:310] Step = 75300 ; steps/s = 1.61, tokens/s = 43119 (43119 target) ; Learning rate = 0.000322 ; Loss = 1.525977\n",
      "2024-11-29 22:29:43.612000: I runner.py:310] Step = 75400 ; steps/s = 1.61, tokens/s = 43117 (43117 target) ; Learning rate = 0.000322 ; Loss = 1.527132\n",
      "2024-11-29 22:30:44.988000: I runner.py:310] Step = 75500 ; steps/s = 1.63, tokens/s = 42700 (42700 target) ; Learning rate = 0.000322 ; Loss = 1.519979\n",
      "2024-11-29 22:31:46.943000: I runner.py:310] Step = 75600 ; steps/s = 1.61, tokens/s = 43132 (43132 target) ; Learning rate = 0.000321 ; Loss = 1.523505\n",
      "2024-11-29 22:32:48.920000: I runner.py:310] Step = 75700 ; steps/s = 1.61, tokens/s = 43099 (43099 target) ; Learning rate = 0.000321 ; Loss = 1.532503\n",
      "2024-11-29 22:33:50.893000: I runner.py:310] Step = 75800 ; steps/s = 1.61, tokens/s = 43120 (43120 target) ; Learning rate = 0.000321 ; Loss = 1.529174\n",
      "2024-11-29 22:34:52.268000: I runner.py:310] Step = 75900 ; steps/s = 1.63, tokens/s = 42675 (42675 target) ; Learning rate = 0.000321 ; Loss = 1.523103\n",
      "2024-11-29 22:35:54.251000: I runner.py:310] Step = 76000 ; steps/s = 1.61, tokens/s = 43106 (43106 target) ; Learning rate = 0.000321 ; Loss = 1.523354\n",
      "2024-11-29 22:36:56.193000: I runner.py:310] Step = 76100 ; steps/s = 1.61, tokens/s = 43161 (43161 target) ; Learning rate = 0.000320 ; Loss = 1.523633\n",
      "2024-11-29 22:37:58.100000: I runner.py:310] Step = 76200 ; steps/s = 1.62, tokens/s = 43164 (43164 target) ; Learning rate = 0.000320 ; Loss = 1.520596\n",
      "2024-11-29 22:38:59.459000: I runner.py:310] Step = 76300 ; steps/s = 1.63, tokens/s = 42687 (42687 target) ; Learning rate = 0.000320 ; Loss = 1.519307\n",
      "2024-11-29 22:40:01.407000: I runner.py:310] Step = 76400 ; steps/s = 1.61, tokens/s = 43129 (43129 target) ; Learning rate = 0.000320 ; Loss = 1.522513\n",
      "2024-11-29 22:41:03.389000: I runner.py:310] Step = 76500 ; steps/s = 1.61, tokens/s = 43115 (43115 target) ; Learning rate = 0.000320 ; Loss = 1.529761\n",
      "2024-11-29 22:42:05.350000: I runner.py:310] Step = 76600 ; steps/s = 1.61, tokens/s = 43105 (43105 target) ; Learning rate = 0.000319 ; Loss = 1.528353\n",
      "2024-11-29 22:43:06.833000: I runner.py:310] Step = 76700 ; steps/s = 1.63, tokens/s = 42638 (42638 target) ; Learning rate = 0.000319 ; Loss = 1.530604\n",
      "2024-11-29 22:44:08.832000: I runner.py:310] Step = 76800 ; steps/s = 1.61, tokens/s = 43091 (43091 target) ; Learning rate = 0.000319 ; Loss = 1.519120\n",
      "2024-11-29 22:45:10.782000: I runner.py:310] Step = 76900 ; steps/s = 1.61, tokens/s = 43144 (43144 target) ; Learning rate = 0.000319 ; Loss = 1.519199\n",
      "2024-11-29 22:46:12.745000: I runner.py:310] Step = 77000 ; steps/s = 1.61, tokens/s = 43122 (43122 target) ; Learning rate = 0.000319 ; Loss = 1.523151\n",
      "2024-11-29 22:47:14.143000: I runner.py:310] Step = 77100 ; steps/s = 1.63, tokens/s = 42678 (42678 target) ; Learning rate = 0.000318 ; Loss = 1.531421\n",
      "2024-11-29 22:48:16.101000: I runner.py:310] Step = 77200 ; steps/s = 1.61, tokens/s = 43114 (43114 target) ; Learning rate = 0.000318 ; Loss = 1.518863\n",
      "2024-11-29 22:49:18.024000: I runner.py:310] Step = 77300 ; steps/s = 1.62, tokens/s = 43138 (43138 target) ; Learning rate = 0.000318 ; Loss = 1.517239\n",
      "2024-11-29 22:50:20.031000: I runner.py:310] Step = 77400 ; steps/s = 1.61, tokens/s = 43078 (43078 target) ; Learning rate = 0.000318 ; Loss = 1.522198\n",
      "2024-11-29 22:51:21.470000: I runner.py:310] Step = 77500 ; steps/s = 1.63, tokens/s = 42668 (42668 target) ; Learning rate = 0.000317 ; Loss = 1.523556\n",
      "2024-11-29 22:52:23.470000: I runner.py:310] Step = 77600 ; steps/s = 1.61, tokens/s = 43051 (43051 target) ; Learning rate = 0.000317 ; Loss = 1.527844\n",
      "2024-11-29 22:53:25.393000: I runner.py:310] Step = 77700 ; steps/s = 1.62, tokens/s = 43149 (43149 target) ; Learning rate = 0.000317 ; Loss = 1.529049\n",
      "2024-11-29 22:54:27.356000: I runner.py:310] Step = 77800 ; steps/s = 1.61, tokens/s = 43151 (43151 target) ; Learning rate = 0.000317 ; Loss = 1.537856\n",
      "2024-11-29 22:55:28.795000: I runner.py:310] Step = 77900 ; steps/s = 1.63, tokens/s = 42682 (42682 target) ; Learning rate = 0.000317 ; Loss = 1.513435\n",
      "2024-11-29 22:56:30.799000: I runner.py:310] Step = 78000 ; steps/s = 1.61, tokens/s = 43090 (43090 target) ; Learning rate = 0.000316 ; Loss = 1.523312\n",
      "2024-11-29 22:57:32.786000: I runner.py:310] Step = 78100 ; steps/s = 1.61, tokens/s = 43105 (43105 target) ; Learning rate = 0.000316 ; Loss = 1.525459\n",
      "2024-11-29 22:58:34.199000: I runner.py:310] Step = 78200 ; steps/s = 1.63, tokens/s = 42618 (42618 target) ; Learning rate = 0.000316 ; Loss = 1.541189\n",
      "2024-11-29 22:59:36.150000: I runner.py:310] Step = 78300 ; steps/s = 1.61, tokens/s = 43160 (43160 target) ; Learning rate = 0.000316 ; Loss = 1.526744\n",
      "2024-11-29 23:00:38.155000: I runner.py:310] Step = 78400 ; steps/s = 1.61, tokens/s = 43083 (43083 target) ; Learning rate = 0.000316 ; Loss = 1.523180\n",
      "2024-11-29 23:01:40.143000: I runner.py:310] Step = 78500 ; steps/s = 1.61, tokens/s = 43091 (43091 target) ; Learning rate = 0.000315 ; Loss = 1.515403\n",
      "2024-11-29 23:02:41.651000: I runner.py:310] Step = 78600 ; steps/s = 1.63, tokens/s = 42604 (42604 target) ; Learning rate = 0.000315 ; Loss = 1.526389\n",
      "2024-11-29 23:03:43.622000: I runner.py:310] Step = 78700 ; steps/s = 1.61, tokens/s = 43154 (43154 target) ; Learning rate = 0.000315 ; Loss = 1.518214\n",
      "2024-11-29 23:04:45.565000: I runner.py:310] Step = 78800 ; steps/s = 1.61, tokens/s = 43115 (43115 target) ; Learning rate = 0.000315 ; Loss = 1.535207\n",
      "2024-11-29 23:05:47.536000: I runner.py:310] Step = 78900 ; steps/s = 1.61, tokens/s = 43100 (43100 target) ; Learning rate = 0.000315 ; Loss = 1.527697\n",
      "2024-11-29 23:06:49.086000: I runner.py:310] Step = 79000 ; steps/s = 1.62, tokens/s = 42561 (42561 target) ; Learning rate = 0.000314 ; Loss = 1.523289\n",
      "2024-11-29 23:07:51.099000: I runner.py:310] Step = 79100 ; steps/s = 1.61, tokens/s = 43096 (43096 target) ; Learning rate = 0.000314 ; Loss = 1.523695\n",
      "2024-11-29 23:08:53.069000: I runner.py:310] Step = 79200 ; steps/s = 1.61, tokens/s = 43131 (43131 target) ; Learning rate = 0.000314 ; Loss = 1.516429\n",
      "2024-11-29 23:09:55.053000: I runner.py:310] Step = 79300 ; steps/s = 1.61, tokens/s = 43092 (43092 target) ; Learning rate = 0.000314 ; Loss = 1.524291\n",
      "2024-11-29 23:10:56.463000: I runner.py:310] Step = 79400 ; steps/s = 1.63, tokens/s = 42667 (42667 target) ; Learning rate = 0.000314 ; Loss = 1.527442\n",
      "2024-11-29 23:11:58.424000: I runner.py:310] Step = 79500 ; steps/s = 1.61, tokens/s = 43094 (43094 target) ; Learning rate = 0.000313 ; Loss = 1.520218\n",
      "2024-11-29 23:13:00.421000: I runner.py:310] Step = 79600 ; steps/s = 1.61, tokens/s = 43119 (43119 target) ; Learning rate = 0.000313 ; Loss = 1.515382\n",
      "2024-11-29 23:14:02.366000: I runner.py:310] Step = 79700 ; steps/s = 1.61, tokens/s = 43127 (43127 target) ; Learning rate = 0.000313 ; Loss = 1.517746\n",
      "2024-11-29 23:15:03.788000: I runner.py:310] Step = 79800 ; steps/s = 1.63, tokens/s = 42668 (42668 target) ; Learning rate = 0.000313 ; Loss = 1.520301\n",
      "2024-11-29 23:16:05.764000: I runner.py:310] Step = 79900 ; steps/s = 1.61, tokens/s = 43106 (43106 target) ; Learning rate = 0.000313 ; Loss = 1.515282\n",
      "2024-11-29 23:17:07.784000: I runner.py:310] Step = 80000 ; steps/s = 1.61, tokens/s = 43065 (43065 target) ; Learning rate = 0.000312 ; Loss = 1.518678\n",
      "2024-11-29 23:17:10.004000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-80000\n",
      "2024-11-29 23:17:10.004000: I training.py:192] Running evaluation for step 80000\n",
      "2024-11-29 23:21:25.761000: I training.py:192] Evaluation result for step 80000: loss = 1.188896 ; perplexity = 3.283454\n",
      "2024-11-29 23:22:27.570000: I runner.py:310] Step = 80100 ; steps/s = 1.62, tokens/s = 43254 (43254 target) ; Learning rate = 0.000312 ; Loss = 1.529636\n",
      "2024-11-29 23:23:29.042000: I runner.py:310] Step = 80200 ; steps/s = 1.63, tokens/s = 42640 (42640 target) ; Learning rate = 0.000312 ; Loss = 1.525251\n",
      "2024-11-29 23:24:30.957000: I runner.py:310] Step = 80300 ; steps/s = 1.62, tokens/s = 43118 (43118 target) ; Learning rate = 0.000312 ; Loss = 1.517693\n",
      "2024-11-29 23:25:32.970000: I runner.py:310] Step = 80400 ; steps/s = 1.61, tokens/s = 43076 (43076 target) ; Learning rate = 0.000312 ; Loss = 1.520988\n",
      "2024-11-29 23:26:34.909000: I runner.py:310] Step = 80500 ; steps/s = 1.61, tokens/s = 43157 (43157 target) ; Learning rate = 0.000312 ; Loss = 1.516561\n",
      "2024-11-29 23:27:36.314000: I runner.py:310] Step = 80600 ; steps/s = 1.63, tokens/s = 42698 (42698 target) ; Learning rate = 0.000311 ; Loss = 1.511149\n",
      "2024-11-29 23:28:38.269000: I runner.py:310] Step = 80700 ; steps/s = 1.61, tokens/s = 43121 (43121 target) ; Learning rate = 0.000311 ; Loss = 1.518074\n",
      "2024-11-29 23:29:40.366000: I runner.py:310] Step = 80800 ; steps/s = 1.61, tokens/s = 43036 (43036 target) ; Learning rate = 0.000311 ; Loss = 1.516250\n",
      "2024-11-29 23:30:42.395000: I runner.py:310] Step = 80900 ; steps/s = 1.61, tokens/s = 43058 (43058 target) ; Learning rate = 0.000311 ; Loss = 1.519909\n",
      "2024-11-29 23:31:43.798000: I runner.py:310] Step = 81000 ; steps/s = 1.63, tokens/s = 42672 (42672 target) ; Learning rate = 0.000311 ; Loss = 1.515879\n",
      "2024-11-29 23:32:45.834000: I runner.py:310] Step = 81100 ; steps/s = 1.61, tokens/s = 43078 (43078 target) ; Learning rate = 0.000310 ; Loss = 1.517421\n",
      "2024-11-29 23:33:47.827000: I runner.py:310] Step = 81200 ; steps/s = 1.61, tokens/s = 43102 (43102 target) ; Learning rate = 0.000310 ; Loss = 1.517550\n",
      "2024-11-29 23:34:49.766000: I runner.py:310] Step = 81300 ; steps/s = 1.61, tokens/s = 43118 (43118 target) ; Learning rate = 0.000310 ; Loss = 1.518922\n",
      "2024-11-29 23:35:51.111000: I runner.py:310] Step = 81400 ; steps/s = 1.63, tokens/s = 42727 (42727 target) ; Learning rate = 0.000310 ; Loss = 1.531099\n",
      "2024-11-29 23:36:53.059000: I runner.py:310] Step = 81500 ; steps/s = 1.61, tokens/s = 43119 (43119 target) ; Learning rate = 0.000310 ; Loss = 1.515465\n",
      "2024-11-29 23:37:54.971000: I runner.py:310] Step = 81600 ; steps/s = 1.62, tokens/s = 43168 (43168 target) ; Learning rate = 0.000309 ; Loss = 1.518576\n",
      "2024-11-29 23:38:56.939000: I runner.py:310] Step = 81700 ; steps/s = 1.61, tokens/s = 43086 (43086 target) ; Learning rate = 0.000309 ; Loss = 1.514283\n",
      "2024-11-29 23:39:58.328000: I runner.py:310] Step = 81800 ; steps/s = 1.63, tokens/s = 42700 (42700 target) ; Learning rate = 0.000309 ; Loss = 1.526899\n",
      "2024-11-29 23:41:00.794000: I runner.py:310] Step = 81900 ; steps/s = 1.60, tokens/s = 42759 (42759 target) ; Learning rate = 0.000309 ; Loss = 1.515940\n",
      "2024-11-29 23:42:02.740000: I runner.py:310] Step = 82000 ; steps/s = 1.61, tokens/s = 43147 (43147 target) ; Learning rate = 0.000309 ; Loss = 1.514792\n",
      "2024-11-29 23:43:04.737000: I runner.py:310] Step = 82100 ; steps/s = 1.61, tokens/s = 43094 (43094 target) ; Learning rate = 0.000308 ; Loss = 1.516465\n",
      "2024-11-29 23:44:06.125000: I runner.py:310] Step = 82200 ; steps/s = 1.63, tokens/s = 42704 (42704 target) ; Learning rate = 0.000308 ; Loss = 1.514342\n",
      "2024-11-29 23:45:08.097000: I runner.py:310] Step = 82300 ; steps/s = 1.61, tokens/s = 43093 (43093 target) ; Learning rate = 0.000308 ; Loss = 1.520238\n",
      "2024-11-29 23:46:10.091000: I runner.py:310] Step = 82400 ; steps/s = 1.61, tokens/s = 43089 (43089 target) ; Learning rate = 0.000308 ; Loss = 1.516530\n",
      "2024-11-29 23:47:11.749000: I runner.py:310] Step = 82500 ; steps/s = 1.62, tokens/s = 42783 (42783 target) ; Learning rate = 0.000308 ; Loss = 1.523863\n",
      "2024-11-29 23:48:13.505000: I runner.py:310] Step = 82600 ; steps/s = 1.62, tokens/s = 42986 (42986 target) ; Learning rate = 0.000308 ; Loss = 1.523824\n",
      "2024-11-29 23:49:15.462000: I runner.py:310] Step = 82700 ; steps/s = 1.61, tokens/s = 43127 (43127 target) ; Learning rate = 0.000307 ; Loss = 1.513443\n",
      "2024-11-29 23:50:17.442000: I runner.py:310] Step = 82800 ; steps/s = 1.61, tokens/s = 43111 (43111 target) ; Learning rate = 0.000307 ; Loss = 1.519085\n",
      "2024-11-29 23:51:18.868000: I runner.py:310] Step = 82900 ; steps/s = 1.63, tokens/s = 42648 (42648 target) ; Learning rate = 0.000307 ; Loss = 1.521064\n",
      "2024-11-29 23:52:20.836000: I runner.py:310] Step = 83000 ; steps/s = 1.61, tokens/s = 43140 (43140 target) ; Learning rate = 0.000307 ; Loss = 1.513196\n",
      "2024-11-29 23:53:22.825000: I runner.py:310] Step = 83100 ; steps/s = 1.61, tokens/s = 43089 (43089 target) ; Learning rate = 0.000307 ; Loss = 1.532135\n",
      "2024-11-29 23:54:24.774000: I runner.py:310] Step = 83200 ; steps/s = 1.61, tokens/s = 43115 (43115 target) ; Learning rate = 0.000306 ; Loss = 1.523776\n",
      "2024-11-29 23:55:26.122000: I runner.py:310] Step = 83300 ; steps/s = 1.63, tokens/s = 42722 (42722 target) ; Learning rate = 0.000306 ; Loss = 1.515343\n",
      "2024-11-29 23:56:28.083000: I runner.py:310] Step = 83400 ; steps/s = 1.61, tokens/s = 43121 (43121 target) ; Learning rate = 0.000306 ; Loss = 1.513610\n",
      "2024-11-29 23:57:30.101000: I runner.py:310] Step = 83500 ; steps/s = 1.61, tokens/s = 43098 (43098 target) ; Learning rate = 0.000306 ; Loss = 1.516569\n",
      "2024-11-29 23:58:32.142000: I runner.py:310] Step = 83600 ; steps/s = 1.61, tokens/s = 43069 (43069 target) ; Learning rate = 0.000306 ; Loss = 1.517516\n",
      "2024-11-29 23:59:33.538000: I runner.py:310] Step = 83700 ; steps/s = 1.63, tokens/s = 42649 (42649 target) ; Learning rate = 0.000306 ; Loss = 1.509958\n",
      "2024-11-30 00:00:35.512000: I runner.py:310] Step = 83800 ; steps/s = 1.61, tokens/s = 43108 (43108 target) ; Learning rate = 0.000305 ; Loss = 1.520789\n",
      "2024-11-30 00:01:37.531000: I runner.py:310] Step = 83900 ; steps/s = 1.61, tokens/s = 43128 (43128 target) ; Learning rate = 0.000305 ; Loss = 1.521707\n",
      "2024-11-30 00:02:39.512000: I runner.py:310] Step = 84000 ; steps/s = 1.61, tokens/s = 43072 (43072 target) ; Learning rate = 0.000305 ; Loss = 1.522504\n",
      "2024-11-30 00:03:40.959000: I runner.py:310] Step = 84100 ; steps/s = 1.63, tokens/s = 42635 (42635 target) ; Learning rate = 0.000305 ; Loss = 1.516344\n",
      "2024-11-30 00:04:42.916000: I runner.py:310] Step = 84200 ; steps/s = 1.61, tokens/s = 43139 (43139 target) ; Learning rate = 0.000305 ; Loss = 1.514655\n",
      "2024-11-30 00:05:44.918000: I runner.py:310] Step = 84300 ; steps/s = 1.61, tokens/s = 43063 (43063 target) ; Learning rate = 0.000304 ; Loss = 1.511339\n",
      "2024-11-30 00:06:46.879000: I runner.py:310] Step = 84400 ; steps/s = 1.61, tokens/s = 43140 (43140 target) ; Learning rate = 0.000304 ; Loss = 1.520625\n",
      "2024-11-30 00:07:48.282000: I runner.py:310] Step = 84500 ; steps/s = 1.63, tokens/s = 42691 (42691 target) ; Learning rate = 0.000304 ; Loss = 1.528696\n",
      "2024-11-30 00:08:50.262000: I runner.py:310] Step = 84600 ; steps/s = 1.61, tokens/s = 43117 (43117 target) ; Learning rate = 0.000304 ; Loss = 1.516649\n",
      "2024-11-30 00:09:52.260000: I runner.py:310] Step = 84700 ; steps/s = 1.61, tokens/s = 43076 (43076 target) ; Learning rate = 0.000304 ; Loss = 1.517562\n",
      "2024-11-30 00:10:54.198000: I runner.py:310] Step = 84800 ; steps/s = 1.61, tokens/s = 43140 (43140 target) ; Learning rate = 0.000304 ; Loss = 1.514127\n",
      "2024-11-30 00:11:55.620000: I runner.py:310] Step = 84900 ; steps/s = 1.63, tokens/s = 42668 (42668 target) ; Learning rate = 0.000303 ; Loss = 1.523167\n",
      "2024-11-30 00:12:57.622000: I runner.py:310] Step = 85000 ; steps/s = 1.61, tokens/s = 43083 (43083 target) ; Learning rate = 0.000303 ; Loss = 1.518637\n",
      "2024-11-30 00:12:57.624000: I training.py:192] Running evaluation for step 85000\n",
      "2024-11-30 00:17:04.313000: I training.py:192] Evaluation result for step 85000: loss = 1.193896 ; perplexity = 3.299914\n",
      "2024-11-30 00:18:06.124000: I runner.py:310] Step = 85100 ; steps/s = 1.62, tokens/s = 43261 (43261 target) ; Learning rate = 0.000303 ; Loss = 1.509788\n",
      "2024-11-30 00:19:08.112000: I runner.py:310] Step = 85200 ; steps/s = 1.61, tokens/s = 43081 (43081 target) ; Learning rate = 0.000303 ; Loss = 1.517153\n",
      "2024-11-30 00:20:09.484000: I runner.py:310] Step = 85300 ; steps/s = 1.63, tokens/s = 42706 (42706 target) ; Learning rate = 0.000303 ; Loss = 1.508412\n",
      "2024-11-30 00:21:11.470000: I runner.py:310] Step = 85400 ; steps/s = 1.61, tokens/s = 43079 (43079 target) ; Learning rate = 0.000302 ; Loss = 1.512079\n",
      "2024-11-30 00:22:13.464000: I runner.py:310] Step = 85500 ; steps/s = 1.61, tokens/s = 43085 (43085 target) ; Learning rate = 0.000302 ; Loss = 1.518869\n",
      "2024-11-30 00:23:15.433000: I runner.py:310] Step = 85600 ; steps/s = 1.61, tokens/s = 43129 (43129 target) ; Learning rate = 0.000302 ; Loss = 1.520279\n",
      "2024-11-30 00:24:16.800000: I runner.py:310] Step = 85700 ; steps/s = 1.63, tokens/s = 42705 (42705 target) ; Learning rate = 0.000302 ; Loss = 1.522041\n",
      "2024-11-30 00:25:18.765000: I runner.py:310] Step = 85800 ; steps/s = 1.61, tokens/s = 43157 (43157 target) ; Learning rate = 0.000302 ; Loss = 1.510040\n",
      "2024-11-30 00:26:20.787000: I runner.py:310] Step = 85900 ; steps/s = 1.61, tokens/s = 43077 (43077 target) ; Learning rate = 0.000302 ; Loss = 1.514845\n",
      "2024-11-30 00:27:22.721000: I runner.py:310] Step = 86000 ; steps/s = 1.61, tokens/s = 43092 (43092 target) ; Learning rate = 0.000301 ; Loss = 1.520168\n",
      "2024-11-30 00:28:24.145000: I runner.py:310] Step = 86100 ; steps/s = 1.63, tokens/s = 42671 (42671 target) ; Learning rate = 0.000301 ; Loss = 1.515673\n",
      "2024-11-30 00:29:26.054000: I runner.py:310] Step = 86200 ; steps/s = 1.62, tokens/s = 43152 (43152 target) ; Learning rate = 0.000301 ; Loss = 1.514512\n",
      "2024-11-30 00:30:28.024000: I runner.py:310] Step = 86300 ; steps/s = 1.61, tokens/s = 43122 (43122 target) ; Learning rate = 0.000301 ; Loss = 1.520744\n",
      "2024-11-30 00:31:30.041000: I runner.py:310] Step = 86400 ; steps/s = 1.61, tokens/s = 43070 (43070 target) ; Learning rate = 0.000301 ; Loss = 1.517552\n",
      "2024-11-30 00:32:31.452000: I runner.py:310] Step = 86500 ; steps/s = 1.63, tokens/s = 42682 (42682 target) ; Learning rate = 0.000301 ; Loss = 1.506235\n",
      "2024-11-30 00:33:33.467000: I runner.py:310] Step = 86600 ; steps/s = 1.61, tokens/s = 43034 (43034 target) ; Learning rate = 0.000300 ; Loss = 1.520635\n",
      "2024-11-30 00:34:35.439000: I runner.py:310] Step = 86700 ; steps/s = 1.61, tokens/s = 43135 (43135 target) ; Learning rate = 0.000300 ; Loss = 1.523991\n",
      "2024-11-30 00:35:37.199000: I runner.py:310] Step = 86800 ; steps/s = 1.62, tokens/s = 42962 (42962 target) ; Learning rate = 0.000300 ; Loss = 1.517509\n",
      "2024-11-30 00:36:38.896000: I runner.py:310] Step = 86900 ; steps/s = 1.62, tokens/s = 42820 (42820 target) ; Learning rate = 0.000300 ; Loss = 1.520861\n",
      "2024-11-30 00:37:40.834000: I runner.py:310] Step = 87000 ; steps/s = 1.61, tokens/s = 43134 (43134 target) ; Learning rate = 0.000300 ; Loss = 1.515266\n",
      "2024-11-30 00:38:42.764000: I runner.py:310] Step = 87100 ; steps/s = 1.61, tokens/s = 43146 (43146 target) ; Learning rate = 0.000299 ; Loss = 1.516389\n",
      "2024-11-30 00:39:44.150000: I runner.py:310] Step = 87200 ; steps/s = 1.63, tokens/s = 42656 (42656 target) ; Learning rate = 0.000299 ; Loss = 1.519428\n",
      "2024-11-30 00:40:46.122000: I runner.py:310] Step = 87300 ; steps/s = 1.61, tokens/s = 43133 (43133 target) ; Learning rate = 0.000299 ; Loss = 1.505196\n",
      "2024-11-30 00:41:48.139000: I runner.py:310] Step = 87400 ; steps/s = 1.61, tokens/s = 43055 (43055 target) ; Learning rate = 0.000299 ; Loss = 1.521238\n",
      "2024-11-30 00:42:50.071000: I runner.py:310] Step = 87500 ; steps/s = 1.61, tokens/s = 43139 (43139 target) ; Learning rate = 0.000299 ; Loss = 1.518818\n",
      "2024-11-30 00:43:51.576000: I runner.py:310] Step = 87600 ; steps/s = 1.63, tokens/s = 42622 (42622 target) ; Learning rate = 0.000299 ; Loss = 1.516063\n",
      "2024-11-30 00:44:53.550000: I runner.py:310] Step = 87700 ; steps/s = 1.61, tokens/s = 43124 (43124 target) ; Learning rate = 0.000298 ; Loss = 1.507942\n",
      "2024-11-30 00:45:55.486000: I runner.py:310] Step = 87800 ; steps/s = 1.61, tokens/s = 43125 (43125 target) ; Learning rate = 0.000298 ; Loss = 1.519059\n",
      "2024-11-30 00:46:57.490000: I runner.py:310] Step = 87900 ; steps/s = 1.61, tokens/s = 43070 (43070 target) ; Learning rate = 0.000298 ; Loss = 1.517709\n",
      "2024-11-30 00:47:58.957000: I runner.py:310] Step = 88000 ; steps/s = 1.63, tokens/s = 42647 (42647 target) ; Learning rate = 0.000298 ; Loss = 1.513362\n",
      "2024-11-30 00:49:00.888000: I runner.py:310] Step = 88100 ; steps/s = 1.61, tokens/s = 43133 (43133 target) ; Learning rate = 0.000298 ; Loss = 1.517452\n",
      "2024-11-30 00:50:02.842000: I runner.py:310] Step = 88200 ; steps/s = 1.61, tokens/s = 43133 (43133 target) ; Learning rate = 0.000298 ; Loss = 1.509426\n",
      "2024-11-30 00:51:04.851000: I runner.py:310] Step = 88300 ; steps/s = 1.61, tokens/s = 43084 (43084 target) ; Learning rate = 0.000297 ; Loss = 1.513018\n",
      "2024-11-30 00:52:06.178000: I runner.py:310] Step = 88400 ; steps/s = 1.63, tokens/s = 42726 (42726 target) ; Learning rate = 0.000297 ; Loss = 1.510729\n",
      "2024-11-30 00:53:08.159000: I runner.py:310] Step = 88500 ; steps/s = 1.61, tokens/s = 43103 (43103 target) ; Learning rate = 0.000297 ; Loss = 1.509407\n",
      "2024-11-30 00:54:10.147000: I runner.py:310] Step = 88600 ; steps/s = 1.61, tokens/s = 43082 (43082 target) ; Learning rate = 0.000297 ; Loss = 1.512743\n",
      "2024-11-30 00:55:12.130000: I runner.py:310] Step = 88700 ; steps/s = 1.61, tokens/s = 43124 (43124 target) ; Learning rate = 0.000297 ; Loss = 1.516336\n",
      "2024-11-30 00:56:13.547000: I runner.py:310] Step = 88800 ; steps/s = 1.63, tokens/s = 42674 (42674 target) ; Learning rate = 0.000297 ; Loss = 1.515482\n",
      "2024-11-30 00:57:15.493000: I runner.py:310] Step = 88900 ; steps/s = 1.61, tokens/s = 43135 (43135 target) ; Learning rate = 0.000296 ; Loss = 1.522959\n",
      "2024-11-30 00:58:17.411000: I runner.py:310] Step = 89000 ; steps/s = 1.62, tokens/s = 43155 (43155 target) ; Learning rate = 0.000296 ; Loss = 1.511343\n",
      "2024-11-30 00:59:19.418000: I runner.py:310] Step = 89100 ; steps/s = 1.61, tokens/s = 43090 (43090 target) ; Learning rate = 0.000296 ; Loss = 1.516393\n",
      "2024-11-30 01:00:20.836000: I runner.py:310] Step = 89200 ; steps/s = 1.63, tokens/s = 42638 (42638 target) ; Learning rate = 0.000296 ; Loss = 1.523017\n",
      "2024-11-30 01:01:22.788000: I runner.py:310] Step = 89300 ; steps/s = 1.61, tokens/s = 43137 (43137 target) ; Learning rate = 0.000296 ; Loss = 1.513582\n",
      "2024-11-30 01:02:24.810000: I runner.py:310] Step = 89400 ; steps/s = 1.61, tokens/s = 43107 (43107 target) ; Learning rate = 0.000296 ; Loss = 1.518334\n",
      "2024-11-30 01:03:26.776000: I runner.py:310] Step = 89500 ; steps/s = 1.61, tokens/s = 43096 (43096 target) ; Learning rate = 0.000295 ; Loss = 1.524382\n",
      "2024-11-30 01:04:28.158000: I runner.py:310] Step = 89600 ; steps/s = 1.63, tokens/s = 42693 (42693 target) ; Learning rate = 0.000295 ; Loss = 1.523531\n",
      "2024-11-30 01:05:30.151000: I runner.py:310] Step = 89700 ; steps/s = 1.61, tokens/s = 43137 (43137 target) ; Learning rate = 0.000295 ; Loss = 1.510911\n",
      "2024-11-30 01:06:32.177000: I runner.py:310] Step = 89800 ; steps/s = 1.61, tokens/s = 43069 (43069 target) ; Learning rate = 0.000295 ; Loss = 1.506952\n",
      "2024-11-30 01:07:34.145000: I runner.py:310] Step = 89900 ; steps/s = 1.61, tokens/s = 43083 (43083 target) ; Learning rate = 0.000295 ; Loss = 1.512876\n",
      "2024-11-30 01:08:35.567000: I runner.py:310] Step = 90000 ; steps/s = 1.63, tokens/s = 42692 (42692 target) ; Learning rate = 0.000295 ; Loss = 1.506457\n",
      "2024-11-30 01:08:37.864000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-90000\n",
      "2024-11-30 01:08:37.865000: I training.py:192] Running evaluation for step 90000\n",
      "2024-11-30 01:12:46.600000: I training.py:192] Evaluation result for step 90000: loss = 1.196875 ; perplexity = 3.309757\n",
      "2024-11-30 01:13:48.397000: I runner.py:310] Step = 90100 ; steps/s = 1.62, tokens/s = 43234 (43234 target) ; Learning rate = 0.000294 ; Loss = 1.515576\n",
      "2024-11-30 01:14:50.351000: I runner.py:310] Step = 90200 ; steps/s = 1.61, tokens/s = 43106 (43106 target) ; Learning rate = 0.000294 ; Loss = 1.514760\n",
      "2024-11-30 01:15:52.359000: I runner.py:310] Step = 90300 ; steps/s = 1.61, tokens/s = 43088 (43088 target) ; Learning rate = 0.000294 ; Loss = 1.514114\n",
      "2024-11-30 01:16:53.822000: I runner.py:310] Step = 90400 ; steps/s = 1.63, tokens/s = 42629 (42629 target) ; Learning rate = 0.000294 ; Loss = 1.505162\n",
      "2024-11-30 01:17:55.887000: I runner.py:310] Step = 90500 ; steps/s = 1.61, tokens/s = 43078 (43078 target) ; Learning rate = 0.000294 ; Loss = 1.516639\n",
      "2024-11-30 01:18:57.913000: I runner.py:310] Step = 90600 ; steps/s = 1.61, tokens/s = 43052 (43052 target) ; Learning rate = 0.000294 ; Loss = 1.511986\n",
      "2024-11-30 01:19:59.958000: I runner.py:310] Step = 90700 ; steps/s = 1.61, tokens/s = 43050 (43050 target) ; Learning rate = 0.000293 ; Loss = 1.520723\n",
      "2024-11-30 01:21:01.401000: I runner.py:310] Step = 90800 ; steps/s = 1.63, tokens/s = 42662 (42662 target) ; Learning rate = 0.000293 ; Loss = 1.521966\n",
      "2024-11-30 01:22:03.399000: I runner.py:310] Step = 90900 ; steps/s = 1.61, tokens/s = 43082 (43082 target) ; Learning rate = 0.000293 ; Loss = 1.512649\n",
      "2024-11-30 01:23:05.425000: I runner.py:310] Step = 91000 ; steps/s = 1.61, tokens/s = 43061 (43061 target) ; Learning rate = 0.000293 ; Loss = 1.518763\n",
      "2024-11-30 01:24:07.364000: I runner.py:310] Step = 91100 ; steps/s = 1.61, tokens/s = 43025 (43025 target) ; Learning rate = 0.000293 ; Loss = 1.522238\n",
      "2024-11-30 01:25:08.861000: I runner.py:310] Step = 91200 ; steps/s = 1.63, tokens/s = 42730 (42730 target) ; Learning rate = 0.000293 ; Loss = 1.507922\n",
      "2024-11-30 01:26:10.853000: I runner.py:310] Step = 91300 ; steps/s = 1.61, tokens/s = 43144 (43144 target) ; Learning rate = 0.000293 ; Loss = 1.511464\n",
      "2024-11-30 01:27:12.842000: I runner.py:310] Step = 91400 ; steps/s = 1.61, tokens/s = 43105 (43105 target) ; Learning rate = 0.000292 ; Loss = 1.521635\n",
      "2024-11-30 01:28:14.180000: I runner.py:310] Step = 91500 ; steps/s = 1.63, tokens/s = 42671 (42671 target) ; Learning rate = 0.000292 ; Loss = 1.513056\n",
      "2024-11-30 01:29:16.128000: I runner.py:310] Step = 91600 ; steps/s = 1.61, tokens/s = 43130 (43130 target) ; Learning rate = 0.000292 ; Loss = 1.512286\n",
      "2024-11-30 01:30:18.100000: I runner.py:310] Step = 91700 ; steps/s = 1.61, tokens/s = 43114 (43114 target) ; Learning rate = 0.000292 ; Loss = 1.509242\n",
      "2024-11-30 01:31:20.056000: I runner.py:310] Step = 91800 ; steps/s = 1.61, tokens/s = 43114 (43114 target) ; Learning rate = 0.000292 ; Loss = 1.518708\n",
      "2024-11-30 01:32:21.467000: I runner.py:310] Step = 91900 ; steps/s = 1.63, tokens/s = 42691 (42691 target) ; Learning rate = 0.000292 ; Loss = 1.520053\n",
      "2024-11-30 01:33:23.423000: I runner.py:310] Step = 92000 ; steps/s = 1.61, tokens/s = 43094 (43094 target) ; Learning rate = 0.000291 ; Loss = 1.510869\n",
      "2024-11-30 01:34:25.414000: I runner.py:310] Step = 92100 ; steps/s = 1.61, tokens/s = 43103 (43103 target) ; Learning rate = 0.000291 ; Loss = 1.505371\n",
      "2024-11-30 01:35:27.450000: I runner.py:310] Step = 92200 ; steps/s = 1.61, tokens/s = 43083 (43083 target) ; Learning rate = 0.000291 ; Loss = 1.511440\n",
      "2024-11-30 01:36:28.895000: I runner.py:310] Step = 92300 ; steps/s = 1.63, tokens/s = 42668 (42668 target) ; Learning rate = 0.000291 ; Loss = 1.510943\n",
      "2024-11-30 01:37:30.941000: I runner.py:310] Step = 92400 ; steps/s = 1.61, tokens/s = 43046 (43046 target) ; Learning rate = 0.000291 ; Loss = 1.511898\n",
      "2024-11-30 01:38:32.882000: I runner.py:310] Step = 92500 ; steps/s = 1.61, tokens/s = 43132 (43132 target) ; Learning rate = 0.000291 ; Loss = 1.511834\n",
      "2024-11-30 01:39:35.338000: I runner.py:310] Step = 92600 ; steps/s = 1.60, tokens/s = 42762 (42762 target) ; Learning rate = 0.000290 ; Loss = 1.517234\n",
      "2024-11-30 01:40:36.717000: I runner.py:310] Step = 92700 ; steps/s = 1.63, tokens/s = 42732 (42732 target) ; Learning rate = 0.000290 ; Loss = 1.515533\n",
      "2024-11-30 01:41:38.648000: I runner.py:310] Step = 92800 ; steps/s = 1.61, tokens/s = 43149 (43149 target) ; Learning rate = 0.000290 ; Loss = 1.509624\n",
      "2024-11-30 01:42:40.616000: I runner.py:310] Step = 92900 ; steps/s = 1.61, tokens/s = 43116 (43116 target) ; Learning rate = 0.000290 ; Loss = 1.508079\n",
      "2024-11-30 01:43:42.567000: I runner.py:310] Step = 93000 ; steps/s = 1.61, tokens/s = 43099 (43099 target) ; Learning rate = 0.000290 ; Loss = 1.508238\n",
      "2024-11-30 01:44:43.936000: I runner.py:310] Step = 93100 ; steps/s = 1.63, tokens/s = 42707 (42707 target) ; Learning rate = 0.000290 ; Loss = 1.510312\n",
      "2024-11-30 01:45:45.853000: I runner.py:310] Step = 93200 ; steps/s = 1.62, tokens/s = 43149 (43149 target) ; Learning rate = 0.000290 ; Loss = 1.512140\n",
      "2024-11-30 01:46:47.787000: I runner.py:310] Step = 93300 ; steps/s = 1.61, tokens/s = 43155 (43155 target) ; Learning rate = 0.000289 ; Loss = 1.512132\n",
      "2024-11-30 01:47:49.644000: I runner.py:310] Step = 93400 ; steps/s = 1.62, tokens/s = 43194 (43194 target) ; Learning rate = 0.000289 ; Loss = 1.517410\n",
      "2024-11-30 01:48:51.077000: I runner.py:310] Step = 93500 ; steps/s = 1.63, tokens/s = 42604 (42604 target) ; Learning rate = 0.000289 ; Loss = 1.506202\n",
      "2024-11-30 01:49:53.031000: I runner.py:310] Step = 93600 ; steps/s = 1.61, tokens/s = 43142 (43142 target) ; Learning rate = 0.000289 ; Loss = 1.515365\n",
      "2024-11-30 01:50:54.888000: I runner.py:310] Step = 93700 ; steps/s = 1.62, tokens/s = 43207 (43207 target) ; Learning rate = 0.000289 ; Loss = 1.511200\n",
      "2024-11-30 01:51:56.779000: I runner.py:310] Step = 93800 ; steps/s = 1.62, tokens/s = 43153 (43153 target) ; Learning rate = 0.000289 ; Loss = 1.515623\n",
      "2024-11-30 01:52:58.101000: I runner.py:310] Step = 93900 ; steps/s = 1.63, tokens/s = 42746 (42746 target) ; Learning rate = 0.000288 ; Loss = 1.517760\n",
      "2024-11-30 01:54:00.028000: I runner.py:310] Step = 94000 ; steps/s = 1.61, tokens/s = 43137 (43137 target) ; Learning rate = 0.000288 ; Loss = 1.504880\n",
      "2024-11-30 01:55:01.935000: I runner.py:310] Step = 94100 ; steps/s = 1.62, tokens/s = 43152 (43152 target) ; Learning rate = 0.000288 ; Loss = 1.514038\n",
      "2024-11-30 01:56:03.976000: I runner.py:310] Step = 94200 ; steps/s = 1.61, tokens/s = 43078 (43078 target) ; Learning rate = 0.000288 ; Loss = 1.509548\n",
      "2024-11-30 01:57:05.305000: I runner.py:310] Step = 94300 ; steps/s = 1.63, tokens/s = 42737 (42737 target) ; Learning rate = 0.000288 ; Loss = 1.512048\n",
      "2024-11-30 01:58:07.261000: I runner.py:310] Step = 94400 ; steps/s = 1.61, tokens/s = 43125 (43125 target) ; Learning rate = 0.000288 ; Loss = 1.510023\n",
      "2024-11-30 01:59:09.893000: I runner.py:310] Step = 94500 ; steps/s = 1.60, tokens/s = 42642 (42642 target) ; Learning rate = 0.000288 ; Loss = 1.514663\n",
      "2024-11-30 02:00:12.392000: I runner.py:310] Step = 94600 ; steps/s = 1.60, tokens/s = 42742 (42742 target) ; Learning rate = 0.000287 ; Loss = 1.512080\n",
      "2024-11-30 02:01:13.728000: I runner.py:310] Step = 94700 ; steps/s = 1.63, tokens/s = 42732 (42732 target) ; Learning rate = 0.000287 ; Loss = 1.529378\n",
      "2024-11-30 02:02:15.676000: I runner.py:310] Step = 94800 ; steps/s = 1.61, tokens/s = 43166 (43166 target) ; Learning rate = 0.000287 ; Loss = 1.508976\n",
      "2024-11-30 02:03:17.583000: I runner.py:310] Step = 94900 ; steps/s = 1.62, tokens/s = 43126 (43126 target) ; Learning rate = 0.000287 ; Loss = 1.513308\n",
      "2024-11-30 02:04:19.528000: I runner.py:310] Step = 95000 ; steps/s = 1.61, tokens/s = 43116 (43116 target) ; Learning rate = 0.000287 ; Loss = 1.515250\n",
      "2024-11-30 02:04:19.530000: I training.py:192] Running evaluation for step 95000\n",
      "2024-11-30 02:08:25.177000: I training.py:192] Evaluation result for step 95000: loss = 1.203368 ; perplexity = 3.331317\n",
      "2024-11-30 02:09:26.363000: I runner.py:310] Step = 95100 ; steps/s = 1.63, tokens/s = 42832 (42832 target) ; Learning rate = 0.000287 ; Loss = 1.513028\n",
      "2024-11-30 02:10:28.277000: I runner.py:310] Step = 95200 ; steps/s = 1.62, tokens/s = 43170 (43170 target) ; Learning rate = 0.000286 ; Loss = 1.510079\n",
      "2024-11-30 02:11:30.164000: I runner.py:310] Step = 95300 ; steps/s = 1.62, tokens/s = 43181 (43181 target) ; Learning rate = 0.000286 ; Loss = 1.510793\n",
      "2024-11-30 02:12:32.041000: I runner.py:310] Step = 95400 ; steps/s = 1.62, tokens/s = 43145 (43145 target) ; Learning rate = 0.000286 ; Loss = 1.511015\n",
      "2024-11-30 02:13:33.376000: I runner.py:310] Step = 95500 ; steps/s = 1.63, tokens/s = 42749 (42749 target) ; Learning rate = 0.000286 ; Loss = 1.511844\n",
      "2024-11-30 02:14:35.248000: I runner.py:310] Step = 95600 ; steps/s = 1.62, tokens/s = 43179 (43179 target) ; Learning rate = 0.000286 ; Loss = 1.509433\n",
      "2024-11-30 02:15:37.117000: I runner.py:310] Step = 95700 ; steps/s = 1.62, tokens/s = 43158 (43158 target) ; Learning rate = 0.000286 ; Loss = 1.515816\n",
      "2024-11-30 02:16:38.375000: I runner.py:310] Step = 95800 ; steps/s = 1.63, tokens/s = 42793 (42793 target) ; Learning rate = 0.000286 ; Loss = 1.514536\n",
      "2024-11-30 02:17:40.280000: I runner.py:310] Step = 95900 ; steps/s = 1.62, tokens/s = 43170 (43170 target) ; Learning rate = 0.000285 ; Loss = 1.509584\n",
      "2024-11-30 02:18:42.131000: I runner.py:310] Step = 96000 ; steps/s = 1.62, tokens/s = 43180 (43180 target) ; Learning rate = 0.000285 ; Loss = 1.511034\n",
      "2024-11-30 02:19:43.960000: I runner.py:310] Step = 96100 ; steps/s = 1.62, tokens/s = 43208 (43208 target) ; Learning rate = 0.000285 ; Loss = 1.511645\n",
      "2024-11-30 02:20:45.329000: I runner.py:310] Step = 96200 ; steps/s = 1.63, tokens/s = 42708 (42708 target) ; Learning rate = 0.000285 ; Loss = 1.512696\n",
      "2024-11-30 02:21:47.154000: I runner.py:310] Step = 96300 ; steps/s = 1.62, tokens/s = 43233 (43233 target) ; Learning rate = 0.000285 ; Loss = 1.522782\n",
      "2024-11-30 02:22:49.182000: I runner.py:310] Step = 96400 ; steps/s = 1.61, tokens/s = 43061 (43061 target) ; Learning rate = 0.000285 ; Loss = 1.507889\n",
      "2024-11-30 02:23:51.288000: I runner.py:310] Step = 96500 ; steps/s = 1.61, tokens/s = 43041 (43041 target) ; Learning rate = 0.000285 ; Loss = 1.509734\n",
      "2024-11-30 02:24:52.672000: I runner.py:310] Step = 96600 ; steps/s = 1.63, tokens/s = 42682 (42682 target) ; Learning rate = 0.000284 ; Loss = 1.506342\n",
      "2024-11-30 02:25:54.687000: I runner.py:310] Step = 96700 ; steps/s = 1.61, tokens/s = 43092 (43092 target) ; Learning rate = 0.000284 ; Loss = 1.511228\n",
      "2024-11-30 02:26:56.596000: I runner.py:310] Step = 96800 ; steps/s = 1.62, tokens/s = 43140 (43140 target) ; Learning rate = 0.000284 ; Loss = 1.508432\n",
      "2024-11-30 02:27:58.476000: I runner.py:310] Step = 96900 ; steps/s = 1.62, tokens/s = 43175 (43175 target) ; Learning rate = 0.000284 ; Loss = 1.508564\n",
      "2024-11-30 02:28:59.903000: I runner.py:310] Step = 97000 ; steps/s = 1.63, tokens/s = 42674 (42674 target) ; Learning rate = 0.000284 ; Loss = 1.508512\n",
      "2024-11-30 02:30:01.873000: I runner.py:310] Step = 97100 ; steps/s = 1.61, tokens/s = 43106 (43106 target) ; Learning rate = 0.000284 ; Loss = 1.508843\n",
      "2024-11-30 02:31:03.808000: I runner.py:310] Step = 97200 ; steps/s = 1.61, tokens/s = 43104 (43104 target) ; Learning rate = 0.000284 ; Loss = 1.510731\n",
      "2024-11-30 02:32:05.763000: I runner.py:310] Step = 97300 ; steps/s = 1.61, tokens/s = 43127 (43127 target) ; Learning rate = 0.000283 ; Loss = 1.512321\n",
      "2024-11-30 02:33:07.260000: I runner.py:310] Step = 97400 ; steps/s = 1.63, tokens/s = 42637 (42637 target) ; Learning rate = 0.000283 ; Loss = 1.513377\n",
      "2024-11-30 02:34:09.235000: I runner.py:310] Step = 97500 ; steps/s = 1.61, tokens/s = 43117 (43117 target) ; Learning rate = 0.000283 ; Loss = 1.509390\n",
      "2024-11-30 02:35:11.264000: I runner.py:310] Step = 97600 ; steps/s = 1.61, tokens/s = 43073 (43073 target) ; Learning rate = 0.000283 ; Loss = 1.505840\n",
      "2024-11-30 02:36:13.291000: I runner.py:310] Step = 97700 ; steps/s = 1.61, tokens/s = 43101 (43101 target) ; Learning rate = 0.000283 ; Loss = 1.509183\n",
      "2024-11-30 02:37:14.620000: I runner.py:310] Step = 97800 ; steps/s = 1.63, tokens/s = 42703 (42703 target) ; Learning rate = 0.000283 ; Loss = 1.515460\n",
      "2024-11-30 02:38:16.588000: I runner.py:310] Step = 97900 ; steps/s = 1.61, tokens/s = 43125 (43125 target) ; Learning rate = 0.000282 ; Loss = 1.503201\n",
      "2024-11-30 02:39:18.548000: I runner.py:310] Step = 98000 ; steps/s = 1.61, tokens/s = 43123 (43123 target) ; Learning rate = 0.000282 ; Loss = 1.507666\n",
      "2024-11-30 02:40:20.484000: I runner.py:310] Step = 98100 ; steps/s = 1.61, tokens/s = 43113 (43113 target) ; Learning rate = 0.000282 ; Loss = 1.507245\n",
      "2024-11-30 02:41:21.921000: I runner.py:310] Step = 98200 ; steps/s = 1.63, tokens/s = 42651 (42651 target) ; Learning rate = 0.000282 ; Loss = 1.515587\n",
      "2024-11-30 02:42:23.824000: I runner.py:310] Step = 98300 ; steps/s = 1.62, tokens/s = 43169 (43169 target) ; Learning rate = 0.000282 ; Loss = 1.505366\n",
      "2024-11-30 02:43:25.879000: I runner.py:310] Step = 98400 ; steps/s = 1.61, tokens/s = 43067 (43067 target) ; Learning rate = 0.000282 ; Loss = 1.507986\n",
      "2024-11-30 02:44:27.866000: I runner.py:310] Step = 98500 ; steps/s = 1.61, tokens/s = 43099 (43099 target) ; Learning rate = 0.000282 ; Loss = 1.506551\n",
      "2024-11-30 02:45:29.289000: I runner.py:310] Step = 98600 ; steps/s = 1.63, tokens/s = 42635 (42635 target) ; Learning rate = 0.000281 ; Loss = 1.511616\n",
      "2024-11-30 02:46:31.262000: I runner.py:310] Step = 98700 ; steps/s = 1.61, tokens/s = 43138 (43138 target) ; Learning rate = 0.000281 ; Loss = 1.502951\n",
      "2024-11-30 02:47:33.132000: I runner.py:310] Step = 98800 ; steps/s = 1.62, tokens/s = 43162 (43162 target) ; Learning rate = 0.000281 ; Loss = 1.505688\n",
      "2024-11-30 02:48:35.041000: I runner.py:310] Step = 98900 ; steps/s = 1.62, tokens/s = 43151 (43151 target) ; Learning rate = 0.000281 ; Loss = 1.510299\n",
      "2024-11-30 02:49:36.584000: I runner.py:310] Step = 99000 ; steps/s = 1.63, tokens/s = 42606 (42606 target) ; Learning rate = 0.000281 ; Loss = 1.509410\n",
      "2024-11-30 02:50:38.418000: I runner.py:310] Step = 99100 ; steps/s = 1.62, tokens/s = 43213 (43213 target) ; Learning rate = 0.000281 ; Loss = 1.506079\n",
      "2024-11-30 02:51:40.312000: I runner.py:310] Step = 99200 ; steps/s = 1.62, tokens/s = 43154 (43154 target) ; Learning rate = 0.000281 ; Loss = 1.508059\n",
      "2024-11-30 02:52:42.281000: I runner.py:310] Step = 99300 ; steps/s = 1.61, tokens/s = 43109 (43109 target) ; Learning rate = 0.000280 ; Loss = 1.507626\n",
      "2024-11-30 02:53:43.618000: I runner.py:310] Step = 99400 ; steps/s = 1.63, tokens/s = 42739 (42739 target) ; Learning rate = 0.000280 ; Loss = 1.499863\n",
      "2024-11-30 02:54:45.522000: I runner.py:310] Step = 99500 ; steps/s = 1.62, tokens/s = 43147 (43147 target) ; Learning rate = 0.000280 ; Loss = 1.509829\n",
      "2024-11-30 02:55:47.492000: I runner.py:310] Step = 99600 ; steps/s = 1.61, tokens/s = 43100 (43100 target) ; Learning rate = 0.000280 ; Loss = 1.513646\n",
      "2024-11-30 02:56:49.879000: I runner.py:310] Step = 99700 ; steps/s = 1.60, tokens/s = 42829 (42829 target) ; Learning rate = 0.000280 ; Loss = 1.515517\n",
      "2024-11-30 02:57:52.874000: I runner.py:310] Step = 99800 ; steps/s = 1.59, tokens/s = 41610 (41610 target) ; Learning rate = 0.000280 ; Loss = 1.510773\n",
      "2024-11-30 02:58:56.398000: I runner.py:310] Step = 99900 ; steps/s = 1.57, tokens/s = 42059 (42059 target) ; Learning rate = 0.000280 ; Loss = 1.507328\n",
      "2024-11-30 02:59:59.669000: I runner.py:310] Step = 100000 ; steps/s = 1.58, tokens/s = 42210 (42210 target) ; Learning rate = 0.000280 ; Loss = 1.507688\n",
      "2024-11-30 03:00:01.862000: I training.py:176] Saved checkpoint POS_TR_KK_EN-2/ckpt-100000\n",
      "2024-11-30 03:00:01.863000: I training.py:192] Running evaluation for step 100000\n",
      "2024-11-30 03:04:00.856000: I training.py:192] Evaluation result for step 100000: loss = 1.203854 ; perplexity = 3.332937\n",
      "2024-11-30 03:05:02.551000: I runner.py:310] Step = 100100 ; steps/s = 1.62, tokens/s = 42485 (42485 target) ; Learning rate = 0.000279 ; Loss = 1.506700\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Tr-En(Tatoeba)(POS Tags) -> Kk-En(POS Tags)\n",
    "!onmt-main --model kk-tr-en-modelim.py --config kk-tr-en-pos-2.yml --auto_config --checkpoint_path POS_TR_KK_EN/ckpt-100000 train --with_eval --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1caece96-9fd7-4686-8a2d-6cbe039d7826",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-30 03:05:27.557556: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 03:05:28.341958: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-30 03:05:28.342028: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-30 03:05:28.342036: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-11-30 03:05:29.323000: I main.py:308] Loading model description from POS_TR_KK_EN-2/model_description.py\n",
      "2024-11-30 03:05:29.520000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-11-30 03:05:29.520000: I main.py:315] Using model:\n",
      "(model): MultiFeaturesTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): ParallelInputter(\n",
      "      (inputters): ListWrapper(\n",
      "        (0): WordEmbedder()\n",
      "        (1): WordEmbedder()\n",
      "      )\n",
      "      (reducer): ConcatReducer()\n",
      "    )\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): ParallelInputter(\n",
      "        (inputters): ListWrapper(\n",
      "          (0): WordEmbedder()\n",
      "          (1): WordEmbedder()\n",
      "        )\n",
      "        (reducer): ConcatReducer()\n",
      "      )\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-11-30 03:05:29.525000: I main.py:340] Using parameters:\n",
      "data:\n",
      "  eval_features_file:\n",
      "  - KK_tokens_valid_shared\n",
      "  - KK_pos_tags_valid_shared.txt\n",
      "  eval_labels_file: KK_valid_target_tokens_shared.txt\n",
      "  source_1_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  source_2_vocabulary: RoBERTa_KK_unique_pos\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file:\n",
      "  - KK_tokens_train_shared\n",
      "  - KK_pos_tags_train_shared.txt\n",
      "  train_labels_file: KK_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: POS_TR_KK_EN-2\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 205000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-11-30 03:05:29.705347: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 03:05:30.303772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-11-30 03:05:30.457000: I inputter.py:316] Initialized source_1 input layer:\n",
      "2024-11-30 03:05:30.457000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-11-30 03:05:30.457000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-11-30 03:05:30.460000: I inputter.py:316] Initialized source_2 input layer:\n",
      "2024-11-30 03:05:30.460000: I inputter.py:316]  - vocabulary size: 21\n",
      "2024-11-30 03:05:30.460000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-11-30 03:05:30.533000: I inputter.py:316] Initialized target input layer:\n",
      "2024-11-30 03:05:30.533000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-11-30 03:05:30.533000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-11-30 03:05:30.553000: I runner.py:462] Restored checkpoint POS_TR_KK_EN-2/ckpt-100000\n",
      "2024-11-30 03:05:30.595000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-11-30 03:05:31.339173: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-11-30 03:05:31.458000: I runner.py:471] Tracing and optimizing the inference graph...\n",
      "2024-11-30 03:05:45.113954: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-11-30 03:05:45.967851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-11-30 03:05:57.559000: I runner.py:471] 1055 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-11-30 03:06:07.751000: I runner.py:471] 2047 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-11-30 03:06:17.816000: I runner.py:471] 2911 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-11-30 03:06:28.223000: I runner.py:471] 3807 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-11-30 03:06:38.413000: I runner.py:471] 4767 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-11-30 03:06:48.493000: I runner.py:471] 5695 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-11-30 03:06:59.006000: I runner.py:471] 6559 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-11-30 03:07:09.212000: I runner.py:471] 7391 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-11-30 03:07:19.947000: I runner.py:471] 8383 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-11-30 03:07:30.059000: I runner.py:471] 9311 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-11-30 03:07:40.274000: I runner.py:471] 10271 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-11-30 03:07:50.371000: I runner.py:471] 11071 predictions are buffered, but waiting for the prediction of queued line 3 to advance the output...\n",
      "2024-11-30 03:08:02.100000: I runner.py:471] 12046 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-11-30 03:08:12.294000: I runner.py:471] 12750 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-11-30 03:08:22.616000: I runner.py:471] 13710 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-11-30 03:08:32.759000: I runner.py:471] 14638 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-11-30 03:08:42.875000: I runner.py:471] 15438 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-11-30 03:08:53.120000: I runner.py:471] 16430 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-11-30 03:09:03.751000: I runner.py:471] 17326 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-11-30 03:09:13.905000: I runner.py:471] 17880 predictions are buffered, but waiting for the prediction of queued line 52 to advance the output...\n",
      "2024-11-30 03:09:30.025000: I runner.py:471] 17797 predictions are buffered, but waiting for the prediction of queued line 281 to advance the output...\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 onmt-main --config kk-tr-en-pos-2.yml --auto_config --checkpoint_path POS_TR_KK_EN-2/ckpt-100000 infer --features_file KK_tokens_test_shared KK_pos_tags_test_shared.txt --predictions_file output_tr_kk_en_pos.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa86b92-877e-4dde-bd2e-9e16038b71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 MT-Preparation/subwording/3-desubword.py en_shared_vocab.model output_tr_kk_en_pos.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25bc6d2e-2da1-4044-9cd5-545299bd702d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference first sentence: In the developed world, this figure is 35 25%\n",
      "Translated first sentence: In the developed countries of the world , this figure is 355%\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "BLEU:  BLEU = 52.47 73.0/56.8/46.6/39.2 (BP = 1.000 ratio = 1.106 hyp_len = 458384 ref_len = 414303)\n",
      "CHRF:  chrF2 = 76.48\n"
     ]
    }
   ],
   "source": [
    "# BLEU and chrF scores\n",
    "!python3 compute-bleu.py en_test_shuffled.txt-filtered.en output_tr_kk_en_pos.txt.desubword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1753970-ef96-4558-8a7f-1941f7e78b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama METEOR Puan: 0.7632754220252569\n"
     ]
    }
   ],
   "source": [
    "# Average METEOR score (Ortalama METEOR Puan)\n",
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "def read_and_tokenize_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return [nltk.word_tokenize(line.strip()) for line in lines]\n",
    "\n",
    "def calculate_meteor(reference_file, hypothesis_file):\n",
    "    references = read_and_tokenize_file(reference_file)\n",
    "    hypotheses = read_and_tokenize_file(hypothesis_file)\n",
    "    \n",
    "    if len(references) != len(hypotheses):\n",
    "        raise ValueError(\"Dosyalarn satr saylar elemiyor\")\n",
    "\n",
    "    total_meteor_score = 0.0\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        total_meteor_score += meteor_score([ref], hyp)\n",
    "\n",
    "    average_meteor_score = total_meteor_score / len(references)\n",
    "    return average_meteor_score\n",
    "\n",
    "reference_file = 'en_test_shuffled.txt-filtered.en'\n",
    "hypothesis_file = 'output_tr_kk_en_pos.txt.desubword'\n",
    "\n",
    "score = calculate_meteor(reference_file, hypothesis_file)\n",
    "print(f\"Ortalama METEOR Puan: {score}\") #Average METEOR score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db9fe25c-2d71-40d1-ad1a-573ce32e7b94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-30 03:11:08.073872: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 03:11:08.827506: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-30 03:11:08.827586: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-30 03:11:08.827597: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-11-30 03:11:09.767000: I onmt-main:8] Creating model directory POS_KK_TR_EN\n",
      "2024-11-30 03:11:09.966000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-11-30 03:11:09.966000: I main.py:315] Using model:\n",
      "(model): MultiFeaturesTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): ParallelInputter(\n",
      "      (inputters): ListWrapper(\n",
      "        (0): WordEmbedder()\n",
      "        (1): WordEmbedder()\n",
      "      )\n",
      "      (reducer): ConcatReducer()\n",
      "    )\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): ParallelInputter(\n",
      "        (inputters): ListWrapper(\n",
      "          (0): WordEmbedder()\n",
      "          (1): WordEmbedder()\n",
      "        )\n",
      "        (reducer): ConcatReducer()\n",
      "      )\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-11-30 03:11:09.969546: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 03:11:11.516444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-11-30 03:11:11.517098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7772 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-11-30 03:11:11.517571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 6099 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "2024-11-30 03:11:11.525000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file:\n",
      "  - KK_tokens_valid_shared\n",
      "  - KK_pos_tags_valid_shared.txt\n",
      "  eval_labels_file: KK_valid_target_tokens_shared.txt\n",
      "  source_1_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  source_2_vocabulary: RoBERTa_KK_unique_pos\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file:\n",
      "  - KK_tokens_train_shared\n",
      "  - KK_pos_tags_train_shared.txt\n",
      "  train_labels_file: KK_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: POS_KK_TR_EN\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 205000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-11-30 03:11:11.849000: I inputter.py:316] Initialized source_1 input layer:\n",
      "2024-11-30 03:11:11.849000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-11-30 03:11:11.850000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-11-30 03:11:11.853000: I inputter.py:316] Initialized source_2 input layer:\n",
      "2024-11-30 03:11:11.853000: I inputter.py:316]  - vocabulary size: 21\n",
      "2024-11-30 03:11:11.853000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-11-30 03:11:11.923000: I inputter.py:316] Initialized target input layer:\n",
      "2024-11-30 03:11:11.923000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-11-30 03:11:11.923000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-11-30 03:11:11.928000: W runner.py:269] No checkpoint to restore in POS_KK_TR_EN\n",
      "2024-11-30 03:11:11.930000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "2024-11-30 03:11:11.978000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-11-30 03:11:13.053502: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-11-30 03:11:13.177000: I main.py:325] Accumulate gradients of 7 iterations to reach effective batch size of 25000\n",
      "2024-11-30 03:11:13.298000: I mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2024-11-30 03:11:13.553000: I dataset_ops.py:2542] Training on 318032 examples\n",
      "2024-11-30 03:12:19.410278: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-11-30 03:12:20.461621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-11-30 03:12:20.761170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-11-30 03:12:29.912000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-30 03:12:29.938000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-30 03:12:31.523000: I cross_device_ops.py:897] batch_all_reduce: 261 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-11-30 03:12:35.490000: I cross_device_ops.py:897] batch_all_reduce: 261 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-11-30 03:12:41.170000: I runner.py:310] Number of model parameters: 93357261\n",
      "2024-11-30 03:12:41.174000: I runner.py:310] Number of model weights: 261 (trainable = 261, non trainable = 0)\n",
      "2024-11-30 03:12:41.207000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-30 03:12:43.289000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-1\n",
      "2024-11-30 03:12:43.904000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-30 03:12:43.927000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-30 03:12:44.557000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-30 03:12:44.579000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-30 03:12:45.186000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-30 03:12:45.207000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-30 03:12:45.803000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-30 03:13:45.451000: I runner.py:310] Step = 100 ; steps/s = 1.60, tokens/s = 42705 (42705 target) ; Learning rate = 0.000009 ; Loss = 9.742953\n",
      "2024-11-30 03:14:47.377000: I runner.py:310] Step = 200 ; steps/s = 1.62, tokens/s = 43146 (43146 target) ; Learning rate = 0.000018 ; Loss = 8.931789\n",
      "2024-11-30 03:15:49.188000: I runner.py:310] Step = 300 ; steps/s = 1.62, tokens/s = 43228 (43228 target) ; Learning rate = 0.000027 ; Loss = 8.055304\n",
      "2024-11-30 03:16:51.132000: I runner.py:310] Step = 400 ; steps/s = 1.61, tokens/s = 42262 (42262 target) ; Learning rate = 0.000035 ; Loss = 7.393646\n",
      "2024-11-30 03:17:52.901000: I runner.py:310] Step = 500 ; steps/s = 1.62, tokens/s = 43267 (43267 target) ; Learning rate = 0.000044 ; Loss = 6.990420\n",
      "2024-11-30 03:18:54.675000: I runner.py:310] Step = 600 ; steps/s = 1.62, tokens/s = 43265 (43265 target) ; Learning rate = 0.000053 ; Loss = 6.670130\n",
      "2024-11-30 03:19:56.506000: I runner.py:310] Step = 700 ; steps/s = 1.62, tokens/s = 43201 (43201 target) ; Learning rate = 0.000062 ; Loss = 6.319157\n",
      "2024-11-30 03:20:58.034000: I runner.py:310] Step = 800 ; steps/s = 1.63, tokens/s = 42585 (42585 target) ; Learning rate = 0.000071 ; Loss = 6.102314\n",
      "2024-11-30 03:21:59.838000: I runner.py:310] Step = 900 ; steps/s = 1.62, tokens/s = 43251 (43251 target) ; Learning rate = 0.000080 ; Loss = 5.929038\n",
      "2024-11-30 03:23:01.597000: I runner.py:310] Step = 1000 ; steps/s = 1.62, tokens/s = 43273 (43273 target) ; Learning rate = 0.000088 ; Loss = 5.795255\n",
      "2024-11-30 03:24:03.371000: I runner.py:310] Step = 1100 ; steps/s = 1.62, tokens/s = 43218 (43218 target) ; Learning rate = 0.000097 ; Loss = 5.597751\n",
      "2024-11-30 03:25:04.595000: I runner.py:310] Step = 1200 ; steps/s = 1.63, tokens/s = 42796 (42796 target) ; Learning rate = 0.000106 ; Loss = 5.478508\n",
      "2024-11-30 03:26:06.309000: I runner.py:310] Step = 1300 ; steps/s = 1.62, tokens/s = 43319 (43319 target) ; Learning rate = 0.000115 ; Loss = 5.414905\n",
      "2024-11-30 03:27:08.117000: I runner.py:310] Step = 1400 ; steps/s = 1.62, tokens/s = 43248 (43248 target) ; Learning rate = 0.000124 ; Loss = 5.306397\n",
      "2024-11-30 03:28:09.845000: I runner.py:310] Step = 1500 ; steps/s = 1.62, tokens/s = 43262 (43262 target) ; Learning rate = 0.000133 ; Loss = 5.151987\n",
      "2024-11-30 03:29:11.004000: I runner.py:310] Step = 1600 ; steps/s = 1.64, tokens/s = 42840 (42840 target) ; Learning rate = 0.000142 ; Loss = 5.076851\n",
      "2024-11-30 03:30:12.717000: I runner.py:310] Step = 1700 ; steps/s = 1.62, tokens/s = 43301 (43301 target) ; Learning rate = 0.000150 ; Loss = 5.032812\n",
      "2024-11-30 03:31:14.467000: I runner.py:310] Step = 1800 ; steps/s = 1.62, tokens/s = 43273 (43273 target) ; Learning rate = 0.000159 ; Loss = 4.928637\n",
      "2024-11-30 03:32:16.240000: I runner.py:310] Step = 1900 ; steps/s = 1.62, tokens/s = 43242 (43242 target) ; Learning rate = 0.000168 ; Loss = 4.781479\n",
      "2024-11-30 03:33:17.403000: I runner.py:310] Step = 2000 ; steps/s = 1.64, tokens/s = 42846 (42846 target) ; Learning rate = 0.000177 ; Loss = 4.668741\n",
      "2024-11-30 03:34:19.104000: I runner.py:310] Step = 2100 ; steps/s = 1.62, tokens/s = 43304 (43304 target) ; Learning rate = 0.000186 ; Loss = 4.518550\n",
      "2024-11-30 03:35:20.926000: I runner.py:310] Step = 2200 ; steps/s = 1.62, tokens/s = 43229 (43229 target) ; Learning rate = 0.000195 ; Loss = 4.335777\n",
      "2024-11-30 03:36:22.676000: I runner.py:310] Step = 2300 ; steps/s = 1.62, tokens/s = 43247 (43247 target) ; Learning rate = 0.000203 ; Loss = 4.173038\n",
      "2024-11-30 03:37:23.826000: I runner.py:310] Step = 2400 ; steps/s = 1.64, tokens/s = 42841 (42841 target) ; Learning rate = 0.000212 ; Loss = 4.052838\n",
      "2024-11-30 03:38:25.553000: I runner.py:310] Step = 2500 ; steps/s = 1.62, tokens/s = 43283 (43283 target) ; Learning rate = 0.000221 ; Loss = 3.835738\n",
      "2024-11-30 03:39:27.396000: I runner.py:310] Step = 2600 ; steps/s = 1.62, tokens/s = 43224 (43224 target) ; Learning rate = 0.000230 ; Loss = 3.745687\n",
      "2024-11-30 03:40:29.021000: I runner.py:310] Step = 2700 ; steps/s = 1.62, tokens/s = 43358 (43358 target) ; Learning rate = 0.000239 ; Loss = 3.688426\n",
      "2024-11-30 03:41:30.199000: I runner.py:310] Step = 2800 ; steps/s = 1.63, tokens/s = 42804 (42804 target) ; Learning rate = 0.000248 ; Loss = 3.583553\n",
      "2024-11-30 03:42:31.893000: I runner.py:310] Step = 2900 ; steps/s = 1.62, tokens/s = 43321 (43321 target) ; Learning rate = 0.000256 ; Loss = 3.519549\n",
      "2024-11-30 03:43:33.636000: I runner.py:310] Step = 3000 ; steps/s = 1.62, tokens/s = 43257 (43257 target) ; Learning rate = 0.000265 ; Loss = 3.387248\n",
      "2024-11-30 03:44:35.459000: I runner.py:310] Step = 3100 ; steps/s = 1.62, tokens/s = 43214 (43214 target) ; Learning rate = 0.000274 ; Loss = 3.315106\n",
      "2024-11-30 03:45:36.639000: I runner.py:310] Step = 3200 ; steps/s = 1.63, tokens/s = 42862 (42862 target) ; Learning rate = 0.000283 ; Loss = 3.177274\n",
      "2024-11-30 03:46:38.375000: I runner.py:310] Step = 3300 ; steps/s = 1.62, tokens/s = 43253 (43253 target) ; Learning rate = 0.000292 ; Loss = 3.187243\n",
      "2024-11-30 03:47:40.194000: I runner.py:310] Step = 3400 ; steps/s = 1.62, tokens/s = 43226 (43226 target) ; Learning rate = 0.000301 ; Loss = 3.106481\n",
      "2024-11-30 03:48:41.958000: I runner.py:310] Step = 3500 ; steps/s = 1.62, tokens/s = 43270 (43270 target) ; Learning rate = 0.000309 ; Loss = 3.048397\n",
      "2024-11-30 03:49:43.130000: I runner.py:310] Step = 3600 ; steps/s = 1.63, tokens/s = 42832 (42832 target) ; Learning rate = 0.000318 ; Loss = 2.904268\n",
      "2024-11-30 03:50:44.875000: I runner.py:310] Step = 3700 ; steps/s = 1.62, tokens/s = 43290 (43290 target) ; Learning rate = 0.000327 ; Loss = 2.867090\n",
      "2024-11-30 03:51:46.577000: I runner.py:310] Step = 3800 ; steps/s = 1.62, tokens/s = 43286 (43286 target) ; Learning rate = 0.000336 ; Loss = 2.864112\n",
      "2024-11-30 03:52:48.322000: I runner.py:310] Step = 3900 ; steps/s = 1.62, tokens/s = 43255 (43255 target) ; Learning rate = 0.000345 ; Loss = 2.826564\n",
      "2024-11-30 03:53:49.496000: I runner.py:310] Step = 4000 ; steps/s = 1.63, tokens/s = 42876 (42876 target) ; Learning rate = 0.000354 ; Loss = 2.762013\n",
      "2024-11-30 03:54:51.267000: I runner.py:310] Step = 4100 ; steps/s = 1.62, tokens/s = 43229 (43229 target) ; Learning rate = 0.000362 ; Loss = 2.734759\n",
      "2024-11-30 03:55:53.011000: I runner.py:310] Step = 4200 ; steps/s = 1.62, tokens/s = 43260 (43260 target) ; Learning rate = 0.000371 ; Loss = 2.857067\n",
      "2024-11-30 03:56:54.406000: I runner.py:310] Step = 4300 ; steps/s = 1.63, tokens/s = 42964 (42964 target) ; Learning rate = 0.000380 ; Loss = 2.884974\n",
      "2024-11-30 03:57:55.962000: I runner.py:310] Step = 4400 ; steps/s = 1.62, tokens/s = 43133 (43133 target) ; Learning rate = 0.000389 ; Loss = 2.627304\n",
      "2024-11-30 03:58:57.705000: I runner.py:310] Step = 4500 ; steps/s = 1.62, tokens/s = 43263 (43263 target) ; Learning rate = 0.000398 ; Loss = 2.595926\n",
      "2024-11-30 03:59:59.493000: I runner.py:310] Step = 4600 ; steps/s = 1.62, tokens/s = 43252 (43252 target) ; Learning rate = 0.000407 ; Loss = 2.615073\n",
      "2024-11-30 04:01:00.732000: I runner.py:310] Step = 4700 ; steps/s = 1.63, tokens/s = 42785 (42785 target) ; Learning rate = 0.000416 ; Loss = 2.608114\n",
      "2024-11-30 04:02:02.436000: I runner.py:310] Step = 4800 ; steps/s = 1.62, tokens/s = 43303 (43303 target) ; Learning rate = 0.000424 ; Loss = 2.555486\n",
      "2024-11-30 04:03:04.193000: I runner.py:310] Step = 4900 ; steps/s = 1.62, tokens/s = 43241 (43241 target) ; Learning rate = 0.000433 ; Loss = 2.543610\n",
      "2024-11-30 04:04:05.953000: I runner.py:310] Step = 5000 ; steps/s = 1.62, tokens/s = 43259 (43259 target) ; Learning rate = 0.000442 ; Loss = 2.507700\n",
      "2024-11-30 04:04:05.955000: I training.py:192] Running evaluation for step 5000\n",
      "2024-11-30 04:13:54.626000: I training.py:192] Evaluation result for step 5000: loss = 1.324112 ; perplexity = 3.758847\n",
      "2024-11-30 04:14:55.841000: I runner.py:310] Step = 5100 ; steps/s = 1.63, tokens/s = 42843 (42843 target) ; Learning rate = 0.000451 ; Loss = 2.470509\n",
      "2024-11-30 04:15:57.689000: I runner.py:310] Step = 5200 ; steps/s = 1.62, tokens/s = 43188 (43188 target) ; Learning rate = 0.000460 ; Loss = 2.480001\n",
      "2024-11-30 04:16:59.614000: I runner.py:310] Step = 5300 ; steps/s = 1.62, tokens/s = 43153 (43153 target) ; Learning rate = 0.000469 ; Loss = 2.427785\n",
      "2024-11-30 04:18:01.495000: I runner.py:310] Step = 5400 ; steps/s = 1.62, tokens/s = 43184 (43184 target) ; Learning rate = 0.000477 ; Loss = 2.445115\n",
      "2024-11-30 04:19:02.805000: I runner.py:310] Step = 5500 ; steps/s = 1.63, tokens/s = 42745 (42745 target) ; Learning rate = 0.000486 ; Loss = 2.359244\n",
      "2024-11-30 04:20:04.686000: I runner.py:310] Step = 5600 ; steps/s = 1.62, tokens/s = 43164 (43164 target) ; Learning rate = 0.000495 ; Loss = 2.381198\n",
      "2024-11-30 04:21:06.615000: I runner.py:310] Step = 5700 ; steps/s = 1.61, tokens/s = 43159 (43159 target) ; Learning rate = 0.000504 ; Loss = 2.389504\n",
      "2024-11-30 04:22:08.432000: I runner.py:310] Step = 5800 ; steps/s = 1.62, tokens/s = 43212 (43212 target) ; Learning rate = 0.000513 ; Loss = 2.355648\n",
      "2024-11-30 04:23:09.689000: I runner.py:310] Step = 5900 ; steps/s = 1.63, tokens/s = 42776 (42776 target) ; Learning rate = 0.000522 ; Loss = 2.301012\n",
      "2024-11-30 04:24:11.470000: I runner.py:310] Step = 6000 ; steps/s = 1.62, tokens/s = 43217 (43217 target) ; Learning rate = 0.000530 ; Loss = 2.310751\n",
      "2024-11-30 04:25:13.299000: I runner.py:310] Step = 6100 ; steps/s = 1.62, tokens/s = 43216 (43216 target) ; Learning rate = 0.000539 ; Loss = 2.285158\n",
      "2024-11-30 04:26:15.156000: I runner.py:310] Step = 6200 ; steps/s = 1.62, tokens/s = 43195 (43195 target) ; Learning rate = 0.000548 ; Loss = 2.317568\n",
      "2024-11-30 04:27:16.368000: I runner.py:310] Step = 6300 ; steps/s = 1.63, tokens/s = 42841 (42841 target) ; Learning rate = 0.000557 ; Loss = 2.284407\n",
      "2024-11-30 04:28:18.108000: I runner.py:310] Step = 6400 ; steps/s = 1.62, tokens/s = 43288 (43288 target) ; Learning rate = 0.000566 ; Loss = 2.205891\n",
      "2024-11-30 04:29:19.932000: I runner.py:310] Step = 6500 ; steps/s = 1.62, tokens/s = 43213 (43213 target) ; Learning rate = 0.000575 ; Loss = 2.270543\n",
      "2024-11-30 04:30:21.677000: I runner.py:310] Step = 6600 ; steps/s = 1.62, tokens/s = 43256 (43256 target) ; Learning rate = 0.000583 ; Loss = 2.251440\n",
      "2024-11-30 04:31:22.925000: I runner.py:310] Step = 6700 ; steps/s = 1.63, tokens/s = 42798 (42798 target) ; Learning rate = 0.000592 ; Loss = 2.240289\n",
      "2024-11-30 04:32:24.758000: I runner.py:310] Step = 6800 ; steps/s = 1.62, tokens/s = 43233 (43233 target) ; Learning rate = 0.000601 ; Loss = 2.210872\n",
      "2024-11-30 04:33:26.538000: I runner.py:310] Step = 6900 ; steps/s = 1.62, tokens/s = 43228 (43228 target) ; Learning rate = 0.000610 ; Loss = 2.204405\n",
      "2024-11-30 04:34:28.261000: I runner.py:310] Step = 7000 ; steps/s = 1.62, tokens/s = 43275 (43275 target) ; Learning rate = 0.000619 ; Loss = 2.176432\n",
      "2024-11-30 04:35:29.459000: I runner.py:310] Step = 7100 ; steps/s = 1.63, tokens/s = 42822 (42822 target) ; Learning rate = 0.000628 ; Loss = 2.159710\n",
      "2024-11-30 04:36:31.140000: I runner.py:310] Step = 7200 ; steps/s = 1.62, tokens/s = 43312 (43312 target) ; Learning rate = 0.000636 ; Loss = 2.200348\n",
      "2024-11-30 04:37:32.922000: I runner.py:310] Step = 7300 ; steps/s = 1.62, tokens/s = 43234 (43234 target) ; Learning rate = 0.000645 ; Loss = 2.182997\n",
      "2024-11-30 04:38:34.679000: I runner.py:310] Step = 7400 ; steps/s = 1.62, tokens/s = 43265 (43265 target) ; Learning rate = 0.000654 ; Loss = 2.197660\n",
      "2024-11-30 04:39:35.927000: I runner.py:310] Step = 7500 ; steps/s = 1.63, tokens/s = 42790 (42790 target) ; Learning rate = 0.000663 ; Loss = 2.151930\n",
      "2024-11-30 04:40:37.718000: I runner.py:310] Step = 7600 ; steps/s = 1.62, tokens/s = 43245 (43245 target) ; Learning rate = 0.000672 ; Loss = 2.098062\n",
      "2024-11-30 04:41:39.539000: I runner.py:310] Step = 7700 ; steps/s = 1.62, tokens/s = 43203 (43203 target) ; Learning rate = 0.000681 ; Loss = 2.113285\n",
      "2024-11-30 04:42:41.316000: I runner.py:310] Step = 7800 ; steps/s = 1.62, tokens/s = 43246 (43246 target) ; Learning rate = 0.000690 ; Loss = 2.136870\n",
      "2024-11-30 04:43:42.490000: I runner.py:310] Step = 7900 ; steps/s = 1.63, tokens/s = 42848 (42848 target) ; Learning rate = 0.000698 ; Loss = 2.098976\n",
      "2024-11-30 04:44:44.252000: I runner.py:310] Step = 8000 ; steps/s = 1.62, tokens/s = 43244 (43244 target) ; Learning rate = 0.000707 ; Loss = 2.109571\n",
      "2024-11-30 04:45:46.037000: I runner.py:310] Step = 8100 ; steps/s = 1.62, tokens/s = 43227 (43227 target) ; Learning rate = 0.000716 ; Loss = 2.120326\n",
      "2024-11-30 04:46:47.846000: I runner.py:310] Step = 8200 ; steps/s = 1.62, tokens/s = 43256 (43256 target) ; Learning rate = 0.000725 ; Loss = 2.073437\n",
      "2024-11-30 04:47:48.997000: I runner.py:310] Step = 8300 ; steps/s = 1.64, tokens/s = 42859 (42859 target) ; Learning rate = 0.000734 ; Loss = 2.079105\n",
      "2024-11-30 04:48:50.749000: I runner.py:310] Step = 8400 ; steps/s = 1.62, tokens/s = 43250 (43250 target) ; Learning rate = 0.000743 ; Loss = 2.052955\n",
      "2024-11-30 04:49:52.562000: I runner.py:310] Step = 8500 ; steps/s = 1.62, tokens/s = 43237 (43237 target) ; Learning rate = 0.000751 ; Loss = 2.041141\n",
      "2024-11-30 04:50:54.106000: I runner.py:310] Step = 8600 ; steps/s = 1.63, tokens/s = 43087 (43087 target) ; Learning rate = 0.000760 ; Loss = 2.089856\n",
      "2024-11-30 04:51:55.491000: I runner.py:310] Step = 8700 ; steps/s = 1.63, tokens/s = 43034 (43034 target) ; Learning rate = 0.000769 ; Loss = 2.076320\n",
      "2024-11-30 04:52:57.217000: I runner.py:310] Step = 8800 ; steps/s = 1.62, tokens/s = 43287 (43287 target) ; Learning rate = 0.000778 ; Loss = 2.041312\n",
      "2024-11-30 04:53:58.960000: I runner.py:310] Step = 8900 ; steps/s = 1.62, tokens/s = 43262 (43262 target) ; Learning rate = 0.000787 ; Loss = 2.044216\n",
      "2024-11-30 04:55:00.206000: I runner.py:310] Step = 9000 ; steps/s = 1.63, tokens/s = 42767 (42767 target) ; Learning rate = 0.000796 ; Loss = 2.080229\n",
      "2024-11-30 04:56:01.969000: I runner.py:310] Step = 9100 ; steps/s = 1.62, tokens/s = 43281 (43281 target) ; Learning rate = 0.000804 ; Loss = 2.000468\n",
      "2024-11-30 04:57:03.710000: I runner.py:310] Step = 9200 ; steps/s = 1.62, tokens/s = 43280 (43280 target) ; Learning rate = 0.000813 ; Loss = 2.022911\n",
      "2024-11-30 04:58:05.526000: I runner.py:310] Step = 9300 ; steps/s = 1.62, tokens/s = 43232 (43232 target) ; Learning rate = 0.000822 ; Loss = 2.015018\n",
      "2024-11-30 04:59:06.693000: I runner.py:310] Step = 9400 ; steps/s = 1.64, tokens/s = 42804 (42804 target) ; Learning rate = 0.000831 ; Loss = 2.025929\n",
      "2024-11-30 05:00:08.467000: I runner.py:310] Step = 9500 ; steps/s = 1.62, tokens/s = 43234 (43234 target) ; Learning rate = 0.000840 ; Loss = 1.981496\n",
      "2024-11-30 05:01:10.232000: I runner.py:310] Step = 9600 ; steps/s = 1.62, tokens/s = 43254 (43254 target) ; Learning rate = 0.000849 ; Loss = 2.003244\n",
      "2024-11-30 05:02:11.985000: I runner.py:310] Step = 9700 ; steps/s = 1.62, tokens/s = 43286 (43286 target) ; Learning rate = 0.000857 ; Loss = 2.033443\n",
      "2024-11-30 05:03:13.178000: I runner.py:310] Step = 9800 ; steps/s = 1.63, tokens/s = 42834 (42834 target) ; Learning rate = 0.000866 ; Loss = 1.983529\n",
      "2024-11-30 05:04:14.887000: I runner.py:310] Step = 9900 ; steps/s = 1.62, tokens/s = 43305 (43305 target) ; Learning rate = 0.000875 ; Loss = 1.967682\n",
      "2024-11-30 05:05:16.690000: I runner.py:310] Step = 10000 ; steps/s = 1.62, tokens/s = 43199 (43199 target) ; Learning rate = 0.000884 ; Loss = 1.992640\n",
      "2024-11-30 05:05:18.471000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-10000\n",
      "2024-11-30 05:05:18.471000: I training.py:192] Running evaluation for step 10000\n",
      "2024-11-30 05:12:12.392000: I training.py:192] Evaluation result for step 10000: loss = 1.087724 ; perplexity = 2.967513\n",
      "2024-11-30 05:13:14.134000: I runner.py:310] Step = 10100 ; steps/s = 1.62, tokens/s = 43274 (43274 target) ; Learning rate = 0.000879 ; Loss = 1.993273\n",
      "2024-11-30 05:14:15.392000: I runner.py:310] Step = 10200 ; steps/s = 1.63, tokens/s = 42814 (42814 target) ; Learning rate = 0.000875 ; Loss = 1.932630\n",
      "2024-11-30 05:15:17.189000: I runner.py:310] Step = 10300 ; steps/s = 1.62, tokens/s = 43227 (43227 target) ; Learning rate = 0.000871 ; Loss = 1.957732\n",
      "2024-11-30 05:16:19.023000: I runner.py:310] Step = 10400 ; steps/s = 1.62, tokens/s = 43193 (43193 target) ; Learning rate = 0.000867 ; Loss = 1.978585\n",
      "2024-11-30 05:17:20.788000: I runner.py:310] Step = 10500 ; steps/s = 1.62, tokens/s = 43270 (43270 target) ; Learning rate = 0.000863 ; Loss = 1.971072\n",
      "2024-11-30 05:18:22.022000: I runner.py:310] Step = 10600 ; steps/s = 1.63, tokens/s = 42800 (42800 target) ; Learning rate = 0.000858 ; Loss = 1.945350\n",
      "2024-11-30 05:19:23.813000: I runner.py:310] Step = 10700 ; steps/s = 1.62, tokens/s = 43252 (43252 target) ; Learning rate = 0.000854 ; Loss = 1.941528\n",
      "2024-11-30 05:20:25.623000: I runner.py:310] Step = 10800 ; steps/s = 1.62, tokens/s = 43199 (43199 target) ; Learning rate = 0.000850 ; Loss = 1.936577\n",
      "2024-11-30 05:21:27.452000: I runner.py:310] Step = 10900 ; steps/s = 1.62, tokens/s = 43226 (43226 target) ; Learning rate = 0.000847 ; Loss = 1.942276\n",
      "2024-11-30 05:22:28.657000: I runner.py:310] Step = 11000 ; steps/s = 1.63, tokens/s = 42837 (42837 target) ; Learning rate = 0.000843 ; Loss = 1.888860\n",
      "2024-11-30 05:23:30.387000: I runner.py:310] Step = 11100 ; steps/s = 1.62, tokens/s = 43262 (43262 target) ; Learning rate = 0.000839 ; Loss = 1.932028\n",
      "2024-11-30 05:24:32.168000: I runner.py:310] Step = 11200 ; steps/s = 1.62, tokens/s = 43243 (43243 target) ; Learning rate = 0.000835 ; Loss = 1.921611\n",
      "2024-11-30 05:25:34.014000: I runner.py:310] Step = 11300 ; steps/s = 1.62, tokens/s = 43202 (43202 target) ; Learning rate = 0.000831 ; Loss = 1.941936\n",
      "2024-11-30 05:26:35.195000: I runner.py:310] Step = 11400 ; steps/s = 1.63, tokens/s = 42822 (42822 target) ; Learning rate = 0.000828 ; Loss = 1.909420\n",
      "2024-11-30 05:27:36.913000: I runner.py:310] Step = 11500 ; steps/s = 1.62, tokens/s = 43298 (43298 target) ; Learning rate = 0.000824 ; Loss = 1.885540\n",
      "2024-11-30 05:28:38.717000: I runner.py:310] Step = 11600 ; steps/s = 1.62, tokens/s = 43225 (43225 target) ; Learning rate = 0.000821 ; Loss = 1.909566\n",
      "2024-11-30 05:29:40.505000: I runner.py:310] Step = 11700 ; steps/s = 1.62, tokens/s = 43243 (43243 target) ; Learning rate = 0.000817 ; Loss = 1.875229\n",
      "2024-11-30 05:30:41.715000: I runner.py:310] Step = 11800 ; steps/s = 1.63, tokens/s = 42805 (42805 target) ; Learning rate = 0.000814 ; Loss = 1.846916\n",
      "2024-11-30 05:31:43.493000: I runner.py:310] Step = 11900 ; steps/s = 1.62, tokens/s = 43265 (43265 target) ; Learning rate = 0.000810 ; Loss = 1.890372\n",
      "2024-11-30 05:32:45.273000: I runner.py:310] Step = 12000 ; steps/s = 1.62, tokens/s = 43242 (43242 target) ; Learning rate = 0.000807 ; Loss = 1.888104\n",
      "2024-11-30 05:33:47.097000: I runner.py:310] Step = 12100 ; steps/s = 1.62, tokens/s = 43212 (43212 target) ; Learning rate = 0.000803 ; Loss = 1.893270\n",
      "2024-11-30 05:34:48.332000: I runner.py:310] Step = 12200 ; steps/s = 1.63, tokens/s = 42779 (42779 target) ; Learning rate = 0.000800 ; Loss = 1.893166\n",
      "2024-11-30 05:35:50.181000: I runner.py:310] Step = 12300 ; steps/s = 1.62, tokens/s = 43204 (43204 target) ; Learning rate = 0.000797 ; Loss = 1.849943\n",
      "2024-11-30 05:36:52.014000: I runner.py:310] Step = 12400 ; steps/s = 1.62, tokens/s = 43183 (43183 target) ; Learning rate = 0.000794 ; Loss = 1.845755\n",
      "2024-11-30 05:37:53.794000: I runner.py:310] Step = 12500 ; steps/s = 1.62, tokens/s = 43267 (43267 target) ; Learning rate = 0.000791 ; Loss = 1.866231\n",
      "2024-11-30 05:38:55.040000: I runner.py:310] Step = 12600 ; steps/s = 1.63, tokens/s = 42807 (42807 target) ; Learning rate = 0.000787 ; Loss = 1.848706\n",
      "2024-11-30 05:39:56.829000: I runner.py:310] Step = 12700 ; steps/s = 1.62, tokens/s = 43232 (43232 target) ; Learning rate = 0.000784 ; Loss = 1.835315\n",
      "2024-11-30 05:40:58.616000: I runner.py:310] Step = 12800 ; steps/s = 1.62, tokens/s = 43250 (43250 target) ; Learning rate = 0.000781 ; Loss = 1.853367\n",
      "2024-11-30 05:42:00.375000: I runner.py:310] Step = 12900 ; steps/s = 1.62, tokens/s = 43123 (43123 target) ; Learning rate = 0.000778 ; Loss = 1.875008\n",
      "2024-11-30 05:43:01.642000: I runner.py:310] Step = 13000 ; steps/s = 1.63, tokens/s = 42913 (42913 target) ; Learning rate = 0.000775 ; Loss = 1.838685\n",
      "2024-11-30 05:44:03.371000: I runner.py:310] Step = 13100 ; steps/s = 1.62, tokens/s = 43261 (43261 target) ; Learning rate = 0.000772 ; Loss = 1.819024\n",
      "2024-11-30 05:45:05.176000: I runner.py:310] Step = 13200 ; steps/s = 1.62, tokens/s = 43209 (43209 target) ; Learning rate = 0.000769 ; Loss = 1.813499\n",
      "2024-11-30 05:46:06.383000: I runner.py:310] Step = 13300 ; steps/s = 1.63, tokens/s = 42831 (42831 target) ; Learning rate = 0.000766 ; Loss = 1.814492\n",
      "2024-11-30 05:47:08.212000: I runner.py:310] Step = 13400 ; steps/s = 1.62, tokens/s = 43217 (43217 target) ; Learning rate = 0.000764 ; Loss = 1.795525\n",
      "2024-11-30 05:48:09.998000: I runner.py:310] Step = 13500 ; steps/s = 1.62, tokens/s = 43240 (43240 target) ; Learning rate = 0.000761 ; Loss = 1.816392\n",
      "2024-11-30 05:49:11.824000: I runner.py:310] Step = 13600 ; steps/s = 1.62, tokens/s = 43238 (43238 target) ; Learning rate = 0.000758 ; Loss = 1.818668\n",
      "2024-11-30 05:50:13.093000: I runner.py:310] Step = 13700 ; steps/s = 1.63, tokens/s = 42751 (42751 target) ; Learning rate = 0.000755 ; Loss = 1.814229\n",
      "2024-11-30 05:51:14.867000: I runner.py:310] Step = 13800 ; steps/s = 1.62, tokens/s = 43255 (43255 target) ; Learning rate = 0.000752 ; Loss = 1.814555\n",
      "2024-11-30 05:52:16.610000: I runner.py:310] Step = 13900 ; steps/s = 1.62, tokens/s = 43284 (43284 target) ; Learning rate = 0.000750 ; Loss = 1.802833\n",
      "2024-11-30 05:53:18.371000: I runner.py:310] Step = 14000 ; steps/s = 1.62, tokens/s = 43241 (43241 target) ; Learning rate = 0.000747 ; Loss = 1.810177\n",
      "2024-11-30 05:54:19.575000: I runner.py:310] Step = 14100 ; steps/s = 1.63, tokens/s = 42831 (42831 target) ; Learning rate = 0.000744 ; Loss = 1.769480\n",
      "2024-11-30 05:55:21.290000: I runner.py:310] Step = 14200 ; steps/s = 1.62, tokens/s = 43295 (43295 target) ; Learning rate = 0.000742 ; Loss = 1.779400\n",
      "2024-11-30 05:56:23.132000: I runner.py:310] Step = 14300 ; steps/s = 1.62, tokens/s = 43191 (43191 target) ; Learning rate = 0.000739 ; Loss = 1.802356\n",
      "2024-11-30 05:57:24.949000: I runner.py:310] Step = 14400 ; steps/s = 1.62, tokens/s = 43214 (43214 target) ; Learning rate = 0.000737 ; Loss = 1.816454\n",
      "2024-11-30 05:58:26.121000: I runner.py:310] Step = 14500 ; steps/s = 1.63, tokens/s = 42859 (42859 target) ; Learning rate = 0.000734 ; Loss = 1.772528\n",
      "2024-11-30 05:59:27.913000: I runner.py:310] Step = 14600 ; steps/s = 1.62, tokens/s = 43233 (43233 target) ; Learning rate = 0.000731 ; Loss = 1.776260\n",
      "2024-11-30 06:00:29.672000: I runner.py:310] Step = 14700 ; steps/s = 1.62, tokens/s = 43241 (43241 target) ; Learning rate = 0.000729 ; Loss = 1.795943\n",
      "2024-11-30 06:01:31.520000: I runner.py:310] Step = 14800 ; steps/s = 1.62, tokens/s = 43202 (43202 target) ; Learning rate = 0.000727 ; Loss = 1.797030\n",
      "2024-11-30 06:02:32.742000: I runner.py:310] Step = 14900 ; steps/s = 1.63, tokens/s = 42837 (42837 target) ; Learning rate = 0.000724 ; Loss = 1.778328\n",
      "2024-11-30 06:03:34.576000: I runner.py:310] Step = 15000 ; steps/s = 1.62, tokens/s = 43209 (43209 target) ; Learning rate = 0.000722 ; Loss = 1.771193\n",
      "2024-11-30 06:03:34.577000: I training.py:192] Running evaluation for step 15000\n",
      "2024-11-30 06:09:42.885000: I training.py:192] Evaluation result for step 15000: loss = 1.067329 ; perplexity = 2.907603\n",
      "2024-11-30 06:10:44.522000: I runner.py:310] Step = 15100 ; steps/s = 1.62, tokens/s = 43344 (43344 target) ; Learning rate = 0.000719 ; Loss = 1.779966\n",
      "2024-11-30 06:11:46.227000: I runner.py:310] Step = 15200 ; steps/s = 1.62, tokens/s = 43298 (43298 target) ; Learning rate = 0.000717 ; Loss = 1.763985\n",
      "2024-11-30 06:12:47.447000: I runner.py:310] Step = 15300 ; steps/s = 1.63, tokens/s = 42792 (42792 target) ; Learning rate = 0.000715 ; Loss = 1.765885\n",
      "2024-11-30 06:13:49.272000: I runner.py:310] Step = 15400 ; steps/s = 1.62, tokens/s = 43232 (43232 target) ; Learning rate = 0.000712 ; Loss = 1.751403\n",
      "2024-11-30 06:14:51.063000: I runner.py:310] Step = 15500 ; steps/s = 1.62, tokens/s = 43236 (43236 target) ; Learning rate = 0.000710 ; Loss = 1.770345\n",
      "2024-11-30 06:15:52.844000: I runner.py:310] Step = 15600 ; steps/s = 1.62, tokens/s = 43237 (43237 target) ; Learning rate = 0.000708 ; Loss = 1.761622\n",
      "2024-11-30 06:16:53.998000: I runner.py:310] Step = 15700 ; steps/s = 1.64, tokens/s = 42859 (42859 target) ; Learning rate = 0.000705 ; Loss = 1.729923\n",
      "2024-11-30 06:17:55.742000: I runner.py:310] Step = 15800 ; steps/s = 1.62, tokens/s = 43262 (43262 target) ; Learning rate = 0.000703 ; Loss = 1.750324\n",
      "2024-11-30 06:18:57.535000: I runner.py:310] Step = 15900 ; steps/s = 1.62, tokens/s = 43256 (43256 target) ; Learning rate = 0.000701 ; Loss = 1.776208\n",
      "2024-11-30 06:19:59.266000: I runner.py:310] Step = 16000 ; steps/s = 1.62, tokens/s = 43278 (43278 target) ; Learning rate = 0.000699 ; Loss = 1.767318\n",
      "2024-11-30 06:21:00.454000: I runner.py:310] Step = 16100 ; steps/s = 1.63, tokens/s = 42807 (42807 target) ; Learning rate = 0.000697 ; Loss = 1.723896\n",
      "2024-11-30 06:22:02.206000: I runner.py:310] Step = 16200 ; steps/s = 1.62, tokens/s = 43291 (43291 target) ; Learning rate = 0.000694 ; Loss = 1.762424\n",
      "2024-11-30 06:23:03.988000: I runner.py:310] Step = 16300 ; steps/s = 1.62, tokens/s = 43217 (43217 target) ; Learning rate = 0.000692 ; Loss = 1.763564\n",
      "2024-11-30 06:24:05.739000: I runner.py:310] Step = 16400 ; steps/s = 1.62, tokens/s = 43261 (43261 target) ; Learning rate = 0.000690 ; Loss = 1.769951\n",
      "2024-11-30 06:25:06.845000: I runner.py:310] Step = 16500 ; steps/s = 1.64, tokens/s = 42900 (42900 target) ; Learning rate = 0.000688 ; Loss = 1.734237\n",
      "2024-11-30 06:26:08.600000: I runner.py:310] Step = 16600 ; steps/s = 1.62, tokens/s = 43279 (43279 target) ; Learning rate = 0.000686 ; Loss = 1.735695\n",
      "2024-11-30 06:27:10.342000: I runner.py:310] Step = 16700 ; steps/s = 1.62, tokens/s = 43268 (43268 target) ; Learning rate = 0.000684 ; Loss = 1.739801\n",
      "2024-11-30 06:28:12.117000: I runner.py:310] Step = 16800 ; steps/s = 1.62, tokens/s = 43236 (43236 target) ; Learning rate = 0.000682 ; Loss = 1.748363\n",
      "2024-11-30 06:29:13.289000: I runner.py:310] Step = 16900 ; steps/s = 1.63, tokens/s = 42832 (42832 target) ; Learning rate = 0.000680 ; Loss = 1.728429\n",
      "2024-11-30 06:30:15.073000: I runner.py:310] Step = 17000 ; steps/s = 1.62, tokens/s = 43255 (43255 target) ; Learning rate = 0.000678 ; Loss = 1.725401\n",
      "2024-11-30 06:31:16.845000: I runner.py:310] Step = 17100 ; steps/s = 1.62, tokens/s = 43233 (43233 target) ; Learning rate = 0.000676 ; Loss = 1.740667\n",
      "2024-11-30 06:32:18.617000: I runner.py:310] Step = 17200 ; steps/s = 1.62, tokens/s = 43258 (43258 target) ; Learning rate = 0.000674 ; Loss = 1.721305\n",
      "2024-11-30 06:33:19.727000: I runner.py:310] Step = 17300 ; steps/s = 1.64, tokens/s = 42909 (42909 target) ; Learning rate = 0.000672 ; Loss = 1.703181\n",
      "2024-11-30 06:34:21.425000: I runner.py:310] Step = 17400 ; steps/s = 1.62, tokens/s = 43309 (43309 target) ; Learning rate = 0.000670 ; Loss = 1.723848\n",
      "2024-11-30 06:35:23.149000: I runner.py:310] Step = 17500 ; steps/s = 1.62, tokens/s = 43251 (43251 target) ; Learning rate = 0.000668 ; Loss = 1.753131\n",
      "2024-11-30 06:36:24.317000: I runner.py:310] Step = 17600 ; steps/s = 1.63, tokens/s = 42836 (42836 target) ; Learning rate = 0.000666 ; Loss = 1.708966\n",
      "2024-11-30 06:37:26.037000: I runner.py:310] Step = 17700 ; steps/s = 1.62, tokens/s = 43277 (43277 target) ; Learning rate = 0.000664 ; Loss = 1.737445\n",
      "2024-11-30 06:38:27.720000: I runner.py:310] Step = 17800 ; steps/s = 1.62, tokens/s = 43345 (43345 target) ; Learning rate = 0.000662 ; Loss = 1.721113\n",
      "2024-11-30 06:39:29.534000: I runner.py:310] Step = 17900 ; steps/s = 1.62, tokens/s = 43207 (43207 target) ; Learning rate = 0.000661 ; Loss = 1.743029\n",
      "2024-11-30 06:40:30.674000: I runner.py:310] Step = 18000 ; steps/s = 1.64, tokens/s = 42861 (42861 target) ; Learning rate = 0.000659 ; Loss = 1.714735\n",
      "2024-11-30 06:41:32.443000: I runner.py:310] Step = 18100 ; steps/s = 1.62, tokens/s = 43259 (43259 target) ; Learning rate = 0.000657 ; Loss = 1.711949\n",
      "2024-11-30 06:42:34.148000: I runner.py:310] Step = 18200 ; steps/s = 1.62, tokens/s = 43293 (43293 target) ; Learning rate = 0.000655 ; Loss = 1.710375\n",
      "2024-11-30 06:43:35.831000: I runner.py:310] Step = 18300 ; steps/s = 1.62, tokens/s = 43316 (43316 target) ; Learning rate = 0.000653 ; Loss = 1.719354\n",
      "2024-11-30 06:44:37.045000: I runner.py:310] Step = 18400 ; steps/s = 1.63, tokens/s = 42810 (42810 target) ; Learning rate = 0.000652 ; Loss = 1.715634\n",
      "2024-11-30 06:45:38.875000: I runner.py:310] Step = 18500 ; steps/s = 1.62, tokens/s = 43200 (43200 target) ; Learning rate = 0.000650 ; Loss = 1.697863\n",
      "2024-11-30 06:46:40.620000: I runner.py:310] Step = 18600 ; steps/s = 1.62, tokens/s = 43271 (43271 target) ; Learning rate = 0.000648 ; Loss = 1.691545\n",
      "2024-11-30 06:47:42.370000: I runner.py:310] Step = 18700 ; steps/s = 1.62, tokens/s = 43270 (43270 target) ; Learning rate = 0.000646 ; Loss = 1.706172\n",
      "2024-11-30 06:48:43.504000: I runner.py:310] Step = 18800 ; steps/s = 1.64, tokens/s = 42891 (42891 target) ; Learning rate = 0.000645 ; Loss = 1.678173\n",
      "2024-11-30 06:49:45.294000: I runner.py:310] Step = 18900 ; steps/s = 1.62, tokens/s = 43230 (43230 target) ; Learning rate = 0.000643 ; Loss = 1.679864\n",
      "2024-11-30 06:50:47.055000: I runner.py:310] Step = 19000 ; steps/s = 1.62, tokens/s = 43260 (43260 target) ; Learning rate = 0.000641 ; Loss = 1.687299\n",
      "2024-11-30 06:51:48.826000: I runner.py:310] Step = 19100 ; steps/s = 1.62, tokens/s = 43234 (43234 target) ; Learning rate = 0.000640 ; Loss = 1.731269\n",
      "2024-11-30 06:52:50.020000: I runner.py:310] Step = 19200 ; steps/s = 1.63, tokens/s = 42819 (42819 target) ; Learning rate = 0.000638 ; Loss = 1.695445\n",
      "2024-11-30 06:53:51.737000: I runner.py:310] Step = 19300 ; steps/s = 1.62, tokens/s = 43311 (43311 target) ; Learning rate = 0.000636 ; Loss = 1.685471\n",
      "2024-11-30 06:54:53.447000: I runner.py:310] Step = 19400 ; steps/s = 1.62, tokens/s = 43283 (43283 target) ; Learning rate = 0.000635 ; Loss = 1.688774\n",
      "2024-11-30 06:55:55.232000: I runner.py:310] Step = 19500 ; steps/s = 1.62, tokens/s = 43245 (43245 target) ; Learning rate = 0.000633 ; Loss = 1.709588\n",
      "2024-11-30 06:56:56.420000: I runner.py:310] Step = 19600 ; steps/s = 1.63, tokens/s = 42851 (42851 target) ; Learning rate = 0.000631 ; Loss = 1.672731\n",
      "2024-11-30 06:57:58.178000: I runner.py:310] Step = 19700 ; steps/s = 1.62, tokens/s = 43236 (43236 target) ; Learning rate = 0.000630 ; Loss = 1.686133\n",
      "2024-11-30 06:58:59.874000: I runner.py:310] Step = 19800 ; steps/s = 1.62, tokens/s = 43305 (43305 target) ; Learning rate = 0.000628 ; Loss = 1.691126\n",
      "2024-11-30 07:00:01.641000: I runner.py:310] Step = 19900 ; steps/s = 1.62, tokens/s = 43253 (43253 target) ; Learning rate = 0.000627 ; Loss = 1.700734\n",
      "2024-11-30 07:01:02.802000: I runner.py:310] Step = 20000 ; steps/s = 1.64, tokens/s = 42838 (42838 target) ; Learning rate = 0.000625 ; Loss = 1.662814\n",
      "2024-11-30 07:01:04.594000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-20000\n",
      "2024-11-30 07:01:04.595000: I training.py:192] Running evaluation for step 20000\n",
      "2024-11-30 07:06:28.929000: I training.py:192] Evaluation result for step 20000: loss = 1.087478 ; perplexity = 2.966782\n",
      "2024-11-30 07:07:30.562000: I runner.py:310] Step = 20100 ; steps/s = 1.62, tokens/s = 43349 (43349 target) ; Learning rate = 0.000623 ; Loss = 1.683520\n",
      "2024-11-30 07:08:32.328000: I runner.py:310] Step = 20200 ; steps/s = 1.62, tokens/s = 43278 (43278 target) ; Learning rate = 0.000622 ; Loss = 1.702369\n",
      "2024-11-30 07:09:34.142000: I runner.py:310] Step = 20300 ; steps/s = 1.62, tokens/s = 43225 (43225 target) ; Learning rate = 0.000620 ; Loss = 1.696363\n",
      "2024-11-30 07:10:35.380000: I runner.py:310] Step = 20400 ; steps/s = 1.63, tokens/s = 42807 (42807 target) ; Learning rate = 0.000619 ; Loss = 1.686672\n",
      "2024-11-30 07:11:37.191000: I runner.py:310] Step = 20500 ; steps/s = 1.62, tokens/s = 43235 (43235 target) ; Learning rate = 0.000617 ; Loss = 1.663158\n",
      "2024-11-30 07:12:38.937000: I runner.py:310] Step = 20600 ; steps/s = 1.62, tokens/s = 43251 (43251 target) ; Learning rate = 0.000616 ; Loss = 1.687959\n",
      "2024-11-30 07:13:40.732000: I runner.py:310] Step = 20700 ; steps/s = 1.62, tokens/s = 43233 (43233 target) ; Learning rate = 0.000614 ; Loss = 1.685992\n",
      "2024-11-30 07:14:41.890000: I runner.py:310] Step = 20800 ; steps/s = 1.64, tokens/s = 42840 (42840 target) ; Learning rate = 0.000613 ; Loss = 1.681915\n",
      "2024-11-30 07:15:43.656000: I runner.py:310] Step = 20900 ; steps/s = 1.62, tokens/s = 43282 (43282 target) ; Learning rate = 0.000611 ; Loss = 1.655206\n",
      "2024-11-30 07:16:45.436000: I runner.py:310] Step = 21000 ; steps/s = 1.62, tokens/s = 43243 (43243 target) ; Learning rate = 0.000610 ; Loss = 1.666086\n",
      "2024-11-30 07:17:47.252000: I runner.py:310] Step = 21100 ; steps/s = 1.62, tokens/s = 43201 (43201 target) ; Learning rate = 0.000608 ; Loss = 1.683062\n",
      "2024-11-30 07:18:48.434000: I runner.py:310] Step = 21200 ; steps/s = 1.63, tokens/s = 42824 (42824 target) ; Learning rate = 0.000607 ; Loss = 1.636358\n",
      "2024-11-30 07:19:50.244000: I runner.py:310] Step = 21300 ; steps/s = 1.62, tokens/s = 43238 (43238 target) ; Learning rate = 0.000606 ; Loss = 1.692454\n",
      "2024-11-30 07:20:51.981000: I runner.py:310] Step = 21400 ; steps/s = 1.62, tokens/s = 43260 (43260 target) ; Learning rate = 0.000604 ; Loss = 1.682733\n",
      "2024-11-30 07:21:53.705000: I runner.py:310] Step = 21500 ; steps/s = 1.62, tokens/s = 43296 (43296 target) ; Learning rate = 0.000603 ; Loss = 1.682093\n",
      "2024-11-30 07:22:54.894000: I runner.py:310] Step = 21600 ; steps/s = 1.63, tokens/s = 42872 (42872 target) ; Learning rate = 0.000601 ; Loss = 1.647269\n",
      "2024-11-30 07:23:56.711000: I runner.py:310] Step = 21700 ; steps/s = 1.62, tokens/s = 43209 (43209 target) ; Learning rate = 0.000600 ; Loss = 1.673172\n",
      "2024-11-30 07:24:58.513000: I runner.py:310] Step = 21800 ; steps/s = 1.62, tokens/s = 43224 (43224 target) ; Learning rate = 0.000599 ; Loss = 1.669460\n",
      "2024-11-30 07:25:59.756000: I runner.py:310] Step = 21900 ; steps/s = 1.63, tokens/s = 42755 (42755 target) ; Learning rate = 0.000597 ; Loss = 1.653103\n",
      "2024-11-30 07:27:01.491000: I runner.py:310] Step = 22000 ; steps/s = 1.62, tokens/s = 43304 (43304 target) ; Learning rate = 0.000596 ; Loss = 1.654975\n",
      "2024-11-30 07:28:03.232000: I runner.py:310] Step = 22100 ; steps/s = 1.62, tokens/s = 43250 (43250 target) ; Learning rate = 0.000595 ; Loss = 1.652661\n",
      "2024-11-30 07:29:05.059000: I runner.py:310] Step = 22200 ; steps/s = 1.62, tokens/s = 43217 (43217 target) ; Learning rate = 0.000593 ; Loss = 1.675989\n",
      "2024-11-30 07:30:06.253000: I runner.py:310] Step = 22300 ; steps/s = 1.63, tokens/s = 42833 (42833 target) ; Learning rate = 0.000592 ; Loss = 1.645982\n",
      "2024-11-30 07:31:08.033000: I runner.py:310] Step = 22400 ; steps/s = 1.62, tokens/s = 43248 (43248 target) ; Learning rate = 0.000591 ; Loss = 1.651089\n",
      "2024-11-30 07:32:09.812000: I runner.py:310] Step = 22500 ; steps/s = 1.62, tokens/s = 43262 (43262 target) ; Learning rate = 0.000589 ; Loss = 1.653247\n",
      "2024-11-30 07:33:11.590000: I runner.py:310] Step = 22600 ; steps/s = 1.62, tokens/s = 43222 (43222 target) ; Learning rate = 0.000588 ; Loss = 1.660900\n",
      "2024-11-30 07:34:12.774000: I runner.py:310] Step = 22700 ; steps/s = 1.63, tokens/s = 42829 (42829 target) ; Learning rate = 0.000587 ; Loss = 1.650278\n",
      "2024-11-30 07:35:14.529000: I runner.py:310] Step = 22800 ; steps/s = 1.62, tokens/s = 43290 (43290 target) ; Learning rate = 0.000585 ; Loss = 1.639619\n",
      "2024-11-30 07:36:16.254000: I runner.py:310] Step = 22900 ; steps/s = 1.62, tokens/s = 43300 (43300 target) ; Learning rate = 0.000584 ; Loss = 1.654017\n",
      "2024-11-30 07:37:18.045000: I runner.py:310] Step = 23000 ; steps/s = 1.62, tokens/s = 43247 (43247 target) ; Learning rate = 0.000583 ; Loss = 1.655249\n",
      "2024-11-30 07:38:19.269000: I runner.py:310] Step = 23100 ; steps/s = 1.63, tokens/s = 42768 (42768 target) ; Learning rate = 0.000582 ; Loss = 1.644767\n",
      "2024-11-30 07:39:21.017000: I runner.py:310] Step = 23200 ; steps/s = 1.62, tokens/s = 43270 (43270 target) ; Learning rate = 0.000580 ; Loss = 1.644941\n",
      "2024-11-30 07:40:22.754000: I runner.py:310] Step = 23300 ; steps/s = 1.62, tokens/s = 43284 (43284 target) ; Learning rate = 0.000579 ; Loss = 1.643818\n",
      "2024-11-30 07:41:24.537000: I runner.py:310] Step = 23400 ; steps/s = 1.62, tokens/s = 43222 (43222 target) ; Learning rate = 0.000578 ; Loss = 1.654607\n",
      "2024-11-30 07:42:25.771000: I runner.py:310] Step = 23500 ; steps/s = 1.63, tokens/s = 42830 (42830 target) ; Learning rate = 0.000577 ; Loss = 1.651269\n",
      "2024-11-30 07:43:27.582000: I runner.py:310] Step = 23600 ; steps/s = 1.62, tokens/s = 43226 (43226 target) ; Learning rate = 0.000575 ; Loss = 1.641919\n",
      "2024-11-30 07:44:29.361000: I runner.py:310] Step = 23700 ; steps/s = 1.62, tokens/s = 43233 (43233 target) ; Learning rate = 0.000574 ; Loss = 1.635582\n",
      "2024-11-30 07:45:31.125000: I runner.py:310] Step = 23800 ; steps/s = 1.62, tokens/s = 43236 (43236 target) ; Learning rate = 0.000573 ; Loss = 1.664848\n",
      "2024-11-30 07:46:32.332000: I runner.py:310] Step = 23900 ; steps/s = 1.63, tokens/s = 42827 (42827 target) ; Learning rate = 0.000572 ; Loss = 1.619791\n",
      "2024-11-30 07:47:34.047000: I runner.py:310] Step = 24000 ; steps/s = 1.62, tokens/s = 43320 (43320 target) ; Learning rate = 0.000571 ; Loss = 1.633834\n",
      "2024-11-30 07:48:35.893000: I runner.py:310] Step = 24100 ; steps/s = 1.62, tokens/s = 43154 (43154 target) ; Learning rate = 0.000569 ; Loss = 1.647156\n",
      "2024-11-30 07:49:37.619000: I runner.py:310] Step = 24200 ; steps/s = 1.62, tokens/s = 43312 (43312 target) ; Learning rate = 0.000568 ; Loss = 1.657408\n",
      "2024-11-30 07:50:38.844000: I runner.py:310] Step = 24300 ; steps/s = 1.63, tokens/s = 42803 (42803 target) ; Learning rate = 0.000567 ; Loss = 1.619831\n",
      "2024-11-30 07:51:40.606000: I runner.py:310] Step = 24400 ; steps/s = 1.62, tokens/s = 43271 (43271 target) ; Learning rate = 0.000566 ; Loss = 1.642256\n",
      "2024-11-30 07:52:42.431000: I runner.py:310] Step = 24500 ; steps/s = 1.62, tokens/s = 43200 (43200 target) ; Learning rate = 0.000565 ; Loss = 1.651657\n",
      "2024-11-30 07:53:44.241000: I runner.py:310] Step = 24600 ; steps/s = 1.62, tokens/s = 43224 (43224 target) ; Learning rate = 0.000564 ; Loss = 1.647254\n",
      "2024-11-30 07:54:45.437000: I runner.py:310] Step = 24700 ; steps/s = 1.63, tokens/s = 42800 (42800 target) ; Learning rate = 0.000562 ; Loss = 1.615593\n",
      "2024-11-30 07:55:47.233000: I runner.py:310] Step = 24800 ; steps/s = 1.62, tokens/s = 43261 (43261 target) ; Learning rate = 0.000561 ; Loss = 1.636954\n",
      "2024-11-30 07:56:49.060000: I runner.py:310] Step = 24900 ; steps/s = 1.62, tokens/s = 43173 (43173 target) ; Learning rate = 0.000560 ; Loss = 1.646063\n",
      "2024-11-30 07:57:50.819000: I runner.py:310] Step = 25000 ; steps/s = 1.62, tokens/s = 43293 (43293 target) ; Learning rate = 0.000559 ; Loss = 1.640478\n",
      "2024-11-30 07:57:50.820000: I training.py:192] Running evaluation for step 25000\n",
      "2024-11-30 08:02:36.933000: I training.py:192] Evaluation result for step 25000: loss = 1.101162 ; perplexity = 3.007658\n",
      "2024-11-30 08:03:37.989000: I runner.py:310] Step = 25100 ; steps/s = 1.64, tokens/s = 42932 (42932 target) ; Learning rate = 0.000558 ; Loss = 1.616937\n",
      "2024-11-30 08:04:39.713000: I runner.py:310] Step = 25200 ; steps/s = 1.62, tokens/s = 43279 (43279 target) ; Learning rate = 0.000557 ; Loss = 1.633718\n",
      "2024-11-30 08:05:41.453000: I runner.py:310] Step = 25300 ; steps/s = 1.62, tokens/s = 43253 (43253 target) ; Learning rate = 0.000556 ; Loss = 1.634032\n",
      "2024-11-30 08:06:43.182000: I runner.py:310] Step = 25400 ; steps/s = 1.62, tokens/s = 43291 (43291 target) ; Learning rate = 0.000555 ; Loss = 1.637492\n",
      "2024-11-30 08:07:44.392000: I runner.py:310] Step = 25500 ; steps/s = 1.63, tokens/s = 42837 (42837 target) ; Learning rate = 0.000553 ; Loss = 1.632274\n",
      "2024-11-30 08:08:46.090000: I runner.py:310] Step = 25600 ; steps/s = 1.62, tokens/s = 43305 (43305 target) ; Learning rate = 0.000552 ; Loss = 1.627319\n",
      "2024-11-30 08:09:47.802000: I runner.py:310] Step = 25700 ; steps/s = 1.62, tokens/s = 43285 (43285 target) ; Learning rate = 0.000551 ; Loss = 1.616813\n",
      "2024-11-30 08:10:49.515000: I runner.py:310] Step = 25800 ; steps/s = 1.62, tokens/s = 43273 (43273 target) ; Learning rate = 0.000550 ; Loss = 1.653300\n",
      "2024-11-30 08:11:50.657000: I runner.py:310] Step = 25900 ; steps/s = 1.64, tokens/s = 42891 (42891 target) ; Learning rate = 0.000549 ; Loss = 1.605266\n",
      "2024-11-30 08:12:52.436000: I runner.py:310] Step = 26000 ; steps/s = 1.62, tokens/s = 43233 (43233 target) ; Learning rate = 0.000548 ; Loss = 1.637936\n",
      "2024-11-30 08:13:54.178000: I runner.py:310] Step = 26100 ; steps/s = 1.62, tokens/s = 43267 (43267 target) ; Learning rate = 0.000547 ; Loss = 1.644876\n",
      "2024-11-30 08:14:55.402000: I runner.py:310] Step = 26200 ; steps/s = 1.63, tokens/s = 42791 (42791 target) ; Learning rate = 0.000546 ; Loss = 1.627313\n",
      "2024-11-30 08:15:57.147000: I runner.py:310] Step = 26300 ; steps/s = 1.62, tokens/s = 43309 (43309 target) ; Learning rate = 0.000545 ; Loss = 1.612840\n",
      "2024-11-30 08:16:58.967000: I runner.py:310] Step = 26400 ; steps/s = 1.62, tokens/s = 43207 (43207 target) ; Learning rate = 0.000544 ; Loss = 1.633426\n",
      "2024-11-30 08:18:00.766000: I runner.py:310] Step = 26500 ; steps/s = 1.62, tokens/s = 43210 (43210 target) ; Learning rate = 0.000543 ; Loss = 1.647510\n",
      "2024-11-30 08:19:01.957000: I runner.py:310] Step = 26600 ; steps/s = 1.63, tokens/s = 42839 (42839 target) ; Learning rate = 0.000542 ; Loss = 1.613156\n",
      "2024-11-30 08:20:03.679000: I runner.py:310] Step = 26700 ; steps/s = 1.62, tokens/s = 43274 (43274 target) ; Learning rate = 0.000541 ; Loss = 1.624774\n",
      "2024-11-30 08:21:05.445000: I runner.py:310] Step = 26800 ; steps/s = 1.62, tokens/s = 43263 (43263 target) ; Learning rate = 0.000540 ; Loss = 1.621400\n",
      "2024-11-30 08:22:07.177000: I runner.py:310] Step = 26900 ; steps/s = 1.62, tokens/s = 43283 (43283 target) ; Learning rate = 0.000539 ; Loss = 1.623339\n",
      "2024-11-30 08:23:08.405000: I runner.py:310] Step = 27000 ; steps/s = 1.63, tokens/s = 42796 (42796 target) ; Learning rate = 0.000538 ; Loss = 1.617742\n",
      "2024-11-30 08:24:10.143000: I runner.py:310] Step = 27100 ; steps/s = 1.62, tokens/s = 43289 (43289 target) ; Learning rate = 0.000537 ; Loss = 1.612020\n",
      "2024-11-30 08:25:11.923000: I runner.py:310] Step = 27200 ; steps/s = 1.62, tokens/s = 43249 (43249 target) ; Learning rate = 0.000536 ; Loss = 1.629111\n",
      "2024-11-30 08:26:13.731000: I runner.py:310] Step = 27300 ; steps/s = 1.62, tokens/s = 43194 (43194 target) ; Learning rate = 0.000535 ; Loss = 1.621063\n",
      "2024-11-30 08:27:14.907000: I runner.py:310] Step = 27400 ; steps/s = 1.63, tokens/s = 42860 (42860 target) ; Learning rate = 0.000534 ; Loss = 1.616994\n",
      "2024-11-30 08:28:16.667000: I runner.py:310] Step = 27500 ; steps/s = 1.62, tokens/s = 43266 (43266 target) ; Learning rate = 0.000533 ; Loss = 1.617089\n",
      "2024-11-30 08:29:18.417000: I runner.py:310] Step = 27600 ; steps/s = 1.62, tokens/s = 43249 (43249 target) ; Learning rate = 0.000532 ; Loss = 1.604664\n",
      "2024-11-30 08:30:20.147000: I runner.py:310] Step = 27700 ; steps/s = 1.62, tokens/s = 43269 (43269 target) ; Learning rate = 0.000531 ; Loss = 1.622816\n",
      "2024-11-30 08:31:21.333000: I runner.py:310] Step = 27800 ; steps/s = 1.63, tokens/s = 42859 (42859 target) ; Learning rate = 0.000530 ; Loss = 1.617977\n",
      "2024-11-30 08:32:23.082000: I runner.py:310] Step = 27900 ; steps/s = 1.62, tokens/s = 43277 (43277 target) ; Learning rate = 0.000529 ; Loss = 1.612085\n",
      "2024-11-30 08:33:24.885000: I runner.py:310] Step = 28000 ; steps/s = 1.62, tokens/s = 43201 (43201 target) ; Learning rate = 0.000528 ; Loss = 1.608463\n",
      "2024-11-30 08:34:26.653000: I runner.py:310] Step = 28100 ; steps/s = 1.62, tokens/s = 43258 (43258 target) ; Learning rate = 0.000527 ; Loss = 1.611670\n",
      "2024-11-30 08:35:27.771000: I runner.py:310] Step = 28200 ; steps/s = 1.64, tokens/s = 42889 (42889 target) ; Learning rate = 0.000526 ; Loss = 1.615795\n",
      "2024-11-30 08:36:29.480000: I runner.py:310] Step = 28300 ; steps/s = 1.62, tokens/s = 43270 (43270 target) ; Learning rate = 0.000525 ; Loss = 1.599005\n",
      "2024-11-30 08:37:31.194000: I runner.py:310] Step = 28400 ; steps/s = 1.62, tokens/s = 43282 (43282 target) ; Learning rate = 0.000524 ; Loss = 1.607623\n",
      "2024-11-30 08:38:32.941000: I runner.py:310] Step = 28500 ; steps/s = 1.62, tokens/s = 43278 (43278 target) ; Learning rate = 0.000524 ; Loss = 1.616638\n",
      "2024-11-30 08:39:34.147000: I runner.py:310] Step = 28600 ; steps/s = 1.63, tokens/s = 42852 (42852 target) ; Learning rate = 0.000523 ; Loss = 1.600245\n",
      "2024-11-30 08:40:35.906000: I runner.py:310] Step = 28700 ; steps/s = 1.62, tokens/s = 43266 (43266 target) ; Learning rate = 0.000522 ; Loss = 1.627319\n",
      "2024-11-30 08:41:37.699000: I runner.py:310] Step = 28800 ; steps/s = 1.62, tokens/s = 43218 (43218 target) ; Learning rate = 0.000521 ; Loss = 1.611396\n",
      "2024-11-30 08:42:39.501000: I runner.py:310] Step = 28900 ; steps/s = 1.62, tokens/s = 43228 (43228 target) ; Learning rate = 0.000520 ; Loss = 1.622996\n",
      "2024-11-30 08:43:40.713000: I runner.py:310] Step = 29000 ; steps/s = 1.63, tokens/s = 42834 (42834 target) ; Learning rate = 0.000519 ; Loss = 1.602738\n",
      "2024-11-30 08:44:42.441000: I runner.py:310] Step = 29100 ; steps/s = 1.62, tokens/s = 43268 (43268 target) ; Learning rate = 0.000518 ; Loss = 1.611572\n",
      "2024-11-30 08:45:44.239000: I runner.py:310] Step = 29200 ; steps/s = 1.62, tokens/s = 43226 (43226 target) ; Learning rate = 0.000517 ; Loss = 1.618298\n",
      "2024-11-30 08:46:46.006000: I runner.py:310] Step = 29300 ; steps/s = 1.62, tokens/s = 43256 (43256 target) ; Learning rate = 0.000516 ; Loss = 1.614810\n",
      "2024-11-30 08:47:47.221000: I runner.py:310] Step = 29400 ; steps/s = 1.63, tokens/s = 42813 (42813 target) ; Learning rate = 0.000515 ; Loss = 1.602594\n",
      "2024-11-30 08:48:48.962000: I runner.py:310] Step = 29500 ; steps/s = 1.62, tokens/s = 43283 (43283 target) ; Learning rate = 0.000515 ; Loss = 1.604372\n",
      "2024-11-30 08:49:50.751000: I runner.py:310] Step = 29600 ; steps/s = 1.62, tokens/s = 43216 (43216 target) ; Learning rate = 0.000514 ; Loss = 1.617240\n",
      "2024-11-30 08:50:52.544000: I runner.py:310] Step = 29700 ; steps/s = 1.62, tokens/s = 43252 (43252 target) ; Learning rate = 0.000513 ; Loss = 1.625530\n",
      "2024-11-30 08:51:53.764000: I runner.py:310] Step = 29800 ; steps/s = 1.63, tokens/s = 42848 (42848 target) ; Learning rate = 0.000512 ; Loss = 1.612954\n",
      "2024-11-30 08:52:55.570000: I runner.py:310] Step = 29900 ; steps/s = 1.62, tokens/s = 43234 (43234 target) ; Learning rate = 0.000511 ; Loss = 1.588275\n",
      "2024-11-30 08:53:57.364000: I runner.py:310] Step = 30000 ; steps/s = 1.62, tokens/s = 43218 (43218 target) ; Learning rate = 0.000510 ; Loss = 1.607243\n",
      "2024-11-30 08:53:59.243000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-30000\n",
      "2024-11-30 08:53:59.243000: I training.py:192] Running evaluation for step 30000\n",
      "2024-11-30 08:58:30.352000: I training.py:192] Evaluation result for step 30000: loss = 1.121354 ; perplexity = 3.069006\n",
      "2024-11-30 08:59:31.999000: I runner.py:310] Step = 30100 ; steps/s = 1.62, tokens/s = 43307 (43307 target) ; Learning rate = 0.000509 ; Loss = 1.619353\n",
      "2024-11-30 09:00:33.179000: I runner.py:310] Step = 30200 ; steps/s = 1.63, tokens/s = 42861 (42861 target) ; Learning rate = 0.000509 ; Loss = 1.603021\n",
      "2024-11-30 09:01:34.974000: I runner.py:310] Step = 30300 ; steps/s = 1.62, tokens/s = 43223 (43223 target) ; Learning rate = 0.000508 ; Loss = 1.606516\n",
      "2024-11-30 09:02:36.759000: I runner.py:310] Step = 30400 ; steps/s = 1.62, tokens/s = 43234 (43234 target) ; Learning rate = 0.000507 ; Loss = 1.607539\n",
      "2024-11-30 09:03:37.945000: I runner.py:310] Step = 30500 ; steps/s = 1.63, tokens/s = 42818 (42818 target) ; Learning rate = 0.000506 ; Loss = 1.601186\n",
      "2024-11-30 09:04:39.722000: I runner.py:310] Step = 30600 ; steps/s = 1.62, tokens/s = 43251 (43251 target) ; Learning rate = 0.000505 ; Loss = 1.593418\n",
      "2024-11-30 09:05:41.551000: I runner.py:310] Step = 30700 ; steps/s = 1.62, tokens/s = 43186 (43186 target) ; Learning rate = 0.000504 ; Loss = 1.603016\n",
      "2024-11-30 09:06:43.330000: I runner.py:310] Step = 30800 ; steps/s = 1.62, tokens/s = 43280 (43280 target) ; Learning rate = 0.000504 ; Loss = 1.608930\n",
      "2024-11-30 09:07:44.552000: I runner.py:310] Step = 30900 ; steps/s = 1.63, tokens/s = 42808 (42808 target) ; Learning rate = 0.000503 ; Loss = 1.593581\n",
      "2024-11-30 09:08:46.327000: I runner.py:310] Step = 31000 ; steps/s = 1.62, tokens/s = 43267 (43267 target) ; Learning rate = 0.000502 ; Loss = 1.585326\n",
      "2024-11-30 09:09:48.041000: I runner.py:310] Step = 31100 ; steps/s = 1.62, tokens/s = 43287 (43287 target) ; Learning rate = 0.000501 ; Loss = 1.608456\n",
      "2024-11-30 09:10:49.845000: I runner.py:310] Step = 31200 ; steps/s = 1.62, tokens/s = 43222 (43222 target) ; Learning rate = 0.000500 ; Loss = 1.605169\n",
      "2024-11-30 09:11:51.027000: I runner.py:310] Step = 31300 ; steps/s = 1.63, tokens/s = 42809 (42809 target) ; Learning rate = 0.000500 ; Loss = 1.595507\n",
      "2024-11-30 09:12:52.769000: I runner.py:310] Step = 31400 ; steps/s = 1.62, tokens/s = 43266 (43266 target) ; Learning rate = 0.000499 ; Loss = 1.594420\n",
      "2024-11-30 09:13:54.558000: I runner.py:310] Step = 31500 ; steps/s = 1.62, tokens/s = 43248 (43248 target) ; Learning rate = 0.000498 ; Loss = 1.595077\n",
      "2024-11-30 09:14:56.386000: I runner.py:310] Step = 31600 ; steps/s = 1.62, tokens/s = 43232 (43232 target) ; Learning rate = 0.000497 ; Loss = 1.603632\n",
      "2024-11-30 09:15:57.577000: I runner.py:310] Step = 31700 ; steps/s = 1.63, tokens/s = 42821 (42821 target) ; Learning rate = 0.000496 ; Loss = 1.580708\n",
      "2024-11-30 09:16:59.336000: I runner.py:310] Step = 31800 ; steps/s = 1.62, tokens/s = 43249 (43249 target) ; Learning rate = 0.000496 ; Loss = 1.586902\n",
      "2024-11-30 09:18:01.037000: I runner.py:310] Step = 31900 ; steps/s = 1.62, tokens/s = 43307 (43307 target) ; Learning rate = 0.000495 ; Loss = 1.604852\n",
      "2024-11-30 09:19:02.874000: I runner.py:310] Step = 32000 ; steps/s = 1.62, tokens/s = 43200 (43200 target) ; Learning rate = 0.000494 ; Loss = 1.602352\n",
      "2024-11-30 09:20:04.085000: I runner.py:310] Step = 32100 ; steps/s = 1.63, tokens/s = 42842 (42842 target) ; Learning rate = 0.000493 ; Loss = 1.594199\n",
      "2024-11-30 09:21:05.835000: I runner.py:310] Step = 32200 ; steps/s = 1.62, tokens/s = 43271 (43271 target) ; Learning rate = 0.000493 ; Loss = 1.591222\n",
      "2024-11-30 09:22:07.654000: I runner.py:310] Step = 32300 ; steps/s = 1.62, tokens/s = 43234 (43234 target) ; Learning rate = 0.000492 ; Loss = 1.597876\n",
      "2024-11-30 09:23:09.460000: I runner.py:310] Step = 32400 ; steps/s = 1.62, tokens/s = 43203 (43203 target) ; Learning rate = 0.000491 ; Loss = 1.590673\n",
      "2024-11-30 09:24:10.646000: I runner.py:310] Step = 32500 ; steps/s = 1.63, tokens/s = 42831 (42831 target) ; Learning rate = 0.000490 ; Loss = 1.596058\n",
      "2024-11-30 09:25:12.433000: I runner.py:310] Step = 32600 ; steps/s = 1.62, tokens/s = 43256 (43256 target) ; Learning rate = 0.000490 ; Loss = 1.580951\n",
      "2024-11-30 09:26:14.243000: I runner.py:310] Step = 32700 ; steps/s = 1.62, tokens/s = 43217 (43217 target) ; Learning rate = 0.000489 ; Loss = 1.593208\n",
      "2024-11-30 09:27:16.022000: I runner.py:310] Step = 32800 ; steps/s = 1.62, tokens/s = 43219 (43219 target) ; Learning rate = 0.000488 ; Loss = 1.591656\n",
      "2024-11-30 09:28:17.207000: I runner.py:310] Step = 32900 ; steps/s = 1.63, tokens/s = 42846 (42846 target) ; Learning rate = 0.000487 ; Loss = 1.594511\n",
      "2024-11-30 09:29:18.967000: I runner.py:310] Step = 33000 ; steps/s = 1.62, tokens/s = 43256 (43256 target) ; Learning rate = 0.000487 ; Loss = 1.590751\n",
      "2024-11-30 09:30:20.716000: I runner.py:310] Step = 33100 ; steps/s = 1.62, tokens/s = 43275 (43275 target) ; Learning rate = 0.000486 ; Loss = 1.590070\n",
      "2024-11-30 09:31:22.509000: I runner.py:310] Step = 33200 ; steps/s = 1.62, tokens/s = 43232 (43232 target) ; Learning rate = 0.000485 ; Loss = 1.599035\n",
      "2024-11-30 09:32:23.706000: I runner.py:310] Step = 33300 ; steps/s = 1.63, tokens/s = 42823 (42823 target) ; Learning rate = 0.000484 ; Loss = 1.579816\n",
      "2024-11-30 09:33:25.493000: I runner.py:310] Step = 33400 ; steps/s = 1.62, tokens/s = 43243 (43243 target) ; Learning rate = 0.000484 ; Loss = 1.584267\n",
      "2024-11-30 09:34:27.282000: I runner.py:310] Step = 33500 ; steps/s = 1.62, tokens/s = 43225 (43225 target) ; Learning rate = 0.000483 ; Loss = 1.597684\n",
      "2024-11-30 09:35:29.067000: I runner.py:310] Step = 33600 ; steps/s = 1.62, tokens/s = 43253 (43253 target) ; Learning rate = 0.000482 ; Loss = 1.599540\n",
      "2024-11-30 09:36:30.329000: I runner.py:310] Step = 33700 ; steps/s = 1.63, tokens/s = 42778 (42778 target) ; Learning rate = 0.000481 ; Loss = 1.587633\n",
      "2024-11-30 09:37:32.123000: I runner.py:310] Step = 33800 ; steps/s = 1.62, tokens/s = 43233 (43233 target) ; Learning rate = 0.000481 ; Loss = 1.572074\n",
      "2024-11-30 09:38:33.849000: I runner.py:310] Step = 33900 ; steps/s = 1.62, tokens/s = 43254 (43254 target) ; Learning rate = 0.000480 ; Loss = 1.586550\n",
      "2024-11-30 09:39:35.639000: I runner.py:310] Step = 34000 ; steps/s = 1.62, tokens/s = 43258 (43258 target) ; Learning rate = 0.000479 ; Loss = 1.590656\n",
      "2024-11-30 09:40:36.858000: I runner.py:310] Step = 34100 ; steps/s = 1.63, tokens/s = 42834 (42834 target) ; Learning rate = 0.000479 ; Loss = 1.569070\n",
      "2024-11-30 09:41:38.631000: I runner.py:310] Step = 34200 ; steps/s = 1.62, tokens/s = 43253 (43253 target) ; Learning rate = 0.000478 ; Loss = 1.578614\n",
      "2024-11-30 09:42:40.377000: I runner.py:310] Step = 34300 ; steps/s = 1.62, tokens/s = 43253 (43253 target) ; Learning rate = 0.000477 ; Loss = 1.590893\n",
      "2024-11-30 09:43:42.153000: I runner.py:310] Step = 34400 ; steps/s = 1.62, tokens/s = 43236 (43236 target) ; Learning rate = 0.000477 ; Loss = 1.582770\n",
      "2024-11-30 09:44:43.336000: I runner.py:310] Step = 34500 ; steps/s = 1.63, tokens/s = 42854 (42854 target) ; Learning rate = 0.000476 ; Loss = 1.583631\n",
      "2024-11-30 09:45:45.058000: I runner.py:310] Step = 34600 ; steps/s = 1.62, tokens/s = 43290 (43290 target) ; Learning rate = 0.000475 ; Loss = 1.576123\n",
      "2024-11-30 09:46:46.875000: I runner.py:310] Step = 34700 ; steps/s = 1.62, tokens/s = 43167 (43167 target) ; Learning rate = 0.000474 ; Loss = 1.585167\n",
      "2024-11-30 09:47:48.097000: I runner.py:310] Step = 34800 ; steps/s = 1.63, tokens/s = 42822 (42822 target) ; Learning rate = 0.000474 ; Loss = 1.570076\n",
      "2024-11-30 09:48:49.880000: I runner.py:310] Step = 34900 ; steps/s = 1.62, tokens/s = 43274 (43274 target) ; Learning rate = 0.000473 ; Loss = 1.580275\n",
      "2024-11-30 09:49:51.677000: I runner.py:310] Step = 35000 ; steps/s = 1.62, tokens/s = 43220 (43220 target) ; Learning rate = 0.000472 ; Loss = 1.574746\n",
      "2024-11-30 09:49:51.678000: I training.py:192] Running evaluation for step 35000\n",
      "2024-11-30 09:54:23.689000: I training.py:192] Evaluation result for step 35000: loss = 1.136671 ; perplexity = 3.116376\n",
      "2024-11-30 09:55:25.407000: I runner.py:310] Step = 35100 ; steps/s = 1.62, tokens/s = 43294 (43294 target) ; Learning rate = 0.000472 ; Loss = 1.583295\n",
      "2024-11-30 09:56:26.642000: I runner.py:310] Step = 35200 ; steps/s = 1.63, tokens/s = 42803 (42803 target) ; Learning rate = 0.000471 ; Loss = 1.581644\n",
      "2024-11-30 09:57:28.421000: I runner.py:310] Step = 35300 ; steps/s = 1.62, tokens/s = 43262 (43262 target) ; Learning rate = 0.000470 ; Loss = 1.586706\n",
      "2024-11-30 09:58:30.127000: I runner.py:310] Step = 35400 ; steps/s = 1.62, tokens/s = 43276 (43276 target) ; Learning rate = 0.000470 ; Loss = 1.573799\n",
      "2024-11-30 09:59:31.904000: I runner.py:310] Step = 35500 ; steps/s = 1.62, tokens/s = 43249 (43249 target) ; Learning rate = 0.000469 ; Loss = 1.579673\n",
      "2024-11-30 10:00:33.155000: I runner.py:310] Step = 35600 ; steps/s = 1.63, tokens/s = 42802 (42802 target) ; Learning rate = 0.000468 ; Loss = 1.581625\n",
      "2024-11-30 10:01:34.913000: I runner.py:310] Step = 35700 ; steps/s = 1.62, tokens/s = 43284 (43284 target) ; Learning rate = 0.000468 ; Loss = 1.588067\n",
      "2024-11-30 10:02:36.738000: I runner.py:310] Step = 35800 ; steps/s = 1.62, tokens/s = 43228 (43228 target) ; Learning rate = 0.000467 ; Loss = 1.573947\n",
      "2024-11-30 10:03:38.550000: I runner.py:310] Step = 35900 ; steps/s = 1.62, tokens/s = 43198 (43198 target) ; Learning rate = 0.000466 ; Loss = 1.579129\n",
      "2024-11-30 10:04:39.730000: I runner.py:310] Step = 36000 ; steps/s = 1.63, tokens/s = 42808 (42808 target) ; Learning rate = 0.000466 ; Loss = 1.571559\n",
      "2024-11-30 10:05:41.497000: I runner.py:310] Step = 36100 ; steps/s = 1.62, tokens/s = 43248 (43248 target) ; Learning rate = 0.000465 ; Loss = 1.574054\n",
      "2024-11-30 10:06:43.233000: I runner.py:310] Step = 36200 ; steps/s = 1.62, tokens/s = 43287 (43287 target) ; Learning rate = 0.000465 ; Loss = 1.585775\n",
      "2024-11-30 10:07:45.016000: I runner.py:310] Step = 36300 ; steps/s = 1.62, tokens/s = 43224 (43224 target) ; Learning rate = 0.000464 ; Loss = 1.594590\n",
      "2024-11-30 10:08:46.132000: I runner.py:310] Step = 36400 ; steps/s = 1.64, tokens/s = 42900 (42900 target) ; Learning rate = 0.000463 ; Loss = 1.570150\n",
      "2024-11-30 10:09:47.818000: I runner.py:310] Step = 36500 ; steps/s = 1.62, tokens/s = 43306 (43306 target) ; Learning rate = 0.000463 ; Loss = 1.570231\n",
      "2024-11-30 10:10:49.597000: I runner.py:310] Step = 36600 ; steps/s = 1.62, tokens/s = 43266 (43266 target) ; Learning rate = 0.000462 ; Loss = 1.582358\n",
      "2024-11-30 10:11:51.407000: I runner.py:310] Step = 36700 ; steps/s = 1.62, tokens/s = 43228 (43228 target) ; Learning rate = 0.000461 ; Loss = 1.581274\n",
      "2024-11-30 10:12:52.609000: I runner.py:310] Step = 36800 ; steps/s = 1.63, tokens/s = 42818 (42818 target) ; Learning rate = 0.000461 ; Loss = 1.579060\n",
      "2024-11-30 10:13:54.371000: I runner.py:310] Step = 36900 ; steps/s = 1.62, tokens/s = 43283 (43283 target) ; Learning rate = 0.000460 ; Loss = 1.569411\n",
      "2024-11-30 10:14:56.153000: I runner.py:310] Step = 37000 ; steps/s = 1.62, tokens/s = 43235 (43235 target) ; Learning rate = 0.000460 ; Loss = 1.571188\n",
      "2024-11-30 10:15:57.876000: I runner.py:310] Step = 37100 ; steps/s = 1.62, tokens/s = 43261 (43261 target) ; Learning rate = 0.000459 ; Loss = 1.584011\n",
      "2024-11-30 10:16:59.064000: I runner.py:310] Step = 37200 ; steps/s = 1.63, tokens/s = 42838 (42838 target) ; Learning rate = 0.000458 ; Loss = 1.558613\n",
      "2024-11-30 10:18:00.891000: I runner.py:310] Step = 37300 ; steps/s = 1.62, tokens/s = 43223 (43223 target) ; Learning rate = 0.000458 ; Loss = 1.575834\n",
      "2024-11-30 10:19:02.697000: I runner.py:310] Step = 37400 ; steps/s = 1.62, tokens/s = 43220 (43220 target) ; Learning rate = 0.000457 ; Loss = 1.586620\n",
      "2024-11-30 10:20:04.480000: I runner.py:310] Step = 37500 ; steps/s = 1.62, tokens/s = 43251 (43251 target) ; Learning rate = 0.000456 ; Loss = 1.584288\n",
      "2024-11-30 10:21:05.706000: I runner.py:310] Step = 37600 ; steps/s = 1.63, tokens/s = 42799 (42799 target) ; Learning rate = 0.000456 ; Loss = 1.577389\n",
      "2024-11-30 10:22:07.563000: I runner.py:310] Step = 37700 ; steps/s = 1.62, tokens/s = 43191 (43191 target) ; Learning rate = 0.000455 ; Loss = 1.567620\n",
      "2024-11-30 10:23:09.312000: I runner.py:310] Step = 37800 ; steps/s = 1.62, tokens/s = 43289 (43289 target) ; Learning rate = 0.000455 ; Loss = 1.576532\n",
      "2024-11-30 10:24:11.133000: I runner.py:310] Step = 37900 ; steps/s = 1.62, tokens/s = 43192 (43192 target) ; Learning rate = 0.000454 ; Loss = 1.573198\n",
      "2024-11-30 10:25:12.310000: I runner.py:310] Step = 38000 ; steps/s = 1.63, tokens/s = 42834 (42834 target) ; Learning rate = 0.000453 ; Loss = 1.559211\n",
      "2024-11-30 10:26:14.101000: I runner.py:310] Step = 38100 ; steps/s = 1.62, tokens/s = 43221 (43221 target) ; Learning rate = 0.000453 ; Loss = 1.562085\n",
      "2024-11-30 10:27:15.868000: I runner.py:310] Step = 38200 ; steps/s = 1.62, tokens/s = 43234 (43234 target) ; Learning rate = 0.000452 ; Loss = 1.576035\n",
      "2024-11-30 10:28:17.618000: I runner.py:310] Step = 38300 ; steps/s = 1.62, tokens/s = 43290 (43290 target) ; Learning rate = 0.000452 ; Loss = 1.579026\n",
      "2024-11-30 10:29:18.796000: I runner.py:310] Step = 38400 ; steps/s = 1.63, tokens/s = 42851 (42851 target) ; Learning rate = 0.000451 ; Loss = 1.579225\n",
      "2024-11-30 10:30:20.550000: I runner.py:310] Step = 38500 ; steps/s = 1.62, tokens/s = 43263 (43263 target) ; Learning rate = 0.000450 ; Loss = 1.573873\n",
      "2024-11-30 10:31:22.285000: I runner.py:310] Step = 38600 ; steps/s = 1.62, tokens/s = 43262 (43262 target) ; Learning rate = 0.000450 ; Loss = 1.570897\n",
      "2024-11-30 10:32:24.061000: I runner.py:310] Step = 38700 ; steps/s = 1.62, tokens/s = 43263 (43263 target) ; Learning rate = 0.000449 ; Loss = 1.573060\n",
      "2024-11-30 10:33:25.230000: I runner.py:310] Step = 38800 ; steps/s = 1.63, tokens/s = 42813 (42813 target) ; Learning rate = 0.000449 ; Loss = 1.559079\n",
      "2024-11-30 10:34:26.998000: I runner.py:310] Step = 38900 ; steps/s = 1.62, tokens/s = 43253 (43253 target) ; Learning rate = 0.000448 ; Loss = 1.564985\n",
      "2024-11-30 10:35:28.776000: I runner.py:310] Step = 39000 ; steps/s = 1.62, tokens/s = 43241 (43241 target) ; Learning rate = 0.000448 ; Loss = 1.580148\n",
      "2024-11-30 10:36:30.067000: I runner.py:310] Step = 39100 ; steps/s = 1.63, tokens/s = 42771 (42771 target) ; Learning rate = 0.000447 ; Loss = 1.599258\n",
      "2024-11-30 10:37:31.769000: I runner.py:310] Step = 39200 ; steps/s = 1.62, tokens/s = 43317 (43317 target) ; Learning rate = 0.000446 ; Loss = 1.569058\n",
      "2024-11-30 10:38:33.506000: I runner.py:310] Step = 39300 ; steps/s = 1.62, tokens/s = 43272 (43272 target) ; Learning rate = 0.000446 ; Loss = 1.562720\n",
      "2024-11-30 10:39:35.324000: I runner.py:310] Step = 39400 ; steps/s = 1.62, tokens/s = 43214 (43214 target) ; Learning rate = 0.000445 ; Loss = 1.570238\n",
      "2024-11-30 10:40:36.496000: I runner.py:310] Step = 39500 ; steps/s = 1.63, tokens/s = 42852 (42852 target) ; Learning rate = 0.000445 ; Loss = 1.575266\n",
      "2024-11-30 10:41:38.279000: I runner.py:310] Step = 39600 ; steps/s = 1.62, tokens/s = 43302 (43302 target) ; Learning rate = 0.000444 ; Loss = 1.561334\n",
      "2024-11-30 10:42:40.072000: I runner.py:310] Step = 39700 ; steps/s = 1.62, tokens/s = 43204 (43204 target) ; Learning rate = 0.000444 ; Loss = 1.573931\n",
      "2024-11-30 10:43:41.871000: I runner.py:310] Step = 39800 ; steps/s = 1.62, tokens/s = 43201 (43201 target) ; Learning rate = 0.000443 ; Loss = 1.571442\n",
      "2024-11-30 10:44:43.077000: I runner.py:310] Step = 39900 ; steps/s = 1.63, tokens/s = 42821 (42821 target) ; Learning rate = 0.000442 ; Loss = 1.556442\n",
      "2024-11-30 10:45:44.916000: I runner.py:310] Step = 40000 ; steps/s = 1.62, tokens/s = 43227 (43227 target) ; Learning rate = 0.000442 ; Loss = 1.556713\n",
      "2024-11-30 10:45:46.829000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-40000\n",
      "2024-11-30 10:45:46.830000: I training.py:192] Running evaluation for step 40000\n",
      "2024-11-30 10:50:17.026000: I training.py:192] Evaluation result for step 40000: loss = 1.148771 ; perplexity = 3.154315\n",
      "2024-11-30 10:51:18.799000: I runner.py:310] Step = 40100 ; steps/s = 1.62, tokens/s = 43265 (43265 target) ; Learning rate = 0.000441 ; Loss = 1.563671\n",
      "2024-11-30 10:52:20.580000: I runner.py:310] Step = 40200 ; steps/s = 1.62, tokens/s = 43214 (43214 target) ; Learning rate = 0.000441 ; Loss = 1.577362\n",
      "2024-11-30 10:53:21.787000: I runner.py:310] Step = 40300 ; steps/s = 1.63, tokens/s = 42818 (42818 target) ; Learning rate = 0.000440 ; Loss = 1.563543\n",
      "2024-11-30 10:54:23.617000: I runner.py:310] Step = 40400 ; steps/s = 1.62, tokens/s = 43220 (43220 target) ; Learning rate = 0.000440 ; Loss = 1.561535\n",
      "2024-11-30 10:55:25.453000: I runner.py:310] Step = 40500 ; steps/s = 1.62, tokens/s = 43227 (43227 target) ; Learning rate = 0.000439 ; Loss = 1.569651\n",
      "2024-11-30 10:56:27.248000: I runner.py:310] Step = 40600 ; steps/s = 1.62, tokens/s = 43225 (43225 target) ; Learning rate = 0.000439 ; Loss = 1.568011\n",
      "2024-11-30 10:57:28.497000: I runner.py:310] Step = 40700 ; steps/s = 1.63, tokens/s = 42774 (42774 target) ; Learning rate = 0.000438 ; Loss = 1.563031\n",
      "2024-11-30 10:58:30.258000: I runner.py:310] Step = 40800 ; steps/s = 1.62, tokens/s = 43235 (43235 target) ; Learning rate = 0.000438 ; Loss = 1.563435\n",
      "2024-11-30 10:59:32.057000: I runner.py:310] Step = 40900 ; steps/s = 1.62, tokens/s = 43255 (43255 target) ; Learning rate = 0.000437 ; Loss = 1.567602\n",
      "2024-11-30 11:00:33.827000: I runner.py:310] Step = 41000 ; steps/s = 1.62, tokens/s = 43241 (43241 target) ; Learning rate = 0.000437 ; Loss = 1.568306\n",
      "2024-11-30 11:01:35.105000: I runner.py:310] Step = 41100 ; steps/s = 1.63, tokens/s = 42794 (42794 target) ; Learning rate = 0.000436 ; Loss = 1.560235\n",
      "2024-11-30 11:02:36.868000: I runner.py:310] Step = 41200 ; steps/s = 1.62, tokens/s = 43241 (43241 target) ; Learning rate = 0.000435 ; Loss = 1.559149\n",
      "2024-11-30 11:03:38.669000: I runner.py:310] Step = 41300 ; steps/s = 1.62, tokens/s = 43216 (43216 target) ; Learning rate = 0.000435 ; Loss = 1.568311\n",
      "2024-11-30 11:04:40.444000: I runner.py:310] Step = 41400 ; steps/s = 1.62, tokens/s = 43239 (43239 target) ; Learning rate = 0.000434 ; Loss = 1.575561\n",
      "2024-11-30 11:05:41.731000: I runner.py:310] Step = 41500 ; steps/s = 1.63, tokens/s = 42774 (42774 target) ; Learning rate = 0.000434 ; Loss = 1.549662\n",
      "2024-11-30 11:06:43.546000: I runner.py:310] Step = 41600 ; steps/s = 1.62, tokens/s = 43239 (43239 target) ; Learning rate = 0.000433 ; Loss = 1.558642\n",
      "2024-11-30 11:07:45.335000: I runner.py:310] Step = 41700 ; steps/s = 1.62, tokens/s = 43254 (43254 target) ; Learning rate = 0.000433 ; Loss = 1.561051\n",
      "2024-11-30 11:08:47.150000: I runner.py:310] Step = 41800 ; steps/s = 1.62, tokens/s = 43215 (43215 target) ; Learning rate = 0.000432 ; Loss = 1.566134\n",
      "2024-11-30 11:09:48.403000: I runner.py:310] Step = 41900 ; steps/s = 1.63, tokens/s = 42791 (42791 target) ; Learning rate = 0.000432 ; Loss = 1.572211\n",
      "2024-11-30 11:10:50.196000: I runner.py:310] Step = 42000 ; steps/s = 1.62, tokens/s = 43261 (43261 target) ; Learning rate = 0.000431 ; Loss = 1.557406\n",
      "2024-11-30 11:11:52.027000: I runner.py:310] Step = 42100 ; steps/s = 1.62, tokens/s = 43168 (43168 target) ; Learning rate = 0.000431 ; Loss = 1.554453\n",
      "2024-11-30 11:12:53.806000: I runner.py:310] Step = 42200 ; steps/s = 1.62, tokens/s = 43254 (43254 target) ; Learning rate = 0.000430 ; Loss = 1.561642\n",
      "2024-11-30 11:13:55.037000: I runner.py:310] Step = 42300 ; steps/s = 1.63, tokens/s = 42780 (42780 target) ; Learning rate = 0.000430 ; Loss = 1.553223\n",
      "2024-11-30 11:14:56.800000: I runner.py:310] Step = 42400 ; steps/s = 1.62, tokens/s = 43251 (43251 target) ; Learning rate = 0.000429 ; Loss = 1.560422\n",
      "2024-11-30 11:15:58.556000: I runner.py:310] Step = 42500 ; steps/s = 1.62, tokens/s = 43264 (43264 target) ; Learning rate = 0.000429 ; Loss = 1.572390\n",
      "2024-11-30 11:17:00.342000: I runner.py:310] Step = 42600 ; steps/s = 1.62, tokens/s = 43249 (43249 target) ; Learning rate = 0.000428 ; Loss = 1.565772\n",
      "2024-11-30 11:18:01.485000: I runner.py:310] Step = 42700 ; steps/s = 1.64, tokens/s = 42854 (42854 target) ; Learning rate = 0.000428 ; Loss = 1.563624\n",
      "2024-11-30 11:19:03.305000: I runner.py:310] Step = 42800 ; steps/s = 1.62, tokens/s = 43203 (43203 target) ; Learning rate = 0.000427 ; Loss = 1.550824\n",
      "2024-11-30 11:20:05.129000: I runner.py:310] Step = 42900 ; steps/s = 1.62, tokens/s = 43216 (43216 target) ; Learning rate = 0.000427 ; Loss = 1.557011\n",
      "2024-11-30 11:21:06.975000: I runner.py:310] Step = 43000 ; steps/s = 1.62, tokens/s = 43216 (43216 target) ; Learning rate = 0.000426 ; Loss = 1.552576\n",
      "2024-11-30 11:22:08.169000: I runner.py:310] Step = 43100 ; steps/s = 1.63, tokens/s = 42822 (42822 target) ; Learning rate = 0.000426 ; Loss = 1.543856\n",
      "2024-11-30 11:23:09.961000: I runner.py:310] Step = 43200 ; steps/s = 1.62, tokens/s = 43236 (43236 target) ; Learning rate = 0.000425 ; Loss = 1.555549\n",
      "2024-11-30 11:24:11.760000: I runner.py:310] Step = 43300 ; steps/s = 1.62, tokens/s = 43233 (43233 target) ; Learning rate = 0.000425 ; Loss = 1.572198\n",
      "2024-11-30 11:25:13.197000: I runner.py:310] Step = 43400 ; steps/s = 1.63, tokens/s = 42943 (42943 target) ; Learning rate = 0.000424 ; Loss = 1.559714\n",
      "2024-11-30 11:26:14.763000: I runner.py:310] Step = 43500 ; steps/s = 1.62, tokens/s = 43100 (43100 target) ; Learning rate = 0.000424 ; Loss = 1.565073\n",
      "2024-11-30 11:27:16.587000: I runner.py:310] Step = 43600 ; steps/s = 1.62, tokens/s = 43213 (43213 target) ; Learning rate = 0.000423 ; Loss = 1.557915\n",
      "2024-11-30 11:28:18.351000: I runner.py:310] Step = 43700 ; steps/s = 1.62, tokens/s = 43257 (43257 target) ; Learning rate = 0.000423 ; Loss = 1.555415\n",
      "2024-11-30 11:29:19.557000: I runner.py:310] Step = 43800 ; steps/s = 1.63, tokens/s = 42824 (42824 target) ; Learning rate = 0.000422 ; Loss = 1.560557\n",
      "2024-11-30 11:30:21.338000: I runner.py:310] Step = 43900 ; steps/s = 1.62, tokens/s = 43272 (43272 target) ; Learning rate = 0.000422 ; Loss = 1.553910\n",
      "2024-11-30 11:31:23.149000: I runner.py:310] Step = 44000 ; steps/s = 1.62, tokens/s = 43203 (43203 target) ; Learning rate = 0.000421 ; Loss = 1.546273\n",
      "2024-11-30 11:32:24.928000: I runner.py:310] Step = 44100 ; steps/s = 1.62, tokens/s = 43247 (43247 target) ; Learning rate = 0.000421 ; Loss = 1.559889\n",
      "2024-11-30 11:33:26.218000: I runner.py:310] Step = 44200 ; steps/s = 1.63, tokens/s = 42759 (42759 target) ; Learning rate = 0.000420 ; Loss = 1.550315\n",
      "2024-11-30 11:34:28.058000: I runner.py:310] Step = 44300 ; steps/s = 1.62, tokens/s = 43229 (43229 target) ; Learning rate = 0.000420 ; Loss = 1.539785\n",
      "2024-11-30 11:35:29.794000: I runner.py:310] Step = 44400 ; steps/s = 1.62, tokens/s = 43280 (43280 target) ; Learning rate = 0.000419 ; Loss = 1.557108\n",
      "2024-11-30 11:36:31.581000: I runner.py:310] Step = 44500 ; steps/s = 1.62, tokens/s = 43232 (43232 target) ; Learning rate = 0.000419 ; Loss = 1.555936\n",
      "2024-11-30 11:37:32.752000: I runner.py:310] Step = 44600 ; steps/s = 1.64, tokens/s = 42818 (42818 target) ; Learning rate = 0.000419 ; Loss = 1.544583\n",
      "2024-11-30 11:38:34.559000: I runner.py:310] Step = 44700 ; steps/s = 1.62, tokens/s = 43229 (43229 target) ; Learning rate = 0.000418 ; Loss = 1.546348\n",
      "2024-11-30 11:39:36.431000: I runner.py:310] Step = 44800 ; steps/s = 1.62, tokens/s = 43210 (43210 target) ; Learning rate = 0.000418 ; Loss = 1.557007\n",
      "2024-11-30 11:40:38.210000: I runner.py:310] Step = 44900 ; steps/s = 1.62, tokens/s = 43211 (43211 target) ; Learning rate = 0.000417 ; Loss = 1.549948\n",
      "2024-11-30 11:41:39.437000: I runner.py:310] Step = 45000 ; steps/s = 1.63, tokens/s = 42819 (42819 target) ; Learning rate = 0.000417 ; Loss = 1.549962\n",
      "2024-11-30 11:41:39.438000: I training.py:192] Running evaluation for step 45000\n",
      "2024-11-30 11:46:05.750000: I training.py:192] Evaluation result for step 45000: loss = 1.155352 ; perplexity = 3.175141\n",
      "2024-11-30 11:47:07.492000: I runner.py:310] Step = 45100 ; steps/s = 1.62, tokens/s = 43289 (43289 target) ; Learning rate = 0.000416 ; Loss = 1.546729\n",
      "2024-11-30 11:48:09.327000: I runner.py:310] Step = 45200 ; steps/s = 1.62, tokens/s = 43211 (43211 target) ; Learning rate = 0.000416 ; Loss = 1.552988\n",
      "2024-11-30 11:49:11.158000: I runner.py:310] Step = 45300 ; steps/s = 1.62, tokens/s = 43200 (43200 target) ; Learning rate = 0.000415 ; Loss = 1.553258\n",
      "2024-11-30 11:50:12.403000: I runner.py:310] Step = 45400 ; steps/s = 1.63, tokens/s = 42784 (42784 target) ; Learning rate = 0.000415 ; Loss = 1.544276\n",
      "2024-11-30 11:51:14.262000: I runner.py:310] Step = 45500 ; steps/s = 1.62, tokens/s = 43193 (43193 target) ; Learning rate = 0.000414 ; Loss = 1.557809\n",
      "2024-11-30 11:52:16.028000: I runner.py:310] Step = 45600 ; steps/s = 1.62, tokens/s = 43270 (43270 target) ; Learning rate = 0.000414 ; Loss = 1.555169\n",
      "2024-11-30 11:53:17.795000: I runner.py:310] Step = 45700 ; steps/s = 1.62, tokens/s = 43239 (43239 target) ; Learning rate = 0.000413 ; Loss = 1.567580\n",
      "2024-11-30 11:54:19.063000: I runner.py:310] Step = 45800 ; steps/s = 1.63, tokens/s = 42779 (42779 target) ; Learning rate = 0.000413 ; Loss = 1.550062\n",
      "2024-11-30 11:55:20.880000: I runner.py:310] Step = 45900 ; steps/s = 1.62, tokens/s = 43242 (43242 target) ; Learning rate = 0.000413 ; Loss = 1.550566\n",
      "2024-11-30 11:56:22.749000: I runner.py:310] Step = 46000 ; steps/s = 1.62, tokens/s = 43193 (43193 target) ; Learning rate = 0.000412 ; Loss = 1.548840\n",
      "2024-11-30 11:57:24.518000: I runner.py:310] Step = 46100 ; steps/s = 1.62, tokens/s = 43223 (43223 target) ; Learning rate = 0.000412 ; Loss = 1.555068\n",
      "2024-11-30 11:58:25.760000: I runner.py:310] Step = 46200 ; steps/s = 1.63, tokens/s = 42763 (42763 target) ; Learning rate = 0.000411 ; Loss = 1.561757\n",
      "2024-11-30 11:59:27.558000: I runner.py:310] Step = 46300 ; steps/s = 1.62, tokens/s = 43260 (43260 target) ; Learning rate = 0.000411 ; Loss = 1.545871\n",
      "2024-11-30 12:00:29.380000: I runner.py:310] Step = 46400 ; steps/s = 1.62, tokens/s = 43224 (43224 target) ; Learning rate = 0.000410 ; Loss = 1.545937\n",
      "2024-11-30 12:01:31.156000: I runner.py:310] Step = 46500 ; steps/s = 1.62, tokens/s = 43249 (43249 target) ; Learning rate = 0.000410 ; Loss = 1.550651\n",
      "2024-11-30 12:02:32.360000: I runner.py:310] Step = 46600 ; steps/s = 1.63, tokens/s = 42791 (42791 target) ; Learning rate = 0.000409 ; Loss = 1.537871\n",
      "2024-11-30 12:03:34.151000: I runner.py:310] Step = 46700 ; steps/s = 1.62, tokens/s = 43223 (43223 target) ; Learning rate = 0.000409 ; Loss = 1.545598\n",
      "2024-11-30 12:04:35.920000: I runner.py:310] Step = 46800 ; steps/s = 1.62, tokens/s = 43269 (43269 target) ; Learning rate = 0.000409 ; Loss = 1.561423\n",
      "2024-11-30 12:05:37.740000: I runner.py:310] Step = 46900 ; steps/s = 1.62, tokens/s = 43224 (43224 target) ; Learning rate = 0.000408 ; Loss = 1.547863\n",
      "2024-11-30 12:06:38.973000: I runner.py:310] Step = 47000 ; steps/s = 1.63, tokens/s = 42794 (42794 target) ; Learning rate = 0.000408 ; Loss = 1.540707\n",
      "2024-11-30 12:07:40.762000: I runner.py:310] Step = 47100 ; steps/s = 1.62, tokens/s = 43245 (43245 target) ; Learning rate = 0.000407 ; Loss = 1.549423\n",
      "2024-11-30 12:08:42.589000: I runner.py:310] Step = 47200 ; steps/s = 1.62, tokens/s = 43237 (43237 target) ; Learning rate = 0.000407 ; Loss = 1.550161\n",
      "2024-11-30 12:09:44.414000: I runner.py:310] Step = 47300 ; steps/s = 1.62, tokens/s = 43203 (43203 target) ; Learning rate = 0.000406 ; Loss = 1.552835\n",
      "2024-11-30 12:10:45.644000: I runner.py:310] Step = 47400 ; steps/s = 1.63, tokens/s = 42804 (42804 target) ; Learning rate = 0.000406 ; Loss = 1.548680\n",
      "2024-11-30 12:11:47.435000: I runner.py:310] Step = 47500 ; steps/s = 1.62, tokens/s = 43244 (43244 target) ; Learning rate = 0.000406 ; Loss = 1.553023\n",
      "2024-11-30 12:12:49.220000: I runner.py:310] Step = 47600 ; steps/s = 1.62, tokens/s = 43249 (43249 target) ; Learning rate = 0.000405 ; Loss = 1.544300\n",
      "2024-11-30 12:13:50.770000: I runner.py:310] Step = 47700 ; steps/s = 1.62, tokens/s = 43057 (43057 target) ; Learning rate = 0.000405 ; Loss = 1.545159\n",
      "2024-11-30 12:14:52.175000: I runner.py:310] Step = 47800 ; steps/s = 1.63, tokens/s = 43006 (43006 target) ; Learning rate = 0.000404 ; Loss = 1.533259\n",
      "2024-11-30 12:15:53.960000: I runner.py:310] Step = 47900 ; steps/s = 1.62, tokens/s = 43240 (43240 target) ; Learning rate = 0.000404 ; Loss = 1.553261\n",
      "2024-11-30 12:16:55.766000: I runner.py:310] Step = 48000 ; steps/s = 1.62, tokens/s = 43240 (43240 target) ; Learning rate = 0.000403 ; Loss = 1.547714\n",
      "2024-11-30 12:17:56.975000: I runner.py:310] Step = 48100 ; steps/s = 1.63, tokens/s = 42794 (42794 target) ; Learning rate = 0.000403 ; Loss = 1.549420\n",
      "2024-11-30 12:18:58.774000: I runner.py:310] Step = 48200 ; steps/s = 1.62, tokens/s = 43211 (43211 target) ; Learning rate = 0.000403 ; Loss = 1.545705\n",
      "2024-11-30 12:20:00.610000: I runner.py:310] Step = 48300 ; steps/s = 1.62, tokens/s = 43217 (43217 target) ; Learning rate = 0.000402 ; Loss = 1.549503\n",
      "2024-11-30 12:21:02.452000: I runner.py:310] Step = 48400 ; steps/s = 1.62, tokens/s = 43200 (43200 target) ; Learning rate = 0.000402 ; Loss = 1.547558\n",
      "2024-11-30 12:22:03.683000: I runner.py:310] Step = 48500 ; steps/s = 1.63, tokens/s = 42820 (42820 target) ; Learning rate = 0.000401 ; Loss = 1.546362\n",
      "2024-11-30 12:23:05.392000: I runner.py:310] Step = 48600 ; steps/s = 1.62, tokens/s = 43308 (43308 target) ; Learning rate = 0.000401 ; Loss = 1.550680\n",
      "2024-11-30 12:24:07.294000: I runner.py:310] Step = 48700 ; steps/s = 1.62, tokens/s = 43169 (43169 target) ; Learning rate = 0.000401 ; Loss = 1.539442\n",
      "2024-11-30 12:25:09.018000: I runner.py:310] Step = 48800 ; steps/s = 1.62, tokens/s = 43259 (43259 target) ; Learning rate = 0.000400 ; Loss = 1.543573\n",
      "2024-11-30 12:26:10.291000: I runner.py:310] Step = 48900 ; steps/s = 1.63, tokens/s = 42775 (42775 target) ; Learning rate = 0.000400 ; Loss = 1.541473\n",
      "2024-11-30 12:27:12.066000: I runner.py:310] Step = 49000 ; steps/s = 1.62, tokens/s = 43244 (43244 target) ; Learning rate = 0.000399 ; Loss = 1.546103\n",
      "2024-11-30 12:28:13.862000: I runner.py:310] Step = 49100 ; steps/s = 1.62, tokens/s = 43219 (43219 target) ; Learning rate = 0.000399 ; Loss = 1.547763\n",
      "2024-11-30 12:29:15.648000: I runner.py:310] Step = 49200 ; steps/s = 1.62, tokens/s = 43265 (43265 target) ; Learning rate = 0.000398 ; Loss = 1.541339\n",
      "2024-11-30 12:30:16.823000: I runner.py:310] Step = 49300 ; steps/s = 1.63, tokens/s = 42843 (42843 target) ; Learning rate = 0.000398 ; Loss = 1.545460\n",
      "2024-11-30 12:31:18.591000: I runner.py:310] Step = 49400 ; steps/s = 1.62, tokens/s = 43212 (43212 target) ; Learning rate = 0.000398 ; Loss = 1.542857\n",
      "2024-11-30 12:32:20.389000: I runner.py:310] Step = 49500 ; steps/s = 1.62, tokens/s = 43266 (43266 target) ; Learning rate = 0.000397 ; Loss = 1.544738\n",
      "2024-11-30 12:33:22.188000: I runner.py:310] Step = 49600 ; steps/s = 1.62, tokens/s = 43249 (43249 target) ; Learning rate = 0.000397 ; Loss = 1.541687\n",
      "2024-11-30 12:34:23.441000: I runner.py:310] Step = 49700 ; steps/s = 1.63, tokens/s = 42769 (42769 target) ; Learning rate = 0.000396 ; Loss = 1.549212\n",
      "2024-11-30 12:35:25.284000: I runner.py:310] Step = 49800 ; steps/s = 1.62, tokens/s = 43235 (43235 target) ; Learning rate = 0.000396 ; Loss = 1.536732\n",
      "2024-11-30 12:36:27.026000: I runner.py:310] Step = 49900 ; steps/s = 1.62, tokens/s = 43269 (43269 target) ; Learning rate = 0.000396 ; Loss = 1.542111\n",
      "2024-11-30 12:37:28.838000: I runner.py:310] Step = 50000 ; steps/s = 1.62, tokens/s = 43210 (43210 target) ; Learning rate = 0.000395 ; Loss = 1.541964\n",
      "2024-11-30 12:37:30.806000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-50000\n",
      "2024-11-30 12:37:30.806000: I training.py:192] Running evaluation for step 50000\n",
      "2024-11-30 12:41:49.929000: I training.py:192] Evaluation result for step 50000: loss = 1.165683 ; perplexity = 3.208112\n",
      "2024-11-30 12:42:51.110000: I runner.py:310] Step = 50100 ; steps/s = 1.63, tokens/s = 42822 (42822 target) ; Learning rate = 0.000395 ; Loss = 1.537949\n",
      "2024-11-30 12:43:52.907000: I runner.py:310] Step = 50200 ; steps/s = 1.62, tokens/s = 43232 (43232 target) ; Learning rate = 0.000394 ; Loss = 1.545238\n",
      "2024-11-30 12:44:54.689000: I runner.py:310] Step = 50300 ; steps/s = 1.62, tokens/s = 43232 (43232 target) ; Learning rate = 0.000394 ; Loss = 1.545233\n",
      "2024-11-30 12:45:56.573000: I runner.py:310] Step = 50400 ; steps/s = 1.62, tokens/s = 43178 (43178 target) ; Learning rate = 0.000394 ; Loss = 1.551716\n",
      "2024-11-30 12:46:57.796000: I runner.py:310] Step = 50500 ; steps/s = 1.63, tokens/s = 42840 (42840 target) ; Learning rate = 0.000393 ; Loss = 1.527280\n",
      "2024-11-30 12:47:59.639000: I runner.py:310] Step = 50600 ; steps/s = 1.62, tokens/s = 43207 (43207 target) ; Learning rate = 0.000393 ; Loss = 1.538007\n",
      "2024-11-30 12:49:01.383000: I runner.py:310] Step = 50700 ; steps/s = 1.62, tokens/s = 43253 (43253 target) ; Learning rate = 0.000393 ; Loss = 1.552393\n",
      "2024-11-30 12:50:03.206000: I runner.py:310] Step = 50800 ; steps/s = 1.62, tokens/s = 43200 (43200 target) ; Learning rate = 0.000392 ; Loss = 1.548245\n",
      "2024-11-30 12:51:04.391000: I runner.py:310] Step = 50900 ; steps/s = 1.63, tokens/s = 42828 (42828 target) ; Learning rate = 0.000392 ; Loss = 1.545145\n",
      "2024-11-30 12:52:06.162000: I runner.py:310] Step = 51000 ; steps/s = 1.62, tokens/s = 43274 (43274 target) ; Learning rate = 0.000391 ; Loss = 1.539475\n",
      "2024-11-30 12:53:07.901000: I runner.py:310] Step = 51100 ; steps/s = 1.62, tokens/s = 43306 (43306 target) ; Learning rate = 0.000391 ; Loss = 1.539420\n",
      "2024-11-30 12:54:09.739000: I runner.py:310] Step = 51200 ; steps/s = 1.62, tokens/s = 43170 (43170 target) ; Learning rate = 0.000391 ; Loss = 1.545368\n",
      "2024-11-30 12:55:10.933000: I runner.py:310] Step = 51300 ; steps/s = 1.63, tokens/s = 42832 (42832 target) ; Learning rate = 0.000390 ; Loss = 1.550767\n",
      "2024-11-30 12:56:12.759000: I runner.py:310] Step = 51400 ; steps/s = 1.62, tokens/s = 43201 (43201 target) ; Learning rate = 0.000390 ; Loss = 1.534415\n",
      "2024-11-30 12:57:14.620000: I runner.py:310] Step = 51500 ; steps/s = 1.62, tokens/s = 43224 (43224 target) ; Learning rate = 0.000389 ; Loss = 1.545913\n",
      "2024-11-30 12:58:16.470000: I runner.py:310] Step = 51600 ; steps/s = 1.62, tokens/s = 43170 (43170 target) ; Learning rate = 0.000389 ; Loss = 1.545783\n",
      "2024-11-30 12:59:17.741000: I runner.py:310] Step = 51700 ; steps/s = 1.63, tokens/s = 42797 (42797 target) ; Learning rate = 0.000389 ; Loss = 1.534357\n",
      "2024-11-30 13:00:19.514000: I runner.py:310] Step = 51800 ; steps/s = 1.62, tokens/s = 43228 (43228 target) ; Learning rate = 0.000388 ; Loss = 1.544020\n",
      "2024-11-30 13:01:21.308000: I runner.py:310] Step = 51900 ; steps/s = 1.62, tokens/s = 43237 (43237 target) ; Learning rate = 0.000388 ; Loss = 1.543875\n",
      "2024-11-30 13:02:22.979000: I runner.py:310] Step = 52000 ; steps/s = 1.62, tokens/s = 43188 (43188 target) ; Learning rate = 0.000388 ; Loss = 1.553799\n",
      "2024-11-30 13:03:24.290000: I runner.py:310] Step = 52100 ; steps/s = 1.63, tokens/s = 42853 (42853 target) ; Learning rate = 0.000387 ; Loss = 1.525954\n",
      "2024-11-30 13:04:26.144000: I runner.py:310] Step = 52200 ; steps/s = 1.62, tokens/s = 43188 (43188 target) ; Learning rate = 0.000387 ; Loss = 1.536011\n",
      "2024-11-30 13:05:27.957000: I runner.py:310] Step = 52300 ; steps/s = 1.62, tokens/s = 43239 (43239 target) ; Learning rate = 0.000386 ; Loss = 1.544042\n",
      "2024-11-30 13:06:29.218000: I runner.py:310] Step = 52400 ; steps/s = 1.63, tokens/s = 42789 (42789 target) ; Learning rate = 0.000386 ; Loss = 1.536354\n",
      "2024-11-30 13:07:31.050000: I runner.py:310] Step = 52500 ; steps/s = 1.62, tokens/s = 43230 (43230 target) ; Learning rate = 0.000386 ; Loss = 1.541934\n",
      "2024-11-30 13:08:32.820000: I runner.py:310] Step = 52600 ; steps/s = 1.62, tokens/s = 43262 (43262 target) ; Learning rate = 0.000385 ; Loss = 1.532739\n",
      "2024-11-30 13:09:34.650000: I runner.py:310] Step = 52700 ; steps/s = 1.62, tokens/s = 43210 (43210 target) ; Learning rate = 0.000385 ; Loss = 1.544340\n",
      "2024-11-30 13:10:35.873000: I runner.py:310] Step = 52800 ; steps/s = 1.63, tokens/s = 42794 (42794 target) ; Learning rate = 0.000385 ; Loss = 1.538037\n",
      "2024-11-30 13:11:37.653000: I runner.py:310] Step = 52900 ; steps/s = 1.62, tokens/s = 43238 (43238 target) ; Learning rate = 0.000384 ; Loss = 1.542003\n",
      "2024-11-30 13:12:39.471000: I runner.py:310] Step = 53000 ; steps/s = 1.62, tokens/s = 43204 (43204 target) ; Learning rate = 0.000384 ; Loss = 1.539166\n",
      "2024-11-30 13:13:41.242000: I runner.py:310] Step = 53100 ; steps/s = 1.62, tokens/s = 43277 (43277 target) ; Learning rate = 0.000384 ; Loss = 1.538121\n",
      "2024-11-30 13:14:42.465000: I runner.py:310] Step = 53200 ; steps/s = 1.63, tokens/s = 42788 (42788 target) ; Learning rate = 0.000383 ; Loss = 1.530279\n",
      "2024-11-30 13:15:44.260000: I runner.py:310] Step = 53300 ; steps/s = 1.62, tokens/s = 43236 (43236 target) ; Learning rate = 0.000383 ; Loss = 1.538644\n",
      "2024-11-30 13:16:46.028000: I runner.py:310] Step = 53400 ; steps/s = 1.62, tokens/s = 43262 (43262 target) ; Learning rate = 0.000382 ; Loss = 1.540990\n",
      "2024-11-30 13:17:47.841000: I runner.py:310] Step = 53500 ; steps/s = 1.62, tokens/s = 43219 (43219 target) ; Learning rate = 0.000382 ; Loss = 1.547080\n",
      "2024-11-30 13:18:49.039000: I runner.py:310] Step = 53600 ; steps/s = 1.63, tokens/s = 42819 (42819 target) ; Learning rate = 0.000382 ; Loss = 1.531033\n",
      "2024-11-30 13:19:50.852000: I runner.py:310] Step = 53700 ; steps/s = 1.62, tokens/s = 43234 (43234 target) ; Learning rate = 0.000381 ; Loss = 1.530078\n",
      "2024-11-30 13:20:52.645000: I runner.py:310] Step = 53800 ; steps/s = 1.62, tokens/s = 43240 (43240 target) ; Learning rate = 0.000381 ; Loss = 1.539625\n",
      "2024-11-30 13:21:54.425000: I runner.py:310] Step = 53900 ; steps/s = 1.62, tokens/s = 43261 (43261 target) ; Learning rate = 0.000381 ; Loss = 1.541488\n",
      "2024-11-30 13:22:55.597000: I runner.py:310] Step = 54000 ; steps/s = 1.63, tokens/s = 42812 (42812 target) ; Learning rate = 0.000380 ; Loss = 1.537877\n",
      "2024-11-30 13:23:57.384000: I runner.py:310] Step = 54100 ; steps/s = 1.62, tokens/s = 43243 (43243 target) ; Learning rate = 0.000380 ; Loss = 1.525357\n",
      "2024-11-30 13:24:59.164000: I runner.py:310] Step = 54200 ; steps/s = 1.62, tokens/s = 43240 (43240 target) ; Learning rate = 0.000380 ; Loss = 1.543197\n",
      "2024-11-30 13:26:01.066000: I runner.py:310] Step = 54300 ; steps/s = 1.62, tokens/s = 43163 (43163 target) ; Learning rate = 0.000379 ; Loss = 1.543655\n",
      "2024-11-30 13:27:02.316000: I runner.py:310] Step = 54400 ; steps/s = 1.63, tokens/s = 42781 (42781 target) ; Learning rate = 0.000379 ; Loss = 1.542057\n",
      "2024-11-30 13:28:04.172000: I runner.py:310] Step = 54500 ; steps/s = 1.62, tokens/s = 43199 (43199 target) ; Learning rate = 0.000379 ; Loss = 1.532285\n",
      "2024-11-30 13:29:05.963000: I runner.py:310] Step = 54600 ; steps/s = 1.62, tokens/s = 43253 (43253 target) ; Learning rate = 0.000378 ; Loss = 1.534750\n",
      "2024-11-30 13:30:07.718000: I runner.py:310] Step = 54700 ; steps/s = 1.62, tokens/s = 43256 (43256 target) ; Learning rate = 0.000378 ; Loss = 1.529424\n",
      "2024-11-30 13:31:08.950000: I runner.py:310] Step = 54800 ; steps/s = 1.63, tokens/s = 42781 (42781 target) ; Learning rate = 0.000378 ; Loss = 1.524394\n",
      "2024-11-30 13:32:10.729000: I runner.py:310] Step = 54900 ; steps/s = 1.62, tokens/s = 43239 (43239 target) ; Learning rate = 0.000377 ; Loss = 1.535542\n",
      "2024-11-30 13:33:12.517000: I runner.py:310] Step = 55000 ; steps/s = 1.62, tokens/s = 43238 (43238 target) ; Learning rate = 0.000377 ; Loss = 1.539208\n",
      "2024-11-30 13:33:12.518000: I training.py:192] Running evaluation for step 55000\n",
      "2024-11-30 13:37:24.277000: I training.py:192] Evaluation result for step 55000: loss = 1.171964 ; perplexity = 3.228327\n",
      "2024-11-30 13:38:25.959000: I runner.py:310] Step = 55100 ; steps/s = 1.62, tokens/s = 43321 (43321 target) ; Learning rate = 0.000377 ; Loss = 1.535912\n",
      "2024-11-30 13:39:27.175000: I runner.py:310] Step = 55200 ; steps/s = 1.63, tokens/s = 42861 (42861 target) ; Learning rate = 0.000376 ; Loss = 1.539814\n",
      "2024-11-30 13:40:29.073000: I runner.py:310] Step = 55300 ; steps/s = 1.62, tokens/s = 43146 (43146 target) ; Learning rate = 0.000376 ; Loss = 1.525991\n",
      "2024-11-30 13:41:30.867000: I runner.py:310] Step = 55400 ; steps/s = 1.62, tokens/s = 43245 (43245 target) ; Learning rate = 0.000376 ; Loss = 1.538910\n",
      "2024-11-30 13:42:32.668000: I runner.py:310] Step = 55500 ; steps/s = 1.62, tokens/s = 43199 (43199 target) ; Learning rate = 0.000375 ; Loss = 1.545315\n",
      "2024-11-30 13:43:33.839000: I runner.py:310] Step = 55600 ; steps/s = 1.63, tokens/s = 42885 (42885 target) ; Learning rate = 0.000375 ; Loss = 1.531894\n",
      "2024-11-30 13:44:35.636000: I runner.py:310] Step = 55700 ; steps/s = 1.62, tokens/s = 43221 (43221 target) ; Learning rate = 0.000375 ; Loss = 1.536319\n",
      "2024-11-30 13:45:37.505000: I runner.py:310] Step = 55800 ; steps/s = 1.62, tokens/s = 43189 (43189 target) ; Learning rate = 0.000374 ; Loss = 1.535862\n",
      "2024-11-30 13:46:39.292000: I runner.py:310] Step = 55900 ; steps/s = 1.62, tokens/s = 43222 (43222 target) ; Learning rate = 0.000374 ; Loss = 1.541162\n",
      "2024-11-30 13:47:40.542000: I runner.py:310] Step = 56000 ; steps/s = 1.63, tokens/s = 42793 (42793 target) ; Learning rate = 0.000374 ; Loss = 1.539869\n",
      "2024-11-30 13:48:42.330000: I runner.py:310] Step = 56100 ; steps/s = 1.62, tokens/s = 43234 (43234 target) ; Learning rate = 0.000373 ; Loss = 1.535288\n",
      "2024-11-30 13:49:44.145000: I runner.py:310] Step = 56200 ; steps/s = 1.62, tokens/s = 43205 (43205 target) ; Learning rate = 0.000373 ; Loss = 1.524520\n",
      "2024-11-30 13:50:45.976000: I runner.py:310] Step = 56300 ; steps/s = 1.62, tokens/s = 43206 (43206 target) ; Learning rate = 0.000373 ; Loss = 1.535144\n",
      "2024-11-30 13:51:47.239000: I runner.py:310] Step = 56400 ; steps/s = 1.63, tokens/s = 42797 (42797 target) ; Learning rate = 0.000372 ; Loss = 1.533565\n",
      "2024-11-30 13:52:49.001000: I runner.py:310] Step = 56500 ; steps/s = 1.62, tokens/s = 43273 (43273 target) ; Learning rate = 0.000372 ; Loss = 1.525505\n",
      "2024-11-30 13:53:50.830000: I runner.py:310] Step = 56600 ; steps/s = 1.62, tokens/s = 43179 (43179 target) ; Learning rate = 0.000372 ; Loss = 1.531226\n",
      "2024-11-30 13:54:52.079000: I runner.py:310] Step = 56700 ; steps/s = 1.63, tokens/s = 42794 (42794 target) ; Learning rate = 0.000371 ; Loss = 1.532268\n",
      "2024-11-30 13:55:53.873000: I runner.py:310] Step = 56800 ; steps/s = 1.62, tokens/s = 43232 (43232 target) ; Learning rate = 0.000371 ; Loss = 1.527894\n",
      "2024-11-30 13:56:55.693000: I runner.py:310] Step = 56900 ; steps/s = 1.62, tokens/s = 43227 (43227 target) ; Learning rate = 0.000371 ; Loss = 1.537601\n",
      "2024-11-30 13:57:57.521000: I runner.py:310] Step = 57000 ; steps/s = 1.62, tokens/s = 43235 (43235 target) ; Learning rate = 0.000370 ; Loss = 1.549974\n",
      "2024-11-30 13:58:58.733000: I runner.py:310] Step = 57100 ; steps/s = 1.63, tokens/s = 42791 (42791 target) ; Learning rate = 0.000370 ; Loss = 1.534749\n",
      "2024-11-30 14:00:00.572000: I runner.py:310] Step = 57200 ; steps/s = 1.62, tokens/s = 43185 (43185 target) ; Learning rate = 0.000370 ; Loss = 1.535498\n",
      "2024-11-30 14:01:02.343000: I runner.py:310] Step = 57300 ; steps/s = 1.62, tokens/s = 43278 (43278 target) ; Learning rate = 0.000369 ; Loss = 1.528263\n",
      "2024-11-30 14:02:04.123000: I runner.py:310] Step = 57400 ; steps/s = 1.62, tokens/s = 43236 (43236 target) ; Learning rate = 0.000369 ; Loss = 1.526302\n",
      "2024-11-30 14:03:05.279000: I runner.py:310] Step = 57500 ; steps/s = 1.64, tokens/s = 42860 (42860 target) ; Learning rate = 0.000369 ; Loss = 1.533539\n",
      "2024-11-30 14:04:07.190000: I runner.py:310] Step = 57600 ; steps/s = 1.62, tokens/s = 43165 (43165 target) ; Learning rate = 0.000368 ; Loss = 1.534854\n",
      "2024-11-30 14:05:08.938000: I runner.py:310] Step = 57700 ; steps/s = 1.62, tokens/s = 43272 (43272 target) ; Learning rate = 0.000368 ; Loss = 1.526895\n",
      "2024-11-30 14:06:10.743000: I runner.py:310] Step = 57800 ; steps/s = 1.62, tokens/s = 43216 (43216 target) ; Learning rate = 0.000368 ; Loss = 1.530306\n",
      "2024-11-30 14:07:11.916000: I runner.py:310] Step = 57900 ; steps/s = 1.63, tokens/s = 42841 (42841 target) ; Learning rate = 0.000367 ; Loss = 1.531741\n",
      "2024-11-30 14:08:13.675000: I runner.py:310] Step = 58000 ; steps/s = 1.62, tokens/s = 43269 (43269 target) ; Learning rate = 0.000367 ; Loss = 1.523535\n",
      "2024-11-30 14:09:15.451000: I runner.py:310] Step = 58100 ; steps/s = 1.62, tokens/s = 43244 (43244 target) ; Learning rate = 0.000367 ; Loss = 1.525608\n",
      "2024-11-30 14:10:17.302000: I runner.py:310] Step = 58200 ; steps/s = 1.62, tokens/s = 43194 (43194 target) ; Learning rate = 0.000366 ; Loss = 1.538708\n",
      "2024-11-30 14:11:18.480000: I runner.py:310] Step = 58300 ; steps/s = 1.63, tokens/s = 42844 (42844 target) ; Learning rate = 0.000366 ; Loss = 1.537278\n",
      "2024-11-30 14:12:20.230000: I runner.py:310] Step = 58400 ; steps/s = 1.62, tokens/s = 43240 (43240 target) ; Learning rate = 0.000366 ; Loss = 1.525242\n",
      "2024-11-30 14:13:22.459000: I runner.py:310] Step = 58500 ; steps/s = 1.61, tokens/s = 42930 (42930 target) ; Learning rate = 0.000365 ; Loss = 1.535985\n",
      "2024-11-30 14:14:24.313000: I runner.py:310] Step = 58600 ; steps/s = 1.62, tokens/s = 43184 (43184 target) ; Learning rate = 0.000365 ; Loss = 1.535847\n",
      "2024-11-30 14:15:25.579000: I runner.py:310] Step = 58700 ; steps/s = 1.63, tokens/s = 42805 (42805 target) ; Learning rate = 0.000365 ; Loss = 1.537387\n",
      "2024-11-30 14:16:27.372000: I runner.py:310] Step = 58800 ; steps/s = 1.62, tokens/s = 43233 (43233 target) ; Learning rate = 0.000365 ; Loss = 1.527837\n",
      "2024-11-30 14:17:29.169000: I runner.py:310] Step = 58900 ; steps/s = 1.62, tokens/s = 43222 (43222 target) ; Learning rate = 0.000364 ; Loss = 1.524879\n",
      "2024-11-30 14:18:31.004000: I runner.py:310] Step = 59000 ; steps/s = 1.62, tokens/s = 43206 (43206 target) ; Learning rate = 0.000364 ; Loss = 1.534327\n",
      "2024-11-30 14:19:32.283000: I runner.py:310] Step = 59100 ; steps/s = 1.63, tokens/s = 42787 (42787 target) ; Learning rate = 0.000364 ; Loss = 1.525901\n",
      "2024-11-30 14:20:34.039000: I runner.py:310] Step = 59200 ; steps/s = 1.62, tokens/s = 43242 (43242 target) ; Learning rate = 0.000363 ; Loss = 1.526181\n",
      "2024-11-30 14:21:35.870000: I runner.py:310] Step = 59300 ; steps/s = 1.62, tokens/s = 43231 (43231 target) ; Learning rate = 0.000363 ; Loss = 1.524079\n",
      "2024-11-30 14:22:37.609000: I runner.py:310] Step = 59400 ; steps/s = 1.62, tokens/s = 43258 (43258 target) ; Learning rate = 0.000363 ; Loss = 1.532323\n",
      "2024-11-30 14:23:38.901000: I runner.py:310] Step = 59500 ; steps/s = 1.63, tokens/s = 42765 (42765 target) ; Learning rate = 0.000362 ; Loss = 1.515606\n",
      "2024-11-30 14:24:40.679000: I runner.py:310] Step = 59600 ; steps/s = 1.62, tokens/s = 43259 (43259 target) ; Learning rate = 0.000362 ; Loss = 1.525288\n",
      "2024-11-30 14:25:42.501000: I runner.py:310] Step = 59700 ; steps/s = 1.62, tokens/s = 43164 (43164 target) ; Learning rate = 0.000362 ; Loss = 1.527615\n",
      "2024-11-30 14:26:44.375000: I runner.py:310] Step = 59800 ; steps/s = 1.62, tokens/s = 43203 (43203 target) ; Learning rate = 0.000361 ; Loss = 1.538132\n",
      "2024-11-30 14:27:45.588000: I runner.py:310] Step = 59900 ; steps/s = 1.63, tokens/s = 42841 (42841 target) ; Learning rate = 0.000361 ; Loss = 1.530082\n",
      "2024-11-30 14:28:47.340000: I runner.py:310] Step = 60000 ; steps/s = 1.62, tokens/s = 43254 (43254 target) ; Learning rate = 0.000361 ; Loss = 1.525156\n",
      "2024-11-30 14:28:49.913000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-60000\n",
      "2024-11-30 14:28:49.913000: I training.py:192] Running evaluation for step 60000\n",
      "2024-11-30 14:33:18.595000: I training.py:192] Evaluation result for step 60000: loss = 1.180895 ; perplexity = 3.257288\n",
      "2024-11-30 14:34:20.265000: I runner.py:310] Step = 60100 ; steps/s = 1.62, tokens/s = 43342 (43342 target) ; Learning rate = 0.000361 ; Loss = 1.526158\n",
      "2024-11-30 14:35:22.055000: I runner.py:310] Step = 60200 ; steps/s = 1.62, tokens/s = 43223 (43223 target) ; Learning rate = 0.000360 ; Loss = 1.527416\n",
      "2024-11-30 14:36:23.280000: I runner.py:310] Step = 60300 ; steps/s = 1.63, tokens/s = 42798 (42798 target) ; Learning rate = 0.000360 ; Loss = 1.518421\n",
      "2024-11-30 14:37:25.131000: I runner.py:310] Step = 60400 ; steps/s = 1.62, tokens/s = 43200 (43200 target) ; Learning rate = 0.000360 ; Loss = 1.527374\n",
      "2024-11-30 14:38:26.917000: I runner.py:310] Step = 60500 ; steps/s = 1.62, tokens/s = 43238 (43238 target) ; Learning rate = 0.000359 ; Loss = 1.525944\n",
      "2024-11-30 14:39:28.708000: I runner.py:310] Step = 60600 ; steps/s = 1.62, tokens/s = 43237 (43237 target) ; Learning rate = 0.000359 ; Loss = 1.539421\n",
      "2024-11-30 14:40:30.015000: I runner.py:310] Step = 60700 ; steps/s = 1.63, tokens/s = 42772 (42772 target) ; Learning rate = 0.000359 ; Loss = 1.522208\n",
      "2024-11-30 14:41:31.780000: I runner.py:310] Step = 60800 ; steps/s = 1.62, tokens/s = 43224 (43224 target) ; Learning rate = 0.000358 ; Loss = 1.523307\n",
      "2024-11-30 14:42:33.600000: I runner.py:310] Step = 60900 ; steps/s = 1.62, tokens/s = 43236 (43236 target) ; Learning rate = 0.000358 ; Loss = 1.524515\n",
      "2024-11-30 14:43:34.810000: I runner.py:310] Step = 61000 ; steps/s = 1.63, tokens/s = 42791 (42791 target) ; Learning rate = 0.000358 ; Loss = 1.528734\n",
      "2024-11-30 14:44:36.612000: I runner.py:310] Step = 61100 ; steps/s = 1.62, tokens/s = 43231 (43231 target) ; Learning rate = 0.000358 ; Loss = 1.530438\n",
      "2024-11-30 14:45:38.402000: I runner.py:310] Step = 61200 ; steps/s = 1.62, tokens/s = 43238 (43238 target) ; Learning rate = 0.000357 ; Loss = 1.524069\n",
      "2024-11-30 14:46:40.209000: I runner.py:310] Step = 61300 ; steps/s = 1.62, tokens/s = 43221 (43221 target) ; Learning rate = 0.000357 ; Loss = 1.529549\n",
      "2024-11-30 14:47:41.440000: I runner.py:310] Step = 61400 ; steps/s = 1.63, tokens/s = 42821 (42821 target) ; Learning rate = 0.000357 ; Loss = 1.518470\n",
      "2024-11-30 14:48:43.287000: I runner.py:310] Step = 61500 ; steps/s = 1.62, tokens/s = 43193 (43193 target) ; Learning rate = 0.000356 ; Loss = 1.520323\n",
      "2024-11-30 14:49:45.057000: I runner.py:310] Step = 61600 ; steps/s = 1.62, tokens/s = 43243 (43243 target) ; Learning rate = 0.000356 ; Loss = 1.528600\n",
      "2024-11-30 14:50:46.909000: I runner.py:310] Step = 61700 ; steps/s = 1.62, tokens/s = 43209 (43209 target) ; Learning rate = 0.000356 ; Loss = 1.527933\n",
      "2024-11-30 14:51:48.219000: I runner.py:310] Step = 61800 ; steps/s = 1.63, tokens/s = 42757 (42757 target) ; Learning rate = 0.000356 ; Loss = 1.531950\n",
      "2024-11-30 14:52:50.039000: I runner.py:310] Step = 61900 ; steps/s = 1.62, tokens/s = 43226 (43226 target) ; Learning rate = 0.000355 ; Loss = 1.521030\n",
      "2024-11-30 14:53:51.775000: I runner.py:310] Step = 62000 ; steps/s = 1.62, tokens/s = 43275 (43275 target) ; Learning rate = 0.000355 ; Loss = 1.522379\n",
      "2024-11-30 14:54:53.605000: I runner.py:310] Step = 62100 ; steps/s = 1.62, tokens/s = 43194 (43194 target) ; Learning rate = 0.000355 ; Loss = 1.526844\n",
      "2024-11-30 14:55:54.856000: I runner.py:310] Step = 62200 ; steps/s = 1.63, tokens/s = 42804 (42804 target) ; Learning rate = 0.000354 ; Loss = 1.527566\n",
      "2024-11-30 14:56:56.735000: I runner.py:310] Step = 62300 ; steps/s = 1.62, tokens/s = 43202 (43202 target) ; Learning rate = 0.000354 ; Loss = 1.524845\n",
      "2024-11-30 14:57:58.536000: I runner.py:310] Step = 62400 ; steps/s = 1.62, tokens/s = 43221 (43221 target) ; Learning rate = 0.000354 ; Loss = 1.522212\n",
      "2024-11-30 14:59:00.379000: I runner.py:310] Step = 62500 ; steps/s = 1.62, tokens/s = 43201 (43201 target) ; Learning rate = 0.000354 ; Loss = 1.527831\n",
      "2024-11-30 15:00:01.589000: I runner.py:310] Step = 62600 ; steps/s = 1.63, tokens/s = 42789 (42789 target) ; Learning rate = 0.000353 ; Loss = 1.515515\n",
      "2024-11-30 15:01:03.400000: I runner.py:310] Step = 62700 ; steps/s = 1.62, tokens/s = 43242 (43242 target) ; Learning rate = 0.000353 ; Loss = 1.525026\n",
      "2024-11-30 15:02:05.220000: I runner.py:310] Step = 62800 ; steps/s = 1.62, tokens/s = 43197 (43197 target) ; Learning rate = 0.000353 ; Loss = 1.537316\n",
      "2024-11-30 15:03:07.016000: I runner.py:310] Step = 62900 ; steps/s = 1.62, tokens/s = 43221 (43221 target) ; Learning rate = 0.000352 ; Loss = 1.523642\n",
      "2024-11-30 15:04:08.352000: I runner.py:310] Step = 63000 ; steps/s = 1.63, tokens/s = 42715 (42715 target) ; Learning rate = 0.000352 ; Loss = 1.530266\n",
      "2024-11-30 15:05:10.209000: I runner.py:310] Step = 63100 ; steps/s = 1.62, tokens/s = 43214 (43214 target) ; Learning rate = 0.000352 ; Loss = 1.523657\n",
      "2024-11-30 15:06:12.007000: I runner.py:310] Step = 63200 ; steps/s = 1.62, tokens/s = 43222 (43222 target) ; Learning rate = 0.000352 ; Loss = 1.520365\n",
      "2024-11-30 15:07:13.878000: I runner.py:310] Step = 63300 ; steps/s = 1.62, tokens/s = 43188 (43188 target) ; Learning rate = 0.000351 ; Loss = 1.531628\n",
      "2024-11-30 15:08:15.109000: I runner.py:310] Step = 63400 ; steps/s = 1.63, tokens/s = 42839 (42839 target) ; Learning rate = 0.000351 ; Loss = 1.508930\n",
      "2024-11-30 15:09:16.872000: I runner.py:310] Step = 63500 ; steps/s = 1.62, tokens/s = 43208 (43208 target) ; Learning rate = 0.000351 ; Loss = 1.529866\n",
      "2024-11-30 15:10:18.683000: I runner.py:310] Step = 63600 ; steps/s = 1.62, tokens/s = 43206 (43206 target) ; Learning rate = 0.000350 ; Loss = 1.525527\n",
      "2024-11-30 15:11:20.447000: I runner.py:310] Step = 63700 ; steps/s = 1.62, tokens/s = 43298 (43298 target) ; Learning rate = 0.000350 ; Loss = 1.528489\n",
      "2024-11-30 15:12:21.713000: I runner.py:310] Step = 63800 ; steps/s = 1.63, tokens/s = 42763 (42763 target) ; Learning rate = 0.000350 ; Loss = 1.528506\n",
      "2024-11-30 15:13:23.544000: I runner.py:310] Step = 63900 ; steps/s = 1.62, tokens/s = 43215 (43215 target) ; Learning rate = 0.000350 ; Loss = 1.516937\n",
      "2024-11-30 15:14:25.329000: I runner.py:310] Step = 64000 ; steps/s = 1.62, tokens/s = 43248 (43248 target) ; Learning rate = 0.000349 ; Loss = 1.532725\n",
      "2024-11-30 15:15:27.113000: I runner.py:310] Step = 64100 ; steps/s = 1.62, tokens/s = 43240 (43240 target) ; Learning rate = 0.000349 ; Loss = 1.527246\n",
      "2024-11-30 15:16:28.333000: I runner.py:310] Step = 64200 ; steps/s = 1.63, tokens/s = 42784 (42784 target) ; Learning rate = 0.000349 ; Loss = 1.519122\n",
      "2024-11-30 15:17:30.169000: I runner.py:310] Step = 64300 ; steps/s = 1.62, tokens/s = 43207 (43207 target) ; Learning rate = 0.000349 ; Loss = 1.524365\n",
      "2024-11-30 15:18:31.989000: I runner.py:310] Step = 64400 ; steps/s = 1.62, tokens/s = 43219 (43219 target) ; Learning rate = 0.000348 ; Loss = 1.525039\n",
      "2024-11-30 15:19:33.751000: I runner.py:310] Step = 64500 ; steps/s = 1.62, tokens/s = 43253 (43253 target) ; Learning rate = 0.000348 ; Loss = 1.532574\n",
      "2024-11-30 15:20:34.975000: I runner.py:310] Step = 64600 ; steps/s = 1.63, tokens/s = 42834 (42834 target) ; Learning rate = 0.000348 ; Loss = 1.514037\n",
      "2024-11-30 15:21:36.795000: I runner.py:310] Step = 64700 ; steps/s = 1.62, tokens/s = 43227 (43227 target) ; Learning rate = 0.000347 ; Loss = 1.517298\n",
      "2024-11-30 15:22:38.664000: I runner.py:310] Step = 64800 ; steps/s = 1.62, tokens/s = 43175 (43175 target) ; Learning rate = 0.000347 ; Loss = 1.530876\n",
      "2024-11-30 15:23:40.465000: I runner.py:310] Step = 64900 ; steps/s = 1.62, tokens/s = 43208 (43208 target) ; Learning rate = 0.000347 ; Loss = 1.534509\n",
      "2024-11-30 15:24:41.735000: I runner.py:310] Step = 65000 ; steps/s = 1.63, tokens/s = 42778 (42778 target) ; Learning rate = 0.000347 ; Loss = 1.515437\n",
      "2024-11-30 15:24:41.736000: I training.py:192] Running evaluation for step 65000\n",
      "2024-11-30 15:28:55.703000: I training.py:192] Evaluation result for step 65000: loss = 1.190370 ; perplexity = 3.288298\n",
      "2024-11-30 15:29:57.406000: I runner.py:310] Step = 65100 ; steps/s = 1.62, tokens/s = 43311 (43311 target) ; Learning rate = 0.000346 ; Loss = 1.518459\n",
      "2024-11-30 15:30:59.139000: I runner.py:310] Step = 65200 ; steps/s = 1.62, tokens/s = 43251 (43251 target) ; Learning rate = 0.000346 ; Loss = 1.534614\n",
      "2024-11-30 15:32:00.409000: I runner.py:310] Step = 65300 ; steps/s = 1.63, tokens/s = 42783 (42783 target) ; Learning rate = 0.000346 ; Loss = 1.523976\n",
      "2024-11-30 15:33:02.133000: I runner.py:310] Step = 65400 ; steps/s = 1.62, tokens/s = 43305 (43305 target) ; Learning rate = 0.000346 ; Loss = 1.522334\n",
      "2024-11-30 15:34:03.940000: I runner.py:310] Step = 65500 ; steps/s = 1.62, tokens/s = 43251 (43251 target) ; Learning rate = 0.000345 ; Loss = 1.515387\n",
      "2024-11-30 15:35:05.736000: I runner.py:310] Step = 65600 ; steps/s = 1.62, tokens/s = 43233 (43233 target) ; Learning rate = 0.000345 ; Loss = 1.522292\n",
      "2024-11-30 15:36:07.023000: I runner.py:310] Step = 65700 ; steps/s = 1.63, tokens/s = 42726 (42726 target) ; Learning rate = 0.000345 ; Loss = 1.517733\n",
      "2024-11-30 15:37:08.754000: I runner.py:310] Step = 65800 ; steps/s = 1.62, tokens/s = 43282 (43282 target) ; Learning rate = 0.000345 ; Loss = 1.514603\n",
      "2024-11-30 15:38:10.473000: I runner.py:310] Step = 65900 ; steps/s = 1.62, tokens/s = 43292 (43292 target) ; Learning rate = 0.000344 ; Loss = 1.518560\n",
      "2024-11-30 15:39:12.269000: I runner.py:310] Step = 66000 ; steps/s = 1.62, tokens/s = 43242 (43242 target) ; Learning rate = 0.000344 ; Loss = 1.532864\n",
      "2024-11-30 15:40:13.447000: I runner.py:310] Step = 66100 ; steps/s = 1.63, tokens/s = 42843 (42843 target) ; Learning rate = 0.000344 ; Loss = 1.526034\n",
      "2024-11-30 15:41:15.235000: I runner.py:310] Step = 66200 ; steps/s = 1.62, tokens/s = 43249 (43249 target) ; Learning rate = 0.000344 ; Loss = 1.515232\n",
      "2024-11-30 15:42:17.002000: I runner.py:310] Step = 66300 ; steps/s = 1.62, tokens/s = 43243 (43243 target) ; Learning rate = 0.000343 ; Loss = 1.514728\n",
      "2024-11-30 15:43:18.714000: I runner.py:310] Step = 66400 ; steps/s = 1.62, tokens/s = 43282 (43282 target) ; Learning rate = 0.000343 ; Loss = 1.524847\n",
      "2024-11-30 15:44:19.923000: I runner.py:310] Step = 66500 ; steps/s = 1.63, tokens/s = 42816 (42816 target) ; Learning rate = 0.000343 ; Loss = 1.516063\n",
      "2024-11-30 15:45:21.733000: I runner.py:310] Step = 66600 ; steps/s = 1.62, tokens/s = 43211 (43211 target) ; Learning rate = 0.000342 ; Loss = 1.521617\n",
      "2024-11-30 15:46:23.495000: I runner.py:310] Step = 66700 ; steps/s = 1.62, tokens/s = 43256 (43256 target) ; Learning rate = 0.000342 ; Loss = 1.519055\n",
      "2024-11-30 15:47:25.305000: I runner.py:310] Step = 66800 ; steps/s = 1.62, tokens/s = 43225 (43225 target) ; Learning rate = 0.000342 ; Loss = 1.524072\n",
      "2024-11-30 15:48:26.474000: I runner.py:310] Step = 66900 ; steps/s = 1.63, tokens/s = 42867 (42867 target) ; Learning rate = 0.000342 ; Loss = 1.526708\n",
      "2024-11-30 15:49:28.251000: I runner.py:310] Step = 67000 ; steps/s = 1.62, tokens/s = 43243 (43243 target) ; Learning rate = 0.000341 ; Loss = 1.520645\n",
      "2024-11-30 15:50:30.060000: I runner.py:310] Step = 67100 ; steps/s = 1.62, tokens/s = 43225 (43225 target) ; Learning rate = 0.000341 ; Loss = 1.517331\n",
      "2024-11-30 15:51:31.841000: I runner.py:310] Step = 67200 ; steps/s = 1.62, tokens/s = 43239 (43239 target) ; Learning rate = 0.000341 ; Loss = 1.511545\n",
      "2024-11-30 15:52:33.076000: I runner.py:310] Step = 67300 ; steps/s = 1.63, tokens/s = 42806 (42806 target) ; Learning rate = 0.000341 ; Loss = 1.528506\n",
      "2024-11-30 15:53:34.840000: I runner.py:310] Step = 67400 ; steps/s = 1.62, tokens/s = 43270 (43270 target) ; Learning rate = 0.000340 ; Loss = 1.513761\n",
      "2024-11-30 15:54:36.656000: I runner.py:310] Step = 67500 ; steps/s = 1.62, tokens/s = 43214 (43214 target) ; Learning rate = 0.000340 ; Loss = 1.520784\n",
      "2024-11-30 15:55:38.439000: I runner.py:310] Step = 67600 ; steps/s = 1.62, tokens/s = 43207 (43207 target) ; Learning rate = 0.000340 ; Loss = 1.518317\n",
      "2024-11-30 15:56:39.676000: I runner.py:310] Step = 67700 ; steps/s = 1.63, tokens/s = 42825 (42825 target) ; Learning rate = 0.000340 ; Loss = 1.514983\n",
      "2024-11-30 15:57:41.467000: I runner.py:310] Step = 67800 ; steps/s = 1.62, tokens/s = 43244 (43244 target) ; Learning rate = 0.000339 ; Loss = 1.526501\n",
      "2024-11-30 15:58:43.260000: I runner.py:310] Step = 67900 ; steps/s = 1.62, tokens/s = 43228 (43228 target) ; Learning rate = 0.000339 ; Loss = 1.519319\n",
      "2024-11-30 15:59:44.965000: I runner.py:310] Step = 68000 ; steps/s = 1.62, tokens/s = 43287 (43287 target) ; Learning rate = 0.000339 ; Loss = 1.518594\n",
      "2024-11-30 16:00:46.153000: I runner.py:310] Step = 68100 ; steps/s = 1.63, tokens/s = 42853 (42853 target) ; Learning rate = 0.000339 ; Loss = 1.520044\n",
      "2024-11-30 16:01:47.917000: I runner.py:310] Step = 68200 ; steps/s = 1.62, tokens/s = 43247 (43247 target) ; Learning rate = 0.000338 ; Loss = 1.511346\n",
      "2024-11-30 16:02:49.609000: I runner.py:310] Step = 68300 ; steps/s = 1.62, tokens/s = 43304 (43304 target) ; Learning rate = 0.000338 ; Loss = 1.513800\n",
      "2024-11-30 16:03:51.417000: I runner.py:310] Step = 68400 ; steps/s = 1.62, tokens/s = 43224 (43224 target) ; Learning rate = 0.000338 ; Loss = 1.518107\n",
      "2024-11-30 16:04:52.665000: I runner.py:310] Step = 68500 ; steps/s = 1.63, tokens/s = 42784 (42784 target) ; Learning rate = 0.000338 ; Loss = 1.521337\n",
      "2024-11-30 16:05:54.447000: I runner.py:310] Step = 68600 ; steps/s = 1.62, tokens/s = 43242 (43242 target) ; Learning rate = 0.000337 ; Loss = 1.513809\n",
      "2024-11-30 16:06:56.240000: I runner.py:310] Step = 68700 ; steps/s = 1.62, tokens/s = 43243 (43243 target) ; Learning rate = 0.000337 ; Loss = 1.513868\n",
      "2024-11-30 16:07:58.030000: I runner.py:310] Step = 68800 ; steps/s = 1.62, tokens/s = 43241 (43241 target) ; Learning rate = 0.000337 ; Loss = 1.517087\n",
      "2024-11-30 16:08:59.245000: I runner.py:310] Step = 68900 ; steps/s = 1.63, tokens/s = 42825 (42825 target) ; Learning rate = 0.000337 ; Loss = 1.508006\n",
      "2024-11-30 16:10:01.058000: I runner.py:310] Step = 69000 ; steps/s = 1.62, tokens/s = 43236 (43236 target) ; Learning rate = 0.000336 ; Loss = 1.521552\n",
      "2024-11-30 16:11:02.899000: I runner.py:310] Step = 69100 ; steps/s = 1.62, tokens/s = 43159 (43159 target) ; Learning rate = 0.000336 ; Loss = 1.524732\n",
      "2024-11-30 16:12:04.613000: I runner.py:310] Step = 69200 ; steps/s = 1.62, tokens/s = 43295 (43295 target) ; Learning rate = 0.000336 ; Loss = 1.524424\n",
      "2024-11-30 16:13:05.800000: I runner.py:310] Step = 69300 ; steps/s = 1.63, tokens/s = 42854 (42854 target) ; Learning rate = 0.000336 ; Loss = 1.510890\n",
      "2024-11-30 16:14:07.631000: I runner.py:310] Step = 69400 ; steps/s = 1.62, tokens/s = 43188 (43188 target) ; Learning rate = 0.000336 ; Loss = 1.520020\n",
      "2024-11-30 16:15:09.433000: I runner.py:310] Step = 69500 ; steps/s = 1.62, tokens/s = 43223 (43223 target) ; Learning rate = 0.000335 ; Loss = 1.524387\n",
      "2024-11-30 16:16:10.630000: I runner.py:310] Step = 69600 ; steps/s = 1.63, tokens/s = 42816 (42816 target) ; Learning rate = 0.000335 ; Loss = 1.514498\n",
      "2024-11-30 16:17:12.387000: I runner.py:310] Step = 69700 ; steps/s = 1.62, tokens/s = 43289 (43289 target) ; Learning rate = 0.000335 ; Loss = 1.512684\n",
      "2024-11-30 16:18:14.171000: I runner.py:310] Step = 69800 ; steps/s = 1.62, tokens/s = 43236 (43236 target) ; Learning rate = 0.000335 ; Loss = 1.515193\n",
      "2024-11-30 16:19:15.932000: I runner.py:310] Step = 69900 ; steps/s = 1.62, tokens/s = 43246 (43246 target) ; Learning rate = 0.000334 ; Loss = 1.519775\n",
      "2024-11-30 16:20:17.160000: I runner.py:310] Step = 70000 ; steps/s = 1.63, tokens/s = 42803 (42803 target) ; Learning rate = 0.000334 ; Loss = 1.515374\n",
      "2024-11-30 16:20:19.177000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-70000\n",
      "2024-11-30 16:20:19.177000: I training.py:192] Running evaluation for step 70000\n",
      "2024-11-30 16:24:37.640000: I training.py:192] Evaluation result for step 70000: loss = 1.191789 ; perplexity = 3.292968\n",
      "2024-11-30 16:25:39.315000: I runner.py:310] Step = 70100 ; steps/s = 1.62, tokens/s = 43333 (43333 target) ; Learning rate = 0.000334 ; Loss = 1.525342\n",
      "2024-11-30 16:26:41.096000: I runner.py:310] Step = 70200 ; steps/s = 1.62, tokens/s = 43255 (43255 target) ; Learning rate = 0.000334 ; Loss = 1.510598\n",
      "2024-11-30 16:27:42.848000: I runner.py:310] Step = 70300 ; steps/s = 1.62, tokens/s = 43245 (43245 target) ; Learning rate = 0.000333 ; Loss = 1.518189\n",
      "2024-11-30 16:28:44.039000: I runner.py:310] Step = 70400 ; steps/s = 1.63, tokens/s = 42832 (42832 target) ; Learning rate = 0.000333 ; Loss = 1.522081\n",
      "2024-11-30 16:29:45.771000: I runner.py:310] Step = 70500 ; steps/s = 1.62, tokens/s = 43278 (43278 target) ; Learning rate = 0.000333 ; Loss = 1.517584\n",
      "2024-11-30 16:30:47.548000: I runner.py:310] Step = 70600 ; steps/s = 1.62, tokens/s = 43249 (43249 target) ; Learning rate = 0.000333 ; Loss = 1.517255\n",
      "2024-11-30 16:31:49.320000: I runner.py:310] Step = 70700 ; steps/s = 1.62, tokens/s = 43233 (43233 target) ; Learning rate = 0.000332 ; Loss = 1.527770\n",
      "2024-11-30 16:32:50.543000: I runner.py:310] Step = 70800 ; steps/s = 1.63, tokens/s = 42833 (42833 target) ; Learning rate = 0.000332 ; Loss = 1.515394\n",
      "2024-11-30 16:33:52.315000: I runner.py:310] Step = 70900 ; steps/s = 1.62, tokens/s = 43231 (43231 target) ; Learning rate = 0.000332 ; Loss = 1.509398\n",
      "2024-11-30 16:34:54.053000: I runner.py:310] Step = 71000 ; steps/s = 1.62, tokens/s = 43276 (43276 target) ; Learning rate = 0.000332 ; Loss = 1.515731\n",
      "2024-11-30 16:35:55.894000: I runner.py:310] Step = 71100 ; steps/s = 1.62, tokens/s = 43203 (43203 target) ; Learning rate = 0.000331 ; Loss = 1.512744\n",
      "2024-11-30 16:36:57.068000: I runner.py:310] Step = 71200 ; steps/s = 1.63, tokens/s = 42841 (42841 target) ; Learning rate = 0.000331 ; Loss = 1.512439\n",
      "2024-11-30 16:37:58.836000: I runner.py:310] Step = 71300 ; steps/s = 1.62, tokens/s = 43268 (43268 target) ; Learning rate = 0.000331 ; Loss = 1.511258\n",
      "2024-11-30 16:39:00.631000: I runner.py:310] Step = 71400 ; steps/s = 1.62, tokens/s = 43239 (43239 target) ; Learning rate = 0.000331 ; Loss = 1.512198\n",
      "2024-11-30 16:40:02.462000: I runner.py:310] Step = 71500 ; steps/s = 1.62, tokens/s = 43214 (43214 target) ; Learning rate = 0.000331 ; Loss = 1.517261\n",
      "2024-11-30 16:41:03.692000: I runner.py:310] Step = 71600 ; steps/s = 1.63, tokens/s = 42765 (42765 target) ; Learning rate = 0.000330 ; Loss = 1.513372\n",
      "2024-11-30 16:42:06.020000: I runner.py:310] Step = 71700 ; steps/s = 1.60, tokens/s = 42896 (42896 target) ; Learning rate = 0.000330 ; Loss = 1.514817\n",
      "2024-11-30 16:43:07.782000: I runner.py:310] Step = 71800 ; steps/s = 1.62, tokens/s = 43262 (43262 target) ; Learning rate = 0.000330 ; Loss = 1.523048\n",
      "2024-11-30 16:44:09.600000: I runner.py:310] Step = 71900 ; steps/s = 1.62, tokens/s = 43209 (43209 target) ; Learning rate = 0.000330 ; Loss = 1.525461\n",
      "2024-11-30 16:45:10.788000: I runner.py:310] Step = 72000 ; steps/s = 1.63, tokens/s = 42818 (42818 target) ; Learning rate = 0.000329 ; Loss = 1.508767\n",
      "2024-11-30 16:46:12.619000: I runner.py:310] Step = 72100 ; steps/s = 1.62, tokens/s = 43198 (43198 target) ; Learning rate = 0.000329 ; Loss = 1.513137\n",
      "2024-11-30 16:47:14.387000: I runner.py:310] Step = 72200 ; steps/s = 1.62, tokens/s = 43249 (43249 target) ; Learning rate = 0.000329 ; Loss = 1.510742\n",
      "2024-11-30 16:48:16.156000: I runner.py:310] Step = 72300 ; steps/s = 1.62, tokens/s = 43253 (43253 target) ; Learning rate = 0.000329 ; Loss = 1.523908\n",
      "2024-11-30 16:49:17.444000: I runner.py:310] Step = 72400 ; steps/s = 1.63, tokens/s = 42799 (42799 target) ; Learning rate = 0.000328 ; Loss = 1.524413\n",
      "2024-11-30 16:50:19.332000: I runner.py:310] Step = 72500 ; steps/s = 1.62, tokens/s = 43158 (43158 target) ; Learning rate = 0.000328 ; Loss = 1.514801\n",
      "2024-11-30 16:51:21.101000: I runner.py:310] Step = 72600 ; steps/s = 1.62, tokens/s = 43274 (43274 target) ; Learning rate = 0.000328 ; Loss = 1.518202\n",
      "2024-11-30 16:52:22.899000: I runner.py:310] Step = 72700 ; steps/s = 1.62, tokens/s = 43223 (43223 target) ; Learning rate = 0.000328 ; Loss = 1.509447\n",
      "2024-11-30 16:53:24.121000: I runner.py:310] Step = 72800 ; steps/s = 1.63, tokens/s = 42785 (42785 target) ; Learning rate = 0.000328 ; Loss = 1.517290\n",
      "2024-11-30 16:54:25.950000: I runner.py:310] Step = 72900 ; steps/s = 1.62, tokens/s = 43228 (43228 target) ; Learning rate = 0.000327 ; Loss = 1.515018\n",
      "2024-11-30 16:55:27.770000: I runner.py:310] Step = 73000 ; steps/s = 1.62, tokens/s = 43206 (43206 target) ; Learning rate = 0.000327 ; Loss = 1.518743\n",
      "2024-11-30 16:56:29.563000: I runner.py:310] Step = 73100 ; steps/s = 1.62, tokens/s = 43232 (43232 target) ; Learning rate = 0.000327 ; Loss = 1.512858\n",
      "2024-11-30 16:57:30.786000: I runner.py:310] Step = 73200 ; steps/s = 1.63, tokens/s = 42817 (42817 target) ; Learning rate = 0.000327 ; Loss = 1.524009\n",
      "2024-11-30 16:58:32.623000: I runner.py:310] Step = 73300 ; steps/s = 1.62, tokens/s = 43206 (43206 target) ; Learning rate = 0.000326 ; Loss = 1.513465\n",
      "2024-11-30 16:59:34.414000: I runner.py:310] Step = 73400 ; steps/s = 1.62, tokens/s = 43243 (43243 target) ; Learning rate = 0.000326 ; Loss = 1.508654\n",
      "2024-11-30 17:00:36.195000: I runner.py:310] Step = 73500 ; steps/s = 1.62, tokens/s = 43238 (43238 target) ; Learning rate = 0.000326 ; Loss = 1.510387\n",
      "2024-11-30 17:01:37.470000: I runner.py:310] Step = 73600 ; steps/s = 1.63, tokens/s = 42791 (42791 target) ; Learning rate = 0.000326 ; Loss = 1.523700\n",
      "2024-11-30 17:02:39.247000: I runner.py:310] Step = 73700 ; steps/s = 1.62, tokens/s = 43260 (43260 target) ; Learning rate = 0.000326 ; Loss = 1.515258\n",
      "2024-11-30 17:03:41.009000: I runner.py:310] Step = 73800 ; steps/s = 1.62, tokens/s = 43227 (43227 target) ; Learning rate = 0.000325 ; Loss = 1.508970\n",
      "2024-11-30 17:04:42.300000: I runner.py:310] Step = 73900 ; steps/s = 1.63, tokens/s = 42733 (42733 target) ; Learning rate = 0.000325 ; Loss = 1.514261\n",
      "2024-11-30 17:05:44.016000: I runner.py:310] Step = 74000 ; steps/s = 1.62, tokens/s = 43319 (43319 target) ; Learning rate = 0.000325 ; Loss = 1.510964\n",
      "2024-11-30 17:06:45.841000: I runner.py:310] Step = 74100 ; steps/s = 1.62, tokens/s = 43204 (43204 target) ; Learning rate = 0.000325 ; Loss = 1.518237\n",
      "2024-11-30 17:07:47.626000: I runner.py:310] Step = 74200 ; steps/s = 1.62, tokens/s = 43233 (43233 target) ; Learning rate = 0.000324 ; Loss = 1.525372\n",
      "2024-11-30 17:08:48.816000: I runner.py:310] Step = 74300 ; steps/s = 1.63, tokens/s = 42827 (42827 target) ; Learning rate = 0.000324 ; Loss = 1.514571\n",
      "2024-11-30 17:09:50.608000: I runner.py:310] Step = 74400 ; steps/s = 1.62, tokens/s = 43227 (43227 target) ; Learning rate = 0.000324 ; Loss = 1.519887\n",
      "2024-11-30 17:10:52.423000: I runner.py:310] Step = 74500 ; steps/s = 1.62, tokens/s = 43232 (43232 target) ; Learning rate = 0.000324 ; Loss = 1.511641\n",
      "2024-11-30 17:11:54.209000: I runner.py:310] Step = 74600 ; steps/s = 1.62, tokens/s = 43226 (43226 target) ; Learning rate = 0.000324 ; Loss = 1.507557\n",
      "2024-11-30 17:12:55.385000: I runner.py:310] Step = 74700 ; steps/s = 1.63, tokens/s = 42860 (42860 target) ; Learning rate = 0.000323 ; Loss = 1.515992\n",
      "2024-11-30 17:13:57.189000: I runner.py:310] Step = 74800 ; steps/s = 1.62, tokens/s = 43233 (43233 target) ; Learning rate = 0.000323 ; Loss = 1.513607\n",
      "2024-11-30 17:14:58.905000: I runner.py:310] Step = 74900 ; steps/s = 1.62, tokens/s = 43294 (43294 target) ; Learning rate = 0.000323 ; Loss = 1.513881\n",
      "2024-11-30 17:16:00.693000: I runner.py:310] Step = 75000 ; steps/s = 1.62, tokens/s = 43233 (43233 target) ; Learning rate = 0.000323 ; Loss = 1.520783\n",
      "2024-11-30 17:16:00.695000: I training.py:192] Running evaluation for step 75000\n",
      "2024-11-30 17:20:22.925000: I training.py:192] Evaluation result for step 75000: loss = 1.196064 ; perplexity = 3.307075\n",
      "2024-11-30 17:21:24.075000: I runner.py:310] Step = 75100 ; steps/s = 1.64, tokens/s = 42854 (42854 target) ; Learning rate = 0.000323 ; Loss = 1.512891\n",
      "2024-11-30 17:22:25.949000: I runner.py:310] Step = 75200 ; steps/s = 1.62, tokens/s = 43183 (43183 target) ; Learning rate = 0.000322 ; Loss = 1.506377\n",
      "2024-11-30 17:23:27.680000: I runner.py:310] Step = 75300 ; steps/s = 1.62, tokens/s = 43290 (43290 target) ; Learning rate = 0.000322 ; Loss = 1.511750\n",
      "2024-11-30 17:24:29.490000: I runner.py:310] Step = 75400 ; steps/s = 1.62, tokens/s = 43198 (43198 target) ; Learning rate = 0.000322 ; Loss = 1.521719\n",
      "2024-11-30 17:25:30.753000: I runner.py:310] Step = 75500 ; steps/s = 1.63, tokens/s = 42794 (42794 target) ; Learning rate = 0.000322 ; Loss = 1.512645\n",
      "2024-11-30 17:26:32.530000: I runner.py:310] Step = 75600 ; steps/s = 1.62, tokens/s = 43276 (43276 target) ; Learning rate = 0.000321 ; Loss = 1.512308\n",
      "2024-11-30 17:27:34.286000: I runner.py:310] Step = 75700 ; steps/s = 1.62, tokens/s = 43254 (43254 target) ; Learning rate = 0.000321 ; Loss = 1.508236\n",
      "2024-11-30 17:28:36.120000: I runner.py:310] Step = 75800 ; steps/s = 1.62, tokens/s = 43204 (43204 target) ; Learning rate = 0.000321 ; Loss = 1.514287\n",
      "2024-11-30 17:29:37.314000: I runner.py:310] Step = 75900 ; steps/s = 1.63, tokens/s = 42818 (42818 target) ; Learning rate = 0.000321 ; Loss = 1.506401\n",
      "2024-11-30 17:30:39.145000: I runner.py:310] Step = 76000 ; steps/s = 1.62, tokens/s = 43221 (43221 target) ; Learning rate = 0.000321 ; Loss = 1.515966\n",
      "2024-11-30 17:31:40.956000: I runner.py:310] Step = 76100 ; steps/s = 1.62, tokens/s = 43198 (43198 target) ; Learning rate = 0.000320 ; Loss = 1.508388\n",
      "2024-11-30 17:32:42.745000: I runner.py:310] Step = 76200 ; steps/s = 1.62, tokens/s = 43214 (43214 target) ; Learning rate = 0.000320 ; Loss = 1.519059\n",
      "2024-11-30 17:33:44.003000: I runner.py:310] Step = 76300 ; steps/s = 1.63, tokens/s = 42829 (42829 target) ; Learning rate = 0.000320 ; Loss = 1.506320\n",
      "2024-11-30 17:34:45.820000: I runner.py:310] Step = 76400 ; steps/s = 1.62, tokens/s = 43230 (43230 target) ; Learning rate = 0.000320 ; Loss = 1.513459\n",
      "2024-11-30 17:35:47.585000: I runner.py:310] Step = 76500 ; steps/s = 1.62, tokens/s = 43257 (43257 target) ; Learning rate = 0.000320 ; Loss = 1.511286\n",
      "2024-11-30 17:36:49.337000: I runner.py:310] Step = 76600 ; steps/s = 1.62, tokens/s = 43223 (43223 target) ; Learning rate = 0.000319 ; Loss = 1.520730\n",
      "2024-11-30 17:37:50.596000: I runner.py:310] Step = 76700 ; steps/s = 1.63, tokens/s = 42800 (42800 target) ; Learning rate = 0.000319 ; Loss = 1.501111\n",
      "2024-11-30 17:38:52.386000: I runner.py:310] Step = 76800 ; steps/s = 1.62, tokens/s = 43247 (43247 target) ; Learning rate = 0.000319 ; Loss = 1.509341\n",
      "2024-11-30 17:39:54.182000: I runner.py:310] Step = 76900 ; steps/s = 1.62, tokens/s = 43239 (43239 target) ; Learning rate = 0.000319 ; Loss = 1.526156\n",
      "2024-11-30 17:40:55.914000: I runner.py:310] Step = 77000 ; steps/s = 1.62, tokens/s = 43290 (43290 target) ; Learning rate = 0.000319 ; Loss = 1.517497\n",
      "2024-11-30 17:41:57.071000: I runner.py:310] Step = 77100 ; steps/s = 1.64, tokens/s = 42822 (42822 target) ; Learning rate = 0.000318 ; Loss = 1.519599\n",
      "2024-11-30 17:42:58.853000: I runner.py:310] Step = 77200 ; steps/s = 1.62, tokens/s = 43249 (43249 target) ; Learning rate = 0.000318 ; Loss = 1.517656\n",
      "2024-11-30 17:44:00.714000: I runner.py:310] Step = 77300 ; steps/s = 1.62, tokens/s = 43177 (43177 target) ; Learning rate = 0.000318 ; Loss = 1.508501\n",
      "2024-11-30 17:45:02.537000: I runner.py:310] Step = 77400 ; steps/s = 1.62, tokens/s = 43241 (43241 target) ; Learning rate = 0.000318 ; Loss = 1.510149\n",
      "2024-11-30 17:46:03.735000: I runner.py:310] Step = 77500 ; steps/s = 1.63, tokens/s = 42816 (42816 target) ; Learning rate = 0.000317 ; Loss = 1.514563\n",
      "2024-11-30 17:47:05.490000: I runner.py:310] Step = 77600 ; steps/s = 1.62, tokens/s = 43271 (43271 target) ; Learning rate = 0.000317 ; Loss = 1.503651\n",
      "2024-11-30 17:48:07.329000: I runner.py:310] Step = 77700 ; steps/s = 1.62, tokens/s = 43212 (43212 target) ; Learning rate = 0.000317 ; Loss = 1.515825\n",
      "2024-11-30 17:49:09.137000: I runner.py:310] Step = 77800 ; steps/s = 1.62, tokens/s = 43211 (43211 target) ; Learning rate = 0.000317 ; Loss = 1.513112\n",
      "2024-11-30 17:50:10.296000: I runner.py:310] Step = 77900 ; steps/s = 1.64, tokens/s = 42855 (42855 target) ; Learning rate = 0.000317 ; Loss = 1.509089\n",
      "2024-11-30 17:51:12.027000: I runner.py:310] Step = 78000 ; steps/s = 1.62, tokens/s = 43297 (43297 target) ; Learning rate = 0.000316 ; Loss = 1.511458\n",
      "2024-11-30 17:52:13.762000: I runner.py:310] Step = 78100 ; steps/s = 1.62, tokens/s = 43274 (43274 target) ; Learning rate = 0.000316 ; Loss = 1.511583\n",
      "2024-11-30 17:53:14.985000: I runner.py:310] Step = 78200 ; steps/s = 1.63, tokens/s = 42764 (42764 target) ; Learning rate = 0.000316 ; Loss = 1.539941\n",
      "2024-11-30 17:54:16.790000: I runner.py:310] Step = 78300 ; steps/s = 1.62, tokens/s = 43251 (43251 target) ; Learning rate = 0.000316 ; Loss = 1.505741\n",
      "2024-11-30 17:55:18.524000: I runner.py:310] Step = 78400 ; steps/s = 1.62, tokens/s = 43248 (43248 target) ; Learning rate = 0.000316 ; Loss = 1.511773\n",
      "2024-11-30 17:56:20.288000: I runner.py:310] Step = 78500 ; steps/s = 1.62, tokens/s = 43261 (43261 target) ; Learning rate = 0.000315 ; Loss = 1.515334\n",
      "2024-11-30 17:57:21.463000: I runner.py:310] Step = 78600 ; steps/s = 1.63, tokens/s = 42855 (42855 target) ; Learning rate = 0.000315 ; Loss = 1.512760\n",
      "2024-11-30 17:58:23.235000: I runner.py:310] Step = 78700 ; steps/s = 1.62, tokens/s = 43254 (43254 target) ; Learning rate = 0.000315 ; Loss = 1.508590\n",
      "2024-11-30 17:59:24.986000: I runner.py:310] Step = 78800 ; steps/s = 1.62, tokens/s = 43268 (43268 target) ; Learning rate = 0.000315 ; Loss = 1.512068\n",
      "2024-11-30 18:00:26.783000: I runner.py:310] Step = 78900 ; steps/s = 1.62, tokens/s = 43216 (43216 target) ; Learning rate = 0.000315 ; Loss = 1.519773\n",
      "2024-11-30 18:01:28.031000: I runner.py:310] Step = 79000 ; steps/s = 1.63, tokens/s = 42815 (42815 target) ; Learning rate = 0.000314 ; Loss = 1.515465\n",
      "2024-11-30 18:02:29.816000: I runner.py:310] Step = 79100 ; steps/s = 1.62, tokens/s = 43228 (43228 target) ; Learning rate = 0.000314 ; Loss = 1.502700\n",
      "2024-11-30 18:03:31.626000: I runner.py:310] Step = 79200 ; steps/s = 1.62, tokens/s = 43213 (43213 target) ; Learning rate = 0.000314 ; Loss = 1.502284\n",
      "2024-11-30 18:04:33.390000: I runner.py:310] Step = 79300 ; steps/s = 1.62, tokens/s = 43265 (43265 target) ; Learning rate = 0.000314 ; Loss = 1.513073\n",
      "2024-11-30 18:05:34.626000: I runner.py:310] Step = 79400 ; steps/s = 1.63, tokens/s = 42806 (42806 target) ; Learning rate = 0.000314 ; Loss = 1.502965\n",
      "2024-11-30 18:06:36.350000: I runner.py:310] Step = 79500 ; steps/s = 1.62, tokens/s = 43299 (43299 target) ; Learning rate = 0.000313 ; Loss = 1.506959\n",
      "2024-11-30 18:07:38.122000: I runner.py:310] Step = 79600 ; steps/s = 1.62, tokens/s = 43266 (43266 target) ; Learning rate = 0.000313 ; Loss = 1.504134\n",
      "2024-11-30 18:08:39.897000: I runner.py:310] Step = 79700 ; steps/s = 1.62, tokens/s = 43215 (43215 target) ; Learning rate = 0.000313 ; Loss = 1.512509\n",
      "2024-11-30 18:09:41.095000: I runner.py:310] Step = 79800 ; steps/s = 1.63, tokens/s = 42855 (42855 target) ; Learning rate = 0.000313 ; Loss = 1.510641\n",
      "2024-11-30 18:10:42.802000: I runner.py:310] Step = 79900 ; steps/s = 1.62, tokens/s = 43308 (43308 target) ; Learning rate = 0.000313 ; Loss = 1.508084\n",
      "2024-11-30 18:11:44.577000: I runner.py:310] Step = 80000 ; steps/s = 1.62, tokens/s = 43234 (43234 target) ; Learning rate = 0.000312 ; Loss = 1.506130\n",
      "2024-11-30 18:11:46.710000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-80000\n",
      "2024-11-30 18:11:46.710000: I training.py:192] Running evaluation for step 80000\n",
      "2024-11-30 18:16:09.064000: I training.py:192] Evaluation result for step 80000: loss = 1.204262 ; perplexity = 3.334296\n",
      "2024-11-30 18:17:10.692000: I runner.py:310] Step = 80100 ; steps/s = 1.62, tokens/s = 43358 (43358 target) ; Learning rate = 0.000312 ; Loss = 1.509959\n",
      "2024-11-30 18:18:11.942000: I runner.py:310] Step = 80200 ; steps/s = 1.63, tokens/s = 42785 (42785 target) ; Learning rate = 0.000312 ; Loss = 1.506768\n",
      "2024-11-30 18:19:13.694000: I runner.py:310] Step = 80300 ; steps/s = 1.62, tokens/s = 43241 (43241 target) ; Learning rate = 0.000312 ; Loss = 1.505542\n",
      "2024-11-30 18:20:15.516000: I runner.py:310] Step = 80400 ; steps/s = 1.62, tokens/s = 43198 (43198 target) ; Learning rate = 0.000312 ; Loss = 1.511866\n",
      "2024-11-30 18:21:17.375000: I runner.py:310] Step = 80500 ; steps/s = 1.62, tokens/s = 43211 (43211 target) ; Learning rate = 0.000312 ; Loss = 1.510631\n",
      "2024-11-30 18:22:18.591000: I runner.py:310] Step = 80600 ; steps/s = 1.63, tokens/s = 42815 (42815 target) ; Learning rate = 0.000311 ; Loss = 1.503879\n",
      "2024-11-30 18:23:20.417000: I runner.py:310] Step = 80700 ; steps/s = 1.62, tokens/s = 43242 (43242 target) ; Learning rate = 0.000311 ; Loss = 1.507055\n",
      "2024-11-30 18:24:22.228000: I runner.py:310] Step = 80800 ; steps/s = 1.62, tokens/s = 43205 (43205 target) ; Learning rate = 0.000311 ; Loss = 1.509093\n",
      "2024-11-30 18:25:24.000000: I runner.py:310] Step = 80900 ; steps/s = 1.62, tokens/s = 43256 (43256 target) ; Learning rate = 0.000311 ; Loss = 1.512222\n",
      "2024-11-30 18:26:25.209000: I runner.py:310] Step = 81000 ; steps/s = 1.63, tokens/s = 42828 (42828 target) ; Learning rate = 0.000311 ; Loss = 1.513564\n",
      "2024-11-30 18:27:27.040000: I runner.py:310] Step = 81100 ; steps/s = 1.62, tokens/s = 43187 (43187 target) ; Learning rate = 0.000310 ; Loss = 1.507633\n",
      "2024-11-30 18:28:28.799000: I runner.py:310] Step = 81200 ; steps/s = 1.62, tokens/s = 43299 (43299 target) ; Learning rate = 0.000310 ; Loss = 1.505805\n",
      "2024-11-30 18:29:30.586000: I runner.py:310] Step = 81300 ; steps/s = 1.62, tokens/s = 43226 (43226 target) ; Learning rate = 0.000310 ; Loss = 1.506612\n",
      "2024-11-30 18:30:31.787000: I runner.py:310] Step = 81400 ; steps/s = 1.63, tokens/s = 42789 (42789 target) ; Learning rate = 0.000310 ; Loss = 1.499858\n",
      "2024-11-30 18:31:33.559000: I runner.py:310] Step = 81500 ; steps/s = 1.62, tokens/s = 43258 (43258 target) ; Learning rate = 0.000310 ; Loss = 1.513288\n",
      "2024-11-30 18:32:35.339000: I runner.py:310] Step = 81600 ; steps/s = 1.62, tokens/s = 43237 (43237 target) ; Learning rate = 0.000309 ; Loss = 1.509921\n",
      "2024-11-30 18:33:37.151000: I runner.py:310] Step = 81700 ; steps/s = 1.62, tokens/s = 43245 (43245 target) ; Learning rate = 0.000309 ; Loss = 1.510292\n",
      "2024-11-30 18:34:38.417000: I runner.py:310] Step = 81800 ; steps/s = 1.63, tokens/s = 42766 (42766 target) ; Learning rate = 0.000309 ; Loss = 1.506249\n",
      "2024-11-30 18:35:40.219000: I runner.py:310] Step = 81900 ; steps/s = 1.62, tokens/s = 43248 (43248 target) ; Learning rate = 0.000309 ; Loss = 1.503699\n",
      "2024-11-30 18:36:42.065000: I runner.py:310] Step = 82000 ; steps/s = 1.62, tokens/s = 43180 (43180 target) ; Learning rate = 0.000309 ; Loss = 1.503381\n",
      "2024-11-30 18:37:43.880000: I runner.py:310] Step = 82100 ; steps/s = 1.62, tokens/s = 43226 (43226 target) ; Learning rate = 0.000308 ; Loss = 1.505917\n",
      "2024-11-30 18:38:45.039000: I runner.py:310] Step = 82200 ; steps/s = 1.64, tokens/s = 42833 (42833 target) ; Learning rate = 0.000308 ; Loss = 1.498101\n",
      "2024-11-30 18:39:46.848000: I runner.py:310] Step = 82300 ; steps/s = 1.62, tokens/s = 43209 (43209 target) ; Learning rate = 0.000308 ; Loss = 1.513908\n",
      "2024-11-30 18:40:48.603000: I runner.py:310] Step = 82400 ; steps/s = 1.62, tokens/s = 43278 (43278 target) ; Learning rate = 0.000308 ; Loss = 1.511156\n",
      "2024-11-30 18:41:50.064000: I runner.py:310] Step = 82500 ; steps/s = 1.63, tokens/s = 42930 (42930 target) ; Learning rate = 0.000308 ; Loss = 1.509717\n",
      "2024-11-30 18:42:51.614000: I runner.py:310] Step = 82600 ; steps/s = 1.62, tokens/s = 43141 (43141 target) ; Learning rate = 0.000308 ; Loss = 1.505807\n",
      "2024-11-30 18:43:53.412000: I runner.py:310] Step = 82700 ; steps/s = 1.62, tokens/s = 43235 (43235 target) ; Learning rate = 0.000307 ; Loss = 1.504252\n",
      "2024-11-30 18:44:55.264000: I runner.py:310] Step = 82800 ; steps/s = 1.62, tokens/s = 43191 (43191 target) ; Learning rate = 0.000307 ; Loss = 1.507597\n",
      "2024-11-30 18:45:56.487000: I runner.py:310] Step = 82900 ; steps/s = 1.63, tokens/s = 42790 (42790 target) ; Learning rate = 0.000307 ; Loss = 1.514915\n",
      "2024-11-30 18:46:58.267000: I runner.py:310] Step = 83000 ; steps/s = 1.62, tokens/s = 43229 (43229 target) ; Learning rate = 0.000307 ; Loss = 1.512723\n",
      "2024-11-30 18:47:59.985000: I runner.py:310] Step = 83100 ; steps/s = 1.62, tokens/s = 43312 (43312 target) ; Learning rate = 0.000307 ; Loss = 1.507460\n",
      "2024-11-30 18:49:01.768000: I runner.py:310] Step = 83200 ; steps/s = 1.62, tokens/s = 43240 (43240 target) ; Learning rate = 0.000306 ; Loss = 1.504138\n",
      "2024-11-30 18:50:03.019000: I runner.py:310] Step = 83300 ; steps/s = 1.63, tokens/s = 42802 (42802 target) ; Learning rate = 0.000306 ; Loss = 1.513187\n",
      "2024-11-30 18:51:04.823000: I runner.py:310] Step = 83400 ; steps/s = 1.62, tokens/s = 43202 (43202 target) ; Learning rate = 0.000306 ; Loss = 1.507212\n",
      "2024-11-30 18:52:06.626000: I runner.py:310] Step = 83500 ; steps/s = 1.62, tokens/s = 43208 (43208 target) ; Learning rate = 0.000306 ; Loss = 1.502619\n",
      "2024-11-30 18:53:08.419000: I runner.py:310] Step = 83600 ; steps/s = 1.62, tokens/s = 43238 (43238 target) ; Learning rate = 0.000306 ; Loss = 1.503667\n",
      "2024-11-30 18:54:09.613000: I runner.py:310] Step = 83700 ; steps/s = 1.63, tokens/s = 42871 (42871 target) ; Learning rate = 0.000306 ; Loss = 1.505401\n",
      "2024-11-30 18:55:11.426000: I runner.py:310] Step = 83800 ; steps/s = 1.62, tokens/s = 43234 (43234 target) ; Learning rate = 0.000305 ; Loss = 1.505244\n",
      "2024-11-30 18:56:13.229000: I runner.py:310] Step = 83900 ; steps/s = 1.62, tokens/s = 43213 (43213 target) ; Learning rate = 0.000305 ; Loss = 1.505752\n",
      "2024-11-30 18:57:15.010000: I runner.py:310] Step = 84000 ; steps/s = 1.62, tokens/s = 43237 (43237 target) ; Learning rate = 0.000305 ; Loss = 1.507068\n",
      "2024-11-30 18:58:16.247000: I runner.py:310] Step = 84100 ; steps/s = 1.63, tokens/s = 42807 (42807 target) ; Learning rate = 0.000305 ; Loss = 1.503818\n",
      "2024-11-30 18:59:18.039000: I runner.py:310] Step = 84200 ; steps/s = 1.62, tokens/s = 43224 (43224 target) ; Learning rate = 0.000305 ; Loss = 1.502550\n",
      "2024-11-30 19:00:19.793000: I runner.py:310] Step = 84300 ; steps/s = 1.62, tokens/s = 43280 (43280 target) ; Learning rate = 0.000304 ; Loss = 1.505749\n",
      "2024-11-30 19:01:21.567000: I runner.py:310] Step = 84400 ; steps/s = 1.62, tokens/s = 43236 (43236 target) ; Learning rate = 0.000304 ; Loss = 1.499535\n",
      "2024-11-30 19:02:22.840000: I runner.py:310] Step = 84500 ; steps/s = 1.63, tokens/s = 42770 (42770 target) ; Learning rate = 0.000304 ; Loss = 1.498850\n",
      "2024-11-30 19:03:24.598000: I runner.py:310] Step = 84600 ; steps/s = 1.62, tokens/s = 43279 (43279 target) ; Learning rate = 0.000304 ; Loss = 1.511230\n",
      "2024-11-30 19:04:26.400000: I runner.py:310] Step = 84700 ; steps/s = 1.62, tokens/s = 43213 (43213 target) ; Learning rate = 0.000304 ; Loss = 1.505528\n",
      "2024-11-30 19:05:28.202000: I runner.py:310] Step = 84800 ; steps/s = 1.62, tokens/s = 43216 (43216 target) ; Learning rate = 0.000304 ; Loss = 1.509164\n",
      "2024-11-30 19:06:29.379000: I runner.py:310] Step = 84900 ; steps/s = 1.63, tokens/s = 42842 (42842 target) ; Learning rate = 0.000303 ; Loss = 1.500597\n",
      "2024-11-30 19:07:31.107000: I runner.py:310] Step = 85000 ; steps/s = 1.62, tokens/s = 43288 (43288 target) ; Learning rate = 0.000303 ; Loss = 1.503599\n",
      "2024-11-30 19:07:31.110000: I training.py:192] Running evaluation for step 85000\n",
      "2024-11-30 19:12:00.340000: I training.py:192] Evaluation result for step 85000: loss = 1.207821 ; perplexity = 3.346184\n",
      "2024-11-30 19:13:02.071000: I runner.py:310] Step = 85100 ; steps/s = 1.62, tokens/s = 43292 (43292 target) ; Learning rate = 0.000303 ; Loss = 1.515087\n",
      "2024-11-30 19:14:03.899000: I runner.py:310] Step = 85200 ; steps/s = 1.62, tokens/s = 43213 (43213 target) ; Learning rate = 0.000303 ; Loss = 1.508936\n",
      "2024-11-30 19:15:05.117000: I runner.py:310] Step = 85300 ; steps/s = 1.63, tokens/s = 42803 (42803 target) ; Learning rate = 0.000303 ; Loss = 1.500142\n",
      "2024-11-30 19:16:06.903000: I runner.py:310] Step = 85400 ; steps/s = 1.62, tokens/s = 43251 (43251 target) ; Learning rate = 0.000302 ; Loss = 1.503268\n",
      "2024-11-30 19:17:08.751000: I runner.py:310] Step = 85500 ; steps/s = 1.62, tokens/s = 43179 (43179 target) ; Learning rate = 0.000302 ; Loss = 1.514108\n",
      "2024-11-30 19:18:10.533000: I runner.py:310] Step = 85600 ; steps/s = 1.62, tokens/s = 43250 (43250 target) ; Learning rate = 0.000302 ; Loss = 1.501828\n",
      "2024-11-30 19:19:11.753000: I runner.py:310] Step = 85700 ; steps/s = 1.63, tokens/s = 42793 (42793 target) ; Learning rate = 0.000302 ; Loss = 1.509741\n",
      "2024-11-30 19:20:13.582000: I runner.py:310] Step = 85800 ; steps/s = 1.62, tokens/s = 43216 (43216 target) ; Learning rate = 0.000302 ; Loss = 1.499550\n",
      "2024-11-30 19:21:15.405000: I runner.py:310] Step = 85900 ; steps/s = 1.62, tokens/s = 43206 (43206 target) ; Learning rate = 0.000302 ; Loss = 1.509343\n",
      "2024-11-30 19:22:17.196000: I runner.py:310] Step = 86000 ; steps/s = 1.62, tokens/s = 43270 (43270 target) ; Learning rate = 0.000301 ; Loss = 1.503790\n",
      "2024-11-30 19:23:18.374000: I runner.py:310] Step = 86100 ; steps/s = 1.63, tokens/s = 42869 (42869 target) ; Learning rate = 0.000301 ; Loss = 1.508220\n",
      "2024-11-30 19:24:20.187000: I runner.py:310] Step = 86200 ; steps/s = 1.62, tokens/s = 43215 (43215 target) ; Learning rate = 0.000301 ; Loss = 1.501086\n",
      "2024-11-30 19:25:21.966000: I runner.py:310] Step = 86300 ; steps/s = 1.62, tokens/s = 43229 (43229 target) ; Learning rate = 0.000301 ; Loss = 1.503584\n",
      "2024-11-30 19:26:23.823000: I runner.py:310] Step = 86400 ; steps/s = 1.62, tokens/s = 43179 (43179 target) ; Learning rate = 0.000301 ; Loss = 1.506840\n",
      "2024-11-30 19:27:25.097000: I runner.py:310] Step = 86500 ; steps/s = 1.63, tokens/s = 42753 (42753 target) ; Learning rate = 0.000301 ; Loss = 1.497339\n",
      "2024-11-30 19:28:26.905000: I runner.py:310] Step = 86600 ; steps/s = 1.62, tokens/s = 43237 (43237 target) ; Learning rate = 0.000300 ; Loss = 1.511093\n",
      "2024-11-30 19:29:28.652000: I runner.py:310] Step = 86700 ; steps/s = 1.62, tokens/s = 43267 (43267 target) ; Learning rate = 0.000300 ; Loss = 1.511462\n",
      "2024-11-30 19:30:30.222000: I runner.py:310] Step = 86800 ; steps/s = 1.62, tokens/s = 43074 (43074 target) ; Learning rate = 0.000300 ; Loss = 1.509320\n",
      "2024-11-30 19:31:31.643000: I runner.py:310] Step = 86900 ; steps/s = 1.63, tokens/s = 42995 (42995 target) ; Learning rate = 0.000300 ; Loss = 1.508746\n",
      "2024-11-30 19:32:33.471000: I runner.py:310] Step = 87000 ; steps/s = 1.62, tokens/s = 43191 (43191 target) ; Learning rate = 0.000300 ; Loss = 1.509589\n",
      "2024-11-30 19:33:35.266000: I runner.py:310] Step = 87100 ; steps/s = 1.62, tokens/s = 43214 (43214 target) ; Learning rate = 0.000299 ; Loss = 1.498786\n",
      "2024-11-30 19:34:36.483000: I runner.py:310] Step = 87200 ; steps/s = 1.63, tokens/s = 42846 (42846 target) ; Learning rate = 0.000299 ; Loss = 1.507496\n",
      "2024-11-30 19:35:38.235000: I runner.py:310] Step = 87300 ; steps/s = 1.62, tokens/s = 43278 (43278 target) ; Learning rate = 0.000299 ; Loss = 1.501396\n",
      "2024-11-30 19:36:40.072000: I runner.py:310] Step = 87400 ; steps/s = 1.62, tokens/s = 43197 (43197 target) ; Learning rate = 0.000299 ; Loss = 1.501260\n",
      "2024-11-30 19:37:41.868000: I runner.py:310] Step = 87500 ; steps/s = 1.62, tokens/s = 43206 (43206 target) ; Learning rate = 0.000299 ; Loss = 1.503846\n",
      "2024-11-30 19:38:43.102000: I runner.py:310] Step = 87600 ; steps/s = 1.63, tokens/s = 42834 (42834 target) ; Learning rate = 0.000299 ; Loss = 1.513771\n",
      "2024-11-30 19:39:44.872000: I runner.py:310] Step = 87700 ; steps/s = 1.62, tokens/s = 43243 (43243 target) ; Learning rate = 0.000298 ; Loss = 1.515537\n",
      "2024-11-30 19:40:46.674000: I runner.py:310] Step = 87800 ; steps/s = 1.62, tokens/s = 43197 (43197 target) ; Learning rate = 0.000298 ; Loss = 1.505181\n",
      "2024-11-30 19:41:48.415000: I runner.py:310] Step = 87900 ; steps/s = 1.62, tokens/s = 43300 (43300 target) ; Learning rate = 0.000298 ; Loss = 1.503987\n",
      "2024-11-30 19:42:49.632000: I runner.py:310] Step = 88000 ; steps/s = 1.63, tokens/s = 42817 (42817 target) ; Learning rate = 0.000298 ; Loss = 1.502251\n",
      "2024-11-30 19:43:51.398000: I runner.py:310] Step = 88100 ; steps/s = 1.62, tokens/s = 43268 (43268 target) ; Learning rate = 0.000298 ; Loss = 1.503834\n",
      "2024-11-30 19:44:53.127000: I runner.py:310] Step = 88200 ; steps/s = 1.62, tokens/s = 43291 (43291 target) ; Learning rate = 0.000298 ; Loss = 1.503243\n",
      "2024-11-30 19:45:54.920000: I runner.py:310] Step = 88300 ; steps/s = 1.62, tokens/s = 43245 (43245 target) ; Learning rate = 0.000297 ; Loss = 1.499676\n",
      "2024-11-30 19:46:56.080000: I runner.py:310] Step = 88400 ; steps/s = 1.64, tokens/s = 42825 (42825 target) ; Learning rate = 0.000297 ; Loss = 1.499960\n",
      "2024-11-30 19:47:57.868000: I runner.py:310] Step = 88500 ; steps/s = 1.62, tokens/s = 43251 (43251 target) ; Learning rate = 0.000297 ; Loss = 1.498260\n",
      "2024-11-30 19:48:59.698000: I runner.py:310] Step = 88600 ; steps/s = 1.62, tokens/s = 43185 (43185 target) ; Learning rate = 0.000297 ; Loss = 1.508867\n",
      "2024-11-30 19:50:01.435000: I runner.py:310] Step = 88700 ; steps/s = 1.62, tokens/s = 43275 (43275 target) ; Learning rate = 0.000297 ; Loss = 1.507079\n",
      "2024-11-30 19:51:02.663000: I runner.py:310] Step = 88800 ; steps/s = 1.63, tokens/s = 42830 (42830 target) ; Learning rate = 0.000297 ; Loss = 1.495989\n",
      "2024-11-30 19:52:04.452000: I runner.py:310] Step = 88900 ; steps/s = 1.62, tokens/s = 43217 (43217 target) ; Learning rate = 0.000296 ; Loss = 1.502659\n",
      "2024-11-30 19:53:06.255000: I runner.py:310] Step = 89000 ; steps/s = 1.62, tokens/s = 43234 (43234 target) ; Learning rate = 0.000296 ; Loss = 1.504286\n",
      "2024-11-30 19:54:08.066000: I runner.py:310] Step = 89100 ; steps/s = 1.62, tokens/s = 43231 (43231 target) ; Learning rate = 0.000296 ; Loss = 1.505341\n",
      "2024-11-30 19:55:09.266000: I runner.py:310] Step = 89200 ; steps/s = 1.63, tokens/s = 42816 (42816 target) ; Learning rate = 0.000296 ; Loss = 1.501472\n",
      "2024-11-30 19:56:11.021000: I runner.py:310] Step = 89300 ; steps/s = 1.62, tokens/s = 43280 (43280 target) ; Learning rate = 0.000296 ; Loss = 1.507895\n",
      "2024-11-30 19:57:12.775000: I runner.py:310] Step = 89400 ; steps/s = 1.62, tokens/s = 43238 (43238 target) ; Learning rate = 0.000296 ; Loss = 1.504411\n",
      "2024-11-30 19:58:14.552000: I runner.py:310] Step = 89500 ; steps/s = 1.62, tokens/s = 43288 (43288 target) ; Learning rate = 0.000295 ; Loss = 1.509432\n",
      "2024-11-30 19:59:15.820000: I runner.py:310] Step = 89600 ; steps/s = 1.63, tokens/s = 42760 (42760 target) ; Learning rate = 0.000295 ; Loss = 1.496942\n",
      "2024-11-30 20:00:17.617000: I runner.py:310] Step = 89700 ; steps/s = 1.62, tokens/s = 43219 (43219 target) ; Learning rate = 0.000295 ; Loss = 1.500862\n",
      "2024-11-30 20:01:19.350000: I runner.py:310] Step = 89800 ; steps/s = 1.62, tokens/s = 43270 (43270 target) ; Learning rate = 0.000295 ; Loss = 1.508739\n",
      "2024-11-30 20:02:21.150000: I runner.py:310] Step = 89900 ; steps/s = 1.62, tokens/s = 43250 (43250 target) ; Learning rate = 0.000295 ; Loss = 1.505969\n",
      "2024-11-30 20:03:22.371000: I runner.py:310] Step = 90000 ; steps/s = 1.63, tokens/s = 42790 (42790 target) ; Learning rate = 0.000295 ; Loss = 1.495473\n",
      "2024-11-30 20:03:24.526000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-90000\n",
      "2024-11-30 20:03:24.526000: I training.py:192] Running evaluation for step 90000\n",
      "2024-11-30 20:07:48.398000: I training.py:192] Evaluation result for step 90000: loss = 1.210114 ; perplexity = 3.353867\n",
      "2024-11-30 20:08:50.092000: I runner.py:310] Step = 90100 ; steps/s = 1.62, tokens/s = 43306 (43306 target) ; Learning rate = 0.000294 ; Loss = 1.502633\n",
      "2024-11-30 20:09:51.882000: I runner.py:310] Step = 90200 ; steps/s = 1.62, tokens/s = 43252 (43252 target) ; Learning rate = 0.000294 ; Loss = 1.505981\n",
      "2024-11-30 20:10:53.653000: I runner.py:310] Step = 90300 ; steps/s = 1.62, tokens/s = 43241 (43241 target) ; Learning rate = 0.000294 ; Loss = 1.505802\n",
      "2024-11-30 20:11:54.890000: I runner.py:310] Step = 90400 ; steps/s = 1.63, tokens/s = 42802 (42802 target) ; Learning rate = 0.000294 ; Loss = 1.507112\n",
      "2024-11-30 20:12:56.704000: I runner.py:310] Step = 90500 ; steps/s = 1.62, tokens/s = 43217 (43217 target) ; Learning rate = 0.000294 ; Loss = 1.497982\n",
      "2024-11-30 20:13:58.477000: I runner.py:310] Step = 90600 ; steps/s = 1.62, tokens/s = 43218 (43218 target) ; Learning rate = 0.000294 ; Loss = 1.494313\n",
      "2024-11-30 20:15:00.301000: I runner.py:310] Step = 90700 ; steps/s = 1.62, tokens/s = 43246 (43246 target) ; Learning rate = 0.000293 ; Loss = 1.499337\n",
      "2024-11-30 20:16:01.530000: I runner.py:310] Step = 90800 ; steps/s = 1.63, tokens/s = 42835 (42835 target) ; Learning rate = 0.000293 ; Loss = 1.499858\n",
      "2024-11-30 20:17:03.273000: I runner.py:310] Step = 90900 ; steps/s = 1.62, tokens/s = 43231 (43231 target) ; Learning rate = 0.000293 ; Loss = 1.502262\n",
      "2024-11-30 20:18:04.982000: I runner.py:310] Step = 91000 ; steps/s = 1.62, tokens/s = 43310 (43310 target) ; Learning rate = 0.000293 ; Loss = 1.504248\n",
      "2024-11-30 20:19:06.723000: I runner.py:310] Step = 91100 ; steps/s = 1.62, tokens/s = 43150 (43150 target) ; Learning rate = 0.000293 ; Loss = 1.520667\n",
      "2024-11-30 20:20:07.982000: I runner.py:310] Step = 91200 ; steps/s = 1.63, tokens/s = 42905 (42905 target) ; Learning rate = 0.000293 ; Loss = 1.498439\n",
      "2024-11-30 20:21:09.827000: I runner.py:310] Step = 91300 ; steps/s = 1.62, tokens/s = 43200 (43200 target) ; Learning rate = 0.000293 ; Loss = 1.505234\n",
      "2024-11-30 20:22:11.616000: I runner.py:310] Step = 91400 ; steps/s = 1.62, tokens/s = 43244 (43244 target) ; Learning rate = 0.000292 ; Loss = 1.501076\n",
      "2024-11-30 20:23:12.859000: I runner.py:310] Step = 91500 ; steps/s = 1.63, tokens/s = 42780 (42780 target) ; Learning rate = 0.000292 ; Loss = 1.498159\n",
      "2024-11-30 20:24:14.603000: I runner.py:310] Step = 91600 ; steps/s = 1.62, tokens/s = 43292 (43292 target) ; Learning rate = 0.000292 ; Loss = 1.503191\n",
      "2024-11-30 20:25:16.381000: I runner.py:310] Step = 91700 ; steps/s = 1.62, tokens/s = 43210 (43210 target) ; Learning rate = 0.000292 ; Loss = 1.502671\n",
      "2024-11-30 20:26:18.171000: I runner.py:310] Step = 91800 ; steps/s = 1.62, tokens/s = 43244 (43244 target) ; Learning rate = 0.000292 ; Loss = 1.500695\n",
      "2024-11-30 20:27:19.302000: I runner.py:310] Step = 91900 ; steps/s = 1.64, tokens/s = 42888 (42888 target) ; Learning rate = 0.000292 ; Loss = 1.503548\n",
      "2024-11-30 20:28:21.084000: I runner.py:310] Step = 92000 ; steps/s = 1.62, tokens/s = 43257 (43257 target) ; Learning rate = 0.000291 ; Loss = 1.501087\n",
      "2024-11-30 20:29:22.834000: I runner.py:310] Step = 92100 ; steps/s = 1.62, tokens/s = 43247 (43247 target) ; Learning rate = 0.000291 ; Loss = 1.500139\n",
      "2024-11-30 20:30:24.636000: I runner.py:310] Step = 92200 ; steps/s = 1.62, tokens/s = 43262 (43262 target) ; Learning rate = 0.000291 ; Loss = 1.508036\n",
      "2024-11-30 20:31:25.889000: I runner.py:310] Step = 92300 ; steps/s = 1.63, tokens/s = 42753 (42753 target) ; Learning rate = 0.000291 ; Loss = 1.507265\n",
      "2024-11-30 20:32:27.740000: I runner.py:310] Step = 92400 ; steps/s = 1.62, tokens/s = 43220 (43220 target) ; Learning rate = 0.000291 ; Loss = 1.499509\n",
      "2024-11-30 20:33:29.580000: I runner.py:310] Step = 92500 ; steps/s = 1.62, tokens/s = 43170 (43170 target) ; Learning rate = 0.000291 ; Loss = 1.498679\n",
      "2024-11-30 20:34:31.369000: I runner.py:310] Step = 92600 ; steps/s = 1.62, tokens/s = 43253 (43253 target) ; Learning rate = 0.000290 ; Loss = 1.498399\n",
      "2024-11-30 20:35:32.656000: I runner.py:310] Step = 92700 ; steps/s = 1.63, tokens/s = 42754 (42754 target) ; Learning rate = 0.000290 ; Loss = 1.501729\n",
      "2024-11-30 20:36:34.478000: I runner.py:310] Step = 92800 ; steps/s = 1.62, tokens/s = 43224 (43224 target) ; Learning rate = 0.000290 ; Loss = 1.498526\n",
      "2024-11-30 20:37:36.246000: I runner.py:310] Step = 92900 ; steps/s = 1.62, tokens/s = 43263 (43263 target) ; Learning rate = 0.000290 ; Loss = 1.496523\n",
      "2024-11-30 20:38:38.019000: I runner.py:310] Step = 93000 ; steps/s = 1.62, tokens/s = 43264 (43264 target) ; Learning rate = 0.000290 ; Loss = 1.493505\n",
      "2024-11-30 20:39:39.211000: I runner.py:310] Step = 93100 ; steps/s = 1.63, tokens/s = 42813 (42813 target) ; Learning rate = 0.000290 ; Loss = 1.493379\n",
      "2024-11-30 20:40:41.030000: I runner.py:310] Step = 93200 ; steps/s = 1.62, tokens/s = 43237 (43237 target) ; Learning rate = 0.000290 ; Loss = 1.506220\n",
      "2024-11-30 20:41:42.838000: I runner.py:310] Step = 93300 ; steps/s = 1.62, tokens/s = 43222 (43222 target) ; Learning rate = 0.000289 ; Loss = 1.500961\n",
      "2024-11-30 20:42:44.657000: I runner.py:310] Step = 93400 ; steps/s = 1.62, tokens/s = 43196 (43196 target) ; Learning rate = 0.000289 ; Loss = 1.505162\n",
      "2024-11-30 20:43:45.897000: I runner.py:310] Step = 93500 ; steps/s = 1.63, tokens/s = 42804 (42804 target) ; Learning rate = 0.000289 ; Loss = 1.489829\n",
      "2024-11-30 20:44:47.727000: I runner.py:310] Step = 93600 ; steps/s = 1.62, tokens/s = 43217 (43217 target) ; Learning rate = 0.000289 ; Loss = 1.500851\n",
      "2024-11-30 20:45:49.495000: I runner.py:310] Step = 93700 ; steps/s = 1.62, tokens/s = 43258 (43258 target) ; Learning rate = 0.000289 ; Loss = 1.503098\n",
      "2024-11-30 20:46:51.309000: I runner.py:310] Step = 93800 ; steps/s = 1.62, tokens/s = 43186 (43186 target) ; Learning rate = 0.000289 ; Loss = 1.504191\n",
      "2024-11-30 20:47:52.549000: I runner.py:310] Step = 93900 ; steps/s = 1.63, tokens/s = 42804 (42804 target) ; Learning rate = 0.000288 ; Loss = 1.497919\n",
      "2024-11-30 20:48:54.280000: I runner.py:310] Step = 94000 ; steps/s = 1.62, tokens/s = 43265 (43265 target) ; Learning rate = 0.000288 ; Loss = 1.499981\n",
      "2024-11-30 20:49:56.069000: I runner.py:310] Step = 94100 ; steps/s = 1.62, tokens/s = 43263 (43263 target) ; Learning rate = 0.000288 ; Loss = 1.502663\n",
      "2024-11-30 20:50:57.839000: I runner.py:310] Step = 94200 ; steps/s = 1.62, tokens/s = 43240 (43240 target) ; Learning rate = 0.000288 ; Loss = 1.498608\n",
      "2024-11-30 20:51:59.125000: I runner.py:310] Step = 94300 ; steps/s = 1.63, tokens/s = 42790 (42790 target) ; Learning rate = 0.000288 ; Loss = 1.497626\n",
      "2024-11-30 20:53:00.897000: I runner.py:310] Step = 94400 ; steps/s = 1.62, tokens/s = 43194 (43194 target) ; Learning rate = 0.000288 ; Loss = 1.503070\n",
      "2024-11-30 20:54:02.715000: I runner.py:310] Step = 94500 ; steps/s = 1.62, tokens/s = 43229 (43229 target) ; Learning rate = 0.000288 ; Loss = 1.512749\n",
      "2024-11-30 20:55:04.577000: I runner.py:310] Step = 94600 ; steps/s = 1.62, tokens/s = 43208 (43208 target) ; Learning rate = 0.000287 ; Loss = 1.504170\n",
      "2024-11-30 20:56:05.861000: I runner.py:310] Step = 94700 ; steps/s = 1.63, tokens/s = 42757 (42757 target) ; Learning rate = 0.000287 ; Loss = 1.503917\n",
      "2024-11-30 20:57:07.685000: I runner.py:310] Step = 94800 ; steps/s = 1.62, tokens/s = 43218 (43218 target) ; Learning rate = 0.000287 ; Loss = 1.490415\n",
      "2024-11-30 20:58:09.468000: I runner.py:310] Step = 94900 ; steps/s = 1.62, tokens/s = 43245 (43245 target) ; Learning rate = 0.000287 ; Loss = 1.501491\n",
      "2024-11-30 20:59:11.292000: I runner.py:310] Step = 95000 ; steps/s = 1.62, tokens/s = 43220 (43220 target) ; Learning rate = 0.000287 ; Loss = 1.503935\n",
      "2024-11-30 20:59:11.294000: I training.py:192] Running evaluation for step 95000\n",
      "2024-11-30 21:03:34.651000: I training.py:192] Evaluation result for step 95000: loss = 1.213809 ; perplexity = 3.366281\n",
      "2024-11-30 21:04:35.843000: I runner.py:310] Step = 95100 ; steps/s = 1.63, tokens/s = 42830 (42830 target) ; Learning rate = 0.000287 ; Loss = 1.494468\n",
      "2024-11-30 21:05:37.641000: I runner.py:310] Step = 95200 ; steps/s = 1.62, tokens/s = 43251 (43251 target) ; Learning rate = 0.000286 ; Loss = 1.502976\n",
      "2024-11-30 21:06:39.477000: I runner.py:310] Step = 95300 ; steps/s = 1.62, tokens/s = 43203 (43203 target) ; Learning rate = 0.000286 ; Loss = 1.501782\n",
      "2024-11-30 21:07:41.207000: I runner.py:310] Step = 95400 ; steps/s = 1.62, tokens/s = 43273 (43273 target) ; Learning rate = 0.000286 ; Loss = 1.502447\n",
      "2024-11-30 21:08:42.380000: I runner.py:310] Step = 95500 ; steps/s = 1.63, tokens/s = 42843 (42843 target) ; Learning rate = 0.000286 ; Loss = 1.499919\n",
      "2024-11-30 21:09:44.165000: I runner.py:310] Step = 95600 ; steps/s = 1.62, tokens/s = 43260 (43260 target) ; Learning rate = 0.000286 ; Loss = 1.497273\n",
      "2024-11-30 21:10:46.009000: I runner.py:310] Step = 95700 ; steps/s = 1.62, tokens/s = 43174 (43174 target) ; Learning rate = 0.000286 ; Loss = 1.503255\n",
      "2024-11-30 21:11:47.230000: I runner.py:310] Step = 95800 ; steps/s = 1.63, tokens/s = 42805 (42805 target) ; Learning rate = 0.000286 ; Loss = 1.498544\n",
      "2024-11-30 21:12:49.001000: I runner.py:310] Step = 95900 ; steps/s = 1.62, tokens/s = 43276 (43276 target) ; Learning rate = 0.000285 ; Loss = 1.491635\n",
      "2024-11-30 21:13:50.767000: I runner.py:310] Step = 96000 ; steps/s = 1.62, tokens/s = 43284 (43284 target) ; Learning rate = 0.000285 ; Loss = 1.495780\n",
      "2024-11-30 21:14:52.541000: I runner.py:310] Step = 96100 ; steps/s = 1.62, tokens/s = 43237 (43237 target) ; Learning rate = 0.000285 ; Loss = 1.499582\n",
      "2024-11-30 21:15:53.765000: I runner.py:310] Step = 96200 ; steps/s = 1.63, tokens/s = 42767 (42767 target) ; Learning rate = 0.000285 ; Loss = 1.501785\n",
      "2024-11-30 21:16:55.503000: I runner.py:310] Step = 96300 ; steps/s = 1.62, tokens/s = 43293 (43293 target) ; Learning rate = 0.000285 ; Loss = 1.498765\n",
      "2024-11-30 21:17:57.267000: I runner.py:310] Step = 96400 ; steps/s = 1.62, tokens/s = 43263 (43263 target) ; Learning rate = 0.000285 ; Loss = 1.498849\n",
      "2024-11-30 21:18:59.091000: I runner.py:310] Step = 96500 ; steps/s = 1.62, tokens/s = 43197 (43197 target) ; Learning rate = 0.000285 ; Loss = 1.498834\n",
      "2024-11-30 21:20:00.369000: I runner.py:310] Step = 96600 ; steps/s = 1.63, tokens/s = 42790 (42790 target) ; Learning rate = 0.000284 ; Loss = 1.498274\n",
      "2024-11-30 21:21:02.147000: I runner.py:310] Step = 96700 ; steps/s = 1.62, tokens/s = 43209 (43209 target) ; Learning rate = 0.000284 ; Loss = 1.495507\n",
      "2024-11-30 21:22:03.978000: I runner.py:310] Step = 96800 ; steps/s = 1.62, tokens/s = 43196 (43196 target) ; Learning rate = 0.000284 ; Loss = 1.497934\n",
      "2024-11-30 21:23:05.784000: I runner.py:310] Step = 96900 ; steps/s = 1.62, tokens/s = 43264 (43264 target) ; Learning rate = 0.000284 ; Loss = 1.496521\n",
      "2024-11-30 21:24:07.084000: I runner.py:310] Step = 97000 ; steps/s = 1.63, tokens/s = 42747 (42747 target) ; Learning rate = 0.000284 ; Loss = 1.504937\n",
      "2024-11-30 21:25:08.939000: I runner.py:310] Step = 97100 ; steps/s = 1.62, tokens/s = 43198 (43198 target) ; Learning rate = 0.000284 ; Loss = 1.502811\n",
      "2024-11-30 21:26:10.795000: I runner.py:310] Step = 97200 ; steps/s = 1.62, tokens/s = 43217 (43217 target) ; Learning rate = 0.000284 ; Loss = 1.494698\n",
      "2024-11-30 21:27:12.572000: I runner.py:310] Step = 97300 ; steps/s = 1.62, tokens/s = 43253 (43253 target) ; Learning rate = 0.000283 ; Loss = 1.497148\n",
      "2024-11-30 21:28:13.797000: I runner.py:310] Step = 97400 ; steps/s = 1.63, tokens/s = 42792 (42792 target) ; Learning rate = 0.000283 ; Loss = 1.501854\n",
      "2024-11-30 21:29:15.558000: I runner.py:310] Step = 97500 ; steps/s = 1.62, tokens/s = 43270 (43270 target) ; Learning rate = 0.000283 ; Loss = 1.493287\n",
      "2024-11-30 21:30:17.312000: I runner.py:310] Step = 97600 ; steps/s = 1.62, tokens/s = 43247 (43247 target) ; Learning rate = 0.000283 ; Loss = 1.498418\n",
      "2024-11-30 21:31:19.153000: I runner.py:310] Step = 97700 ; steps/s = 1.62, tokens/s = 43180 (43180 target) ; Learning rate = 0.000283 ; Loss = 1.500004\n",
      "2024-11-30 21:32:20.337000: I runner.py:310] Step = 97800 ; steps/s = 1.63, tokens/s = 42845 (42845 target) ; Learning rate = 0.000283 ; Loss = 1.505965\n",
      "2024-11-30 21:33:22.111000: I runner.py:310] Step = 97900 ; steps/s = 1.62, tokens/s = 43243 (43243 target) ; Learning rate = 0.000282 ; Loss = 1.500737\n",
      "2024-11-30 21:34:23.908000: I runner.py:310] Step = 98000 ; steps/s = 1.62, tokens/s = 43232 (43232 target) ; Learning rate = 0.000282 ; Loss = 1.491272\n",
      "2024-11-30 21:35:25.707000: I runner.py:310] Step = 98100 ; steps/s = 1.62, tokens/s = 43236 (43236 target) ; Learning rate = 0.000282 ; Loss = 1.497294\n",
      "2024-11-30 21:36:26.946000: I runner.py:310] Step = 98200 ; steps/s = 1.63, tokens/s = 42812 (42812 target) ; Learning rate = 0.000282 ; Loss = 1.495438\n",
      "2024-11-30 21:37:28.715000: I runner.py:310] Step = 98300 ; steps/s = 1.62, tokens/s = 43233 (43233 target) ; Learning rate = 0.000282 ; Loss = 1.495506\n",
      "2024-11-30 21:38:30.538000: I runner.py:310] Step = 98400 ; steps/s = 1.62, tokens/s = 43212 (43212 target) ; Learning rate = 0.000282 ; Loss = 1.502214\n",
      "2024-11-30 21:39:32.351000: I runner.py:310] Step = 98500 ; steps/s = 1.62, tokens/s = 43214 (43214 target) ; Learning rate = 0.000282 ; Loss = 1.504242\n",
      "2024-11-30 21:40:33.630000: I runner.py:310] Step = 98600 ; steps/s = 1.63, tokens/s = 42780 (42780 target) ; Learning rate = 0.000281 ; Loss = 1.484857\n",
      "2024-11-30 21:41:35.495000: I runner.py:310] Step = 98700 ; steps/s = 1.62, tokens/s = 43174 (43174 target) ; Learning rate = 0.000281 ; Loss = 1.496269\n",
      "2024-11-30 21:42:37.303000: I runner.py:310] Step = 98800 ; steps/s = 1.62, tokens/s = 43238 (43238 target) ; Learning rate = 0.000281 ; Loss = 1.500035\n",
      "2024-11-30 21:43:39.164000: I runner.py:310] Step = 98900 ; steps/s = 1.62, tokens/s = 43191 (43191 target) ; Learning rate = 0.000281 ; Loss = 1.494369\n",
      "2024-11-30 21:44:40.463000: I runner.py:310] Step = 99000 ; steps/s = 1.63, tokens/s = 42769 (42769 target) ; Learning rate = 0.000281 ; Loss = 1.502845\n",
      "2024-11-30 21:45:42.301000: I runner.py:310] Step = 99100 ; steps/s = 1.62, tokens/s = 43217 (43217 target) ; Learning rate = 0.000281 ; Loss = 1.497101\n",
      "2024-11-30 21:46:44.133000: I runner.py:310] Step = 99200 ; steps/s = 1.62, tokens/s = 43211 (43211 target) ; Learning rate = 0.000281 ; Loss = 1.498224\n",
      "2024-11-30 21:47:45.926000: I runner.py:310] Step = 99300 ; steps/s = 1.62, tokens/s = 43208 (43208 target) ; Learning rate = 0.000280 ; Loss = 1.495401\n",
      "2024-11-30 21:48:47.255000: I runner.py:310] Step = 99400 ; steps/s = 1.63, tokens/s = 42747 (42747 target) ; Learning rate = 0.000280 ; Loss = 1.489553\n",
      "2024-11-30 21:49:49.014000: I runner.py:310] Step = 99500 ; steps/s = 1.62, tokens/s = 43242 (43242 target) ; Learning rate = 0.000280 ; Loss = 1.500020\n",
      "2024-11-30 21:50:50.775000: I runner.py:310] Step = 99600 ; steps/s = 1.62, tokens/s = 43252 (43252 target) ; Learning rate = 0.000280 ; Loss = 1.503937\n",
      "2024-11-30 21:51:52.532000: I runner.py:310] Step = 99700 ; steps/s = 1.62, tokens/s = 43259 (43259 target) ; Learning rate = 0.000280 ; Loss = 1.501898\n",
      "2024-11-30 21:52:53.778000: I runner.py:310] Step = 99800 ; steps/s = 1.63, tokens/s = 42831 (42831 target) ; Learning rate = 0.000280 ; Loss = 1.497647\n",
      "2024-11-30 21:53:55.564000: I runner.py:310] Step = 99900 ; steps/s = 1.62, tokens/s = 43227 (43227 target) ; Learning rate = 0.000280 ; Loss = 1.497878\n",
      "2024-11-30 21:54:57.329000: I runner.py:310] Step = 100000 ; steps/s = 1.62, tokens/s = 43237 (43237 target) ; Learning rate = 0.000280 ; Loss = 1.498766\n",
      "2024-11-30 21:54:59.473000: I training.py:176] Saved checkpoint POS_KK_TR_EN/ckpt-100000\n",
      "2024-11-30 21:54:59.473000: I training.py:192] Running evaluation for step 100000\n",
      "2024-11-30 21:59:18.908000: I training.py:192] Evaluation result for step 100000: loss = 1.219518 ; perplexity = 3.385555\n",
      "2024-11-30 22:00:20.058000: I runner.py:310] Step = 100100 ; steps/s = 1.64, tokens/s = 42853 (42853 target) ; Learning rate = 0.000279 ; Loss = 1.500883\n",
      "2024-11-30 22:01:21.807000: I runner.py:310] Step = 100200 ; steps/s = 1.62, tokens/s = 43311 (43311 target) ; Learning rate = 0.000279 ; Loss = 1.493024\n",
      "2024-11-30 22:02:23.619000: I runner.py:310] Step = 100300 ; steps/s = 1.62, tokens/s = 43199 (43199 target) ; Learning rate = 0.000279 ; Loss = 1.500678\n",
      "2024-11-30 22:03:25.369000: I runner.py:310] Step = 100400 ; steps/s = 1.62, tokens/s = 43229 (43229 target) ; Learning rate = 0.000279 ; Loss = 1.506910\n",
      "2024-11-30 22:04:26.608000: I runner.py:310] Step = 100500 ; steps/s = 1.63, tokens/s = 42818 (42818 target) ; Learning rate = 0.000279 ; Loss = 1.495394\n",
      "2024-11-30 22:05:28.443000: I runner.py:310] Step = 100600 ; steps/s = 1.62, tokens/s = 43221 (43221 target) ; Learning rate = 0.000279 ; Loss = 1.496729\n",
      "2024-11-30 22:06:30.275000: I runner.py:310] Step = 100700 ; steps/s = 1.62, tokens/s = 43194 (43194 target) ; Learning rate = 0.000279 ; Loss = 1.496536\n",
      "2024-11-30 22:07:32.116000: I runner.py:310] Step = 100800 ; steps/s = 1.62, tokens/s = 43204 (43204 target) ; Learning rate = 0.000278 ; Loss = 1.499165\n",
      "2024-11-30 22:08:33.350000: I runner.py:310] Step = 100900 ; steps/s = 1.63, tokens/s = 42797 (42797 target) ; Learning rate = 0.000278 ; Loss = 1.495296\n",
      "2024-11-30 22:09:35.186000: I runner.py:310] Step = 101000 ; steps/s = 1.62, tokens/s = 43179 (43179 target) ; Learning rate = 0.000278 ; Loss = 1.491347\n",
      "2024-11-30 22:10:36.976000: I runner.py:310] Step = 101100 ; steps/s = 1.62, tokens/s = 43255 (43255 target) ; Learning rate = 0.000278 ; Loss = 1.496332\n",
      "2024-11-30 22:11:38.756000: I runner.py:310] Step = 101200 ; steps/s = 1.62, tokens/s = 43252 (43252 target) ; Learning rate = 0.000278 ; Loss = 1.494752\n",
      "2024-11-30 22:12:40.279000: I runner.py:310] Step = 101300 ; steps/s = 1.63, tokens/s = 42580 (42580 target) ; Learning rate = 0.000278 ; Loss = 1.495281\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "#Kk-En (POS Tags) -> Tr-En (Tatoeba)(POS Tags)\n",
    "!onmt-main --model kk-tr-en-modelim.py --config kk-tr-en-pos-asl.yml --auto_config train --with_eval --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbacf489-b87a-42f5-b94a-e2c16a6bb6b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-30 22:15:22.840054: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 22:15:23.628115: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-30 22:15:23.628190: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-30 22:15:23.628198: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-11-30 22:15:24.626000: I onmt-main:8] Creating model directory POS_KK_TR_EN_2\n",
      "2024-11-30 22:15:24.833000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-11-30 22:15:24.833000: I main.py:315] Using model:\n",
      "(model): MultiFeaturesTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): ParallelInputter(\n",
      "      (inputters): ListWrapper(\n",
      "        (0): WordEmbedder()\n",
      "        (1): WordEmbedder()\n",
      "      )\n",
      "      (reducer): ConcatReducer()\n",
      "    )\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): ParallelInputter(\n",
      "        (inputters): ListWrapper(\n",
      "          (0): WordEmbedder()\n",
      "          (1): WordEmbedder()\n",
      "        )\n",
      "        (reducer): ConcatReducer()\n",
      "      )\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-11-30 22:15:24.836224: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 22:15:26.386512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-11-30 22:15:26.387118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7740 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-11-30 22:15:26.387526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 6099 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "2024-11-30 22:15:26.391000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file:\n",
      "  - Tatoeba_tokens_dev_shared\n",
      "  - Tatoeba_pos_tags_dev_shared.txt\n",
      "  eval_labels_file: Tatoeba_dev_target_tokens_shared.txt\n",
      "  source_1_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  source_2_vocabulary: Zemberek_TR_unique_pos\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file:\n",
      "  - Tatoeba_tokens_train_shared\n",
      "  - Tatoeba_pos_tags_train_shared.txt\n",
      "  train_labels_file: Tatoeba_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: POS_KK_TR_EN_2\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 205000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-11-30 22:15:26.729000: I inputter.py:316] Initialized source_1 input layer:\n",
      "2024-11-30 22:15:26.729000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-11-30 22:15:26.729000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-11-30 22:15:26.733000: I inputter.py:316] Initialized source_2 input layer:\n",
      "2024-11-30 22:15:26.733000: I inputter.py:316]  - vocabulary size: 21\n",
      "2024-11-30 22:15:26.733000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-11-30 22:15:26.804000: I inputter.py:316] Initialized target input layer:\n",
      "2024-11-30 22:15:26.804000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-11-30 22:15:26.804000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-11-30 22:15:26.828000: I runner.py:269] Restored checkpoint POS_KK_TR_EN/ckpt-100000\n",
      "2024-11-30 22:15:26.830000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "2024-11-30 22:15:26.884000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-11-30 22:15:27.982887: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-11-30 22:15:28.207000: I main.py:325] Accumulate gradients of 7 iterations to reach effective batch size of 25000\n",
      "2024-11-30 22:15:28.329000: I mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2024-11-30 22:15:28.478000: I dataset_ops.py:2542] Training on 647485 examples\n",
      "2024-11-30 22:16:33.435602: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-11-30 22:16:34.420726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-11-30 22:16:34.733387: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-11-30 22:16:43.751000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-30 22:16:43.773000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-30 22:16:45.315000: I cross_device_ops.py:897] batch_all_reduce: 261 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-11-30 22:16:50.298000: I cross_device_ops.py:897] batch_all_reduce: 261 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-11-30 22:16:57.009000: I runner.py:310] Number of model parameters: 93357261\n",
      "2024-11-30 22:16:57.013000: I runner.py:310] Number of model weights: 261 (trainable = 261, non trainable = 0)\n",
      "2024-11-30 22:16:57.049000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-30 22:16:59.046000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-1\n",
      "2024-11-30 22:16:59.790000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-30 22:16:59.815000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-30 22:17:00.507000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-30 22:17:00.534000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-30 22:17:01.139000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-30 22:17:01.163000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-30 22:17:01.766000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-11-30 22:18:00.620000: I runner.py:310] Step = 100 ; steps/s = 1.62, tokens/s = 43071 (43071 target) ; Learning rate = 0.000009 ; Loss = 6.947912\n",
      "2024-11-30 22:19:01.842000: I runner.py:310] Step = 200 ; steps/s = 1.63, tokens/s = 43561 (43561 target) ; Learning rate = 0.000018 ; Loss = 5.561613\n",
      "2024-11-30 22:20:02.869000: I runner.py:310] Step = 300 ; steps/s = 1.64, tokens/s = 42885 (42885 target) ; Learning rate = 0.000027 ; Loss = 5.160065\n",
      "2024-11-30 22:21:04.013000: I runner.py:310] Step = 400 ; steps/s = 1.64, tokens/s = 43617 (43617 target) ; Learning rate = 0.000035 ; Loss = 4.881313\n",
      "2024-11-30 22:22:05.126000: I runner.py:310] Step = 500 ; steps/s = 1.64, tokens/s = 43637 (43637 target) ; Learning rate = 0.000044 ; Loss = 4.663108\n",
      "2024-11-30 22:23:05.639000: I runner.py:310] Step = 600 ; steps/s = 1.65, tokens/s = 43262 (43262 target) ; Learning rate = 0.000053 ; Loss = 4.336116\n",
      "2024-11-30 22:24:06.684000: I runner.py:310] Step = 700 ; steps/s = 1.64, tokens/s = 43675 (43675 target) ; Learning rate = 0.000062 ; Loss = 3.976036\n",
      "2024-11-30 22:25:07.242000: I runner.py:310] Step = 800 ; steps/s = 1.65, tokens/s = 43208 (43208 target) ; Learning rate = 0.000071 ; Loss = 3.455456\n",
      "2024-11-30 22:26:08.374000: I runner.py:310] Step = 900 ; steps/s = 1.64, tokens/s = 43621 (43621 target) ; Learning rate = 0.000080 ; Loss = 3.151729\n",
      "2024-11-30 22:27:09.431000: I runner.py:310] Step = 1000 ; steps/s = 1.64, tokens/s = 43678 (43678 target) ; Learning rate = 0.000088 ; Loss = 3.043244\n",
      "2024-11-30 22:28:10.029000: I runner.py:310] Step = 1100 ; steps/s = 1.65, tokens/s = 43189 (43189 target) ; Learning rate = 0.000097 ; Loss = 2.882485\n",
      "2024-11-30 22:29:11.058000: I runner.py:310] Step = 1200 ; steps/s = 1.64, tokens/s = 43695 (43695 target) ; Learning rate = 0.000106 ; Loss = 2.993358\n",
      "2024-11-30 22:30:11.628000: I runner.py:310] Step = 1300 ; steps/s = 1.65, tokens/s = 43207 (43207 target) ; Learning rate = 0.000115 ; Loss = 2.603195\n",
      "2024-11-30 22:31:12.642000: I runner.py:310] Step = 1400 ; steps/s = 1.64, tokens/s = 43713 (43713 target) ; Learning rate = 0.000124 ; Loss = 2.579501\n",
      "2024-11-30 22:32:13.629000: I runner.py:310] Step = 1500 ; steps/s = 1.64, tokens/s = 43729 (43729 target) ; Learning rate = 0.000133 ; Loss = 2.561048\n",
      "2024-11-30 22:33:14.168000: I runner.py:310] Step = 1600 ; steps/s = 1.65, tokens/s = 43220 (43220 target) ; Learning rate = 0.000142 ; Loss = 2.496612\n",
      "2024-11-30 22:34:15.204000: I runner.py:310] Step = 1700 ; steps/s = 1.64, tokens/s = 43697 (43697 target) ; Learning rate = 0.000150 ; Loss = 2.420615\n",
      "2024-11-30 22:35:16.236000: I runner.py:310] Step = 1800 ; steps/s = 1.64, tokens/s = 43689 (43689 target) ; Learning rate = 0.000159 ; Loss = 2.439751\n",
      "2024-11-30 22:36:16.739000: I runner.py:310] Step = 1900 ; steps/s = 1.65, tokens/s = 43252 (43252 target) ; Learning rate = 0.000168 ; Loss = 2.349468\n",
      "2024-11-30 22:37:17.724000: I runner.py:310] Step = 2000 ; steps/s = 1.64, tokens/s = 43728 (43728 target) ; Learning rate = 0.000177 ; Loss = 2.362536\n",
      "2024-11-30 22:38:18.242000: I runner.py:310] Step = 2100 ; steps/s = 1.65, tokens/s = 43242 (43242 target) ; Learning rate = 0.000186 ; Loss = 2.377177\n",
      "2024-11-30 22:39:19.263000: I runner.py:310] Step = 2200 ; steps/s = 1.64, tokens/s = 43707 (43707 target) ; Learning rate = 0.000195 ; Loss = 2.327426\n",
      "2024-11-30 22:40:20.327000: I runner.py:310] Step = 2300 ; steps/s = 1.64, tokens/s = 43666 (43666 target) ; Learning rate = 0.000203 ; Loss = 2.223336\n",
      "2024-11-30 22:41:20.924000: I runner.py:310] Step = 2400 ; steps/s = 1.65, tokens/s = 43185 (43185 target) ; Learning rate = 0.000212 ; Loss = 2.229621\n",
      "2024-11-30 22:42:21.984000: I runner.py:310] Step = 2500 ; steps/s = 1.64, tokens/s = 43664 (43664 target) ; Learning rate = 0.000221 ; Loss = 2.237187\n",
      "2024-11-30 22:43:22.553000: I runner.py:310] Step = 2600 ; steps/s = 1.65, tokens/s = 43210 (43210 target) ; Learning rate = 0.000230 ; Loss = 2.143371\n",
      "2024-11-30 22:44:23.579000: I runner.py:310] Step = 2700 ; steps/s = 1.64, tokens/s = 43694 (43694 target) ; Learning rate = 0.000239 ; Loss = 2.144321\n",
      "2024-11-30 22:45:24.591000: I runner.py:310] Step = 2800 ; steps/s = 1.64, tokens/s = 43705 (43705 target) ; Learning rate = 0.000248 ; Loss = 2.071190\n",
      "2024-11-30 22:46:25.150000: I runner.py:310] Step = 2900 ; steps/s = 1.65, tokens/s = 43227 (43227 target) ; Learning rate = 0.000256 ; Loss = 2.056788\n",
      "2024-11-30 22:47:26.171000: I runner.py:310] Step = 3000 ; steps/s = 1.64, tokens/s = 43704 (43704 target) ; Learning rate = 0.000265 ; Loss = 2.121627\n",
      "2024-11-30 22:48:27.127000: I runner.py:310] Step = 3100 ; steps/s = 1.64, tokens/s = 43746 (43746 target) ; Learning rate = 0.000274 ; Loss = 2.086795\n",
      "2024-11-30 22:49:27.692000: I runner.py:310] Step = 3200 ; steps/s = 1.65, tokens/s = 43211 (43211 target) ; Learning rate = 0.000283 ; Loss = 2.060708\n",
      "2024-11-30 22:50:28.714000: I runner.py:310] Step = 3300 ; steps/s = 1.64, tokens/s = 43695 (43695 target) ; Learning rate = 0.000292 ; Loss = 2.022026\n",
      "2024-11-30 22:51:29.252000: I runner.py:310] Step = 3400 ; steps/s = 1.65, tokens/s = 43237 (43237 target) ; Learning rate = 0.000301 ; Loss = 1.963512\n",
      "2024-11-30 22:52:30.249000: I runner.py:310] Step = 3500 ; steps/s = 1.64, tokens/s = 43715 (43715 target) ; Learning rate = 0.000309 ; Loss = 2.011620\n",
      "2024-11-30 22:53:31.242000: I runner.py:310] Step = 3600 ; steps/s = 1.64, tokens/s = 43717 (43717 target) ; Learning rate = 0.000318 ; Loss = 2.009944\n",
      "2024-11-30 22:54:31.742000: I runner.py:310] Step = 3700 ; steps/s = 1.65, tokens/s = 43250 (43250 target) ; Learning rate = 0.000327 ; Loss = 1.988364\n",
      "2024-11-30 22:55:32.732000: I runner.py:310] Step = 3800 ; steps/s = 1.64, tokens/s = 43717 (43717 target) ; Learning rate = 0.000336 ; Loss = 1.952463\n",
      "2024-11-30 22:56:33.326000: I runner.py:310] Step = 3900 ; steps/s = 1.65, tokens/s = 43202 (43202 target) ; Learning rate = 0.000345 ; Loss = 1.937503\n",
      "2024-11-30 22:57:34.325000: I runner.py:310] Step = 4000 ; steps/s = 1.64, tokens/s = 43719 (43719 target) ; Learning rate = 0.000354 ; Loss = 1.889767\n",
      "2024-11-30 22:58:35.327000: I runner.py:310] Step = 4100 ; steps/s = 1.64, tokens/s = 43710 (43710 target) ; Learning rate = 0.000362 ; Loss = 1.930713\n",
      "2024-11-30 22:59:35.926000: I runner.py:310] Step = 4200 ; steps/s = 1.65, tokens/s = 43191 (43191 target) ; Learning rate = 0.000371 ; Loss = 1.858036\n",
      "2024-11-30 23:00:36.951000: I runner.py:310] Step = 4300 ; steps/s = 1.64, tokens/s = 43707 (43707 target) ; Learning rate = 0.000380 ; Loss = 1.981498\n",
      "2024-11-30 23:01:37.947000: I runner.py:310] Step = 4400 ; steps/s = 1.64, tokens/s = 43707 (43707 target) ; Learning rate = 0.000389 ; Loss = 1.917369\n",
      "2024-11-30 23:02:38.472000: I runner.py:310] Step = 4500 ; steps/s = 1.65, tokens/s = 43247 (43247 target) ; Learning rate = 0.000398 ; Loss = 1.863944\n",
      "2024-11-30 23:03:39.466000: I runner.py:310] Step = 4600 ; steps/s = 1.64, tokens/s = 43720 (43720 target) ; Learning rate = 0.000407 ; Loss = 1.873294\n",
      "2024-11-30 23:04:40.028000: I runner.py:310] Step = 4700 ; steps/s = 1.65, tokens/s = 43210 (43210 target) ; Learning rate = 0.000416 ; Loss = 1.910606\n",
      "2024-11-30 23:05:40.931000: I runner.py:310] Step = 4800 ; steps/s = 1.64, tokens/s = 43787 (43787 target) ; Learning rate = 0.000424 ; Loss = 1.883901\n",
      "2024-11-30 23:06:41.928000: I runner.py:310] Step = 4900 ; steps/s = 1.64, tokens/s = 43711 (43711 target) ; Learning rate = 0.000433 ; Loss = 1.858807\n",
      "2024-11-30 23:07:42.489000: I runner.py:310] Step = 5000 ; steps/s = 1.65, tokens/s = 43216 (43216 target) ; Learning rate = 0.000442 ; Loss = 1.816400\n",
      "2024-11-30 23:07:42.490000: I training.py:192] Running evaluation for step 5000\n",
      "2024-11-30 23:08:39.813000: I training.py:192] Evaluation result for step 5000: loss = 0.685089 ; perplexity = 1.983948\n",
      "2024-11-30 23:09:40.928000: I runner.py:310] Step = 5100 ; steps/s = 1.64, tokens/s = 43640 (43640 target) ; Learning rate = 0.000451 ; Loss = 1.836837\n",
      "2024-11-30 23:10:41.634000: I runner.py:310] Step = 5200 ; steps/s = 1.65, tokens/s = 43108 (43108 target) ; Learning rate = 0.000460 ; Loss = 1.801571\n",
      "2024-11-30 23:11:42.778000: I runner.py:310] Step = 5300 ; steps/s = 1.64, tokens/s = 43608 (43608 target) ; Learning rate = 0.000469 ; Loss = 1.828876\n",
      "2024-11-30 23:12:43.924000: I runner.py:310] Step = 5400 ; steps/s = 1.64, tokens/s = 43608 (43608 target) ; Learning rate = 0.000477 ; Loss = 1.953633\n",
      "2024-11-30 23:13:44.592000: I runner.py:310] Step = 5500 ; steps/s = 1.65, tokens/s = 43141 (43141 target) ; Learning rate = 0.000486 ; Loss = 1.809944\n",
      "2024-11-30 23:14:45.731000: I runner.py:310] Step = 5600 ; steps/s = 1.64, tokens/s = 43628 (43628 target) ; Learning rate = 0.000495 ; Loss = 1.812268\n",
      "2024-11-30 23:15:46.930000: I runner.py:310] Step = 5700 ; steps/s = 1.63, tokens/s = 43566 (43566 target) ; Learning rate = 0.000504 ; Loss = 1.816533\n",
      "2024-11-30 23:16:47.618000: I runner.py:310] Step = 5800 ; steps/s = 1.65, tokens/s = 43123 (43123 target) ; Learning rate = 0.000513 ; Loss = 1.810974\n",
      "2024-11-30 23:17:48.714000: I runner.py:310] Step = 5900 ; steps/s = 1.64, tokens/s = 43653 (43653 target) ; Learning rate = 0.000522 ; Loss = 1.798484\n",
      "2024-11-30 23:18:49.361000: I runner.py:310] Step = 6000 ; steps/s = 1.65, tokens/s = 43146 (43146 target) ; Learning rate = 0.000530 ; Loss = 1.774995\n",
      "2024-11-30 23:19:50.428000: I runner.py:310] Step = 6100 ; steps/s = 1.64, tokens/s = 43682 (43682 target) ; Learning rate = 0.000539 ; Loss = 1.790199\n",
      "2024-11-30 23:20:51.582000: I runner.py:310] Step = 6200 ; steps/s = 1.64, tokens/s = 43598 (43598 target) ; Learning rate = 0.000548 ; Loss = 1.763854\n",
      "2024-11-30 23:21:52.216000: I runner.py:310] Step = 6300 ; steps/s = 1.65, tokens/s = 43165 (43165 target) ; Learning rate = 0.000557 ; Loss = 1.768988\n",
      "2024-11-30 23:22:53.304000: I runner.py:310] Step = 6400 ; steps/s = 1.64, tokens/s = 43664 (43664 target) ; Learning rate = 0.000566 ; Loss = 1.803208\n",
      "2024-11-30 23:23:53.957000: I runner.py:310] Step = 6500 ; steps/s = 1.65, tokens/s = 43137 (43137 target) ; Learning rate = 0.000575 ; Loss = 1.726930\n",
      "2024-11-30 23:24:55.072000: I runner.py:310] Step = 6600 ; steps/s = 1.64, tokens/s = 43641 (43641 target) ; Learning rate = 0.000583 ; Loss = 1.762020\n",
      "2024-11-30 23:25:56.127000: I runner.py:310] Step = 6700 ; steps/s = 1.64, tokens/s = 43670 (43670 target) ; Learning rate = 0.000592 ; Loss = 1.748886\n",
      "2024-11-30 23:26:56.784000: I runner.py:310] Step = 6800 ; steps/s = 1.65, tokens/s = 43154 (43154 target) ; Learning rate = 0.000601 ; Loss = 1.739849\n",
      "2024-11-30 23:27:57.870000: I runner.py:310] Step = 6900 ; steps/s = 1.64, tokens/s = 43663 (43663 target) ; Learning rate = 0.000610 ; Loss = 1.736536\n",
      "2024-11-30 23:28:59.017000: I runner.py:310] Step = 7000 ; steps/s = 1.64, tokens/s = 43601 (43601 target) ; Learning rate = 0.000619 ; Loss = 1.745716\n",
      "2024-11-30 23:29:59.644000: I runner.py:310] Step = 7100 ; steps/s = 1.65, tokens/s = 43165 (43165 target) ; Learning rate = 0.000628 ; Loss = 1.716624\n",
      "2024-11-30 23:31:00.658000: I runner.py:310] Step = 7200 ; steps/s = 1.64, tokens/s = 43700 (43700 target) ; Learning rate = 0.000636 ; Loss = 1.745424\n",
      "2024-11-30 23:32:01.242000: I runner.py:310] Step = 7300 ; steps/s = 1.65, tokens/s = 43202 (43202 target) ; Learning rate = 0.000645 ; Loss = 1.733314\n",
      "2024-11-30 23:33:02.297000: I runner.py:310] Step = 7400 ; steps/s = 1.64, tokens/s = 43688 (43688 target) ; Learning rate = 0.000654 ; Loss = 1.712702\n",
      "2024-11-30 23:34:03.379000: I runner.py:310] Step = 7500 ; steps/s = 1.64, tokens/s = 43645 (43645 target) ; Learning rate = 0.000663 ; Loss = 1.748958\n",
      "2024-11-30 23:35:03.994000: I runner.py:310] Step = 7600 ; steps/s = 1.65, tokens/s = 43180 (43180 target) ; Learning rate = 0.000672 ; Loss = 1.688961\n",
      "2024-11-30 23:36:04.934000: I runner.py:310] Step = 7700 ; steps/s = 1.64, tokens/s = 43758 (43758 target) ; Learning rate = 0.000681 ; Loss = 1.697662\n",
      "2024-11-30 23:37:05.532000: I runner.py:310] Step = 7800 ; steps/s = 1.65, tokens/s = 43182 (43182 target) ; Learning rate = 0.000690 ; Loss = 1.688584\n",
      "2024-11-30 23:38:06.595000: I runner.py:310] Step = 7900 ; steps/s = 1.64, tokens/s = 43671 (43671 target) ; Learning rate = 0.000698 ; Loss = 1.692253\n",
      "2024-11-30 23:39:07.711000: I runner.py:310] Step = 8000 ; steps/s = 1.64, tokens/s = 43629 (43629 target) ; Learning rate = 0.000707 ; Loss = 1.689176\n",
      "2024-11-30 23:40:08.316000: I runner.py:310] Step = 8100 ; steps/s = 1.65, tokens/s = 43192 (43192 target) ; Learning rate = 0.000716 ; Loss = 1.700055\n",
      "2024-11-30 23:41:09.340000: I runner.py:310] Step = 8200 ; steps/s = 1.64, tokens/s = 43704 (43704 target) ; Learning rate = 0.000725 ; Loss = 1.704277\n",
      "2024-11-30 23:42:10.385000: I runner.py:310] Step = 8300 ; steps/s = 1.64, tokens/s = 43674 (43674 target) ; Learning rate = 0.000734 ; Loss = 1.685493\n",
      "2024-11-30 23:43:10.867000: I runner.py:310] Step = 8400 ; steps/s = 1.65, tokens/s = 43274 (43274 target) ; Learning rate = 0.000743 ; Loss = 1.704069\n",
      "2024-11-30 23:44:11.955000: I runner.py:310] Step = 8500 ; steps/s = 1.64, tokens/s = 43643 (43643 target) ; Learning rate = 0.000751 ; Loss = 1.693785\n",
      "2024-11-30 23:45:12.575000: I runner.py:310] Step = 8600 ; steps/s = 1.65, tokens/s = 43179 (43179 target) ; Learning rate = 0.000760 ; Loss = 1.667949\n",
      "2024-11-30 23:46:13.690000: I runner.py:310] Step = 8700 ; steps/s = 1.64, tokens/s = 43626 (43626 target) ; Learning rate = 0.000769 ; Loss = 1.674757\n",
      "2024-11-30 23:47:14.753000: I runner.py:310] Step = 8800 ; steps/s = 1.64, tokens/s = 43676 (43676 target) ; Learning rate = 0.000778 ; Loss = 1.673890\n",
      "2024-11-30 23:48:15.311000: I runner.py:310] Step = 8900 ; steps/s = 1.65, tokens/s = 43223 (43223 target) ; Learning rate = 0.000787 ; Loss = 1.653217\n",
      "2024-11-30 23:49:16.338000: I runner.py:310] Step = 9000 ; steps/s = 1.64, tokens/s = 43688 (43688 target) ; Learning rate = 0.000796 ; Loss = 1.666853\n",
      "2024-11-30 23:50:16.887000: I runner.py:310] Step = 9100 ; steps/s = 1.65, tokens/s = 43218 (43218 target) ; Learning rate = 0.000804 ; Loss = 1.655540\n",
      "2024-11-30 23:51:17.979000: I runner.py:310] Step = 9200 ; steps/s = 1.64, tokens/s = 43655 (43655 target) ; Learning rate = 0.000813 ; Loss = 1.675001\n",
      "2024-11-30 23:52:19.031000: I runner.py:310] Step = 9300 ; steps/s = 1.64, tokens/s = 43669 (43669 target) ; Learning rate = 0.000822 ; Loss = 1.681752\n",
      "2024-11-30 23:53:19.681000: I runner.py:310] Step = 9400 ; steps/s = 1.65, tokens/s = 43159 (43159 target) ; Learning rate = 0.000831 ; Loss = 1.650078\n",
      "2024-11-30 23:54:20.712000: I runner.py:310] Step = 9500 ; steps/s = 1.64, tokens/s = 43686 (43686 target) ; Learning rate = 0.000840 ; Loss = 1.669698\n",
      "2024-11-30 23:55:21.759000: I runner.py:310] Step = 9600 ; steps/s = 1.64, tokens/s = 43687 (43687 target) ; Learning rate = 0.000849 ; Loss = 1.667636\n",
      "2024-11-30 23:56:22.341000: I runner.py:310] Step = 9700 ; steps/s = 1.65, tokens/s = 43207 (43207 target) ; Learning rate = 0.000857 ; Loss = 1.656025\n",
      "2024-11-30 23:57:23.413000: I runner.py:310] Step = 9800 ; steps/s = 1.64, tokens/s = 43659 (43659 target) ; Learning rate = 0.000866 ; Loss = 1.658026\n",
      "2024-11-30 23:58:24.012000: I runner.py:310] Step = 9900 ; steps/s = 1.65, tokens/s = 43185 (43185 target) ; Learning rate = 0.000875 ; Loss = 1.637529\n",
      "2024-11-30 23:59:25.072000: I runner.py:310] Step = 10000 ; steps/s = 1.64, tokens/s = 43668 (43668 target) ; Learning rate = 0.000884 ; Loss = 1.639262\n",
      "2024-11-30 23:59:26.850000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-10000\n",
      "2024-11-30 23:59:26.850000: I training.py:192] Running evaluation for step 10000\n",
      "2024-12-01 00:00:16.924000: I training.py:192] Evaluation result for step 10000: loss = 0.651261 ; perplexity = 1.917957\n",
      "2024-12-01 00:01:17.909000: I runner.py:310] Step = 10100 ; steps/s = 1.64, tokens/s = 43734 (43734 target) ; Learning rate = 0.000879 ; Loss = 1.668587\n",
      "2024-12-01 00:02:18.446000: I runner.py:310] Step = 10200 ; steps/s = 1.65, tokens/s = 43244 (43244 target) ; Learning rate = 0.000875 ; Loss = 1.635851\n",
      "2024-12-01 00:03:19.568000: I runner.py:310] Step = 10300 ; steps/s = 1.64, tokens/s = 43622 (43622 target) ; Learning rate = 0.000871 ; Loss = 1.637846\n",
      "2024-12-01 00:04:20.219000: I runner.py:310] Step = 10400 ; steps/s = 1.65, tokens/s = 43147 (43147 target) ; Learning rate = 0.000867 ; Loss = 1.619121\n",
      "2024-12-01 00:05:21.272000: I runner.py:310] Step = 10500 ; steps/s = 1.64, tokens/s = 43687 (43687 target) ; Learning rate = 0.000863 ; Loss = 1.632759\n",
      "2024-12-01 00:06:22.363000: I runner.py:310] Step = 10600 ; steps/s = 1.64, tokens/s = 43645 (43645 target) ; Learning rate = 0.000858 ; Loss = 1.623300\n",
      "2024-12-01 00:07:22.976000: I runner.py:310] Step = 10700 ; steps/s = 1.65, tokens/s = 43166 (43166 target) ; Learning rate = 0.000854 ; Loss = 1.624023\n",
      "2024-12-01 00:08:24.052000: I runner.py:310] Step = 10800 ; steps/s = 1.64, tokens/s = 43652 (43652 target) ; Learning rate = 0.000850 ; Loss = 1.627668\n",
      "2024-12-01 00:09:25.106000: I runner.py:310] Step = 10900 ; steps/s = 1.64, tokens/s = 43692 (43692 target) ; Learning rate = 0.000847 ; Loss = 1.646603\n",
      "2024-12-01 00:10:25.719000: I runner.py:310] Step = 11000 ; steps/s = 1.65, tokens/s = 43182 (43182 target) ; Learning rate = 0.000843 ; Loss = 1.624518\n",
      "2024-12-01 00:11:26.796000: I runner.py:310] Step = 11100 ; steps/s = 1.64, tokens/s = 43658 (43658 target) ; Learning rate = 0.000839 ; Loss = 1.625975\n",
      "2024-12-01 00:12:27.358000: I runner.py:310] Step = 11200 ; steps/s = 1.65, tokens/s = 43208 (43208 target) ; Learning rate = 0.000835 ; Loss = 1.617309\n",
      "2024-12-01 00:13:28.340000: I runner.py:310] Step = 11300 ; steps/s = 1.64, tokens/s = 43728 (43728 target) ; Learning rate = 0.000831 ; Loss = 1.619328\n",
      "2024-12-01 00:14:29.420000: I runner.py:310] Step = 11400 ; steps/s = 1.64, tokens/s = 43661 (43661 target) ; Learning rate = 0.000828 ; Loss = 1.611392\n",
      "2024-12-01 00:15:30.048000: I runner.py:310] Step = 11500 ; steps/s = 1.65, tokens/s = 43166 (43166 target) ; Learning rate = 0.000824 ; Loss = 1.611163\n",
      "2024-12-01 00:16:31.074000: I runner.py:310] Step = 11600 ; steps/s = 1.64, tokens/s = 43691 (43691 target) ; Learning rate = 0.000821 ; Loss = 1.610815\n",
      "2024-12-01 00:17:31.681000: I runner.py:310] Step = 11700 ; steps/s = 1.65, tokens/s = 43187 (43187 target) ; Learning rate = 0.000817 ; Loss = 1.598006\n",
      "2024-12-01 00:18:32.772000: I runner.py:310] Step = 11800 ; steps/s = 1.64, tokens/s = 43642 (43642 target) ; Learning rate = 0.000814 ; Loss = 1.601470\n",
      "2024-12-01 00:19:33.821000: I runner.py:310] Step = 11900 ; steps/s = 1.64, tokens/s = 43682 (43682 target) ; Learning rate = 0.000810 ; Loss = 1.606566\n",
      "2024-12-01 00:20:34.460000: I runner.py:310] Step = 12000 ; steps/s = 1.65, tokens/s = 43173 (43173 target) ; Learning rate = 0.000807 ; Loss = 1.605962\n",
      "2024-12-01 00:21:35.472000: I runner.py:310] Step = 12100 ; steps/s = 1.64, tokens/s = 43697 (43697 target) ; Learning rate = 0.000803 ; Loss = 1.608381\n",
      "2024-12-01 00:22:36.544000: I runner.py:310] Step = 12200 ; steps/s = 1.64, tokens/s = 43665 (43665 target) ; Learning rate = 0.000800 ; Loss = 1.605707\n",
      "2024-12-01 00:23:37.191000: I runner.py:310] Step = 12300 ; steps/s = 1.65, tokens/s = 43152 (43152 target) ; Learning rate = 0.000797 ; Loss = 1.600390\n",
      "2024-12-01 00:24:38.232000: I runner.py:310] Step = 12400 ; steps/s = 1.64, tokens/s = 43680 (43680 target) ; Learning rate = 0.000794 ; Loss = 1.611581\n",
      "2024-12-01 00:25:38.810000: I runner.py:310] Step = 12500 ; steps/s = 1.65, tokens/s = 43211 (43211 target) ; Learning rate = 0.000791 ; Loss = 1.591976\n",
      "2024-12-01 00:26:39.821000: I runner.py:310] Step = 12600 ; steps/s = 1.64, tokens/s = 43708 (43708 target) ; Learning rate = 0.000787 ; Loss = 1.605108\n",
      "2024-12-01 00:27:40.889000: I runner.py:310] Step = 12700 ; steps/s = 1.64, tokens/s = 43663 (43663 target) ; Learning rate = 0.000784 ; Loss = 1.596662\n",
      "2024-12-01 00:28:41.469000: I runner.py:310] Step = 12800 ; steps/s = 1.65, tokens/s = 43205 (43205 target) ; Learning rate = 0.000781 ; Loss = 1.590810\n",
      "2024-12-01 00:29:42.554000: I runner.py:310] Step = 12900 ; steps/s = 1.64, tokens/s = 43655 (43655 target) ; Learning rate = 0.000778 ; Loss = 1.590851\n",
      "2024-12-01 00:30:43.193000: I runner.py:310] Step = 13000 ; steps/s = 1.65, tokens/s = 43156 (43156 target) ; Learning rate = 0.000775 ; Loss = 1.567594\n",
      "2024-12-01 00:31:44.209000: I runner.py:310] Step = 13100 ; steps/s = 1.64, tokens/s = 43707 (43707 target) ; Learning rate = 0.000772 ; Loss = 1.586855\n",
      "2024-12-01 00:32:45.307000: I runner.py:310] Step = 13200 ; steps/s = 1.64, tokens/s = 43644 (43644 target) ; Learning rate = 0.000769 ; Loss = 1.586943\n",
      "2024-12-01 00:33:45.976000: I runner.py:310] Step = 13300 ; steps/s = 1.65, tokens/s = 43136 (43136 target) ; Learning rate = 0.000766 ; Loss = 1.584982\n",
      "2024-12-01 00:34:47.012000: I runner.py:310] Step = 13400 ; steps/s = 1.64, tokens/s = 43690 (43690 target) ; Learning rate = 0.000764 ; Loss = 1.590758\n",
      "2024-12-01 00:35:48.043000: I runner.py:310] Step = 13500 ; steps/s = 1.64, tokens/s = 43695 (43695 target) ; Learning rate = 0.000761 ; Loss = 1.590829\n",
      "2024-12-01 00:36:48.613000: I runner.py:310] Step = 13600 ; steps/s = 1.65, tokens/s = 43201 (43201 target) ; Learning rate = 0.000758 ; Loss = 1.574938\n",
      "2024-12-01 00:37:49.670000: I runner.py:310] Step = 13700 ; steps/s = 1.64, tokens/s = 43669 (43669 target) ; Learning rate = 0.000755 ; Loss = 1.580446\n",
      "2024-12-01 00:38:50.322000: I runner.py:310] Step = 13800 ; steps/s = 1.65, tokens/s = 43162 (43162 target) ; Learning rate = 0.000752 ; Loss = 1.571638\n",
      "2024-12-01 00:39:51.426000: I runner.py:310] Step = 13900 ; steps/s = 1.64, tokens/s = 43642 (43642 target) ; Learning rate = 0.000750 ; Loss = 1.578731\n",
      "2024-12-01 00:40:52.484000: I runner.py:310] Step = 14000 ; steps/s = 1.64, tokens/s = 43672 (43672 target) ; Learning rate = 0.000747 ; Loss = 1.581407\n",
      "2024-12-01 00:41:53.088000: I runner.py:310] Step = 14100 ; steps/s = 1.65, tokens/s = 43185 (43185 target) ; Learning rate = 0.000744 ; Loss = 1.571917\n",
      "2024-12-01 00:42:54.148000: I runner.py:310] Step = 14200 ; steps/s = 1.64, tokens/s = 43678 (43678 target) ; Learning rate = 0.000742 ; Loss = 1.572813\n",
      "2024-12-01 00:43:54.774000: I runner.py:310] Step = 14300 ; steps/s = 1.65, tokens/s = 43151 (43151 target) ; Learning rate = 0.000739 ; Loss = 1.563977\n",
      "2024-12-01 00:44:55.804000: I runner.py:310] Step = 14400 ; steps/s = 1.64, tokens/s = 43706 (43706 target) ; Learning rate = 0.000737 ; Loss = 1.569259\n",
      "2024-12-01 00:45:56.934000: I runner.py:310] Step = 14500 ; steps/s = 1.64, tokens/s = 43618 (43618 target) ; Learning rate = 0.000734 ; Loss = 1.572403\n",
      "2024-12-01 00:46:57.469000: I runner.py:310] Step = 14600 ; steps/s = 1.65, tokens/s = 43223 (43223 target) ; Learning rate = 0.000731 ; Loss = 1.571366\n",
      "2024-12-01 00:47:58.491000: I runner.py:310] Step = 14700 ; steps/s = 1.64, tokens/s = 43709 (43709 target) ; Learning rate = 0.000729 ; Loss = 1.580476\n",
      "2024-12-01 00:48:59.520000: I runner.py:310] Step = 14800 ; steps/s = 1.64, tokens/s = 43694 (43694 target) ; Learning rate = 0.000727 ; Loss = 1.566931\n",
      "2024-12-01 00:50:00.122000: I runner.py:310] Step = 14900 ; steps/s = 1.65, tokens/s = 43185 (43185 target) ; Learning rate = 0.000724 ; Loss = 1.564561\n",
      "2024-12-01 00:51:01.180000: I runner.py:310] Step = 15000 ; steps/s = 1.64, tokens/s = 43671 (43671 target) ; Learning rate = 0.000722 ; Loss = 1.571130\n",
      "2024-12-01 00:51:01.182000: I training.py:192] Running evaluation for step 15000\n",
      "2024-12-01 00:51:48.169000: I training.py:192] Evaluation result for step 15000: loss = 0.669005 ; perplexity = 1.952294\n",
      "2024-12-01 00:52:48.724000: I runner.py:310] Step = 15100 ; steps/s = 1.65, tokens/s = 43224 (43224 target) ; Learning rate = 0.000719 ; Loss = 1.566180\n",
      "2024-12-01 00:53:49.800000: I runner.py:310] Step = 15200 ; steps/s = 1.64, tokens/s = 43659 (43659 target) ; Learning rate = 0.000717 ; Loss = 1.568949\n",
      "2024-12-01 00:54:50.915000: I runner.py:310] Step = 15300 ; steps/s = 1.64, tokens/s = 43635 (43635 target) ; Learning rate = 0.000715 ; Loss = 1.574767\n",
      "2024-12-01 00:55:51.490000: I runner.py:310] Step = 15400 ; steps/s = 1.65, tokens/s = 43203 (43203 target) ; Learning rate = 0.000712 ; Loss = 1.568337\n",
      "2024-12-01 00:56:52.535000: I runner.py:310] Step = 15500 ; steps/s = 1.64, tokens/s = 43687 (43687 target) ; Learning rate = 0.000710 ; Loss = 1.562380\n",
      "2024-12-01 00:57:53.136000: I runner.py:310] Step = 15600 ; steps/s = 1.65, tokens/s = 43181 (43181 target) ; Learning rate = 0.000708 ; Loss = 1.548793\n",
      "2024-12-01 00:58:54.188000: I runner.py:310] Step = 15700 ; steps/s = 1.64, tokens/s = 43683 (43683 target) ; Learning rate = 0.000705 ; Loss = 1.558334\n",
      "2024-12-01 00:59:55.248000: I runner.py:310] Step = 15800 ; steps/s = 1.64, tokens/s = 43680 (43680 target) ; Learning rate = 0.000703 ; Loss = 1.562072\n",
      "2024-12-01 01:00:55.922000: I runner.py:310] Step = 15900 ; steps/s = 1.65, tokens/s = 43128 (43128 target) ; Learning rate = 0.000701 ; Loss = 1.564110\n",
      "2024-12-01 01:01:57.056000: I runner.py:310] Step = 16000 ; steps/s = 1.64, tokens/s = 43621 (43621 target) ; Learning rate = 0.000699 ; Loss = 1.568356\n",
      "2024-12-01 01:02:58.073000: I runner.py:310] Step = 16100 ; steps/s = 1.64, tokens/s = 43513 (43513 target) ; Learning rate = 0.000697 ; Loss = 1.574736\n",
      "2024-12-01 01:03:58.828000: I runner.py:310] Step = 16200 ; steps/s = 1.65, tokens/s = 43265 (43265 target) ; Learning rate = 0.000694 ; Loss = 1.539892\n",
      "2024-12-01 01:04:59.888000: I runner.py:310] Step = 16300 ; steps/s = 1.64, tokens/s = 43664 (43664 target) ; Learning rate = 0.000692 ; Loss = 1.566273\n",
      "2024-12-01 01:06:00.441000: I runner.py:310] Step = 16400 ; steps/s = 1.65, tokens/s = 43219 (43219 target) ; Learning rate = 0.000690 ; Loss = 1.553772\n",
      "2024-12-01 01:07:01.511000: I runner.py:310] Step = 16500 ; steps/s = 1.64, tokens/s = 43667 (43667 target) ; Learning rate = 0.000688 ; Loss = 1.553702\n",
      "2024-12-01 01:08:02.568000: I runner.py:310] Step = 16600 ; steps/s = 1.64, tokens/s = 43679 (43679 target) ; Learning rate = 0.000686 ; Loss = 1.562110\n",
      "2024-12-01 01:09:03.235000: I runner.py:310] Step = 16700 ; steps/s = 1.65, tokens/s = 43143 (43143 target) ; Learning rate = 0.000684 ; Loss = 1.551196\n",
      "2024-12-01 01:10:04.336000: I runner.py:310] Step = 16800 ; steps/s = 1.64, tokens/s = 43646 (43646 target) ; Learning rate = 0.000682 ; Loss = 1.550320\n",
      "2024-12-01 01:11:05.026000: I runner.py:310] Step = 16900 ; steps/s = 1.65, tokens/s = 43113 (43113 target) ; Learning rate = 0.000680 ; Loss = 1.550929\n",
      "2024-12-01 01:12:06.099000: I runner.py:310] Step = 17000 ; steps/s = 1.64, tokens/s = 43663 (43663 target) ; Learning rate = 0.000678 ; Loss = 1.551520\n",
      "2024-12-01 01:13:07.198000: I runner.py:310] Step = 17100 ; steps/s = 1.64, tokens/s = 43631 (43631 target) ; Learning rate = 0.000676 ; Loss = 1.552029\n",
      "2024-12-01 01:14:07.768000: I runner.py:310] Step = 17200 ; steps/s = 1.65, tokens/s = 43218 (43218 target) ; Learning rate = 0.000674 ; Loss = 1.553343\n",
      "2024-12-01 01:15:08.824000: I runner.py:310] Step = 17300 ; steps/s = 1.64, tokens/s = 43686 (43686 target) ; Learning rate = 0.000672 ; Loss = 1.544945\n",
      "2024-12-01 01:16:09.623000: I runner.py:310] Step = 17400 ; steps/s = 1.64, tokens/s = 43368 (43368 target) ; Learning rate = 0.000670 ; Loss = 1.579812\n",
      "2024-12-01 01:17:10.468000: I runner.py:310] Step = 17500 ; steps/s = 1.64, tokens/s = 43489 (43489 target) ; Learning rate = 0.000668 ; Loss = 1.539588\n",
      "2024-12-01 01:18:11.570000: I runner.py:310] Step = 17600 ; steps/s = 1.64, tokens/s = 43638 (43638 target) ; Learning rate = 0.000666 ; Loss = 1.549275\n",
      "2024-12-01 01:19:12.214000: I runner.py:310] Step = 17700 ; steps/s = 1.65, tokens/s = 43155 (43155 target) ; Learning rate = 0.000664 ; Loss = 1.534190\n",
      "2024-12-01 01:20:13.286000: I runner.py:310] Step = 17800 ; steps/s = 1.64, tokens/s = 43667 (43667 target) ; Learning rate = 0.000662 ; Loss = 1.553431\n",
      "2024-12-01 01:21:14.420000: I runner.py:310] Step = 17900 ; steps/s = 1.64, tokens/s = 43626 (43626 target) ; Learning rate = 0.000661 ; Loss = 1.552749\n",
      "2024-12-01 01:22:15.050000: I runner.py:310] Step = 18000 ; steps/s = 1.65, tokens/s = 43159 (43159 target) ; Learning rate = 0.000659 ; Loss = 1.540089\n",
      "2024-12-01 01:23:16.101000: I runner.py:310] Step = 18100 ; steps/s = 1.64, tokens/s = 43673 (43673 target) ; Learning rate = 0.000657 ; Loss = 1.544299\n",
      "2024-12-01 01:24:16.777000: I runner.py:310] Step = 18200 ; steps/s = 1.65, tokens/s = 43137 (43137 target) ; Learning rate = 0.000655 ; Loss = 1.536987\n",
      "2024-12-01 01:25:17.831000: I runner.py:310] Step = 18300 ; steps/s = 1.64, tokens/s = 43679 (43679 target) ; Learning rate = 0.000653 ; Loss = 1.536945\n",
      "2024-12-01 01:26:18.929000: I runner.py:310] Step = 18400 ; steps/s = 1.64, tokens/s = 43650 (43650 target) ; Learning rate = 0.000652 ; Loss = 1.539534\n",
      "2024-12-01 01:27:19.497000: I runner.py:310] Step = 18500 ; steps/s = 1.65, tokens/s = 43211 (43211 target) ; Learning rate = 0.000650 ; Loss = 1.529209\n",
      "2024-12-01 01:28:20.588000: I runner.py:310] Step = 18600 ; steps/s = 1.64, tokens/s = 43655 (43655 target) ; Learning rate = 0.000648 ; Loss = 1.546949\n",
      "2024-12-01 01:29:21.297000: I runner.py:310] Step = 18700 ; steps/s = 1.65, tokens/s = 43097 (43097 target) ; Learning rate = 0.000646 ; Loss = 1.523330\n",
      "2024-12-01 01:30:22.398000: I runner.py:310] Step = 18800 ; steps/s = 1.64, tokens/s = 43652 (43652 target) ; Learning rate = 0.000645 ; Loss = 1.528349\n",
      "2024-12-01 01:31:23.514000: I runner.py:310] Step = 18900 ; steps/s = 1.64, tokens/s = 43632 (43632 target) ; Learning rate = 0.000643 ; Loss = 1.533222\n",
      "2024-12-01 01:32:24.198000: I runner.py:310] Step = 19000 ; steps/s = 1.65, tokens/s = 43127 (43127 target) ; Learning rate = 0.000641 ; Loss = 1.533195\n",
      "2024-12-01 01:33:25.296000: I runner.py:310] Step = 19100 ; steps/s = 1.64, tokens/s = 43646 (43646 target) ; Learning rate = 0.000640 ; Loss = 1.550711\n",
      "2024-12-01 01:34:26.430000: I runner.py:310] Step = 19200 ; steps/s = 1.64, tokens/s = 43615 (43615 target) ; Learning rate = 0.000638 ; Loss = 1.539388\n",
      "2024-12-01 01:35:27.055000: I runner.py:310] Step = 19300 ; steps/s = 1.65, tokens/s = 43168 (43168 target) ; Learning rate = 0.000636 ; Loss = 1.529163\n",
      "2024-12-01 01:36:28.187000: I runner.py:310] Step = 19400 ; steps/s = 1.64, tokens/s = 43615 (43615 target) ; Learning rate = 0.000635 ; Loss = 1.532876\n",
      "2024-12-01 01:37:28.842000: I runner.py:310] Step = 19500 ; steps/s = 1.65, tokens/s = 43143 (43143 target) ; Learning rate = 0.000633 ; Loss = 1.526579\n",
      "2024-12-01 01:38:29.968000: I runner.py:310] Step = 19600 ; steps/s = 1.64, tokens/s = 43628 (43628 target) ; Learning rate = 0.000631 ; Loss = 1.527369\n",
      "2024-12-01 01:39:31.053000: I runner.py:310] Step = 19700 ; steps/s = 1.64, tokens/s = 43655 (43655 target) ; Learning rate = 0.000630 ; Loss = 1.536226\n",
      "2024-12-01 01:40:31.673000: I runner.py:310] Step = 19800 ; steps/s = 1.65, tokens/s = 43167 (43167 target) ; Learning rate = 0.000628 ; Loss = 1.528408\n",
      "2024-12-01 01:41:32.804000: I runner.py:310] Step = 19900 ; steps/s = 1.64, tokens/s = 43625 (43625 target) ; Learning rate = 0.000627 ; Loss = 1.531741\n",
      "2024-12-01 01:42:33.365000: I runner.py:310] Step = 20000 ; steps/s = 1.65, tokens/s = 43211 (43211 target) ; Learning rate = 0.000625 ; Loss = 1.526497\n",
      "2024-12-01 01:42:35.157000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-20000\n",
      "2024-12-01 01:42:35.157000: I training.py:192] Running evaluation for step 20000\n",
      "2024-12-01 01:43:23.158000: I training.py:192] Evaluation result for step 20000: loss = 0.686181 ; perplexity = 1.986116\n",
      "2024-12-01 01:44:24.114000: I runner.py:310] Step = 20100 ; steps/s = 1.64, tokens/s = 43764 (43764 target) ; Learning rate = 0.000623 ; Loss = 1.529555\n",
      "2024-12-01 01:45:25.243000: I runner.py:310] Step = 20200 ; steps/s = 1.64, tokens/s = 43616 (43616 target) ; Learning rate = 0.000622 ; Loss = 1.526641\n",
      "2024-12-01 01:46:25.916000: I runner.py:310] Step = 20300 ; steps/s = 1.65, tokens/s = 43139 (43139 target) ; Learning rate = 0.000620 ; Loss = 1.528171\n",
      "2024-12-01 01:47:27.040000: I runner.py:310] Step = 20400 ; steps/s = 1.64, tokens/s = 43618 (43618 target) ; Learning rate = 0.000619 ; Loss = 1.542724\n",
      "2024-12-01 01:48:28.139000: I runner.py:310] Step = 20500 ; steps/s = 1.64, tokens/s = 43653 (43653 target) ; Learning rate = 0.000617 ; Loss = 1.532730\n",
      "2024-12-01 01:49:28.858000: I runner.py:310] Step = 20600 ; steps/s = 1.65, tokens/s = 43094 (43094 target) ; Learning rate = 0.000616 ; Loss = 1.527286\n",
      "2024-12-01 01:50:29.939000: I runner.py:310] Step = 20700 ; steps/s = 1.64, tokens/s = 43658 (43658 target) ; Learning rate = 0.000614 ; Loss = 1.530194\n",
      "2024-12-01 01:51:30.563000: I runner.py:310] Step = 20800 ; steps/s = 1.65, tokens/s = 43171 (43171 target) ; Learning rate = 0.000613 ; Loss = 1.518869\n",
      "2024-12-01 01:52:31.594000: I runner.py:310] Step = 20900 ; steps/s = 1.64, tokens/s = 43694 (43694 target) ; Learning rate = 0.000611 ; Loss = 1.533835\n",
      "2024-12-01 01:53:32.678000: I runner.py:310] Step = 21000 ; steps/s = 1.64, tokens/s = 43651 (43651 target) ; Learning rate = 0.000610 ; Loss = 1.528080\n",
      "2024-12-01 01:54:33.279000: I runner.py:310] Step = 21100 ; steps/s = 1.65, tokens/s = 43192 (43192 target) ; Learning rate = 0.000608 ; Loss = 1.523258\n",
      "2024-12-01 01:55:34.386000: I runner.py:310] Step = 21200 ; steps/s = 1.64, tokens/s = 43638 (43638 target) ; Learning rate = 0.000607 ; Loss = 1.521345\n",
      "2024-12-01 01:56:35.049000: I runner.py:310] Step = 21300 ; steps/s = 1.65, tokens/s = 43140 (43140 target) ; Learning rate = 0.000606 ; Loss = 1.515034\n",
      "2024-12-01 01:57:36.123000: I runner.py:310] Step = 21400 ; steps/s = 1.64, tokens/s = 43661 (43661 target) ; Learning rate = 0.000604 ; Loss = 1.516044\n",
      "2024-12-01 01:58:37.193000: I runner.py:310] Step = 21500 ; steps/s = 1.64, tokens/s = 43659 (43659 target) ; Learning rate = 0.000603 ; Loss = 1.522566\n",
      "2024-12-01 01:59:37.831000: I runner.py:310] Step = 21600 ; steps/s = 1.65, tokens/s = 43174 (43174 target) ; Learning rate = 0.000601 ; Loss = 1.518533\n",
      "2024-12-01 02:00:38.945000: I runner.py:310] Step = 21700 ; steps/s = 1.64, tokens/s = 43636 (43636 target) ; Learning rate = 0.000600 ; Loss = 1.539455\n",
      "2024-12-01 02:01:39.995000: I runner.py:310] Step = 21800 ; steps/s = 1.64, tokens/s = 43672 (43672 target) ; Learning rate = 0.000599 ; Loss = 1.527041\n",
      "2024-12-01 02:02:40.561000: I runner.py:310] Step = 21900 ; steps/s = 1.65, tokens/s = 43205 (43205 target) ; Learning rate = 0.000597 ; Loss = 1.519317\n",
      "2024-12-01 02:03:41.661000: I runner.py:310] Step = 22000 ; steps/s = 1.64, tokens/s = 43637 (43637 target) ; Learning rate = 0.000596 ; Loss = 1.524201\n",
      "2024-12-01 02:04:42.286000: I runner.py:310] Step = 22100 ; steps/s = 1.65, tokens/s = 43179 (43179 target) ; Learning rate = 0.000595 ; Loss = 1.513258\n",
      "2024-12-01 02:05:43.344000: I runner.py:310] Step = 22200 ; steps/s = 1.64, tokens/s = 43668 (43668 target) ; Learning rate = 0.000593 ; Loss = 1.510362\n",
      "2024-12-01 02:06:44.412000: I runner.py:310] Step = 22300 ; steps/s = 1.64, tokens/s = 43669 (43669 target) ; Learning rate = 0.000592 ; Loss = 1.524414\n",
      "2024-12-01 02:07:44.999000: I runner.py:310] Step = 22400 ; steps/s = 1.65, tokens/s = 43202 (43202 target) ; Learning rate = 0.000591 ; Loss = 1.523608\n",
      "2024-12-01 02:08:46.073000: I runner.py:310] Step = 22500 ; steps/s = 1.64, tokens/s = 43659 (43659 target) ; Learning rate = 0.000589 ; Loss = 1.524454\n",
      "2024-12-01 02:09:46.712000: I runner.py:310] Step = 22600 ; steps/s = 1.65, tokens/s = 43157 (43157 target) ; Learning rate = 0.000588 ; Loss = 1.506760\n",
      "2024-12-01 02:10:47.805000: I runner.py:310] Step = 22700 ; steps/s = 1.64, tokens/s = 43656 (43656 target) ; Learning rate = 0.000587 ; Loss = 1.516403\n",
      "2024-12-01 02:11:48.949000: I runner.py:310] Step = 22800 ; steps/s = 1.64, tokens/s = 43606 (43606 target) ; Learning rate = 0.000585 ; Loss = 1.521448\n",
      "2024-12-01 02:12:49.551000: I runner.py:310] Step = 22900 ; steps/s = 1.65, tokens/s = 43184 (43184 target) ; Learning rate = 0.000584 ; Loss = 1.505556\n",
      "2024-12-01 02:13:50.649000: I runner.py:310] Step = 23000 ; steps/s = 1.64, tokens/s = 43641 (43641 target) ; Learning rate = 0.000583 ; Loss = 1.527643\n",
      "2024-12-01 02:14:51.759000: I runner.py:310] Step = 23100 ; steps/s = 1.64, tokens/s = 43640 (43640 target) ; Learning rate = 0.000582 ; Loss = 1.520238\n",
      "2024-12-01 02:15:52.414000: I runner.py:310] Step = 23200 ; steps/s = 1.65, tokens/s = 43144 (43144 target) ; Learning rate = 0.000580 ; Loss = 1.508992\n",
      "2024-12-01 02:16:53.527000: I runner.py:310] Step = 23300 ; steps/s = 1.64, tokens/s = 43639 (43639 target) ; Learning rate = 0.000579 ; Loss = 1.521057\n",
      "2024-12-01 02:17:54.182000: I runner.py:310] Step = 23400 ; steps/s = 1.65, tokens/s = 43150 (43150 target) ; Learning rate = 0.000578 ; Loss = 1.516183\n",
      "2024-12-01 02:18:55.226000: I runner.py:310] Step = 23500 ; steps/s = 1.64, tokens/s = 43689 (43689 target) ; Learning rate = 0.000577 ; Loss = 1.513022\n",
      "2024-12-01 02:19:56.303000: I runner.py:310] Step = 23600 ; steps/s = 1.64, tokens/s = 43656 (43656 target) ; Learning rate = 0.000575 ; Loss = 1.512507\n",
      "2024-12-01 02:20:56.916000: I runner.py:310] Step = 23700 ; steps/s = 1.65, tokens/s = 43161 (43161 target) ; Learning rate = 0.000574 ; Loss = 1.508196\n",
      "2024-12-01 02:21:57.964000: I runner.py:310] Step = 23800 ; steps/s = 1.64, tokens/s = 43684 (43684 target) ; Learning rate = 0.000573 ; Loss = 1.514499\n",
      "2024-12-01 02:22:58.647000: I runner.py:310] Step = 23900 ; steps/s = 1.65, tokens/s = 43131 (43131 target) ; Learning rate = 0.000572 ; Loss = 1.509750\n",
      "2024-12-01 02:23:59.751000: I runner.py:310] Step = 24000 ; steps/s = 1.64, tokens/s = 43647 (43647 target) ; Learning rate = 0.000571 ; Loss = 1.503232\n",
      "2024-12-01 02:25:00.910000: I runner.py:310] Step = 24100 ; steps/s = 1.64, tokens/s = 43597 (43597 target) ; Learning rate = 0.000569 ; Loss = 1.506201\n",
      "2024-12-01 02:26:01.593000: I runner.py:310] Step = 24200 ; steps/s = 1.65, tokens/s = 43127 (43127 target) ; Learning rate = 0.000568 ; Loss = 1.499579\n",
      "2024-12-01 02:27:02.632000: I runner.py:310] Step = 24300 ; steps/s = 1.64, tokens/s = 43693 (43693 target) ; Learning rate = 0.000567 ; Loss = 1.522001\n",
      "2024-12-01 02:28:03.736000: I runner.py:310] Step = 24400 ; steps/s = 1.64, tokens/s = 43632 (43632 target) ; Learning rate = 0.000566 ; Loss = 1.517324\n",
      "2024-12-01 02:29:04.296000: I runner.py:310] Step = 24500 ; steps/s = 1.65, tokens/s = 43218 (43218 target) ; Learning rate = 0.000565 ; Loss = 1.512087\n",
      "2024-12-01 02:30:05.421000: I runner.py:310] Step = 24600 ; steps/s = 1.64, tokens/s = 43620 (43620 target) ; Learning rate = 0.000564 ; Loss = 1.502697\n",
      "2024-12-01 02:31:06.115000: I runner.py:310] Step = 24700 ; steps/s = 1.65, tokens/s = 43120 (43120 target) ; Learning rate = 0.000562 ; Loss = 1.498756\n",
      "2024-12-01 02:32:07.225000: I runner.py:310] Step = 24800 ; steps/s = 1.64, tokens/s = 43633 (43633 target) ; Learning rate = 0.000561 ; Loss = 1.502981\n",
      "2024-12-01 02:33:08.294000: I runner.py:310] Step = 24900 ; steps/s = 1.64, tokens/s = 43677 (43677 target) ; Learning rate = 0.000560 ; Loss = 1.513362\n",
      "2024-12-01 02:34:08.969000: I runner.py:310] Step = 25000 ; steps/s = 1.65, tokens/s = 43117 (43117 target) ; Learning rate = 0.000559 ; Loss = 1.511906\n",
      "2024-12-01 02:34:08.970000: I training.py:192] Running evaluation for step 25000\n",
      "2024-12-01 02:34:57.048000: I training.py:192] Evaluation result for step 25000: loss = 0.706437 ; perplexity = 2.026757\n",
      "2024-12-01 02:35:58.025000: I runner.py:310] Step = 25100 ; steps/s = 1.64, tokens/s = 43748 (43748 target) ; Learning rate = 0.000558 ; Loss = 1.510284\n",
      "2024-12-01 02:36:58.637000: I runner.py:310] Step = 25200 ; steps/s = 1.65, tokens/s = 43179 (43179 target) ; Learning rate = 0.000557 ; Loss = 1.508754\n",
      "2024-12-01 02:37:59.789000: I runner.py:310] Step = 25300 ; steps/s = 1.64, tokens/s = 43610 (43610 target) ; Learning rate = 0.000556 ; Loss = 1.511609\n",
      "2024-12-01 02:39:00.928000: I runner.py:310] Step = 25400 ; steps/s = 1.64, tokens/s = 43614 (43614 target) ; Learning rate = 0.000555 ; Loss = 1.511046\n",
      "2024-12-01 02:40:01.532000: I runner.py:310] Step = 25500 ; steps/s = 1.65, tokens/s = 43176 (43176 target) ; Learning rate = 0.000553 ; Loss = 1.503561\n",
      "2024-12-01 02:41:02.670000: I runner.py:310] Step = 25600 ; steps/s = 1.64, tokens/s = 43613 (43613 target) ; Learning rate = 0.000552 ; Loss = 1.507564\n",
      "2024-12-01 02:42:03.757000: I runner.py:310] Step = 25700 ; steps/s = 1.64, tokens/s = 43662 (43662 target) ; Learning rate = 0.000551 ; Loss = 1.505465\n",
      "2024-12-01 02:43:04.417000: I runner.py:310] Step = 25800 ; steps/s = 1.65, tokens/s = 43146 (43146 target) ; Learning rate = 0.000550 ; Loss = 1.503291\n",
      "2024-12-01 02:44:05.481000: I runner.py:310] Step = 25900 ; steps/s = 1.64, tokens/s = 43661 (43661 target) ; Learning rate = 0.000549 ; Loss = 1.514067\n",
      "2024-12-01 02:45:06.179000: I runner.py:310] Step = 26000 ; steps/s = 1.65, tokens/s = 43125 (43125 target) ; Learning rate = 0.000548 ; Loss = 1.494652\n",
      "2024-12-01 02:46:07.288000: I runner.py:310] Step = 26100 ; steps/s = 1.64, tokens/s = 43629 (43629 target) ; Learning rate = 0.000547 ; Loss = 1.499761\n",
      "2024-12-01 02:47:08.409000: I runner.py:310] Step = 26200 ; steps/s = 1.64, tokens/s = 43627 (43627 target) ; Learning rate = 0.000546 ; Loss = 1.512867\n",
      "2024-12-01 02:48:08.952000: I runner.py:310] Step = 26300 ; steps/s = 1.65, tokens/s = 43236 (43236 target) ; Learning rate = 0.000545 ; Loss = 1.496776\n",
      "2024-12-01 02:49:09.995000: I runner.py:310] Step = 26400 ; steps/s = 1.64, tokens/s = 43687 (43687 target) ; Learning rate = 0.000544 ; Loss = 1.508633\n",
      "2024-12-01 02:50:10.622000: I runner.py:310] Step = 26500 ; steps/s = 1.65, tokens/s = 43160 (43160 target) ; Learning rate = 0.000543 ; Loss = 1.496701\n",
      "2024-12-01 02:51:11.729000: I runner.py:310] Step = 26600 ; steps/s = 1.64, tokens/s = 43645 (43645 target) ; Learning rate = 0.000542 ; Loss = 1.506481\n",
      "2024-12-01 02:52:12.804000: I runner.py:310] Step = 26700 ; steps/s = 1.64, tokens/s = 43659 (43659 target) ; Learning rate = 0.000541 ; Loss = 1.507251\n",
      "2024-12-01 02:53:13.474000: I runner.py:310] Step = 26800 ; steps/s = 1.65, tokens/s = 43130 (43130 target) ; Learning rate = 0.000540 ; Loss = 1.503347\n",
      "2024-12-01 02:54:14.521000: I runner.py:310] Step = 26900 ; steps/s = 1.64, tokens/s = 43684 (43684 target) ; Learning rate = 0.000539 ; Loss = 1.507527\n",
      "2024-12-01 02:55:15.640000: I runner.py:310] Step = 27000 ; steps/s = 1.64, tokens/s = 43636 (43636 target) ; Learning rate = 0.000538 ; Loss = 1.505336\n",
      "2024-12-01 02:56:16.244000: I runner.py:310] Step = 27100 ; steps/s = 1.65, tokens/s = 43187 (43187 target) ; Learning rate = 0.000537 ; Loss = 1.496024\n",
      "2024-12-01 02:57:17.315000: I runner.py:310] Step = 27200 ; steps/s = 1.64, tokens/s = 43659 (43659 target) ; Learning rate = 0.000536 ; Loss = 1.502537\n",
      "2024-12-01 02:58:17.905000: I runner.py:310] Step = 27300 ; steps/s = 1.65, tokens/s = 43203 (43203 target) ; Learning rate = 0.000535 ; Loss = 1.497279\n",
      "2024-12-01 02:59:19.058000: I runner.py:310] Step = 27400 ; steps/s = 1.64, tokens/s = 43603 (43603 target) ; Learning rate = 0.000534 ; Loss = 1.504040\n",
      "2024-12-01 03:00:20.089000: I runner.py:310] Step = 27500 ; steps/s = 1.64, tokens/s = 43692 (43692 target) ; Learning rate = 0.000533 ; Loss = 1.503703\n",
      "2024-12-01 03:01:20.684000: I runner.py:310] Step = 27600 ; steps/s = 1.65, tokens/s = 43195 (43195 target) ; Learning rate = 0.000532 ; Loss = 1.499339\n",
      "2024-12-01 03:02:21.802000: I runner.py:310] Step = 27700 ; steps/s = 1.64, tokens/s = 43622 (43622 target) ; Learning rate = 0.000531 ; Loss = 1.499930\n",
      "2024-12-01 03:03:22.467000: I runner.py:310] Step = 27800 ; steps/s = 1.65, tokens/s = 43135 (43135 target) ; Learning rate = 0.000530 ; Loss = 1.498952\n",
      "2024-12-01 03:04:23.565000: I runner.py:310] Step = 27900 ; steps/s = 1.64, tokens/s = 43647 (43647 target) ; Learning rate = 0.000529 ; Loss = 1.500554\n",
      "2024-12-01 03:05:24.617000: I runner.py:310] Step = 28000 ; steps/s = 1.64, tokens/s = 43682 (43682 target) ; Learning rate = 0.000528 ; Loss = 1.499758\n",
      "2024-12-01 03:06:25.246000: I runner.py:310] Step = 28100 ; steps/s = 1.65, tokens/s = 43173 (43173 target) ; Learning rate = 0.000527 ; Loss = 1.499713\n",
      "2024-12-01 03:07:26.313000: I runner.py:310] Step = 28200 ; steps/s = 1.64, tokens/s = 43661 (43661 target) ; Learning rate = 0.000526 ; Loss = 1.507293\n",
      "2024-12-01 03:08:27.403000: I runner.py:310] Step = 28300 ; steps/s = 1.64, tokens/s = 43651 (43651 target) ; Learning rate = 0.000525 ; Loss = 1.499355\n",
      "2024-12-01 03:09:28.007000: I runner.py:310] Step = 28400 ; steps/s = 1.65, tokens/s = 43181 (43181 target) ; Learning rate = 0.000524 ; Loss = 1.497676\n",
      "2024-12-01 03:10:29.056000: I runner.py:310] Step = 28500 ; steps/s = 1.64, tokens/s = 43686 (43686 target) ; Learning rate = 0.000524 ; Loss = 1.500178\n",
      "2024-12-01 03:11:29.771000: I runner.py:310] Step = 28600 ; steps/s = 1.65, tokens/s = 43110 (43110 target) ; Learning rate = 0.000523 ; Loss = 1.493259\n",
      "2024-12-01 03:12:30.884000: I runner.py:310] Step = 28700 ; steps/s = 1.64, tokens/s = 43635 (43635 target) ; Learning rate = 0.000522 ; Loss = 1.500752\n",
      "2024-12-01 03:13:31.957000: I runner.py:310] Step = 28800 ; steps/s = 1.64, tokens/s = 43658 (43658 target) ; Learning rate = 0.000521 ; Loss = 1.499584\n",
      "2024-12-01 03:14:32.646000: I runner.py:310] Step = 28900 ; steps/s = 1.65, tokens/s = 43123 (43123 target) ; Learning rate = 0.000520 ; Loss = 1.500956\n",
      "2024-12-01 03:15:33.701000: I runner.py:310] Step = 29000 ; steps/s = 1.64, tokens/s = 43677 (43677 target) ; Learning rate = 0.000519 ; Loss = 1.497669\n",
      "2024-12-01 03:16:34.322000: I runner.py:310] Step = 29100 ; steps/s = 1.65, tokens/s = 43171 (43171 target) ; Learning rate = 0.000518 ; Loss = 1.491237\n",
      "2024-12-01 03:17:35.425000: I runner.py:310] Step = 29200 ; steps/s = 1.64, tokens/s = 43646 (43646 target) ; Learning rate = 0.000517 ; Loss = 1.499559\n",
      "2024-12-01 03:18:36.552000: I runner.py:310] Step = 29300 ; steps/s = 1.64, tokens/s = 43617 (43617 target) ; Learning rate = 0.000516 ; Loss = 1.499905\n",
      "2024-12-01 03:19:37.193000: I runner.py:310] Step = 29400 ; steps/s = 1.65, tokens/s = 43159 (43159 target) ; Learning rate = 0.000515 ; Loss = 1.493059\n",
      "2024-12-01 03:20:38.263000: I runner.py:310] Step = 29500 ; steps/s = 1.64, tokens/s = 43673 (43673 target) ; Learning rate = 0.000515 ; Loss = 1.498233\n",
      "2024-12-01 03:21:39.343000: I runner.py:310] Step = 29600 ; steps/s = 1.64, tokens/s = 43656 (43656 target) ; Learning rate = 0.000514 ; Loss = 1.500858\n",
      "2024-12-01 03:22:39.983000: I runner.py:310] Step = 29700 ; steps/s = 1.65, tokens/s = 43159 (43159 target) ; Learning rate = 0.000513 ; Loss = 1.493401\n",
      "2024-12-01 03:23:41.075000: I runner.py:310] Step = 29800 ; steps/s = 1.64, tokens/s = 43645 (43645 target) ; Learning rate = 0.000512 ; Loss = 1.498945\n",
      "2024-12-01 03:24:41.728000: I runner.py:310] Step = 29900 ; steps/s = 1.65, tokens/s = 43145 (43145 target) ; Learning rate = 0.000511 ; Loss = 1.491877\n",
      "2024-12-01 03:25:42.854000: I runner.py:310] Step = 30000 ; steps/s = 1.64, tokens/s = 43630 (43630 target) ; Learning rate = 0.000510 ; Loss = 1.496129\n",
      "2024-12-01 03:25:44.740000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-30000\n",
      "2024-12-01 03:25:44.740000: I training.py:192] Running evaluation for step 30000\n",
      "2024-12-01 03:26:32.742000: I training.py:192] Evaluation result for step 30000: loss = 0.720475 ; perplexity = 2.055410\n",
      "2024-12-01 03:27:33.784000: I runner.py:310] Step = 30100 ; steps/s = 1.64, tokens/s = 43697 (43697 target) ; Learning rate = 0.000509 ; Loss = 1.500142\n",
      "2024-12-01 03:28:34.435000: I runner.py:310] Step = 30200 ; steps/s = 1.65, tokens/s = 43148 (43148 target) ; Learning rate = 0.000509 ; Loss = 1.491572\n",
      "2024-12-01 03:29:35.540000: I runner.py:310] Step = 30300 ; steps/s = 1.64, tokens/s = 43636 (43636 target) ; Learning rate = 0.000508 ; Loss = 1.500547\n",
      "2024-12-01 03:30:36.294000: I runner.py:310] Step = 30400 ; steps/s = 1.65, tokens/s = 43076 (43076 target) ; Learning rate = 0.000507 ; Loss = 1.484695\n",
      "2024-12-01 03:31:37.402000: I runner.py:310] Step = 30500 ; steps/s = 1.64, tokens/s = 43654 (43654 target) ; Learning rate = 0.000506 ; Loss = 1.497377\n",
      "2024-12-01 03:32:38.520000: I runner.py:310] Step = 30600 ; steps/s = 1.64, tokens/s = 43620 (43620 target) ; Learning rate = 0.000505 ; Loss = 1.496024\n",
      "2024-12-01 03:33:39.173000: I runner.py:310] Step = 30700 ; steps/s = 1.65, tokens/s = 43149 (43149 target) ; Learning rate = 0.000504 ; Loss = 1.489943\n",
      "2024-12-01 03:34:40.284000: I runner.py:310] Step = 30800 ; steps/s = 1.64, tokens/s = 43629 (43629 target) ; Learning rate = 0.000504 ; Loss = 1.496635\n",
      "2024-12-01 03:35:41.301000: I runner.py:310] Step = 30900 ; steps/s = 1.64, tokens/s = 43714 (43714 target) ; Learning rate = 0.000503 ; Loss = 1.498767\n",
      "2024-12-01 03:36:41.888000: I runner.py:310] Step = 31000 ; steps/s = 1.65, tokens/s = 43200 (43200 target) ; Learning rate = 0.000502 ; Loss = 1.493106\n",
      "2024-12-01 03:37:43.015000: I runner.py:310] Step = 31100 ; steps/s = 1.64, tokens/s = 43623 (43623 target) ; Learning rate = 0.000501 ; Loss = 1.487610\n",
      "2024-12-01 03:38:43.656000: I runner.py:310] Step = 31200 ; steps/s = 1.65, tokens/s = 43154 (43154 target) ; Learning rate = 0.000500 ; Loss = 1.494496\n",
      "2024-12-01 03:39:44.679000: I runner.py:310] Step = 31300 ; steps/s = 1.64, tokens/s = 43697 (43697 target) ; Learning rate = 0.000500 ; Loss = 1.489283\n",
      "2024-12-01 03:40:45.777000: I runner.py:310] Step = 31400 ; steps/s = 1.64, tokens/s = 43649 (43649 target) ; Learning rate = 0.000499 ; Loss = 1.496291\n",
      "2024-12-01 03:41:46.344000: I runner.py:310] Step = 31500 ; steps/s = 1.65, tokens/s = 43204 (43204 target) ; Learning rate = 0.000498 ; Loss = 1.488266\n",
      "2024-12-01 03:42:47.422000: I runner.py:310] Step = 31600 ; steps/s = 1.64, tokens/s = 43666 (43666 target) ; Learning rate = 0.000497 ; Loss = 1.498167\n",
      "2024-12-01 03:43:48.014000: I runner.py:310] Step = 31700 ; steps/s = 1.65, tokens/s = 43190 (43190 target) ; Learning rate = 0.000496 ; Loss = 1.486374\n",
      "2024-12-01 03:44:49.161000: I runner.py:310] Step = 31800 ; steps/s = 1.64, tokens/s = 43610 (43610 target) ; Learning rate = 0.000496 ; Loss = 1.487263\n",
      "2024-12-01 03:45:50.211000: I runner.py:310] Step = 31900 ; steps/s = 1.64, tokens/s = 43673 (43673 target) ; Learning rate = 0.000495 ; Loss = 1.495174\n",
      "2024-12-01 03:46:50.789000: I runner.py:310] Step = 32000 ; steps/s = 1.65, tokens/s = 43209 (43209 target) ; Learning rate = 0.000494 ; Loss = 1.484960\n",
      "2024-12-01 03:47:51.856000: I runner.py:310] Step = 32100 ; steps/s = 1.64, tokens/s = 43662 (43662 target) ; Learning rate = 0.000493 ; Loss = 1.490484\n",
      "2024-12-01 03:48:52.937000: I runner.py:310] Step = 32200 ; steps/s = 1.64, tokens/s = 43660 (43660 target) ; Learning rate = 0.000493 ; Loss = 1.495233\n",
      "2024-12-01 03:49:53.568000: I runner.py:310] Step = 32300 ; steps/s = 1.65, tokens/s = 43169 (43169 target) ; Learning rate = 0.000492 ; Loss = 1.483504\n",
      "2024-12-01 03:50:54.636000: I runner.py:310] Step = 32400 ; steps/s = 1.64, tokens/s = 43666 (43666 target) ; Learning rate = 0.000491 ; Loss = 1.492394\n",
      "2024-12-01 03:51:55.312000: I runner.py:310] Step = 32500 ; steps/s = 1.65, tokens/s = 43125 (43125 target) ; Learning rate = 0.000490 ; Loss = 1.485715\n",
      "2024-12-01 03:52:56.453000: I runner.py:310] Step = 32600 ; steps/s = 1.64, tokens/s = 43613 (43613 target) ; Learning rate = 0.000490 ; Loss = 1.492918\n",
      "2024-12-01 03:53:57.549000: I runner.py:310] Step = 32700 ; steps/s = 1.64, tokens/s = 43651 (43651 target) ; Learning rate = 0.000489 ; Loss = 1.496736\n",
      "2024-12-01 03:54:58.105000: I runner.py:310] Step = 32800 ; steps/s = 1.65, tokens/s = 43229 (43229 target) ; Learning rate = 0.000488 ; Loss = 1.491856\n",
      "2024-12-01 03:55:59.139000: I runner.py:310] Step = 32900 ; steps/s = 1.64, tokens/s = 43694 (43694 target) ; Learning rate = 0.000487 ; Loss = 1.491104\n",
      "2024-12-01 03:56:59.827000: I runner.py:310] Step = 33000 ; steps/s = 1.65, tokens/s = 43117 (43117 target) ; Learning rate = 0.000487 ; Loss = 1.482992\n",
      "2024-12-01 03:58:00.881000: I runner.py:310] Step = 33100 ; steps/s = 1.64, tokens/s = 43669 (43669 target) ; Learning rate = 0.000486 ; Loss = 1.485682\n",
      "2024-12-01 03:59:01.974000: I runner.py:310] Step = 33200 ; steps/s = 1.64, tokens/s = 43659 (43659 target) ; Learning rate = 0.000485 ; Loss = 1.486756\n",
      "2024-12-01 04:00:02.624000: I runner.py:310] Step = 33300 ; steps/s = 1.65, tokens/s = 43146 (43146 target) ; Learning rate = 0.000484 ; Loss = 1.486447\n",
      "2024-12-01 04:01:03.699000: I runner.py:310] Step = 33400 ; steps/s = 1.64, tokens/s = 43669 (43669 target) ; Learning rate = 0.000484 ; Loss = 1.493291\n",
      "2024-12-01 04:02:04.723000: I runner.py:310] Step = 33500 ; steps/s = 1.64, tokens/s = 43651 (43651 target) ; Learning rate = 0.000483 ; Loss = 1.500524\n",
      "2024-12-01 04:03:05.376000: I runner.py:310] Step = 33600 ; steps/s = 1.65, tokens/s = 43187 (43187 target) ; Learning rate = 0.000482 ; Loss = 1.486844\n",
      "2024-12-01 04:04:06.477000: I runner.py:310] Step = 33700 ; steps/s = 1.64, tokens/s = 43643 (43643 target) ; Learning rate = 0.000481 ; Loss = 1.486114\n",
      "2024-12-01 04:05:07.168000: I runner.py:310] Step = 33800 ; steps/s = 1.65, tokens/s = 43115 (43115 target) ; Learning rate = 0.000481 ; Loss = 1.479896\n",
      "2024-12-01 04:06:08.234000: I runner.py:310] Step = 33900 ; steps/s = 1.64, tokens/s = 43664 (43664 target) ; Learning rate = 0.000480 ; Loss = 1.485374\n",
      "2024-12-01 04:07:09.327000: I runner.py:310] Step = 34000 ; steps/s = 1.64, tokens/s = 43657 (43657 target) ; Learning rate = 0.000479 ; Loss = 1.490873\n",
      "2024-12-01 04:08:09.949000: I runner.py:310] Step = 34100 ; steps/s = 1.65, tokens/s = 43173 (43173 target) ; Learning rate = 0.000479 ; Loss = 1.482340\n",
      "2024-12-01 04:09:11.034000: I runner.py:310] Step = 34200 ; steps/s = 1.64, tokens/s = 43663 (43663 target) ; Learning rate = 0.000478 ; Loss = 1.483060\n",
      "2024-12-01 04:10:11.647000: I runner.py:310] Step = 34300 ; steps/s = 1.65, tokens/s = 43169 (43169 target) ; Learning rate = 0.000477 ; Loss = 1.481783\n",
      "2024-12-01 04:11:12.740000: I runner.py:310] Step = 34400 ; steps/s = 1.64, tokens/s = 43647 (43647 target) ; Learning rate = 0.000477 ; Loss = 1.486382\n",
      "2024-12-01 04:12:13.908000: I runner.py:310] Step = 34500 ; steps/s = 1.63, tokens/s = 43596 (43596 target) ; Learning rate = 0.000476 ; Loss = 1.485025\n",
      "2024-12-01 04:13:14.474000: I runner.py:310] Step = 34600 ; steps/s = 1.65, tokens/s = 43206 (43206 target) ; Learning rate = 0.000475 ; Loss = 1.485471\n",
      "2024-12-01 04:14:15.565000: I runner.py:310] Step = 34700 ; steps/s = 1.64, tokens/s = 43644 (43644 target) ; Learning rate = 0.000474 ; Loss = 1.488110\n",
      "2024-12-01 04:15:16.501000: I runner.py:310] Step = 34800 ; steps/s = 1.64, tokens/s = 43520 (43520 target) ; Learning rate = 0.000474 ; Loss = 1.508841\n",
      "2024-12-01 04:16:17.246000: I runner.py:310] Step = 34900 ; steps/s = 1.65, tokens/s = 43327 (43327 target) ; Learning rate = 0.000473 ; Loss = 1.483985\n",
      "2024-12-01 04:17:18.310000: I runner.py:310] Step = 35000 ; steps/s = 1.64, tokens/s = 43668 (43668 target) ; Learning rate = 0.000472 ; Loss = 1.482107\n",
      "2024-12-01 04:17:18.312000: I training.py:192] Running evaluation for step 35000\n",
      "2024-12-01 04:18:05.406000: I training.py:192] Evaluation result for step 35000: loss = 0.733083 ; perplexity = 2.081489\n",
      "2024-12-01 04:19:05.944000: I runner.py:310] Step = 35100 ; steps/s = 1.65, tokens/s = 43243 (43243 target) ; Learning rate = 0.000472 ; Loss = 1.484206\n",
      "2024-12-01 04:20:07.087000: I runner.py:310] Step = 35200 ; steps/s = 1.64, tokens/s = 43623 (43623 target) ; Learning rate = 0.000471 ; Loss = 1.482557\n",
      "2024-12-01 04:21:08.195000: I runner.py:310] Step = 35300 ; steps/s = 1.64, tokens/s = 43631 (43631 target) ; Learning rate = 0.000470 ; Loss = 1.484272\n",
      "2024-12-01 04:22:08.864000: I runner.py:310] Step = 35400 ; steps/s = 1.65, tokens/s = 43129 (43129 target) ; Learning rate = 0.000470 ; Loss = 1.488051\n",
      "2024-12-01 04:23:10.019000: I runner.py:310] Step = 35500 ; steps/s = 1.64, tokens/s = 43601 (43601 target) ; Learning rate = 0.000469 ; Loss = 1.488076\n",
      "2024-12-01 04:24:10.710000: I runner.py:310] Step = 35600 ; steps/s = 1.65, tokens/s = 43120 (43120 target) ; Learning rate = 0.000468 ; Loss = 1.478283\n",
      "2024-12-01 04:25:11.820000: I runner.py:310] Step = 35700 ; steps/s = 1.64, tokens/s = 43643 (43643 target) ; Learning rate = 0.000468 ; Loss = 1.474029\n",
      "2024-12-01 04:26:13.031000: I runner.py:310] Step = 35800 ; steps/s = 1.63, tokens/s = 43555 (43555 target) ; Learning rate = 0.000467 ; Loss = 1.485350\n",
      "2024-12-01 04:27:13.739000: I runner.py:310] Step = 35900 ; steps/s = 1.65, tokens/s = 43112 (43112 target) ; Learning rate = 0.000466 ; Loss = 1.481505\n",
      "2024-12-01 04:28:14.866000: I runner.py:310] Step = 36000 ; steps/s = 1.64, tokens/s = 43634 (43634 target) ; Learning rate = 0.000466 ; Loss = 1.488023\n",
      "2024-12-01 04:29:15.623000: I runner.py:310] Step = 36100 ; steps/s = 1.65, tokens/s = 43185 (43185 target) ; Learning rate = 0.000465 ; Loss = 1.479231\n",
      "2024-12-01 04:30:16.734000: I runner.py:310] Step = 36200 ; steps/s = 1.64, tokens/s = 43526 (43526 target) ; Learning rate = 0.000465 ; Loss = 1.483074\n",
      "2024-12-01 04:31:17.919000: I runner.py:310] Step = 36300 ; steps/s = 1.63, tokens/s = 43576 (43576 target) ; Learning rate = 0.000464 ; Loss = 1.480827\n",
      "2024-12-01 04:32:18.588000: I runner.py:310] Step = 36400 ; steps/s = 1.65, tokens/s = 43145 (43145 target) ; Learning rate = 0.000463 ; Loss = 1.487896\n",
      "2024-12-01 04:33:19.792000: I runner.py:310] Step = 36500 ; steps/s = 1.63, tokens/s = 43572 (43572 target) ; Learning rate = 0.000463 ; Loss = 1.493733\n",
      "2024-12-01 04:34:20.986000: I runner.py:310] Step = 36600 ; steps/s = 1.63, tokens/s = 43568 (43568 target) ; Learning rate = 0.000462 ; Loss = 1.487571\n",
      "2024-12-01 04:35:21.644000: I runner.py:310] Step = 36700 ; steps/s = 1.65, tokens/s = 43155 (43155 target) ; Learning rate = 0.000461 ; Loss = 1.482189\n",
      "2024-12-01 04:36:22.837000: I runner.py:310] Step = 36800 ; steps/s = 1.63, tokens/s = 43571 (43571 target) ; Learning rate = 0.000461 ; Loss = 1.482808\n",
      "2024-12-01 04:37:23.574000: I runner.py:310] Step = 36900 ; steps/s = 1.65, tokens/s = 43087 (43087 target) ; Learning rate = 0.000460 ; Loss = 1.477187\n",
      "2024-12-01 04:38:24.742000: I runner.py:310] Step = 37000 ; steps/s = 1.63, tokens/s = 43598 (43598 target) ; Learning rate = 0.000460 ; Loss = 1.481077\n",
      "2024-12-01 04:39:25.894000: I runner.py:310] Step = 37100 ; steps/s = 1.64, tokens/s = 43605 (43605 target) ; Learning rate = 0.000459 ; Loss = 1.485286\n",
      "2024-12-01 04:40:26.563000: I runner.py:310] Step = 37200 ; steps/s = 1.65, tokens/s = 43141 (43141 target) ; Learning rate = 0.000458 ; Loss = 1.485321\n",
      "2024-12-01 04:41:27.639000: I runner.py:310] Step = 37300 ; steps/s = 1.64, tokens/s = 43670 (43670 target) ; Learning rate = 0.000458 ; Loss = 1.482155\n",
      "2024-12-01 04:42:28.333000: I runner.py:310] Step = 37400 ; steps/s = 1.65, tokens/s = 43106 (43106 target) ; Learning rate = 0.000457 ; Loss = 1.472922\n",
      "2024-12-01 04:43:29.398000: I runner.py:310] Step = 37500 ; steps/s = 1.64, tokens/s = 43677 (43677 target) ; Learning rate = 0.000456 ; Loss = 1.483636\n",
      "2024-12-01 04:44:30.551000: I runner.py:310] Step = 37600 ; steps/s = 1.64, tokens/s = 43600 (43600 target) ; Learning rate = 0.000456 ; Loss = 1.480891\n",
      "2024-12-01 04:45:31.258000: I runner.py:310] Step = 37700 ; steps/s = 1.65, tokens/s = 43106 (43106 target) ; Learning rate = 0.000455 ; Loss = 1.477642\n",
      "2024-12-01 04:46:32.426000: I runner.py:310] Step = 37800 ; steps/s = 1.64, tokens/s = 43605 (43605 target) ; Learning rate = 0.000455 ; Loss = 1.485141\n",
      "2024-12-01 04:47:33.553000: I runner.py:310] Step = 37900 ; steps/s = 1.64, tokens/s = 43626 (43626 target) ; Learning rate = 0.000454 ; Loss = 1.483797\n",
      "2024-12-01 04:48:34.268000: I runner.py:310] Step = 38000 ; steps/s = 1.65, tokens/s = 43100 (43100 target) ; Learning rate = 0.000453 ; Loss = 1.480083\n",
      "2024-12-01 04:49:35.454000: I runner.py:310] Step = 38100 ; steps/s = 1.63, tokens/s = 43582 (43582 target) ; Learning rate = 0.000453 ; Loss = 1.481281\n",
      "2024-12-01 04:50:36.140000: I runner.py:310] Step = 38200 ; steps/s = 1.65, tokens/s = 43122 (43122 target) ; Learning rate = 0.000452 ; Loss = 1.476973\n",
      "2024-12-01 04:51:37.300000: I runner.py:310] Step = 38300 ; steps/s = 1.64, tokens/s = 43613 (43613 target) ; Learning rate = 0.000452 ; Loss = 1.475862\n",
      "2024-12-01 04:52:38.436000: I runner.py:310] Step = 38400 ; steps/s = 1.64, tokens/s = 43598 (43598 target) ; Learning rate = 0.000451 ; Loss = 1.482416\n",
      "2024-12-01 04:53:39.124000: I runner.py:310] Step = 38500 ; steps/s = 1.65, tokens/s = 43133 (43133 target) ; Learning rate = 0.000450 ; Loss = 1.481188\n",
      "2024-12-01 04:54:40.245000: I runner.py:310] Step = 38600 ; steps/s = 1.64, tokens/s = 43626 (43626 target) ; Learning rate = 0.000450 ; Loss = 1.482316\n",
      "2024-12-01 04:55:40.948000: I runner.py:310] Step = 38700 ; steps/s = 1.65, tokens/s = 43116 (43116 target) ; Learning rate = 0.000449 ; Loss = 1.477061\n",
      "2024-12-01 04:56:42.084000: I runner.py:310] Step = 38800 ; steps/s = 1.64, tokens/s = 43622 (43622 target) ; Learning rate = 0.000449 ; Loss = 1.474188\n",
      "2024-12-01 04:57:43.289000: I runner.py:310] Step = 38900 ; steps/s = 1.63, tokens/s = 43558 (43558 target) ; Learning rate = 0.000448 ; Loss = 1.483119\n",
      "2024-12-01 04:58:43.944000: I runner.py:310] Step = 39000 ; steps/s = 1.65, tokens/s = 43160 (43160 target) ; Learning rate = 0.000448 ; Loss = 1.477059\n",
      "2024-12-01 04:59:45.065000: I runner.py:310] Step = 39100 ; steps/s = 1.64, tokens/s = 43631 (43631 target) ; Learning rate = 0.000447 ; Loss = 1.483267\n",
      "2024-12-01 05:00:46.320000: I runner.py:310] Step = 39200 ; steps/s = 1.63, tokens/s = 43530 (43530 target) ; Learning rate = 0.000446 ; Loss = 1.476596\n",
      "2024-12-01 05:01:47.004000: I runner.py:310] Step = 39300 ; steps/s = 1.65, tokens/s = 43132 (43132 target) ; Learning rate = 0.000446 ; Loss = 1.474872\n",
      "2024-12-01 05:02:48.122000: I runner.py:310] Step = 39400 ; steps/s = 1.64, tokens/s = 43618 (43618 target) ; Learning rate = 0.000445 ; Loss = 1.480285\n",
      "2024-12-01 05:03:48.801000: I runner.py:310] Step = 39500 ; steps/s = 1.65, tokens/s = 43131 (43131 target) ; Learning rate = 0.000445 ; Loss = 1.475453\n",
      "2024-12-01 05:04:49.902000: I runner.py:310] Step = 39600 ; steps/s = 1.64, tokens/s = 43636 (43636 target) ; Learning rate = 0.000444 ; Loss = 1.472173\n",
      "2024-12-01 05:05:51.021000: I runner.py:310] Step = 39700 ; steps/s = 1.64, tokens/s = 43630 (43630 target) ; Learning rate = 0.000444 ; Loss = 1.481660\n",
      "2024-12-01 05:06:51.640000: I runner.py:310] Step = 39800 ; steps/s = 1.65, tokens/s = 43179 (43179 target) ; Learning rate = 0.000443 ; Loss = 1.477903\n",
      "2024-12-01 05:07:52.838000: I runner.py:310] Step = 39900 ; steps/s = 1.63, tokens/s = 43578 (43578 target) ; Learning rate = 0.000442 ; Loss = 1.482755\n",
      "2024-12-01 05:08:53.488000: I runner.py:310] Step = 40000 ; steps/s = 1.65, tokens/s = 43151 (43151 target) ; Learning rate = 0.000442 ; Loss = 1.470612\n",
      "2024-12-01 05:08:55.428000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-40000\n",
      "2024-12-01 05:08:55.428000: I training.py:192] Running evaluation for step 40000\n",
      "2024-12-01 05:09:43.048000: I training.py:192] Evaluation result for step 40000: loss = 0.748731 ; perplexity = 2.114314\n",
      "2024-12-01 05:10:43.999000: I runner.py:310] Step = 40100 ; steps/s = 1.64, tokens/s = 43758 (43758 target) ; Learning rate = 0.000441 ; Loss = 1.480600\n",
      "2024-12-01 05:11:45.156000: I runner.py:310] Step = 40200 ; steps/s = 1.64, tokens/s = 43602 (43602 target) ; Learning rate = 0.000441 ; Loss = 1.472719\n",
      "2024-12-01 05:12:45.843000: I runner.py:310] Step = 40300 ; steps/s = 1.65, tokens/s = 43117 (43117 target) ; Learning rate = 0.000440 ; Loss = 1.477038\n",
      "2024-12-01 05:13:46.936000: I runner.py:310] Step = 40400 ; steps/s = 1.64, tokens/s = 43645 (43645 target) ; Learning rate = 0.000440 ; Loss = 1.473626\n",
      "2024-12-01 05:14:48.127000: I runner.py:310] Step = 40500 ; steps/s = 1.63, tokens/s = 43590 (43590 target) ; Learning rate = 0.000439 ; Loss = 1.484208\n",
      "2024-12-01 05:15:48.817000: I runner.py:310] Step = 40600 ; steps/s = 1.65, tokens/s = 43117 (43117 target) ; Learning rate = 0.000439 ; Loss = 1.474399\n",
      "2024-12-01 05:16:49.930000: I runner.py:310] Step = 40700 ; steps/s = 1.64, tokens/s = 43644 (43644 target) ; Learning rate = 0.000438 ; Loss = 1.474369\n",
      "2024-12-01 05:17:50.617000: I runner.py:310] Step = 40800 ; steps/s = 1.65, tokens/s = 43114 (43114 target) ; Learning rate = 0.000438 ; Loss = 1.468055\n",
      "2024-12-01 05:18:51.722000: I runner.py:310] Step = 40900 ; steps/s = 1.64, tokens/s = 43641 (43641 target) ; Learning rate = 0.000437 ; Loss = 1.472449\n",
      "2024-12-01 05:19:52.955000: I runner.py:310] Step = 41000 ; steps/s = 1.63, tokens/s = 43551 (43551 target) ; Learning rate = 0.000437 ; Loss = 1.479463\n",
      "2024-12-01 05:20:53.666000: I runner.py:310] Step = 41100 ; steps/s = 1.65, tokens/s = 43107 (43107 target) ; Learning rate = 0.000436 ; Loss = 1.476851\n",
      "2024-12-01 05:21:54.860000: I runner.py:310] Step = 41200 ; steps/s = 1.63, tokens/s = 43586 (43586 target) ; Learning rate = 0.000435 ; Loss = 1.481536\n",
      "2024-12-01 05:22:55.627000: I runner.py:310] Step = 41300 ; steps/s = 1.65, tokens/s = 43059 (43059 target) ; Learning rate = 0.000435 ; Loss = 1.469437\n",
      "2024-12-01 05:23:56.868000: I runner.py:310] Step = 41400 ; steps/s = 1.63, tokens/s = 43529 (43529 target) ; Learning rate = 0.000434 ; Loss = 1.469606\n",
      "2024-12-01 05:24:58.075000: I runner.py:310] Step = 41500 ; steps/s = 1.63, tokens/s = 43582 (43582 target) ; Learning rate = 0.000434 ; Loss = 1.478093\n",
      "2024-12-01 05:25:58.822000: I runner.py:310] Step = 41600 ; steps/s = 1.65, tokens/s = 43077 (43077 target) ; Learning rate = 0.000433 ; Loss = 1.471621\n",
      "2024-12-01 05:26:59.998000: I runner.py:310] Step = 41700 ; steps/s = 1.63, tokens/s = 43594 (43594 target) ; Learning rate = 0.000433 ; Loss = 1.475003\n",
      "2024-12-01 05:28:01.081000: I runner.py:310] Step = 41800 ; steps/s = 1.64, tokens/s = 43659 (43659 target) ; Learning rate = 0.000432 ; Loss = 1.478842\n",
      "2024-12-01 05:29:01.732000: I runner.py:310] Step = 41900 ; steps/s = 1.65, tokens/s = 43155 (43155 target) ; Learning rate = 0.000432 ; Loss = 1.472254\n",
      "2024-12-01 05:30:02.972000: I runner.py:310] Step = 42000 ; steps/s = 1.63, tokens/s = 43542 (43542 target) ; Learning rate = 0.000431 ; Loss = 1.474160\n",
      "2024-12-01 05:31:03.681000: I runner.py:310] Step = 42100 ; steps/s = 1.65, tokens/s = 43104 (43104 target) ; Learning rate = 0.000431 ; Loss = 1.466697\n",
      "2024-12-01 05:32:04.895000: I runner.py:310] Step = 42200 ; steps/s = 1.63, tokens/s = 43569 (43569 target) ; Learning rate = 0.000430 ; Loss = 1.482762\n",
      "2024-12-01 05:33:06.088000: I runner.py:310] Step = 42300 ; steps/s = 1.63, tokens/s = 43575 (43575 target) ; Learning rate = 0.000430 ; Loss = 1.474348\n",
      "2024-12-01 05:34:06.797000: I runner.py:310] Step = 42400 ; steps/s = 1.65, tokens/s = 43098 (43098 target) ; Learning rate = 0.000429 ; Loss = 1.470724\n",
      "2024-12-01 05:35:07.933000: I runner.py:310] Step = 42500 ; steps/s = 1.64, tokens/s = 43630 (43630 target) ; Learning rate = 0.000429 ; Loss = 1.477165\n",
      "2024-12-01 05:36:08.647000: I runner.py:310] Step = 42600 ; steps/s = 1.65, tokens/s = 43104 (43104 target) ; Learning rate = 0.000428 ; Loss = 1.466648\n",
      "2024-12-01 05:37:09.839000: I runner.py:310] Step = 42700 ; steps/s = 1.63, tokens/s = 43573 (43573 target) ; Learning rate = 0.000428 ; Loss = 1.466097\n",
      "2024-12-01 05:38:10.962000: I runner.py:310] Step = 42800 ; steps/s = 1.64, tokens/s = 43628 (43628 target) ; Learning rate = 0.000427 ; Loss = 1.471290\n",
      "2024-12-01 05:39:11.718000: I runner.py:310] Step = 42900 ; steps/s = 1.65, tokens/s = 43091 (43091 target) ; Learning rate = 0.000427 ; Loss = 1.472407\n",
      "2024-12-01 05:40:12.874000: I runner.py:310] Step = 43000 ; steps/s = 1.64, tokens/s = 43592 (43592 target) ; Learning rate = 0.000426 ; Loss = 1.476410\n",
      "2024-12-01 05:41:14.124000: I runner.py:310] Step = 43100 ; steps/s = 1.63, tokens/s = 43538 (43538 target) ; Learning rate = 0.000426 ; Loss = 1.475325\n",
      "2024-12-01 05:42:14.889000: I runner.py:310] Step = 43200 ; steps/s = 1.65, tokens/s = 43065 (43065 target) ; Learning rate = 0.000425 ; Loss = 1.477476\n",
      "2024-12-01 05:43:16.090000: I runner.py:310] Step = 43300 ; steps/s = 1.63, tokens/s = 43580 (43580 target) ; Learning rate = 0.000425 ; Loss = 1.474382\n",
      "2024-12-01 05:44:16.745000: I runner.py:310] Step = 43400 ; steps/s = 1.65, tokens/s = 43140 (43140 target) ; Learning rate = 0.000424 ; Loss = 1.464057\n",
      "2024-12-01 05:45:17.891000: I runner.py:310] Step = 43500 ; steps/s = 1.64, tokens/s = 43616 (43616 target) ; Learning rate = 0.000424 ; Loss = 1.473552\n",
      "2024-12-01 05:46:19.064000: I runner.py:310] Step = 43600 ; steps/s = 1.63, tokens/s = 43597 (43597 target) ; Learning rate = 0.000423 ; Loss = 1.472853\n",
      "2024-12-01 05:47:19.768000: I runner.py:310] Step = 43700 ; steps/s = 1.65, tokens/s = 43108 (43108 target) ; Learning rate = 0.000423 ; Loss = 1.472263\n",
      "2024-12-01 05:48:21.005000: I runner.py:310] Step = 43800 ; steps/s = 1.63, tokens/s = 43548 (43548 target) ; Learning rate = 0.000422 ; Loss = 1.474234\n",
      "2024-12-01 05:49:21.673000: I runner.py:310] Step = 43900 ; steps/s = 1.65, tokens/s = 43136 (43136 target) ; Learning rate = 0.000422 ; Loss = 1.464634\n",
      "2024-12-01 05:50:22.874000: I runner.py:310] Step = 44000 ; steps/s = 1.63, tokens/s = 43578 (43578 target) ; Learning rate = 0.000421 ; Loss = 1.466859\n",
      "2024-12-01 05:51:24.088000: I runner.py:310] Step = 44100 ; steps/s = 1.63, tokens/s = 43560 (43560 target) ; Learning rate = 0.000421 ; Loss = 1.472807\n",
      "2024-12-01 05:52:24.896000: I runner.py:310] Step = 44200 ; steps/s = 1.64, tokens/s = 43041 (43041 target) ; Learning rate = 0.000420 ; Loss = 1.468109\n",
      "2024-12-01 05:53:26.024000: I runner.py:310] Step = 44300 ; steps/s = 1.64, tokens/s = 43616 (43616 target) ; Learning rate = 0.000420 ; Loss = 1.467137\n",
      "2024-12-01 05:54:27.178000: I runner.py:310] Step = 44400 ; steps/s = 1.64, tokens/s = 43607 (43607 target) ; Learning rate = 0.000419 ; Loss = 1.476200\n",
      "2024-12-01 05:55:27.904000: I runner.py:310] Step = 44500 ; steps/s = 1.65, tokens/s = 43093 (43093 target) ; Learning rate = 0.000419 ; Loss = 1.474195\n",
      "2024-12-01 05:56:29.029000: I runner.py:310] Step = 44600 ; steps/s = 1.64, tokens/s = 43622 (43622 target) ; Learning rate = 0.000419 ; Loss = 1.466454\n",
      "2024-12-01 05:57:29.777000: I runner.py:310] Step = 44700 ; steps/s = 1.65, tokens/s = 43088 (43088 target) ; Learning rate = 0.000418 ; Loss = 1.463307\n",
      "2024-12-01 05:58:30.952000: I runner.py:310] Step = 44800 ; steps/s = 1.63, tokens/s = 43595 (43595 target) ; Learning rate = 0.000418 ; Loss = 1.473978\n",
      "2024-12-01 05:59:32.088000: I runner.py:310] Step = 44900 ; steps/s = 1.64, tokens/s = 43615 (43615 target) ; Learning rate = 0.000417 ; Loss = 1.476201\n",
      "2024-12-01 06:00:32.879000: I runner.py:310] Step = 45000 ; steps/s = 1.65, tokens/s = 43045 (43045 target) ; Learning rate = 0.000417 ; Loss = 1.467854\n",
      "2024-12-01 06:00:32.881000: I training.py:192] Running evaluation for step 45000\n",
      "2024-12-01 06:01:20.635000: I training.py:192] Evaluation result for step 45000: loss = 0.754351 ; perplexity = 2.126231\n",
      "2024-12-01 06:02:21.651000: I runner.py:310] Step = 45100 ; steps/s = 1.64, tokens/s = 43712 (43712 target) ; Learning rate = 0.000416 ; Loss = 1.472130\n",
      "2024-12-01 06:03:22.363000: I runner.py:310] Step = 45200 ; steps/s = 1.65, tokens/s = 43109 (43109 target) ; Learning rate = 0.000416 ; Loss = 1.464476\n",
      "2024-12-01 06:04:23.491000: I runner.py:310] Step = 45300 ; steps/s = 1.64, tokens/s = 43633 (43633 target) ; Learning rate = 0.000415 ; Loss = 1.476465\n",
      "2024-12-01 06:05:24.666000: I runner.py:310] Step = 45400 ; steps/s = 1.63, tokens/s = 43583 (43583 target) ; Learning rate = 0.000415 ; Loss = 1.466365\n",
      "2024-12-01 06:06:25.363000: I runner.py:310] Step = 45500 ; steps/s = 1.65, tokens/s = 43123 (43123 target) ; Learning rate = 0.000414 ; Loss = 1.470039\n",
      "2024-12-01 06:07:26.512000: I runner.py:310] Step = 45600 ; steps/s = 1.64, tokens/s = 43610 (43610 target) ; Learning rate = 0.000414 ; Loss = 1.470236\n",
      "2024-12-01 06:08:27.728000: I runner.py:310] Step = 45700 ; steps/s = 1.63, tokens/s = 43550 (43550 target) ; Learning rate = 0.000413 ; Loss = 1.468838\n",
      "2024-12-01 06:09:28.394000: I runner.py:310] Step = 45800 ; steps/s = 1.65, tokens/s = 43143 (43143 target) ; Learning rate = 0.000413 ; Loss = 1.462587\n",
      "2024-12-01 06:10:29.544000: I runner.py:310] Step = 45900 ; steps/s = 1.64, tokens/s = 43605 (43605 target) ; Learning rate = 0.000413 ; Loss = 1.474123\n",
      "2024-12-01 06:11:30.292000: I runner.py:310] Step = 46000 ; steps/s = 1.65, tokens/s = 43086 (43086 target) ; Learning rate = 0.000412 ; Loss = 1.467129\n",
      "2024-12-01 06:12:31.427000: I runner.py:310] Step = 46100 ; steps/s = 1.64, tokens/s = 43620 (43620 target) ; Learning rate = 0.000412 ; Loss = 1.472518\n",
      "2024-12-01 06:13:32.649000: I runner.py:310] Step = 46200 ; steps/s = 1.63, tokens/s = 43550 (43550 target) ; Learning rate = 0.000411 ; Loss = 1.471711\n",
      "2024-12-01 06:14:33.334000: I runner.py:310] Step = 46300 ; steps/s = 1.65, tokens/s = 43133 (43133 target) ; Learning rate = 0.000411 ; Loss = 1.472758\n",
      "2024-12-01 06:15:34.483000: I runner.py:310] Step = 46400 ; steps/s = 1.64, tokens/s = 43607 (43607 target) ; Learning rate = 0.000410 ; Loss = 1.466266\n",
      "2024-12-01 06:16:35.217000: I runner.py:310] Step = 46500 ; steps/s = 1.65, tokens/s = 43083 (43083 target) ; Learning rate = 0.000410 ; Loss = 1.467301\n",
      "2024-12-01 06:17:36.436000: I runner.py:310] Step = 46600 ; steps/s = 1.63, tokens/s = 43571 (43571 target) ; Learning rate = 0.000409 ; Loss = 1.467559\n",
      "2024-12-01 06:18:37.590000: I runner.py:310] Step = 46700 ; steps/s = 1.64, tokens/s = 43594 (43594 target) ; Learning rate = 0.000409 ; Loss = 1.480923\n",
      "2024-12-01 06:19:38.322000: I runner.py:310] Step = 46800 ; steps/s = 1.65, tokens/s = 43091 (43091 target) ; Learning rate = 0.000409 ; Loss = 1.471086\n",
      "2024-12-01 06:20:39.445000: I runner.py:310] Step = 46900 ; steps/s = 1.64, tokens/s = 43636 (43636 target) ; Learning rate = 0.000408 ; Loss = 1.471013\n",
      "2024-12-01 06:21:40.554000: I runner.py:310] Step = 47000 ; steps/s = 1.64, tokens/s = 43633 (43633 target) ; Learning rate = 0.000408 ; Loss = 1.466616\n",
      "2024-12-01 06:22:41.262000: I runner.py:310] Step = 47100 ; steps/s = 1.65, tokens/s = 43110 (43110 target) ; Learning rate = 0.000407 ; Loss = 1.467984\n",
      "2024-12-01 06:23:42.382000: I runner.py:310] Step = 47200 ; steps/s = 1.64, tokens/s = 43632 (43632 target) ; Learning rate = 0.000407 ; Loss = 1.473013\n",
      "2024-12-01 06:24:43.058000: I runner.py:310] Step = 47300 ; steps/s = 1.65, tokens/s = 43134 (43134 target) ; Learning rate = 0.000406 ; Loss = 1.467318\n",
      "2024-12-01 06:25:44.275000: I runner.py:310] Step = 47400 ; steps/s = 1.63, tokens/s = 43559 (43559 target) ; Learning rate = 0.000406 ; Loss = 1.470578\n",
      "2024-12-01 06:26:45.464000: I runner.py:310] Step = 47500 ; steps/s = 1.63, tokens/s = 43572 (43572 target) ; Learning rate = 0.000406 ; Loss = 1.474224\n",
      "2024-12-01 06:27:46.224000: I runner.py:310] Step = 47600 ; steps/s = 1.65, tokens/s = 43072 (43072 target) ; Learning rate = 0.000405 ; Loss = 1.465517\n",
      "2024-12-01 06:28:47.393000: I runner.py:310] Step = 47700 ; steps/s = 1.64, tokens/s = 43600 (43600 target) ; Learning rate = 0.000405 ; Loss = 1.466902\n",
      "2024-12-01 06:29:48.111000: I runner.py:310] Step = 47800 ; steps/s = 1.65, tokens/s = 43109 (43109 target) ; Learning rate = 0.000404 ; Loss = 1.463990\n",
      "2024-12-01 06:30:49.301000: I runner.py:310] Step = 47900 ; steps/s = 1.63, tokens/s = 43578 (43578 target) ; Learning rate = 0.000404 ; Loss = 1.470466\n",
      "2024-12-01 06:31:50.428000: I runner.py:310] Step = 48000 ; steps/s = 1.64, tokens/s = 43626 (43626 target) ; Learning rate = 0.000403 ; Loss = 1.472402\n",
      "2024-12-01 06:32:51.140000: I runner.py:310] Step = 48100 ; steps/s = 1.65, tokens/s = 43113 (43113 target) ; Learning rate = 0.000403 ; Loss = 1.463021\n",
      "2024-12-01 06:33:52.275000: I runner.py:310] Step = 48200 ; steps/s = 1.64, tokens/s = 43611 (43611 target) ; Learning rate = 0.000403 ; Loss = 1.467983\n",
      "2024-12-01 06:34:53.440000: I runner.py:310] Step = 48300 ; steps/s = 1.64, tokens/s = 43604 (43604 target) ; Learning rate = 0.000402 ; Loss = 1.472119\n",
      "2024-12-01 06:35:54.184000: I runner.py:310] Step = 48400 ; steps/s = 1.65, tokens/s = 43088 (43088 target) ; Learning rate = 0.000402 ; Loss = 1.463943\n",
      "2024-12-01 06:36:55.311000: I runner.py:310] Step = 48500 ; steps/s = 1.64, tokens/s = 43623 (43623 target) ; Learning rate = 0.000401 ; Loss = 1.471949\n",
      "2024-12-01 06:37:56.078000: I runner.py:310] Step = 48600 ; steps/s = 1.65, tokens/s = 43060 (43060 target) ; Learning rate = 0.000401 ; Loss = 1.461969\n",
      "2024-12-01 06:38:57.195000: I runner.py:310] Step = 48700 ; steps/s = 1.64, tokens/s = 43626 (43626 target) ; Learning rate = 0.000401 ; Loss = 1.466745\n",
      "2024-12-01 06:39:58.367000: I runner.py:310] Step = 48800 ; steps/s = 1.63, tokens/s = 43602 (43602 target) ; Learning rate = 0.000400 ; Loss = 1.468303\n",
      "2024-12-01 06:40:59.107000: I runner.py:310] Step = 48900 ; steps/s = 1.65, tokens/s = 43074 (43074 target) ; Learning rate = 0.000400 ; Loss = 1.469873\n",
      "2024-12-01 06:42:00.211000: I runner.py:310] Step = 49000 ; steps/s = 1.64, tokens/s = 43648 (43648 target) ; Learning rate = 0.000399 ; Loss = 1.459225\n",
      "2024-12-01 06:43:00.902000: I runner.py:310] Step = 49100 ; steps/s = 1.65, tokens/s = 43124 (43124 target) ; Learning rate = 0.000399 ; Loss = 1.464544\n",
      "2024-12-01 06:44:02.296000: I runner.py:310] Step = 49200 ; steps/s = 1.63, tokens/s = 43440 (43440 target) ; Learning rate = 0.000398 ; Loss = 1.464829\n",
      "2024-12-01 06:45:03.545000: I runner.py:310] Step = 49300 ; steps/s = 1.63, tokens/s = 43538 (43538 target) ; Learning rate = 0.000398 ; Loss = 1.466034\n",
      "2024-12-01 06:46:04.297000: I runner.py:310] Step = 49400 ; steps/s = 1.65, tokens/s = 43078 (43078 target) ; Learning rate = 0.000398 ; Loss = 1.468804\n",
      "2024-12-01 06:47:05.479000: I runner.py:310] Step = 49500 ; steps/s = 1.63, tokens/s = 43574 (43574 target) ; Learning rate = 0.000397 ; Loss = 1.464650\n",
      "2024-12-01 06:48:06.647000: I runner.py:310] Step = 49600 ; steps/s = 1.63, tokens/s = 43602 (43602 target) ; Learning rate = 0.000397 ; Loss = 1.473444\n",
      "2024-12-01 06:49:07.364000: I runner.py:310] Step = 49700 ; steps/s = 1.65, tokens/s = 43108 (43108 target) ; Learning rate = 0.000396 ; Loss = 1.462961\n",
      "2024-12-01 06:50:08.571000: I runner.py:310] Step = 49800 ; steps/s = 1.63, tokens/s = 43574 (43574 target) ; Learning rate = 0.000396 ; Loss = 1.466076\n",
      "2024-12-01 06:51:09.350000: I runner.py:310] Step = 49900 ; steps/s = 1.65, tokens/s = 43040 (43040 target) ; Learning rate = 0.000396 ; Loss = 1.462872\n",
      "2024-12-01 06:52:10.481000: I runner.py:310] Step = 50000 ; steps/s = 1.64, tokens/s = 43635 (43635 target) ; Learning rate = 0.000395 ; Loss = 1.471246\n",
      "2024-12-01 06:52:12.542000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-50000\n",
      "2024-12-01 06:52:12.542000: I training.py:192] Running evaluation for step 50000\n",
      "2024-12-01 06:52:59.957000: I training.py:192] Evaluation result for step 50000: loss = 0.762131 ; perplexity = 2.142837\n",
      "2024-12-01 06:54:01.016000: I runner.py:310] Step = 50100 ; steps/s = 1.64, tokens/s = 43681 (43681 target) ; Learning rate = 0.000395 ; Loss = 1.466542\n",
      "2024-12-01 06:55:01.687000: I runner.py:310] Step = 50200 ; steps/s = 1.65, tokens/s = 43130 (43130 target) ; Learning rate = 0.000394 ; Loss = 1.471024\n",
      "2024-12-01 06:56:02.817000: I runner.py:310] Step = 50300 ; steps/s = 1.64, tokens/s = 43621 (43621 target) ; Learning rate = 0.000394 ; Loss = 1.471869\n",
      "2024-12-01 06:57:03.569000: I runner.py:310] Step = 50400 ; steps/s = 1.65, tokens/s = 43081 (43081 target) ; Learning rate = 0.000394 ; Loss = 1.462599\n",
      "2024-12-01 06:58:04.735000: I runner.py:310] Step = 50500 ; steps/s = 1.64, tokens/s = 43592 (43592 target) ; Learning rate = 0.000393 ; Loss = 1.462102\n",
      "2024-12-01 06:59:05.960000: I runner.py:310] Step = 50600 ; steps/s = 1.63, tokens/s = 43558 (43558 target) ; Learning rate = 0.000393 ; Loss = 1.468318\n",
      "2024-12-01 07:00:06.632000: I runner.py:310] Step = 50700 ; steps/s = 1.65, tokens/s = 43139 (43139 target) ; Learning rate = 0.000393 ; Loss = 1.463154\n",
      "2024-12-01 07:01:07.818000: I runner.py:310] Step = 50800 ; steps/s = 1.63, tokens/s = 43579 (43579 target) ; Learning rate = 0.000392 ; Loss = 1.469779\n",
      "2024-12-01 07:02:09.005000: I runner.py:310] Step = 50900 ; steps/s = 1.63, tokens/s = 43580 (43580 target) ; Learning rate = 0.000392 ; Loss = 1.467598\n",
      "2024-12-01 07:03:09.694000: I runner.py:310] Step = 51000 ; steps/s = 1.65, tokens/s = 43123 (43123 target) ; Learning rate = 0.000391 ; Loss = 1.458916\n",
      "2024-12-01 07:04:10.795000: I runner.py:310] Step = 51100 ; steps/s = 1.64, tokens/s = 43644 (43644 target) ; Learning rate = 0.000391 ; Loss = 1.466131\n",
      "2024-12-01 07:05:11.570000: I runner.py:310] Step = 51200 ; steps/s = 1.65, tokens/s = 43062 (43062 target) ; Learning rate = 0.000391 ; Loss = 1.467895\n",
      "2024-12-01 07:06:12.689000: I runner.py:310] Step = 51300 ; steps/s = 1.64, tokens/s = 43626 (43626 target) ; Learning rate = 0.000390 ; Loss = 1.468497\n",
      "2024-12-01 07:07:13.861000: I runner.py:310] Step = 51400 ; steps/s = 1.63, tokens/s = 43594 (43594 target) ; Learning rate = 0.000390 ; Loss = 1.466204\n",
      "2024-12-01 07:08:14.597000: I runner.py:310] Step = 51500 ; steps/s = 1.65, tokens/s = 43091 (43091 target) ; Learning rate = 0.000389 ; Loss = 1.461343\n",
      "2024-12-01 07:09:15.754000: I runner.py:310] Step = 51600 ; steps/s = 1.64, tokens/s = 43597 (43597 target) ; Learning rate = 0.000389 ; Loss = 1.467483\n",
      "2024-12-01 07:10:16.443000: I runner.py:310] Step = 51700 ; steps/s = 1.65, tokens/s = 43121 (43121 target) ; Learning rate = 0.000389 ; Loss = 1.462297\n",
      "2024-12-01 07:11:17.607000: I runner.py:310] Step = 51800 ; steps/s = 1.64, tokens/s = 43596 (43596 target) ; Learning rate = 0.000388 ; Loss = 1.458080\n",
      "2024-12-01 07:12:18.764000: I runner.py:310] Step = 51900 ; steps/s = 1.64, tokens/s = 43605 (43605 target) ; Learning rate = 0.000388 ; Loss = 1.469193\n",
      "2024-12-01 07:13:19.469000: I runner.py:310] Step = 52000 ; steps/s = 1.65, tokens/s = 43110 (43110 target) ; Learning rate = 0.000388 ; Loss = 1.468727\n",
      "2024-12-01 07:14:20.616000: I runner.py:310] Step = 52100 ; steps/s = 1.64, tokens/s = 43604 (43604 target) ; Learning rate = 0.000387 ; Loss = 1.467252\n",
      "2024-12-01 07:15:21.706000: I runner.py:310] Step = 52200 ; steps/s = 1.64, tokens/s = 43546 (43546 target) ; Learning rate = 0.000387 ; Loss = 1.471645\n",
      "2024-12-01 07:16:22.453000: I runner.py:310] Step = 52300 ; steps/s = 1.65, tokens/s = 43201 (43201 target) ; Learning rate = 0.000386 ; Loss = 1.458284\n",
      "2024-12-01 07:17:23.613000: I runner.py:310] Step = 52400 ; steps/s = 1.64, tokens/s = 43597 (43597 target) ; Learning rate = 0.000386 ; Loss = 1.461896\n",
      "2024-12-01 07:18:24.326000: I runner.py:310] Step = 52500 ; steps/s = 1.65, tokens/s = 43104 (43104 target) ; Learning rate = 0.000386 ; Loss = 1.466537\n",
      "2024-12-01 07:19:25.435000: I runner.py:310] Step = 52600 ; steps/s = 1.64, tokens/s = 43630 (43630 target) ; Learning rate = 0.000385 ; Loss = 1.459917\n",
      "2024-12-01 07:20:26.645000: I runner.py:310] Step = 52700 ; steps/s = 1.63, tokens/s = 43571 (43571 target) ; Learning rate = 0.000385 ; Loss = 1.470808\n",
      "2024-12-01 07:21:27.316000: I runner.py:310] Step = 52800 ; steps/s = 1.65, tokens/s = 43131 (43131 target) ; Learning rate = 0.000385 ; Loss = 1.458130\n",
      "2024-12-01 07:22:28.486000: I runner.py:310] Step = 52900 ; steps/s = 1.64, tokens/s = 43597 (43597 target) ; Learning rate = 0.000384 ; Loss = 1.464841\n",
      "2024-12-01 07:23:29.179000: I runner.py:310] Step = 53000 ; steps/s = 1.65, tokens/s = 43119 (43119 target) ; Learning rate = 0.000384 ; Loss = 1.459109\n",
      "2024-12-01 07:24:30.305000: I runner.py:310] Step = 53100 ; steps/s = 1.64, tokens/s = 43635 (43635 target) ; Learning rate = 0.000384 ; Loss = 1.466245\n",
      "2024-12-01 07:25:31.469000: I runner.py:310] Step = 53200 ; steps/s = 1.64, tokens/s = 43585 (43585 target) ; Learning rate = 0.000383 ; Loss = 1.461988\n",
      "2024-12-01 07:26:32.165000: I runner.py:310] Step = 53300 ; steps/s = 1.65, tokens/s = 43117 (43117 target) ; Learning rate = 0.000383 ; Loss = 1.463528\n",
      "2024-12-01 07:27:33.344000: I runner.py:310] Step = 53400 ; steps/s = 1.63, tokens/s = 43592 (43592 target) ; Learning rate = 0.000382 ; Loss = 1.463268\n",
      "2024-12-01 07:28:34.277000: I runner.py:310] Step = 53500 ; steps/s = 1.64, tokens/s = 43381 (43381 target) ; Learning rate = 0.000382 ; Loss = 1.486745\n",
      "2024-12-01 07:29:35.153000: I runner.py:310] Step = 53600 ; steps/s = 1.64, tokens/s = 43376 (43376 target) ; Learning rate = 0.000382 ; Loss = 1.463713\n",
      "2024-12-01 07:30:36.342000: I runner.py:310] Step = 53700 ; steps/s = 1.63, tokens/s = 43574 (43574 target) ; Learning rate = 0.000381 ; Loss = 1.462027\n",
      "2024-12-01 07:31:37.127000: I runner.py:310] Step = 53800 ; steps/s = 1.65, tokens/s = 43056 (43056 target) ; Learning rate = 0.000381 ; Loss = 1.452919\n",
      "2024-12-01 07:32:38.290000: I runner.py:310] Step = 53900 ; steps/s = 1.64, tokens/s = 43601 (43601 target) ; Learning rate = 0.000381 ; Loss = 1.469141\n",
      "2024-12-01 07:33:39.471000: I runner.py:310] Step = 54000 ; steps/s = 1.63, tokens/s = 43582 (43582 target) ; Learning rate = 0.000380 ; Loss = 1.466948\n",
      "2024-12-01 07:34:40.202000: I runner.py:310] Step = 54100 ; steps/s = 1.65, tokens/s = 43096 (43096 target) ; Learning rate = 0.000380 ; Loss = 1.463523\n",
      "2024-12-01 07:35:41.395000: I runner.py:310] Step = 54200 ; steps/s = 1.63, tokens/s = 43569 (43569 target) ; Learning rate = 0.000380 ; Loss = 1.468066\n",
      "2024-12-01 07:36:42.126000: I runner.py:310] Step = 54300 ; steps/s = 1.65, tokens/s = 43094 (43094 target) ; Learning rate = 0.000379 ; Loss = 1.458210\n",
      "2024-12-01 07:37:43.249000: I runner.py:310] Step = 54400 ; steps/s = 1.64, tokens/s = 43628 (43628 target) ; Learning rate = 0.000379 ; Loss = 1.462735\n",
      "2024-12-01 07:38:44.473000: I runner.py:310] Step = 54500 ; steps/s = 1.63, tokens/s = 43560 (43560 target) ; Learning rate = 0.000379 ; Loss = 1.464525\n",
      "2024-12-01 07:39:45.232000: I runner.py:310] Step = 54600 ; steps/s = 1.65, tokens/s = 43064 (43064 target) ; Learning rate = 0.000378 ; Loss = 1.462351\n",
      "2024-12-01 07:40:46.415000: I runner.py:310] Step = 54700 ; steps/s = 1.63, tokens/s = 43581 (43581 target) ; Learning rate = 0.000378 ; Loss = 1.461205\n",
      "2024-12-01 07:41:47.193000: I runner.py:310] Step = 54800 ; steps/s = 1.65, tokens/s = 43068 (43068 target) ; Learning rate = 0.000378 ; Loss = 1.460263\n",
      "2024-12-01 07:42:48.340000: I runner.py:310] Step = 54900 ; steps/s = 1.64, tokens/s = 43620 (43620 target) ; Learning rate = 0.000377 ; Loss = 1.463151\n",
      "2024-12-01 07:43:49.464000: I runner.py:310] Step = 55000 ; steps/s = 1.64, tokens/s = 43616 (43616 target) ; Learning rate = 0.000377 ; Loss = 1.463524\n",
      "2024-12-01 07:43:49.465000: I training.py:192] Running evaluation for step 55000\n",
      "2024-12-01 07:44:37.294000: I training.py:192] Evaluation result for step 55000: loss = 0.770411 ; perplexity = 2.160653\n",
      "2024-12-01 07:45:37.828000: I runner.py:310] Step = 55100 ; steps/s = 1.65, tokens/s = 43256 (43256 target) ; Learning rate = 0.000377 ; Loss = 1.465021\n",
      "2024-12-01 07:46:39.067000: I runner.py:310] Step = 55200 ; steps/s = 1.63, tokens/s = 43531 (43531 target) ; Learning rate = 0.000376 ; Loss = 1.463344\n",
      "2024-12-01 07:47:40.213000: I runner.py:310] Step = 55300 ; steps/s = 1.64, tokens/s = 43612 (43612 target) ; Learning rate = 0.000376 ; Loss = 1.467152\n",
      "2024-12-01 07:48:40.966000: I runner.py:310] Step = 55400 ; steps/s = 1.65, tokens/s = 43077 (43077 target) ; Learning rate = 0.000376 ; Loss = 1.462924\n",
      "2024-12-01 07:49:42.131000: I runner.py:310] Step = 55500 ; steps/s = 1.64, tokens/s = 43594 (43594 target) ; Learning rate = 0.000375 ; Loss = 1.468353\n",
      "2024-12-01 07:50:42.936000: I runner.py:310] Step = 55600 ; steps/s = 1.64, tokens/s = 43039 (43039 target) ; Learning rate = 0.000375 ; Loss = 1.457193\n",
      "2024-12-01 07:51:44.048000: I runner.py:310] Step = 55700 ; steps/s = 1.64, tokens/s = 43635 (43635 target) ; Learning rate = 0.000375 ; Loss = 1.456208\n",
      "2024-12-01 07:52:45.255000: I runner.py:310] Step = 55800 ; steps/s = 1.63, tokens/s = 43571 (43571 target) ; Learning rate = 0.000374 ; Loss = 1.464081\n",
      "2024-12-01 07:53:45.903000: I runner.py:310] Step = 55900 ; steps/s = 1.65, tokens/s = 43159 (43159 target) ; Learning rate = 0.000374 ; Loss = 1.462357\n",
      "2024-12-01 07:54:47.110000: I runner.py:310] Step = 56000 ; steps/s = 1.63, tokens/s = 43568 (43568 target) ; Learning rate = 0.000374 ; Loss = 1.466664\n",
      "2024-12-01 07:55:47.848000: I runner.py:310] Step = 56100 ; steps/s = 1.65, tokens/s = 43074 (43074 target) ; Learning rate = 0.000373 ; Loss = 1.461810\n",
      "2024-12-01 07:56:49.005000: I runner.py:310] Step = 56200 ; steps/s = 1.64, tokens/s = 43612 (43612 target) ; Learning rate = 0.000373 ; Loss = 1.459262\n",
      "2024-12-01 07:57:50.141000: I runner.py:310] Step = 56300 ; steps/s = 1.64, tokens/s = 43603 (43603 target) ; Learning rate = 0.000373 ; Loss = 1.462013\n",
      "2024-12-01 07:58:50.774000: I runner.py:310] Step = 56400 ; steps/s = 1.65, tokens/s = 43179 (43179 target) ; Learning rate = 0.000372 ; Loss = 1.459771\n",
      "2024-12-01 07:59:51.957000: I runner.py:310] Step = 56500 ; steps/s = 1.63, tokens/s = 43592 (43592 target) ; Learning rate = 0.000372 ; Loss = 1.462030\n",
      "2024-12-01 08:00:53.183000: I runner.py:310] Step = 56600 ; steps/s = 1.63, tokens/s = 43545 (43545 target) ; Learning rate = 0.000372 ; Loss = 1.462008\n",
      "2024-12-01 08:01:53.900000: I runner.py:310] Step = 56700 ; steps/s = 1.65, tokens/s = 43103 (43103 target) ; Learning rate = 0.000371 ; Loss = 1.460225\n",
      "2024-12-01 08:02:55.056000: I runner.py:310] Step = 56800 ; steps/s = 1.64, tokens/s = 43599 (43599 target) ; Learning rate = 0.000371 ; Loss = 1.464725\n",
      "2024-12-01 08:03:55.843000: I runner.py:310] Step = 56900 ; steps/s = 1.65, tokens/s = 43048 (43048 target) ; Learning rate = 0.000371 ; Loss = 1.457274\n",
      "2024-12-01 08:04:56.943000: I runner.py:310] Step = 57000 ; steps/s = 1.64, tokens/s = 43643 (43643 target) ; Learning rate = 0.000370 ; Loss = 1.460366\n",
      "2024-12-01 08:05:58.169000: I runner.py:310] Step = 57100 ; steps/s = 1.63, tokens/s = 43554 (43554 target) ; Learning rate = 0.000370 ; Loss = 1.464997\n",
      "2024-12-01 08:06:58.858000: I runner.py:310] Step = 57200 ; steps/s = 1.65, tokens/s = 43126 (43126 target) ; Learning rate = 0.000370 ; Loss = 1.456035\n",
      "2024-12-01 08:08:00.065000: I runner.py:310] Step = 57300 ; steps/s = 1.63, tokens/s = 43568 (43568 target) ; Learning rate = 0.000369 ; Loss = 1.462537\n",
      "2024-12-01 08:09:00.785000: I runner.py:310] Step = 57400 ; steps/s = 1.65, tokens/s = 43096 (43096 target) ; Learning rate = 0.000369 ; Loss = 1.462749\n",
      "2024-12-01 08:10:01.950000: I runner.py:310] Step = 57500 ; steps/s = 1.64, tokens/s = 43604 (43604 target) ; Learning rate = 0.000369 ; Loss = 1.457137\n",
      "2024-12-01 08:11:03.131000: I runner.py:310] Step = 57600 ; steps/s = 1.63, tokens/s = 43583 (43583 target) ; Learning rate = 0.000368 ; Loss = 1.455960\n",
      "2024-12-01 08:12:03.851000: I runner.py:310] Step = 57700 ; steps/s = 1.65, tokens/s = 43100 (43100 target) ; Learning rate = 0.000368 ; Loss = 1.456533\n",
      "2024-12-01 08:13:05.079000: I runner.py:310] Step = 57800 ; steps/s = 1.63, tokens/s = 43555 (43555 target) ; Learning rate = 0.000368 ; Loss = 1.460225\n",
      "2024-12-01 08:14:06.282000: I runner.py:310] Step = 57900 ; steps/s = 1.63, tokens/s = 43572 (43572 target) ; Learning rate = 0.000367 ; Loss = 1.462553\n",
      "2024-12-01 08:15:07.016000: I runner.py:310] Step = 58000 ; steps/s = 1.65, tokens/s = 43093 (43093 target) ; Learning rate = 0.000367 ; Loss = 1.455965\n",
      "2024-12-01 08:16:08.135000: I runner.py:310] Step = 58100 ; steps/s = 1.64, tokens/s = 43629 (43629 target) ; Learning rate = 0.000367 ; Loss = 1.459725\n",
      "2024-12-01 08:17:08.811000: I runner.py:310] Step = 58200 ; steps/s = 1.65, tokens/s = 43130 (43130 target) ; Learning rate = 0.000366 ; Loss = 1.456602\n",
      "2024-12-01 08:18:09.973000: I runner.py:310] Step = 58300 ; steps/s = 1.64, tokens/s = 43600 (43600 target) ; Learning rate = 0.000366 ; Loss = 1.457929\n",
      "2024-12-01 08:19:11.183000: I runner.py:310] Step = 58400 ; steps/s = 1.63, tokens/s = 43570 (43570 target) ; Learning rate = 0.000366 ; Loss = 1.461640\n",
      "2024-12-01 08:20:11.930000: I runner.py:310] Step = 58500 ; steps/s = 1.65, tokens/s = 43075 (43075 target) ; Learning rate = 0.000365 ; Loss = 1.460050\n",
      "2024-12-01 08:21:13.117000: I runner.py:310] Step = 58600 ; steps/s = 1.63, tokens/s = 43590 (43590 target) ; Learning rate = 0.000365 ; Loss = 1.457228\n",
      "2024-12-01 08:22:13.768000: I runner.py:310] Step = 58700 ; steps/s = 1.65, tokens/s = 43145 (43145 target) ; Learning rate = 0.000365 ; Loss = 1.455866\n",
      "2024-12-01 08:23:14.897000: I runner.py:310] Step = 58800 ; steps/s = 1.64, tokens/s = 43628 (43628 target) ; Learning rate = 0.000365 ; Loss = 1.456601\n",
      "2024-12-01 08:24:16.060000: I runner.py:310] Step = 58900 ; steps/s = 1.64, tokens/s = 43600 (43600 target) ; Learning rate = 0.000364 ; Loss = 1.459736\n",
      "2024-12-01 08:25:16.724000: I runner.py:310] Step = 59000 ; steps/s = 1.65, tokens/s = 43128 (43128 target) ; Learning rate = 0.000364 ; Loss = 1.460367\n",
      "2024-12-01 08:26:17.903000: I runner.py:310] Step = 59100 ; steps/s = 1.63, tokens/s = 43584 (43584 target) ; Learning rate = 0.000364 ; Loss = 1.455646\n",
      "2024-12-01 08:27:19.073000: I runner.py:310] Step = 59200 ; steps/s = 1.64, tokens/s = 43601 (43601 target) ; Learning rate = 0.000363 ; Loss = 1.462989\n",
      "2024-12-01 08:28:19.737000: I runner.py:310] Step = 59300 ; steps/s = 1.65, tokens/s = 43143 (43143 target) ; Learning rate = 0.000363 ; Loss = 1.461067\n",
      "2024-12-01 08:29:20.888000: I runner.py:310] Step = 59400 ; steps/s = 1.64, tokens/s = 43604 (43604 target) ; Learning rate = 0.000363 ; Loss = 1.464283\n",
      "2024-12-01 08:30:21.633000: I runner.py:310] Step = 59500 ; steps/s = 1.65, tokens/s = 43085 (43085 target) ; Learning rate = 0.000362 ; Loss = 1.451215\n",
      "2024-12-01 08:31:22.749000: I runner.py:310] Step = 59600 ; steps/s = 1.64, tokens/s = 43637 (43637 target) ; Learning rate = 0.000362 ; Loss = 1.458399\n",
      "2024-12-01 08:32:23.958000: I runner.py:310] Step = 59700 ; steps/s = 1.63, tokens/s = 43564 (43564 target) ; Learning rate = 0.000362 ; Loss = 1.454320\n",
      "2024-12-01 08:33:24.701000: I runner.py:310] Step = 59800 ; steps/s = 1.65, tokens/s = 43077 (43077 target) ; Learning rate = 0.000361 ; Loss = 1.460519\n",
      "2024-12-01 08:34:25.894000: I runner.py:310] Step = 59900 ; steps/s = 1.63, tokens/s = 43582 (43582 target) ; Learning rate = 0.000361 ; Loss = 1.457885\n",
      "2024-12-01 08:35:26.573000: I runner.py:310] Step = 60000 ; steps/s = 1.65, tokens/s = 43124 (43124 target) ; Learning rate = 0.000361 ; Loss = 1.459092\n",
      "2024-12-01 08:35:28.726000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-60000\n",
      "2024-12-01 08:35:28.726000: I training.py:192] Running evaluation for step 60000\n",
      "2024-12-01 08:36:15.281000: I training.py:192] Evaluation result for step 60000: loss = 0.777426 ; perplexity = 2.175865\n",
      "2024-12-01 08:37:16.315000: I runner.py:310] Step = 60100 ; steps/s = 1.64, tokens/s = 43713 (43713 target) ; Learning rate = 0.000361 ; Loss = 1.456881\n",
      "2024-12-01 08:38:17.530000: I runner.py:310] Step = 60200 ; steps/s = 1.63, tokens/s = 43549 (43549 target) ; Learning rate = 0.000360 ; Loss = 1.460636\n",
      "2024-12-01 08:39:18.248000: I runner.py:310] Step = 60300 ; steps/s = 1.65, tokens/s = 43114 (43114 target) ; Learning rate = 0.000360 ; Loss = 1.460299\n",
      "2024-12-01 08:40:19.434000: I runner.py:310] Step = 60400 ; steps/s = 1.63, tokens/s = 43578 (43578 target) ; Learning rate = 0.000360 ; Loss = 1.458200\n",
      "2024-12-01 08:41:20.610000: I runner.py:310] Step = 60500 ; steps/s = 1.63, tokens/s = 43580 (43580 target) ; Learning rate = 0.000359 ; Loss = 1.458677\n",
      "2024-12-01 08:42:21.374000: I runner.py:310] Step = 60600 ; steps/s = 1.65, tokens/s = 43072 (43072 target) ; Learning rate = 0.000359 ; Loss = 1.458605\n",
      "2024-12-01 08:43:22.583000: I runner.py:310] Step = 60700 ; steps/s = 1.63, tokens/s = 43568 (43568 target) ; Learning rate = 0.000359 ; Loss = 1.453764\n",
      "2024-12-01 08:44:23.335000: I runner.py:310] Step = 60800 ; steps/s = 1.65, tokens/s = 43078 (43078 target) ; Learning rate = 0.000358 ; Loss = 1.459218\n",
      "2024-12-01 08:45:24.520000: I runner.py:310] Step = 60900 ; steps/s = 1.63, tokens/s = 43586 (43586 target) ; Learning rate = 0.000358 ; Loss = 1.461267\n",
      "2024-12-01 08:46:25.701000: I runner.py:310] Step = 61000 ; steps/s = 1.63, tokens/s = 43582 (43582 target) ; Learning rate = 0.000358 ; Loss = 1.455057\n",
      "2024-12-01 08:47:26.475000: I runner.py:310] Step = 61100 ; steps/s = 1.65, tokens/s = 43057 (43057 target) ; Learning rate = 0.000358 ; Loss = 1.453283\n",
      "2024-12-01 08:48:27.669000: I runner.py:310] Step = 61200 ; steps/s = 1.63, tokens/s = 43569 (43569 target) ; Learning rate = 0.000357 ; Loss = 1.461836\n",
      "2024-12-01 08:49:28.400000: I runner.py:310] Step = 61300 ; steps/s = 1.65, tokens/s = 43106 (43106 target) ; Learning rate = 0.000357 ; Loss = 1.457515\n",
      "2024-12-01 08:50:29.522000: I runner.py:310] Step = 61400 ; steps/s = 1.64, tokens/s = 43630 (43630 target) ; Learning rate = 0.000357 ; Loss = 1.452917\n",
      "2024-12-01 08:51:30.637000: I runner.py:310] Step = 61500 ; steps/s = 1.64, tokens/s = 43637 (43637 target) ; Learning rate = 0.000356 ; Loss = 1.459631\n",
      "2024-12-01 08:52:31.426000: I runner.py:310] Step = 61600 ; steps/s = 1.65, tokens/s = 43046 (43046 target) ; Learning rate = 0.000356 ; Loss = 1.458000\n",
      "2024-12-01 08:53:32.578000: I runner.py:310] Step = 61700 ; steps/s = 1.64, tokens/s = 43600 (43600 target) ; Learning rate = 0.000356 ; Loss = 1.462384\n",
      "2024-12-01 08:54:33.695000: I runner.py:310] Step = 61800 ; steps/s = 1.64, tokens/s = 43636 (43636 target) ; Learning rate = 0.000356 ; Loss = 1.465256\n",
      "2024-12-01 08:55:34.396000: I runner.py:310] Step = 61900 ; steps/s = 1.65, tokens/s = 43119 (43119 target) ; Learning rate = 0.000355 ; Loss = 1.456267\n",
      "2024-12-01 08:56:35.661000: I runner.py:310] Step = 62000 ; steps/s = 1.63, tokens/s = 43525 (43525 target) ; Learning rate = 0.000355 ; Loss = 1.458958\n",
      "2024-12-01 08:57:36.385000: I runner.py:310] Step = 62100 ; steps/s = 1.65, tokens/s = 43102 (43102 target) ; Learning rate = 0.000355 ; Loss = 1.452016\n",
      "2024-12-01 08:58:37.579000: I runner.py:310] Step = 62200 ; steps/s = 1.63, tokens/s = 43573 (43573 target) ; Learning rate = 0.000354 ; Loss = 1.455221\n",
      "2024-12-01 08:59:38.787000: I runner.py:310] Step = 62300 ; steps/s = 1.63, tokens/s = 43566 (43566 target) ; Learning rate = 0.000354 ; Loss = 1.456376\n",
      "2024-12-01 09:00:39.517000: I runner.py:310] Step = 62400 ; steps/s = 1.65, tokens/s = 43103 (43103 target) ; Learning rate = 0.000354 ; Loss = 1.456262\n",
      "2024-12-01 09:01:40.634000: I runner.py:310] Step = 62500 ; steps/s = 1.64, tokens/s = 43620 (43620 target) ; Learning rate = 0.000354 ; Loss = 1.461458\n",
      "2024-12-01 09:02:41.377000: I runner.py:310] Step = 62600 ; steps/s = 1.65, tokens/s = 43092 (43092 target) ; Learning rate = 0.000353 ; Loss = 1.454597\n",
      "2024-12-01 09:03:42.554000: I runner.py:310] Step = 62700 ; steps/s = 1.63, tokens/s = 43602 (43602 target) ; Learning rate = 0.000353 ; Loss = 1.455014\n",
      "2024-12-01 09:04:43.760000: I runner.py:310] Step = 62800 ; steps/s = 1.63, tokens/s = 43554 (43554 target) ; Learning rate = 0.000353 ; Loss = 1.459198\n",
      "2024-12-01 09:05:44.433000: I runner.py:310] Step = 62900 ; steps/s = 1.65, tokens/s = 43141 (43141 target) ; Learning rate = 0.000352 ; Loss = 1.455404\n",
      "2024-12-01 09:06:45.642000: I runner.py:310] Step = 63000 ; steps/s = 1.63, tokens/s = 43567 (43567 target) ; Learning rate = 0.000352 ; Loss = 1.457802\n",
      "2024-12-01 09:07:46.824000: I runner.py:310] Step = 63100 ; steps/s = 1.63, tokens/s = 43583 (43583 target) ; Learning rate = 0.000352 ; Loss = 1.458501\n",
      "2024-12-01 09:08:47.572000: I runner.py:310] Step = 63200 ; steps/s = 1.65, tokens/s = 43077 (43077 target) ; Learning rate = 0.000352 ; Loss = 1.453673\n",
      "2024-12-01 09:09:48.723000: I runner.py:310] Step = 63300 ; steps/s = 1.64, tokens/s = 43619 (43619 target) ; Learning rate = 0.000351 ; Loss = 1.456520\n",
      "2024-12-01 09:10:49.482000: I runner.py:310] Step = 63400 ; steps/s = 1.65, tokens/s = 43066 (43066 target) ; Learning rate = 0.000351 ; Loss = 1.453626\n",
      "2024-12-01 09:11:50.694000: I runner.py:310] Step = 63500 ; steps/s = 1.63, tokens/s = 43556 (43556 target) ; Learning rate = 0.000351 ; Loss = 1.458471\n",
      "2024-12-01 09:12:51.895000: I runner.py:310] Step = 63600 ; steps/s = 1.63, tokens/s = 43581 (43581 target) ; Learning rate = 0.000350 ; Loss = 1.455231\n",
      "2024-12-01 09:13:52.560000: I runner.py:310] Step = 63700 ; steps/s = 1.65, tokens/s = 43136 (43136 target) ; Learning rate = 0.000350 ; Loss = 1.456939\n",
      "2024-12-01 09:14:53.710000: I runner.py:310] Step = 63800 ; steps/s = 1.64, tokens/s = 43599 (43599 target) ; Learning rate = 0.000350 ; Loss = 1.454893\n",
      "2024-12-01 09:15:54.438000: I runner.py:310] Step = 63900 ; steps/s = 1.65, tokens/s = 43107 (43107 target) ; Learning rate = 0.000350 ; Loss = 1.445026\n",
      "2024-12-01 09:16:55.628000: I runner.py:310] Step = 64000 ; steps/s = 1.63, tokens/s = 43581 (43581 target) ; Learning rate = 0.000349 ; Loss = 1.460720\n",
      "2024-12-01 09:17:56.786000: I runner.py:310] Step = 64100 ; steps/s = 1.64, tokens/s = 43607 (43607 target) ; Learning rate = 0.000349 ; Loss = 1.460691\n",
      "2024-12-01 09:18:57.555000: I runner.py:310] Step = 64200 ; steps/s = 1.65, tokens/s = 43063 (43063 target) ; Learning rate = 0.000349 ; Loss = 1.457085\n",
      "2024-12-01 09:19:58.728000: I runner.py:310] Step = 64300 ; steps/s = 1.63, tokens/s = 43596 (43596 target) ; Learning rate = 0.000349 ; Loss = 1.458114\n",
      "2024-12-01 09:20:59.967000: I runner.py:310] Step = 64400 ; steps/s = 1.63, tokens/s = 43537 (43537 target) ; Learning rate = 0.000348 ; Loss = 1.460380\n",
      "2024-12-01 09:22:00.694000: I runner.py:310] Step = 64500 ; steps/s = 1.65, tokens/s = 43102 (43102 target) ; Learning rate = 0.000348 ; Loss = 1.453437\n",
      "2024-12-01 09:23:01.851000: I runner.py:310] Step = 64600 ; steps/s = 1.64, tokens/s = 43601 (43601 target) ; Learning rate = 0.000348 ; Loss = 1.451615\n",
      "2024-12-01 09:24:02.530000: I runner.py:310] Step = 64700 ; steps/s = 1.65, tokens/s = 43122 (43122 target) ; Learning rate = 0.000347 ; Loss = 1.451183\n",
      "2024-12-01 09:25:03.701000: I runner.py:310] Step = 64800 ; steps/s = 1.63, tokens/s = 43599 (43599 target) ; Learning rate = 0.000347 ; Loss = 1.461233\n",
      "2024-12-01 09:26:04.880000: I runner.py:310] Step = 64900 ; steps/s = 1.63, tokens/s = 43580 (43580 target) ; Learning rate = 0.000347 ; Loss = 1.454126\n",
      "2024-12-01 09:27:05.617000: I runner.py:310] Step = 65000 ; steps/s = 1.65, tokens/s = 43088 (43088 target) ; Learning rate = 0.000347 ; Loss = 1.459968\n",
      "2024-12-01 09:27:05.620000: I training.py:192] Running evaluation for step 65000\n",
      "2024-12-01 09:27:52.230000: I training.py:192] Evaluation result for step 65000: loss = 0.787142 ; perplexity = 2.197108\n",
      "2024-12-01 09:28:53.345000: I runner.py:310] Step = 65100 ; steps/s = 1.64, tokens/s = 43644 (43644 target) ; Learning rate = 0.000346 ; Loss = 1.459224\n",
      "2024-12-01 09:29:54.014000: I runner.py:310] Step = 65200 ; steps/s = 1.65, tokens/s = 43136 (43136 target) ; Learning rate = 0.000346 ; Loss = 1.453124\n",
      "2024-12-01 09:30:55.237000: I runner.py:310] Step = 65300 ; steps/s = 1.63, tokens/s = 43553 (43553 target) ; Learning rate = 0.000346 ; Loss = 1.446749\n",
      "2024-12-01 09:31:56.422000: I runner.py:310] Step = 65400 ; steps/s = 1.63, tokens/s = 43587 (43587 target) ; Learning rate = 0.000346 ; Loss = 1.460411\n",
      "2024-12-01 09:32:57.095000: I runner.py:310] Step = 65500 ; steps/s = 1.65, tokens/s = 43129 (43129 target) ; Learning rate = 0.000345 ; Loss = 1.455176\n",
      "2024-12-01 09:33:58.259000: I runner.py:310] Step = 65600 ; steps/s = 1.64, tokens/s = 43608 (43608 target) ; Learning rate = 0.000345 ; Loss = 1.455512\n",
      "2024-12-01 09:34:59.462000: I runner.py:310] Step = 65700 ; steps/s = 1.63, tokens/s = 43561 (43561 target) ; Learning rate = 0.000345 ; Loss = 1.454183\n",
      "2024-12-01 09:36:00.214000: I runner.py:310] Step = 65800 ; steps/s = 1.65, tokens/s = 43089 (43089 target) ; Learning rate = 0.000345 ; Loss = 1.454848\n",
      "2024-12-01 09:37:01.421000: I runner.py:310] Step = 65900 ; steps/s = 1.63, tokens/s = 43571 (43571 target) ; Learning rate = 0.000344 ; Loss = 1.453186\n",
      "2024-12-01 09:38:02.154000: I runner.py:310] Step = 66000 ; steps/s = 1.65, tokens/s = 43089 (43089 target) ; Learning rate = 0.000344 ; Loss = 1.458791\n",
      "2024-12-01 09:39:03.392000: I runner.py:310] Step = 66100 ; steps/s = 1.63, tokens/s = 43544 (43544 target) ; Learning rate = 0.000344 ; Loss = 1.455438\n",
      "2024-12-01 09:40:04.585000: I runner.py:310] Step = 66200 ; steps/s = 1.63, tokens/s = 43574 (43574 target) ; Learning rate = 0.000344 ; Loss = 1.452438\n",
      "2024-12-01 09:41:05.329000: I runner.py:310] Step = 66300 ; steps/s = 1.65, tokens/s = 43083 (43083 target) ; Learning rate = 0.000343 ; Loss = 1.456908\n",
      "2024-12-01 09:42:06.533000: I runner.py:310] Step = 66400 ; steps/s = 1.63, tokens/s = 43557 (43557 target) ; Learning rate = 0.000343 ; Loss = 1.461391\n",
      "2024-12-01 09:43:07.236000: I runner.py:310] Step = 66500 ; steps/s = 1.65, tokens/s = 43122 (43122 target) ; Learning rate = 0.000343 ; Loss = 1.445056\n",
      "2024-12-01 09:44:08.448000: I runner.py:310] Step = 66600 ; steps/s = 1.63, tokens/s = 43568 (43568 target) ; Learning rate = 0.000342 ; Loss = 1.452873\n",
      "2024-12-01 09:45:09.732000: I runner.py:310] Step = 66700 ; steps/s = 1.63, tokens/s = 43522 (43522 target) ; Learning rate = 0.000342 ; Loss = 1.458130\n",
      "2024-12-01 09:46:10.515000: I runner.py:310] Step = 66800 ; steps/s = 1.65, tokens/s = 43037 (43037 target) ; Learning rate = 0.000342 ; Loss = 1.461997\n",
      "2024-12-01 09:47:11.713000: I runner.py:310] Step = 66900 ; steps/s = 1.63, tokens/s = 43587 (43587 target) ; Learning rate = 0.000342 ; Loss = 1.457745\n",
      "2024-12-01 09:48:12.842000: I runner.py:310] Step = 67000 ; steps/s = 1.64, tokens/s = 43626 (43626 target) ; Learning rate = 0.000341 ; Loss = 1.453882\n",
      "2024-12-01 09:49:13.548000: I runner.py:310] Step = 67100 ; steps/s = 1.65, tokens/s = 43105 (43105 target) ; Learning rate = 0.000341 ; Loss = 1.455286\n",
      "2024-12-01 09:50:14.759000: I runner.py:310] Step = 67200 ; steps/s = 1.63, tokens/s = 43559 (43559 target) ; Learning rate = 0.000341 ; Loss = 1.459628\n",
      "2024-12-01 09:51:15.495000: I runner.py:310] Step = 67300 ; steps/s = 1.65, tokens/s = 43097 (43097 target) ; Learning rate = 0.000341 ; Loss = 1.455340\n",
      "2024-12-01 09:52:16.665000: I runner.py:310] Step = 67400 ; steps/s = 1.63, tokens/s = 43594 (43594 target) ; Learning rate = 0.000340 ; Loss = 1.454915\n",
      "2024-12-01 09:53:17.894000: I runner.py:310] Step = 67500 ; steps/s = 1.63, tokens/s = 43554 (43554 target) ; Learning rate = 0.000340 ; Loss = 1.454904\n",
      "2024-12-01 09:54:18.615000: I runner.py:310] Step = 67600 ; steps/s = 1.65, tokens/s = 43101 (43101 target) ; Learning rate = 0.000340 ; Loss = 1.453861\n",
      "2024-12-01 09:55:19.803000: I runner.py:310] Step = 67700 ; steps/s = 1.63, tokens/s = 43585 (43585 target) ; Learning rate = 0.000340 ; Loss = 1.455018\n",
      "2024-12-01 09:56:20.508000: I runner.py:310] Step = 67800 ; steps/s = 1.65, tokens/s = 43100 (43100 target) ; Learning rate = 0.000339 ; Loss = 1.447638\n",
      "2024-12-01 09:57:21.700000: I runner.py:310] Step = 67900 ; steps/s = 1.63, tokens/s = 43582 (43582 target) ; Learning rate = 0.000339 ; Loss = 1.453564\n",
      "2024-12-01 09:58:22.932000: I runner.py:310] Step = 68000 ; steps/s = 1.63, tokens/s = 43547 (43547 target) ; Learning rate = 0.000339 ; Loss = 1.461414\n",
      "2024-12-01 09:59:23.677000: I runner.py:310] Step = 68100 ; steps/s = 1.65, tokens/s = 43088 (43088 target) ; Learning rate = 0.000339 ; Loss = 1.452689\n",
      "2024-12-01 10:00:24.827000: I runner.py:310] Step = 68200 ; steps/s = 1.64, tokens/s = 43608 (43608 target) ; Learning rate = 0.000338 ; Loss = 1.454250\n",
      "2024-12-01 10:01:25.981000: I runner.py:310] Step = 68300 ; steps/s = 1.64, tokens/s = 43599 (43599 target) ; Learning rate = 0.000338 ; Loss = 1.449402\n",
      "2024-12-01 10:02:26.696000: I runner.py:310] Step = 68400 ; steps/s = 1.65, tokens/s = 43107 (43107 target) ; Learning rate = 0.000338 ; Loss = 1.450376\n",
      "2024-12-01 10:03:27.921000: I runner.py:310] Step = 68500 ; steps/s = 1.63, tokens/s = 43557 (43557 target) ; Learning rate = 0.000338 ; Loss = 1.451740\n",
      "2024-12-01 10:04:28.637000: I runner.py:310] Step = 68600 ; steps/s = 1.65, tokens/s = 43105 (43105 target) ; Learning rate = 0.000337 ; Loss = 1.446753\n",
      "2024-12-01 10:05:29.851000: I runner.py:310] Step = 68700 ; steps/s = 1.63, tokens/s = 43565 (43565 target) ; Learning rate = 0.000337 ; Loss = 1.453756\n",
      "2024-12-01 10:06:31.034000: I runner.py:310] Step = 68800 ; steps/s = 1.63, tokens/s = 43582 (43582 target) ; Learning rate = 0.000337 ; Loss = 1.452839\n",
      "2024-12-01 10:07:31.703000: I runner.py:310] Step = 68900 ; steps/s = 1.65, tokens/s = 43135 (43135 target) ; Learning rate = 0.000337 ; Loss = 1.455724\n",
      "2024-12-01 10:08:32.908000: I runner.py:310] Step = 69000 ; steps/s = 1.63, tokens/s = 43572 (43572 target) ; Learning rate = 0.000336 ; Loss = 1.451055\n",
      "2024-12-01 10:09:33.616000: I runner.py:310] Step = 69100 ; steps/s = 1.65, tokens/s = 43105 (43105 target) ; Learning rate = 0.000336 ; Loss = 1.451556\n",
      "2024-12-01 10:10:34.767000: I runner.py:310] Step = 69200 ; steps/s = 1.64, tokens/s = 43609 (43609 target) ; Learning rate = 0.000336 ; Loss = 1.454701\n",
      "2024-12-01 10:11:35.985000: I runner.py:310] Step = 69300 ; steps/s = 1.63, tokens/s = 43561 (43561 target) ; Learning rate = 0.000336 ; Loss = 1.456509\n",
      "2024-12-01 10:12:36.757000: I runner.py:310] Step = 69400 ; steps/s = 1.65, tokens/s = 43059 (43059 target) ; Learning rate = 0.000336 ; Loss = 1.454188\n",
      "2024-12-01 10:13:37.906000: I runner.py:310] Step = 69500 ; steps/s = 1.64, tokens/s = 43612 (43612 target) ; Learning rate = 0.000335 ; Loss = 1.453313\n",
      "2024-12-01 10:14:39.091000: I runner.py:310] Step = 69600 ; steps/s = 1.63, tokens/s = 43581 (43581 target) ; Learning rate = 0.000335 ; Loss = 1.451680\n",
      "2024-12-01 10:15:39.809000: I runner.py:310] Step = 69700 ; steps/s = 1.65, tokens/s = 43093 (43093 target) ; Learning rate = 0.000335 ; Loss = 1.452592\n",
      "2024-12-01 10:16:40.981000: I runner.py:310] Step = 69800 ; steps/s = 1.63, tokens/s = 43594 (43594 target) ; Learning rate = 0.000335 ; Loss = 1.450516\n",
      "2024-12-01 10:17:41.761000: I runner.py:310] Step = 69900 ; steps/s = 1.65, tokens/s = 43064 (43064 target) ; Learning rate = 0.000334 ; Loss = 1.456131\n",
      "2024-12-01 10:18:42.911000: I runner.py:310] Step = 70000 ; steps/s = 1.64, tokens/s = 43613 (43613 target) ; Learning rate = 0.000334 ; Loss = 1.450974\n",
      "2024-12-01 10:18:45.532000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-70000\n",
      "2024-12-01 10:18:45.532000: I training.py:192] Running evaluation for step 70000\n",
      "2024-12-01 10:19:32.466000: I training.py:192] Evaluation result for step 70000: loss = 0.791906 ; perplexity = 2.207601\n",
      "2024-12-01 10:20:33.548000: I runner.py:310] Step = 70100 ; steps/s = 1.64, tokens/s = 43661 (43661 target) ; Learning rate = 0.000334 ; Loss = 1.453185\n",
      "2024-12-01 10:21:34.255000: I runner.py:310] Step = 70200 ; steps/s = 1.65, tokens/s = 43122 (43122 target) ; Learning rate = 0.000334 ; Loss = 1.450268\n",
      "2024-12-01 10:22:35.401000: I runner.py:310] Step = 70300 ; steps/s = 1.64, tokens/s = 43611 (43611 target) ; Learning rate = 0.000333 ; Loss = 1.451982\n",
      "2024-12-01 10:23:36.214000: I runner.py:310] Step = 70400 ; steps/s = 1.64, tokens/s = 43027 (43027 target) ; Learning rate = 0.000333 ; Loss = 1.446056\n",
      "2024-12-01 10:24:37.429000: I runner.py:310] Step = 70500 ; steps/s = 1.63, tokens/s = 43568 (43568 target) ; Learning rate = 0.000333 ; Loss = 1.454986\n",
      "2024-12-01 10:25:38.647000: I runner.py:310] Step = 70600 ; steps/s = 1.63, tokens/s = 43552 (43552 target) ; Learning rate = 0.000333 ; Loss = 1.456648\n",
      "2024-12-01 10:26:39.354000: I runner.py:310] Step = 70700 ; steps/s = 1.65, tokens/s = 43102 (43102 target) ; Learning rate = 0.000332 ; Loss = 1.450989\n",
      "2024-12-01 10:27:40.585000: I runner.py:310] Step = 70800 ; steps/s = 1.63, tokens/s = 43558 (43558 target) ; Learning rate = 0.000332 ; Loss = 1.451801\n",
      "2024-12-01 10:28:41.703000: I runner.py:310] Step = 70900 ; steps/s = 1.64, tokens/s = 43417 (43417 target) ; Learning rate = 0.000332 ; Loss = 1.464506\n",
      "2024-12-01 10:29:42.466000: I runner.py:310] Step = 71000 ; steps/s = 1.65, tokens/s = 43287 (43287 target) ; Learning rate = 0.000332 ; Loss = 1.448494\n",
      "2024-12-01 10:30:43.662000: I runner.py:310] Step = 71100 ; steps/s = 1.63, tokens/s = 43570 (43570 target) ; Learning rate = 0.000331 ; Loss = 1.455757\n",
      "2024-12-01 10:31:44.374000: I runner.py:310] Step = 71200 ; steps/s = 1.65, tokens/s = 43107 (43107 target) ; Learning rate = 0.000331 ; Loss = 1.449202\n",
      "2024-12-01 10:32:45.528000: I runner.py:310] Step = 71300 ; steps/s = 1.64, tokens/s = 43603 (43603 target) ; Learning rate = 0.000331 ; Loss = 1.448451\n",
      "2024-12-01 10:33:46.752000: I runner.py:310] Step = 71400 ; steps/s = 1.63, tokens/s = 43562 (43562 target) ; Learning rate = 0.000331 ; Loss = 1.457387\n",
      "2024-12-01 10:34:47.486000: I runner.py:310] Step = 71500 ; steps/s = 1.65, tokens/s = 43100 (43100 target) ; Learning rate = 0.000331 ; Loss = 1.451279\n",
      "2024-12-01 10:35:48.687000: I runner.py:310] Step = 71600 ; steps/s = 1.63, tokens/s = 43567 (43567 target) ; Learning rate = 0.000330 ; Loss = 1.454449\n",
      "2024-12-01 10:36:49.422000: I runner.py:310] Step = 71700 ; steps/s = 1.65, tokens/s = 43089 (43089 target) ; Learning rate = 0.000330 ; Loss = 1.449470\n",
      "2024-12-01 10:37:50.589000: I runner.py:310] Step = 71800 ; steps/s = 1.64, tokens/s = 43597 (43597 target) ; Learning rate = 0.000330 ; Loss = 1.452950\n",
      "2024-12-01 10:38:51.750000: I runner.py:310] Step = 71900 ; steps/s = 1.64, tokens/s = 43595 (43595 target) ; Learning rate = 0.000330 ; Loss = 1.450140\n",
      "2024-12-01 10:39:52.502000: I runner.py:310] Step = 72000 ; steps/s = 1.65, tokens/s = 43081 (43081 target) ; Learning rate = 0.000329 ; Loss = 1.451372\n",
      "2024-12-01 10:40:53.674000: I runner.py:310] Step = 72100 ; steps/s = 1.63, tokens/s = 43597 (43597 target) ; Learning rate = 0.000329 ; Loss = 1.453998\n",
      "2024-12-01 10:41:54.534000: I runner.py:310] Step = 72200 ; steps/s = 1.64, tokens/s = 43227 (43227 target) ; Learning rate = 0.000329 ; Loss = 1.457173\n",
      "2024-12-01 10:42:55.612000: I runner.py:310] Step = 72300 ; steps/s = 1.64, tokens/s = 43429 (43429 target) ; Learning rate = 0.000329 ; Loss = 1.452187\n",
      "2024-12-01 10:43:56.781000: I runner.py:310] Step = 72400 ; steps/s = 1.63, tokens/s = 43593 (43593 target) ; Learning rate = 0.000328 ; Loss = 1.453522\n",
      "2024-12-01 10:44:57.517000: I runner.py:310] Step = 72500 ; steps/s = 1.65, tokens/s = 43095 (43095 target) ; Learning rate = 0.000328 ; Loss = 1.449255\n",
      "2024-12-01 10:45:58.688000: I runner.py:310] Step = 72600 ; steps/s = 1.63, tokens/s = 43595 (43595 target) ; Learning rate = 0.000328 ; Loss = 1.452257\n",
      "2024-12-01 10:46:59.807000: I runner.py:310] Step = 72700 ; steps/s = 1.64, tokens/s = 43629 (43629 target) ; Learning rate = 0.000328 ; Loss = 1.452255\n",
      "2024-12-01 10:48:00.518000: I runner.py:310] Step = 72800 ; steps/s = 1.65, tokens/s = 43088 (43088 target) ; Learning rate = 0.000328 ; Loss = 1.454639\n",
      "2024-12-01 10:49:01.693000: I runner.py:310] Step = 72900 ; steps/s = 1.63, tokens/s = 43594 (43594 target) ; Learning rate = 0.000327 ; Loss = 1.457145\n",
      "2024-12-01 10:50:02.489000: I runner.py:310] Step = 73000 ; steps/s = 1.65, tokens/s = 43051 (43051 target) ; Learning rate = 0.000327 ; Loss = 1.448586\n",
      "2024-12-01 10:51:03.648000: I runner.py:310] Step = 73100 ; steps/s = 1.64, tokens/s = 43604 (43604 target) ; Learning rate = 0.000327 ; Loss = 1.454656\n",
      "2024-12-01 10:52:04.856000: I runner.py:310] Step = 73200 ; steps/s = 1.63, tokens/s = 43560 (43560 target) ; Learning rate = 0.000327 ; Loss = 1.456244\n",
      "2024-12-01 10:53:05.625000: I runner.py:310] Step = 73300 ; steps/s = 1.65, tokens/s = 43077 (43077 target) ; Learning rate = 0.000326 ; Loss = 1.450712\n",
      "2024-12-01 10:54:06.802000: I runner.py:310] Step = 73400 ; steps/s = 1.63, tokens/s = 43598 (43598 target) ; Learning rate = 0.000326 ; Loss = 1.451017\n",
      "2024-12-01 10:55:07.555000: I runner.py:310] Step = 73500 ; steps/s = 1.65, tokens/s = 43059 (43059 target) ; Learning rate = 0.000326 ; Loss = 1.447978\n",
      "2024-12-01 10:56:08.726000: I runner.py:310] Step = 73600 ; steps/s = 1.63, tokens/s = 43595 (43595 target) ; Learning rate = 0.000326 ; Loss = 1.449775\n",
      "2024-12-01 10:57:09.906000: I runner.py:310] Step = 73700 ; steps/s = 1.63, tokens/s = 43591 (43591 target) ; Learning rate = 0.000326 ; Loss = 1.451764\n",
      "2024-12-01 10:58:10.679000: I runner.py:310] Step = 73800 ; steps/s = 1.65, tokens/s = 43055 (43055 target) ; Learning rate = 0.000325 ; Loss = 1.450715\n",
      "2024-12-01 10:59:11.862000: I runner.py:310] Step = 73900 ; steps/s = 1.63, tokens/s = 43588 (43588 target) ; Learning rate = 0.000325 ; Loss = 1.452790\n",
      "2024-12-01 11:00:13.084000: I runner.py:310] Step = 74000 ; steps/s = 1.63, tokens/s = 43553 (43553 target) ; Learning rate = 0.000325 ; Loss = 1.455206\n",
      "2024-12-01 11:01:13.778000: I runner.py:310] Step = 74100 ; steps/s = 1.65, tokens/s = 43127 (43127 target) ; Learning rate = 0.000325 ; Loss = 1.453654\n",
      "2024-12-01 11:02:14.976000: I runner.py:310] Step = 74200 ; steps/s = 1.63, tokens/s = 43559 (43559 target) ; Learning rate = 0.000324 ; Loss = 1.451546\n",
      "2024-12-01 11:03:15.732000: I runner.py:310] Step = 74300 ; steps/s = 1.65, tokens/s = 43094 (43094 target) ; Learning rate = 0.000324 ; Loss = 1.444053\n",
      "2024-12-01 11:04:16.929000: I runner.py:310] Step = 74400 ; steps/s = 1.63, tokens/s = 43580 (43580 target) ; Learning rate = 0.000324 ; Loss = 1.453195\n",
      "2024-12-01 11:05:18.149000: I runner.py:310] Step = 74500 ; steps/s = 1.63, tokens/s = 43556 (43556 target) ; Learning rate = 0.000324 ; Loss = 1.448636\n",
      "2024-12-01 11:06:18.884000: I runner.py:310] Step = 74600 ; steps/s = 1.65, tokens/s = 43077 (43077 target) ; Learning rate = 0.000324 ; Loss = 1.450802\n",
      "2024-12-01 11:07:20.043000: I runner.py:310] Step = 74700 ; steps/s = 1.64, tokens/s = 43610 (43610 target) ; Learning rate = 0.000323 ; Loss = 1.452603\n",
      "2024-12-01 11:08:20.844000: I runner.py:310] Step = 74800 ; steps/s = 1.64, tokens/s = 43036 (43036 target) ; Learning rate = 0.000323 ; Loss = 1.448224\n",
      "2024-12-01 11:09:22.021000: I runner.py:310] Step = 74900 ; steps/s = 1.63, tokens/s = 43586 (43586 target) ; Learning rate = 0.000323 ; Loss = 1.444584\n",
      "2024-12-01 11:10:23.267000: I runner.py:310] Step = 75000 ; steps/s = 1.63, tokens/s = 43536 (43536 target) ; Learning rate = 0.000323 ; Loss = 1.450768\n",
      "2024-12-01 11:10:23.268000: I training.py:192] Running evaluation for step 75000\n",
      "2024-12-01 11:11:10.864000: I training.py:192] Evaluation result for step 75000: loss = 0.794241 ; perplexity = 2.212762\n",
      "2024-12-01 11:12:11.479000: I runner.py:310] Step = 75100 ; steps/s = 1.65, tokens/s = 43198 (43198 target) ; Learning rate = 0.000323 ; Loss = 1.447603\n",
      "2024-12-01 11:13:12.715000: I runner.py:310] Step = 75200 ; steps/s = 1.63, tokens/s = 43541 (43541 target) ; Learning rate = 0.000322 ; Loss = 1.453015\n",
      "2024-12-01 11:14:13.950000: I runner.py:310] Step = 75300 ; steps/s = 1.63, tokens/s = 43549 (43549 target) ; Learning rate = 0.000322 ; Loss = 1.455568\n",
      "2024-12-01 11:15:14.698000: I runner.py:310] Step = 75400 ; steps/s = 1.65, tokens/s = 43083 (43083 target) ; Learning rate = 0.000322 ; Loss = 1.448036\n",
      "2024-12-01 11:16:15.913000: I runner.py:310] Step = 75500 ; steps/s = 1.63, tokens/s = 43560 (43560 target) ; Learning rate = 0.000322 ; Loss = 1.450937\n",
      "2024-12-01 11:17:16.650000: I runner.py:310] Step = 75600 ; steps/s = 1.65, tokens/s = 43082 (43082 target) ; Learning rate = 0.000321 ; Loss = 1.443755\n",
      "2024-12-01 11:18:17.866000: I runner.py:310] Step = 75700 ; steps/s = 1.63, tokens/s = 43578 (43578 target) ; Learning rate = 0.000321 ; Loss = 1.452504\n",
      "2024-12-01 11:19:19.085000: I runner.py:310] Step = 75800 ; steps/s = 1.63, tokens/s = 43553 (43553 target) ; Learning rate = 0.000321 ; Loss = 1.455229\n",
      "2024-12-01 11:20:19.853000: I runner.py:310] Step = 75900 ; steps/s = 1.65, tokens/s = 43058 (43058 target) ; Learning rate = 0.000321 ; Loss = 1.448457\n",
      "2024-12-01 11:21:21.029000: I runner.py:310] Step = 76000 ; steps/s = 1.63, tokens/s = 43603 (43603 target) ; Learning rate = 0.000321 ; Loss = 1.452937\n",
      "2024-12-01 11:22:21.923000: I runner.py:310] Step = 76100 ; steps/s = 1.64, tokens/s = 42969 (42969 target) ; Learning rate = 0.000320 ; Loss = 1.443983\n",
      "2024-12-01 11:23:23.169000: I runner.py:310] Step = 76200 ; steps/s = 1.63, tokens/s = 43547 (43547 target) ; Learning rate = 0.000320 ; Loss = 1.452442\n",
      "2024-12-01 11:24:24.458000: I runner.py:310] Step = 76300 ; steps/s = 1.63, tokens/s = 43501 (43501 target) ; Learning rate = 0.000320 ; Loss = 1.451869\n",
      "2024-12-01 11:25:25.242000: I runner.py:310] Step = 76400 ; steps/s = 1.65, tokens/s = 43057 (43057 target) ; Learning rate = 0.000320 ; Loss = 1.451045\n",
      "2024-12-01 11:26:26.475000: I runner.py:310] Step = 76500 ; steps/s = 1.63, tokens/s = 43549 (43549 target) ; Learning rate = 0.000320 ; Loss = 1.451409\n",
      "2024-12-01 11:27:27.758000: I runner.py:310] Step = 76600 ; steps/s = 1.63, tokens/s = 43508 (43508 target) ; Learning rate = 0.000319 ; Loss = 1.449568\n",
      "2024-12-01 11:28:28.512000: I runner.py:310] Step = 76700 ; steps/s = 1.65, tokens/s = 43082 (43082 target) ; Learning rate = 0.000319 ; Loss = 1.446546\n",
      "2024-12-01 11:29:29.696000: I runner.py:310] Step = 76800 ; steps/s = 1.63, tokens/s = 43590 (43590 target) ; Learning rate = 0.000319 ; Loss = 1.453687\n",
      "2024-12-01 11:30:30.428000: I runner.py:310] Step = 76900 ; steps/s = 1.65, tokens/s = 43090 (43090 target) ; Learning rate = 0.000319 ; Loss = 1.445010\n",
      "2024-12-01 11:31:31.666000: I runner.py:310] Step = 77000 ; steps/s = 1.63, tokens/s = 43543 (43543 target) ; Learning rate = 0.000319 ; Loss = 1.448705\n",
      "2024-12-01 11:32:32.944000: I runner.py:310] Step = 77100 ; steps/s = 1.63, tokens/s = 43517 (43517 target) ; Learning rate = 0.000318 ; Loss = 1.445588\n",
      "2024-12-01 11:33:33.628000: I runner.py:310] Step = 77200 ; steps/s = 1.65, tokens/s = 43131 (43131 target) ; Learning rate = 0.000318 ; Loss = 1.452988\n",
      "2024-12-01 11:34:34.843000: I runner.py:310] Step = 77300 ; steps/s = 1.63, tokens/s = 43575 (43575 target) ; Learning rate = 0.000318 ; Loss = 1.451798\n",
      "2024-12-01 11:35:35.564000: I runner.py:310] Step = 77400 ; steps/s = 1.65, tokens/s = 43081 (43081 target) ; Learning rate = 0.000318 ; Loss = 1.445687\n",
      "2024-12-01 11:36:36.823000: I runner.py:310] Step = 77500 ; steps/s = 1.63, tokens/s = 43535 (43535 target) ; Learning rate = 0.000317 ; Loss = 1.446423\n",
      "2024-12-01 11:37:38.030000: I runner.py:310] Step = 77600 ; steps/s = 1.63, tokens/s = 43560 (43560 target) ; Learning rate = 0.000317 ; Loss = 1.454881\n",
      "2024-12-01 11:38:38.839000: I runner.py:310] Step = 77700 ; steps/s = 1.64, tokens/s = 43048 (43048 target) ; Learning rate = 0.000317 ; Loss = 1.450860\n",
      "2024-12-01 11:39:40.074000: I runner.py:310] Step = 77800 ; steps/s = 1.63, tokens/s = 43552 (43552 target) ; Learning rate = 0.000317 ; Loss = 1.456099\n",
      "2024-12-01 11:40:41.267000: I runner.py:310] Step = 77900 ; steps/s = 1.63, tokens/s = 43573 (43573 target) ; Learning rate = 0.000317 ; Loss = 1.452083\n",
      "2024-12-01 11:41:42.055000: I runner.py:310] Step = 78000 ; steps/s = 1.65, tokens/s = 43056 (43056 target) ; Learning rate = 0.000316 ; Loss = 1.447198\n",
      "2024-12-01 11:42:43.341000: I runner.py:310] Step = 78100 ; steps/s = 1.63, tokens/s = 43515 (43515 target) ; Learning rate = 0.000316 ; Loss = 1.453539\n",
      "2024-12-01 11:43:43.998000: I runner.py:310] Step = 78200 ; steps/s = 1.65, tokens/s = 43135 (43135 target) ; Learning rate = 0.000316 ; Loss = 1.453477\n",
      "2024-12-01 11:44:45.182000: I runner.py:310] Step = 78300 ; steps/s = 1.63, tokens/s = 43588 (43588 target) ; Learning rate = 0.000316 ; Loss = 1.446782\n",
      "2024-12-01 11:45:46.367000: I runner.py:310] Step = 78400 ; steps/s = 1.63, tokens/s = 43576 (43576 target) ; Learning rate = 0.000316 ; Loss = 1.450974\n",
      "2024-12-01 11:46:47.124000: I runner.py:310] Step = 78500 ; steps/s = 1.65, tokens/s = 43073 (43073 target) ; Learning rate = 0.000315 ; Loss = 1.450809\n",
      "2024-12-01 11:47:48.268000: I runner.py:310] Step = 78600 ; steps/s = 1.64, tokens/s = 43613 (43613 target) ; Learning rate = 0.000315 ; Loss = 1.449351\n",
      "2024-12-01 11:48:49.032000: I runner.py:310] Step = 78700 ; steps/s = 1.65, tokens/s = 43070 (43070 target) ; Learning rate = 0.000315 ; Loss = 1.447140\n",
      "2024-12-01 11:49:50.215000: I runner.py:310] Step = 78800 ; steps/s = 1.63, tokens/s = 43581 (43581 target) ; Learning rate = 0.000315 ; Loss = 1.439637\n",
      "2024-12-01 11:50:51.480000: I runner.py:310] Step = 78900 ; steps/s = 1.63, tokens/s = 43522 (43522 target) ; Learning rate = 0.000315 ; Loss = 1.451362\n",
      "2024-12-01 11:51:52.240000: I runner.py:310] Step = 79000 ; steps/s = 1.65, tokens/s = 43088 (43088 target) ; Learning rate = 0.000314 ; Loss = 1.448657\n",
      "2024-12-01 11:52:53.468000: I runner.py:310] Step = 79100 ; steps/s = 1.63, tokens/s = 43554 (43554 target) ; Learning rate = 0.000314 ; Loss = 1.450765\n",
      "2024-12-01 11:53:54.729000: I runner.py:310] Step = 79200 ; steps/s = 1.63, tokens/s = 43523 (43523 target) ; Learning rate = 0.000314 ; Loss = 1.447514\n",
      "2024-12-01 11:54:55.524000: I runner.py:310] Step = 79300 ; steps/s = 1.65, tokens/s = 43048 (43048 target) ; Learning rate = 0.000314 ; Loss = 1.452090\n",
      "2024-12-01 11:55:56.719000: I runner.py:310] Step = 79400 ; steps/s = 1.63, tokens/s = 43575 (43575 target) ; Learning rate = 0.000314 ; Loss = 1.451391\n",
      "2024-12-01 11:56:57.510000: I runner.py:310] Step = 79500 ; steps/s = 1.65, tokens/s = 43054 (43054 target) ; Learning rate = 0.000313 ; Loss = 1.445329\n",
      "2024-12-01 11:57:58.752000: I runner.py:310] Step = 79600 ; steps/s = 1.63, tokens/s = 43549 (43549 target) ; Learning rate = 0.000313 ; Loss = 1.445796\n",
      "2024-12-01 11:58:59.988000: I runner.py:310] Step = 79700 ; steps/s = 1.63, tokens/s = 43552 (43552 target) ; Learning rate = 0.000313 ; Loss = 1.451861\n",
      "2024-12-01 12:00:00.737000: I runner.py:310] Step = 79800 ; steps/s = 1.65, tokens/s = 43076 (43076 target) ; Learning rate = 0.000313 ; Loss = 1.450594\n",
      "2024-12-01 12:01:02.039000: I runner.py:310] Step = 79900 ; steps/s = 1.63, tokens/s = 43504 (43504 target) ; Learning rate = 0.000313 ; Loss = 1.449233\n",
      "2024-12-01 12:02:02.870000: I runner.py:310] Step = 80000 ; steps/s = 1.64, tokens/s = 43011 (43011 target) ; Learning rate = 0.000312 ; Loss = 1.444139\n",
      "2024-12-01 12:02:05.115000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-80000\n",
      "2024-12-01 12:02:05.115000: I training.py:192] Running evaluation for step 80000\n",
      "2024-12-01 12:02:52.418000: I training.py:192] Evaluation result for step 80000: loss = 0.799030 ; perplexity = 2.223384\n",
      "2024-12-01 12:03:53.524000: I runner.py:310] Step = 80100 ; steps/s = 1.64, tokens/s = 43656 (43656 target) ; Learning rate = 0.000312 ; Loss = 1.442863\n",
      "2024-12-01 12:04:54.717000: I runner.py:310] Step = 80200 ; steps/s = 1.63, tokens/s = 43571 (43571 target) ; Learning rate = 0.000312 ; Loss = 1.447149\n",
      "2024-12-01 12:05:55.437000: I runner.py:310] Step = 80300 ; steps/s = 1.65, tokens/s = 43105 (43105 target) ; Learning rate = 0.000312 ; Loss = 1.446879\n",
      "2024-12-01 12:06:56.673000: I runner.py:310] Step = 80400 ; steps/s = 1.63, tokens/s = 43551 (43551 target) ; Learning rate = 0.000312 ; Loss = 1.452720\n",
      "2024-12-01 12:07:57.811000: I runner.py:310] Step = 80500 ; steps/s = 1.64, tokens/s = 43606 (43606 target) ; Learning rate = 0.000312 ; Loss = 1.448630\n",
      "2024-12-01 12:08:58.557000: I runner.py:310] Step = 80600 ; steps/s = 1.65, tokens/s = 43075 (43075 target) ; Learning rate = 0.000311 ; Loss = 1.443287\n",
      "2024-12-01 12:09:59.767000: I runner.py:310] Step = 80700 ; steps/s = 1.63, tokens/s = 43564 (43564 target) ; Learning rate = 0.000311 ; Loss = 1.453548\n",
      "2024-12-01 12:11:00.523000: I runner.py:310] Step = 80800 ; steps/s = 1.65, tokens/s = 43070 (43070 target) ; Learning rate = 0.000311 ; Loss = 1.442420\n",
      "2024-12-01 12:12:01.736000: I runner.py:310] Step = 80900 ; steps/s = 1.63, tokens/s = 43574 (43574 target) ; Learning rate = 0.000311 ; Loss = 1.453419\n",
      "2024-12-01 12:13:02.957000: I runner.py:310] Step = 81000 ; steps/s = 1.63, tokens/s = 43554 (43554 target) ; Learning rate = 0.000311 ; Loss = 1.448355\n",
      "2024-12-01 12:14:03.636000: I runner.py:310] Step = 81100 ; steps/s = 1.65, tokens/s = 43136 (43136 target) ; Learning rate = 0.000310 ; Loss = 1.450408\n",
      "2024-12-01 12:15:04.849000: I runner.py:310] Step = 81200 ; steps/s = 1.63, tokens/s = 43564 (43564 target) ; Learning rate = 0.000310 ; Loss = 1.445553\n",
      "2024-12-01 12:16:05.590000: I runner.py:310] Step = 81300 ; steps/s = 1.65, tokens/s = 43080 (43080 target) ; Learning rate = 0.000310 ; Loss = 1.450141\n",
      "2024-12-01 12:17:06.874000: I runner.py:310] Step = 81400 ; steps/s = 1.63, tokens/s = 43526 (43526 target) ; Learning rate = 0.000310 ; Loss = 1.446578\n",
      "2024-12-01 12:18:08.022000: I runner.py:310] Step = 81500 ; steps/s = 1.64, tokens/s = 43609 (43609 target) ; Learning rate = 0.000310 ; Loss = 1.442779\n",
      "2024-12-01 12:19:08.746000: I runner.py:310] Step = 81600 ; steps/s = 1.65, tokens/s = 43091 (43091 target) ; Learning rate = 0.000309 ; Loss = 1.447881\n",
      "2024-12-01 12:20:10.000000: I runner.py:310] Step = 81700 ; steps/s = 1.63, tokens/s = 43534 (43534 target) ; Learning rate = 0.000309 ; Loss = 1.452238\n",
      "2024-12-01 12:21:11.243000: I runner.py:310] Step = 81800 ; steps/s = 1.63, tokens/s = 43538 (43538 target) ; Learning rate = 0.000309 ; Loss = 1.447753\n",
      "2024-12-01 12:22:12.009000: I runner.py:310] Step = 81900 ; steps/s = 1.65, tokens/s = 43069 (43069 target) ; Learning rate = 0.000309 ; Loss = 1.444666\n",
      "2024-12-01 12:23:13.211000: I runner.py:310] Step = 82000 ; steps/s = 1.63, tokens/s = 43574 (43574 target) ; Learning rate = 0.000309 ; Loss = 1.446896\n",
      "2024-12-01 12:24:13.992000: I runner.py:310] Step = 82100 ; steps/s = 1.65, tokens/s = 43053 (43053 target) ; Learning rate = 0.000308 ; Loss = 1.441778\n",
      "2024-12-01 12:25:15.170000: I runner.py:310] Step = 82200 ; steps/s = 1.63, tokens/s = 43593 (43593 target) ; Learning rate = 0.000308 ; Loss = 1.450957\n",
      "2024-12-01 12:26:16.345000: I runner.py:310] Step = 82300 ; steps/s = 1.63, tokens/s = 43595 (43595 target) ; Learning rate = 0.000308 ; Loss = 1.444649\n",
      "2024-12-01 12:27:17.120000: I runner.py:310] Step = 82400 ; steps/s = 1.65, tokens/s = 43054 (43054 target) ; Learning rate = 0.000308 ; Loss = 1.445562\n",
      "2024-12-01 12:28:18.294000: I runner.py:310] Step = 82500 ; steps/s = 1.63, tokens/s = 43591 (43591 target) ; Learning rate = 0.000308 ; Loss = 1.449994\n",
      "2024-12-01 12:29:18.983000: I runner.py:310] Step = 82600 ; steps/s = 1.65, tokens/s = 43124 (43124 target) ; Learning rate = 0.000308 ; Loss = 1.444876\n",
      "2024-12-01 12:30:20.135000: I runner.py:310] Step = 82700 ; steps/s = 1.64, tokens/s = 43611 (43611 target) ; Learning rate = 0.000307 ; Loss = 1.445699\n",
      "2024-12-01 12:31:21.308000: I runner.py:310] Step = 82800 ; steps/s = 1.63, tokens/s = 43584 (43584 target) ; Learning rate = 0.000307 ; Loss = 1.446115\n",
      "2024-12-01 12:32:22.049000: I runner.py:310] Step = 82900 ; steps/s = 1.65, tokens/s = 43099 (43099 target) ; Learning rate = 0.000307 ; Loss = 1.444447\n",
      "2024-12-01 12:33:23.226000: I runner.py:310] Step = 83000 ; steps/s = 1.63, tokens/s = 43595 (43595 target) ; Learning rate = 0.000307 ; Loss = 1.452027\n",
      "2024-12-01 12:34:24.429000: I runner.py:310] Step = 83100 ; steps/s = 1.63, tokens/s = 43557 (43557 target) ; Learning rate = 0.000307 ; Loss = 1.447286\n",
      "2024-12-01 12:35:25.173000: I runner.py:310] Step = 83200 ; steps/s = 1.65, tokens/s = 43084 (43084 target) ; Learning rate = 0.000306 ; Loss = 1.445847\n",
      "2024-12-01 12:36:26.343000: I runner.py:310] Step = 83300 ; steps/s = 1.63, tokens/s = 43581 (43581 target) ; Learning rate = 0.000306 ; Loss = 1.448998\n",
      "2024-12-01 12:37:27.153000: I runner.py:310] Step = 83400 ; steps/s = 1.64, tokens/s = 43059 (43059 target) ; Learning rate = 0.000306 ; Loss = 1.444511\n",
      "2024-12-01 12:38:28.319000: I runner.py:310] Step = 83500 ; steps/s = 1.64, tokens/s = 43590 (43590 target) ; Learning rate = 0.000306 ; Loss = 1.443963\n",
      "2024-12-01 12:39:29.519000: I runner.py:310] Step = 83600 ; steps/s = 1.63, tokens/s = 43569 (43569 target) ; Learning rate = 0.000306 ; Loss = 1.449617\n",
      "2024-12-01 12:40:30.120000: I runner.py:310] Step = 83700 ; steps/s = 1.65, tokens/s = 43175 (43175 target) ; Learning rate = 0.000306 ; Loss = 1.445948\n",
      "2024-12-01 12:41:31.307000: I runner.py:310] Step = 83800 ; steps/s = 1.63, tokens/s = 43589 (43589 target) ; Learning rate = 0.000305 ; Loss = 1.447623\n",
      "2024-12-01 12:42:32.063000: I runner.py:310] Step = 83900 ; steps/s = 1.65, tokens/s = 43078 (43078 target) ; Learning rate = 0.000305 ; Loss = 1.445284\n",
      "2024-12-01 12:43:33.272000: I runner.py:310] Step = 84000 ; steps/s = 1.63, tokens/s = 43563 (43563 target) ; Learning rate = 0.000305 ; Loss = 1.446054\n",
      "2024-12-01 12:44:34.446000: I runner.py:310] Step = 84100 ; steps/s = 1.63, tokens/s = 43589 (43589 target) ; Learning rate = 0.000305 ; Loss = 1.452284\n",
      "2024-12-01 12:45:35.268000: I runner.py:310] Step = 84200 ; steps/s = 1.64, tokens/s = 43026 (43026 target) ; Learning rate = 0.000305 ; Loss = 1.445709\n",
      "2024-12-01 12:46:36.493000: I runner.py:310] Step = 84300 ; steps/s = 1.63, tokens/s = 43553 (43553 target) ; Learning rate = 0.000304 ; Loss = 1.449848\n",
      "2024-12-01 12:47:37.732000: I runner.py:310] Step = 84400 ; steps/s = 1.63, tokens/s = 43554 (43554 target) ; Learning rate = 0.000304 ; Loss = 1.449025\n",
      "2024-12-01 12:48:38.478000: I runner.py:310] Step = 84500 ; steps/s = 1.65, tokens/s = 43086 (43086 target) ; Learning rate = 0.000304 ; Loss = 1.447094\n",
      "2024-12-01 12:49:39.611000: I runner.py:310] Step = 84600 ; steps/s = 1.64, tokens/s = 43613 (43613 target) ; Learning rate = 0.000304 ; Loss = 1.450224\n",
      "2024-12-01 12:50:40.387000: I runner.py:310] Step = 84700 ; steps/s = 1.65, tokens/s = 43064 (43064 target) ; Learning rate = 0.000304 ; Loss = 1.445360\n",
      "2024-12-01 12:51:41.604000: I runner.py:310] Step = 84800 ; steps/s = 1.63, tokens/s = 43569 (43569 target) ; Learning rate = 0.000304 ; Loss = 1.447786\n",
      "2024-12-01 12:52:42.797000: I runner.py:310] Step = 84900 ; steps/s = 1.63, tokens/s = 43567 (43567 target) ; Learning rate = 0.000303 ; Loss = 1.445826\n",
      "2024-12-01 12:53:43.541000: I runner.py:310] Step = 85000 ; steps/s = 1.65, tokens/s = 43087 (43087 target) ; Learning rate = 0.000303 ; Loss = 1.448313\n",
      "2024-12-01 12:53:43.543000: I training.py:192] Running evaluation for step 85000\n",
      "2024-12-01 12:54:30.509000: I training.py:192] Evaluation result for step 85000: loss = 0.804983 ; perplexity = 2.236657\n",
      "2024-12-01 12:55:31.622000: I runner.py:310] Step = 85100 ; steps/s = 1.64, tokens/s = 43643 (43643 target) ; Learning rate = 0.000303 ; Loss = 1.450144\n",
      "2024-12-01 12:56:32.375000: I runner.py:310] Step = 85200 ; steps/s = 1.65, tokens/s = 43079 (43079 target) ; Learning rate = 0.000303 ; Loss = 1.441218\n",
      "2024-12-01 12:57:33.587000: I runner.py:310] Step = 85300 ; steps/s = 1.63, tokens/s = 43570 (43570 target) ; Learning rate = 0.000303 ; Loss = 1.446555\n",
      "2024-12-01 12:58:34.743000: I runner.py:310] Step = 85400 ; steps/s = 1.64, tokens/s = 43600 (43600 target) ; Learning rate = 0.000302 ; Loss = 1.448209\n",
      "2024-12-01 12:59:35.463000: I runner.py:310] Step = 85500 ; steps/s = 1.65, tokens/s = 43094 (43094 target) ; Learning rate = 0.000302 ; Loss = 1.446990\n",
      "2024-12-01 13:00:36.632000: I runner.py:310] Step = 85600 ; steps/s = 1.64, tokens/s = 43592 (43592 target) ; Learning rate = 0.000302 ; Loss = 1.447289\n",
      "2024-12-01 13:01:37.807000: I runner.py:310] Step = 85700 ; steps/s = 1.63, tokens/s = 43596 (43596 target) ; Learning rate = 0.000302 ; Loss = 1.447448\n",
      "2024-12-01 13:02:38.521000: I runner.py:310] Step = 85800 ; steps/s = 1.65, tokens/s = 43105 (43105 target) ; Learning rate = 0.000302 ; Loss = 1.440576\n",
      "2024-12-01 13:03:39.729000: I runner.py:310] Step = 85900 ; steps/s = 1.63, tokens/s = 43561 (43561 target) ; Learning rate = 0.000302 ; Loss = 1.446749\n",
      "2024-12-01 13:04:40.477000: I runner.py:310] Step = 86000 ; steps/s = 1.65, tokens/s = 43092 (43092 target) ; Learning rate = 0.000301 ; Loss = 1.446881\n",
      "2024-12-01 13:05:41.645000: I runner.py:310] Step = 86100 ; steps/s = 1.64, tokens/s = 43593 (43593 target) ; Learning rate = 0.000301 ; Loss = 1.445625\n",
      "2024-12-01 13:06:42.900000: I runner.py:310] Step = 86200 ; steps/s = 1.63, tokens/s = 43529 (43529 target) ; Learning rate = 0.000301 ; Loss = 1.447362\n",
      "2024-12-01 13:07:43.666000: I runner.py:310] Step = 86300 ; steps/s = 1.65, tokens/s = 43063 (43063 target) ; Learning rate = 0.000301 ; Loss = 1.443552\n",
      "2024-12-01 13:08:44.858000: I runner.py:310] Step = 86400 ; steps/s = 1.63, tokens/s = 43573 (43573 target) ; Learning rate = 0.000301 ; Loss = 1.446952\n",
      "2024-12-01 13:09:45.544000: I runner.py:310] Step = 86500 ; steps/s = 1.65, tokens/s = 43132 (43132 target) ; Learning rate = 0.000301 ; Loss = 1.446871\n",
      "2024-12-01 13:10:46.731000: I runner.py:310] Step = 86600 ; steps/s = 1.63, tokens/s = 43584 (43584 target) ; Learning rate = 0.000300 ; Loss = 1.449008\n",
      "2024-12-01 13:11:47.905000: I runner.py:310] Step = 86700 ; steps/s = 1.63, tokens/s = 43595 (43595 target) ; Learning rate = 0.000300 ; Loss = 1.446527\n",
      "2024-12-01 13:12:48.651000: I runner.py:310] Step = 86800 ; steps/s = 1.65, tokens/s = 43080 (43080 target) ; Learning rate = 0.000300 ; Loss = 1.447379\n",
      "2024-12-01 13:13:49.808000: I runner.py:310] Step = 86900 ; steps/s = 1.64, tokens/s = 43595 (43595 target) ; Learning rate = 0.000300 ; Loss = 1.448314\n",
      "2024-12-01 13:14:51.013000: I runner.py:310] Step = 87000 ; steps/s = 1.63, tokens/s = 43580 (43580 target) ; Learning rate = 0.000300 ; Loss = 1.451773\n",
      "2024-12-01 13:15:51.741000: I runner.py:310] Step = 87100 ; steps/s = 1.65, tokens/s = 43104 (43104 target) ; Learning rate = 0.000299 ; Loss = 1.441790\n",
      "2024-12-01 13:16:52.904000: I runner.py:310] Step = 87200 ; steps/s = 1.64, tokens/s = 43589 (43589 target) ; Learning rate = 0.000299 ; Loss = 1.448884\n",
      "2024-12-01 13:17:53.585000: I runner.py:310] Step = 87300 ; steps/s = 1.65, tokens/s = 43124 (43124 target) ; Learning rate = 0.000299 ; Loss = 1.444827\n",
      "2024-12-01 13:18:54.743000: I runner.py:310] Step = 87400 ; steps/s = 1.64, tokens/s = 43602 (43602 target) ; Learning rate = 0.000299 ; Loss = 1.445403\n",
      "2024-12-01 13:19:55.929000: I runner.py:310] Step = 87500 ; steps/s = 1.63, tokens/s = 43588 (43588 target) ; Learning rate = 0.000299 ; Loss = 1.449738\n",
      "2024-12-01 13:20:56.707000: I runner.py:310] Step = 87600 ; steps/s = 1.65, tokens/s = 43064 (43064 target) ; Learning rate = 0.000299 ; Loss = 1.441778\n",
      "2024-12-01 13:21:57.928000: I runner.py:310] Step = 87700 ; steps/s = 1.63, tokens/s = 43554 (43554 target) ; Learning rate = 0.000298 ; Loss = 1.449046\n",
      "2024-12-01 13:22:58.652000: I runner.py:310] Step = 87800 ; steps/s = 1.65, tokens/s = 43100 (43100 target) ; Learning rate = 0.000298 ; Loss = 1.441963\n",
      "2024-12-01 13:23:59.807000: I runner.py:310] Step = 87900 ; steps/s = 1.64, tokens/s = 43606 (43606 target) ; Learning rate = 0.000298 ; Loss = 1.448457\n",
      "2024-12-01 13:25:00.988000: I runner.py:310] Step = 88000 ; steps/s = 1.63, tokens/s = 43585 (43585 target) ; Learning rate = 0.000298 ; Loss = 1.444209\n",
      "2024-12-01 13:26:01.686000: I runner.py:310] Step = 88100 ; steps/s = 1.65, tokens/s = 43112 (43112 target) ; Learning rate = 0.000298 ; Loss = 1.442084\n",
      "2024-12-01 13:27:02.847000: I runner.py:310] Step = 88200 ; steps/s = 1.64, tokens/s = 43599 (43599 target) ; Learning rate = 0.000298 ; Loss = 1.450524\n",
      "2024-12-01 13:28:03.994000: I runner.py:310] Step = 88300 ; steps/s = 1.64, tokens/s = 43547 (43547 target) ; Learning rate = 0.000297 ; Loss = 1.452471\n",
      "2024-12-01 13:29:04.770000: I runner.py:310] Step = 88400 ; steps/s = 1.65, tokens/s = 43132 (43132 target) ; Learning rate = 0.000297 ; Loss = 1.443199\n",
      "2024-12-01 13:30:05.984000: I runner.py:310] Step = 88500 ; steps/s = 1.63, tokens/s = 43562 (43562 target) ; Learning rate = 0.000297 ; Loss = 1.442005\n",
      "2024-12-01 13:31:06.708000: I runner.py:310] Step = 88600 ; steps/s = 1.65, tokens/s = 43090 (43090 target) ; Learning rate = 0.000297 ; Loss = 1.446361\n",
      "2024-12-01 13:32:07.873000: I runner.py:310] Step = 88700 ; steps/s = 1.64, tokens/s = 43608 (43608 target) ; Learning rate = 0.000297 ; Loss = 1.449526\n",
      "2024-12-01 13:33:09.058000: I runner.py:310] Step = 88800 ; steps/s = 1.63, tokens/s = 43581 (43581 target) ; Learning rate = 0.000297 ; Loss = 1.447186\n",
      "2024-12-01 13:34:09.812000: I runner.py:310] Step = 88900 ; steps/s = 1.65, tokens/s = 43077 (43077 target) ; Learning rate = 0.000296 ; Loss = 1.447656\n",
      "2024-12-01 13:35:11.057000: I runner.py:310] Step = 89000 ; steps/s = 1.63, tokens/s = 43533 (43533 target) ; Learning rate = 0.000296 ; Loss = 1.449344\n",
      "2024-12-01 13:36:11.735000: I runner.py:310] Step = 89100 ; steps/s = 1.65, tokens/s = 43132 (43132 target) ; Learning rate = 0.000296 ; Loss = 1.445413\n",
      "2024-12-01 13:37:12.915000: I runner.py:310] Step = 89200 ; steps/s = 1.63, tokens/s = 43598 (43598 target) ; Learning rate = 0.000296 ; Loss = 1.441678\n",
      "2024-12-01 13:38:14.014000: I runner.py:310] Step = 89300 ; steps/s = 1.64, tokens/s = 43634 (43634 target) ; Learning rate = 0.000296 ; Loss = 1.443531\n",
      "2024-12-01 13:39:14.761000: I runner.py:310] Step = 89400 ; steps/s = 1.65, tokens/s = 43088 (43088 target) ; Learning rate = 0.000296 ; Loss = 1.447832\n",
      "2024-12-01 13:40:15.976000: I runner.py:310] Step = 89500 ; steps/s = 1.63, tokens/s = 43552 (43552 target) ; Learning rate = 0.000295 ; Loss = 1.447748\n",
      "2024-12-01 13:41:17.063000: I runner.py:310] Step = 89600 ; steps/s = 1.64, tokens/s = 43361 (43361 target) ; Learning rate = 0.000295 ; Loss = 1.470928\n",
      "2024-12-01 13:42:17.952000: I runner.py:310] Step = 89700 ; steps/s = 1.64, tokens/s = 43276 (43276 target) ; Learning rate = 0.000295 ; Loss = 1.444160\n",
      "2024-12-01 13:43:19.110000: I runner.py:310] Step = 89800 ; steps/s = 1.64, tokens/s = 43601 (43601 target) ; Learning rate = 0.000295 ; Loss = 1.450288\n",
      "2024-12-01 13:44:19.834000: I runner.py:310] Step = 89900 ; steps/s = 1.65, tokens/s = 43101 (43101 target) ; Learning rate = 0.000295 ; Loss = 1.442495\n",
      "2024-12-01 13:45:21.060000: I runner.py:310] Step = 90000 ; steps/s = 1.63, tokens/s = 43561 (43561 target) ; Learning rate = 0.000295 ; Loss = 1.445040\n",
      "2024-12-01 13:45:23.378000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-90000\n",
      "2024-12-01 13:45:23.378000: I training.py:192] Running evaluation for step 90000\n",
      "2024-12-01 13:46:10.562000: I training.py:192] Evaluation result for step 90000: loss = 0.806481 ; perplexity = 2.240011\n",
      "2024-12-01 13:47:11.692000: I runner.py:310] Step = 90100 ; steps/s = 1.64, tokens/s = 43630 (43630 target) ; Learning rate = 0.000294 ; Loss = 1.444078\n",
      "2024-12-01 13:48:12.405000: I runner.py:310] Step = 90200 ; steps/s = 1.65, tokens/s = 43111 (43111 target) ; Learning rate = 0.000294 ; Loss = 1.448454\n",
      "2024-12-01 13:49:13.692000: I runner.py:310] Step = 90300 ; steps/s = 1.63, tokens/s = 43501 (43501 target) ; Learning rate = 0.000294 ; Loss = 1.440869\n",
      "2024-12-01 13:50:14.422000: I runner.py:310] Step = 90400 ; steps/s = 1.65, tokens/s = 43085 (43085 target) ; Learning rate = 0.000294 ; Loss = 1.440637\n",
      "2024-12-01 13:51:15.650000: I runner.py:310] Step = 90500 ; steps/s = 1.63, tokens/s = 43553 (43553 target) ; Learning rate = 0.000294 ; Loss = 1.445625\n",
      "2024-12-01 13:52:16.844000: I runner.py:310] Step = 90600 ; steps/s = 1.63, tokens/s = 43577 (43577 target) ; Learning rate = 0.000294 ; Loss = 1.441906\n",
      "2024-12-01 13:53:17.532000: I runner.py:310] Step = 90700 ; steps/s = 1.65, tokens/s = 43133 (43133 target) ; Learning rate = 0.000293 ; Loss = 1.444302\n",
      "2024-12-01 13:54:18.724000: I runner.py:310] Step = 90800 ; steps/s = 1.63, tokens/s = 43567 (43567 target) ; Learning rate = 0.000293 ; Loss = 1.451275\n",
      "2024-12-01 13:55:19.415000: I runner.py:310] Step = 90900 ; steps/s = 1.65, tokens/s = 43125 (43125 target) ; Learning rate = 0.000293 ; Loss = 1.443656\n",
      "2024-12-01 13:56:20.626000: I runner.py:310] Step = 91000 ; steps/s = 1.63, tokens/s = 43569 (43569 target) ; Learning rate = 0.000293 ; Loss = 1.445960\n",
      "2024-12-01 13:57:21.816000: I runner.py:310] Step = 91100 ; steps/s = 1.63, tokens/s = 43579 (43579 target) ; Learning rate = 0.000293 ; Loss = 1.446123\n",
      "2024-12-01 13:58:22.588000: I runner.py:310] Step = 91200 ; steps/s = 1.65, tokens/s = 43062 (43062 target) ; Learning rate = 0.000293 ; Loss = 1.444790\n",
      "2024-12-01 13:59:23.769000: I runner.py:310] Step = 91300 ; steps/s = 1.63, tokens/s = 43595 (43595 target) ; Learning rate = 0.000293 ; Loss = 1.443815\n",
      "2024-12-01 14:00:24.962000: I runner.py:310] Step = 91400 ; steps/s = 1.63, tokens/s = 43570 (43570 target) ; Learning rate = 0.000292 ; Loss = 1.445216\n",
      "2024-12-01 14:01:25.641000: I runner.py:310] Step = 91500 ; steps/s = 1.65, tokens/s = 43142 (43142 target) ; Learning rate = 0.000292 ; Loss = 1.440347\n",
      "2024-12-01 14:02:26.846000: I runner.py:310] Step = 91600 ; steps/s = 1.63, tokens/s = 43550 (43550 target) ; Learning rate = 0.000292 ; Loss = 1.443368\n",
      "2024-12-01 14:03:27.621000: I runner.py:310] Step = 91700 ; steps/s = 1.65, tokens/s = 43061 (43061 target) ; Learning rate = 0.000292 ; Loss = 1.441557\n",
      "2024-12-01 14:04:28.794000: I runner.py:310] Step = 91800 ; steps/s = 1.63, tokens/s = 43595 (43595 target) ; Learning rate = 0.000292 ; Loss = 1.445381\n",
      "2024-12-01 14:05:29.970000: I runner.py:310] Step = 91900 ; steps/s = 1.63, tokens/s = 43595 (43595 target) ; Learning rate = 0.000292 ; Loss = 1.443851\n",
      "2024-12-01 14:06:30.701000: I runner.py:310] Step = 92000 ; steps/s = 1.65, tokens/s = 43086 (43086 target) ; Learning rate = 0.000291 ; Loss = 1.440342\n",
      "2024-12-01 14:07:31.865000: I runner.py:310] Step = 92100 ; steps/s = 1.64, tokens/s = 43594 (43594 target) ; Learning rate = 0.000291 ; Loss = 1.449643\n",
      "2024-12-01 14:08:32.705000: I runner.py:310] Step = 92200 ; steps/s = 1.64, tokens/s = 43018 (43018 target) ; Learning rate = 0.000291 ; Loss = 1.440045\n",
      "2024-12-01 14:09:33.898000: I runner.py:310] Step = 92300 ; steps/s = 1.63, tokens/s = 43586 (43586 target) ; Learning rate = 0.000291 ; Loss = 1.444008\n",
      "2024-12-01 14:10:35.155000: I runner.py:310] Step = 92400 ; steps/s = 1.63, tokens/s = 43525 (43525 target) ; Learning rate = 0.000291 ; Loss = 1.443027\n",
      "2024-12-01 14:11:35.902000: I runner.py:310] Step = 92500 ; steps/s = 1.65, tokens/s = 43092 (43092 target) ; Learning rate = 0.000291 ; Loss = 1.443465\n",
      "2024-12-01 14:12:37.064000: I runner.py:310] Step = 92600 ; steps/s = 1.64, tokens/s = 43588 (43588 target) ; Learning rate = 0.000290 ; Loss = 1.445663\n",
      "2024-12-01 14:13:38.323000: I runner.py:310] Step = 92700 ; steps/s = 1.63, tokens/s = 43535 (43535 target) ; Learning rate = 0.000290 ; Loss = 1.445285\n",
      "2024-12-01 14:14:39.009000: I runner.py:310] Step = 92800 ; steps/s = 1.65, tokens/s = 43130 (43130 target) ; Learning rate = 0.000290 ; Loss = 1.440381\n",
      "2024-12-01 14:15:40.200000: I runner.py:310] Step = 92900 ; steps/s = 1.63, tokens/s = 43572 (43572 target) ; Learning rate = 0.000290 ; Loss = 1.445656\n",
      "2024-12-01 14:16:40.922000: I runner.py:310] Step = 93000 ; steps/s = 1.65, tokens/s = 43097 (43097 target) ; Learning rate = 0.000290 ; Loss = 1.440509\n",
      "2024-12-01 14:17:42.100000: I runner.py:310] Step = 93100 ; steps/s = 1.63, tokens/s = 43592 (43592 target) ; Learning rate = 0.000290 ; Loss = 1.443755\n",
      "2024-12-01 14:18:43.306000: I runner.py:310] Step = 93200 ; steps/s = 1.63, tokens/s = 43566 (43566 target) ; Learning rate = 0.000290 ; Loss = 1.443948\n",
      "2024-12-01 14:19:44.016000: I runner.py:310] Step = 93300 ; steps/s = 1.65, tokens/s = 43113 (43113 target) ; Learning rate = 0.000289 ; Loss = 1.443881\n",
      "2024-12-01 14:20:45.143000: I runner.py:310] Step = 93400 ; steps/s = 1.64, tokens/s = 43620 (43620 target) ; Learning rate = 0.000289 ; Loss = 1.448732\n",
      "2024-12-01 14:21:45.854000: I runner.py:310] Step = 93500 ; steps/s = 1.65, tokens/s = 43109 (43109 target) ; Learning rate = 0.000289 ; Loss = 1.443520\n",
      "2024-12-01 14:22:47.075000: I runner.py:310] Step = 93600 ; steps/s = 1.63, tokens/s = 43566 (43566 target) ; Learning rate = 0.000289 ; Loss = 1.440931\n",
      "2024-12-01 14:23:48.297000: I runner.py:310] Step = 93700 ; steps/s = 1.63, tokens/s = 43555 (43555 target) ; Learning rate = 0.000289 ; Loss = 1.442060\n",
      "2024-12-01 14:24:49.019000: I runner.py:310] Step = 93800 ; steps/s = 1.65, tokens/s = 43096 (43096 target) ; Learning rate = 0.000289 ; Loss = 1.442798\n",
      "2024-12-01 14:25:50.207000: I runner.py:310] Step = 93900 ; steps/s = 1.63, tokens/s = 43572 (43572 target) ; Learning rate = 0.000288 ; Loss = 1.443752\n",
      "2024-12-01 14:26:51.503000: I runner.py:310] Step = 94000 ; steps/s = 1.63, tokens/s = 43511 (43511 target) ; Learning rate = 0.000288 ; Loss = 1.445084\n",
      "2024-12-01 14:27:52.241000: I runner.py:310] Step = 94100 ; steps/s = 1.65, tokens/s = 43084 (43084 target) ; Learning rate = 0.000288 ; Loss = 1.440982\n",
      "2024-12-01 14:28:53.440000: I runner.py:310] Step = 94200 ; steps/s = 1.63, tokens/s = 43577 (43577 target) ; Learning rate = 0.000288 ; Loss = 1.443608\n",
      "2024-12-01 14:29:54.119000: I runner.py:310] Step = 94300 ; steps/s = 1.65, tokens/s = 43135 (43135 target) ; Learning rate = 0.000288 ; Loss = 1.441839\n",
      "2024-12-01 14:30:55.236000: I runner.py:310] Step = 94400 ; steps/s = 1.64, tokens/s = 43634 (43634 target) ; Learning rate = 0.000288 ; Loss = 1.445184\n",
      "2024-12-01 14:31:56.446000: I runner.py:310] Step = 94500 ; steps/s = 1.63, tokens/s = 43562 (43562 target) ; Learning rate = 0.000288 ; Loss = 1.443992\n",
      "2024-12-01 14:32:57.138000: I runner.py:310] Step = 94600 ; steps/s = 1.65, tokens/s = 43116 (43116 target) ; Learning rate = 0.000287 ; Loss = 1.448897\n",
      "2024-12-01 14:33:58.397000: I runner.py:310] Step = 94700 ; steps/s = 1.63, tokens/s = 43514 (43514 target) ; Learning rate = 0.000287 ; Loss = 1.445778\n",
      "2024-12-01 14:34:59.259000: I runner.py:310] Step = 94800 ; steps/s = 1.64, tokens/s = 43018 (43018 target) ; Learning rate = 0.000287 ; Loss = 1.442398\n",
      "2024-12-01 14:36:00.500000: I runner.py:310] Step = 94900 ; steps/s = 1.63, tokens/s = 43545 (43545 target) ; Learning rate = 0.000287 ; Loss = 1.440647\n",
      "2024-12-01 14:37:01.697000: I runner.py:310] Step = 95000 ; steps/s = 1.63, tokens/s = 43574 (43574 target) ; Learning rate = 0.000287 ; Loss = 1.445689\n",
      "2024-12-01 14:37:01.701000: I training.py:192] Running evaluation for step 95000\n",
      "2024-12-01 14:37:49.552000: I training.py:192] Evaluation result for step 95000: loss = 0.810402 ; perplexity = 2.248811\n",
      "2024-12-01 14:38:50.184000: I runner.py:310] Step = 95100 ; steps/s = 1.65, tokens/s = 43175 (43175 target) ; Learning rate = 0.000287 ; Loss = 1.444657\n",
      "2024-12-01 14:39:51.360000: I runner.py:310] Step = 95200 ; steps/s = 1.63, tokens/s = 43580 (43580 target) ; Learning rate = 0.000286 ; Loss = 1.448112\n",
      "2024-12-01 14:40:52.569000: I runner.py:310] Step = 95300 ; steps/s = 1.63, tokens/s = 43568 (43568 target) ; Learning rate = 0.000286 ; Loss = 1.447266\n",
      "2024-12-01 14:41:53.317000: I runner.py:310] Step = 95400 ; steps/s = 1.65, tokens/s = 43081 (43081 target) ; Learning rate = 0.000286 ; Loss = 1.445988\n",
      "2024-12-01 14:42:54.547000: I runner.py:310] Step = 95500 ; steps/s = 1.63, tokens/s = 43553 (43553 target) ; Learning rate = 0.000286 ; Loss = 1.443661\n",
      "2024-12-01 14:43:55.302000: I runner.py:310] Step = 95600 ; steps/s = 1.65, tokens/s = 43073 (43073 target) ; Learning rate = 0.000286 ; Loss = 1.439000\n",
      "2024-12-01 14:44:56.537000: I runner.py:310] Step = 95700 ; steps/s = 1.63, tokens/s = 43558 (43558 target) ; Learning rate = 0.000286 ; Loss = 1.444803\n",
      "2024-12-01 14:45:57.763000: I runner.py:310] Step = 95800 ; steps/s = 1.63, tokens/s = 43553 (43553 target) ; Learning rate = 0.000286 ; Loss = 1.448498\n",
      "2024-12-01 14:46:58.515000: I runner.py:310] Step = 95900 ; steps/s = 1.65, tokens/s = 43079 (43079 target) ; Learning rate = 0.000285 ; Loss = 1.445096\n",
      "2024-12-01 14:47:59.753000: I runner.py:310] Step = 96000 ; steps/s = 1.63, tokens/s = 43542 (43542 target) ; Learning rate = 0.000285 ; Loss = 1.448137\n",
      "2024-12-01 14:49:00.562000: I runner.py:310] Step = 96100 ; steps/s = 1.64, tokens/s = 43039 (43039 target) ; Learning rate = 0.000285 ; Loss = 1.440614\n",
      "2024-12-01 14:50:01.721000: I runner.py:310] Step = 96200 ; steps/s = 1.64, tokens/s = 43595 (43595 target) ; Learning rate = 0.000285 ; Loss = 1.438145\n",
      "2024-12-01 14:51:02.915000: I runner.py:310] Step = 96300 ; steps/s = 1.63, tokens/s = 43576 (43576 target) ; Learning rate = 0.000285 ; Loss = 1.439854\n",
      "2024-12-01 14:52:03.625000: I runner.py:310] Step = 96400 ; steps/s = 1.65, tokens/s = 43107 (43107 target) ; Learning rate = 0.000285 ; Loss = 1.439714\n",
      "2024-12-01 14:53:04.859000: I runner.py:310] Step = 96500 ; steps/s = 1.63, tokens/s = 43557 (43557 target) ; Learning rate = 0.000285 ; Loss = 1.445554\n",
      "2024-12-01 14:54:06.098000: I runner.py:310] Step = 96600 ; steps/s = 1.63, tokens/s = 43546 (43546 target) ; Learning rate = 0.000284 ; Loss = 1.444632\n",
      "2024-12-01 14:55:06.846000: I runner.py:310] Step = 96700 ; steps/s = 1.65, tokens/s = 43082 (43082 target) ; Learning rate = 0.000284 ; Loss = 1.442690\n",
      "2024-12-01 14:56:08.016000: I runner.py:310] Step = 96800 ; steps/s = 1.63, tokens/s = 43592 (43592 target) ; Learning rate = 0.000284 ; Loss = 1.438620\n",
      "2024-12-01 14:57:08.801000: I runner.py:310] Step = 96900 ; steps/s = 1.65, tokens/s = 43051 (43051 target) ; Learning rate = 0.000284 ; Loss = 1.435984\n",
      "2024-12-01 14:58:10.035000: I runner.py:310] Step = 97000 ; steps/s = 1.63, tokens/s = 43537 (43537 target) ; Learning rate = 0.000284 ; Loss = 1.439078\n",
      "2024-12-01 14:59:11.299000: I runner.py:310] Step = 97100 ; steps/s = 1.63, tokens/s = 43535 (43535 target) ; Learning rate = 0.000284 ; Loss = 1.444851\n",
      "2024-12-01 15:00:11.957000: I runner.py:310] Step = 97200 ; steps/s = 1.65, tokens/s = 43153 (43153 target) ; Learning rate = 0.000284 ; Loss = 1.442897\n",
      "2024-12-01 15:01:13.121000: I runner.py:310] Step = 97300 ; steps/s = 1.64, tokens/s = 43591 (43591 target) ; Learning rate = 0.000283 ; Loss = 1.443395\n",
      "2024-12-01 15:02:13.849000: I runner.py:310] Step = 97400 ; steps/s = 1.65, tokens/s = 43098 (43098 target) ; Learning rate = 0.000283 ; Loss = 1.441470\n",
      "2024-12-01 15:03:15.056000: I runner.py:310] Step = 97500 ; steps/s = 1.63, tokens/s = 43565 (43565 target) ; Learning rate = 0.000283 ; Loss = 1.439341\n",
      "2024-12-01 15:04:16.276000: I runner.py:310] Step = 97600 ; steps/s = 1.63, tokens/s = 43556 (43556 target) ; Learning rate = 0.000283 ; Loss = 1.445737\n",
      "2024-12-01 15:05:17.058000: I runner.py:310] Step = 97700 ; steps/s = 1.65, tokens/s = 43055 (43055 target) ; Learning rate = 0.000283 ; Loss = 1.445245\n",
      "2024-12-01 15:06:18.318000: I runner.py:310] Step = 97800 ; steps/s = 1.63, tokens/s = 43534 (43534 target) ; Learning rate = 0.000283 ; Loss = 1.444242\n",
      "2024-12-01 15:07:19.518000: I runner.py:310] Step = 97900 ; steps/s = 1.63, tokens/s = 43577 (43577 target) ; Learning rate = 0.000282 ; Loss = 1.444604\n",
      "2024-12-01 15:08:20.236000: I runner.py:310] Step = 98000 ; steps/s = 1.65, tokens/s = 43109 (43109 target) ; Learning rate = 0.000282 ; Loss = 1.440107\n",
      "2024-12-01 15:09:21.477000: I runner.py:310] Step = 98100 ; steps/s = 1.63, tokens/s = 43524 (43524 target) ; Learning rate = 0.000282 ; Loss = 1.442060\n",
      "2024-12-01 15:10:22.207000: I runner.py:310] Step = 98200 ; steps/s = 1.65, tokens/s = 43104 (43104 target) ; Learning rate = 0.000282 ; Loss = 1.438257\n",
      "2024-12-01 15:11:23.404000: I runner.py:310] Step = 98300 ; steps/s = 1.63, tokens/s = 43575 (43575 target) ; Learning rate = 0.000282 ; Loss = 1.446562\n",
      "2024-12-01 15:12:24.628000: I runner.py:310] Step = 98400 ; steps/s = 1.63, tokens/s = 43562 (43562 target) ; Learning rate = 0.000282 ; Loss = 1.445560\n",
      "2024-12-01 15:13:25.426000: I runner.py:310] Step = 98500 ; steps/s = 1.64, tokens/s = 43046 (43046 target) ; Learning rate = 0.000282 ; Loss = 1.439817\n",
      "2024-12-01 15:14:26.637000: I runner.py:310] Step = 98600 ; steps/s = 1.63, tokens/s = 43564 (43564 target) ; Learning rate = 0.000281 ; Loss = 1.437622\n",
      "2024-12-01 15:15:27.357000: I runner.py:310] Step = 98700 ; steps/s = 1.65, tokens/s = 43092 (43092 target) ; Learning rate = 0.000281 ; Loss = 1.441493\n",
      "2024-12-01 15:16:28.570000: I runner.py:310] Step = 98800 ; steps/s = 1.63, tokens/s = 43562 (43562 target) ; Learning rate = 0.000281 ; Loss = 1.441816\n",
      "2024-12-01 15:17:29.813000: I runner.py:310] Step = 98900 ; steps/s = 1.63, tokens/s = 43539 (43539 target) ; Learning rate = 0.000281 ; Loss = 1.453711\n",
      "2024-12-01 15:18:30.600000: I runner.py:310] Step = 99000 ; steps/s = 1.65, tokens/s = 43058 (43058 target) ; Learning rate = 0.000281 ; Loss = 1.442368\n",
      "2024-12-01 15:19:31.848000: I runner.py:310] Step = 99100 ; steps/s = 1.63, tokens/s = 43541 (43541 target) ; Learning rate = 0.000281 ; Loss = 1.444659\n",
      "2024-12-01 15:20:33.095000: I runner.py:310] Step = 99200 ; steps/s = 1.63, tokens/s = 43538 (43538 target) ; Learning rate = 0.000281 ; Loss = 1.448235\n",
      "2024-12-01 15:21:33.760000: I runner.py:310] Step = 99300 ; steps/s = 1.65, tokens/s = 43141 (43141 target) ; Learning rate = 0.000280 ; Loss = 1.442021\n",
      "2024-12-01 15:22:34.997000: I runner.py:310] Step = 99400 ; steps/s = 1.63, tokens/s = 43535 (43535 target) ; Learning rate = 0.000280 ; Loss = 1.443377\n",
      "2024-12-01 15:23:35.713000: I runner.py:310] Step = 99500 ; steps/s = 1.65, tokens/s = 43113 (43113 target) ; Learning rate = 0.000280 ; Loss = 1.443418\n",
      "2024-12-01 15:24:36.898000: I runner.py:310] Step = 99600 ; steps/s = 1.63, tokens/s = 43576 (43576 target) ; Learning rate = 0.000280 ; Loss = 1.439114\n",
      "2024-12-01 15:25:38.173000: I runner.py:310] Step = 99700 ; steps/s = 1.63, tokens/s = 43524 (43524 target) ; Learning rate = 0.000280 ; Loss = 1.441332\n",
      "2024-12-01 15:26:38.867000: I runner.py:310] Step = 99800 ; steps/s = 1.65, tokens/s = 43123 (43123 target) ; Learning rate = 0.000280 ; Loss = 1.447724\n",
      "2024-12-01 15:27:40.084000: I runner.py:310] Step = 99900 ; steps/s = 1.63, tokens/s = 43561 (43561 target) ; Learning rate = 0.000280 ; Loss = 1.439259\n",
      "2024-12-01 15:28:40.766000: I runner.py:310] Step = 100000 ; steps/s = 1.65, tokens/s = 43124 (43124 target) ; Learning rate = 0.000280 ; Loss = 1.437882\n",
      "2024-12-01 15:28:42.952000: I training.py:176] Saved checkpoint POS_KK_TR_EN_2/ckpt-100000\n",
      "2024-12-01 15:28:42.953000: I training.py:192] Running evaluation for step 100000\n",
      "2024-12-01 15:29:30.482000: I training.py:192] Evaluation result for step 100000: loss = 0.819466 ; perplexity = 2.269287\n",
      "2024-12-01 15:30:31.550000: I runner.py:310] Step = 100100 ; steps/s = 1.64, tokens/s = 43682 (43682 target) ; Learning rate = 0.000279 ; Loss = 1.439920\n",
      "2024-12-01 15:31:32.799000: I runner.py:310] Step = 100200 ; steps/s = 1.63, tokens/s = 43541 (43541 target) ; Learning rate = 0.000279 ; Loss = 1.448944\n",
      "2024-12-01 15:32:33.557000: I runner.py:310] Step = 100300 ; steps/s = 1.65, tokens/s = 43062 (43062 target) ; Learning rate = 0.000279 ; Loss = 1.442056\n",
      "2024-12-01 15:33:34.735000: I runner.py:310] Step = 100400 ; steps/s = 1.63, tokens/s = 43600 (43600 target) ; Learning rate = 0.000279 ; Loss = 1.449631\n",
      "2024-12-01 15:34:36.000000: I runner.py:310] Step = 100500 ; steps/s = 1.63, tokens/s = 43517 (43517 target) ; Learning rate = 0.000279 ; Loss = 1.439896\n",
      "2024-12-01 15:35:36.825000: I runner.py:310] Step = 100600 ; steps/s = 1.64, tokens/s = 43026 (43026 target) ; Learning rate = 0.000279 ; Loss = 1.441125\n",
      "2024-12-01 15:36:38.082000: I runner.py:310] Step = 100700 ; steps/s = 1.63, tokens/s = 43529 (43529 target) ; Learning rate = 0.000279 ; Loss = 1.442705\n",
      "2024-12-01 15:37:38.914000: I runner.py:310] Step = 100800 ; steps/s = 1.64, tokens/s = 43018 (43018 target) ; Learning rate = 0.000278 ; Loss = 1.439095\n",
      "2024-12-01 15:38:40.127000: I runner.py:310] Step = 100900 ; steps/s = 1.63, tokens/s = 43562 (43562 target) ; Learning rate = 0.000278 ; Loss = 1.441446\n",
      "2024-12-01 15:39:41.381000: I runner.py:310] Step = 101000 ; steps/s = 1.63, tokens/s = 43542 (43542 target) ; Learning rate = 0.000278 ; Loss = 1.437430\n",
      "2024-12-01 15:40:42.106000: I runner.py:310] Step = 101100 ; steps/s = 1.65, tokens/s = 43099 (43099 target) ; Learning rate = 0.000278 ; Loss = 1.440004\n",
      "2024-12-01 15:41:43.357000: I runner.py:310] Step = 101200 ; steps/s = 1.63, tokens/s = 43538 (43538 target) ; Learning rate = 0.000278 ; Loss = 1.443844\n",
      "2024-12-01 15:42:44.155000: I runner.py:310] Step = 101300 ; steps/s = 1.64, tokens/s = 43043 (43043 target) ; Learning rate = 0.000278 ; Loss = 1.439135\n",
      "2024-12-01 15:43:45.376000: I runner.py:310] Step = 101400 ; steps/s = 1.63, tokens/s = 43550 (43550 target) ; Learning rate = 0.000278 ; Loss = 1.438218\n",
      "2024-12-01 15:44:46.606000: I runner.py:310] Step = 101500 ; steps/s = 1.63, tokens/s = 43546 (43546 target) ; Learning rate = 0.000277 ; Loss = 1.442715\n",
      "2024-12-01 15:45:47.422000: I runner.py:310] Step = 101600 ; steps/s = 1.64, tokens/s = 43042 (43042 target) ; Learning rate = 0.000277 ; Loss = 1.438999\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "#Kk-En (POS Tags) -> Tr-En (Tatoeba)(POS Tags)\n",
    "!onmt-main --model kk-tr-en-modelim.py --config kk-tr-en-pos-asl-2.yml --auto_config --checkpoint_path POS_KK_TR_EN/ckpt-100000 train --with_eval --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06c8304d-3343-4a77-8ab4-41687decf1e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-01 15:47:25.160438: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-01 15:47:25.977042: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-01 15:47:25.977116: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-01 15:47:25.977125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-01 15:47:26.988000: I main.py:308] Loading model description from POS_KK_TR_EN_2/model_description.py\n",
      "2024-12-01 15:47:27.190000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-01 15:47:27.190000: I main.py:315] Using model:\n",
      "(model): MultiFeaturesTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): ParallelInputter(\n",
      "      (inputters): ListWrapper(\n",
      "        (0): WordEmbedder()\n",
      "        (1): WordEmbedder()\n",
      "      )\n",
      "      (reducer): ConcatReducer()\n",
      "    )\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): ParallelInputter(\n",
      "        (inputters): ListWrapper(\n",
      "          (0): WordEmbedder()\n",
      "          (1): WordEmbedder()\n",
      "        )\n",
      "        (reducer): ConcatReducer()\n",
      "      )\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-01 15:47:27.195000: I main.py:340] Using parameters:\n",
      "data:\n",
      "  eval_features_file:\n",
      "  - Tatoeba_tokens_dev_shared\n",
      "  - Tatoeba_pos_tags_dev_shared.txt\n",
      "  eval_labels_file: Tatoeba_dev_target_tokens_shared.txt\n",
      "  source_1_vocabulary: kk_tr_shared_vocab.vocab\n",
      "  source_2_vocabulary: Zemberek_TR_unique_pos\n",
      "  target_vocabulary: en_shared_vocab.vocab\n",
      "  train_features_file:\n",
      "  - Tatoeba_tokens_train_shared\n",
      "  - Tatoeba_pos_tags_train_shared.txt\n",
      "  train_labels_file: Tatoeba_train_target_tokens_shared.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: POS_KK_TR_EN_2\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 205000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-01 15:47:27.375101: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-01 15:47:27.978724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-01 15:47:28.132000: I inputter.py:316] Initialized source_1 input layer:\n",
      "2024-12-01 15:47:28.132000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-01 15:47:28.132000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-01 15:47:28.135000: I inputter.py:316] Initialized source_2 input layer:\n",
      "2024-12-01 15:47:28.135000: I inputter.py:316]  - vocabulary size: 21\n",
      "2024-12-01 15:47:28.135000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-01 15:47:28.210000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-01 15:47:28.210000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-01 15:47:28.210000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-01 15:47:28.229000: I runner.py:462] Restored checkpoint POS_KK_TR_EN_2/ckpt-100000\n",
      "2024-12-01 15:47:28.272000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-01 15:47:28.994548: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-01 15:47:29.108000: I runner.py:471] Tracing and optimizing the inference graph...\n",
      "2024-12-01 15:47:42.758348: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-01 15:47:43.656347: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-01 15:47:54.631000: I runner.py:471] 2441 predictions are buffered, but waiting for the prediction of queued line 25 to advance the output...\n",
      "2024-12-01 15:48:04.681000: I runner.py:471] 4713 predictions are buffered, but waiting for the prediction of queued line 25 to advance the output...\n",
      "2024-12-01 15:48:14.798000: I runner.py:471] 7049 predictions are buffered, but waiting for the prediction of queued line 25 to advance the output...\n",
      "2024-12-01 15:48:24.881000: I runner.py:471] 9321 predictions are buffered, but waiting for the prediction of queued line 25 to advance the output...\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 onmt-main --config kk-tr-en-pos-asl-2.yml --auto_config --checkpoint_path POS_KK_TR_EN_2/ckpt-100000 infer --features_file Tatoeba_tokens_test_shared Tatoeba_pos_tags_test_shared.txt --predictions_file output_kk_tr_en_pos.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7938ae-840f-4e4d-8e07-80cef6eda673",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 MT-Preparation/subwording/3-desubword.py en_shared_vocab.model output_kk_tr_en_pos.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7b95ddb-506a-4e92-aa3c-bda91b190993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference first sentence: I won't stay there very long.\n",
      "Translated first sentence: I won't stay there very long .\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "BLEU:  BLEU = 60.01 80.3/64.6/54.3/46.1 (BP = 1.000 ratio = 1.018 hyp_len = 79015 ref_len = 77587)\n",
      "CHRF:  chrF2 = 74.12\n"
     ]
    }
   ],
   "source": [
    "#BLEU and chrF scores\n",
    "!python3 compute-bleu.py Tatoeba.en-tr.en-filtered.en.test output_kk_tr_en_pos.txt.desubword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "077a9bfd-e19f-48f7-a4cc-7880c39b82fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama METEOR Puan: 0.816043893864375\n"
     ]
    }
   ],
   "source": [
    "# Average METEOR score (Ortalama METEOR Puan)\n",
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "def read_and_tokenize_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return [nltk.word_tokenize(line.strip()) for line in lines]\n",
    "\n",
    "def calculate_meteor(reference_file, hypothesis_file):\n",
    "    references = read_and_tokenize_file(reference_file)\n",
    "    hypotheses = read_and_tokenize_file(hypothesis_file)\n",
    "    \n",
    "    if len(references) != len(hypotheses):\n",
    "        raise ValueError(\"Dosyalarn satr saylar elemiyor\")\n",
    "\n",
    "    total_meteor_score = 0.0\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        total_meteor_score += meteor_score([ref], hyp)\n",
    "\n",
    "    average_meteor_score = total_meteor_score / len(references)\n",
    "    return average_meteor_score\n",
    "\n",
    "reference_file = 'Tatoeba.en-tr.en-filtered.en.test'\n",
    "hypothesis_file = 'output_kk_tr_en_pos.txt.desubword'\n",
    "\n",
    "score = calculate_meteor(reference_file, hypothesis_file)\n",
    "print(f\"Ortalama METEOR Puan: {score}\") #Average METEOR score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e166941-79e0-4213-bcfd-7e6c98797afa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 09:43:44.127767: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-05 09:43:44.899212: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-05 09:43:44.899275: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-05 09:43:44.899283: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-05 09:43:45.854000: I onmt-main:8] Creating model directory TR-EN_std\n",
      "2024-12-05 09:43:46.052000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-05 09:43:46.052000: I main.py:315] Using model:\n",
      "(model): MyCustomTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): WordEmbedder()\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): WordEmbedder()\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-05 09:43:46.055847: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-05 09:43:47.635119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-05 09:43:47.635773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7698 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-12-05 09:43:47.636760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 6099 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "2024-12-05 09:43:47.643000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file: Tatoeba_tokens_dev\n",
      "  eval_labels_file: Tatoeba_dev_target_tokens.txt\n",
      "  source_vocabulary: tr_vocab.vocab\n",
      "  target_vocabulary: en_vocab.vocab\n",
      "  train_features_file: Tatoeba_tokens_train\n",
      "  train_labels_file: Tatoeba_train_target_tokens.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: TR-EN_std\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 250000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-05 09:43:47.983000: I inputter.py:316] Initialized source input layer:\n",
      "2024-12-05 09:43:47.983000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-05 09:43:47.983000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-05 09:43:48.058000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-05 09:43:48.058000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-05 09:43:48.058000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-05 09:43:48.062000: W runner.py:269] No checkpoint to restore in TR-EN_std\n",
      "2024-12-05 09:43:48.065000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "2024-12-05 09:43:48.112000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-05 09:43:49.017299: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-05 09:43:49.148000: I main.py:325] Accumulate gradients of 7 iterations to reach effective batch size of 25000\n",
      "2024-12-05 09:43:49.272000: I mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2024-12-05 09:43:49.511000: I dataset_ops.py:2542] Training on 647485 examples\n",
      "2024-12-05 09:44:52.560524: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-05 09:44:53.537043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-05 09:44:53.853906: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-05 09:45:02.834000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-05 09:45:02.859000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-05 09:45:04.420000: I cross_device_ops.py:897] batch_all_reduce: 260 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-05 09:45:09.346000: I cross_device_ops.py:897] batch_all_reduce: 260 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-05 09:45:16.004000: I runner.py:310] Number of model parameters: 93326081\n",
      "2024-12-05 09:45:16.008000: I runner.py:310] Number of model weights: 260 (trainable = 260, non trainable = 0)\n",
      "2024-12-05 09:45:16.040000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-05 09:45:16.047000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-05 09:45:18.083000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-1\n",
      "2024-12-05 09:45:18.798000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-05 09:45:18.823000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-05 09:45:19.489000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-05 09:45:19.513000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-05 09:45:20.123000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-05 09:45:20.146000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-05 09:46:19.283000: I runner.py:310] Step = 100 ; steps/s = 1.63, tokens/s = 74328 (30381 source, 43947 target) ; Learning rate = 0.000009 ; Loss = 9.581657\n",
      "2024-12-05 09:47:20.188000: I runner.py:310] Step = 200 ; steps/s = 1.64, tokens/s = 75109 (30695 source, 44414 target) ; Learning rate = 0.000018 ; Loss = 8.491051\n",
      "2024-12-05 09:48:20.800000: I runner.py:310] Step = 300 ; steps/s = 1.65, tokens/s = 74188 (30358 source, 43830 target) ; Learning rate = 0.000027 ; Loss = 7.157213\n",
      "2024-12-05 09:49:21.544000: I runner.py:310] Step = 400 ; steps/s = 1.65, tokens/s = 75315 (30787 source, 44528 target) ; Learning rate = 0.000035 ; Loss = 6.216323\n",
      "2024-12-05 09:50:22.397000: I runner.py:310] Step = 500 ; steps/s = 1.64, tokens/s = 75191 (30748 source, 44443 target) ; Learning rate = 0.000044 ; Loss = 5.679247\n",
      "2024-12-05 09:51:22.706000: I runner.py:310] Step = 600 ; steps/s = 1.66, tokens/s = 74550 (30500 source, 44050 target) ; Learning rate = 0.000053 ; Loss = 5.293134\n",
      "2024-12-05 09:52:23.486000: I runner.py:310] Step = 700 ; steps/s = 1.65, tokens/s = 75285 (30785 source, 44500 target) ; Learning rate = 0.000062 ; Loss = 4.923573\n",
      "2024-12-05 09:53:23.818000: I runner.py:310] Step = 800 ; steps/s = 1.66, tokens/s = 74527 (30496 source, 44031 target) ; Learning rate = 0.000071 ; Loss = 4.667871\n",
      "2024-12-05 09:54:24.587000: I runner.py:310] Step = 900 ; steps/s = 1.65, tokens/s = 75276 (30764 source, 44512 target) ; Learning rate = 0.000080 ; Loss = 4.430717\n",
      "2024-12-05 09:55:25.289000: I runner.py:310] Step = 1000 ; steps/s = 1.65, tokens/s = 75373 (30817 source, 44556 target) ; Learning rate = 0.000088 ; Loss = 4.317149\n",
      "2024-12-05 09:56:25.583000: I runner.py:310] Step = 1100 ; steps/s = 1.66, tokens/s = 74581 (30518 source, 44063 target) ; Learning rate = 0.000097 ; Loss = 4.276008\n",
      "2024-12-05 09:57:26.339000: I runner.py:310] Step = 1200 ; steps/s = 1.65, tokens/s = 75294 (30778 source, 44516 target) ; Learning rate = 0.000106 ; Loss = 3.918098\n",
      "2024-12-05 09:58:26.610000: I runner.py:310] Step = 1300 ; steps/s = 1.66, tokens/s = 74617 (30534 source, 44083 target) ; Learning rate = 0.000115 ; Loss = 3.868480\n",
      "2024-12-05 09:59:27.324000: I runner.py:310] Step = 1400 ; steps/s = 1.65, tokens/s = 75354 (30803 source, 44551 target) ; Learning rate = 0.000124 ; Loss = 3.521414\n",
      "2024-12-05 10:00:28.066000: I runner.py:310] Step = 1500 ; steps/s = 1.65, tokens/s = 75324 (30799 source, 44525 target) ; Learning rate = 0.000133 ; Loss = 3.296766\n",
      "2024-12-05 10:01:28.281000: I runner.py:310] Step = 1600 ; steps/s = 1.66, tokens/s = 74670 (30554 source, 44116 target) ; Learning rate = 0.000142 ; Loss = 3.069588\n",
      "2024-12-05 10:02:29.042000: I runner.py:310] Step = 1700 ; steps/s = 1.65, tokens/s = 75297 (30791 source, 44506 target) ; Learning rate = 0.000150 ; Loss = 2.900774\n",
      "2024-12-05 10:03:29.303000: I runner.py:310] Step = 1800 ; steps/s = 1.66, tokens/s = 74613 (30518 source, 44095 target) ; Learning rate = 0.000159 ; Loss = 2.912951\n",
      "2024-12-05 10:04:30.047000: I runner.py:310] Step = 1900 ; steps/s = 1.65, tokens/s = 75306 (30779 source, 44527 target) ; Learning rate = 0.000168 ; Loss = 2.825097\n",
      "2024-12-05 10:05:30.749000: I runner.py:310] Step = 2000 ; steps/s = 1.65, tokens/s = 75391 (30833 source, 44558 target) ; Learning rate = 0.000177 ; Loss = 2.667919\n",
      "2024-12-05 10:06:31.021000: I runner.py:310] Step = 2100 ; steps/s = 1.66, tokens/s = 74577 (30505 source, 44072 target) ; Learning rate = 0.000186 ; Loss = 2.657210\n",
      "2024-12-05 10:07:31.673000: I runner.py:310] Step = 2200 ; steps/s = 1.65, tokens/s = 75421 (30823 source, 44598 target) ; Learning rate = 0.000195 ; Loss = 2.592135\n",
      "2024-12-05 10:08:31.943000: I runner.py:310] Step = 2300 ; steps/s = 1.66, tokens/s = 74639 (30557 source, 44082 target) ; Learning rate = 0.000203 ; Loss = 2.373816\n",
      "2024-12-05 10:09:32.646000: I runner.py:310] Step = 2400 ; steps/s = 1.65, tokens/s = 75368 (30814 source, 44554 target) ; Learning rate = 0.000212 ; Loss = 2.424615\n",
      "2024-12-05 10:10:33.327000: I runner.py:310] Step = 2500 ; steps/s = 1.65, tokens/s = 75395 (30821 source, 44574 target) ; Learning rate = 0.000221 ; Loss = 2.463050\n",
      "2024-12-05 10:11:33.602000: I runner.py:310] Step = 2600 ; steps/s = 1.66, tokens/s = 74596 (30519 source, 44077 target) ; Learning rate = 0.000230 ; Loss = 2.405637\n",
      "2024-12-05 10:12:34.304000: I runner.py:310] Step = 2700 ; steps/s = 1.65, tokens/s = 75383 (30826 source, 44557 target) ; Learning rate = 0.000239 ; Loss = 2.312999\n",
      "2024-12-05 10:13:34.551000: I runner.py:310] Step = 2800 ; steps/s = 1.66, tokens/s = 74628 (30534 source, 44094 target) ; Learning rate = 0.000248 ; Loss = 2.411741\n",
      "2024-12-05 10:14:35.234000: I runner.py:310] Step = 2900 ; steps/s = 1.65, tokens/s = 75380 (30807 source, 44573 target) ; Learning rate = 0.000256 ; Loss = 2.251316\n",
      "2024-12-05 10:15:35.986000: I runner.py:310] Step = 3000 ; steps/s = 1.65, tokens/s = 75328 (30806 source, 44522 target) ; Learning rate = 0.000265 ; Loss = 2.244994\n",
      "2024-12-05 10:16:36.280000: I runner.py:310] Step = 3100 ; steps/s = 1.66, tokens/s = 74572 (30509 source, 44063 target) ; Learning rate = 0.000274 ; Loss = 2.208446\n",
      "2024-12-05 10:17:37.570000: I runner.py:310] Step = 3200 ; steps/s = 1.63, tokens/s = 74651 (30520 source, 44131 target) ; Learning rate = 0.000283 ; Loss = 2.356409\n",
      "2024-12-05 10:18:37.860000: I runner.py:310] Step = 3300 ; steps/s = 1.66, tokens/s = 74574 (30514 source, 44060 target) ; Learning rate = 0.000292 ; Loss = 2.231638\n",
      "2024-12-05 10:19:38.503000: I runner.py:310] Step = 3400 ; steps/s = 1.65, tokens/s = 75439 (30841 source, 44598 target) ; Learning rate = 0.000301 ; Loss = 2.193604\n",
      "2024-12-05 10:20:39.310000: I runner.py:310] Step = 3500 ; steps/s = 1.64, tokens/s = 75252 (30769 source, 44483 target) ; Learning rate = 0.000309 ; Loss = 2.157512\n",
      "2024-12-05 10:21:39.534000: I runner.py:310] Step = 3600 ; steps/s = 1.66, tokens/s = 74649 (30534 source, 44115 target) ; Learning rate = 0.000318 ; Loss = 2.115562\n",
      "2024-12-05 10:22:40.254000: I runner.py:310] Step = 3700 ; steps/s = 1.65, tokens/s = 75334 (30785 source, 44549 target) ; Learning rate = 0.000327 ; Loss = 2.121583\n",
      "2024-12-05 10:23:40.503000: I runner.py:310] Step = 3800 ; steps/s = 1.66, tokens/s = 74662 (30574 source, 44088 target) ; Learning rate = 0.000336 ; Loss = 2.237525\n",
      "2024-12-05 10:24:41.236000: I runner.py:310] Step = 3900 ; steps/s = 1.65, tokens/s = 75302 (30764 source, 44538 target) ; Learning rate = 0.000345 ; Loss = 2.097905\n",
      "2024-12-05 10:25:41.991000: I runner.py:310] Step = 4000 ; steps/s = 1.65, tokens/s = 75278 (30769 source, 44509 target) ; Learning rate = 0.000354 ; Loss = 2.060863\n",
      "2024-12-05 10:26:42.253000: I runner.py:310] Step = 4100 ; steps/s = 1.66, tokens/s = 74646 (30558 source, 44088 target) ; Learning rate = 0.000362 ; Loss = 2.004858\n",
      "2024-12-05 10:27:42.968000: I runner.py:310] Step = 4200 ; steps/s = 1.65, tokens/s = 75347 (30802 source, 44545 target) ; Learning rate = 0.000371 ; Loss = 2.000326\n",
      "2024-12-05 10:28:43.713000: I runner.py:310] Step = 4300 ; steps/s = 1.65, tokens/s = 75321 (30797 source, 44524 target) ; Learning rate = 0.000380 ; Loss = 1.960881\n",
      "2024-12-05 10:29:44.000000: I runner.py:310] Step = 4400 ; steps/s = 1.66, tokens/s = 74581 (30508 source, 44073 target) ; Learning rate = 0.000389 ; Loss = 1.978917\n",
      "2024-12-05 10:30:44.718000: I runner.py:310] Step = 4500 ; steps/s = 1.65, tokens/s = 75354 (30812 source, 44542 target) ; Learning rate = 0.000398 ; Loss = 2.015162\n",
      "2024-12-05 10:31:45.006000: I runner.py:310] Step = 4600 ; steps/s = 1.66, tokens/s = 74584 (30515 source, 44069 target) ; Learning rate = 0.000407 ; Loss = 2.009046\n",
      "2024-12-05 10:32:45.714000: I runner.py:310] Step = 4700 ; steps/s = 1.65, tokens/s = 75352 (30792 source, 44560 target) ; Learning rate = 0.000416 ; Loss = 2.009457\n",
      "2024-12-05 10:33:46.447000: I runner.py:310] Step = 4800 ; steps/s = 1.65, tokens/s = 75352 (30822 source, 44530 target) ; Learning rate = 0.000424 ; Loss = 1.964362\n",
      "2024-12-05 10:34:46.722000: I runner.py:310] Step = 4900 ; steps/s = 1.66, tokens/s = 74589 (30512 source, 44077 target) ; Learning rate = 0.000433 ; Loss = 1.935748\n",
      "2024-12-05 10:35:47.436000: I runner.py:310] Step = 5000 ; steps/s = 1.65, tokens/s = 75380 (30832 source, 44548 target) ; Learning rate = 0.000442 ; Loss = 1.974077\n",
      "2024-12-05 10:35:47.437000: I training.py:192] Running evaluation for step 5000\n",
      "2024-12-05 10:37:02.037000: I training.py:192] Evaluation result for step 5000: loss = 0.822600 ; perplexity = 2.276411\n",
      "2024-12-05 10:38:02.325000: I runner.py:310] Step = 5100 ; steps/s = 1.66, tokens/s = 74573 (30503 source, 44070 target) ; Learning rate = 0.000451 ; Loss = 1.954528\n",
      "2024-12-05 10:39:03.142000: I runner.py:310] Step = 5200 ; steps/s = 1.64, tokens/s = 75239 (30768 source, 44471 target) ; Learning rate = 0.000460 ; Loss = 1.961628\n",
      "2024-12-05 10:40:04.111000: I runner.py:310] Step = 5300 ; steps/s = 1.64, tokens/s = 75042 (30673 source, 44369 target) ; Learning rate = 0.000469 ; Loss = 1.891389\n",
      "2024-12-05 10:41:04.626000: I runner.py:310] Step = 5400 ; steps/s = 1.65, tokens/s = 74306 (30402 source, 43904 target) ; Learning rate = 0.000477 ; Loss = 1.880181\n",
      "2024-12-05 10:42:05.572000: I runner.py:310] Step = 5500 ; steps/s = 1.64, tokens/s = 75053 (30678 source, 44375 target) ; Learning rate = 0.000486 ; Loss = 1.862060\n",
      "2024-12-05 10:43:06.123000: I runner.py:310] Step = 5600 ; steps/s = 1.65, tokens/s = 74274 (30400 source, 43874 target) ; Learning rate = 0.000495 ; Loss = 1.838743\n",
      "2024-12-05 10:44:07.041000: I runner.py:310] Step = 5700 ; steps/s = 1.64, tokens/s = 75079 (30684 source, 44395 target) ; Learning rate = 0.000504 ; Loss = 1.852861\n",
      "2024-12-05 10:45:07.978000: I runner.py:310] Step = 5800 ; steps/s = 1.64, tokens/s = 75102 (30710 source, 44392 target) ; Learning rate = 0.000513 ; Loss = 1.833949\n",
      "2024-12-05 10:46:08.377000: I runner.py:310] Step = 5900 ; steps/s = 1.66, tokens/s = 74441 (30459 source, 43982 target) ; Learning rate = 0.000522 ; Loss = 1.823710\n",
      "2024-12-05 10:47:09.321000: I runner.py:310] Step = 6000 ; steps/s = 1.64, tokens/s = 75083 (30700 source, 44383 target) ; Learning rate = 0.000530 ; Loss = 1.822231\n",
      "2024-12-05 10:48:09.838000: I runner.py:310] Step = 6100 ; steps/s = 1.65, tokens/s = 74301 (30407 source, 43894 target) ; Learning rate = 0.000539 ; Loss = 1.854431\n",
      "2024-12-05 10:49:10.707000: I runner.py:310] Step = 6200 ; steps/s = 1.64, tokens/s = 75166 (30726 source, 44440 target) ; Learning rate = 0.000548 ; Loss = 1.880109\n",
      "2024-12-05 10:50:11.658000: I runner.py:310] Step = 6300 ; steps/s = 1.64, tokens/s = 75052 (30680 source, 44372 target) ; Learning rate = 0.000557 ; Loss = 1.832615\n",
      "2024-12-05 10:51:12.117000: I runner.py:310] Step = 6400 ; steps/s = 1.65, tokens/s = 74368 (30427 source, 43941 target) ; Learning rate = 0.000566 ; Loss = 1.893510\n",
      "2024-12-05 10:52:13.006000: I runner.py:310] Step = 6500 ; steps/s = 1.64, tokens/s = 75143 (30720 source, 44423 target) ; Learning rate = 0.000575 ; Loss = 1.816408\n",
      "2024-12-05 10:53:13.446000: I runner.py:310] Step = 6600 ; steps/s = 1.65, tokens/s = 74403 (30447 source, 43956 target) ; Learning rate = 0.000583 ; Loss = 1.830813\n",
      "2024-12-05 10:54:14.338000: I runner.py:310] Step = 6700 ; steps/s = 1.64, tokens/s = 75112 (30694 source, 44418 target) ; Learning rate = 0.000592 ; Loss = 1.800300\n",
      "2024-12-05 10:55:15.192000: I runner.py:310] Step = 6800 ; steps/s = 1.64, tokens/s = 75188 (30746 source, 44442 target) ; Learning rate = 0.000601 ; Loss = 1.803111\n",
      "2024-12-05 10:56:15.542000: I runner.py:310] Step = 6900 ; steps/s = 1.66, tokens/s = 74514 (30492 source, 44022 target) ; Learning rate = 0.000610 ; Loss = 1.761875\n",
      "2024-12-05 10:57:16.328000: I runner.py:310] Step = 7000 ; steps/s = 1.65, tokens/s = 75262 (30767 source, 44495 target) ; Learning rate = 0.000619 ; Loss = 1.817791\n",
      "2024-12-05 10:58:16.764000: I runner.py:310] Step = 7100 ; steps/s = 1.65, tokens/s = 74399 (30445 source, 43954 target) ; Learning rate = 0.000628 ; Loss = 1.872272\n",
      "2024-12-05 10:59:17.519000: I runner.py:310] Step = 7200 ; steps/s = 1.65, tokens/s = 75317 (30790 source, 44527 target) ; Learning rate = 0.000636 ; Loss = 1.770855\n",
      "2024-12-05 11:00:18.378000: I runner.py:310] Step = 7300 ; steps/s = 1.64, tokens/s = 75169 (30730 source, 44439 target) ; Learning rate = 0.000645 ; Loss = 1.765282\n",
      "2024-12-05 11:01:18.755000: I runner.py:310] Step = 7400 ; steps/s = 1.66, tokens/s = 74461 (30463 source, 43998 target) ; Learning rate = 0.000654 ; Loss = 1.784244\n",
      "2024-12-05 11:02:19.616000: I runner.py:310] Step = 7500 ; steps/s = 1.64, tokens/s = 75193 (30757 source, 44436 target) ; Learning rate = 0.000663 ; Loss = 1.769660\n",
      "2024-12-05 11:03:20.002000: I runner.py:310] Step = 7600 ; steps/s = 1.66, tokens/s = 74472 (30468 source, 44004 target) ; Learning rate = 0.000672 ; Loss = 1.791674\n",
      "2024-12-05 11:04:20.855000: I runner.py:310] Step = 7700 ; steps/s = 1.64, tokens/s = 75163 (30720 source, 44443 target) ; Learning rate = 0.000681 ; Loss = 1.730518\n",
      "2024-12-05 11:05:21.704000: I runner.py:310] Step = 7800 ; steps/s = 1.64, tokens/s = 75182 (30729 source, 44453 target) ; Learning rate = 0.000690 ; Loss = 1.732481\n",
      "2024-12-05 11:06:22.073000: I runner.py:310] Step = 7900 ; steps/s = 1.66, tokens/s = 74486 (30478 source, 44008 target) ; Learning rate = 0.000698 ; Loss = 1.749598\n",
      "2024-12-05 11:07:22.935000: I runner.py:310] Step = 8000 ; steps/s = 1.64, tokens/s = 75166 (30729 source, 44437 target) ; Learning rate = 0.000707 ; Loss = 1.764717\n",
      "2024-12-05 11:08:23.445000: I runner.py:310] Step = 8100 ; steps/s = 1.65, tokens/s = 74721 (30584 source, 44137 target) ; Learning rate = 0.000716 ; Loss = 2.270423\n",
      "2024-12-05 11:09:24.093000: I runner.py:310] Step = 8200 ; steps/s = 1.65, tokens/s = 75039 (30673 source, 44366 target) ; Learning rate = 0.000725 ; Loss = 1.725681\n",
      "2024-12-05 11:10:24.958000: I runner.py:310] Step = 8300 ; steps/s = 1.64, tokens/s = 75157 (30719 source, 44438 target) ; Learning rate = 0.000734 ; Loss = 1.741409\n",
      "2024-12-05 11:11:25.334000: I runner.py:310] Step = 8400 ; steps/s = 1.66, tokens/s = 74478 (30474 source, 44004 target) ; Learning rate = 0.000743 ; Loss = 1.695757\n",
      "2024-12-05 11:12:26.214000: I runner.py:310] Step = 8500 ; steps/s = 1.64, tokens/s = 75162 (30737 source, 44425 target) ; Learning rate = 0.000751 ; Loss = 1.710710\n",
      "2024-12-05 11:13:27.047000: I runner.py:310] Step = 8600 ; steps/s = 1.64, tokens/s = 75200 (30739 source, 44461 target) ; Learning rate = 0.000760 ; Loss = 1.722596\n",
      "2024-12-05 11:14:27.449000: I runner.py:310] Step = 8700 ; steps/s = 1.66, tokens/s = 74445 (30461 source, 43984 target) ; Learning rate = 0.000769 ; Loss = 1.706269\n",
      "2024-12-05 11:15:28.249000: I runner.py:310] Step = 8800 ; steps/s = 1.64, tokens/s = 75234 (30753 source, 44481 target) ; Learning rate = 0.000778 ; Loss = 1.698018\n",
      "2024-12-05 11:16:28.655000: I runner.py:310] Step = 8900 ; steps/s = 1.66, tokens/s = 74456 (30468 source, 43988 target) ; Learning rate = 0.000787 ; Loss = 1.905741\n",
      "2024-12-05 11:17:29.512000: I runner.py:310] Step = 9000 ; steps/s = 1.64, tokens/s = 75161 (30720 source, 44441 target) ; Learning rate = 0.000796 ; Loss = 1.740139\n",
      "2024-12-05 11:18:30.327000: I runner.py:310] Step = 9100 ; steps/s = 1.64, tokens/s = 75246 (30771 source, 44475 target) ; Learning rate = 0.000804 ; Loss = 1.795983\n",
      "2024-12-05 11:19:30.718000: I runner.py:310] Step = 9200 ; steps/s = 1.66, tokens/s = 74447 (30457 source, 43990 target) ; Learning rate = 0.000813 ; Loss = 1.699384\n",
      "2024-12-05 11:20:31.559000: I runner.py:310] Step = 9300 ; steps/s = 1.64, tokens/s = 75211 (30756 source, 44455 target) ; Learning rate = 0.000822 ; Loss = 1.757561\n",
      "2024-12-05 11:21:32.030000: I runner.py:310] Step = 9400 ; steps/s = 1.65, tokens/s = 74349 (30416 source, 43933 target) ; Learning rate = 0.000831 ; Loss = 1.668862\n",
      "2024-12-05 11:22:32.835000: I runner.py:310] Step = 9500 ; steps/s = 1.64, tokens/s = 75246 (30763 source, 44483 target) ; Learning rate = 0.000840 ; Loss = 1.686236\n",
      "2024-12-05 11:23:33.673000: I runner.py:310] Step = 9600 ; steps/s = 1.64, tokens/s = 75198 (30743 source, 44455 target) ; Learning rate = 0.000849 ; Loss = 1.683676\n",
      "2024-12-05 11:24:34.081000: I runner.py:310] Step = 9700 ; steps/s = 1.66, tokens/s = 74439 (30455 source, 43984 target) ; Learning rate = 0.000857 ; Loss = 1.643992\n",
      "2024-12-05 11:25:34.880000: I runner.py:310] Step = 9800 ; steps/s = 1.64, tokens/s = 75269 (30780 source, 44489 target) ; Learning rate = 0.000866 ; Loss = 1.660129\n",
      "2024-12-05 11:26:35.304000: I runner.py:310] Step = 9900 ; steps/s = 1.66, tokens/s = 74380 (30427 source, 43953 target) ; Learning rate = 0.000875 ; Loss = 1.645617\n",
      "2024-12-05 11:27:36.146000: I runner.py:310] Step = 10000 ; steps/s = 1.64, tokens/s = 75192 (30733 source, 44459 target) ; Learning rate = 0.000884 ; Loss = 1.686059\n",
      "2024-12-05 11:27:37.728000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-10000\n",
      "2024-12-05 11:27:37.728000: I training.py:192] Running evaluation for step 10000\n",
      "2024-12-05 11:28:36.163000: I training.py:192] Evaluation result for step 10000: loss = 0.778830 ; perplexity = 2.178922\n",
      "2024-12-05 11:29:36.815000: I runner.py:310] Step = 10100 ; steps/s = 1.65, tokens/s = 75448 (30849 source, 44599 target) ; Learning rate = 0.000879 ; Loss = 1.677658\n",
      "2024-12-05 11:30:37.303000: I runner.py:310] Step = 10200 ; steps/s = 1.65, tokens/s = 74335 (30407 source, 43928 target) ; Learning rate = 0.000875 ; Loss = 1.655644\n",
      "2024-12-05 11:31:38.165000: I runner.py:310] Step = 10300 ; steps/s = 1.64, tokens/s = 75178 (30743 source, 44435 target) ; Learning rate = 0.000871 ; Loss = 1.644617\n",
      "2024-12-05 11:32:38.569000: I runner.py:310] Step = 10400 ; steps/s = 1.66, tokens/s = 74449 (30466 source, 43983 target) ; Learning rate = 0.000867 ; Loss = 1.631173\n",
      "2024-12-05 11:33:39.431000: I runner.py:310] Step = 10500 ; steps/s = 1.64, tokens/s = 75175 (30737 source, 44438 target) ; Learning rate = 0.000863 ; Loss = 1.632430\n",
      "2024-12-05 11:34:40.282000: I runner.py:310] Step = 10600 ; steps/s = 1.64, tokens/s = 75181 (30734 source, 44447 target) ; Learning rate = 0.000858 ; Loss = 1.659783\n",
      "2024-12-05 11:35:40.678000: I runner.py:310] Step = 10700 ; steps/s = 1.66, tokens/s = 74447 (30462 source, 43985 target) ; Learning rate = 0.000854 ; Loss = 1.634234\n",
      "2024-12-05 11:36:41.523000: I runner.py:310] Step = 10800 ; steps/s = 1.64, tokens/s = 75193 (30744 source, 44449 target) ; Learning rate = 0.000850 ; Loss = 1.645442\n",
      "2024-12-05 11:37:41.924000: I runner.py:310] Step = 10900 ; steps/s = 1.66, tokens/s = 74447 (30458 source, 43989 target) ; Learning rate = 0.000847 ; Loss = 1.643443\n",
      "2024-12-05 11:38:42.752000: I runner.py:310] Step = 11000 ; steps/s = 1.64, tokens/s = 75192 (30730 source, 44462 target) ; Learning rate = 0.000843 ; Loss = 1.631666\n",
      "2024-12-05 11:39:43.625000: I runner.py:310] Step = 11100 ; steps/s = 1.64, tokens/s = 75177 (30743 source, 44434 target) ; Learning rate = 0.000839 ; Loss = 1.641670\n",
      "2024-12-05 11:40:43.982000: I runner.py:310] Step = 11200 ; steps/s = 1.66, tokens/s = 74492 (30478 source, 44014 target) ; Learning rate = 0.000835 ; Loss = 1.629244\n",
      "2024-12-05 11:41:44.854000: I runner.py:310] Step = 11300 ; steps/s = 1.64, tokens/s = 75173 (30734 source, 44439 target) ; Learning rate = 0.000831 ; Loss = 1.664768\n",
      "2024-12-05 11:42:45.210000: I runner.py:310] Step = 11400 ; steps/s = 1.66, tokens/s = 74497 (30486 source, 44011 target) ; Learning rate = 0.000828 ; Loss = 1.615238\n",
      "2024-12-05 11:43:46.018000: I runner.py:310] Step = 11500 ; steps/s = 1.64, tokens/s = 75228 (30749 source, 44479 target) ; Learning rate = 0.000824 ; Loss = 1.616026\n",
      "2024-12-05 11:44:46.889000: I runner.py:310] Step = 11600 ; steps/s = 1.64, tokens/s = 75175 (30738 source, 44437 target) ; Learning rate = 0.000821 ; Loss = 1.643606\n",
      "2024-12-05 11:45:47.307000: I runner.py:310] Step = 11700 ; steps/s = 1.66, tokens/s = 74407 (30434 source, 43973 target) ; Learning rate = 0.000817 ; Loss = 1.600529\n",
      "2024-12-05 11:46:48.197000: I runner.py:310] Step = 11800 ; steps/s = 1.64, tokens/s = 75142 (30722 source, 44420 target) ; Learning rate = 0.000814 ; Loss = 1.621657\n",
      "2024-12-05 11:47:48.630000: I runner.py:310] Step = 11900 ; steps/s = 1.65, tokens/s = 74415 (30458 source, 43957 target) ; Learning rate = 0.000810 ; Loss = 1.608555\n",
      "2024-12-05 11:48:49.479000: I runner.py:310] Step = 12000 ; steps/s = 1.64, tokens/s = 75169 (30720 source, 44449 target) ; Learning rate = 0.000807 ; Loss = 1.603376\n",
      "2024-12-05 11:49:50.351000: I runner.py:310] Step = 12100 ; steps/s = 1.64, tokens/s = 75156 (30729 source, 44427 target) ; Learning rate = 0.000803 ; Loss = 1.606790\n",
      "2024-12-05 11:50:50.672000: I runner.py:310] Step = 12200 ; steps/s = 1.66, tokens/s = 74544 (30499 source, 44045 target) ; Learning rate = 0.000800 ; Loss = 1.600404\n",
      "2024-12-05 11:51:51.502000: I runner.py:310] Step = 12300 ; steps/s = 1.64, tokens/s = 75194 (30723 source, 44471 target) ; Learning rate = 0.000797 ; Loss = 1.610712\n",
      "2024-12-05 11:52:52.438000: I runner.py:310] Step = 12400 ; steps/s = 1.64, tokens/s = 75103 (30722 source, 44381 target) ; Learning rate = 0.000794 ; Loss = 1.608120\n",
      "2024-12-05 11:53:52.769000: I runner.py:310] Step = 12500 ; steps/s = 1.66, tokens/s = 74532 (30501 source, 44031 target) ; Learning rate = 0.000791 ; Loss = 1.582473\n",
      "2024-12-05 11:54:53.654000: I runner.py:310] Step = 12600 ; steps/s = 1.64, tokens/s = 75151 (30725 source, 44426 target) ; Learning rate = 0.000787 ; Loss = 1.599786\n",
      "2024-12-05 11:55:53.981000: I runner.py:310] Step = 12700 ; steps/s = 1.66, tokens/s = 74530 (30487 source, 44043 target) ; Learning rate = 0.000784 ; Loss = 1.605117\n",
      "2024-12-05 11:56:54.839000: I runner.py:310] Step = 12800 ; steps/s = 1.64, tokens/s = 75163 (30724 source, 44439 target) ; Learning rate = 0.000781 ; Loss = 1.592398\n",
      "2024-12-05 11:57:55.753000: I runner.py:310] Step = 12900 ; steps/s = 1.64, tokens/s = 75115 (30711 source, 44404 target) ; Learning rate = 0.000778 ; Loss = 1.607375\n",
      "2024-12-05 11:58:56.193000: I runner.py:310] Step = 13000 ; steps/s = 1.65, tokens/s = 74412 (30447 source, 43965 target) ; Learning rate = 0.000775 ; Loss = 1.587413\n",
      "2024-12-05 11:59:57.030000: I runner.py:310] Step = 13100 ; steps/s = 1.64, tokens/s = 75192 (30743 source, 44449 target) ; Learning rate = 0.000772 ; Loss = 1.587238\n",
      "2024-12-05 12:00:57.499000: I runner.py:310] Step = 13200 ; steps/s = 1.65, tokens/s = 74360 (30427 source, 43933 target) ; Learning rate = 0.000769 ; Loss = 1.561499\n",
      "2024-12-05 12:01:58.339000: I runner.py:310] Step = 13300 ; steps/s = 1.64, tokens/s = 75180 (30721 source, 44459 target) ; Learning rate = 0.000766 ; Loss = 1.586025\n",
      "2024-12-05 12:02:59.200000: I runner.py:310] Step = 13400 ; steps/s = 1.64, tokens/s = 75181 (30743 source, 44438 target) ; Learning rate = 0.000764 ; Loss = 1.582875\n",
      "2024-12-05 12:03:59.514000: I runner.py:310] Step = 13500 ; steps/s = 1.66, tokens/s = 74540 (30497 source, 44043 target) ; Learning rate = 0.000761 ; Loss = 1.569360\n",
      "2024-12-05 12:05:00.440000: I runner.py:310] Step = 13600 ; steps/s = 1.64, tokens/s = 75101 (30702 source, 44399 target) ; Learning rate = 0.000758 ; Loss = 1.578850\n",
      "2024-12-05 12:06:00.817000: I runner.py:310] Step = 13700 ; steps/s = 1.66, tokens/s = 74488 (30487 source, 44001 target) ; Learning rate = 0.000755 ; Loss = 1.565690\n",
      "2024-12-05 12:07:01.681000: I runner.py:310] Step = 13800 ; steps/s = 1.64, tokens/s = 75156 (30718 source, 44438 target) ; Learning rate = 0.000752 ; Loss = 1.567428\n",
      "2024-12-05 12:08:02.539000: I runner.py:310] Step = 13900 ; steps/s = 1.64, tokens/s = 75183 (30740 source, 44443 target) ; Learning rate = 0.000750 ; Loss = 1.585364\n",
      "2024-12-05 12:09:02.942000: I runner.py:310] Step = 14000 ; steps/s = 1.66, tokens/s = 74446 (30466 source, 43980 target) ; Learning rate = 0.000747 ; Loss = 1.560924\n",
      "2024-12-05 12:10:03.769000: I runner.py:310] Step = 14100 ; steps/s = 1.64, tokens/s = 75215 (30751 source, 44464 target) ; Learning rate = 0.000744 ; Loss = 1.570225\n",
      "2024-12-05 12:11:04.140000: I runner.py:310] Step = 14200 ; steps/s = 1.66, tokens/s = 74479 (30469 source, 44010 target) ; Learning rate = 0.000742 ; Loss = 1.558416\n",
      "2024-12-05 12:12:04.995000: I runner.py:310] Step = 14300 ; steps/s = 1.64, tokens/s = 75171 (30731 source, 44440 target) ; Learning rate = 0.000739 ; Loss = 1.552871\n",
      "2024-12-05 12:13:05.840000: I runner.py:310] Step = 14400 ; steps/s = 1.64, tokens/s = 75209 (30754 source, 44455 target) ; Learning rate = 0.000737 ; Loss = 1.571238\n",
      "2024-12-05 12:14:06.261000: I runner.py:310] Step = 14500 ; steps/s = 1.66, tokens/s = 74397 (30429 source, 43968 target) ; Learning rate = 0.000734 ; Loss = 1.555359\n",
      "2024-12-05 12:15:07.090000: I runner.py:310] Step = 14600 ; steps/s = 1.64, tokens/s = 75216 (30747 source, 44469 target) ; Learning rate = 0.000731 ; Loss = 1.559523\n",
      "2024-12-05 12:16:07.517000: I runner.py:310] Step = 14700 ; steps/s = 1.66, tokens/s = 74430 (30468 source, 43962 target) ; Learning rate = 0.000729 ; Loss = 1.539052\n",
      "2024-12-05 12:17:08.348000: I runner.py:310] Step = 14800 ; steps/s = 1.64, tokens/s = 75205 (30735 source, 44470 target) ; Learning rate = 0.000727 ; Loss = 1.559556\n",
      "2024-12-05 12:18:09.209000: I runner.py:310] Step = 14900 ; steps/s = 1.64, tokens/s = 75182 (30747 source, 44435 target) ; Learning rate = 0.000724 ; Loss = 1.553992\n",
      "2024-12-05 12:19:09.628000: I runner.py:310] Step = 15000 ; steps/s = 1.66, tokens/s = 74404 (30435 source, 43969 target) ; Learning rate = 0.000722 ; Loss = 1.537902\n",
      "2024-12-05 12:19:09.630000: I training.py:192] Running evaluation for step 15000\n",
      "2024-12-05 12:20:10.862000: I training.py:192] Evaluation result for step 15000: loss = 0.805745 ; perplexity = 2.238364\n",
      "2024-12-05 12:21:11.507000: I runner.py:310] Step = 15100 ; steps/s = 1.65, tokens/s = 75446 (30838 source, 44608 target) ; Learning rate = 0.000719 ; Loss = 1.550600\n",
      "2024-12-05 12:22:11.885000: I runner.py:310] Step = 15200 ; steps/s = 1.66, tokens/s = 74477 (30481 source, 43996 target) ; Learning rate = 0.000717 ; Loss = 1.543598\n",
      "2024-12-05 12:23:12.738000: I runner.py:310] Step = 15300 ; steps/s = 1.64, tokens/s = 75180 (30736 source, 44444 target) ; Learning rate = 0.000715 ; Loss = 1.544694\n",
      "2024-12-05 12:24:13.614000: I runner.py:310] Step = 15400 ; steps/s = 1.64, tokens/s = 75158 (30724 source, 44434 target) ; Learning rate = 0.000712 ; Loss = 1.555876\n",
      "2024-12-05 12:25:14.012000: I runner.py:310] Step = 15500 ; steps/s = 1.66, tokens/s = 74431 (30447 source, 43984 target) ; Learning rate = 0.000710 ; Loss = 1.544977\n",
      "2024-12-05 12:26:14.879000: I runner.py:310] Step = 15600 ; steps/s = 1.64, tokens/s = 75180 (30743 source, 44437 target) ; Learning rate = 0.000708 ; Loss = 1.558298\n",
      "2024-12-05 12:27:15.336000: I runner.py:310] Step = 15700 ; steps/s = 1.65, tokens/s = 74391 (30447 source, 43944 target) ; Learning rate = 0.000705 ; Loss = 1.545775\n",
      "2024-12-05 12:28:16.175000: I runner.py:310] Step = 15800 ; steps/s = 1.64, tokens/s = 75177 (30726 source, 44451 target) ; Learning rate = 0.000703 ; Loss = 1.527575\n",
      "2024-12-05 12:29:17.085000: I runner.py:310] Step = 15900 ; steps/s = 1.64, tokens/s = 75109 (30707 source, 44402 target) ; Learning rate = 0.000701 ; Loss = 1.551536\n",
      "2024-12-05 12:30:17.461000: I runner.py:310] Step = 16000 ; steps/s = 1.66, tokens/s = 74459 (30458 source, 44001 target) ; Learning rate = 0.000699 ; Loss = 1.546225\n",
      "2024-12-05 12:31:18.356000: I runner.py:310] Step = 16100 ; steps/s = 1.64, tokens/s = 75144 (30721 source, 44423 target) ; Learning rate = 0.000697 ; Loss = 1.542165\n",
      "2024-12-05 12:32:19.106000: I runner.py:310] Step = 16200 ; steps/s = 1.65, tokens/s = 74729 (30584 source, 44145 target) ; Learning rate = 0.000694 ; Loss = 1.643537\n",
      "2024-12-05 12:33:19.679000: I runner.py:310] Step = 16300 ; steps/s = 1.65, tokens/s = 74808 (30572 source, 44236 target) ; Learning rate = 0.000692 ; Loss = 1.531524\n",
      "2024-12-05 12:34:20.507000: I runner.py:310] Step = 16400 ; steps/s = 1.64, tokens/s = 75232 (30772 source, 44460 target) ; Learning rate = 0.000690 ; Loss = 1.541566\n",
      "2024-12-05 12:35:20.972000: I runner.py:310] Step = 16500 ; steps/s = 1.65, tokens/s = 74376 (30434 source, 43942 target) ; Learning rate = 0.000688 ; Loss = 1.529787\n",
      "2024-12-05 12:36:21.850000: I runner.py:310] Step = 16600 ; steps/s = 1.64, tokens/s = 75126 (30701 source, 44425 target) ; Learning rate = 0.000686 ; Loss = 1.530492\n",
      "2024-12-05 12:37:22.753000: I runner.py:310] Step = 16700 ; steps/s = 1.64, tokens/s = 75131 (30719 source, 44412 target) ; Learning rate = 0.000684 ; Loss = 1.539756\n",
      "2024-12-05 12:38:23.132000: I runner.py:310] Step = 16800 ; steps/s = 1.66, tokens/s = 74483 (30479 source, 44004 target) ; Learning rate = 0.000682 ; Loss = 1.525641\n",
      "2024-12-05 12:39:23.914000: I runner.py:310] Step = 16900 ; steps/s = 1.65, tokens/s = 75280 (30779 source, 44501 target) ; Learning rate = 0.000680 ; Loss = 1.536951\n",
      "2024-12-05 12:40:24.361000: I runner.py:310] Step = 17000 ; steps/s = 1.65, tokens/s = 74386 (30439 source, 43947 target) ; Learning rate = 0.000678 ; Loss = 1.514011\n",
      "2024-12-05 12:41:25.234000: I runner.py:310] Step = 17100 ; steps/s = 1.64, tokens/s = 75148 (30718 source, 44430 target) ; Learning rate = 0.000676 ; Loss = 1.540151\n",
      "2024-12-05 12:42:26.118000: I runner.py:310] Step = 17200 ; steps/s = 1.64, tokens/s = 75143 (30721 source, 44422 target) ; Learning rate = 0.000674 ; Loss = 1.540019\n",
      "2024-12-05 12:43:26.553000: I runner.py:310] Step = 17300 ; steps/s = 1.65, tokens/s = 74406 (30451 source, 43955 target) ; Learning rate = 0.000672 ; Loss = 1.518381\n",
      "2024-12-05 12:44:27.411000: I runner.py:310] Step = 17400 ; steps/s = 1.64, tokens/s = 75185 (30738 source, 44447 target) ; Learning rate = 0.000670 ; Loss = 1.530599\n",
      "2024-12-05 12:45:27.852000: I runner.py:310] Step = 17500 ; steps/s = 1.65, tokens/s = 74386 (30432 source, 43954 target) ; Learning rate = 0.000668 ; Loss = 1.517922\n",
      "2024-12-05 12:46:28.741000: I runner.py:310] Step = 17600 ; steps/s = 1.64, tokens/s = 75128 (30706 source, 44422 target) ; Learning rate = 0.000666 ; Loss = 1.526479\n",
      "2024-12-05 12:47:29.639000: I runner.py:310] Step = 17700 ; steps/s = 1.64, tokens/s = 75132 (30715 source, 44417 target) ; Learning rate = 0.000664 ; Loss = 1.526330\n",
      "2024-12-05 12:48:30.039000: I runner.py:310] Step = 17800 ; steps/s = 1.66, tokens/s = 74435 (30456 source, 43979 target) ; Learning rate = 0.000662 ; Loss = 1.528592\n",
      "2024-12-05 12:49:30.933000: I runner.py:310] Step = 17900 ; steps/s = 1.64, tokens/s = 75137 (30718 source, 44419 target) ; Learning rate = 0.000661 ; Loss = 1.522275\n",
      "2024-12-05 12:50:31.315000: I runner.py:310] Step = 18000 ; steps/s = 1.66, tokens/s = 74464 (30469 source, 43995 target) ; Learning rate = 0.000659 ; Loss = 1.515549\n",
      "2024-12-05 12:51:32.173000: I runner.py:310] Step = 18100 ; steps/s = 1.64, tokens/s = 75164 (30723 source, 44441 target) ; Learning rate = 0.000657 ; Loss = 1.519092\n",
      "2024-12-05 12:52:33.059000: I runner.py:310] Step = 18200 ; steps/s = 1.64, tokens/s = 75160 (30732 source, 44428 target) ; Learning rate = 0.000655 ; Loss = 1.526558\n",
      "2024-12-05 12:53:33.401000: I runner.py:310] Step = 18300 ; steps/s = 1.66, tokens/s = 74508 (30477 source, 44031 target) ; Learning rate = 0.000653 ; Loss = 1.516209\n",
      "2024-12-05 12:54:34.213000: I runner.py:310] Step = 18400 ; steps/s = 1.64, tokens/s = 75224 (30750 source, 44474 target) ; Learning rate = 0.000652 ; Loss = 1.521448\n",
      "2024-12-05 12:55:34.675000: I runner.py:310] Step = 18500 ; steps/s = 1.65, tokens/s = 74404 (30464 source, 43940 target) ; Learning rate = 0.000650 ; Loss = 1.508388\n",
      "2024-12-05 12:56:35.536000: I runner.py:310] Step = 18600 ; steps/s = 1.64, tokens/s = 75159 (30720 source, 44439 target) ; Learning rate = 0.000648 ; Loss = 1.522319\n",
      "2024-12-05 12:57:36.403000: I runner.py:310] Step = 18700 ; steps/s = 1.64, tokens/s = 75181 (30743 source, 44438 target) ; Learning rate = 0.000646 ; Loss = 1.525423\n",
      "2024-12-05 12:58:36.846000: I runner.py:310] Step = 18800 ; steps/s = 1.65, tokens/s = 74386 (30427 source, 43959 target) ; Learning rate = 0.000645 ; Loss = 1.513327\n",
      "2024-12-05 12:59:37.714000: I runner.py:310] Step = 18900 ; steps/s = 1.64, tokens/s = 75179 (30746 source, 44433 target) ; Learning rate = 0.000643 ; Loss = 1.507604\n",
      "2024-12-05 13:00:38.146000: I runner.py:310] Step = 19000 ; steps/s = 1.65, tokens/s = 74402 (30441 source, 43961 target) ; Learning rate = 0.000641 ; Loss = 1.509284\n",
      "2024-12-05 13:01:39.014000: I runner.py:310] Step = 19100 ; steps/s = 1.64, tokens/s = 75164 (30727 source, 44437 target) ; Learning rate = 0.000640 ; Loss = 1.513181\n",
      "2024-12-05 13:02:39.944000: I runner.py:310] Step = 19200 ; steps/s = 1.64, tokens/s = 75067 (30681 source, 44386 target) ; Learning rate = 0.000638 ; Loss = 1.516065\n",
      "2024-12-05 13:03:40.320000: I runner.py:310] Step = 19300 ; steps/s = 1.66, tokens/s = 74468 (30466 source, 44002 target) ; Learning rate = 0.000636 ; Loss = 1.509626\n",
      "2024-12-05 13:04:41.878000: I runner.py:310] Step = 19400 ; steps/s = 1.62, tokens/s = 74333 (30396 source, 43937 target) ; Learning rate = 0.000635 ; Loss = 1.504378\n",
      "2024-12-05 13:05:42.917000: I runner.py:310] Step = 19500 ; steps/s = 1.64, tokens/s = 73678 (30155 source, 43523 target) ; Learning rate = 0.000633 ; Loss = 1.499873\n",
      "2024-12-05 13:06:43.781000: I runner.py:310] Step = 19600 ; steps/s = 1.64, tokens/s = 75179 (30735 source, 44444 target) ; Learning rate = 0.000631 ; Loss = 1.500053\n",
      "2024-12-05 13:07:44.656000: I runner.py:310] Step = 19700 ; steps/s = 1.64, tokens/s = 75148 (30717 source, 44431 target) ; Learning rate = 0.000630 ; Loss = 1.519314\n",
      "2024-12-05 13:08:45.087000: I runner.py:310] Step = 19800 ; steps/s = 1.65, tokens/s = 74386 (30431 source, 43955 target) ; Learning rate = 0.000628 ; Loss = 1.495851\n",
      "2024-12-05 13:09:45.973000: I runner.py:310] Step = 19900 ; steps/s = 1.64, tokens/s = 75144 (30721 source, 44423 target) ; Learning rate = 0.000627 ; Loss = 1.516760\n",
      "2024-12-05 13:10:46.452000: I runner.py:310] Step = 20000 ; steps/s = 1.65, tokens/s = 74371 (30438 source, 43933 target) ; Learning rate = 0.000625 ; Loss = 1.499870\n",
      "2024-12-05 13:10:48.094000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-20000\n",
      "2024-12-05 13:10:48.094000: I training.py:192] Running evaluation for step 20000\n",
      "2024-12-05 13:11:46.025000: I training.py:192] Evaluation result for step 20000: loss = 0.832092 ; perplexity = 2.298123\n",
      "2024-12-05 13:12:46.814000: I runner.py:310] Step = 20100 ; steps/s = 1.65, tokens/s = 75307 (30789 source, 44518 target) ; Learning rate = 0.000623 ; Loss = 1.505892\n",
      "2024-12-05 13:13:47.642000: I runner.py:310] Step = 20200 ; steps/s = 1.64, tokens/s = 75202 (30739 source, 44463 target) ; Learning rate = 0.000622 ; Loss = 1.510137\n",
      "2024-12-05 13:14:48.069000: I runner.py:310] Step = 20300 ; steps/s = 1.66, tokens/s = 74403 (30433 source, 43970 target) ; Learning rate = 0.000620 ; Loss = 1.498708\n",
      "2024-12-05 13:15:48.948000: I runner.py:310] Step = 20400 ; steps/s = 1.64, tokens/s = 75153 (30726 source, 44427 target) ; Learning rate = 0.000619 ; Loss = 1.507936\n",
      "2024-12-05 13:16:49.888000: I runner.py:310] Step = 20500 ; steps/s = 1.64, tokens/s = 75081 (30699 source, 44382 target) ; Learning rate = 0.000617 ; Loss = 1.505325\n",
      "2024-12-05 13:17:50.336000: I runner.py:310] Step = 20600 ; steps/s = 1.65, tokens/s = 74372 (30423 source, 43949 target) ; Learning rate = 0.000616 ; Loss = 1.494378\n",
      "2024-12-05 13:18:51.219000: I runner.py:310] Step = 20700 ; steps/s = 1.64, tokens/s = 75153 (30726 source, 44427 target) ; Learning rate = 0.000614 ; Loss = 1.504527\n",
      "2024-12-05 13:19:51.628000: I runner.py:310] Step = 20800 ; steps/s = 1.66, tokens/s = 74432 (30462 source, 43970 target) ; Learning rate = 0.000613 ; Loss = 1.487514\n",
      "2024-12-05 13:20:52.535000: I runner.py:310] Step = 20900 ; steps/s = 1.64, tokens/s = 75113 (30698 source, 44415 target) ; Learning rate = 0.000611 ; Loss = 1.503235\n",
      "2024-12-05 13:21:53.400000: I runner.py:310] Step = 21000 ; steps/s = 1.64, tokens/s = 75184 (30747 source, 44437 target) ; Learning rate = 0.000610 ; Loss = 1.505707\n",
      "2024-12-05 13:22:53.785000: I runner.py:310] Step = 21100 ; steps/s = 1.66, tokens/s = 74451 (30459 source, 43992 target) ; Learning rate = 0.000608 ; Loss = 1.491579\n",
      "2024-12-05 13:23:54.654000: I runner.py:310] Step = 21200 ; steps/s = 1.64, tokens/s = 75161 (30720 source, 44441 target) ; Learning rate = 0.000607 ; Loss = 1.496441\n",
      "2024-12-05 13:24:54.989000: I runner.py:310] Step = 21300 ; steps/s = 1.66, tokens/s = 74525 (30501 source, 44024 target) ; Learning rate = 0.000606 ; Loss = 1.489034\n",
      "2024-12-05 13:25:55.882000: I runner.py:310] Step = 21400 ; steps/s = 1.64, tokens/s = 75155 (30740 source, 44415 target) ; Learning rate = 0.000604 ; Loss = 1.500129\n",
      "2024-12-05 13:26:56.699000: I runner.py:310] Step = 21500 ; steps/s = 1.64, tokens/s = 75211 (30740 source, 44471 target) ; Learning rate = 0.000603 ; Loss = 1.503761\n",
      "2024-12-05 13:27:57.091000: I runner.py:310] Step = 21600 ; steps/s = 1.66, tokens/s = 74461 (30468 source, 43993 target) ; Learning rate = 0.000601 ; Loss = 1.493683\n",
      "2024-12-05 13:28:57.921000: I runner.py:310] Step = 21700 ; steps/s = 1.64, tokens/s = 75206 (30743 source, 44463 target) ; Learning rate = 0.000600 ; Loss = 1.499540\n",
      "2024-12-05 13:29:58.301000: I runner.py:310] Step = 21800 ; steps/s = 1.66, tokens/s = 74473 (30464 source, 44009 target) ; Learning rate = 0.000599 ; Loss = 1.488667\n",
      "2024-12-05 13:30:59.236000: I runner.py:310] Step = 21900 ; steps/s = 1.64, tokens/s = 75066 (30687 source, 44379 target) ; Learning rate = 0.000597 ; Loss = 1.493054\n",
      "2024-12-05 13:32:00.039000: I runner.py:310] Step = 22000 ; steps/s = 1.64, tokens/s = 75260 (30773 source, 44487 target) ; Learning rate = 0.000596 ; Loss = 1.495613\n",
      "2024-12-05 13:33:00.439000: I runner.py:310] Step = 22100 ; steps/s = 1.66, tokens/s = 74429 (30451 source, 43978 target) ; Learning rate = 0.000595 ; Loss = 1.488886\n",
      "2024-12-05 13:34:01.346000: I runner.py:310] Step = 22200 ; steps/s = 1.64, tokens/s = 75125 (30720 source, 44405 target) ; Learning rate = 0.000593 ; Loss = 1.500259\n",
      "2024-12-05 13:35:01.778000: I runner.py:310] Step = 22300 ; steps/s = 1.65, tokens/s = 74408 (30445 source, 43963 target) ; Learning rate = 0.000592 ; Loss = 1.483435\n",
      "2024-12-05 13:36:02.661000: I runner.py:310] Step = 22400 ; steps/s = 1.64, tokens/s = 75121 (30700 source, 44421 target) ; Learning rate = 0.000591 ; Loss = 1.492490\n",
      "2024-12-05 13:37:03.554000: I runner.py:310] Step = 22500 ; steps/s = 1.64, tokens/s = 75156 (30737 source, 44419 target) ; Learning rate = 0.000589 ; Loss = 1.493132\n",
      "2024-12-05 13:38:03.973000: I runner.py:310] Step = 22600 ; steps/s = 1.66, tokens/s = 74401 (30437 source, 43964 target) ; Learning rate = 0.000588 ; Loss = 1.492262\n",
      "2024-12-05 13:39:04.935000: I runner.py:310] Step = 22700 ; steps/s = 1.64, tokens/s = 75068 (30695 source, 44373 target) ; Learning rate = 0.000587 ; Loss = 1.497470\n",
      "2024-12-05 13:40:05.403000: I runner.py:310] Step = 22800 ; steps/s = 1.65, tokens/s = 74368 (30434 source, 43934 target) ; Learning rate = 0.000585 ; Loss = 1.485128\n",
      "2024-12-05 13:41:06.224000: I runner.py:310] Step = 22900 ; steps/s = 1.64, tokens/s = 75212 (30737 source, 44475 target) ; Learning rate = 0.000584 ; Loss = 1.488686\n",
      "2024-12-05 13:42:07.118000: I runner.py:310] Step = 23000 ; steps/s = 1.64, tokens/s = 75131 (30718 source, 44413 target) ; Learning rate = 0.000583 ; Loss = 1.491080\n",
      "2024-12-05 13:43:07.540000: I runner.py:310] Step = 23100 ; steps/s = 1.66, tokens/s = 74399 (30431 source, 43968 target) ; Learning rate = 0.000582 ; Loss = 1.484898\n",
      "2024-12-05 13:44:08.389000: I runner.py:310] Step = 23200 ; steps/s = 1.64, tokens/s = 75207 (30755 source, 44452 target) ; Learning rate = 0.000580 ; Loss = 1.487776\n",
      "2024-12-05 13:45:08.845000: I runner.py:310] Step = 23300 ; steps/s = 1.65, tokens/s = 74387 (30443 source, 43944 target) ; Learning rate = 0.000579 ; Loss = 1.485649\n",
      "2024-12-05 13:46:09.770000: I runner.py:310] Step = 23400 ; steps/s = 1.64, tokens/s = 75076 (30685 source, 44391 target) ; Learning rate = 0.000578 ; Loss = 1.484287\n",
      "2024-12-05 13:47:10.698000: I runner.py:310] Step = 23500 ; steps/s = 1.64, tokens/s = 75105 (30716 source, 44389 target) ; Learning rate = 0.000577 ; Loss = 1.495977\n",
      "2024-12-05 13:48:11.091000: I runner.py:310] Step = 23600 ; steps/s = 1.66, tokens/s = 74445 (30456 source, 43989 target) ; Learning rate = 0.000575 ; Loss = 1.492380\n",
      "2024-12-05 13:49:12.075000: I runner.py:310] Step = 23700 ; steps/s = 1.64, tokens/s = 75029 (30683 source, 44346 target) ; Learning rate = 0.000574 ; Loss = 1.494747\n",
      "2024-12-05 13:50:12.572000: I runner.py:310] Step = 23800 ; steps/s = 1.65, tokens/s = 74320 (30399 source, 43921 target) ; Learning rate = 0.000573 ; Loss = 1.470001\n",
      "2024-12-05 13:51:13.445000: I runner.py:310] Step = 23900 ; steps/s = 1.64, tokens/s = 75152 (30721 source, 44431 target) ; Learning rate = 0.000572 ; Loss = 1.477760\n",
      "2024-12-05 13:52:14.228000: I runner.py:310] Step = 24000 ; steps/s = 1.65, tokens/s = 75276 (30778 source, 44498 target) ; Learning rate = 0.000571 ; Loss = 1.490704\n",
      "2024-12-05 13:53:14.634000: I runner.py:310] Step = 24100 ; steps/s = 1.66, tokens/s = 74428 (30445 source, 43983 target) ; Learning rate = 0.000569 ; Loss = 1.476465\n",
      "2024-12-05 13:54:15.418000: I runner.py:310] Step = 24200 ; steps/s = 1.65, tokens/s = 75270 (30774 source, 44496 target) ; Learning rate = 0.000568 ; Loss = 1.489038\n",
      "2024-12-05 13:55:16.188000: I runner.py:310] Step = 24300 ; steps/s = 1.65, tokens/s = 74841 (30608 source, 44233 target) ; Learning rate = 0.000567 ; Loss = 1.510502\n",
      "2024-12-05 13:56:16.725000: I runner.py:310] Step = 24400 ; steps/s = 1.65, tokens/s = 74709 (30552 source, 44157 target) ; Learning rate = 0.000566 ; Loss = 1.482418\n",
      "2024-12-05 13:57:17.630000: I runner.py:310] Step = 24500 ; steps/s = 1.64, tokens/s = 75110 (30701 source, 44409 target) ; Learning rate = 0.000565 ; Loss = 1.482252\n",
      "2024-12-05 13:58:18.098000: I runner.py:310] Step = 24600 ; steps/s = 1.65, tokens/s = 74370 (30436 source, 43934 target) ; Learning rate = 0.000564 ; Loss = 1.473842\n",
      "2024-12-05 13:59:18.982000: I runner.py:310] Step = 24700 ; steps/s = 1.64, tokens/s = 75151 (30728 source, 44423 target) ; Learning rate = 0.000562 ; Loss = 1.482455\n",
      "2024-12-05 14:00:19.860000: I runner.py:310] Step = 24800 ; steps/s = 1.64, tokens/s = 75158 (30728 source, 44430 target) ; Learning rate = 0.000561 ; Loss = 1.487685\n",
      "2024-12-05 14:01:20.343000: I runner.py:310] Step = 24900 ; steps/s = 1.65, tokens/s = 74325 (30399 source, 43926 target) ; Learning rate = 0.000560 ; Loss = 1.477053\n",
      "2024-12-05 14:02:21.233000: I runner.py:310] Step = 25000 ; steps/s = 1.64, tokens/s = 75141 (30721 source, 44420 target) ; Learning rate = 0.000559 ; Loss = 1.480405\n",
      "2024-12-05 14:02:21.235000: I training.py:192] Running evaluation for step 25000\n",
      "2024-12-05 14:03:13.108000: I training.py:192] Evaluation result for step 25000: loss = 0.854156 ; perplexity = 2.349390\n",
      "2024-12-05 14:04:13.404000: I runner.py:310] Step = 25100 ; steps/s = 1.66, tokens/s = 74582 (30512 source, 44070 target) ; Learning rate = 0.000558 ; Loss = 1.487159\n",
      "2024-12-05 14:05:14.220000: I runner.py:310] Step = 25200 ; steps/s = 1.64, tokens/s = 75226 (30760 source, 44466 target) ; Learning rate = 0.000557 ; Loss = 1.482204\n",
      "2024-12-05 14:06:15.063000: I runner.py:310] Step = 25300 ; steps/s = 1.64, tokens/s = 75212 (30752 source, 44460 target) ; Learning rate = 0.000556 ; Loss = 1.477447\n",
      "2024-12-05 14:07:15.484000: I runner.py:310] Step = 25400 ; steps/s = 1.66, tokens/s = 74419 (30450 source, 43969 target) ; Learning rate = 0.000555 ; Loss = 1.471708\n",
      "2024-12-05 14:08:16.374000: I runner.py:310] Step = 25500 ; steps/s = 1.64, tokens/s = 75136 (30717 source, 44419 target) ; Learning rate = 0.000553 ; Loss = 1.478732\n",
      "2024-12-05 14:09:16.799000: I runner.py:310] Step = 25600 ; steps/s = 1.66, tokens/s = 74423 (30456 source, 43967 target) ; Learning rate = 0.000552 ; Loss = 1.483312\n",
      "2024-12-05 14:10:17.676000: I runner.py:310] Step = 25700 ; steps/s = 1.64, tokens/s = 75150 (30720 source, 44430 target) ; Learning rate = 0.000551 ; Loss = 1.479854\n",
      "2024-12-05 14:11:18.534000: I runner.py:310] Step = 25800 ; steps/s = 1.64, tokens/s = 75191 (30746 source, 44445 target) ; Learning rate = 0.000550 ; Loss = 1.478000\n",
      "2024-12-05 14:12:18.895000: I runner.py:310] Step = 25900 ; steps/s = 1.66, tokens/s = 74462 (30455 source, 44007 target) ; Learning rate = 0.000549 ; Loss = 1.481723\n",
      "2024-12-05 14:13:19.735000: I runner.py:310] Step = 26000 ; steps/s = 1.64, tokens/s = 75219 (30759 source, 44460 target) ; Learning rate = 0.000548 ; Loss = 1.480156\n",
      "2024-12-05 14:14:20.153000: I runner.py:310] Step = 26100 ; steps/s = 1.66, tokens/s = 74428 (30453 source, 43975 target) ; Learning rate = 0.000547 ; Loss = 1.476070\n",
      "2024-12-05 14:15:21.024000: I runner.py:310] Step = 26200 ; steps/s = 1.64, tokens/s = 75145 (30719 source, 44426 target) ; Learning rate = 0.000546 ; Loss = 1.477977\n",
      "2024-12-05 14:16:21.904000: I runner.py:310] Step = 26300 ; steps/s = 1.64, tokens/s = 75168 (30740 source, 44428 target) ; Learning rate = 0.000545 ; Loss = 1.478250\n",
      "2024-12-05 14:17:23.072000: I runner.py:310] Step = 26400 ; steps/s = 1.64, tokens/s = 73492 (30056 source, 43436 target) ; Learning rate = 0.000544 ; Loss = 1.467951\n",
      "2024-12-05 14:18:23.882000: I runner.py:310] Step = 26500 ; steps/s = 1.64, tokens/s = 75253 (30773 source, 44480 target) ; Learning rate = 0.000543 ; Loss = 1.478086\n",
      "2024-12-05 14:19:25.226000: I runner.py:310] Step = 26600 ; steps/s = 1.63, tokens/s = 73286 (29986 source, 43300 target) ; Learning rate = 0.000542 ; Loss = 1.468608\n",
      "2024-12-05 14:20:27.717000: I runner.py:310] Step = 26700 ; steps/s = 1.60, tokens/s = 73196 (29914 source, 43282 target) ; Learning rate = 0.000541 ; Loss = 1.475717\n",
      "2024-12-05 14:21:28.654000: I runner.py:310] Step = 26800 ; steps/s = 1.64, tokens/s = 75102 (30717 source, 44385 target) ; Learning rate = 0.000540 ; Loss = 1.479108\n",
      "2024-12-05 14:22:29.122000: I runner.py:310] Step = 26900 ; steps/s = 1.65, tokens/s = 74335 (30402 source, 43933 target) ; Learning rate = 0.000539 ; Loss = 1.473227\n",
      "2024-12-05 14:23:30.063000: I runner.py:310] Step = 27000 ; steps/s = 1.64, tokens/s = 75094 (30704 source, 44390 target) ; Learning rate = 0.000538 ; Loss = 1.475633\n",
      "2024-12-05 14:24:30.758000: I runner.py:310] Step = 27100 ; steps/s = 1.65, tokens/s = 74094 (30322 source, 43772 target) ; Learning rate = 0.000537 ; Loss = 1.473409\n",
      "2024-12-05 14:25:31.646000: I runner.py:310] Step = 27200 ; steps/s = 1.64, tokens/s = 75146 (30718 source, 44428 target) ; Learning rate = 0.000536 ; Loss = 1.472651\n",
      "2024-12-05 14:26:32.451000: I runner.py:310] Step = 27300 ; steps/s = 1.64, tokens/s = 75238 (30762 source, 44476 target) ; Learning rate = 0.000535 ; Loss = 1.472892\n",
      "2024-12-05 14:27:32.905000: I runner.py:310] Step = 27400 ; steps/s = 1.65, tokens/s = 74365 (30421 source, 43944 target) ; Learning rate = 0.000534 ; Loss = 1.474864\n",
      "2024-12-05 14:28:33.758000: I runner.py:310] Step = 27500 ; steps/s = 1.64, tokens/s = 75199 (30754 source, 44445 target) ; Learning rate = 0.000533 ; Loss = 1.475827\n",
      "2024-12-05 14:29:34.203000: I runner.py:310] Step = 27600 ; steps/s = 1.65, tokens/s = 74394 (30441 source, 43953 target) ; Learning rate = 0.000532 ; Loss = 1.462953\n",
      "2024-12-05 14:30:35.116000: I runner.py:310] Step = 27700 ; steps/s = 1.64, tokens/s = 75085 (30688 source, 44397 target) ; Learning rate = 0.000531 ; Loss = 1.480114\n",
      "2024-12-05 14:31:36.031000: I runner.py:310] Step = 27800 ; steps/s = 1.64, tokens/s = 75117 (30708 source, 44409 target) ; Learning rate = 0.000530 ; Loss = 1.477942\n",
      "2024-12-05 14:32:36.425000: I runner.py:310] Step = 27900 ; steps/s = 1.66, tokens/s = 74442 (30457 source, 43985 target) ; Learning rate = 0.000529 ; Loss = 1.468222\n",
      "2024-12-05 14:33:37.316000: I runner.py:310] Step = 28000 ; steps/s = 1.64, tokens/s = 75151 (30729 source, 44422 target) ; Learning rate = 0.000528 ; Loss = 1.472557\n",
      "2024-12-05 14:34:37.772000: I runner.py:310] Step = 28100 ; steps/s = 1.65, tokens/s = 74389 (30445 source, 43944 target) ; Learning rate = 0.000527 ; Loss = 1.459940\n",
      "2024-12-05 14:35:38.627000: I runner.py:310] Step = 28200 ; steps/s = 1.64, tokens/s = 75164 (30723 source, 44441 target) ; Learning rate = 0.000526 ; Loss = 1.471368\n",
      "2024-12-05 14:36:39.475000: I runner.py:310] Step = 28300 ; steps/s = 1.64, tokens/s = 75199 (30742 source, 44457 target) ; Learning rate = 0.000525 ; Loss = 1.471405\n",
      "2024-12-05 14:37:39.909000: I runner.py:310] Step = 28400 ; steps/s = 1.65, tokens/s = 74376 (30422 source, 43954 target) ; Learning rate = 0.000524 ; Loss = 1.464574\n",
      "2024-12-05 14:38:40.793000: I runner.py:310] Step = 28500 ; steps/s = 1.64, tokens/s = 75150 (30725 source, 44425 target) ; Learning rate = 0.000524 ; Loss = 1.476306\n",
      "2024-12-05 14:39:41.658000: I runner.py:310] Step = 28600 ; steps/s = 1.64, tokens/s = 75184 (30746 source, 44438 target) ; Learning rate = 0.000523 ; Loss = 1.471020\n",
      "2024-12-05 14:40:42.109000: I runner.py:310] Step = 28700 ; steps/s = 1.65, tokens/s = 74372 (30424 source, 43948 target) ; Learning rate = 0.000522 ; Loss = 1.472316\n",
      "2024-12-05 14:41:42.986000: I runner.py:310] Step = 28800 ; steps/s = 1.64, tokens/s = 75177 (30744 source, 44433 target) ; Learning rate = 0.000521 ; Loss = 1.466610\n",
      "2024-12-05 14:42:43.426000: I runner.py:310] Step = 28900 ; steps/s = 1.65, tokens/s = 74356 (30410 source, 43946 target) ; Learning rate = 0.000520 ; Loss = 1.464493\n",
      "2024-12-05 14:43:44.317000: I runner.py:310] Step = 29000 ; steps/s = 1.64, tokens/s = 75138 (30720 source, 44418 target) ; Learning rate = 0.000519 ; Loss = 1.469762\n",
      "2024-12-05 14:44:45.218000: I runner.py:310] Step = 29100 ; steps/s = 1.64, tokens/s = 75130 (30717 source, 44413 target) ; Learning rate = 0.000518 ; Loss = 1.472786\n",
      "2024-12-05 14:45:45.633000: I runner.py:310] Step = 29200 ; steps/s = 1.66, tokens/s = 74443 (30469 source, 43974 target) ; Learning rate = 0.000517 ; Loss = 1.464782\n",
      "2024-12-05 14:46:46.493000: I runner.py:310] Step = 29300 ; steps/s = 1.64, tokens/s = 75174 (30733 source, 44441 target) ; Learning rate = 0.000516 ; Loss = 1.470697\n",
      "2024-12-05 14:47:46.910000: I runner.py:310] Step = 29400 ; steps/s = 1.66, tokens/s = 74391 (30421 source, 43970 target) ; Learning rate = 0.000515 ; Loss = 1.462228\n",
      "2024-12-05 14:48:47.737000: I runner.py:310] Step = 29500 ; steps/s = 1.64, tokens/s = 75230 (30758 source, 44472 target) ; Learning rate = 0.000515 ; Loss = 1.470959\n",
      "2024-12-05 14:49:48.640000: I runner.py:310] Step = 29600 ; steps/s = 1.64, tokens/s = 75135 (30731 source, 44404 target) ; Learning rate = 0.000514 ; Loss = 1.469526\n",
      "2024-12-05 14:50:49.053000: I runner.py:310] Step = 29700 ; steps/s = 1.66, tokens/s = 74420 (30438 source, 43982 target) ; Learning rate = 0.000513 ; Loss = 1.462677\n",
      "2024-12-05 14:51:49.933000: I runner.py:310] Step = 29800 ; steps/s = 1.64, tokens/s = 75152 (30722 source, 44430 target) ; Learning rate = 0.000512 ; Loss = 1.466775\n",
      "2024-12-05 14:52:50.322000: I runner.py:310] Step = 29900 ; steps/s = 1.66, tokens/s = 74449 (30463 source, 43986 target) ; Learning rate = 0.000511 ; Loss = 1.460334\n",
      "2024-12-05 14:53:51.196000: I runner.py:310] Step = 30000 ; steps/s = 1.64, tokens/s = 75156 (30724 source, 44432 target) ; Learning rate = 0.000510 ; Loss = 1.470193\n",
      "2024-12-05 14:53:52.900000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-30000\n",
      "2024-12-05 14:53:52.900000: I training.py:192] Running evaluation for step 30000\n",
      "2024-12-05 14:54:43.208000: I training.py:192] Evaluation result for step 30000: loss = 0.871457 ; perplexity = 2.390392\n",
      "2024-12-05 14:55:43.923000: I runner.py:310] Step = 30100 ; steps/s = 1.65, tokens/s = 75366 (30811 source, 44555 target) ; Learning rate = 0.000509 ; Loss = 1.468152\n",
      "2024-12-05 14:56:44.325000: I runner.py:310] Step = 30200 ; steps/s = 1.66, tokens/s = 74441 (30451 source, 43990 target) ; Learning rate = 0.000509 ; Loss = 1.464267\n",
      "2024-12-05 14:57:45.219000: I runner.py:310] Step = 30300 ; steps/s = 1.64, tokens/s = 75130 (30721 source, 44409 target) ; Learning rate = 0.000508 ; Loss = 1.464758\n",
      "2024-12-05 14:58:45.721000: I runner.py:310] Step = 30400 ; steps/s = 1.65, tokens/s = 74335 (30431 source, 43904 target) ; Learning rate = 0.000507 ; Loss = 1.460838\n",
      "2024-12-05 14:59:46.527000: I runner.py:310] Step = 30500 ; steps/s = 1.64, tokens/s = 75240 (30762 source, 44478 target) ; Learning rate = 0.000506 ; Loss = 1.466834\n",
      "2024-12-05 15:00:47.482000: I runner.py:310] Step = 30600 ; steps/s = 1.64, tokens/s = 75053 (30679 source, 44374 target) ; Learning rate = 0.000505 ; Loss = 1.467747\n",
      "2024-12-05 15:01:47.907000: I runner.py:310] Step = 30700 ; steps/s = 1.66, tokens/s = 74415 (30451 source, 43964 target) ; Learning rate = 0.000504 ; Loss = 1.466927\n",
      "2024-12-05 15:02:48.813000: I runner.py:310] Step = 30800 ; steps/s = 1.64, tokens/s = 75126 (30714 source, 44412 target) ; Learning rate = 0.000504 ; Loss = 1.476033\n",
      "2024-12-05 15:03:49.341000: I runner.py:310] Step = 30900 ; steps/s = 1.65, tokens/s = 74283 (30390 source, 43893 target) ; Learning rate = 0.000503 ; Loss = 1.457001\n",
      "2024-12-05 15:04:50.249000: I runner.py:310] Step = 31000 ; steps/s = 1.64, tokens/s = 75119 (30709 source, 44410 target) ; Learning rate = 0.000502 ; Loss = 1.458389\n",
      "2024-12-05 15:05:51.104000: I runner.py:310] Step = 31100 ; steps/s = 1.64, tokens/s = 75178 (30735 source, 44443 target) ; Learning rate = 0.000501 ; Loss = 1.468000\n",
      "2024-12-05 15:06:51.578000: I runner.py:310] Step = 31200 ; steps/s = 1.65, tokens/s = 74343 (30415 source, 43928 target) ; Learning rate = 0.000500 ; Loss = 1.465707\n",
      "2024-12-05 15:07:52.441000: I runner.py:310] Step = 31300 ; steps/s = 1.64, tokens/s = 75186 (30746 source, 44440 target) ; Learning rate = 0.000500 ; Loss = 1.463039\n",
      "2024-12-05 15:08:52.857000: I runner.py:310] Step = 31400 ; steps/s = 1.66, tokens/s = 74425 (30449 source, 43976 target) ; Learning rate = 0.000499 ; Loss = 1.461158\n",
      "2024-12-05 15:09:53.696000: I runner.py:310] Step = 31500 ; steps/s = 1.64, tokens/s = 75186 (30731 source, 44455 target) ; Learning rate = 0.000498 ; Loss = 1.458911\n",
      "2024-12-05 15:10:54.581000: I runner.py:310] Step = 31600 ; steps/s = 1.64, tokens/s = 75139 (30720 source, 44419 target) ; Learning rate = 0.000497 ; Loss = 1.465824\n",
      "2024-12-05 15:11:54.959000: I runner.py:310] Step = 31700 ; steps/s = 1.66, tokens/s = 74470 (30472 source, 43998 target) ; Learning rate = 0.000496 ; Loss = 1.462500\n",
      "2024-12-05 15:12:55.834000: I runner.py:310] Step = 31800 ; steps/s = 1.64, tokens/s = 75179 (30746 source, 44433 target) ; Learning rate = 0.000496 ; Loss = 1.461053\n",
      "2024-12-05 15:13:56.196000: I runner.py:310] Step = 31900 ; steps/s = 1.66, tokens/s = 74489 (30472 source, 44017 target) ; Learning rate = 0.000495 ; Loss = 1.458297\n",
      "2024-12-05 15:14:57.002000: I runner.py:310] Step = 32000 ; steps/s = 1.64, tokens/s = 75229 (30754 source, 44475 target) ; Learning rate = 0.000494 ; Loss = 1.477473\n",
      "2024-12-05 15:15:57.829000: I runner.py:310] Step = 32100 ; steps/s = 1.64, tokens/s = 75208 (30741 source, 44467 target) ; Learning rate = 0.000493 ; Loss = 1.461972\n",
      "2024-12-05 15:16:58.214000: I runner.py:310] Step = 32200 ; steps/s = 1.66, tokens/s = 74452 (30463 source, 43989 target) ; Learning rate = 0.000493 ; Loss = 1.457604\n",
      "2024-12-05 15:17:59.075000: I runner.py:310] Step = 32300 ; steps/s = 1.64, tokens/s = 75171 (30734 source, 44437 target) ; Learning rate = 0.000492 ; Loss = 1.460983\n",
      "2024-12-05 15:18:59.874000: I runner.py:310] Step = 32400 ; steps/s = 1.64, tokens/s = 75008 (30667 source, 44341 target) ; Learning rate = 0.000491 ; Loss = 1.479071\n",
      "2024-12-05 15:20:00.348000: I runner.py:310] Step = 32500 ; steps/s = 1.65, tokens/s = 74576 (30490 source, 44086 target) ; Learning rate = 0.000490 ; Loss = 1.453135\n",
      "2024-12-05 15:21:01.172000: I runner.py:310] Step = 32600 ; steps/s = 1.64, tokens/s = 75237 (30771 source, 44466 target) ; Learning rate = 0.000490 ; Loss = 1.460681\n",
      "2024-12-05 15:22:01.613000: I runner.py:310] Step = 32700 ; steps/s = 1.65, tokens/s = 74389 (30435 source, 43954 target) ; Learning rate = 0.000489 ; Loss = 1.458581\n",
      "2024-12-05 15:23:02.481000: I runner.py:310] Step = 32800 ; steps/s = 1.64, tokens/s = 75169 (30735 source, 44434 target) ; Learning rate = 0.000488 ; Loss = 1.460875\n",
      "2024-12-05 15:24:03.329000: I runner.py:310] Step = 32900 ; steps/s = 1.64, tokens/s = 75187 (30739 source, 44448 target) ; Learning rate = 0.000487 ; Loss = 1.467242\n",
      "2024-12-05 15:25:03.734000: I runner.py:310] Step = 33000 ; steps/s = 1.66, tokens/s = 74438 (30460 source, 43978 target) ; Learning rate = 0.000487 ; Loss = 1.456590\n",
      "2024-12-05 15:26:04.562000: I runner.py:310] Step = 33100 ; steps/s = 1.64, tokens/s = 75221 (30751 source, 44470 target) ; Learning rate = 0.000486 ; Loss = 1.463701\n",
      "2024-12-05 15:27:04.980000: I runner.py:310] Step = 33200 ; steps/s = 1.66, tokens/s = 74433 (30460 source, 43973 target) ; Learning rate = 0.000485 ; Loss = 1.455691\n",
      "2024-12-05 15:28:05.905000: I runner.py:310] Step = 33300 ; steps/s = 1.64, tokens/s = 75085 (30689 source, 44396 target) ; Learning rate = 0.000484 ; Loss = 1.464851\n",
      "2024-12-05 15:29:06.802000: I runner.py:310] Step = 33400 ; steps/s = 1.64, tokens/s = 75136 (30725 source, 44411 target) ; Learning rate = 0.000484 ; Loss = 1.464338\n",
      "2024-12-05 15:30:07.228000: I runner.py:310] Step = 33500 ; steps/s = 1.66, tokens/s = 74414 (30443 source, 43971 target) ; Learning rate = 0.000483 ; Loss = 1.458880\n",
      "2024-12-05 15:31:08.095000: I runner.py:310] Step = 33600 ; steps/s = 1.64, tokens/s = 75169 (30730 source, 44439 target) ; Learning rate = 0.000482 ; Loss = 1.459412\n",
      "2024-12-05 15:32:08.531000: I runner.py:310] Step = 33700 ; steps/s = 1.65, tokens/s = 74406 (30448 source, 43958 target) ; Learning rate = 0.000481 ; Loss = 1.455209\n",
      "2024-12-05 15:33:09.379000: I runner.py:310] Step = 33800 ; steps/s = 1.64, tokens/s = 75168 (30720 source, 44448 target) ; Learning rate = 0.000481 ; Loss = 1.456238\n",
      "2024-12-05 15:34:10.230000: I runner.py:310] Step = 33900 ; steps/s = 1.64, tokens/s = 75204 (30753 source, 44451 target) ; Learning rate = 0.000480 ; Loss = 1.466020\n",
      "2024-12-05 15:35:10.646000: I runner.py:310] Step = 34000 ; steps/s = 1.66, tokens/s = 74394 (30431 source, 43963 target) ; Learning rate = 0.000479 ; Loss = 1.466404\n",
      "2024-12-05 15:36:11.485000: I runner.py:310] Step = 34100 ; steps/s = 1.64, tokens/s = 75217 (30753 source, 44464 target) ; Learning rate = 0.000479 ; Loss = 1.458000\n",
      "2024-12-05 15:37:11.956000: I runner.py:310] Step = 34200 ; steps/s = 1.65, tokens/s = 74395 (30453 source, 43942 target) ; Learning rate = 0.000478 ; Loss = 1.455089\n",
      "2024-12-05 15:38:12.848000: I runner.py:310] Step = 34300 ; steps/s = 1.64, tokens/s = 75107 (30689 source, 44418 target) ; Learning rate = 0.000477 ; Loss = 1.458054\n",
      "2024-12-05 15:39:13.700000: I runner.py:310] Step = 34400 ; steps/s = 1.64, tokens/s = 75183 (30741 source, 44442 target) ; Learning rate = 0.000477 ; Loss = 1.457050\n",
      "2024-12-05 15:40:14.134000: I runner.py:310] Step = 34500 ; steps/s = 1.65, tokens/s = 74420 (30460 source, 43960 target) ; Learning rate = 0.000476 ; Loss = 1.453261\n",
      "2024-12-05 15:41:14.975000: I runner.py:310] Step = 34600 ; steps/s = 1.64, tokens/s = 75180 (30728 source, 44452 target) ; Learning rate = 0.000475 ; Loss = 1.455423\n",
      "2024-12-05 15:42:15.409000: I runner.py:310] Step = 34700 ; steps/s = 1.65, tokens/s = 74418 (30452 source, 43966 target) ; Learning rate = 0.000474 ; Loss = 1.452245\n",
      "2024-12-05 15:43:16.254000: I runner.py:310] Step = 34800 ; steps/s = 1.64, tokens/s = 75197 (30748 source, 44449 target) ; Learning rate = 0.000474 ; Loss = 1.455567\n",
      "2024-12-05 15:44:17.054000: I runner.py:310] Step = 34900 ; steps/s = 1.64, tokens/s = 75247 (30762 source, 44485 target) ; Learning rate = 0.000473 ; Loss = 1.458857\n",
      "2024-12-05 15:45:17.482000: I runner.py:310] Step = 35000 ; steps/s = 1.66, tokens/s = 74408 (30443 source, 43965 target) ; Learning rate = 0.000472 ; Loss = 1.455112\n",
      "2024-12-05 15:45:17.483000: I training.py:192] Running evaluation for step 35000\n",
      "2024-12-05 15:46:07.017000: I training.py:192] Evaluation result for step 35000: loss = 0.892466 ; perplexity = 2.441141\n",
      "2024-12-05 15:47:07.732000: I runner.py:310] Step = 35100 ; steps/s = 1.65, tokens/s = 75363 (30810 source, 44553 target) ; Learning rate = 0.000472 ; Loss = 1.459943\n",
      "2024-12-05 15:48:08.151000: I runner.py:310] Step = 35200 ; steps/s = 1.66, tokens/s = 74419 (30450 source, 43969 target) ; Learning rate = 0.000471 ; Loss = 1.450334\n",
      "2024-12-05 15:49:09.007000: I runner.py:310] Step = 35300 ; steps/s = 1.64, tokens/s = 75170 (30726 source, 44444 target) ; Learning rate = 0.000470 ; Loss = 1.454226\n",
      "2024-12-05 15:50:10.431000: I runner.py:310] Step = 35400 ; steps/s = 1.63, tokens/s = 74510 (30476 source, 44034 target) ; Learning rate = 0.000470 ; Loss = 1.458458\n",
      "2024-12-05 15:51:12.616000: I runner.py:310] Step = 35500 ; steps/s = 1.61, tokens/s = 72284 (29564 source, 42720 target) ; Learning rate = 0.000469 ; Loss = 1.456780\n",
      "2024-12-05 15:52:15.030000: I runner.py:310] Step = 35600 ; steps/s = 1.60, tokens/s = 73292 (29957 source, 43335 target) ; Learning rate = 0.000468 ; Loss = 1.462785\n",
      "2024-12-05 15:53:17.002000: I runner.py:310] Step = 35700 ; steps/s = 1.61, tokens/s = 72586 (29716 source, 42870 target) ; Learning rate = 0.000468 ; Loss = 1.445572\n",
      "2024-12-05 15:54:19.316000: I runner.py:310] Step = 35800 ; steps/s = 1.60, tokens/s = 73398 (29995 source, 43403 target) ; Learning rate = 0.000467 ; Loss = 1.455885\n",
      "2024-12-05 15:55:21.709000: I runner.py:310] Step = 35900 ; steps/s = 1.60, tokens/s = 73326 (29977 source, 43349 target) ; Learning rate = 0.000466 ; Loss = 1.454047\n",
      "2024-12-05 15:56:23.340000: I runner.py:310] Step = 36000 ; steps/s = 1.62, tokens/s = 72949 (29848 source, 43101 target) ; Learning rate = 0.000466 ; Loss = 1.452644\n",
      "2024-12-05 15:57:25.909000: I runner.py:310] Step = 36100 ; steps/s = 1.60, tokens/s = 73118 (29890 source, 43228 target) ; Learning rate = 0.000465 ; Loss = 1.455334\n",
      "2024-12-05 15:58:28.680000: I runner.py:310] Step = 36200 ; steps/s = 1.59, tokens/s = 71659 (29332 source, 42327 target) ; Learning rate = 0.000465 ; Loss = 1.448121\n",
      "2024-12-05 15:59:30.453000: I runner.py:310] Step = 36300 ; steps/s = 1.62, tokens/s = 74032 (30250 source, 43782 target) ; Learning rate = 0.000464 ; Loss = 1.458929\n",
      "2024-12-05 16:00:32.976000: I runner.py:310] Step = 36400 ; steps/s = 1.60, tokens/s = 73185 (29923 source, 43262 target) ; Learning rate = 0.000463 ; Loss = 1.453859\n",
      "2024-12-05 16:01:34.746000: I runner.py:310] Step = 36500 ; steps/s = 1.62, tokens/s = 72778 (29780 source, 42998 target) ; Learning rate = 0.000463 ; Loss = 1.450111\n",
      "2024-12-05 16:02:37.318000: I runner.py:310] Step = 36600 ; steps/s = 1.60, tokens/s = 73103 (29874 source, 43229 target) ; Learning rate = 0.000462 ; Loss = 1.457967\n",
      "2024-12-05 16:03:39.625000: I runner.py:310] Step = 36700 ; steps/s = 1.61, tokens/s = 73452 (30041 source, 43411 target) ; Learning rate = 0.000461 ; Loss = 1.455520\n",
      "2024-12-05 16:04:42.341000: I runner.py:310] Step = 36800 ; steps/s = 1.59, tokens/s = 71702 (29345 source, 42357 target) ; Learning rate = 0.000461 ; Loss = 1.446632\n",
      "2024-12-05 16:05:44.952000: I runner.py:310] Step = 36900 ; steps/s = 1.60, tokens/s = 73060 (29859 source, 43201 target) ; Learning rate = 0.000460 ; Loss = 1.456113\n",
      "2024-12-05 16:06:46.250000: I runner.py:310] Step = 37000 ; steps/s = 1.63, tokens/s = 73358 (30027 source, 43331 target) ; Learning rate = 0.000460 ; Loss = 1.447952\n",
      "2024-12-05 16:07:48.209000: I runner.py:310] Step = 37100 ; steps/s = 1.61, tokens/s = 73835 (30173 source, 43662 target) ; Learning rate = 0.000459 ; Loss = 1.452207\n",
      "2024-12-05 16:08:49.081000: I runner.py:310] Step = 37200 ; steps/s = 1.64, tokens/s = 75169 (30738 source, 44431 target) ; Learning rate = 0.000458 ; Loss = 1.465611\n",
      "2024-12-05 16:09:49.506000: I runner.py:310] Step = 37300 ; steps/s = 1.66, tokens/s = 74372 (30408 source, 43964 target) ; Learning rate = 0.000458 ; Loss = 1.449828\n",
      "2024-12-05 16:10:50.814000: I runner.py:310] Step = 37400 ; steps/s = 1.63, tokens/s = 74643 (30532 source, 44111 target) ; Learning rate = 0.000457 ; Loss = 1.451707\n",
      "2024-12-05 16:11:51.300000: I runner.py:310] Step = 37500 ; steps/s = 1.65, tokens/s = 74350 (30425 source, 43925 target) ; Learning rate = 0.000456 ; Loss = 1.452519\n",
      "2024-12-05 16:12:52.460000: I runner.py:310] Step = 37600 ; steps/s = 1.64, tokens/s = 74829 (30608 source, 44221 target) ; Learning rate = 0.000456 ; Loss = 1.455645\n",
      "2024-12-05 16:13:53.499000: I runner.py:310] Step = 37700 ; steps/s = 1.64, tokens/s = 74933 (30622 source, 44311 target) ; Learning rate = 0.000455 ; Loss = 1.456897\n",
      "2024-12-05 16:14:53.955000: I runner.py:310] Step = 37800 ; steps/s = 1.65, tokens/s = 74377 (30427 source, 43950 target) ; Learning rate = 0.000455 ; Loss = 1.452345\n",
      "2024-12-05 16:15:54.823000: I runner.py:310] Step = 37900 ; steps/s = 1.64, tokens/s = 75159 (30735 source, 44424 target) ; Learning rate = 0.000454 ; Loss = 1.449854\n",
      "2024-12-05 16:16:55.375000: I runner.py:310] Step = 38000 ; steps/s = 1.65, tokens/s = 74256 (30376 source, 43880 target) ; Learning rate = 0.000453 ; Loss = 1.446252\n",
      "2024-12-05 16:17:56.263000: I runner.py:310] Step = 38100 ; steps/s = 1.64, tokens/s = 75138 (30719 source, 44419 target) ; Learning rate = 0.000453 ; Loss = 1.449045\n",
      "2024-12-05 16:18:57.062000: I runner.py:310] Step = 38200 ; steps/s = 1.65, tokens/s = 75264 (30771 source, 44493 target) ; Learning rate = 0.000452 ; Loss = 1.459349\n",
      "2024-12-05 16:19:57.518000: I runner.py:310] Step = 38300 ; steps/s = 1.65, tokens/s = 74364 (30422 source, 43942 target) ; Learning rate = 0.000452 ; Loss = 1.449578\n",
      "2024-12-05 16:20:58.398000: I runner.py:310] Step = 38400 ; steps/s = 1.64, tokens/s = 75161 (30730 source, 44431 target) ; Learning rate = 0.000451 ; Loss = 1.450440\n",
      "2024-12-05 16:21:58.882000: I runner.py:310] Step = 38500 ; steps/s = 1.65, tokens/s = 74341 (30426 source, 43915 target) ; Learning rate = 0.000450 ; Loss = 1.450512\n",
      "2024-12-05 16:22:59.831000: I runner.py:310] Step = 38600 ; steps/s = 1.64, tokens/s = 75046 (30672 source, 44374 target) ; Learning rate = 0.000450 ; Loss = 1.452054\n",
      "2024-12-05 16:24:00.744000: I runner.py:310] Step = 38700 ; steps/s = 1.64, tokens/s = 75112 (30710 source, 44402 target) ; Learning rate = 0.000449 ; Loss = 1.451841\n",
      "2024-12-05 16:25:01.171000: I runner.py:310] Step = 38800 ; steps/s = 1.66, tokens/s = 74415 (30448 source, 43967 target) ; Learning rate = 0.000449 ; Loss = 1.454910\n",
      "2024-12-05 16:26:02.423000: I runner.py:310] Step = 38900 ; steps/s = 1.63, tokens/s = 74698 (30541 source, 44157 target) ; Learning rate = 0.000448 ; Loss = 1.449851\n",
      "2024-12-05 16:27:02.815000: I runner.py:310] Step = 39000 ; steps/s = 1.66, tokens/s = 74457 (30467 source, 43990 target) ; Learning rate = 0.000448 ; Loss = 1.451916\n",
      "2024-12-05 16:28:03.714000: I runner.py:310] Step = 39100 ; steps/s = 1.64, tokens/s = 75121 (30705 source, 44416 target) ; Learning rate = 0.000447 ; Loss = 1.455136\n",
      "2024-12-05 16:29:04.530000: I runner.py:310] Step = 39200 ; steps/s = 1.64, tokens/s = 75234 (30763 source, 44471 target) ; Learning rate = 0.000446 ; Loss = 1.449607\n",
      "2024-12-05 16:30:04.897000: I runner.py:310] Step = 39300 ; steps/s = 1.66, tokens/s = 74482 (30479 source, 44003 target) ; Learning rate = 0.000446 ; Loss = 1.449353\n",
      "2024-12-05 16:31:05.783000: I runner.py:310] Step = 39400 ; steps/s = 1.64, tokens/s = 75155 (30732 source, 44423 target) ; Learning rate = 0.000445 ; Loss = 1.455736\n",
      "2024-12-05 16:32:06.161000: I runner.py:310] Step = 39500 ; steps/s = 1.66, tokens/s = 74453 (30452 source, 44001 target) ; Learning rate = 0.000445 ; Loss = 1.446321\n",
      "2024-12-05 16:33:07.007000: I runner.py:310] Step = 39600 ; steps/s = 1.64, tokens/s = 75193 (30743 source, 44450 target) ; Learning rate = 0.000444 ; Loss = 1.448155\n",
      "2024-12-05 16:34:07.855000: I runner.py:310] Step = 39700 ; steps/s = 1.64, tokens/s = 75160 (30711 source, 44449 target) ; Learning rate = 0.000444 ; Loss = 1.452451\n",
      "2024-12-05 16:35:08.272000: I runner.py:310] Step = 39800 ; steps/s = 1.66, tokens/s = 74434 (30458 source, 43976 target) ; Learning rate = 0.000443 ; Loss = 1.454184\n",
      "2024-12-05 16:36:09.065000: I runner.py:310] Step = 39900 ; steps/s = 1.65, tokens/s = 75261 (30778 source, 44483 target) ; Learning rate = 0.000442 ; Loss = 1.455653\n",
      "2024-12-05 16:37:09.472000: I runner.py:310] Step = 40000 ; steps/s = 1.66, tokens/s = 74447 (30464 source, 43983 target) ; Learning rate = 0.000442 ; Loss = 1.445740\n",
      "2024-12-05 16:37:11.390000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-40000\n",
      "2024-12-05 16:37:11.390000: I training.py:192] Running evaluation for step 40000\n",
      "2024-12-05 16:37:58.430000: I training.py:192] Evaluation result for step 40000: loss = 0.906121 ; perplexity = 2.474706\n",
      "2024-12-05 16:38:59.178000: I runner.py:310] Step = 40100 ; steps/s = 1.65, tokens/s = 75354 (30794 source, 44560 target) ; Learning rate = 0.000441 ; Loss = 1.445671\n",
      "2024-12-05 16:40:00.008000: I runner.py:310] Step = 40200 ; steps/s = 1.64, tokens/s = 75216 (30755 source, 44461 target) ; Learning rate = 0.000441 ; Loss = 1.449561\n",
      "2024-12-05 16:41:00.501000: I runner.py:310] Step = 40300 ; steps/s = 1.65, tokens/s = 74334 (30418 source, 43916 target) ; Learning rate = 0.000440 ; Loss = 1.457607\n",
      "2024-12-05 16:42:01.400000: I runner.py:310] Step = 40400 ; steps/s = 1.64, tokens/s = 75110 (30698 source, 44412 target) ; Learning rate = 0.000440 ; Loss = 1.450371\n",
      "2024-12-05 16:43:02.276000: I runner.py:310] Step = 40500 ; steps/s = 1.64, tokens/s = 75166 (30740 source, 44426 target) ; Learning rate = 0.000439 ; Loss = 1.457728\n",
      "2024-12-05 16:44:02.724000: I runner.py:310] Step = 40600 ; steps/s = 1.65, tokens/s = 74385 (30434 source, 43951 target) ; Learning rate = 0.000439 ; Loss = 1.440926\n",
      "2024-12-05 16:45:03.537000: I runner.py:310] Step = 40700 ; steps/s = 1.64, tokens/s = 75245 (30772 source, 44473 target) ; Learning rate = 0.000438 ; Loss = 1.453276\n",
      "2024-12-05 16:46:04.045000: I runner.py:310] Step = 40800 ; steps/s = 1.65, tokens/s = 74290 (30379 source, 43911 target) ; Learning rate = 0.000438 ; Loss = 1.445249\n",
      "2024-12-05 16:47:04.911000: I runner.py:310] Step = 40900 ; steps/s = 1.64, tokens/s = 75155 (30719 source, 44436 target) ; Learning rate = 0.000437 ; Loss = 1.450117\n",
      "2024-12-05 16:48:05.802000: I runner.py:310] Step = 41000 ; steps/s = 1.64, tokens/s = 75159 (30739 source, 44420 target) ; Learning rate = 0.000437 ; Loss = 1.452820\n",
      "2024-12-05 16:49:06.241000: I runner.py:310] Step = 41100 ; steps/s = 1.65, tokens/s = 74384 (30436 source, 43948 target) ; Learning rate = 0.000436 ; Loss = 1.442839\n",
      "2024-12-05 16:50:07.089000: I runner.py:310] Step = 41200 ; steps/s = 1.64, tokens/s = 75202 (30751 source, 44451 target) ; Learning rate = 0.000435 ; Loss = 1.449447\n",
      "2024-12-05 16:51:07.574000: I runner.py:310] Step = 41300 ; steps/s = 1.65, tokens/s = 74329 (30401 source, 43928 target) ; Learning rate = 0.000435 ; Loss = 1.449060\n",
      "2024-12-05 16:52:08.397000: I runner.py:310] Step = 41400 ; steps/s = 1.64, tokens/s = 75222 (30755 source, 44467 target) ; Learning rate = 0.000434 ; Loss = 1.447076\n",
      "2024-12-05 16:53:09.235000: I runner.py:310] Step = 41500 ; steps/s = 1.64, tokens/s = 75192 (30734 source, 44458 target) ; Learning rate = 0.000434 ; Loss = 1.449955\n",
      "2024-12-05 16:54:09.734000: I runner.py:310] Step = 41600 ; steps/s = 1.65, tokens/s = 74340 (30428 source, 43912 target) ; Learning rate = 0.000433 ; Loss = 1.443203\n",
      "2024-12-05 16:55:10.593000: I runner.py:310] Step = 41700 ; steps/s = 1.64, tokens/s = 75180 (30739 source, 44441 target) ; Learning rate = 0.000433 ; Loss = 1.449718\n",
      "2024-12-05 16:56:11.025000: I runner.py:310] Step = 41800 ; steps/s = 1.65, tokens/s = 74397 (30436 source, 43961 target) ; Learning rate = 0.000432 ; Loss = 1.443798\n",
      "2024-12-05 16:57:11.893000: I runner.py:310] Step = 41900 ; steps/s = 1.64, tokens/s = 75166 (30730 source, 44436 target) ; Learning rate = 0.000432 ; Loss = 1.444733\n",
      "2024-12-05 16:58:12.755000: I runner.py:310] Step = 42000 ; steps/s = 1.64, tokens/s = 75168 (30728 source, 44440 target) ; Learning rate = 0.000431 ; Loss = 1.450650\n",
      "2024-12-05 16:59:13.173000: I runner.py:310] Step = 42100 ; steps/s = 1.66, tokens/s = 74413 (30437 source, 43976 target) ; Learning rate = 0.000431 ; Loss = 1.443355\n",
      "2024-12-05 17:00:14.006000: I runner.py:310] Step = 42200 ; steps/s = 1.64, tokens/s = 75193 (30736 source, 44457 target) ; Learning rate = 0.000430 ; Loss = 1.447911\n",
      "2024-12-05 17:01:14.438000: I runner.py:310] Step = 42300 ; steps/s = 1.66, tokens/s = 74430 (30473 source, 43957 target) ; Learning rate = 0.000430 ; Loss = 1.441686\n",
      "2024-12-05 17:02:15.224000: I runner.py:310] Step = 42400 ; steps/s = 1.65, tokens/s = 75275 (30774 source, 44501 target) ; Learning rate = 0.000429 ; Loss = 1.444492\n",
      "2024-12-05 17:03:16.049000: I runner.py:310] Step = 42500 ; steps/s = 1.64, tokens/s = 75197 (30735 source, 44462 target) ; Learning rate = 0.000429 ; Loss = 1.443966\n",
      "2024-12-05 17:04:16.449000: I runner.py:310] Step = 42600 ; steps/s = 1.66, tokens/s = 74440 (30457 source, 43983 target) ; Learning rate = 0.000428 ; Loss = 1.445005\n",
      "2024-12-05 17:05:17.298000: I runner.py:310] Step = 42700 ; steps/s = 1.64, tokens/s = 75185 (30736 source, 44449 target) ; Learning rate = 0.000428 ; Loss = 1.447605\n",
      "2024-12-05 17:06:17.789000: I runner.py:310] Step = 42800 ; steps/s = 1.65, tokens/s = 74349 (30430 source, 43919 target) ; Learning rate = 0.000427 ; Loss = 1.440568\n",
      "2024-12-05 17:07:18.628000: I runner.py:310] Step = 42900 ; steps/s = 1.64, tokens/s = 75201 (30743 source, 44458 target) ; Learning rate = 0.000427 ; Loss = 1.444754\n",
      "2024-12-05 17:08:19.488000: I runner.py:310] Step = 43000 ; steps/s = 1.64, tokens/s = 75150 (30714 source, 44436 target) ; Learning rate = 0.000426 ; Loss = 1.446236\n",
      "2024-12-05 17:09:19.924000: I runner.py:310] Step = 43100 ; steps/s = 1.65, tokens/s = 74394 (30433 source, 43961 target) ; Learning rate = 0.000426 ; Loss = 1.442567\n",
      "2024-12-05 17:10:20.748000: I runner.py:310] Step = 43200 ; steps/s = 1.64, tokens/s = 75239 (30772 source, 44467 target) ; Learning rate = 0.000425 ; Loss = 1.445071\n",
      "2024-12-05 17:11:21.211000: I runner.py:310] Step = 43300 ; steps/s = 1.65, tokens/s = 74366 (30426 source, 43940 target) ; Learning rate = 0.000425 ; Loss = 1.444724\n",
      "2024-12-05 17:12:22.069000: I runner.py:310] Step = 43400 ; steps/s = 1.64, tokens/s = 75157 (30716 source, 44441 target) ; Learning rate = 0.000424 ; Loss = 1.445685\n",
      "2024-12-05 17:13:22.908000: I runner.py:310] Step = 43500 ; steps/s = 1.64, tokens/s = 75207 (30752 source, 44455 target) ; Learning rate = 0.000424 ; Loss = 1.447123\n",
      "2024-12-05 17:14:23.341000: I runner.py:310] Step = 43600 ; steps/s = 1.65, tokens/s = 74404 (30449 source, 43955 target) ; Learning rate = 0.000423 ; Loss = 1.443813\n",
      "2024-12-05 17:15:24.250000: I runner.py:310] Step = 43700 ; steps/s = 1.64, tokens/s = 75108 (30696 source, 44412 target) ; Learning rate = 0.000423 ; Loss = 1.443764\n",
      "2024-12-05 17:16:24.646000: I runner.py:310] Step = 43800 ; steps/s = 1.66, tokens/s = 74466 (30486 source, 43980 target) ; Learning rate = 0.000422 ; Loss = 1.434249\n",
      "2024-12-05 17:17:25.492000: I runner.py:310] Step = 43900 ; steps/s = 1.64, tokens/s = 75175 (30731 source, 44444 target) ; Learning rate = 0.000422 ; Loss = 1.443968\n",
      "2024-12-05 17:18:26.348000: I runner.py:310] Step = 44000 ; steps/s = 1.64, tokens/s = 75181 (30734 source, 44447 target) ; Learning rate = 0.000421 ; Loss = 1.449693\n",
      "2024-12-05 17:19:26.793000: I runner.py:310] Step = 44100 ; steps/s = 1.65, tokens/s = 74387 (30429 source, 43958 target) ; Learning rate = 0.000421 ; Loss = 1.446291\n",
      "2024-12-05 17:20:27.647000: I runner.py:310] Step = 44200 ; steps/s = 1.64, tokens/s = 75163 (30721 source, 44442 target) ; Learning rate = 0.000420 ; Loss = 1.444765\n",
      "2024-12-05 17:21:28.043000: I runner.py:310] Step = 44300 ; steps/s = 1.66, tokens/s = 74493 (30501 source, 43992 target) ; Learning rate = 0.000420 ; Loss = 1.474979\n",
      "2024-12-05 17:22:28.897000: I runner.py:310] Step = 44400 ; steps/s = 1.64, tokens/s = 75142 (30705 source, 44437 target) ; Learning rate = 0.000419 ; Loss = 1.443281\n",
      "2024-12-05 17:23:29.749000: I runner.py:310] Step = 44500 ; steps/s = 1.64, tokens/s = 75176 (30732 source, 44444 target) ; Learning rate = 0.000419 ; Loss = 1.450118\n",
      "2024-12-05 17:24:30.205000: I runner.py:310] Step = 44600 ; steps/s = 1.65, tokens/s = 74385 (30437 source, 43948 target) ; Learning rate = 0.000419 ; Loss = 1.441017\n",
      "2024-12-05 17:25:31.041000: I runner.py:310] Step = 44700 ; steps/s = 1.64, tokens/s = 75209 (30748 source, 44461 target) ; Learning rate = 0.000418 ; Loss = 1.448801\n",
      "2024-12-05 17:26:31.902000: I runner.py:310] Step = 44800 ; steps/s = 1.64, tokens/s = 75176 (30737 source, 44439 target) ; Learning rate = 0.000418 ; Loss = 1.446907\n",
      "2024-12-05 17:27:32.288000: I runner.py:310] Step = 44900 ; steps/s = 1.66, tokens/s = 74451 (30462 source, 43989 target) ; Learning rate = 0.000417 ; Loss = 1.438043\n",
      "2024-12-05 17:28:33.114000: I runner.py:310] Step = 45000 ; steps/s = 1.64, tokens/s = 75218 (30750 source, 44468 target) ; Learning rate = 0.000417 ; Loss = 1.447208\n",
      "2024-12-05 17:28:33.115000: I training.py:192] Running evaluation for step 45000\n",
      "2024-12-05 17:29:19.717000: I training.py:192] Evaluation result for step 45000: loss = 0.909400 ; perplexity = 2.482833\n",
      "2024-12-05 17:30:19.880000: I runner.py:310] Step = 45100 ; steps/s = 1.66, tokens/s = 74746 (30574 source, 44172 target) ; Learning rate = 0.000416 ; Loss = 1.442806\n",
      "2024-12-05 17:31:20.727000: I runner.py:310] Step = 45200 ; steps/s = 1.64, tokens/s = 75200 (30754 source, 44446 target) ; Learning rate = 0.000416 ; Loss = 1.444147\n",
      "2024-12-05 17:32:21.608000: I runner.py:310] Step = 45300 ; steps/s = 1.64, tokens/s = 75145 (30719 source, 44426 target) ; Learning rate = 0.000415 ; Loss = 1.442129\n",
      "2024-12-05 17:33:22.065000: I runner.py:310] Step = 45400 ; steps/s = 1.65, tokens/s = 74363 (30417 source, 43946 target) ; Learning rate = 0.000415 ; Loss = 1.438510\n",
      "2024-12-05 17:34:22.966000: I runner.py:310] Step = 45500 ; steps/s = 1.64, tokens/s = 75129 (30722 source, 44407 target) ; Learning rate = 0.000414 ; Loss = 1.443814\n",
      "2024-12-05 17:35:23.403000: I runner.py:310] Step = 45600 ; steps/s = 1.65, tokens/s = 74399 (30447 source, 43952 target) ; Learning rate = 0.000414 ; Loss = 1.436437\n",
      "2024-12-05 17:36:24.238000: I runner.py:310] Step = 45700 ; steps/s = 1.64, tokens/s = 75182 (30722 source, 44460 target) ; Learning rate = 0.000413 ; Loss = 1.444514\n",
      "2024-12-05 17:37:25.093000: I runner.py:310] Step = 45800 ; steps/s = 1.64, tokens/s = 75209 (30763 source, 44446 target) ; Learning rate = 0.000413 ; Loss = 1.446965\n",
      "2024-12-05 17:38:25.451000: I runner.py:310] Step = 45900 ; steps/s = 1.66, tokens/s = 74486 (30475 source, 44011 target) ; Learning rate = 0.000413 ; Loss = 1.439249\n",
      "2024-12-05 17:39:26.347000: I runner.py:310] Step = 46000 ; steps/s = 1.64, tokens/s = 75113 (30700 source, 44413 target) ; Learning rate = 0.000412 ; Loss = 1.444158\n",
      "2024-12-05 17:40:26.762000: I runner.py:310] Step = 46100 ; steps/s = 1.66, tokens/s = 74457 (30475 source, 43982 target) ; Learning rate = 0.000412 ; Loss = 1.440202\n",
      "2024-12-05 17:41:27.635000: I runner.py:310] Step = 46200 ; steps/s = 1.64, tokens/s = 75155 (30724 source, 44431 target) ; Learning rate = 0.000411 ; Loss = 1.442129\n",
      "2024-12-05 17:42:28.399000: I runner.py:310] Step = 46300 ; steps/s = 1.65, tokens/s = 75290 (30780 source, 44510 target) ; Learning rate = 0.000411 ; Loss = 1.443309\n",
      "2024-12-05 17:43:28.831000: I runner.py:310] Step = 46400 ; steps/s = 1.65, tokens/s = 74417 (30451 source, 43966 target) ; Learning rate = 0.000410 ; Loss = 1.436994\n",
      "2024-12-05 17:44:29.739000: I runner.py:310] Step = 46500 ; steps/s = 1.64, tokens/s = 75117 (30713 source, 44404 target) ; Learning rate = 0.000410 ; Loss = 1.442221\n",
      "2024-12-05 17:45:30.200000: I runner.py:310] Step = 46600 ; steps/s = 1.65, tokens/s = 74364 (30423 source, 43941 target) ; Learning rate = 0.000409 ; Loss = 1.435719\n",
      "2024-12-05 17:46:30.977000: I runner.py:310] Step = 46700 ; steps/s = 1.65, tokens/s = 75277 (30774 source, 44503 target) ; Learning rate = 0.000409 ; Loss = 1.448775\n",
      "2024-12-05 17:47:31.864000: I runner.py:310] Step = 46800 ; steps/s = 1.64, tokens/s = 75145 (30723 source, 44422 target) ; Learning rate = 0.000409 ; Loss = 1.445470\n",
      "2024-12-05 17:48:32.244000: I runner.py:310] Step = 46900 ; steps/s = 1.66, tokens/s = 74449 (30457 source, 43992 target) ; Learning rate = 0.000408 ; Loss = 1.439480\n",
      "2024-12-05 17:49:33.127000: I runner.py:310] Step = 47000 ; steps/s = 1.64, tokens/s = 75148 (30726 source, 44422 target) ; Learning rate = 0.000408 ; Loss = 1.439219\n",
      "2024-12-05 17:50:33.547000: I runner.py:310] Step = 47100 ; steps/s = 1.66, tokens/s = 74431 (30458 source, 43973 target) ; Learning rate = 0.000407 ; Loss = 1.438195\n",
      "2024-12-05 17:51:34.410000: I runner.py:310] Step = 47200 ; steps/s = 1.64, tokens/s = 75161 (30726 source, 44435 target) ; Learning rate = 0.000407 ; Loss = 1.440583\n",
      "2024-12-05 17:52:35.234000: I runner.py:310] Step = 47300 ; steps/s = 1.64, tokens/s = 75225 (30751 source, 44474 target) ; Learning rate = 0.000406 ; Loss = 1.445512\n",
      "2024-12-05 17:53:35.628000: I runner.py:310] Step = 47400 ; steps/s = 1.66, tokens/s = 74436 (30452 source, 43984 target) ; Learning rate = 0.000406 ; Loss = 1.442183\n",
      "2024-12-05 17:54:36.488000: I runner.py:310] Step = 47500 ; steps/s = 1.64, tokens/s = 75181 (30737 source, 44444 target) ; Learning rate = 0.000406 ; Loss = 1.440478\n",
      "2024-12-05 17:55:36.930000: I runner.py:310] Step = 47600 ; steps/s = 1.65, tokens/s = 74407 (30453 source, 43954 target) ; Learning rate = 0.000405 ; Loss = 1.434199\n",
      "2024-12-05 17:56:37.770000: I runner.py:310] Step = 47700 ; steps/s = 1.64, tokens/s = 75171 (30723 source, 44448 target) ; Learning rate = 0.000405 ; Loss = 1.435130\n",
      "2024-12-05 17:57:38.547000: I runner.py:310] Step = 47800 ; steps/s = 1.65, tokens/s = 75295 (30786 source, 44509 target) ; Learning rate = 0.000404 ; Loss = 1.454635\n",
      "2024-12-05 17:58:38.956000: I runner.py:310] Step = 47900 ; steps/s = 1.66, tokens/s = 74420 (30442 source, 43978 target) ; Learning rate = 0.000404 ; Loss = 1.440490\n",
      "2024-12-05 17:59:39.774000: I runner.py:310] Step = 48000 ; steps/s = 1.64, tokens/s = 75224 (30758 source, 44466 target) ; Learning rate = 0.000403 ; Loss = 1.445508\n",
      "2024-12-05 18:00:40.220000: I runner.py:310] Step = 48100 ; steps/s = 1.65, tokens/s = 74410 (30456 source, 43954 target) ; Learning rate = 0.000403 ; Loss = 1.437594\n",
      "2024-12-05 18:01:40.965000: I runner.py:310] Step = 48200 ; steps/s = 1.65, tokens/s = 75303 (30781 source, 44522 target) ; Learning rate = 0.000403 ; Loss = 1.438637\n",
      "2024-12-05 18:02:41.536000: I runner.py:310] Step = 48300 ; steps/s = 1.65, tokens/s = 75533 (30881 source, 44652 target) ; Learning rate = 0.000402 ; Loss = 1.441619\n",
      "2024-12-05 18:03:41.701000: I runner.py:310] Step = 48400 ; steps/s = 1.66, tokens/s = 74723 (30559 source, 44164 target) ; Learning rate = 0.000402 ; Loss = 1.436993\n",
      "2024-12-05 18:04:42.222000: I runner.py:310] Step = 48500 ; steps/s = 1.65, tokens/s = 75602 (30913 source, 44689 target) ; Learning rate = 0.000401 ; Loss = 1.442610\n",
      "2024-12-05 18:05:42.766000: I runner.py:310] Step = 48600 ; steps/s = 1.65, tokens/s = 75569 (30901 source, 44668 target) ; Learning rate = 0.000401 ; Loss = 1.443820\n",
      "2024-12-05 18:06:42.944000: I runner.py:310] Step = 48700 ; steps/s = 1.66, tokens/s = 74701 (30557 source, 44144 target) ; Learning rate = 0.000401 ; Loss = 1.436774\n",
      "2024-12-05 18:07:43.509000: I runner.py:310] Step = 48800 ; steps/s = 1.65, tokens/s = 75530 (30877 source, 44653 target) ; Learning rate = 0.000400 ; Loss = 1.443768\n",
      "2024-12-05 18:08:43.685000: I runner.py:310] Step = 48900 ; steps/s = 1.66, tokens/s = 74730 (30576 source, 44154 target) ; Learning rate = 0.000400 ; Loss = 1.434164\n",
      "2024-12-05 18:09:44.230000: I runner.py:310] Step = 49000 ; steps/s = 1.65, tokens/s = 75567 (30899 source, 44668 target) ; Learning rate = 0.000399 ; Loss = 1.442379\n",
      "2024-12-05 18:10:44.803000: I runner.py:310] Step = 49100 ; steps/s = 1.65, tokens/s = 75552 (30890 source, 44662 target) ; Learning rate = 0.000399 ; Loss = 1.440430\n",
      "2024-12-05 18:11:44.892000: I runner.py:310] Step = 49200 ; steps/s = 1.66, tokens/s = 74807 (30598 source, 44209 target) ; Learning rate = 0.000398 ; Loss = 1.438058\n",
      "2024-12-05 18:12:45.491000: I runner.py:310] Step = 49300 ; steps/s = 1.65, tokens/s = 75507 (30878 source, 44629 target) ; Learning rate = 0.000398 ; Loss = 1.438281\n",
      "2024-12-05 18:13:45.647000: I runner.py:310] Step = 49400 ; steps/s = 1.66, tokens/s = 74751 (30582 source, 44169 target) ; Learning rate = 0.000398 ; Loss = 1.437308\n",
      "2024-12-05 18:14:46.207000: I runner.py:310] Step = 49500 ; steps/s = 1.65, tokens/s = 75552 (30889 source, 44663 target) ; Learning rate = 0.000397 ; Loss = 1.444139\n",
      "2024-12-05 18:15:46.766000: I runner.py:310] Step = 49600 ; steps/s = 1.65, tokens/s = 75542 (30882 source, 44660 target) ; Learning rate = 0.000397 ; Loss = 1.439458\n",
      "2024-12-05 18:16:46.927000: I runner.py:310] Step = 49700 ; steps/s = 1.66, tokens/s = 74734 (30578 source, 44156 target) ; Learning rate = 0.000396 ; Loss = 1.437187\n",
      "2024-12-05 18:17:47.482000: I runner.py:310] Step = 49800 ; steps/s = 1.65, tokens/s = 75562 (30895 source, 44667 target) ; Learning rate = 0.000396 ; Loss = 1.441369\n",
      "2024-12-05 18:18:47.554000: I runner.py:310] Step = 49900 ; steps/s = 1.66, tokens/s = 74842 (30618 source, 44224 target) ; Learning rate = 0.000396 ; Loss = 1.440011\n",
      "2024-12-05 18:19:48.083000: I runner.py:310] Step = 50000 ; steps/s = 1.65, tokens/s = 75568 (30887 source, 44681 target) ; Learning rate = 0.000395 ; Loss = 1.436710\n",
      "2024-12-05 18:19:49.922000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-50000\n",
      "2024-12-05 18:19:49.922000: I training.py:192] Running evaluation for step 50000\n",
      "2024-12-05 18:20:35.952000: I training.py:192] Evaluation result for step 50000: loss = 0.928494 ; perplexity = 2.530696\n",
      "2024-12-05 18:21:36.391000: I runner.py:310] Step = 50100 ; steps/s = 1.66, tokens/s = 75731 (30969 source, 44762 target) ; Learning rate = 0.000395 ; Loss = 1.436602\n",
      "2024-12-05 18:22:36.542000: I runner.py:310] Step = 50200 ; steps/s = 1.66, tokens/s = 74756 (30591 source, 44165 target) ; Learning rate = 0.000394 ; Loss = 1.441074\n",
      "2024-12-05 18:23:37.098000: I runner.py:310] Step = 50300 ; steps/s = 1.65, tokens/s = 75555 (30891 source, 44664 target) ; Learning rate = 0.000394 ; Loss = 1.439205\n",
      "2024-12-05 18:24:37.181000: I runner.py:310] Step = 50400 ; steps/s = 1.66, tokens/s = 74842 (30628 source, 44214 target) ; Learning rate = 0.000394 ; Loss = 1.428327\n",
      "2024-12-05 18:25:37.722000: I runner.py:310] Step = 50500 ; steps/s = 1.65, tokens/s = 75547 (30877 source, 44670 target) ; Learning rate = 0.000393 ; Loss = 1.437553\n",
      "2024-12-05 18:26:38.370000: I runner.py:310] Step = 50600 ; steps/s = 1.65, tokens/s = 75451 (30846 source, 44605 target) ; Learning rate = 0.000393 ; Loss = 1.443571\n",
      "2024-12-05 18:27:38.888000: I runner.py:310] Step = 50700 ; steps/s = 1.65, tokens/s = 74270 (30376 source, 43894 target) ; Learning rate = 0.000393 ; Loss = 1.435436\n",
      "2024-12-05 18:28:39.813000: I runner.py:310] Step = 50800 ; steps/s = 1.64, tokens/s = 75102 (30707 source, 44395 target) ; Learning rate = 0.000392 ; Loss = 1.439266\n",
      "2024-12-05 18:29:40.178000: I runner.py:310] Step = 50900 ; steps/s = 1.66, tokens/s = 74511 (30504 source, 44007 target) ; Learning rate = 0.000392 ; Loss = 1.431224\n",
      "2024-12-05 18:30:41.111000: I runner.py:310] Step = 51000 ; steps/s = 1.64, tokens/s = 75073 (30682 source, 44391 target) ; Learning rate = 0.000391 ; Loss = 1.437701\n",
      "2024-12-05 18:31:41.981000: I runner.py:310] Step = 51100 ; steps/s = 1.64, tokens/s = 75171 (30733 source, 44438 target) ; Learning rate = 0.000391 ; Loss = 1.441412\n",
      "2024-12-05 18:32:42.426000: I runner.py:310] Step = 51200 ; steps/s = 1.65, tokens/s = 74374 (30431 source, 43943 target) ; Learning rate = 0.000391 ; Loss = 1.438420\n",
      "2024-12-05 18:33:43.307000: I runner.py:310] Step = 51300 ; steps/s = 1.64, tokens/s = 75166 (30732 source, 44434 target) ; Learning rate = 0.000390 ; Loss = 1.440537\n",
      "2024-12-05 18:34:43.718000: I runner.py:310] Step = 51400 ; steps/s = 1.66, tokens/s = 74438 (30463 source, 43975 target) ; Learning rate = 0.000390 ; Loss = 1.432692\n",
      "2024-12-05 18:35:44.593000: I runner.py:310] Step = 51500 ; steps/s = 1.64, tokens/s = 75140 (30707 source, 44433 target) ; Learning rate = 0.000389 ; Loss = 1.434316\n",
      "2024-12-05 18:36:45.423000: I runner.py:310] Step = 51600 ; steps/s = 1.64, tokens/s = 75221 (30758 source, 44463 target) ; Learning rate = 0.000389 ; Loss = 1.440391\n",
      "2024-12-05 18:37:45.849000: I runner.py:310] Step = 51700 ; steps/s = 1.66, tokens/s = 74408 (30443 source, 43965 target) ; Learning rate = 0.000389 ; Loss = 1.439098\n",
      "2024-12-05 18:38:46.756000: I runner.py:310] Step = 51800 ; steps/s = 1.64, tokens/s = 75108 (30699 source, 44409 target) ; Learning rate = 0.000388 ; Loss = 1.437567\n",
      "2024-12-05 18:39:47.177000: I runner.py:310] Step = 51900 ; steps/s = 1.66, tokens/s = 74436 (30469 source, 43967 target) ; Learning rate = 0.000388 ; Loss = 1.435392\n",
      "2024-12-05 18:40:48.043000: I runner.py:310] Step = 52000 ; steps/s = 1.64, tokens/s = 75138 (30694 source, 44444 target) ; Learning rate = 0.000388 ; Loss = 1.432814\n",
      "2024-12-05 18:41:48.914000: I runner.py:310] Step = 52100 ; steps/s = 1.64, tokens/s = 75162 (30738 source, 44424 target) ; Learning rate = 0.000387 ; Loss = 1.439454\n",
      "2024-12-05 18:42:49.395000: I runner.py:310] Step = 52200 ; steps/s = 1.65, tokens/s = 74359 (30429 source, 43930 target) ; Learning rate = 0.000387 ; Loss = 1.434430\n",
      "2024-12-05 18:43:50.325000: I runner.py:310] Step = 52300 ; steps/s = 1.64, tokens/s = 75080 (30699 source, 44381 target) ; Learning rate = 0.000386 ; Loss = 1.437852\n",
      "2024-12-05 18:44:50.904000: I runner.py:310] Step = 52400 ; steps/s = 1.65, tokens/s = 74634 (30544 source, 44090 target) ; Learning rate = 0.000386 ; Loss = 1.474396\n",
      "2024-12-05 18:45:51.624000: I runner.py:310] Step = 52500 ; steps/s = 1.65, tokens/s = 74956 (30645 source, 44311 target) ; Learning rate = 0.000386 ; Loss = 1.433123\n",
      "2024-12-05 18:46:52.485000: I runner.py:310] Step = 52600 ; steps/s = 1.64, tokens/s = 75164 (30722 source, 44442 target) ; Learning rate = 0.000385 ; Loss = 1.439300\n",
      "2024-12-05 18:47:52.931000: I runner.py:310] Step = 52700 ; steps/s = 1.65, tokens/s = 74379 (30423 source, 43956 target) ; Learning rate = 0.000385 ; Loss = 1.436544\n",
      "2024-12-05 18:48:53.823000: I runner.py:310] Step = 52800 ; steps/s = 1.64, tokens/s = 75131 (30714 source, 44417 target) ; Learning rate = 0.000385 ; Loss = 1.444065\n",
      "2024-12-05 18:49:54.690000: I runner.py:310] Step = 52900 ; steps/s = 1.64, tokens/s = 75172 (30741 source, 44431 target) ; Learning rate = 0.000384 ; Loss = 1.441734\n",
      "2024-12-05 18:50:55.049000: I runner.py:310] Step = 53000 ; steps/s = 1.66, tokens/s = 74509 (30498 source, 44011 target) ; Learning rate = 0.000384 ; Loss = 1.431959\n",
      "2024-12-05 18:51:55.893000: I runner.py:310] Step = 53100 ; steps/s = 1.64, tokens/s = 75184 (30727 source, 44457 target) ; Learning rate = 0.000384 ; Loss = 1.435737\n",
      "2024-12-05 18:52:56.347000: I runner.py:310] Step = 53200 ; steps/s = 1.65, tokens/s = 74372 (30427 source, 43945 target) ; Learning rate = 0.000383 ; Loss = 1.437084\n",
      "2024-12-05 18:53:57.226000: I runner.py:310] Step = 53300 ; steps/s = 1.64, tokens/s = 75152 (30730 source, 44422 target) ; Learning rate = 0.000383 ; Loss = 1.436345\n",
      "2024-12-05 18:54:58.098000: I runner.py:310] Step = 53400 ; steps/s = 1.64, tokens/s = 75162 (30726 source, 44436 target) ; Learning rate = 0.000382 ; Loss = 1.442496\n",
      "2024-12-05 18:55:58.560000: I runner.py:310] Step = 53500 ; steps/s = 1.65, tokens/s = 74367 (30427 source, 43940 target) ; Learning rate = 0.000382 ; Loss = 1.434615\n",
      "2024-12-05 18:56:59.415000: I runner.py:310] Step = 53600 ; steps/s = 1.64, tokens/s = 75179 (30735 source, 44444 target) ; Learning rate = 0.000382 ; Loss = 1.440216\n",
      "2024-12-05 18:57:59.815000: I runner.py:310] Step = 53700 ; steps/s = 1.66, tokens/s = 74463 (30476 source, 43987 target) ; Learning rate = 0.000381 ; Loss = 1.431663\n",
      "2024-12-05 18:59:00.673000: I runner.py:310] Step = 53800 ; steps/s = 1.64, tokens/s = 75166 (30726 source, 44440 target) ; Learning rate = 0.000381 ; Loss = 1.436861\n",
      "2024-12-05 19:00:01.542000: I runner.py:310] Step = 53900 ; steps/s = 1.64, tokens/s = 75157 (30720 source, 44437 target) ; Learning rate = 0.000381 ; Loss = 1.439197\n",
      "2024-12-05 19:01:02.003000: I runner.py:310] Step = 54000 ; steps/s = 1.65, tokens/s = 74380 (30440 source, 43940 target) ; Learning rate = 0.000380 ; Loss = 1.433093\n",
      "2024-12-05 19:02:02.925000: I runner.py:310] Step = 54100 ; steps/s = 1.64, tokens/s = 75101 (30704 source, 44397 target) ; Learning rate = 0.000380 ; Loss = 1.437491\n",
      "2024-12-05 19:03:03.340000: I runner.py:310] Step = 54200 ; steps/s = 1.66, tokens/s = 74413 (30439 source, 43974 target) ; Learning rate = 0.000380 ; Loss = 1.431462\n",
      "2024-12-05 19:04:04.174000: I runner.py:310] Step = 54300 ; steps/s = 1.64, tokens/s = 75192 (30734 source, 44458 target) ; Learning rate = 0.000379 ; Loss = 1.438892\n",
      "2024-12-05 19:05:05.059000: I runner.py:310] Step = 54400 ; steps/s = 1.64, tokens/s = 75171 (30743 source, 44428 target) ; Learning rate = 0.000379 ; Loss = 1.434829\n",
      "2024-12-05 19:06:05.428000: I runner.py:310] Step = 54500 ; steps/s = 1.66, tokens/s = 74448 (30441 source, 44007 target) ; Learning rate = 0.000379 ; Loss = 1.431459\n",
      "2024-12-05 19:07:06.272000: I runner.py:310] Step = 54600 ; steps/s = 1.64, tokens/s = 75212 (30760 source, 44452 target) ; Learning rate = 0.000378 ; Loss = 1.434733\n",
      "2024-12-05 19:08:06.646000: I runner.py:310] Step = 54700 ; steps/s = 1.66, tokens/s = 74476 (30480 source, 43996 target) ; Learning rate = 0.000378 ; Loss = 1.426571\n",
      "2024-12-05 19:09:07.506000: I runner.py:310] Step = 54800 ; steps/s = 1.64, tokens/s = 75182 (30736 source, 44446 target) ; Learning rate = 0.000378 ; Loss = 1.437211\n",
      "2024-12-05 19:10:08.381000: I runner.py:310] Step = 54900 ; steps/s = 1.64, tokens/s = 75150 (30726 source, 44424 target) ; Learning rate = 0.000377 ; Loss = 1.441891\n",
      "2024-12-05 19:11:08.795000: I runner.py:310] Step = 55000 ; steps/s = 1.66, tokens/s = 74425 (30447 source, 43978 target) ; Learning rate = 0.000377 ; Loss = 1.435733\n",
      "2024-12-05 19:11:08.797000: I training.py:192] Running evaluation for step 55000\n",
      "2024-12-05 19:11:57.795000: I training.py:192] Evaluation result for step 55000: loss = 0.945682 ; perplexity = 2.574569\n",
      "2024-12-05 19:12:58.530000: I runner.py:310] Step = 55100 ; steps/s = 1.65, tokens/s = 75348 (30803 source, 44545 target) ; Learning rate = 0.000377 ; Loss = 1.437089\n",
      "2024-12-05 19:13:58.963000: I runner.py:310] Step = 55200 ; steps/s = 1.65, tokens/s = 74405 (30449 source, 43956 target) ; Learning rate = 0.000376 ; Loss = 1.434457\n",
      "2024-12-05 19:14:59.872000: I runner.py:310] Step = 55300 ; steps/s = 1.64, tokens/s = 75111 (30705 source, 44406 target) ; Learning rate = 0.000376 ; Loss = 1.437112\n",
      "2024-12-05 19:16:00.827000: I runner.py:310] Step = 55400 ; steps/s = 1.64, tokens/s = 75076 (30700 source, 44376 target) ; Learning rate = 0.000376 ; Loss = 1.436567\n",
      "2024-12-05 19:17:01.310000: I runner.py:310] Step = 55500 ; steps/s = 1.65, tokens/s = 74327 (30408 source, 43919 target) ; Learning rate = 0.000375 ; Loss = 1.438801\n",
      "2024-12-05 19:18:02.147000: I runner.py:310] Step = 55600 ; steps/s = 1.64, tokens/s = 75208 (30750 source, 44458 target) ; Learning rate = 0.000375 ; Loss = 1.438425\n",
      "2024-12-05 19:19:02.570000: I runner.py:310] Step = 55700 ; steps/s = 1.66, tokens/s = 74411 (30440 source, 43971 target) ; Learning rate = 0.000375 ; Loss = 1.431085\n",
      "2024-12-05 19:20:03.323000: I runner.py:310] Step = 55800 ; steps/s = 1.65, tokens/s = 75285 (30773 source, 44512 target) ; Learning rate = 0.000374 ; Loss = 1.432044\n",
      "2024-12-05 19:21:04.223000: I runner.py:310] Step = 55900 ; steps/s = 1.64, tokens/s = 75124 (30708 source, 44416 target) ; Learning rate = 0.000374 ; Loss = 1.435263\n",
      "2024-12-05 19:22:04.602000: I runner.py:310] Step = 56000 ; steps/s = 1.66, tokens/s = 74474 (30474 source, 44000 target) ; Learning rate = 0.000374 ; Loss = 1.432155\n",
      "2024-12-05 19:23:05.476000: I runner.py:310] Step = 56100 ; steps/s = 1.64, tokens/s = 75158 (30726 source, 44432 target) ; Learning rate = 0.000373 ; Loss = 1.433282\n",
      "2024-12-05 19:24:05.875000: I runner.py:310] Step = 56200 ; steps/s = 1.66, tokens/s = 74470 (30483 source, 43987 target) ; Learning rate = 0.000373 ; Loss = 1.432964\n",
      "2024-12-05 19:25:06.757000: I runner.py:310] Step = 56300 ; steps/s = 1.64, tokens/s = 75134 (30707 source, 44427 target) ; Learning rate = 0.000373 ; Loss = 1.431562\n",
      "2024-12-05 19:26:07.607000: I runner.py:310] Step = 56400 ; steps/s = 1.64, tokens/s = 75182 (30737 source, 44445 target) ; Learning rate = 0.000372 ; Loss = 1.437739\n",
      "2024-12-05 19:27:08.012000: I runner.py:310] Step = 56500 ; steps/s = 1.66, tokens/s = 74455 (30468 source, 43987 target) ; Learning rate = 0.000372 ; Loss = 1.430042\n",
      "2024-12-05 19:28:08.846000: I runner.py:310] Step = 56600 ; steps/s = 1.64, tokens/s = 75213 (30753 source, 44460 target) ; Learning rate = 0.000372 ; Loss = 1.433705\n",
      "2024-12-05 19:29:09.690000: I runner.py:310] Step = 56700 ; steps/s = 1.64, tokens/s = 75185 (30731 source, 44454 target) ; Learning rate = 0.000371 ; Loss = 1.434457\n",
      "2024-12-05 19:30:10.044000: I runner.py:310] Step = 56800 ; steps/s = 1.66, tokens/s = 74486 (30462 source, 44024 target) ; Learning rate = 0.000371 ; Loss = 1.431523\n",
      "2024-12-05 19:31:10.910000: I runner.py:310] Step = 56900 ; steps/s = 1.64, tokens/s = 75166 (30734 source, 44432 target) ; Learning rate = 0.000371 ; Loss = 1.433028\n",
      "2024-12-05 19:32:11.294000: I runner.py:310] Step = 57000 ; steps/s = 1.66, tokens/s = 74465 (30470 source, 43995 target) ; Learning rate = 0.000370 ; Loss = 1.435372\n",
      "2024-12-05 19:33:12.185000: I runner.py:310] Step = 57100 ; steps/s = 1.64, tokens/s = 75150 (30723 source, 44427 target) ; Learning rate = 0.000370 ; Loss = 1.435053\n",
      "2024-12-05 19:34:13.088000: I runner.py:310] Step = 57200 ; steps/s = 1.64, tokens/s = 75125 (30719 source, 44406 target) ; Learning rate = 0.000370 ; Loss = 1.435952\n",
      "2024-12-05 19:35:13.427000: I runner.py:310] Step = 57300 ; steps/s = 1.66, tokens/s = 74509 (30480 source, 44029 target) ; Learning rate = 0.000369 ; Loss = 1.431771\n",
      "2024-12-05 19:36:14.241000: I runner.py:310] Step = 57400 ; steps/s = 1.64, tokens/s = 75231 (30759 source, 44472 target) ; Learning rate = 0.000369 ; Loss = 1.433029\n",
      "2024-12-05 19:37:14.714000: I runner.py:310] Step = 57500 ; steps/s = 1.65, tokens/s = 74358 (30432 source, 43926 target) ; Learning rate = 0.000369 ; Loss = 1.432940\n",
      "2024-12-05 19:38:15.596000: I runner.py:310] Step = 57600 ; steps/s = 1.64, tokens/s = 75136 (30709 source, 44427 target) ; Learning rate = 0.000368 ; Loss = 1.435546\n",
      "2024-12-05 19:39:16.389000: I runner.py:310] Step = 57700 ; steps/s = 1.65, tokens/s = 75280 (30790 source, 44490 target) ; Learning rate = 0.000368 ; Loss = 1.437971\n",
      "2024-12-05 19:40:16.760000: I runner.py:310] Step = 57800 ; steps/s = 1.66, tokens/s = 74473 (30464 source, 44009 target) ; Learning rate = 0.000368 ; Loss = 1.433000\n",
      "2024-12-05 19:41:17.659000: I runner.py:310] Step = 57900 ; steps/s = 1.64, tokens/s = 75141 (30727 source, 44414 target) ; Learning rate = 0.000367 ; Loss = 1.435339\n",
      "2024-12-05 19:42:18.068000: I runner.py:310] Step = 58000 ; steps/s = 1.66, tokens/s = 74422 (30444 source, 43978 target) ; Learning rate = 0.000367 ; Loss = 1.432865\n",
      "2024-12-05 19:43:18.878000: I runner.py:310] Step = 58100 ; steps/s = 1.64, tokens/s = 75235 (30756 source, 44479 target) ; Learning rate = 0.000367 ; Loss = 1.437161\n",
      "2024-12-05 19:44:19.742000: I runner.py:310] Step = 58200 ; steps/s = 1.64, tokens/s = 75168 (30733 source, 44435 target) ; Learning rate = 0.000366 ; Loss = 1.435163\n",
      "2024-12-05 19:45:20.108000: I runner.py:310] Step = 58300 ; steps/s = 1.66, tokens/s = 74483 (30479 source, 44004 target) ; Learning rate = 0.000366 ; Loss = 1.432458\n",
      "2024-12-05 19:46:20.979000: I runner.py:310] Step = 58400 ; steps/s = 1.64, tokens/s = 75175 (30733 source, 44442 target) ; Learning rate = 0.000366 ; Loss = 1.431141\n",
      "2024-12-05 19:47:21.357000: I runner.py:310] Step = 58500 ; steps/s = 1.66, tokens/s = 74466 (30467 source, 43999 target) ; Learning rate = 0.000365 ; Loss = 1.425839\n",
      "2024-12-05 19:48:22.170000: I runner.py:310] Step = 58600 ; steps/s = 1.64, tokens/s = 75247 (30768 source, 44479 target) ; Learning rate = 0.000365 ; Loss = 1.433471\n",
      "2024-12-05 19:49:23.049000: I runner.py:310] Step = 58700 ; steps/s = 1.64, tokens/s = 75143 (30717 source, 44426 target) ; Learning rate = 0.000365 ; Loss = 1.436099\n",
      "2024-12-05 19:50:23.361000: I runner.py:310] Step = 58800 ; steps/s = 1.66, tokens/s = 74537 (30493 source, 44044 target) ; Learning rate = 0.000365 ; Loss = 1.430941\n",
      "2024-12-05 19:51:24.221000: I runner.py:310] Step = 58900 ; steps/s = 1.64, tokens/s = 75178 (30735 source, 44443 target) ; Learning rate = 0.000364 ; Loss = 1.433918\n",
      "2024-12-05 19:52:24.598000: I runner.py:310] Step = 59000 ; steps/s = 1.66, tokens/s = 74474 (30470 source, 44004 target) ; Learning rate = 0.000364 ; Loss = 1.431464\n",
      "2024-12-05 19:53:25.463000: I runner.py:310] Step = 59100 ; steps/s = 1.64, tokens/s = 75167 (30726 source, 44441 target) ; Learning rate = 0.000364 ; Loss = 1.432535\n",
      "2024-12-05 19:54:26.322000: I runner.py:310] Step = 59200 ; steps/s = 1.64, tokens/s = 75181 (30741 source, 44440 target) ; Learning rate = 0.000363 ; Loss = 1.434608\n",
      "2024-12-05 19:55:26.746000: I runner.py:310] Step = 59300 ; steps/s = 1.66, tokens/s = 74433 (30461 source, 43972 target) ; Learning rate = 0.000363 ; Loss = 1.434602\n",
      "2024-12-05 19:56:27.488000: I runner.py:310] Step = 59400 ; steps/s = 1.65, tokens/s = 75298 (30777 source, 44521 target) ; Learning rate = 0.000363 ; Loss = 1.433891\n",
      "2024-12-05 19:57:27.927000: I runner.py:310] Step = 59500 ; steps/s = 1.65, tokens/s = 74407 (30450 source, 43957 target) ; Learning rate = 0.000362 ; Loss = 1.426810\n",
      "2024-12-05 19:58:28.698000: I runner.py:310] Step = 59600 ; steps/s = 1.65, tokens/s = 75290 (30781 source, 44509 target) ; Learning rate = 0.000362 ; Loss = 1.428767\n",
      "2024-12-05 19:59:29.600000: I runner.py:310] Step = 59700 ; steps/s = 1.64, tokens/s = 75135 (30721 source, 44414 target) ; Learning rate = 0.000362 ; Loss = 1.433199\n",
      "2024-12-05 20:00:29.964000: I runner.py:310] Step = 59800 ; steps/s = 1.66, tokens/s = 74483 (30475 source, 44008 target) ; Learning rate = 0.000361 ; Loss = 1.431085\n",
      "2024-12-05 20:01:30.894000: I runner.py:310] Step = 59900 ; steps/s = 1.64, tokens/s = 75048 (30657 source, 44391 target) ; Learning rate = 0.000361 ; Loss = 1.440778\n",
      "2024-12-05 20:02:31.335000: I runner.py:310] Step = 60000 ; steps/s = 1.65, tokens/s = 74440 (30485 source, 43955 target) ; Learning rate = 0.000361 ; Loss = 1.430230\n",
      "2024-12-05 20:02:33.177000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-60000\n",
      "2024-12-05 20:02:33.177000: I training.py:192] Running evaluation for step 60000\n",
      "2024-12-05 20:03:23.503000: I training.py:192] Evaluation result for step 60000: loss = 0.943998 ; perplexity = 2.570235\n",
      "2024-12-05 20:04:24.273000: I runner.py:310] Step = 60100 ; steps/s = 1.65, tokens/s = 75300 (30788 source, 44512 target) ; Learning rate = 0.000361 ; Loss = 1.431063\n",
      "2024-12-05 20:05:25.183000: I runner.py:310] Step = 60200 ; steps/s = 1.64, tokens/s = 75095 (30696 source, 44399 target) ; Learning rate = 0.000360 ; Loss = 1.434525\n",
      "2024-12-05 20:06:25.592000: I runner.py:310] Step = 60300 ; steps/s = 1.66, tokens/s = 74412 (30433 source, 43979 target) ; Learning rate = 0.000360 ; Loss = 1.431829\n",
      "2024-12-05 20:07:26.556000: I runner.py:310] Step = 60400 ; steps/s = 1.64, tokens/s = 75055 (30686 source, 44369 target) ; Learning rate = 0.000360 ; Loss = 1.434973\n",
      "2024-12-05 20:08:27.295000: I runner.py:310] Step = 60500 ; steps/s = 1.65, tokens/s = 74737 (30583 source, 44154 target) ; Learning rate = 0.000359 ; Loss = 1.450437\n",
      "2024-12-05 20:09:27.872000: I runner.py:310] Step = 60600 ; steps/s = 1.65, tokens/s = 74838 (30610 source, 44228 target) ; Learning rate = 0.000359 ; Loss = 1.432301\n",
      "2024-12-05 20:10:28.799000: I runner.py:310] Step = 60700 ; steps/s = 1.64, tokens/s = 75073 (30677 source, 44396 target) ; Learning rate = 0.000359 ; Loss = 1.434752\n",
      "2024-12-05 20:11:29.323000: I runner.py:310] Step = 60800 ; steps/s = 1.65, tokens/s = 74281 (30386 source, 43895 target) ; Learning rate = 0.000358 ; Loss = 1.430597\n",
      "2024-12-05 20:12:30.219000: I runner.py:310] Step = 60900 ; steps/s = 1.64, tokens/s = 75136 (30721 source, 44415 target) ; Learning rate = 0.000358 ; Loss = 1.437422\n",
      "2024-12-05 20:13:31.100000: I runner.py:310] Step = 61000 ; steps/s = 1.64, tokens/s = 75154 (30725 source, 44429 target) ; Learning rate = 0.000358 ; Loss = 1.432570\n",
      "2024-12-05 20:14:31.494000: I runner.py:310] Step = 61100 ; steps/s = 1.66, tokens/s = 74460 (30478 source, 43982 target) ; Learning rate = 0.000358 ; Loss = 1.430724\n",
      "2024-12-05 20:15:32.334000: I runner.py:310] Step = 61200 ; steps/s = 1.64, tokens/s = 75205 (30740 source, 44465 target) ; Learning rate = 0.000357 ; Loss = 1.430253\n",
      "2024-12-05 20:16:32.744000: I runner.py:310] Step = 61300 ; steps/s = 1.66, tokens/s = 74423 (30452 source, 43971 target) ; Learning rate = 0.000357 ; Loss = 1.427129\n",
      "2024-12-05 20:17:33.656000: I runner.py:310] Step = 61400 ; steps/s = 1.64, tokens/s = 75093 (30688 source, 44405 target) ; Learning rate = 0.000357 ; Loss = 1.430989\n",
      "2024-12-05 20:18:34.571000: I runner.py:310] Step = 61500 ; steps/s = 1.64, tokens/s = 75116 (30711 source, 44405 target) ; Learning rate = 0.000356 ; Loss = 1.432905\n",
      "2024-12-05 20:19:35.052000: I runner.py:310] Step = 61600 ; steps/s = 1.65, tokens/s = 74326 (30402 source, 43924 target) ; Learning rate = 0.000356 ; Loss = 1.428735\n",
      "2024-12-05 20:20:35.950000: I runner.py:310] Step = 61700 ; steps/s = 1.64, tokens/s = 75146 (30736 source, 44410 target) ; Learning rate = 0.000356 ; Loss = 1.430629\n",
      "2024-12-05 20:21:36.411000: I runner.py:310] Step = 61800 ; steps/s = 1.65, tokens/s = 74380 (30437 source, 43943 target) ; Learning rate = 0.000356 ; Loss = 1.430151\n",
      "2024-12-05 20:22:37.267000: I runner.py:310] Step = 61900 ; steps/s = 1.64, tokens/s = 75168 (30723 source, 44445 target) ; Learning rate = 0.000355 ; Loss = 1.432124\n",
      "2024-12-05 20:23:38.155000: I runner.py:310] Step = 62000 ; steps/s = 1.64, tokens/s = 75145 (30724 source, 44421 target) ; Learning rate = 0.000355 ; Loss = 1.436723\n",
      "2024-12-05 20:24:38.639000: I runner.py:310] Step = 62100 ; steps/s = 1.65, tokens/s = 74344 (30420 source, 43924 target) ; Learning rate = 0.000355 ; Loss = 1.431351\n",
      "2024-12-05 20:25:39.531000: I runner.py:310] Step = 62200 ; steps/s = 1.64, tokens/s = 75127 (30707 source, 44420 target) ; Learning rate = 0.000354 ; Loss = 1.429967\n",
      "2024-12-05 20:26:39.948000: I runner.py:310] Step = 62300 ; steps/s = 1.66, tokens/s = 74445 (30473 source, 43972 target) ; Learning rate = 0.000354 ; Loss = 1.427861\n",
      "2024-12-05 20:27:40.807000: I runner.py:310] Step = 62400 ; steps/s = 1.64, tokens/s = 75161 (30722 source, 44439 target) ; Learning rate = 0.000354 ; Loss = 1.428598\n",
      "2024-12-05 20:28:41.687000: I runner.py:310] Step = 62500 ; steps/s = 1.64, tokens/s = 75152 (30723 source, 44429 target) ; Learning rate = 0.000354 ; Loss = 1.435360\n",
      "2024-12-05 20:29:42.183000: I runner.py:310] Step = 62600 ; steps/s = 1.65, tokens/s = 74320 (30401 source, 43919 target) ; Learning rate = 0.000353 ; Loss = 1.427679\n",
      "2024-12-05 20:30:43.123000: I runner.py:310] Step = 62700 ; steps/s = 1.64, tokens/s = 75067 (30691 source, 44376 target) ; Learning rate = 0.000353 ; Loss = 1.431288\n",
      "2024-12-05 20:31:43.616000: I runner.py:310] Step = 62800 ; steps/s = 1.65, tokens/s = 74350 (30432 source, 43918 target) ; Learning rate = 0.000353 ; Loss = 1.428646\n",
      "2024-12-05 20:32:44.477000: I runner.py:310] Step = 62900 ; steps/s = 1.64, tokens/s = 75166 (30726 source, 44440 target) ; Learning rate = 0.000352 ; Loss = 1.430149\n",
      "2024-12-05 20:33:45.396000: I runner.py:310] Step = 63000 ; steps/s = 1.64, tokens/s = 75099 (30696 source, 44403 target) ; Learning rate = 0.000352 ; Loss = 1.432421\n",
      "2024-12-05 20:34:45.835000: I runner.py:310] Step = 63100 ; steps/s = 1.65, tokens/s = 74391 (30444 source, 43947 target) ; Learning rate = 0.000352 ; Loss = 1.434189\n",
      "2024-12-05 20:35:46.713000: I runner.py:310] Step = 63200 ; steps/s = 1.64, tokens/s = 75167 (30737 source, 44430 target) ; Learning rate = 0.000352 ; Loss = 1.430585\n",
      "2024-12-05 20:36:47.058000: I runner.py:310] Step = 63300 ; steps/s = 1.66, tokens/s = 74505 (30480 source, 44025 target) ; Learning rate = 0.000351 ; Loss = 1.431670\n",
      "2024-12-05 20:37:47.899000: I runner.py:310] Step = 63400 ; steps/s = 1.64, tokens/s = 75192 (30737 source, 44455 target) ; Learning rate = 0.000351 ; Loss = 1.432797\n",
      "2024-12-05 20:38:48.724000: I runner.py:310] Step = 63500 ; steps/s = 1.64, tokens/s = 75227 (30753 source, 44474 target) ; Learning rate = 0.000351 ; Loss = 1.431378\n",
      "2024-12-05 20:39:49.132000: I runner.py:310] Step = 63600 ; steps/s = 1.66, tokens/s = 74426 (30452 source, 43974 target) ; Learning rate = 0.000350 ; Loss = 1.429648\n",
      "2024-12-05 20:40:50.021000: I runner.py:310] Step = 63700 ; steps/s = 1.64, tokens/s = 75137 (30715 source, 44422 target) ; Learning rate = 0.000350 ; Loss = 1.428952\n",
      "2024-12-05 20:41:50.470000: I runner.py:310] Step = 63800 ; steps/s = 1.65, tokens/s = 74400 (30451 source, 43949 target) ; Learning rate = 0.000350 ; Loss = 1.428958\n",
      "2024-12-05 20:42:51.320000: I runner.py:310] Step = 63900 ; steps/s = 1.64, tokens/s = 75185 (30733 source, 44452 target) ; Learning rate = 0.000350 ; Loss = 1.427807\n",
      "2024-12-05 20:43:52.173000: I runner.py:310] Step = 64000 ; steps/s = 1.64, tokens/s = 75176 (30730 source, 44446 target) ; Learning rate = 0.000349 ; Loss = 1.432322\n",
      "2024-12-05 20:44:52.586000: I runner.py:310] Step = 64100 ; steps/s = 1.66, tokens/s = 74424 (30453 source, 43971 target) ; Learning rate = 0.000349 ; Loss = 1.429913\n",
      "2024-12-05 20:45:53.503000: I runner.py:310] Step = 64200 ; steps/s = 1.64, tokens/s = 75105 (30705 source, 44400 target) ; Learning rate = 0.000349 ; Loss = 1.430288\n",
      "2024-12-05 20:46:53.948000: I runner.py:310] Step = 64300 ; steps/s = 1.65, tokens/s = 74413 (30459 source, 43954 target) ; Learning rate = 0.000349 ; Loss = 1.427989\n",
      "2024-12-05 20:47:54.819000: I runner.py:310] Step = 64400 ; steps/s = 1.64, tokens/s = 75138 (30707 source, 44431 target) ; Learning rate = 0.000348 ; Loss = 1.425391\n",
      "2024-12-05 20:48:55.693000: I runner.py:310] Step = 64500 ; steps/s = 1.64, tokens/s = 75147 (30717 source, 44430 target) ; Learning rate = 0.000348 ; Loss = 1.433239\n",
      "2024-12-05 20:49:56.106000: I runner.py:310] Step = 64600 ; steps/s = 1.66, tokens/s = 74426 (30448 source, 43978 target) ; Learning rate = 0.000348 ; Loss = 1.425937\n",
      "2024-12-05 20:50:57.006000: I runner.py:310] Step = 64700 ; steps/s = 1.64, tokens/s = 75145 (30734 source, 44411 target) ; Learning rate = 0.000347 ; Loss = 1.437013\n",
      "2024-12-05 20:51:57.882000: I runner.py:310] Step = 64800 ; steps/s = 1.64, tokens/s = 75142 (30716 source, 44426 target) ; Learning rate = 0.000347 ; Loss = 1.437467\n",
      "2024-12-05 20:52:58.295000: I runner.py:310] Step = 64900 ; steps/s = 1.66, tokens/s = 74441 (30463 source, 43978 target) ; Learning rate = 0.000347 ; Loss = 1.425578\n",
      "2024-12-05 20:53:59.152000: I runner.py:310] Step = 65000 ; steps/s = 1.64, tokens/s = 75177 (30731 source, 44446 target) ; Learning rate = 0.000347 ; Loss = 1.428388\n",
      "2024-12-05 20:53:59.153000: I training.py:192] Running evaluation for step 65000\n",
      "2024-12-05 20:54:48.361000: I training.py:192] Evaluation result for step 65000: loss = 0.952602 ; perplexity = 2.592446\n",
      "2024-12-05 20:55:48.581000: I runner.py:310] Step = 65100 ; steps/s = 1.66, tokens/s = 74677 (30555 source, 44122 target) ; Learning rate = 0.000346 ; Loss = 1.429919\n",
      "2024-12-05 20:56:49.412000: I runner.py:310] Step = 65200 ; steps/s = 1.64, tokens/s = 75201 (30739 source, 44462 target) ; Learning rate = 0.000346 ; Loss = 1.429401\n",
      "2024-12-05 20:57:50.328000: I runner.py:310] Step = 65300 ; steps/s = 1.64, tokens/s = 75114 (30713 source, 44401 target) ; Learning rate = 0.000346 ; Loss = 1.430986\n",
      "2024-12-05 20:58:50.741000: I runner.py:310] Step = 65400 ; steps/s = 1.66, tokens/s = 74418 (30449 source, 43969 target) ; Learning rate = 0.000346 ; Loss = 1.426505\n",
      "2024-12-05 20:59:51.678000: I runner.py:310] Step = 65500 ; steps/s = 1.64, tokens/s = 75076 (30695 source, 44381 target) ; Learning rate = 0.000345 ; Loss = 1.433171\n",
      "2024-12-05 21:00:52.113000: I runner.py:310] Step = 65600 ; steps/s = 1.65, tokens/s = 74405 (30437 source, 43968 target) ; Learning rate = 0.000345 ; Loss = 1.427760\n",
      "2024-12-05 21:01:53.000000: I runner.py:310] Step = 65700 ; steps/s = 1.64, tokens/s = 75143 (30723 source, 44420 target) ; Learning rate = 0.000345 ; Loss = 1.429680\n",
      "2024-12-05 21:02:53.883000: I runner.py:310] Step = 65800 ; steps/s = 1.64, tokens/s = 75148 (30725 source, 44423 target) ; Learning rate = 0.000345 ; Loss = 1.432703\n",
      "2024-12-05 21:03:54.270000: I runner.py:310] Step = 65900 ; steps/s = 1.66, tokens/s = 74457 (30460 source, 43997 target) ; Learning rate = 0.000344 ; Loss = 1.426544\n",
      "2024-12-05 21:04:55.143000: I runner.py:310] Step = 66000 ; steps/s = 1.64, tokens/s = 75159 (30734 source, 44425 target) ; Learning rate = 0.000344 ; Loss = 1.425605\n",
      "2024-12-05 21:05:55.559000: I runner.py:310] Step = 66100 ; steps/s = 1.66, tokens/s = 74437 (30456 source, 43981 target) ; Learning rate = 0.000344 ; Loss = 1.427093\n",
      "2024-12-05 21:06:56.460000: I runner.py:310] Step = 66200 ; steps/s = 1.64, tokens/s = 75112 (30703 source, 44409 target) ; Learning rate = 0.000344 ; Loss = 1.431409\n",
      "2024-12-05 21:07:57.332000: I runner.py:310] Step = 66300 ; steps/s = 1.64, tokens/s = 75161 (30731 source, 44430 target) ; Learning rate = 0.000343 ; Loss = 1.428935\n",
      "2024-12-05 21:08:57.745000: I runner.py:310] Step = 66400 ; steps/s = 1.66, tokens/s = 74441 (30464 source, 43977 target) ; Learning rate = 0.000343 ; Loss = 1.429587\n",
      "2024-12-05 21:09:58.638000: I runner.py:310] Step = 66500 ; steps/s = 1.64, tokens/s = 75131 (30713 source, 44418 target) ; Learning rate = 0.000343 ; Loss = 1.434258\n",
      "2024-12-05 21:10:59.013000: I runner.py:310] Step = 66600 ; steps/s = 1.66, tokens/s = 74468 (30469 source, 43999 target) ; Learning rate = 0.000342 ; Loss = 1.423904\n",
      "2024-12-05 21:11:59.891000: I runner.py:310] Step = 66700 ; steps/s = 1.64, tokens/s = 75146 (30720 source, 44426 target) ; Learning rate = 0.000342 ; Loss = 1.429228\n",
      "2024-12-05 21:13:00.808000: I runner.py:310] Step = 66800 ; steps/s = 1.64, tokens/s = 75117 (30709 source, 44408 target) ; Learning rate = 0.000342 ; Loss = 1.431580\n",
      "2024-12-05 21:14:01.273000: I runner.py:310] Step = 66900 ; steps/s = 1.65, tokens/s = 74371 (30438 source, 43933 target) ; Learning rate = 0.000342 ; Loss = 1.427316\n",
      "2024-12-05 21:15:02.138000: I runner.py:310] Step = 67000 ; steps/s = 1.64, tokens/s = 75170 (30724 source, 44446 target) ; Learning rate = 0.000341 ; Loss = 1.427800\n",
      "2024-12-05 21:16:02.594000: I runner.py:310] Step = 67100 ; steps/s = 1.65, tokens/s = 74360 (30423 source, 43937 target) ; Learning rate = 0.000341 ; Loss = 1.424730\n",
      "2024-12-05 21:17:03.507000: I runner.py:310] Step = 67200 ; steps/s = 1.64, tokens/s = 75101 (30700 source, 44401 target) ; Learning rate = 0.000341 ; Loss = 1.433093\n",
      "2024-12-05 21:18:04.447000: I runner.py:310] Step = 67300 ; steps/s = 1.64, tokens/s = 75088 (30700 source, 44388 target) ; Learning rate = 0.000341 ; Loss = 1.429094\n",
      "2024-12-05 21:19:04.838000: I runner.py:310] Step = 67400 ; steps/s = 1.66, tokens/s = 74464 (30482 source, 43982 target) ; Learning rate = 0.000340 ; Loss = 1.424910\n",
      "2024-12-05 21:20:05.675000: I runner.py:310] Step = 67500 ; steps/s = 1.64, tokens/s = 75201 (30739 source, 44462 target) ; Learning rate = 0.000340 ; Loss = 1.430729\n",
      "2024-12-05 21:21:06.030000: I runner.py:310] Step = 67600 ; steps/s = 1.66, tokens/s = 74490 (30472 source, 44018 target) ; Learning rate = 0.000340 ; Loss = 1.427386\n",
      "2024-12-05 21:22:06.867000: I runner.py:310] Step = 67700 ; steps/s = 1.64, tokens/s = 75189 (30728 source, 44461 target) ; Learning rate = 0.000340 ; Loss = 1.425990\n",
      "2024-12-05 21:23:07.765000: I runner.py:310] Step = 67800 ; steps/s = 1.64, tokens/s = 75123 (30723 source, 44400 target) ; Learning rate = 0.000339 ; Loss = 1.432660\n",
      "2024-12-05 21:24:08.171000: I runner.py:310] Step = 67900 ; steps/s = 1.66, tokens/s = 74444 (30463 source, 43981 target) ; Learning rate = 0.000339 ; Loss = 1.428453\n",
      "2024-12-05 21:25:08.980000: I runner.py:310] Step = 68000 ; steps/s = 1.64, tokens/s = 75233 (30750 source, 44483 target) ; Learning rate = 0.000339 ; Loss = 1.434131\n",
      "2024-12-05 21:26:09.398000: I runner.py:310] Step = 68100 ; steps/s = 1.66, tokens/s = 74435 (30465 source, 43970 target) ; Learning rate = 0.000339 ; Loss = 1.425730\n",
      "2024-12-05 21:27:10.317000: I runner.py:310] Step = 68200 ; steps/s = 1.64, tokens/s = 75084 (30693 source, 44391 target) ; Learning rate = 0.000338 ; Loss = 1.426193\n",
      "2024-12-05 21:28:11.209000: I runner.py:310] Step = 68300 ; steps/s = 1.64, tokens/s = 75139 (30711 source, 44428 target) ; Learning rate = 0.000338 ; Loss = 1.429963\n",
      "2024-12-05 21:29:11.585000: I runner.py:310] Step = 68400 ; steps/s = 1.66, tokens/s = 74490 (30494 source, 43996 target) ; Learning rate = 0.000338 ; Loss = 1.426782\n",
      "2024-12-05 21:30:12.481000: I runner.py:310] Step = 68500 ; steps/s = 1.64, tokens/s = 75130 (30717 source, 44413 target) ; Learning rate = 0.000338 ; Loss = 1.432834\n",
      "2024-12-05 21:31:13.237000: I runner.py:310] Step = 68600 ; steps/s = 1.65, tokens/s = 74850 (30597 source, 44253 target) ; Learning rate = 0.000337 ; Loss = 1.434464\n",
      "2024-12-05 21:32:13.721000: I runner.py:310] Step = 68700 ; steps/s = 1.65, tokens/s = 74772 (30574 source, 44198 target) ; Learning rate = 0.000337 ; Loss = 1.429823\n",
      "2024-12-05 21:33:14.674000: I runner.py:310] Step = 68800 ; steps/s = 1.64, tokens/s = 75071 (30693 source, 44378 target) ; Learning rate = 0.000337 ; Loss = 1.427416\n",
      "2024-12-05 21:34:15.035000: I runner.py:310] Step = 68900 ; steps/s = 1.66, tokens/s = 74488 (30481 source, 44007 target) ; Learning rate = 0.000337 ; Loss = 1.430086\n",
      "2024-12-05 21:35:15.914000: I runner.py:310] Step = 69000 ; steps/s = 1.64, tokens/s = 75158 (30728 source, 44430 target) ; Learning rate = 0.000336 ; Loss = 1.429183\n",
      "2024-12-05 21:36:16.789000: I runner.py:310] Step = 69100 ; steps/s = 1.64, tokens/s = 75151 (30724 source, 44427 target) ; Learning rate = 0.000336 ; Loss = 1.430671\n",
      "2024-12-05 21:37:17.188000: I runner.py:310] Step = 69200 ; steps/s = 1.66, tokens/s = 74437 (30455 source, 43982 target) ; Learning rate = 0.000336 ; Loss = 1.428487\n",
      "2024-12-05 21:38:18.039000: I runner.py:310] Step = 69300 ; steps/s = 1.64, tokens/s = 75218 (30765 source, 44453 target) ; Learning rate = 0.000336 ; Loss = 1.431045\n",
      "2024-12-05 21:39:18.408000: I runner.py:310] Step = 69400 ; steps/s = 1.66, tokens/s = 74453 (30445 source, 44008 target) ; Learning rate = 0.000336 ; Loss = 1.424742\n",
      "2024-12-05 21:40:19.200000: I runner.py:310] Step = 69500 ; steps/s = 1.65, tokens/s = 75264 (30778 source, 44486 target) ; Learning rate = 0.000335 ; Loss = 1.431597\n",
      "2024-12-05 21:41:20.142000: I runner.py:310] Step = 69600 ; steps/s = 1.64, tokens/s = 75073 (30687 source, 44386 target) ; Learning rate = 0.000335 ; Loss = 1.434799\n",
      "2024-12-05 21:42:20.518000: I runner.py:310] Step = 69700 ; steps/s = 1.66, tokens/s = 74491 (30489 source, 44002 target) ; Learning rate = 0.000335 ; Loss = 1.427130\n",
      "2024-12-05 21:43:21.397000: I runner.py:310] Step = 69800 ; steps/s = 1.64, tokens/s = 75135 (30710 source, 44425 target) ; Learning rate = 0.000335 ; Loss = 1.430101\n",
      "2024-12-05 21:44:21.816000: I runner.py:310] Step = 69900 ; steps/s = 1.66, tokens/s = 74400 (30434 source, 43966 target) ; Learning rate = 0.000334 ; Loss = 1.427575\n",
      "2024-12-05 21:45:22.642000: I runner.py:310] Step = 70000 ; steps/s = 1.64, tokens/s = 75203 (30732 source, 44471 target) ; Learning rate = 0.000334 ; Loss = 1.425551\n",
      "2024-12-05 21:45:24.521000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-70000\n",
      "2024-12-05 21:45:24.522000: I training.py:192] Running evaluation for step 70000\n",
      "2024-12-05 21:46:12.512000: I training.py:192] Evaluation result for step 70000: loss = 0.963178 ; perplexity = 2.620011\n",
      "2024-12-05 21:47:13.270000: I runner.py:310] Step = 70100 ; steps/s = 1.65, tokens/s = 75384 (30837 source, 44547 target) ; Learning rate = 0.000334 ; Loss = 1.427937\n",
      "2024-12-05 21:48:13.690000: I runner.py:310] Step = 70200 ; steps/s = 1.66, tokens/s = 74418 (30446 source, 43972 target) ; Learning rate = 0.000334 ; Loss = 1.426665\n",
      "2024-12-05 21:49:14.595000: I runner.py:310] Step = 70300 ; steps/s = 1.64, tokens/s = 75131 (30722 source, 44409 target) ; Learning rate = 0.000333 ; Loss = 1.425055\n",
      "2024-12-05 21:50:14.994000: I runner.py:310] Step = 70400 ; steps/s = 1.66, tokens/s = 74438 (30456 source, 43982 target) ; Learning rate = 0.000333 ; Loss = 1.426006\n",
      "2024-12-05 21:51:15.915000: I runner.py:310] Step = 70500 ; steps/s = 1.64, tokens/s = 75079 (30686 source, 44393 target) ; Learning rate = 0.000333 ; Loss = 1.431059\n",
      "2024-12-05 21:52:16.774000: I runner.py:310] Step = 70600 ; steps/s = 1.64, tokens/s = 75197 (30745 source, 44452 target) ; Learning rate = 0.000333 ; Loss = 1.433133\n",
      "2024-12-05 21:53:17.209000: I runner.py:310] Step = 70700 ; steps/s = 1.65, tokens/s = 74403 (30452 source, 43951 target) ; Learning rate = 0.000332 ; Loss = 1.429390\n",
      "2024-12-05 21:54:18.072000: I runner.py:310] Step = 70800 ; steps/s = 1.64, tokens/s = 75162 (30717 source, 44445 target) ; Learning rate = 0.000332 ; Loss = 1.429587\n",
      "2024-12-05 21:55:18.560000: I runner.py:310] Step = 70900 ; steps/s = 1.65, tokens/s = 74349 (30431 source, 43918 target) ; Learning rate = 0.000332 ; Loss = 1.427001\n",
      "2024-12-05 21:56:19.401000: I runner.py:310] Step = 71000 ; steps/s = 1.64, tokens/s = 75175 (30720 source, 44455 target) ; Learning rate = 0.000332 ; Loss = 1.429988\n",
      "2024-12-05 21:57:20.248000: I runner.py:310] Step = 71100 ; steps/s = 1.64, tokens/s = 75202 (30753 source, 44449 target) ; Learning rate = 0.000331 ; Loss = 1.425506\n",
      "2024-12-05 21:58:20.702000: I runner.py:310] Step = 71200 ; steps/s = 1.65, tokens/s = 74358 (30414 source, 43944 target) ; Learning rate = 0.000331 ; Loss = 1.428455\n",
      "2024-12-05 21:59:21.603000: I runner.py:310] Step = 71300 ; steps/s = 1.64, tokens/s = 75135 (30728 source, 44407 target) ; Learning rate = 0.000331 ; Loss = 1.426209\n",
      "2024-12-05 22:00:22.022000: I runner.py:310] Step = 71400 ; steps/s = 1.66, tokens/s = 74429 (30456 source, 43973 target) ; Learning rate = 0.000331 ; Loss = 1.424889\n",
      "2024-12-05 22:01:22.878000: I runner.py:310] Step = 71500 ; steps/s = 1.64, tokens/s = 75174 (30726 source, 44448 target) ; Learning rate = 0.000331 ; Loss = 1.428004\n",
      "2024-12-05 22:02:23.767000: I runner.py:310] Step = 71600 ; steps/s = 1.64, tokens/s = 75143 (30720 source, 44423 target) ; Learning rate = 0.000330 ; Loss = 1.424721\n",
      "2024-12-05 22:03:24.143000: I runner.py:310] Step = 71700 ; steps/s = 1.66, tokens/s = 74473 (30480 source, 43993 target) ; Learning rate = 0.000330 ; Loss = 1.426264\n",
      "2024-12-05 22:04:25.058000: I runner.py:310] Step = 71800 ; steps/s = 1.64, tokens/s = 75111 (30710 source, 44401 target) ; Learning rate = 0.000330 ; Loss = 1.428033\n",
      "2024-12-05 22:05:25.462000: I runner.py:310] Step = 71900 ; steps/s = 1.66, tokens/s = 74441 (30455 source, 43986 target) ; Learning rate = 0.000330 ; Loss = 1.425108\n",
      "2024-12-05 22:06:26.356000: I runner.py:310] Step = 72000 ; steps/s = 1.64, tokens/s = 75125 (30708 source, 44417 target) ; Learning rate = 0.000329 ; Loss = 1.425003\n",
      "2024-12-05 22:07:27.239000: I runner.py:310] Step = 72100 ; steps/s = 1.64, tokens/s = 75150 (30725 source, 44425 target) ; Learning rate = 0.000329 ; Loss = 1.429178\n",
      "2024-12-05 22:08:27.651000: I runner.py:310] Step = 72200 ; steps/s = 1.66, tokens/s = 74404 (30431 source, 43973 target) ; Learning rate = 0.000329 ; Loss = 1.426734\n",
      "2024-12-05 22:09:28.485000: I runner.py:310] Step = 72300 ; steps/s = 1.64, tokens/s = 75216 (30760 source, 44456 target) ; Learning rate = 0.000329 ; Loss = 1.428813\n",
      "2024-12-05 22:10:28.934000: I runner.py:310] Step = 72400 ; steps/s = 1.65, tokens/s = 74411 (30456 source, 43955 target) ; Learning rate = 0.000328 ; Loss = 1.429759\n",
      "2024-12-05 22:11:29.816000: I runner.py:310] Step = 72500 ; steps/s = 1.64, tokens/s = 75126 (30702 source, 44424 target) ; Learning rate = 0.000328 ; Loss = 1.427133\n",
      "2024-12-05 22:12:30.719000: I runner.py:310] Step = 72600 ; steps/s = 1.64, tokens/s = 75123 (30713 source, 44410 target) ; Learning rate = 0.000328 ; Loss = 1.428144\n",
      "2024-12-05 22:13:31.119000: I runner.py:310] Step = 72700 ; steps/s = 1.66, tokens/s = 74435 (30453 source, 43982 target) ; Learning rate = 0.000328 ; Loss = 1.428374\n",
      "2024-12-05 22:14:32.005000: I runner.py:310] Step = 72800 ; steps/s = 1.64, tokens/s = 75131 (30700 source, 44431 target) ; Learning rate = 0.000328 ; Loss = 1.429316\n",
      "2024-12-05 22:15:32.926000: I runner.py:310] Step = 72900 ; steps/s = 1.64, tokens/s = 75119 (30730 source, 44389 target) ; Learning rate = 0.000327 ; Loss = 1.426781\n",
      "2024-12-05 22:16:33.330000: I runner.py:310] Step = 73000 ; steps/s = 1.66, tokens/s = 74430 (30449 source, 43981 target) ; Learning rate = 0.000327 ; Loss = 1.423774\n",
      "2024-12-05 22:17:34.214000: I runner.py:310] Step = 73100 ; steps/s = 1.64, tokens/s = 75139 (30715 source, 44424 target) ; Learning rate = 0.000327 ; Loss = 1.428613\n",
      "2024-12-05 22:18:34.683000: I runner.py:310] Step = 73200 ; steps/s = 1.65, tokens/s = 74366 (30431 source, 43935 target) ; Learning rate = 0.000327 ; Loss = 1.426039\n",
      "2024-12-05 22:19:35.568000: I runner.py:310] Step = 73300 ; steps/s = 1.64, tokens/s = 75150 (30730 source, 44420 target) ; Learning rate = 0.000326 ; Loss = 1.425745\n",
      "2024-12-05 22:20:36.399000: I runner.py:310] Step = 73400 ; steps/s = 1.64, tokens/s = 75208 (30743 source, 44465 target) ; Learning rate = 0.000326 ; Loss = 1.425903\n",
      "2024-12-05 22:21:36.852000: I runner.py:310] Step = 73500 ; steps/s = 1.65, tokens/s = 74383 (30433 source, 43950 target) ; Learning rate = 0.000326 ; Loss = 1.427617\n",
      "2024-12-05 22:22:37.809000: I runner.py:310] Step = 73600 ; steps/s = 1.64, tokens/s = 75068 (30694 source, 44374 target) ; Learning rate = 0.000326 ; Loss = 1.426021\n",
      "2024-12-05 22:23:38.221000: I runner.py:310] Step = 73700 ; steps/s = 1.66, tokens/s = 74420 (30448 source, 43972 target) ; Learning rate = 0.000326 ; Loss = 1.421613\n",
      "2024-12-05 22:24:39.090000: I runner.py:310] Step = 73800 ; steps/s = 1.64, tokens/s = 75170 (30737 source, 44433 target) ; Learning rate = 0.000325 ; Loss = 1.423725\n",
      "2024-12-05 22:25:39.960000: I runner.py:310] Step = 73900 ; steps/s = 1.64, tokens/s = 75166 (30733 source, 44433 target) ; Learning rate = 0.000325 ; Loss = 1.428297\n",
      "2024-12-05 22:26:40.334000: I runner.py:310] Step = 74000 ; steps/s = 1.66, tokens/s = 74473 (30466 source, 44007 target) ; Learning rate = 0.000325 ; Loss = 1.424023\n",
      "2024-12-05 22:27:41.313000: I runner.py:310] Step = 74100 ; steps/s = 1.64, tokens/s = 75013 (30664 source, 44349 target) ; Learning rate = 0.000325 ; Loss = 1.427632\n",
      "2024-12-05 22:28:41.708000: I runner.py:310] Step = 74200 ; steps/s = 1.66, tokens/s = 74459 (30460 source, 43999 target) ; Learning rate = 0.000324 ; Loss = 1.424748\n",
      "2024-12-05 22:29:42.561000: I runner.py:310] Step = 74300 ; steps/s = 1.64, tokens/s = 75173 (30727 source, 44446 target) ; Learning rate = 0.000324 ; Loss = 1.426858\n",
      "2024-12-05 22:30:43.471000: I runner.py:310] Step = 74400 ; steps/s = 1.64, tokens/s = 75123 (30722 source, 44401 target) ; Learning rate = 0.000324 ; Loss = 1.430955\n",
      "2024-12-05 22:31:43.855000: I runner.py:310] Step = 74500 ; steps/s = 1.66, tokens/s = 74475 (30483 source, 43992 target) ; Learning rate = 0.000324 ; Loss = 1.425998\n",
      "2024-12-05 22:32:44.719000: I runner.py:310] Step = 74600 ; steps/s = 1.64, tokens/s = 75165 (30728 source, 44437 target) ; Learning rate = 0.000324 ; Loss = 1.426466\n",
      "2024-12-05 22:33:45.095000: I runner.py:310] Step = 74700 ; steps/s = 1.66, tokens/s = 74450 (30450 source, 44000 target) ; Learning rate = 0.000323 ; Loss = 1.425566\n",
      "2024-12-05 22:34:45.910000: I runner.py:310] Step = 74800 ; steps/s = 1.64, tokens/s = 75228 (30747 source, 44481 target) ; Learning rate = 0.000323 ; Loss = 1.427715\n",
      "2024-12-05 22:35:46.802000: I runner.py:310] Step = 74900 ; steps/s = 1.64, tokens/s = 75152 (30734 source, 44418 target) ; Learning rate = 0.000323 ; Loss = 1.424676\n",
      "2024-12-05 22:36:47.253000: I runner.py:310] Step = 75000 ; steps/s = 1.65, tokens/s = 74370 (30421 source, 43949 target) ; Learning rate = 0.000323 ; Loss = 1.426631\n",
      "2024-12-05 22:36:47.254000: I training.py:192] Running evaluation for step 75000\n",
      "2024-12-05 22:37:35.141000: I training.py:192] Evaluation result for step 75000: loss = 0.965562 ; perplexity = 2.626265\n",
      "2024-12-05 22:38:35.850000: I runner.py:310] Step = 75100 ; steps/s = 1.65, tokens/s = 75362 (30803 source, 44559 target) ; Learning rate = 0.000323 ; Loss = 1.423820\n",
      "2024-12-05 22:39:36.280000: I runner.py:310] Step = 75200 ; steps/s = 1.65, tokens/s = 74443 (30480 source, 43963 target) ; Learning rate = 0.000322 ; Loss = 1.422445\n",
      "2024-12-05 22:40:38.081000: I runner.py:310] Step = 75300 ; steps/s = 1.62, tokens/s = 74011 (30251 source, 43760 target) ; Learning rate = 0.000322 ; Loss = 1.426979\n",
      "2024-12-05 22:41:40.030000: I runner.py:310] Step = 75400 ; steps/s = 1.61, tokens/s = 73866 (30207 source, 43659 target) ; Learning rate = 0.000322 ; Loss = 1.425896\n",
      "2024-12-05 22:42:40.885000: I runner.py:310] Step = 75500 ; steps/s = 1.64, tokens/s = 73874 (30213 source, 43661 target) ; Learning rate = 0.000322 ; Loss = 1.425415\n",
      "2024-12-05 22:43:42.289000: I runner.py:310] Step = 75600 ; steps/s = 1.63, tokens/s = 74506 (30463 source, 44043 target) ; Learning rate = 0.000321 ; Loss = 1.426500\n",
      "2024-12-05 22:44:43.879000: I runner.py:310] Step = 75700 ; steps/s = 1.62, tokens/s = 73019 (29883 source, 43136 target) ; Learning rate = 0.000321 ; Loss = 1.422858\n",
      "2024-12-05 22:45:46.668000: I runner.py:310] Step = 75800 ; steps/s = 1.59, tokens/s = 72859 (29781 source, 43078 target) ; Learning rate = 0.000321 ; Loss = 1.430114\n",
      "2024-12-05 22:46:48.375000: I runner.py:310] Step = 75900 ; steps/s = 1.62, tokens/s = 74158 (30329 source, 43829 target) ; Learning rate = 0.000321 ; Loss = 1.430212\n",
      "2024-12-05 22:47:49.573000: I runner.py:310] Step = 76000 ; steps/s = 1.63, tokens/s = 73447 (30039 source, 43408 target) ; Learning rate = 0.000321 ; Loss = 1.423998\n",
      "2024-12-05 22:48:50.892000: I runner.py:310] Step = 76100 ; steps/s = 1.63, tokens/s = 74631 (30522 source, 44109 target) ; Learning rate = 0.000320 ; Loss = 1.426171\n",
      "2024-12-05 22:49:52.111000: I runner.py:310] Step = 76200 ; steps/s = 1.63, tokens/s = 73450 (30056 source, 43394 target) ; Learning rate = 0.000320 ; Loss = 1.421259\n",
      "2024-12-05 22:50:53.938000: I runner.py:310] Step = 76300 ; steps/s = 1.62, tokens/s = 73991 (30247 source, 43744 target) ; Learning rate = 0.000320 ; Loss = 1.421948\n",
      "2024-12-05 22:51:55.282000: I runner.py:310] Step = 76400 ; steps/s = 1.63, tokens/s = 74581 (30485 source, 44096 target) ; Learning rate = 0.000320 ; Loss = 1.427125\n",
      "2024-12-05 22:52:56.410000: I runner.py:310] Step = 76500 ; steps/s = 1.64, tokens/s = 73523 (30072 source, 43451 target) ; Learning rate = 0.000320 ; Loss = 1.424518\n",
      "2024-12-05 22:53:57.323000: I runner.py:310] Step = 76600 ; steps/s = 1.64, tokens/s = 75104 (30700 source, 44404 target) ; Learning rate = 0.000319 ; Loss = 1.425504\n",
      "2024-12-05 22:54:58.131000: I runner.py:310] Step = 76700 ; steps/s = 1.64, tokens/s = 75012 (30684 source, 44328 target) ; Learning rate = 0.000319 ; Loss = 1.436431\n",
      "2024-12-05 22:55:58.632000: I runner.py:310] Step = 76800 ; steps/s = 1.65, tokens/s = 74561 (30495 source, 44066 target) ; Learning rate = 0.000319 ; Loss = 1.433537\n",
      "2024-12-05 22:56:59.505000: I runner.py:310] Step = 76900 ; steps/s = 1.64, tokens/s = 75155 (30732 source, 44423 target) ; Learning rate = 0.000319 ; Loss = 1.425664\n",
      "2024-12-05 22:57:59.965000: I runner.py:310] Step = 77000 ; steps/s = 1.65, tokens/s = 74375 (30432 source, 43943 target) ; Learning rate = 0.000319 ; Loss = 1.421418\n",
      "2024-12-05 22:59:00.816000: I runner.py:310] Step = 77100 ; steps/s = 1.64, tokens/s = 75185 (30735 source, 44450 target) ; Learning rate = 0.000318 ; Loss = 1.426559\n",
      "2024-12-05 23:00:01.648000: I runner.py:310] Step = 77200 ; steps/s = 1.64, tokens/s = 75211 (30747 source, 44464 target) ; Learning rate = 0.000318 ; Loss = 1.425903\n",
      "2024-12-05 23:01:02.063000: I runner.py:310] Step = 77300 ; steps/s = 1.66, tokens/s = 74402 (30436 source, 43966 target) ; Learning rate = 0.000318 ; Loss = 1.419381\n",
      "2024-12-05 23:02:02.906000: I runner.py:310] Step = 77400 ; steps/s = 1.64, tokens/s = 75211 (30754 source, 44457 target) ; Learning rate = 0.000318 ; Loss = 1.426087\n",
      "2024-12-05 23:03:03.303000: I runner.py:310] Step = 77500 ; steps/s = 1.66, tokens/s = 74461 (30475 source, 43986 target) ; Learning rate = 0.000317 ; Loss = 1.422685\n",
      "2024-12-05 23:04:04.145000: I runner.py:310] Step = 77600 ; steps/s = 1.64, tokens/s = 75201 (30748 source, 44453 target) ; Learning rate = 0.000317 ; Loss = 1.424617\n",
      "2024-12-05 23:05:04.948000: I runner.py:310] Step = 77700 ; steps/s = 1.64, tokens/s = 75233 (30747 source, 44486 target) ; Learning rate = 0.000317 ; Loss = 1.425897\n",
      "2024-12-05 23:06:05.287000: I runner.py:310] Step = 77800 ; steps/s = 1.66, tokens/s = 74530 (30493 source, 44037 target) ; Learning rate = 0.000317 ; Loss = 1.422979\n",
      "2024-12-05 23:07:06.153000: I runner.py:310] Step = 77900 ; steps/s = 1.64, tokens/s = 75162 (30731 source, 44431 target) ; Learning rate = 0.000317 ; Loss = 1.423078\n",
      "2024-12-05 23:08:06.502000: I runner.py:310] Step = 78000 ; steps/s = 1.66, tokens/s = 74510 (30495 source, 44015 target) ; Learning rate = 0.000316 ; Loss = 1.420966\n",
      "2024-12-05 23:09:07.269000: I runner.py:310] Step = 78100 ; steps/s = 1.65, tokens/s = 75280 (30762 source, 44518 target) ; Learning rate = 0.000316 ; Loss = 1.428098\n",
      "2024-12-05 23:10:08.072000: I runner.py:310] Step = 78200 ; steps/s = 1.64, tokens/s = 75247 (30770 source, 44477 target) ; Learning rate = 0.000316 ; Loss = 1.426053\n",
      "2024-12-05 23:11:08.459000: I runner.py:310] Step = 78300 ; steps/s = 1.66, tokens/s = 74450 (30456 source, 43994 target) ; Learning rate = 0.000316 ; Loss = 1.421071\n",
      "2024-12-05 23:12:09.271000: I runner.py:310] Step = 78400 ; steps/s = 1.64, tokens/s = 75235 (30762 source, 44473 target) ; Learning rate = 0.000316 ; Loss = 1.424830\n",
      "2024-12-05 23:13:09.665000: I runner.py:310] Step = 78500 ; steps/s = 1.66, tokens/s = 74459 (30470 source, 43989 target) ; Learning rate = 0.000315 ; Loss = 1.418746\n",
      "2024-12-05 23:14:10.505000: I runner.py:310] Step = 78600 ; steps/s = 1.64, tokens/s = 75196 (30738 source, 44458 target) ; Learning rate = 0.000315 ; Loss = 1.424887\n",
      "2024-12-05 23:15:11.310000: I runner.py:310] Step = 78700 ; steps/s = 1.64, tokens/s = 75249 (30767 source, 44482 target) ; Learning rate = 0.000315 ; Loss = 1.428782\n",
      "2024-12-05 23:16:11.710000: I runner.py:310] Step = 78800 ; steps/s = 1.66, tokens/s = 74436 (30451 source, 43985 target) ; Learning rate = 0.000315 ; Loss = 1.425000\n",
      "2024-12-05 23:17:12.542000: I runner.py:310] Step = 78900 ; steps/s = 1.64, tokens/s = 75224 (30761 source, 44463 target) ; Learning rate = 0.000315 ; Loss = 1.422604\n",
      "2024-12-05 23:18:12.974000: I runner.py:310] Step = 79000 ; steps/s = 1.65, tokens/s = 74387 (30430 source, 43957 target) ; Learning rate = 0.000314 ; Loss = 1.422132\n",
      "2024-12-05 23:19:13.786000: I runner.py:310] Step = 79100 ; steps/s = 1.64, tokens/s = 75244 (30767 source, 44477 target) ; Learning rate = 0.000314 ; Loss = 1.424592\n",
      "2024-12-05 23:20:14.637000: I runner.py:310] Step = 79200 ; steps/s = 1.64, tokens/s = 75182 (30735 source, 44447 target) ; Learning rate = 0.000314 ; Loss = 1.422711\n",
      "2024-12-05 23:21:15.052000: I runner.py:310] Step = 79300 ; steps/s = 1.66, tokens/s = 74413 (30439 source, 43974 target) ; Learning rate = 0.000314 ; Loss = 1.424647\n",
      "2024-12-05 23:22:15.871000: I runner.py:310] Step = 79400 ; steps/s = 1.64, tokens/s = 75234 (30770 source, 44464 target) ; Learning rate = 0.000314 ; Loss = 1.424776\n",
      "2024-12-05 23:23:16.280000: I runner.py:310] Step = 79500 ; steps/s = 1.66, tokens/s = 74437 (30455 source, 43982 target) ; Learning rate = 0.000313 ; Loss = 1.423144\n",
      "2024-12-05 23:24:18.020000: I runner.py:310] Step = 79600 ; steps/s = 1.62, tokens/s = 74104 (30296 source, 43808 target) ; Learning rate = 0.000313 ; Loss = 1.422951\n",
      "2024-12-05 23:25:18.956000: I runner.py:310] Step = 79700 ; steps/s = 1.64, tokens/s = 75090 (30703 source, 44387 target) ; Learning rate = 0.000313 ; Loss = 1.429731\n",
      "2024-12-05 23:26:19.271000: I runner.py:310] Step = 79800 ; steps/s = 1.66, tokens/s = 74519 (30473 source, 44046 target) ; Learning rate = 0.000313 ; Loss = 1.425954\n",
      "2024-12-05 23:27:20.100000: I runner.py:310] Step = 79900 ; steps/s = 1.64, tokens/s = 75204 (30743 source, 44461 target) ; Learning rate = 0.000313 ; Loss = 1.423140\n",
      "2024-12-05 23:28:20.458000: I runner.py:310] Step = 80000 ; steps/s = 1.66, tokens/s = 74524 (30508 source, 44016 target) ; Learning rate = 0.000312 ; Loss = 1.423629\n",
      "2024-12-05 23:28:22.952000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-80000\n",
      "2024-12-05 23:28:22.952000: I training.py:192] Running evaluation for step 80000\n",
      "2024-12-05 23:29:09.581000: I training.py:192] Evaluation result for step 80000: loss = 0.979996 ; perplexity = 2.664446\n",
      "2024-12-05 23:30:10.259000: I runner.py:310] Step = 80100 ; steps/s = 1.65, tokens/s = 75423 (30838 source, 44585 target) ; Learning rate = 0.000312 ; Loss = 1.418082\n",
      "2024-12-05 23:31:11.132000: I runner.py:310] Step = 80200 ; steps/s = 1.64, tokens/s = 75134 (30705 source, 44429 target) ; Learning rate = 0.000312 ; Loss = 1.424367\n",
      "2024-12-05 23:32:11.534000: I runner.py:310] Step = 80300 ; steps/s = 1.66, tokens/s = 74448 (30468 source, 43980 target) ; Learning rate = 0.000312 ; Loss = 1.424161\n",
      "2024-12-05 23:33:12.390000: I runner.py:310] Step = 80400 ; steps/s = 1.64, tokens/s = 75181 (30737 source, 44444 target) ; Learning rate = 0.000312 ; Loss = 1.426351\n",
      "2024-12-05 23:34:12.811000: I runner.py:310] Step = 80500 ; steps/s = 1.66, tokens/s = 74434 (30459 source, 43975 target) ; Learning rate = 0.000312 ; Loss = 1.420621\n",
      "2024-12-05 23:35:13.658000: I runner.py:310] Step = 80600 ; steps/s = 1.64, tokens/s = 75168 (30718 source, 44450 target) ; Learning rate = 0.000311 ; Loss = 1.417998\n",
      "2024-12-05 23:36:14.517000: I runner.py:310] Step = 80700 ; steps/s = 1.64, tokens/s = 75179 (30740 source, 44439 target) ; Learning rate = 0.000311 ; Loss = 1.419319\n",
      "2024-12-05 23:37:14.930000: I runner.py:310] Step = 80800 ; steps/s = 1.66, tokens/s = 74437 (30458 source, 43979 target) ; Learning rate = 0.000311 ; Loss = 1.421952\n",
      "2024-12-05 23:38:15.754000: I runner.py:310] Step = 80900 ; steps/s = 1.64, tokens/s = 75198 (30740 source, 44458 target) ; Learning rate = 0.000311 ; Loss = 1.424471\n",
      "2024-12-05 23:39:16.581000: I runner.py:310] Step = 81000 ; steps/s = 1.64, tokens/s = 75226 (30755 source, 44471 target) ; Learning rate = 0.000311 ; Loss = 1.428961\n",
      "2024-12-05 23:40:16.984000: I runner.py:310] Step = 81100 ; steps/s = 1.66, tokens/s = 74441 (30453 source, 43988 target) ; Learning rate = 0.000310 ; Loss = 1.422411\n",
      "2024-12-05 23:41:17.810000: I runner.py:310] Step = 81200 ; steps/s = 1.64, tokens/s = 75211 (30753 source, 44458 target) ; Learning rate = 0.000310 ; Loss = 1.424409\n",
      "2024-12-05 23:42:18.176000: I runner.py:310] Step = 81300 ; steps/s = 1.66, tokens/s = 74487 (30480 source, 44007 target) ; Learning rate = 0.000310 ; Loss = 1.424919\n",
      "2024-12-05 23:43:19.027000: I runner.py:310] Step = 81400 ; steps/s = 1.64, tokens/s = 75176 (30732 source, 44444 target) ; Learning rate = 0.000310 ; Loss = 1.426647\n",
      "2024-12-05 23:44:19.787000: I runner.py:310] Step = 81500 ; steps/s = 1.65, tokens/s = 75301 (30785 source, 44516 target) ; Learning rate = 0.000310 ; Loss = 1.424604\n",
      "2024-12-05 23:45:20.176000: I runner.py:310] Step = 81600 ; steps/s = 1.66, tokens/s = 74452 (30460 source, 43992 target) ; Learning rate = 0.000309 ; Loss = 1.424234\n",
      "2024-12-05 23:46:21.006000: I runner.py:310] Step = 81700 ; steps/s = 1.64, tokens/s = 75226 (30761 source, 44465 target) ; Learning rate = 0.000309 ; Loss = 1.421612\n",
      "2024-12-05 23:47:21.383000: I runner.py:310] Step = 81800 ; steps/s = 1.66, tokens/s = 74460 (30464 source, 43996 target) ; Learning rate = 0.000309 ; Loss = 1.422507\n",
      "2024-12-05 23:48:22.251000: I runner.py:310] Step = 81900 ; steps/s = 1.64, tokens/s = 75154 (30713 source, 44441 target) ; Learning rate = 0.000309 ; Loss = 1.424844\n",
      "2024-12-05 23:49:23.120000: I runner.py:310] Step = 82000 ; steps/s = 1.64, tokens/s = 75182 (30749 source, 44433 target) ; Learning rate = 0.000309 ; Loss = 1.430349\n",
      "2024-12-05 23:50:23.583000: I runner.py:310] Step = 82100 ; steps/s = 1.65, tokens/s = 74346 (30406 source, 43940 target) ; Learning rate = 0.000308 ; Loss = 1.423723\n",
      "2024-12-05 23:51:24.400000: I runner.py:310] Step = 82200 ; steps/s = 1.64, tokens/s = 75242 (30774 source, 44468 target) ; Learning rate = 0.000308 ; Loss = 1.425470\n",
      "2024-12-05 23:52:24.817000: I runner.py:310] Step = 82300 ; steps/s = 1.66, tokens/s = 74421 (30448 source, 43973 target) ; Learning rate = 0.000308 ; Loss = 1.420823\n",
      "2024-12-05 23:53:25.641000: I runner.py:310] Step = 82400 ; steps/s = 1.64, tokens/s = 75219 (30747 source, 44472 target) ; Learning rate = 0.000308 ; Loss = 1.428663\n",
      "2024-12-05 23:54:26.452000: I runner.py:310] Step = 82500 ; steps/s = 1.64, tokens/s = 75232 (30756 source, 44476 target) ; Learning rate = 0.000308 ; Loss = 1.427337\n",
      "2024-12-05 23:55:26.849000: I runner.py:310] Step = 82600 ; steps/s = 1.66, tokens/s = 74445 (30463 source, 43982 target) ; Learning rate = 0.000308 ; Loss = 1.422550\n",
      "2024-12-05 23:56:27.696000: I runner.py:310] Step = 82700 ; steps/s = 1.64, tokens/s = 75212 (30757 source, 44455 target) ; Learning rate = 0.000307 ; Loss = 1.424442\n",
      "2024-12-05 23:57:28.174000: I runner.py:310] Step = 82800 ; steps/s = 1.65, tokens/s = 74338 (30411 source, 43927 target) ; Learning rate = 0.000307 ; Loss = 1.420299\n",
      "2024-12-05 23:58:29.095000: I runner.py:310] Step = 82900 ; steps/s = 1.64, tokens/s = 75092 (30694 source, 44398 target) ; Learning rate = 0.000307 ; Loss = 1.426179\n",
      "2024-12-05 23:59:29.898000: I runner.py:310] Step = 83000 ; steps/s = 1.64, tokens/s = 75245 (30766 source, 44479 target) ; Learning rate = 0.000307 ; Loss = 1.424769\n",
      "2024-12-06 00:00:30.345000: I runner.py:310] Step = 83100 ; steps/s = 1.65, tokens/s = 74378 (30424 source, 43954 target) ; Learning rate = 0.000307 ; Loss = 1.423102\n",
      "2024-12-06 00:01:31.189000: I runner.py:310] Step = 83200 ; steps/s = 1.64, tokens/s = 75204 (30749 source, 44455 target) ; Learning rate = 0.000306 ; Loss = 1.423994\n",
      "2024-12-06 00:02:31.632000: I runner.py:310] Step = 83300 ; steps/s = 1.65, tokens/s = 74406 (30454 source, 43952 target) ; Learning rate = 0.000306 ; Loss = 1.418895\n",
      "2024-12-06 00:03:32.459000: I runner.py:310] Step = 83400 ; steps/s = 1.64, tokens/s = 75186 (30730 source, 44456 target) ; Learning rate = 0.000306 ; Loss = 1.422998\n",
      "2024-12-06 00:04:33.247000: I runner.py:310] Step = 83500 ; steps/s = 1.65, tokens/s = 75282 (30783 source, 44499 target) ; Learning rate = 0.000306 ; Loss = 1.429208\n",
      "2024-12-06 00:05:33.935000: I runner.py:310] Step = 83600 ; steps/s = 1.65, tokens/s = 74074 (30300 source, 43774 target) ; Learning rate = 0.000306 ; Loss = 1.423601\n",
      "2024-12-06 00:06:35.497000: I runner.py:310] Step = 83700 ; steps/s = 1.62, tokens/s = 74339 (30402 source, 43937 target) ; Learning rate = 0.000306 ; Loss = 1.424311\n",
      "2024-12-06 00:07:36.051000: I runner.py:310] Step = 83800 ; steps/s = 1.65, tokens/s = 74253 (30382 source, 43871 target) ; Learning rate = 0.000305 ; Loss = 1.422383\n",
      "2024-12-06 00:08:36.919000: I runner.py:310] Step = 83900 ; steps/s = 1.64, tokens/s = 75147 (30714 source, 44433 target) ; Learning rate = 0.000305 ; Loss = 1.421541\n",
      "2024-12-06 00:09:37.850000: I runner.py:310] Step = 84000 ; steps/s = 1.64, tokens/s = 75098 (30705 source, 44393 target) ; Learning rate = 0.000305 ; Loss = 1.418367\n",
      "2024-12-06 00:10:38.346000: I runner.py:310] Step = 84100 ; steps/s = 1.65, tokens/s = 74317 (30402 source, 43915 target) ; Learning rate = 0.000305 ; Loss = 1.421567\n",
      "2024-12-06 00:11:39.251000: I runner.py:310] Step = 84200 ; steps/s = 1.64, tokens/s = 75088 (30687 source, 44401 target) ; Learning rate = 0.000305 ; Loss = 1.427905\n",
      "2024-12-06 00:12:39.718000: I runner.py:310] Step = 84300 ; steps/s = 1.65, tokens/s = 74400 (30462 source, 43938 target) ; Learning rate = 0.000304 ; Loss = 1.417776\n",
      "2024-12-06 00:13:40.594000: I runner.py:310] Step = 84400 ; steps/s = 1.64, tokens/s = 75136 (30715 source, 44421 target) ; Learning rate = 0.000304 ; Loss = 1.434381\n",
      "2024-12-06 00:14:41.436000: I runner.py:310] Step = 84500 ; steps/s = 1.64, tokens/s = 75184 (30727 source, 44457 target) ; Learning rate = 0.000304 ; Loss = 1.420558\n",
      "2024-12-06 00:15:41.953000: I runner.py:310] Step = 84600 ; steps/s = 1.65, tokens/s = 74326 (30423 source, 43903 target) ; Learning rate = 0.000304 ; Loss = 1.422357\n",
      "2024-12-06 00:16:42.885000: I runner.py:310] Step = 84700 ; steps/s = 1.64, tokens/s = 75071 (30685 source, 44386 target) ; Learning rate = 0.000304 ; Loss = 1.424518\n",
      "2024-12-06 00:17:43.696000: I runner.py:310] Step = 84800 ; steps/s = 1.64, tokens/s = 75243 (30765 source, 44478 target) ; Learning rate = 0.000304 ; Loss = 1.430833\n",
      "2024-12-06 00:18:44.214000: I runner.py:310] Step = 84900 ; steps/s = 1.65, tokens/s = 74279 (30379 source, 43900 target) ; Learning rate = 0.000303 ; Loss = 1.421853\n",
      "2024-12-06 00:19:45.107000: I runner.py:310] Step = 85000 ; steps/s = 1.64, tokens/s = 75141 (30731 source, 44410 target) ; Learning rate = 0.000303 ; Loss = 1.422856\n",
      "2024-12-06 00:19:45.108000: I training.py:192] Running evaluation for step 85000\n",
      "2024-12-06 00:20:32.148000: I training.py:192] Evaluation result for step 85000: loss = 0.981430 ; perplexity = 2.668269\n",
      "2024-12-06 00:21:32.472000: I runner.py:310] Step = 85100 ; steps/s = 1.66, tokens/s = 74551 (30499 source, 44052 target) ; Learning rate = 0.000303 ; Loss = 1.420810\n",
      "2024-12-06 00:22:34.199000: I runner.py:310] Step = 85200 ; steps/s = 1.62, tokens/s = 74107 (30288 source, 43819 target) ; Learning rate = 0.000303 ; Loss = 1.422324\n",
      "2024-12-06 00:23:35.712000: I runner.py:310] Step = 85300 ; steps/s = 1.63, tokens/s = 74401 (30427 source, 43974 target) ; Learning rate = 0.000303 ; Loss = 1.423829\n",
      "2024-12-06 00:24:36.061000: I runner.py:310] Step = 85400 ; steps/s = 1.66, tokens/s = 74492 (30471 source, 44021 target) ; Learning rate = 0.000302 ; Loss = 1.421682\n",
      "2024-12-06 00:25:36.947000: I runner.py:310] Step = 85500 ; steps/s = 1.64, tokens/s = 75170 (30745 source, 44425 target) ; Learning rate = 0.000302 ; Loss = 1.424231\n",
      "2024-12-06 00:26:37.364000: I runner.py:310] Step = 85600 ; steps/s = 1.66, tokens/s = 74399 (30428 source, 43971 target) ; Learning rate = 0.000302 ; Loss = 1.419251\n",
      "2024-12-06 00:27:38.202000: I runner.py:310] Step = 85700 ; steps/s = 1.64, tokens/s = 75218 (30758 source, 44460 target) ; Learning rate = 0.000302 ; Loss = 1.422058\n",
      "2024-12-06 00:28:39.049000: I runner.py:310] Step = 85800 ; steps/s = 1.64, tokens/s = 75183 (30738 source, 44445 target) ; Learning rate = 0.000302 ; Loss = 1.423166\n",
      "2024-12-06 00:29:39.435000: I runner.py:310] Step = 85900 ; steps/s = 1.66, tokens/s = 74458 (30472 source, 43986 target) ; Learning rate = 0.000302 ; Loss = 1.420337\n",
      "2024-12-06 00:30:40.169000: I runner.py:310] Step = 86000 ; steps/s = 1.65, tokens/s = 75333 (30799 source, 44534 target) ; Learning rate = 0.000301 ; Loss = 1.426426\n",
      "2024-12-06 00:31:40.602000: I runner.py:310] Step = 86100 ; steps/s = 1.65, tokens/s = 74387 (30423 source, 43964 target) ; Learning rate = 0.000301 ; Loss = 1.419078\n",
      "2024-12-06 00:32:41.436000: I runner.py:310] Step = 86200 ; steps/s = 1.64, tokens/s = 75218 (30756 source, 44462 target) ; Learning rate = 0.000301 ; Loss = 1.420486\n",
      "2024-12-06 00:33:42.313000: I runner.py:310] Step = 86300 ; steps/s = 1.64, tokens/s = 75150 (30723 source, 44427 target) ; Learning rate = 0.000301 ; Loss = 1.421259\n",
      "2024-12-06 00:34:42.695000: I runner.py:310] Step = 86400 ; steps/s = 1.66, tokens/s = 74461 (30466 source, 43995 target) ; Learning rate = 0.000301 ; Loss = 1.425121\n",
      "2024-12-06 00:35:43.539000: I runner.py:310] Step = 86500 ; steps/s = 1.64, tokens/s = 75222 (30767 source, 44455 target) ; Learning rate = 0.000301 ; Loss = 1.424064\n",
      "2024-12-06 00:36:43.936000: I runner.py:310] Step = 86600 ; steps/s = 1.66, tokens/s = 74432 (30446 source, 43986 target) ; Learning rate = 0.000300 ; Loss = 1.419754\n",
      "2024-12-06 00:37:44.753000: I runner.py:310] Step = 86700 ; steps/s = 1.64, tokens/s = 75210 (30742 source, 44468 target) ; Learning rate = 0.000300 ; Loss = 1.419510\n",
      "2024-12-06 00:38:45.532000: I runner.py:310] Step = 86800 ; steps/s = 1.65, tokens/s = 75275 (30770 source, 44505 target) ; Learning rate = 0.000300 ; Loss = 1.425594\n",
      "2024-12-06 00:39:45.921000: I runner.py:310] Step = 86900 ; steps/s = 1.66, tokens/s = 74474 (30486 source, 43988 target) ; Learning rate = 0.000300 ; Loss = 1.424313\n",
      "2024-12-06 00:40:46.708000: I runner.py:310] Step = 87000 ; steps/s = 1.65, tokens/s = 75264 (30771 source, 44493 target) ; Learning rate = 0.000300 ; Loss = 1.424272\n",
      "2024-12-06 00:41:47.096000: I runner.py:310] Step = 87100 ; steps/s = 1.66, tokens/s = 74446 (30449 source, 43997 target) ; Learning rate = 0.000299 ; Loss = 1.419325\n",
      "2024-12-06 00:42:47.859000: I runner.py:310] Step = 87200 ; steps/s = 1.65, tokens/s = 75271 (30761 source, 44510 target) ; Learning rate = 0.000299 ; Loss = 1.423518\n",
      "2024-12-06 00:43:48.622000: I runner.py:310] Step = 87300 ; steps/s = 1.65, tokens/s = 75314 (30798 source, 44516 target) ; Learning rate = 0.000299 ; Loss = 1.421624\n",
      "2024-12-06 00:44:48.906000: I runner.py:310] Step = 87400 ; steps/s = 1.66, tokens/s = 74582 (30521 source, 44061 target) ; Learning rate = 0.000299 ; Loss = 1.422561\n",
      "2024-12-06 00:45:50.346000: I runner.py:310] Step = 87500 ; steps/s = 1.63, tokens/s = 74464 (30439 source, 44025 target) ; Learning rate = 0.000299 ; Loss = 1.425054\n",
      "2024-12-06 00:46:52.695000: I runner.py:310] Step = 87600 ; steps/s = 1.60, tokens/s = 72134 (29521 source, 42613 target) ; Learning rate = 0.000299 ; Loss = 1.419402\n",
      "2024-12-06 00:47:54.073000: I runner.py:310] Step = 87700 ; steps/s = 1.63, tokens/s = 74532 (30468 source, 44064 target) ; Learning rate = 0.000298 ; Loss = 1.419358\n",
      "2024-12-06 00:48:54.922000: I runner.py:310] Step = 87800 ; steps/s = 1.64, tokens/s = 75200 (30749 source, 44451 target) ; Learning rate = 0.000298 ; Loss = 1.425400\n",
      "2024-12-06 00:49:55.515000: I runner.py:310] Step = 87900 ; steps/s = 1.65, tokens/s = 74189 (30351 source, 43838 target) ; Learning rate = 0.000298 ; Loss = 1.418340\n",
      "2024-12-06 00:50:56.404000: I runner.py:310] Step = 88000 ; steps/s = 1.64, tokens/s = 75158 (30736 source, 44422 target) ; Learning rate = 0.000298 ; Loss = 1.425027\n",
      "2024-12-06 00:51:56.860000: I runner.py:310] Step = 88100 ; steps/s = 1.65, tokens/s = 74381 (30436 source, 43945 target) ; Learning rate = 0.000298 ; Loss = 1.420402\n",
      "2024-12-06 00:52:57.765000: I runner.py:310] Step = 88200 ; steps/s = 1.64, tokens/s = 75106 (30699 source, 44407 target) ; Learning rate = 0.000298 ; Loss = 1.428697\n",
      "2024-12-06 00:53:58.680000: I runner.py:310] Step = 88300 ; steps/s = 1.64, tokens/s = 75097 (30694 source, 44403 target) ; Learning rate = 0.000297 ; Loss = 1.421568\n",
      "2024-12-06 00:54:59.177000: I runner.py:310] Step = 88400 ; steps/s = 1.65, tokens/s = 74323 (30409 source, 43914 target) ; Learning rate = 0.000297 ; Loss = 1.420562\n",
      "2024-12-06 00:56:00.113000: I runner.py:310] Step = 88500 ; steps/s = 1.64, tokens/s = 75098 (30709 source, 44389 target) ; Learning rate = 0.000297 ; Loss = 1.421713\n",
      "2024-12-06 00:57:00.614000: I runner.py:310] Step = 88600 ; steps/s = 1.65, tokens/s = 74329 (30419 source, 43910 target) ; Learning rate = 0.000297 ; Loss = 1.438577\n",
      "2024-12-06 00:58:01.498000: I runner.py:310] Step = 88700 ; steps/s = 1.64, tokens/s = 75120 (30703 source, 44417 target) ; Learning rate = 0.000297 ; Loss = 1.418509\n",
      "2024-12-06 00:59:02.408000: I runner.py:310] Step = 88800 ; steps/s = 1.64, tokens/s = 75110 (30704 source, 44406 target) ; Learning rate = 0.000297 ; Loss = 1.421910\n",
      "2024-12-06 01:00:02.887000: I runner.py:310] Step = 88900 ; steps/s = 1.65, tokens/s = 74352 (30424 source, 43928 target) ; Learning rate = 0.000296 ; Loss = 1.419443\n",
      "2024-12-06 01:01:03.774000: I runner.py:310] Step = 89000 ; steps/s = 1.64, tokens/s = 75130 (30711 source, 44419 target) ; Learning rate = 0.000296 ; Loss = 1.424388\n",
      "2024-12-06 01:02:04.754000: I runner.py:310] Step = 89100 ; steps/s = 1.64, tokens/s = 75032 (30680 source, 44352 target) ; Learning rate = 0.000296 ; Loss = 1.422786\n",
      "2024-12-06 01:03:05.211000: I runner.py:310] Step = 89200 ; steps/s = 1.65, tokens/s = 74378 (30434 source, 43944 target) ; Learning rate = 0.000296 ; Loss = 1.419664\n",
      "2024-12-06 01:04:06.114000: I runner.py:310] Step = 89300 ; steps/s = 1.64, tokens/s = 75101 (30693 source, 44408 target) ; Learning rate = 0.000296 ; Loss = 1.423119\n",
      "2024-12-06 01:05:06.553000: I runner.py:310] Step = 89400 ; steps/s = 1.65, tokens/s = 74412 (30455 source, 43957 target) ; Learning rate = 0.000296 ; Loss = 1.422392\n",
      "2024-12-06 01:06:07.436000: I runner.py:310] Step = 89500 ; steps/s = 1.64, tokens/s = 75160 (30732 source, 44428 target) ; Learning rate = 0.000295 ; Loss = 1.423811\n",
      "2024-12-06 01:07:08.373000: I runner.py:310] Step = 89600 ; steps/s = 1.64, tokens/s = 75060 (30679 source, 44381 target) ; Learning rate = 0.000295 ; Loss = 1.422637\n",
      "2024-12-06 01:08:08.774000: I runner.py:310] Step = 89700 ; steps/s = 1.66, tokens/s = 74432 (30455 source, 43977 target) ; Learning rate = 0.000295 ; Loss = 1.420901\n",
      "2024-12-06 01:09:09.703000: I runner.py:310] Step = 89800 ; steps/s = 1.64, tokens/s = 75088 (30696 source, 44392 target) ; Learning rate = 0.000295 ; Loss = 1.420059\n",
      "2024-12-06 01:10:10.209000: I runner.py:310] Step = 89900 ; steps/s = 1.65, tokens/s = 74324 (30415 source, 43909 target) ; Learning rate = 0.000295 ; Loss = 1.423614\n",
      "2024-12-06 01:11:11.129000: I runner.py:310] Step = 90000 ; steps/s = 1.64, tokens/s = 75106 (30704 source, 44402 target) ; Learning rate = 0.000295 ; Loss = 1.420232\n",
      "2024-12-06 01:11:13.140000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-90000\n",
      "2024-12-06 01:11:13.141000: I training.py:192] Running evaluation for step 90000\n",
      "2024-12-06 01:12:02.206000: I training.py:192] Evaluation result for step 90000: loss = 0.988385 ; perplexity = 2.686891\n",
      "2024-12-06 01:13:02.971000: I runner.py:310] Step = 90100 ; steps/s = 1.65, tokens/s = 75302 (30787 source, 44515 target) ; Learning rate = 0.000294 ; Loss = 1.422603\n",
      "2024-12-06 01:14:03.437000: I runner.py:310] Step = 90200 ; steps/s = 1.65, tokens/s = 74358 (30418 source, 43940 target) ; Learning rate = 0.000294 ; Loss = 1.419924\n",
      "2024-12-06 01:15:04.288000: I runner.py:310] Step = 90300 ; steps/s = 1.64, tokens/s = 75184 (30736 source, 44448 target) ; Learning rate = 0.000294 ; Loss = 1.418443\n",
      "2024-12-06 01:16:04.818000: I runner.py:310] Step = 90400 ; steps/s = 1.65, tokens/s = 74287 (30397 source, 43890 target) ; Learning rate = 0.000294 ; Loss = 1.421416\n",
      "2024-12-06 01:17:05.734000: I runner.py:310] Step = 90500 ; steps/s = 1.64, tokens/s = 75094 (30685 source, 44409 target) ; Learning rate = 0.000294 ; Loss = 1.417062\n",
      "2024-12-06 01:18:06.601000: I runner.py:310] Step = 90600 ; steps/s = 1.64, tokens/s = 75173 (30741 source, 44432 target) ; Learning rate = 0.000294 ; Loss = 1.420227\n",
      "2024-12-06 01:19:07.072000: I runner.py:310] Step = 90700 ; steps/s = 1.65, tokens/s = 74358 (30431 source, 43927 target) ; Learning rate = 0.000293 ; Loss = 1.420746\n",
      "2024-12-06 01:20:08.030000: I runner.py:310] Step = 90800 ; steps/s = 1.64, tokens/s = 75071 (30693 source, 44378 target) ; Learning rate = 0.000293 ; Loss = 1.421421\n",
      "2024-12-06 01:21:08.560000: I runner.py:310] Step = 90900 ; steps/s = 1.65, tokens/s = 74280 (30396 source, 43884 target) ; Learning rate = 0.000293 ; Loss = 1.416414\n",
      "2024-12-06 01:22:09.422000: I runner.py:310] Step = 91000 ; steps/s = 1.64, tokens/s = 75172 (30727 source, 44445 target) ; Learning rate = 0.000293 ; Loss = 1.423986\n",
      "2024-12-06 01:23:10.380000: I runner.py:310] Step = 91100 ; steps/s = 1.64, tokens/s = 75053 (30686 source, 44367 target) ; Learning rate = 0.000293 ; Loss = 1.421460\n",
      "2024-12-06 01:24:10.884000: I runner.py:310] Step = 91200 ; steps/s = 1.65, tokens/s = 74301 (30397 source, 43904 target) ; Learning rate = 0.000293 ; Loss = 1.420455\n",
      "2024-12-06 01:25:11.782000: I runner.py:310] Step = 91300 ; steps/s = 1.64, tokens/s = 75149 (30730 source, 44419 target) ; Learning rate = 0.000293 ; Loss = 1.417213\n",
      "2024-12-06 01:26:12.177000: I runner.py:310] Step = 91400 ; steps/s = 1.66, tokens/s = 74438 (30457 source, 43981 target) ; Learning rate = 0.000292 ; Loss = 1.419007\n",
      "2024-12-06 01:27:13.137000: I runner.py:310] Step = 91500 ; steps/s = 1.64, tokens/s = 75064 (30697 source, 44367 target) ; Learning rate = 0.000292 ; Loss = 1.420842\n",
      "2024-12-06 01:28:14.002000: I runner.py:310] Step = 91600 ; steps/s = 1.64, tokens/s = 75161 (30723 source, 44438 target) ; Learning rate = 0.000292 ; Loss = 1.423288\n",
      "2024-12-06 01:29:14.466000: I runner.py:310] Step = 91700 ; steps/s = 1.65, tokens/s = 74352 (30413 source, 43939 target) ; Learning rate = 0.000292 ; Loss = 1.420825\n",
      "2024-12-06 01:30:15.386000: I runner.py:310] Step = 91800 ; steps/s = 1.64, tokens/s = 75100 (30708 source, 44392 target) ; Learning rate = 0.000292 ; Loss = 1.418623\n",
      "2024-12-06 01:31:15.822000: I runner.py:310] Step = 91900 ; steps/s = 1.65, tokens/s = 74407 (30443 source, 43964 target) ; Learning rate = 0.000292 ; Loss = 1.415201\n",
      "2024-12-06 01:32:16.731000: I runner.py:310] Step = 92000 ; steps/s = 1.64, tokens/s = 75118 (30715 source, 44403 target) ; Learning rate = 0.000291 ; Loss = 1.417996\n",
      "2024-12-06 01:33:17.604000: I runner.py:310] Step = 92100 ; steps/s = 1.64, tokens/s = 75154 (30722 source, 44432 target) ; Learning rate = 0.000291 ; Loss = 1.419845\n",
      "2024-12-06 01:34:18.104000: I runner.py:310] Step = 92200 ; steps/s = 1.65, tokens/s = 74315 (30400 source, 43915 target) ; Learning rate = 0.000291 ; Loss = 1.421205\n",
      "2024-12-06 01:35:18.970000: I runner.py:310] Step = 92300 ; steps/s = 1.64, tokens/s = 75168 (30725 source, 44443 target) ; Learning rate = 0.000291 ; Loss = 1.421870\n",
      "2024-12-06 01:36:19.474000: I runner.py:310] Step = 92400 ; steps/s = 1.65, tokens/s = 74334 (30430 source, 43904 target) ; Learning rate = 0.000291 ; Loss = 1.419832\n",
      "2024-12-06 01:37:20.327000: I runner.py:310] Step = 92500 ; steps/s = 1.64, tokens/s = 75178 (30729 source, 44449 target) ; Learning rate = 0.000291 ; Loss = 1.421997\n",
      "2024-12-06 01:38:21.267000: I runner.py:310] Step = 92600 ; steps/s = 1.64, tokens/s = 75057 (30681 source, 44376 target) ; Learning rate = 0.000290 ; Loss = 1.422482\n",
      "2024-12-06 01:39:21.733000: I runner.py:310] Step = 92700 ; steps/s = 1.65, tokens/s = 74352 (30422 source, 43930 target) ; Learning rate = 0.000290 ; Loss = 1.422444\n",
      "2024-12-06 01:40:22.625000: I runner.py:310] Step = 92800 ; steps/s = 1.64, tokens/s = 75125 (30698 source, 44427 target) ; Learning rate = 0.000290 ; Loss = 1.422744\n",
      "2024-12-06 01:41:23.540000: I runner.py:310] Step = 92900 ; steps/s = 1.64, tokens/s = 75139 (30737 source, 44402 target) ; Learning rate = 0.000290 ; Loss = 1.425196\n",
      "2024-12-06 01:42:23.990000: I runner.py:310] Step = 93000 ; steps/s = 1.65, tokens/s = 74382 (30434 source, 43948 target) ; Learning rate = 0.000290 ; Loss = 1.417861\n",
      "2024-12-06 01:43:24.929000: I runner.py:310] Step = 93100 ; steps/s = 1.64, tokens/s = 75085 (30704 source, 44381 target) ; Learning rate = 0.000290 ; Loss = 1.418628\n",
      "2024-12-06 01:44:25.409000: I runner.py:310] Step = 93200 ; steps/s = 1.65, tokens/s = 74321 (30397 source, 43924 target) ; Learning rate = 0.000290 ; Loss = 1.420762\n",
      "2024-12-06 01:45:26.274000: I runner.py:310] Step = 93300 ; steps/s = 1.64, tokens/s = 75145 (30710 source, 44435 target) ; Learning rate = 0.000289 ; Loss = 1.422308\n",
      "2024-12-06 01:46:27.192000: I runner.py:310] Step = 93400 ; steps/s = 1.64, tokens/s = 75128 (30727 source, 44401 target) ; Learning rate = 0.000289 ; Loss = 1.419951\n",
      "2024-12-06 01:47:27.661000: I runner.py:310] Step = 93500 ; steps/s = 1.65, tokens/s = 74355 (30422 source, 43933 target) ; Learning rate = 0.000289 ; Loss = 1.421378\n",
      "2024-12-06 01:48:28.610000: I runner.py:310] Step = 93600 ; steps/s = 1.64, tokens/s = 75073 (30695 source, 44378 target) ; Learning rate = 0.000289 ; Loss = 1.422920\n",
      "2024-12-06 01:49:29.085000: I runner.py:310] Step = 93700 ; steps/s = 1.65, tokens/s = 74352 (30425 source, 43927 target) ; Learning rate = 0.000289 ; Loss = 1.419289\n",
      "2024-12-06 01:50:30.002000: I runner.py:310] Step = 93800 ; steps/s = 1.64, tokens/s = 75111 (30709 source, 44402 target) ; Learning rate = 0.000289 ; Loss = 1.421914\n",
      "2024-12-06 01:51:30.893000: I runner.py:310] Step = 93900 ; steps/s = 1.64, tokens/s = 75128 (30712 source, 44416 target) ; Learning rate = 0.000288 ; Loss = 1.421557\n",
      "2024-12-06 01:52:31.395000: I runner.py:310] Step = 94000 ; steps/s = 1.65, tokens/s = 74297 (30386 source, 43911 target) ; Learning rate = 0.000288 ; Loss = 1.418102\n",
      "2024-12-06 01:53:32.329000: I runner.py:310] Step = 94100 ; steps/s = 1.64, tokens/s = 75113 (30724 source, 44389 target) ; Learning rate = 0.000288 ; Loss = 1.422250\n",
      "2024-12-06 01:54:32.817000: I runner.py:310] Step = 94200 ; steps/s = 1.65, tokens/s = 74322 (30400 source, 43922 target) ; Learning rate = 0.000288 ; Loss = 1.418061\n",
      "2024-12-06 01:55:33.690000: I runner.py:310] Step = 94300 ; steps/s = 1.64, tokens/s = 75150 (30724 source, 44426 target) ; Learning rate = 0.000288 ; Loss = 1.422675\n",
      "2024-12-06 01:56:34.550000: I runner.py:310] Step = 94400 ; steps/s = 1.64, tokens/s = 75184 (30734 source, 44450 target) ; Learning rate = 0.000288 ; Loss = 1.422310\n",
      "2024-12-06 01:57:35.054000: I runner.py:310] Step = 94500 ; steps/s = 1.65, tokens/s = 74302 (30397 source, 43905 target) ; Learning rate = 0.000288 ; Loss = 1.421273\n",
      "2024-12-06 01:58:35.970000: I runner.py:310] Step = 94600 ; steps/s = 1.64, tokens/s = 75131 (30726 source, 44405 target) ; Learning rate = 0.000287 ; Loss = 1.419848\n",
      "2024-12-06 01:59:36.508000: I runner.py:310] Step = 94700 ; steps/s = 1.65, tokens/s = 74276 (30394 source, 43882 target) ; Learning rate = 0.000287 ; Loss = 1.413931\n",
      "2024-12-06 02:00:37.427000: I runner.py:310] Step = 94800 ; steps/s = 1.64, tokens/s = 75086 (30687 source, 44399 target) ; Learning rate = 0.000287 ; Loss = 1.422237\n",
      "2024-12-06 02:01:38.338000: I runner.py:310] Step = 94900 ; steps/s = 1.64, tokens/s = 75101 (30699 source, 44402 target) ; Learning rate = 0.000287 ; Loss = 1.419464\n",
      "2024-12-06 02:02:38.827000: I runner.py:310] Step = 95000 ; steps/s = 1.65, tokens/s = 74353 (30433 source, 43920 target) ; Learning rate = 0.000287 ; Loss = 1.420259\n",
      "2024-12-06 02:02:38.828000: I training.py:192] Running evaluation for step 95000\n",
      "2024-12-06 02:03:24.749000: I training.py:192] Evaluation result for step 95000: loss = 0.993214 ; perplexity = 2.699897\n",
      "2024-12-06 02:04:25.497000: I runner.py:310] Step = 95100 ; steps/s = 1.65, tokens/s = 75359 (30822 source, 44537 target) ; Learning rate = 0.000287 ; Loss = 1.421732\n",
      "2024-12-06 02:05:25.963000: I runner.py:310] Step = 95200 ; steps/s = 1.65, tokens/s = 74344 (30412 source, 43932 target) ; Learning rate = 0.000286 ; Loss = 1.416869\n",
      "2024-12-06 02:06:26.883000: I runner.py:310] Step = 95300 ; steps/s = 1.64, tokens/s = 75083 (30686 source, 44397 target) ; Learning rate = 0.000286 ; Loss = 1.424140\n",
      "2024-12-06 02:07:27.784000: I runner.py:310] Step = 95400 ; steps/s = 1.64, tokens/s = 75124 (30714 source, 44410 target) ; Learning rate = 0.000286 ; Loss = 1.418801\n",
      "2024-12-06 02:08:28.264000: I runner.py:310] Step = 95500 ; steps/s = 1.65, tokens/s = 74349 (30425 source, 43924 target) ; Learning rate = 0.000286 ; Loss = 1.421826\n",
      "2024-12-06 02:09:29.180000: I runner.py:310] Step = 95600 ; steps/s = 1.64, tokens/s = 75123 (30719 source, 44404 target) ; Learning rate = 0.000286 ; Loss = 1.421479\n",
      "2024-12-06 02:10:29.691000: I runner.py:310] Step = 95700 ; steps/s = 1.65, tokens/s = 74291 (30390 source, 43901 target) ; Learning rate = 0.000286 ; Loss = 1.413989\n",
      "2024-12-06 02:11:30.614000: I runner.py:310] Step = 95800 ; steps/s = 1.64, tokens/s = 75099 (30704 source, 44395 target) ; Learning rate = 0.000286 ; Loss = 1.415490\n",
      "2024-12-06 02:12:31.502000: I runner.py:310] Step = 95900 ; steps/s = 1.64, tokens/s = 75135 (30711 source, 44424 target) ; Learning rate = 0.000285 ; Loss = 1.422525\n",
      "2024-12-06 02:13:32.019000: I runner.py:310] Step = 96000 ; steps/s = 1.65, tokens/s = 74306 (30404 source, 43902 target) ; Learning rate = 0.000285 ; Loss = 1.421680\n",
      "2024-12-06 02:14:32.885000: I runner.py:310] Step = 96100 ; steps/s = 1.64, tokens/s = 75153 (30721 source, 44432 target) ; Learning rate = 0.000285 ; Loss = 1.419076\n",
      "2024-12-06 02:15:33.433000: I runner.py:310] Step = 96200 ; steps/s = 1.65, tokens/s = 74286 (30406 source, 43880 target) ; Learning rate = 0.000285 ; Loss = 1.419264\n",
      "2024-12-06 02:16:34.316000: I runner.py:310] Step = 96300 ; steps/s = 1.64, tokens/s = 75134 (30706 source, 44428 target) ; Learning rate = 0.000285 ; Loss = 1.420107\n",
      "2024-12-06 02:17:35.224000: I runner.py:310] Step = 96400 ; steps/s = 1.64, tokens/s = 75121 (30720 source, 44401 target) ; Learning rate = 0.000285 ; Loss = 1.421700\n",
      "2024-12-06 02:18:35.653000: I runner.py:310] Step = 96500 ; steps/s = 1.66, tokens/s = 74400 (30439 source, 43961 target) ; Learning rate = 0.000285 ; Loss = 1.418190\n",
      "2024-12-06 02:19:36.522000: I runner.py:310] Step = 96600 ; steps/s = 1.64, tokens/s = 75153 (30722 source, 44431 target) ; Learning rate = 0.000284 ; Loss = 1.414183\n",
      "2024-12-06 02:20:37.128000: I runner.py:310] Step = 96700 ; steps/s = 1.65, tokens/s = 74597 (30528 source, 44069 target) ; Learning rate = 0.000284 ; Loss = 1.436712\n",
      "2024-12-06 02:21:37.936000: I runner.py:310] Step = 96800 ; steps/s = 1.64, tokens/s = 74848 (30594 source, 44254 target) ; Learning rate = 0.000284 ; Loss = 1.418431\n",
      "2024-12-06 02:22:38.911000: I runner.py:310] Step = 96900 ; steps/s = 1.64, tokens/s = 75024 (30669 source, 44355 target) ; Learning rate = 0.000284 ; Loss = 1.420014\n",
      "2024-12-06 02:23:39.443000: I runner.py:310] Step = 97000 ; steps/s = 1.65, tokens/s = 74271 (30382 source, 43889 target) ; Learning rate = 0.000284 ; Loss = 1.414345\n",
      "2024-12-06 02:24:40.294000: I runner.py:310] Step = 97100 ; steps/s = 1.64, tokens/s = 75191 (30743 source, 44448 target) ; Learning rate = 0.000284 ; Loss = 1.421071\n",
      "2024-12-06 02:25:41.233000: I runner.py:310] Step = 97200 ; steps/s = 1.64, tokens/s = 75083 (30698 source, 44385 target) ; Learning rate = 0.000284 ; Loss = 1.420105\n",
      "2024-12-06 02:26:41.644000: I runner.py:310] Step = 97300 ; steps/s = 1.66, tokens/s = 74446 (30474 source, 43972 target) ; Learning rate = 0.000283 ; Loss = 1.418923\n",
      "2024-12-06 02:27:42.576000: I runner.py:310] Step = 97400 ; steps/s = 1.64, tokens/s = 75071 (30678 source, 44393 target) ; Learning rate = 0.000283 ; Loss = 1.420421\n",
      "2024-12-06 02:28:43.026000: I runner.py:310] Step = 97500 ; steps/s = 1.65, tokens/s = 74368 (30423 source, 43945 target) ; Learning rate = 0.000283 ; Loss = 1.418884\n",
      "2024-12-06 02:29:43.911000: I runner.py:310] Step = 97600 ; steps/s = 1.64, tokens/s = 75142 (30716 source, 44426 target) ; Learning rate = 0.000283 ; Loss = 1.419009\n",
      "2024-12-06 02:30:44.834000: I runner.py:310] Step = 97700 ; steps/s = 1.64, tokens/s = 75115 (30722 source, 44393 target) ; Learning rate = 0.000283 ; Loss = 1.419464\n",
      "2024-12-06 02:31:45.300000: I runner.py:310] Step = 97800 ; steps/s = 1.65, tokens/s = 74344 (30413 source, 43931 target) ; Learning rate = 0.000283 ; Loss = 1.417249\n",
      "2024-12-06 02:32:46.173000: I runner.py:310] Step = 97900 ; steps/s = 1.64, tokens/s = 75182 (30747 source, 44435 target) ; Learning rate = 0.000282 ; Loss = 1.422003\n",
      "2024-12-06 02:33:46.633000: I runner.py:310] Step = 98000 ; steps/s = 1.65, tokens/s = 74362 (30422 source, 43940 target) ; Learning rate = 0.000282 ; Loss = 1.416905\n",
      "2024-12-06 02:34:47.560000: I runner.py:310] Step = 98100 ; steps/s = 1.64, tokens/s = 75076 (30689 source, 44387 target) ; Learning rate = 0.000282 ; Loss = 1.418416\n",
      "2024-12-06 02:35:48.452000: I runner.py:310] Step = 98200 ; steps/s = 1.64, tokens/s = 75130 (30709 source, 44421 target) ; Learning rate = 0.000282 ; Loss = 1.420762\n",
      "2024-12-06 02:36:48.995000: I runner.py:310] Step = 98300 ; steps/s = 1.65, tokens/s = 74262 (30377 source, 43885 target) ; Learning rate = 0.000282 ; Loss = 1.418680\n",
      "2024-12-06 02:37:49.903000: I runner.py:310] Step = 98400 ; steps/s = 1.64, tokens/s = 75116 (30713 source, 44403 target) ; Learning rate = 0.000282 ; Loss = 1.417433\n",
      "2024-12-06 02:38:50.343000: I runner.py:310] Step = 98500 ; steps/s = 1.65, tokens/s = 74406 (30456 source, 43950 target) ; Learning rate = 0.000282 ; Loss = 1.417006\n",
      "2024-12-06 02:39:51.221000: I runner.py:310] Step = 98600 ; steps/s = 1.64, tokens/s = 75161 (30733 source, 44428 target) ; Learning rate = 0.000281 ; Loss = 1.417885\n",
      "2024-12-06 02:40:52.125000: I runner.py:310] Step = 98700 ; steps/s = 1.64, tokens/s = 75114 (30703 source, 44411 target) ; Learning rate = 0.000281 ; Loss = 1.421162\n",
      "2024-12-06 02:41:52.564000: I runner.py:310] Step = 98800 ; steps/s = 1.65, tokens/s = 74396 (30440 source, 43956 target) ; Learning rate = 0.000281 ; Loss = 1.422758\n",
      "2024-12-06 02:42:53.422000: I runner.py:310] Step = 98900 ; steps/s = 1.64, tokens/s = 75185 (30743 source, 44442 target) ; Learning rate = 0.000281 ; Loss = 1.420915\n",
      "2024-12-06 02:43:53.949000: I runner.py:310] Step = 99000 ; steps/s = 1.65, tokens/s = 74276 (30387 source, 43889 target) ; Learning rate = 0.000281 ; Loss = 1.421286\n",
      "2024-12-06 02:44:54.843000: I runner.py:310] Step = 99100 ; steps/s = 1.64, tokens/s = 75120 (30707 source, 44413 target) ; Learning rate = 0.000281 ; Loss = 1.415049\n",
      "2024-12-06 02:45:55.772000: I runner.py:310] Step = 99200 ; steps/s = 1.64, tokens/s = 75097 (30703 source, 44394 target) ; Learning rate = 0.000281 ; Loss = 1.422522\n",
      "2024-12-06 02:46:56.248000: I runner.py:310] Step = 99300 ; steps/s = 1.65, tokens/s = 74338 (30416 source, 43922 target) ; Learning rate = 0.000280 ; Loss = 1.419031\n",
      "2024-12-06 02:47:57.177000: I runner.py:310] Step = 99400 ; steps/s = 1.64, tokens/s = 75077 (30686 source, 44391 target) ; Learning rate = 0.000280 ; Loss = 1.423348\n",
      "2024-12-06 02:48:57.622000: I runner.py:310] Step = 99500 ; steps/s = 1.65, tokens/s = 74395 (30442 source, 43953 target) ; Learning rate = 0.000280 ; Loss = 1.417003\n",
      "2024-12-06 02:49:58.567000: I runner.py:310] Step = 99600 ; steps/s = 1.64, tokens/s = 75077 (30701 source, 44376 target) ; Learning rate = 0.000280 ; Loss = 1.418889\n",
      "2024-12-06 02:50:59.459000: I runner.py:310] Step = 99700 ; steps/s = 1.64, tokens/s = 75157 (30731 source, 44426 target) ; Learning rate = 0.000280 ; Loss = 1.422153\n",
      "2024-12-06 02:51:59.971000: I runner.py:310] Step = 99800 ; steps/s = 1.65, tokens/s = 74293 (30394 source, 43899 target) ; Learning rate = 0.000280 ; Loss = 1.419443\n",
      "2024-12-06 02:53:00.863000: I runner.py:310] Step = 99900 ; steps/s = 1.64, tokens/s = 75119 (30696 source, 44423 target) ; Learning rate = 0.000280 ; Loss = 1.419658\n",
      "2024-12-06 02:54:01.415000: I runner.py:310] Step = 100000 ; steps/s = 1.65, tokens/s = 74284 (30412 source, 43872 target) ; Learning rate = 0.000280 ; Loss = 1.418326\n",
      "2024-12-06 02:54:03.408000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-100000\n",
      "2024-12-06 02:54:03.408000: I training.py:192] Running evaluation for step 100000\n",
      "2024-12-06 02:54:50.425000: I training.py:192] Evaluation result for step 100000: loss = 0.998566 ; perplexity = 2.714386\n",
      "2024-12-06 02:55:51.285000: I runner.py:310] Step = 100100 ; steps/s = 1.64, tokens/s = 75187 (30735 source, 44452 target) ; Learning rate = 0.000279 ; Loss = 1.420073\n",
      "2024-12-06 02:56:52.234000: I runner.py:310] Step = 100200 ; steps/s = 1.64, tokens/s = 75033 (30665 source, 44368 target) ; Learning rate = 0.000279 ; Loss = 1.415920\n",
      "2024-12-06 02:57:52.733000: I runner.py:310] Step = 100300 ; steps/s = 1.65, tokens/s = 74329 (30416 source, 43913 target) ; Learning rate = 0.000279 ; Loss = 1.420667\n",
      "2024-12-06 02:58:53.588000: I runner.py:310] Step = 100400 ; steps/s = 1.64, tokens/s = 75172 (30728 source, 44444 target) ; Learning rate = 0.000279 ; Loss = 1.416480\n",
      "2024-12-06 02:59:54.056000: I runner.py:310] Step = 100500 ; steps/s = 1.65, tokens/s = 74383 (30448 source, 43935 target) ; Learning rate = 0.000279 ; Loss = 1.420083\n",
      "2024-12-06 03:00:54.951000: I runner.py:310] Step = 100600 ; steps/s = 1.64, tokens/s = 75113 (30697 source, 44416 target) ; Learning rate = 0.000279 ; Loss = 1.418004\n",
      "2024-12-06 03:01:55.915000: I runner.py:310] Step = 100700 ; steps/s = 1.64, tokens/s = 75038 (30672 source, 44366 target) ; Learning rate = 0.000279 ; Loss = 1.419114\n",
      "2024-12-06 03:02:56.436000: I runner.py:310] Step = 100800 ; steps/s = 1.65, tokens/s = 74311 (30415 source, 43896 target) ; Learning rate = 0.000278 ; Loss = 1.424137\n",
      "2024-12-06 03:03:57.383000: I runner.py:310] Step = 100900 ; steps/s = 1.64, tokens/s = 75047 (30668 source, 44379 target) ; Learning rate = 0.000278 ; Loss = 1.419044\n",
      "2024-12-06 03:04:58.299000: I runner.py:310] Step = 101000 ; steps/s = 1.64, tokens/s = 75124 (30724 source, 44400 target) ; Learning rate = 0.000278 ; Loss = 1.419074\n",
      "2024-12-06 03:05:58.750000: I runner.py:310] Step = 101100 ; steps/s = 1.65, tokens/s = 74377 (30435 source, 43942 target) ; Learning rate = 0.000278 ; Loss = 1.415475\n",
      "2024-12-06 03:06:59.640000: I runner.py:310] Step = 101200 ; steps/s = 1.64, tokens/s = 75140 (30717 source, 44423 target) ; Learning rate = 0.000278 ; Loss = 1.420062\n",
      "2024-12-06 03:08:00.099000: I runner.py:310] Step = 101300 ; steps/s = 1.65, tokens/s = 74380 (30441 source, 43939 target) ; Learning rate = 0.000278 ; Loss = 1.420454\n",
      "2024-12-06 03:09:00.967000: I runner.py:310] Step = 101400 ; steps/s = 1.64, tokens/s = 75159 (30723 source, 44436 target) ; Learning rate = 0.000278 ; Loss = 1.420569\n",
      "2024-12-06 03:10:01.962000: I runner.py:310] Step = 101500 ; steps/s = 1.64, tokens/s = 74991 (30648 source, 44343 target) ; Learning rate = 0.000277 ; Loss = 1.419943\n",
      "2024-12-06 03:11:02.402000: I runner.py:310] Step = 101600 ; steps/s = 1.65, tokens/s = 74400 (30443 source, 43957 target) ; Learning rate = 0.000277 ; Loss = 1.415856\n",
      "2024-12-06 03:12:03.272000: I runner.py:310] Step = 101700 ; steps/s = 1.64, tokens/s = 75176 (30746 source, 44430 target) ; Learning rate = 0.000277 ; Loss = 1.419087\n",
      "2024-12-06 03:13:03.809000: I runner.py:310] Step = 101800 ; steps/s = 1.65, tokens/s = 74250 (30370 source, 43880 target) ; Learning rate = 0.000277 ; Loss = 1.415786\n",
      "2024-12-06 03:14:04.719000: I runner.py:310] Step = 101900 ; steps/s = 1.64, tokens/s = 75103 (30696 source, 44407 target) ; Learning rate = 0.000277 ; Loss = 1.419457\n",
      "2024-12-06 03:15:05.612000: I runner.py:310] Step = 102000 ; steps/s = 1.64, tokens/s = 75153 (30734 source, 44419 target) ; Learning rate = 0.000277 ; Loss = 1.421213\n",
      "2024-12-06 03:16:06.113000: I runner.py:310] Step = 102100 ; steps/s = 1.65, tokens/s = 74314 (30409 source, 43905 target) ; Learning rate = 0.000277 ; Loss = 1.418308\n",
      "2024-12-06 03:17:07.017000: I runner.py:310] Step = 102200 ; steps/s = 1.64, tokens/s = 75133 (30721 source, 44412 target) ; Learning rate = 0.000276 ; Loss = 1.419909\n",
      "2024-12-06 03:18:07.554000: I runner.py:310] Step = 102300 ; steps/s = 1.65, tokens/s = 74260 (30374 source, 43886 target) ; Learning rate = 0.000276 ; Loss = 1.417023\n",
      "2024-12-06 03:19:08.478000: I runner.py:310] Step = 102400 ; steps/s = 1.64, tokens/s = 75080 (30692 source, 44388 target) ; Learning rate = 0.000276 ; Loss = 1.416829\n",
      "2024-12-06 03:20:09.394000: I runner.py:310] Step = 102500 ; steps/s = 1.64, tokens/s = 75124 (30720 source, 44404 target) ; Learning rate = 0.000276 ; Loss = 1.418792\n",
      "2024-12-06 03:21:09.868000: I runner.py:310] Step = 102600 ; steps/s = 1.65, tokens/s = 74335 (30406 source, 43929 target) ; Learning rate = 0.000276 ; Loss = 1.419664\n",
      "2024-12-06 03:22:10.873000: I runner.py:310] Step = 102700 ; steps/s = 1.64, tokens/s = 75015 (30673 source, 44342 target) ; Learning rate = 0.000276 ; Loss = 1.416048\n",
      "2024-12-06 03:23:11.396000: I runner.py:310] Step = 102800 ; steps/s = 1.65, tokens/s = 74294 (30406 source, 43888 target) ; Learning rate = 0.000276 ; Loss = 1.416345\n",
      "2024-12-06 03:24:12.357000: I runner.py:310] Step = 102900 ; steps/s = 1.64, tokens/s = 75043 (30674 source, 44369 target) ; Learning rate = 0.000276 ; Loss = 1.418400\n",
      "2024-12-06 03:25:13.221000: I runner.py:310] Step = 103000 ; steps/s = 1.64, tokens/s = 75166 (30731 source, 44435 target) ; Learning rate = 0.000275 ; Loss = 1.420466\n",
      "2024-12-06 03:26:13.671000: I runner.py:310] Step = 103100 ; steps/s = 1.65, tokens/s = 74390 (30443 source, 43947 target) ; Learning rate = 0.000275 ; Loss = 1.419042\n",
      "2024-12-06 03:27:14.523000: I runner.py:310] Step = 103200 ; steps/s = 1.64, tokens/s = 75202 (30750 source, 44452 target) ; Learning rate = 0.000275 ; Loss = 1.419567\n",
      "2024-12-06 03:28:15.017000: I runner.py:310] Step = 103300 ; steps/s = 1.65, tokens/s = 74319 (30408 source, 43911 target) ; Learning rate = 0.000275 ; Loss = 1.417407\n",
      "2024-12-06 03:29:15.907000: I runner.py:310] Step = 103400 ; steps/s = 1.64, tokens/s = 75133 (30711 source, 44422 target) ; Learning rate = 0.000275 ; Loss = 1.415350\n",
      "2024-12-06 03:30:16.805000: I runner.py:310] Step = 103500 ; steps/s = 1.64, tokens/s = 75122 (30706 source, 44416 target) ; Learning rate = 0.000275 ; Loss = 1.416587\n",
      "2024-12-06 03:31:17.255000: I runner.py:310] Step = 103600 ; steps/s = 1.65, tokens/s = 74363 (30419 source, 43944 target) ; Learning rate = 0.000275 ; Loss = 1.417324\n",
      "2024-12-06 03:32:18.191000: I runner.py:310] Step = 103700 ; steps/s = 1.64, tokens/s = 75072 (30686 source, 44386 target) ; Learning rate = 0.000274 ; Loss = 1.415879\n",
      "2024-12-06 03:33:18.674000: I runner.py:310] Step = 103800 ; steps/s = 1.65, tokens/s = 74362 (30439 source, 43923 target) ; Learning rate = 0.000274 ; Loss = 1.414548\n",
      "2024-12-06 03:34:19.650000: I runner.py:310] Step = 103900 ; steps/s = 1.64, tokens/s = 75042 (30686 source, 44356 target) ; Learning rate = 0.000274 ; Loss = 1.414199\n",
      "2024-12-06 03:35:20.534000: I runner.py:310] Step = 104000 ; steps/s = 1.64, tokens/s = 75164 (30741 source, 44423 target) ; Learning rate = 0.000274 ; Loss = 1.413676\n",
      "2024-12-06 03:36:20.975000: I runner.py:310] Step = 104100 ; steps/s = 1.65, tokens/s = 74358 (30409 source, 43949 target) ; Learning rate = 0.000274 ; Loss = 1.417414\n",
      "2024-12-06 03:37:21.900000: I runner.py:310] Step = 104200 ; steps/s = 1.64, tokens/s = 75093 (30699 source, 44394 target) ; Learning rate = 0.000274 ; Loss = 1.420512\n",
      "2024-12-06 03:38:22.451000: I runner.py:310] Step = 104300 ; steps/s = 1.65, tokens/s = 74265 (30389 source, 43876 target) ; Learning rate = 0.000274 ; Loss = 1.416289\n",
      "2024-12-06 03:39:23.375000: I runner.py:310] Step = 104400 ; steps/s = 1.64, tokens/s = 75088 (30691 source, 44397 target) ; Learning rate = 0.000274 ; Loss = 1.417026\n",
      "2024-12-06 03:40:24.212000: I runner.py:310] Step = 104500 ; steps/s = 1.64, tokens/s = 75193 (30736 source, 44457 target) ; Learning rate = 0.000273 ; Loss = 1.417569\n",
      "2024-12-06 03:41:24.713000: I runner.py:310] Step = 104600 ; steps/s = 1.65, tokens/s = 74340 (30431 source, 43909 target) ; Learning rate = 0.000273 ; Loss = 1.416384\n",
      "2024-12-06 03:42:25.610000: I runner.py:310] Step = 104700 ; steps/s = 1.64, tokens/s = 75125 (30704 source, 44421 target) ; Learning rate = 0.000273 ; Loss = 1.417689\n",
      "2024-12-06 03:43:26.337000: I runner.py:310] Step = 104800 ; steps/s = 1.65, tokens/s = 74742 (30583 source, 44159 target) ; Learning rate = 0.000273 ; Loss = 1.434387\n",
      "2024-12-06 03:44:27.003000: I runner.py:310] Step = 104900 ; steps/s = 1.65, tokens/s = 74711 (30543 source, 44168 target) ; Learning rate = 0.000273 ; Loss = 1.421673\n",
      "2024-12-06 03:45:27.909000: I runner.py:310] Step = 105000 ; steps/s = 1.64, tokens/s = 75104 (30702 source, 44402 target) ; Learning rate = 0.000273 ; Loss = 1.418277\n",
      "2024-12-06 03:45:27.910000: I training.py:192] Running evaluation for step 105000\n",
      "2024-12-06 03:46:16.276000: I training.py:192] Evaluation result for step 105000: loss = 1.004043 ; perplexity = 2.729295\n",
      "2024-12-06 03:47:16.604000: I runner.py:310] Step = 105100 ; steps/s = 1.66, tokens/s = 74554 (30505 source, 44049 target) ; Learning rate = 0.000273 ; Loss = 1.418075\n",
      "2024-12-06 03:48:17.508000: I runner.py:310] Step = 105200 ; steps/s = 1.64, tokens/s = 75101 (30700 source, 44401 target) ; Learning rate = 0.000273 ; Loss = 1.418468\n",
      "2024-12-06 03:49:18.462000: I runner.py:310] Step = 105300 ; steps/s = 1.64, tokens/s = 75076 (30695 source, 44381 target) ; Learning rate = 0.000272 ; Loss = 1.421261\n",
      "2024-12-06 03:50:18.935000: I runner.py:310] Step = 105400 ; steps/s = 1.65, tokens/s = 74366 (30438 source, 43928 target) ; Learning rate = 0.000272 ; Loss = 1.415396\n",
      "2024-12-06 03:51:19.924000: I runner.py:310] Step = 105500 ; steps/s = 1.64, tokens/s = 75005 (30657 source, 44348 target) ; Learning rate = 0.000272 ; Loss = 1.421700\n",
      "2024-12-06 03:52:20.463000: I runner.py:310] Step = 105600 ; steps/s = 1.65, tokens/s = 74285 (30396 source, 43889 target) ; Learning rate = 0.000272 ; Loss = 1.415702\n",
      "2024-12-06 03:53:21.393000: I runner.py:310] Step = 105700 ; steps/s = 1.64, tokens/s = 75079 (30690 source, 44389 target) ; Learning rate = 0.000272 ; Loss = 1.418329\n",
      "2024-12-06 03:54:22.352000: I runner.py:310] Step = 105800 ; steps/s = 1.64, tokens/s = 75049 (30684 source, 44365 target) ; Learning rate = 0.000272 ; Loss = 1.420337\n",
      "2024-12-06 03:55:22.840000: I runner.py:310] Step = 105900 ; steps/s = 1.65, tokens/s = 74327 (30405 source, 43922 target) ; Learning rate = 0.000272 ; Loss = 1.419840\n",
      "2024-12-06 03:56:23.764000: I runner.py:310] Step = 106000 ; steps/s = 1.64, tokens/s = 75104 (30711 source, 44393 target) ; Learning rate = 0.000271 ; Loss = 1.419526\n",
      "2024-12-06 03:57:24.329000: I runner.py:310] Step = 106100 ; steps/s = 1.65, tokens/s = 74228 (30367 source, 43861 target) ; Learning rate = 0.000271 ; Loss = 1.414791\n",
      "2024-12-06 03:58:25.286000: I runner.py:310] Step = 106200 ; steps/s = 1.64, tokens/s = 75050 (30680 source, 44370 target) ; Learning rate = 0.000271 ; Loss = 1.422484\n",
      "2024-12-06 03:59:26.219000: I runner.py:310] Step = 106300 ; steps/s = 1.64, tokens/s = 75093 (30706 source, 44387 target) ; Learning rate = 0.000271 ; Loss = 1.418650\n",
      "2024-12-06 04:00:26.725000: I runner.py:310] Step = 106400 ; steps/s = 1.65, tokens/s = 74311 (30401 source, 43910 target) ; Learning rate = 0.000271 ; Loss = 1.414991\n",
      "2024-12-06 04:01:27.652000: I runner.py:310] Step = 106500 ; steps/s = 1.64, tokens/s = 75087 (30697 source, 44390 target) ; Learning rate = 0.000271 ; Loss = 1.419430\n",
      "2024-12-06 04:02:28.149000: I runner.py:310] Step = 106600 ; steps/s = 1.65, tokens/s = 74338 (30425 source, 43913 target) ; Learning rate = 0.000271 ; Loss = 1.416756\n",
      "2024-12-06 04:03:29.115000: I runner.py:310] Step = 106700 ; steps/s = 1.64, tokens/s = 75044 (30680 source, 44364 target) ; Learning rate = 0.000271 ; Loss = 1.417059\n",
      "2024-12-06 04:04:30.011000: I runner.py:310] Step = 106800 ; steps/s = 1.64, tokens/s = 75127 (30709 source, 44418 target) ; Learning rate = 0.000270 ; Loss = 1.418281\n",
      "2024-12-06 04:05:30.511000: I runner.py:310] Step = 106900 ; steps/s = 1.65, tokens/s = 74316 (30410 source, 43906 target) ; Learning rate = 0.000270 ; Loss = 1.418677\n",
      "2024-12-06 04:06:31.443000: I runner.py:310] Step = 107000 ; steps/s = 1.64, tokens/s = 75080 (30694 source, 44386 target) ; Learning rate = 0.000270 ; Loss = 1.416565\n",
      "2024-12-06 04:07:31.966000: I runner.py:310] Step = 107100 ; steps/s = 1.65, tokens/s = 74309 (30410 source, 43899 target) ; Learning rate = 0.000270 ; Loss = 1.413898\n",
      "2024-12-06 04:08:32.849000: I runner.py:310] Step = 107200 ; steps/s = 1.64, tokens/s = 75135 (30713 source, 44422 target) ; Learning rate = 0.000270 ; Loss = 1.419071\n",
      "2024-12-06 04:09:33.743000: I runner.py:310] Step = 107300 ; steps/s = 1.64, tokens/s = 75127 (30711 source, 44416 target) ; Learning rate = 0.000270 ; Loss = 1.416230\n",
      "2024-12-06 04:10:34.245000: I runner.py:310] Step = 107400 ; steps/s = 1.65, tokens/s = 74313 (30405 source, 43908 target) ; Learning rate = 0.000270 ; Loss = 1.413858\n",
      "2024-12-06 04:11:35.158000: I runner.py:310] Step = 107500 ; steps/s = 1.64, tokens/s = 75116 (30709 source, 44407 target) ; Learning rate = 0.000270 ; Loss = 1.413465\n",
      "2024-12-06 04:12:35.644000: I runner.py:310] Step = 107600 ; steps/s = 1.65, tokens/s = 74341 (30424 source, 43917 target) ; Learning rate = 0.000269 ; Loss = 1.413534\n",
      "2024-12-06 04:13:36.587000: I runner.py:310] Step = 107700 ; steps/s = 1.64, tokens/s = 75065 (30685 source, 44380 target) ; Learning rate = 0.000269 ; Loss = 1.416869\n",
      "2024-12-06 04:14:37.545000: I runner.py:310] Step = 107800 ; steps/s = 1.64, tokens/s = 75066 (30690 source, 44376 target) ; Learning rate = 0.000269 ; Loss = 1.421657\n",
      "2024-12-06 04:15:38.017000: I runner.py:310] Step = 107900 ; steps/s = 1.65, tokens/s = 74335 (30412 source, 43923 target) ; Learning rate = 0.000269 ; Loss = 1.416372\n",
      "2024-12-06 04:16:38.899000: I runner.py:310] Step = 108000 ; steps/s = 1.64, tokens/s = 75155 (30725 source, 44430 target) ; Learning rate = 0.000269 ; Loss = 1.420326\n",
      "2024-12-06 04:17:39.369000: I runner.py:310] Step = 108100 ; steps/s = 1.65, tokens/s = 74363 (30429 source, 43934 target) ; Learning rate = 0.000269 ; Loss = 1.415401\n",
      "2024-12-06 04:18:40.269000: I runner.py:310] Step = 108200 ; steps/s = 1.64, tokens/s = 75126 (30716 source, 44410 target) ; Learning rate = 0.000269 ; Loss = 1.414950\n",
      "2024-12-06 04:19:41.238000: I runner.py:310] Step = 108300 ; steps/s = 1.64, tokens/s = 75031 (30664 source, 44367 target) ; Learning rate = 0.000269 ; Loss = 1.421371\n",
      "2024-12-06 04:20:41.708000: I runner.py:310] Step = 108400 ; steps/s = 1.65, tokens/s = 74360 (30434 source, 43926 target) ; Learning rate = 0.000268 ; Loss = 1.412281\n",
      "2024-12-06 04:21:42.677000: I runner.py:310] Step = 108500 ; steps/s = 1.64, tokens/s = 75020 (30656 source, 44364 target) ; Learning rate = 0.000268 ; Loss = 1.427219\n",
      "2024-12-06 04:22:43.081000: I runner.py:310] Step = 108600 ; steps/s = 1.66, tokens/s = 74468 (30488 source, 43980 target) ; Learning rate = 0.000268 ; Loss = 1.413781\n",
      "2024-12-06 04:23:43.994000: I runner.py:310] Step = 108700 ; steps/s = 1.64, tokens/s = 75085 (30684 source, 44401 target) ; Learning rate = 0.000268 ; Loss = 1.415955\n",
      "2024-12-06 04:24:44.839000: I runner.py:310] Step = 108800 ; steps/s = 1.64, tokens/s = 75206 (30755 source, 44451 target) ; Learning rate = 0.000268 ; Loss = 1.416935\n",
      "2024-12-06 04:25:45.269000: I runner.py:310] Step = 108900 ; steps/s = 1.65, tokens/s = 74395 (30439 source, 43956 target) ; Learning rate = 0.000268 ; Loss = 1.415644\n",
      "2024-12-06 04:26:46.143000: I runner.py:310] Step = 109000 ; steps/s = 1.64, tokens/s = 75150 (30719 source, 44431 target) ; Learning rate = 0.000268 ; Loss = 1.417271\n",
      "2024-12-06 04:27:47.076000: I runner.py:310] Step = 109100 ; steps/s = 1.64, tokens/s = 75095 (30703 source, 44392 target) ; Learning rate = 0.000268 ; Loss = 1.424015\n",
      "2024-12-06 04:28:47.529000: I runner.py:310] Step = 109200 ; steps/s = 1.65, tokens/s = 74375 (30430 source, 43945 target) ; Learning rate = 0.000267 ; Loss = 1.413343\n",
      "2024-12-06 04:29:48.467000: I runner.py:310] Step = 109300 ; steps/s = 1.64, tokens/s = 75083 (30706 source, 44377 target) ; Learning rate = 0.000267 ; Loss = 1.417975\n",
      "2024-12-06 04:30:48.945000: I runner.py:310] Step = 109400 ; steps/s = 1.65, tokens/s = 74344 (30411 source, 43933 target) ; Learning rate = 0.000267 ; Loss = 1.419020\n",
      "2024-12-06 04:31:49.846000: I runner.py:310] Step = 109500 ; steps/s = 1.64, tokens/s = 75114 (30708 source, 44406 target) ; Learning rate = 0.000267 ; Loss = 1.416095\n",
      "2024-12-06 04:32:50.722000: I runner.py:310] Step = 109600 ; steps/s = 1.64, tokens/s = 75153 (30719 source, 44434 target) ; Learning rate = 0.000267 ; Loss = 1.418257\n",
      "2024-12-06 04:33:51.208000: I runner.py:310] Step = 109700 ; steps/s = 1.65, tokens/s = 74343 (30416 source, 43927 target) ; Learning rate = 0.000267 ; Loss = 1.415995\n",
      "2024-12-06 04:34:52.122000: I runner.py:310] Step = 109800 ; steps/s = 1.64, tokens/s = 75105 (30710 source, 44395 target) ; Learning rate = 0.000267 ; Loss = 1.418838\n",
      "2024-12-06 04:35:52.628000: I runner.py:310] Step = 109900 ; steps/s = 1.65, tokens/s = 74307 (30397 source, 43910 target) ; Learning rate = 0.000267 ; Loss = 1.416714\n",
      "2024-12-06 04:36:53.549000: I runner.py:310] Step = 110000 ; steps/s = 1.64, tokens/s = 75099 (30706 source, 44393 target) ; Learning rate = 0.000266 ; Loss = 1.417989\n",
      "2024-12-06 04:36:55.514000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-110000\n",
      "2024-12-06 04:36:55.515000: I training.py:192] Running evaluation for step 110000\n",
      "2024-12-06 04:37:41.614000: I training.py:192] Evaluation result for step 110000: loss = 1.008386 ; perplexity = 2.741173\n",
      "2024-12-06 04:38:42.396000: I runner.py:310] Step = 110100 ; steps/s = 1.65, tokens/s = 75284 (30779 source, 44505 target) ; Learning rate = 0.000266 ; Loss = 1.418106\n",
      "2024-12-06 04:39:42.861000: I runner.py:310] Step = 110200 ; steps/s = 1.65, tokens/s = 74364 (30424 source, 43940 target) ; Learning rate = 0.000266 ; Loss = 1.416958\n",
      "2024-12-06 04:40:43.846000: I runner.py:310] Step = 110300 ; steps/s = 1.64, tokens/s = 75010 (30659 source, 44351 target) ; Learning rate = 0.000266 ; Loss = 1.418484\n",
      "2024-12-06 04:41:44.348000: I runner.py:310] Step = 110400 ; steps/s = 1.65, tokens/s = 74339 (30431 source, 43908 target) ; Learning rate = 0.000266 ; Loss = 1.413689\n",
      "2024-12-06 04:42:45.238000: I runner.py:310] Step = 110500 ; steps/s = 1.64, tokens/s = 75137 (30711 source, 44426 target) ; Learning rate = 0.000266 ; Loss = 1.419709\n",
      "2024-12-06 04:43:46.212000: I runner.py:310] Step = 110600 ; steps/s = 1.64, tokens/s = 75026 (30677 source, 44349 target) ; Learning rate = 0.000266 ; Loss = 1.420637\n",
      "2024-12-06 04:44:46.769000: I runner.py:310] Step = 110700 ; steps/s = 1.65, tokens/s = 74252 (30380 source, 43872 target) ; Learning rate = 0.000266 ; Loss = 1.415976\n",
      "2024-12-06 04:45:47.712000: I runner.py:310] Step = 110800 ; steps/s = 1.64, tokens/s = 75076 (30690 source, 44386 target) ; Learning rate = 0.000266 ; Loss = 1.417612\n",
      "2024-12-06 04:46:48.220000: I runner.py:310] Step = 110900 ; steps/s = 1.65, tokens/s = 74308 (30411 source, 43897 target) ; Learning rate = 0.000265 ; Loss = 1.410759\n",
      "2024-12-06 04:47:49.186000: I runner.py:310] Step = 111000 ; steps/s = 1.64, tokens/s = 75043 (30676 source, 44367 target) ; Learning rate = 0.000265 ; Loss = 1.415393\n",
      "2024-12-06 04:48:50.163000: I runner.py:310] Step = 111100 ; steps/s = 1.64, tokens/s = 75024 (30668 source, 44356 target) ; Learning rate = 0.000265 ; Loss = 1.418456\n",
      "2024-12-06 04:49:50.684000: I runner.py:310] Step = 111200 ; steps/s = 1.65, tokens/s = 74278 (30385 source, 43893 target) ; Learning rate = 0.000265 ; Loss = 1.416075\n",
      "2024-12-06 04:50:51.587000: I runner.py:310] Step = 111300 ; steps/s = 1.64, tokens/s = 75126 (30715 source, 44411 target) ; Learning rate = 0.000265 ; Loss = 1.416262\n",
      "2024-12-06 04:51:52.001000: I runner.py:310] Step = 111400 ; steps/s = 1.66, tokens/s = 74431 (30460 source, 43971 target) ; Learning rate = 0.000265 ; Loss = 1.415024\n",
      "2024-12-06 04:52:52.895000: I runner.py:310] Step = 111500 ; steps/s = 1.64, tokens/s = 75131 (30711 source, 44420 target) ; Learning rate = 0.000265 ; Loss = 1.416432\n",
      "2024-12-06 04:53:53.807000: I runner.py:310] Step = 111600 ; steps/s = 1.64, tokens/s = 75124 (30721 source, 44403 target) ; Learning rate = 0.000265 ; Loss = 1.419829\n",
      "2024-12-06 04:54:54.280000: I runner.py:310] Step = 111700 ; steps/s = 1.65, tokens/s = 74354 (30422 source, 43932 target) ; Learning rate = 0.000264 ; Loss = 1.417702\n",
      "2024-12-06 04:55:55.225000: I runner.py:310] Step = 111800 ; steps/s = 1.64, tokens/s = 75054 (30681 source, 44373 target) ; Learning rate = 0.000264 ; Loss = 1.421213\n",
      "2024-12-06 04:56:55.712000: I runner.py:310] Step = 111900 ; steps/s = 1.65, tokens/s = 74357 (30430 source, 43927 target) ; Learning rate = 0.000264 ; Loss = 1.412252\n",
      "2024-12-06 04:57:56.617000: I runner.py:310] Step = 112000 ; steps/s = 1.64, tokens/s = 75121 (30715 source, 44406 target) ; Learning rate = 0.000264 ; Loss = 1.414597\n",
      "2024-12-06 04:58:57.539000: I runner.py:310] Step = 112100 ; steps/s = 1.64, tokens/s = 75085 (30688 source, 44397 target) ; Learning rate = 0.000264 ; Loss = 1.419205\n",
      "2024-12-06 04:59:57.987000: I runner.py:310] Step = 112200 ; steps/s = 1.65, tokens/s = 74375 (30423 source, 43952 target) ; Learning rate = 0.000264 ; Loss = 1.417582\n",
      "2024-12-06 05:00:58.889000: I runner.py:310] Step = 112300 ; steps/s = 1.64, tokens/s = 75109 (30708 source, 44401 target) ; Learning rate = 0.000264 ; Loss = 1.419195\n",
      "2024-12-06 05:01:59.287000: I runner.py:310] Step = 112400 ; steps/s = 1.66, tokens/s = 74468 (30479 source, 43989 target) ; Learning rate = 0.000264 ; Loss = 1.415682\n",
      "2024-12-06 05:03:00.165000: I runner.py:310] Step = 112500 ; steps/s = 1.64, tokens/s = 75145 (30717 source, 44428 target) ; Learning rate = 0.000264 ; Loss = 1.424045\n",
      "2024-12-06 05:04:01.119000: I runner.py:310] Step = 112600 ; steps/s = 1.64, tokens/s = 75057 (30681 source, 44376 target) ; Learning rate = 0.000263 ; Loss = 1.417962\n",
      "2024-12-06 05:05:01.578000: I runner.py:310] Step = 112700 ; steps/s = 1.65, tokens/s = 74350 (30422 source, 43928 target) ; Learning rate = 0.000263 ; Loss = 1.416900\n",
      "2024-12-06 05:06:02.527000: I runner.py:310] Step = 112800 ; steps/s = 1.64, tokens/s = 75060 (30687 source, 44373 target) ; Learning rate = 0.000263 ; Loss = 1.416754\n",
      "2024-12-06 05:07:03.309000: I runner.py:310] Step = 112900 ; steps/s = 1.65, tokens/s = 74850 (30615 source, 44235 target) ; Learning rate = 0.000263 ; Loss = 1.417354\n",
      "2024-12-06 05:08:03.907000: I runner.py:310] Step = 113000 ; steps/s = 1.65, tokens/s = 74644 (30528 source, 44116 target) ; Learning rate = 0.000263 ; Loss = 1.413881\n",
      "2024-12-06 05:09:04.751000: I runner.py:310] Step = 113100 ; steps/s = 1.64, tokens/s = 75194 (30743 source, 44451 target) ; Learning rate = 0.000263 ; Loss = 1.414270\n",
      "2024-12-06 05:10:05.256000: I runner.py:310] Step = 113200 ; steps/s = 1.65, tokens/s = 74299 (30391 source, 43908 target) ; Learning rate = 0.000263 ; Loss = 1.413812\n",
      "2024-12-06 05:11:06.175000: I runner.py:310] Step = 113300 ; steps/s = 1.64, tokens/s = 75104 (30708 source, 44396 target) ; Learning rate = 0.000263 ; Loss = 1.413799\n",
      "2024-12-06 05:12:07.131000: I runner.py:310] Step = 113400 ; steps/s = 1.64, tokens/s = 75068 (30694 source, 44374 target) ; Learning rate = 0.000262 ; Loss = 1.425889\n",
      "2024-12-06 05:13:07.603000: I runner.py:310] Step = 113500 ; steps/s = 1.65, tokens/s = 74342 (30415 source, 43927 target) ; Learning rate = 0.000262 ; Loss = 1.414938\n",
      "2024-12-06 05:14:08.548000: I runner.py:310] Step = 113600 ; steps/s = 1.64, tokens/s = 75061 (30685 source, 44376 target) ; Learning rate = 0.000262 ; Loss = 1.417975\n",
      "2024-12-06 05:15:09.021000: I runner.py:310] Step = 113700 ; steps/s = 1.65, tokens/s = 74354 (30419 source, 43935 target) ; Learning rate = 0.000262 ; Loss = 1.413973\n",
      "2024-12-06 05:16:09.960000: I runner.py:310] Step = 113800 ; steps/s = 1.64, tokens/s = 75073 (30687 source, 44386 target) ; Learning rate = 0.000262 ; Loss = 1.416791\n",
      "2024-12-06 05:17:10.866000: I runner.py:310] Step = 113900 ; steps/s = 1.64, tokens/s = 75133 (30729 source, 44404 target) ; Learning rate = 0.000262 ; Loss = 1.415904\n",
      "2024-12-06 05:18:11.354000: I runner.py:310] Step = 114000 ; steps/s = 1.65, tokens/s = 74328 (30407 source, 43921 target) ; Learning rate = 0.000262 ; Loss = 1.412175\n",
      "2024-12-06 05:19:12.309000: I runner.py:310] Step = 114100 ; steps/s = 1.64, tokens/s = 75080 (30705 source, 44375 target) ; Learning rate = 0.000262 ; Loss = 1.417952\n",
      "2024-12-06 05:20:12.838000: I runner.py:310] Step = 114200 ; steps/s = 1.65, tokens/s = 74270 (30382 source, 43888 target) ; Learning rate = 0.000262 ; Loss = 1.413822\n",
      "2024-12-06 05:21:13.790000: I runner.py:310] Step = 114300 ; steps/s = 1.64, tokens/s = 75037 (30656 source, 44381 target) ; Learning rate = 0.000261 ; Loss = 1.417581\n",
      "2024-12-06 05:22:14.703000: I runner.py:310] Step = 114400 ; steps/s = 1.64, tokens/s = 75139 (30739 source, 44400 target) ; Learning rate = 0.000261 ; Loss = 1.418728\n",
      "2024-12-06 05:23:15.178000: I runner.py:310] Step = 114500 ; steps/s = 1.65, tokens/s = 74338 (30417 source, 43921 target) ; Learning rate = 0.000261 ; Loss = 1.416143\n",
      "2024-12-06 05:24:16.109000: I runner.py:310] Step = 114600 ; steps/s = 1.64, tokens/s = 75123 (30726 source, 44397 target) ; Learning rate = 0.000261 ; Loss = 1.412173\n",
      "2024-12-06 05:25:16.682000: I runner.py:310] Step = 114700 ; steps/s = 1.65, tokens/s = 74211 (30351 source, 43860 target) ; Learning rate = 0.000261 ; Loss = 1.412329\n",
      "2024-12-06 05:26:17.565000: I runner.py:310] Step = 114800 ; steps/s = 1.64, tokens/s = 75138 (30709 source, 44429 target) ; Learning rate = 0.000261 ; Loss = 1.416615\n",
      "2024-12-06 05:27:18.438000: I runner.py:310] Step = 114900 ; steps/s = 1.64, tokens/s = 75174 (30745 source, 44429 target) ; Learning rate = 0.000261 ; Loss = 1.421049\n",
      "2024-12-06 05:28:18.990000: I runner.py:310] Step = 115000 ; steps/s = 1.65, tokens/s = 74259 (30386 source, 43873 target) ; Learning rate = 0.000261 ; Loss = 1.419422\n",
      "2024-12-06 05:28:18.992000: I training.py:192] Running evaluation for step 115000\n",
      "2024-12-06 05:29:04.512000: I training.py:192] Evaluation result for step 115000: loss = 1.010802 ; perplexity = 2.747802\n",
      "2024-12-06 05:30:05.346000: I runner.py:310] Step = 115100 ; steps/s = 1.64, tokens/s = 75210 (30743 source, 44467 target) ; Learning rate = 0.000261 ; Loss = 1.413026\n",
      "2024-12-06 05:31:05.884000: I runner.py:310] Step = 115200 ; steps/s = 1.65, tokens/s = 74276 (30390 source, 43886 target) ; Learning rate = 0.000260 ; Loss = 1.415167\n",
      "2024-12-06 05:32:06.760000: I runner.py:310] Step = 115300 ; steps/s = 1.64, tokens/s = 75136 (30709 source, 44427 target) ; Learning rate = 0.000260 ; Loss = 1.417956\n",
      "2024-12-06 05:33:07.667000: I runner.py:310] Step = 115400 ; steps/s = 1.64, tokens/s = 75148 (30741 source, 44407 target) ; Learning rate = 0.000260 ; Loss = 1.414525\n",
      "2024-12-06 05:34:08.170000: I runner.py:310] Step = 115500 ; steps/s = 1.65, tokens/s = 74316 (30395 source, 43921 target) ; Learning rate = 0.000260 ; Loss = 1.413528\n",
      "2024-12-06 05:35:09.023000: I runner.py:310] Step = 115600 ; steps/s = 1.64, tokens/s = 75184 (30737 source, 44447 target) ; Learning rate = 0.000260 ; Loss = 1.415916\n",
      "2024-12-06 05:36:09.565000: I runner.py:310] Step = 115700 ; steps/s = 1.65, tokens/s = 74269 (30396 source, 43873 target) ; Learning rate = 0.000260 ; Loss = 1.413033\n",
      "2024-12-06 05:37:10.434000: I runner.py:310] Step = 115800 ; steps/s = 1.64, tokens/s = 75173 (30739 source, 44434 target) ; Learning rate = 0.000260 ; Loss = 1.414702\n",
      "2024-12-06 05:38:11.400000: I runner.py:310] Step = 115900 ; steps/s = 1.64, tokens/s = 75040 (30678 source, 44362 target) ; Learning rate = 0.000260 ; Loss = 1.415717\n",
      "2024-12-06 05:39:11.914000: I runner.py:310] Step = 116000 ; steps/s = 1.65, tokens/s = 74288 (30388 source, 43900 target) ; Learning rate = 0.000260 ; Loss = 1.416767\n",
      "2024-12-06 05:40:12.804000: I runner.py:310] Step = 116100 ; steps/s = 1.64, tokens/s = 75145 (30720 source, 44425 target) ; Learning rate = 0.000259 ; Loss = 1.415840\n",
      "2024-12-06 05:41:13.279000: I runner.py:310] Step = 116200 ; steps/s = 1.65, tokens/s = 74345 (30420 source, 43925 target) ; Learning rate = 0.000259 ; Loss = 1.419065\n",
      "2024-12-06 05:42:14.202000: I runner.py:310] Step = 116300 ; steps/s = 1.64, tokens/s = 75088 (30687 source, 44401 target) ; Learning rate = 0.000259 ; Loss = 1.414894\n",
      "2024-12-06 05:43:15.085000: I runner.py:310] Step = 116400 ; steps/s = 1.64, tokens/s = 75142 (30725 source, 44417 target) ; Learning rate = 0.000259 ; Loss = 1.417498\n",
      "2024-12-06 05:44:15.565000: I runner.py:310] Step = 116500 ; steps/s = 1.65, tokens/s = 74367 (30438 source, 43929 target) ; Learning rate = 0.000259 ; Loss = 1.426033\n",
      "2024-12-06 05:45:16.506000: I runner.py:310] Step = 116600 ; steps/s = 1.64, tokens/s = 75042 (30659 source, 44383 target) ; Learning rate = 0.000259 ; Loss = 1.416323\n",
      "2024-12-06 05:46:16.994000: I runner.py:310] Step = 116700 ; steps/s = 1.65, tokens/s = 74378 (30454 source, 43924 target) ; Learning rate = 0.000259 ; Loss = 1.411560\n",
      "2024-12-06 05:47:17.886000: I runner.py:310] Step = 116800 ; steps/s = 1.64, tokens/s = 75118 (30701 source, 44417 target) ; Learning rate = 0.000259 ; Loss = 1.413307\n",
      "2024-12-06 05:48:18.803000: I runner.py:310] Step = 116900 ; steps/s = 1.64, tokens/s = 75094 (30697 source, 44397 target) ; Learning rate = 0.000259 ; Loss = 1.416625\n",
      "2024-12-06 05:49:19.272000: I runner.py:310] Step = 117000 ; steps/s = 1.65, tokens/s = 74377 (30439 source, 43938 target) ; Learning rate = 0.000258 ; Loss = 1.414032\n",
      "2024-12-06 05:50:20.196000: I runner.py:310] Step = 117100 ; steps/s = 1.64, tokens/s = 75098 (30710 source, 44388 target) ; Learning rate = 0.000258 ; Loss = 1.417311\n",
      "2024-12-06 05:51:21.124000: I runner.py:310] Step = 117200 ; steps/s = 1.64, tokens/s = 75082 (30683 source, 44399 target) ; Learning rate = 0.000258 ; Loss = 1.417728\n",
      "2024-12-06 05:52:21.559000: I runner.py:310] Step = 117300 ; steps/s = 1.65, tokens/s = 74397 (30443 source, 43954 target) ; Learning rate = 0.000258 ; Loss = 1.413037\n",
      "2024-12-06 05:53:22.506000: I runner.py:310] Step = 117400 ; steps/s = 1.64, tokens/s = 75072 (30692 source, 44380 target) ; Learning rate = 0.000258 ; Loss = 1.418261\n",
      "2024-12-06 05:54:22.959000: I runner.py:310] Step = 117500 ; steps/s = 1.65, tokens/s = 74367 (30423 source, 43944 target) ; Learning rate = 0.000258 ; Loss = 1.417060\n",
      "2024-12-06 05:55:23.837000: I runner.py:310] Step = 117600 ; steps/s = 1.64, tokens/s = 75150 (30720 source, 44430 target) ; Learning rate = 0.000258 ; Loss = 1.414214\n",
      "2024-12-06 05:56:24.710000: I runner.py:310] Step = 117700 ; steps/s = 1.64, tokens/s = 75159 (30727 source, 44432 target) ; Learning rate = 0.000258 ; Loss = 1.420070\n",
      "2024-12-06 05:57:25.182000: I runner.py:310] Step = 117800 ; steps/s = 1.65, tokens/s = 74345 (30415 source, 43930 target) ; Learning rate = 0.000258 ; Loss = 1.414633\n",
      "2024-12-06 05:58:26.038000: I runner.py:310] Step = 117900 ; steps/s = 1.64, tokens/s = 75198 (30753 source, 44445 target) ; Learning rate = 0.000257 ; Loss = 1.419027\n",
      "2024-12-06 05:59:26.523000: I runner.py:310] Step = 118000 ; steps/s = 1.65, tokens/s = 74334 (30414 source, 43920 target) ; Learning rate = 0.000257 ; Loss = 1.416364\n",
      "2024-12-06 06:00:27.439000: I runner.py:310] Step = 118100 ; steps/s = 1.64, tokens/s = 75103 (30696 source, 44407 target) ; Learning rate = 0.000257 ; Loss = 1.416663\n",
      "2024-12-06 06:01:28.350000: I runner.py:310] Step = 118200 ; steps/s = 1.64, tokens/s = 75124 (30723 source, 44401 target) ; Learning rate = 0.000257 ; Loss = 1.414590\n",
      "2024-12-06 06:02:28.814000: I runner.py:310] Step = 118300 ; steps/s = 1.65, tokens/s = 74344 (30406 source, 43938 target) ; Learning rate = 0.000257 ; Loss = 1.414494\n",
      "2024-12-06 06:03:29.632000: I runner.py:310] Step = 118400 ; steps/s = 1.64, tokens/s = 75228 (30754 source, 44474 target) ; Learning rate = 0.000257 ; Loss = 1.417350\n",
      "2024-12-06 06:04:30.120000: I runner.py:310] Step = 118500 ; steps/s = 1.65, tokens/s = 74343 (30427 source, 43916 target) ; Learning rate = 0.000257 ; Loss = 1.413898\n",
      "2024-12-06 06:05:30.999000: I runner.py:310] Step = 118600 ; steps/s = 1.64, tokens/s = 75161 (30733 source, 44428 target) ; Learning rate = 0.000257 ; Loss = 1.418877\n",
      "2024-12-06 06:06:31.887000: I runner.py:310] Step = 118700 ; steps/s = 1.64, tokens/s = 75140 (30718 source, 44422 target) ; Learning rate = 0.000257 ; Loss = 1.415886\n",
      "2024-12-06 06:07:32.480000: I runner.py:310] Step = 118800 ; steps/s = 1.65, tokens/s = 74189 (30350 source, 43839 target) ; Learning rate = 0.000256 ; Loss = 1.412108\n",
      "2024-12-06 06:08:33.386000: I runner.py:310] Step = 118900 ; steps/s = 1.64, tokens/s = 75107 (30700 source, 44407 target) ; Learning rate = 0.000256 ; Loss = 1.413230\n",
      "2024-12-06 06:09:33.857000: I runner.py:310] Step = 119000 ; steps/s = 1.65, tokens/s = 74375 (30441 source, 43934 target) ; Learning rate = 0.000256 ; Loss = 1.414356\n",
      "2024-12-06 06:10:34.711000: I runner.py:310] Step = 119100 ; steps/s = 1.64, tokens/s = 75183 (30726 source, 44457 target) ; Learning rate = 0.000256 ; Loss = 1.414841\n",
      "2024-12-06 06:11:35.616000: I runner.py:310] Step = 119200 ; steps/s = 1.64, tokens/s = 75120 (30718 source, 44402 target) ; Learning rate = 0.000256 ; Loss = 1.417852\n",
      "2024-12-06 06:12:36.025000: I runner.py:310] Step = 119300 ; steps/s = 1.66, tokens/s = 74428 (30445 source, 43983 target) ; Learning rate = 0.000256 ; Loss = 1.414641\n",
      "2024-12-06 06:13:36.939000: I runner.py:310] Step = 119400 ; steps/s = 1.64, tokens/s = 75119 (30724 source, 44395 target) ; Learning rate = 0.000256 ; Loss = 1.412894\n",
      "2024-12-06 06:14:37.355000: I runner.py:310] Step = 119500 ; steps/s = 1.66, tokens/s = 74418 (30446 source, 43972 target) ; Learning rate = 0.000256 ; Loss = 1.411757\n",
      "2024-12-06 06:15:38.284000: I runner.py:310] Step = 119600 ; steps/s = 1.64, tokens/s = 75100 (30713 source, 44387 target) ; Learning rate = 0.000256 ; Loss = 1.413001\n",
      "2024-12-06 06:16:39.167000: I runner.py:310] Step = 119700 ; steps/s = 1.64, tokens/s = 75134 (30705 source, 44429 target) ; Learning rate = 0.000255 ; Loss = 1.415840\n",
      "2024-12-06 06:17:39.690000: I runner.py:310] Step = 119800 ; steps/s = 1.65, tokens/s = 74298 (30402 source, 43896 target) ; Learning rate = 0.000255 ; Loss = 1.413765\n",
      "2024-12-06 06:18:40.553000: I runner.py:310] Step = 119900 ; steps/s = 1.64, tokens/s = 75160 (30720 source, 44440 target) ; Learning rate = 0.000255 ; Loss = 1.415965\n",
      "2024-12-06 06:19:41.011000: I runner.py:310] Step = 120000 ; steps/s = 1.65, tokens/s = 74382 (30442 source, 43940 target) ; Learning rate = 0.000255 ; Loss = 1.413017\n",
      "2024-12-06 06:19:43.026000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-120000\n",
      "2024-12-06 06:19:43.027000: I training.py:192] Running evaluation for step 120000\n",
      "2024-12-06 06:20:30.218000: I training.py:192] Evaluation result for step 120000: loss = 1.019038 ; perplexity = 2.770527\n",
      "2024-12-06 06:21:30.981000: I runner.py:310] Step = 120100 ; steps/s = 1.65, tokens/s = 75388 (30826 source, 44562 target) ; Learning rate = 0.000255 ; Loss = 1.410731\n",
      "2024-12-06 06:22:31.912000: I runner.py:310] Step = 120200 ; steps/s = 1.64, tokens/s = 75074 (30691 source, 44383 target) ; Learning rate = 0.000255 ; Loss = 1.416402\n",
      "2024-12-06 06:23:32.402000: I runner.py:310] Step = 120300 ; steps/s = 1.65, tokens/s = 74312 (30395 source, 43917 target) ; Learning rate = 0.000255 ; Loss = 1.419102\n",
      "2024-12-06 06:24:33.292000: I runner.py:310] Step = 120400 ; steps/s = 1.64, tokens/s = 75139 (30723 source, 44416 target) ; Learning rate = 0.000255 ; Loss = 1.418048\n",
      "2024-12-06 06:25:33.784000: I runner.py:310] Step = 120500 ; steps/s = 1.65, tokens/s = 74344 (30426 source, 43918 target) ; Learning rate = 0.000255 ; Loss = 1.412449\n",
      "2024-12-06 06:26:34.636000: I runner.py:310] Step = 120600 ; steps/s = 1.64, tokens/s = 75161 (30714 source, 44447 target) ; Learning rate = 0.000255 ; Loss = 1.422934\n",
      "2024-12-06 06:27:35.624000: I runner.py:310] Step = 120700 ; steps/s = 1.64, tokens/s = 75012 (30667 source, 44345 target) ; Learning rate = 0.000254 ; Loss = 1.414516\n",
      "2024-12-06 06:28:36.103000: I runner.py:310] Step = 120800 ; steps/s = 1.65, tokens/s = 74351 (30418 source, 43933 target) ; Learning rate = 0.000254 ; Loss = 1.417664\n",
      "2024-12-06 06:29:36.950000: I runner.py:310] Step = 120900 ; steps/s = 1.64, tokens/s = 75179 (30736 source, 44443 target) ; Learning rate = 0.000254 ; Loss = 1.418342\n",
      "2024-12-06 06:30:37.789000: I runner.py:310] Step = 121000 ; steps/s = 1.64, tokens/s = 74971 (30662 source, 44309 target) ; Learning rate = 0.000254 ; Loss = 1.423988\n",
      "2024-12-06 06:31:38.344000: I runner.py:310] Step = 121100 ; steps/s = 1.65, tokens/s = 74507 (30487 source, 44020 target) ; Learning rate = 0.000254 ; Loss = 1.413006\n",
      "2024-12-06 06:32:39.267000: I runner.py:310] Step = 121200 ; steps/s = 1.64, tokens/s = 75097 (30695 source, 44402 target) ; Learning rate = 0.000254 ; Loss = 1.414740\n",
      "2024-12-06 06:33:39.746000: I runner.py:310] Step = 121300 ; steps/s = 1.65, tokens/s = 74352 (30430 source, 43922 target) ; Learning rate = 0.000254 ; Loss = 1.411345\n",
      "2024-12-06 06:34:40.631000: I runner.py:310] Step = 121400 ; steps/s = 1.64, tokens/s = 75118 (30693 source, 44425 target) ; Learning rate = 0.000254 ; Loss = 1.417433\n",
      "2024-12-06 06:35:41.476000: I runner.py:310] Step = 121500 ; steps/s = 1.64, tokens/s = 75204 (30755 source, 44449 target) ; Learning rate = 0.000254 ; Loss = 1.413321\n",
      "2024-12-06 06:36:42.002000: I runner.py:310] Step = 121600 ; steps/s = 1.65, tokens/s = 74266 (30376 source, 43890 target) ; Learning rate = 0.000253 ; Loss = 1.412084\n",
      "2024-12-06 06:37:42.901000: I runner.py:310] Step = 121700 ; steps/s = 1.64, tokens/s = 75134 (30722 source, 44412 target) ; Learning rate = 0.000253 ; Loss = 1.416760\n",
      "2024-12-06 06:38:43.322000: I runner.py:310] Step = 121800 ; steps/s = 1.66, tokens/s = 74421 (30449 source, 43972 target) ; Learning rate = 0.000253 ; Loss = 1.415068\n",
      "2024-12-06 06:39:44.172000: I runner.py:310] Step = 121900 ; steps/s = 1.64, tokens/s = 75190 (30743 source, 44447 target) ; Learning rate = 0.000253 ; Loss = 1.418703\n",
      "2024-12-06 06:40:45.088000: I runner.py:310] Step = 122000 ; steps/s = 1.64, tokens/s = 75099 (30700 source, 44399 target) ; Learning rate = 0.000253 ; Loss = 1.416567\n",
      "2024-12-06 06:41:45.609000: I runner.py:310] Step = 122100 ; steps/s = 1.65, tokens/s = 74301 (30406 source, 43895 target) ; Learning rate = 0.000253 ; Loss = 1.412980\n",
      "2024-12-06 06:42:46.560000: I runner.py:310] Step = 122200 ; steps/s = 1.64, tokens/s = 75069 (30693 source, 44376 target) ; Learning rate = 0.000253 ; Loss = 1.414560\n",
      "2024-12-06 06:43:47.080000: I runner.py:310] Step = 122300 ; steps/s = 1.65, tokens/s = 74275 (30378 source, 43897 target) ; Learning rate = 0.000253 ; Loss = 1.412912\n",
      "2024-12-06 06:44:47.956000: I runner.py:310] Step = 122400 ; steps/s = 1.64, tokens/s = 75152 (30721 source, 44431 target) ; Learning rate = 0.000253 ; Loss = 1.412127\n",
      "2024-12-06 06:45:48.794000: I runner.py:310] Step = 122500 ; steps/s = 1.64, tokens/s = 75222 (30771 source, 44451 target) ; Learning rate = 0.000253 ; Loss = 1.413923\n",
      "2024-12-06 06:46:49.246000: I runner.py:310] Step = 122600 ; steps/s = 1.65, tokens/s = 74384 (30429 source, 43955 target) ; Learning rate = 0.000252 ; Loss = 1.411747\n",
      "2024-12-06 06:47:50.152000: I runner.py:310] Step = 122700 ; steps/s = 1.64, tokens/s = 75133 (30726 source, 44407 target) ; Learning rate = 0.000252 ; Loss = 1.417121\n",
      "2024-12-06 06:48:50.585000: I runner.py:310] Step = 122800 ; steps/s = 1.65, tokens/s = 74378 (30423 source, 43955 target) ; Learning rate = 0.000252 ; Loss = 1.414348\n",
      "2024-12-06 06:49:51.442000: I runner.py:310] Step = 122900 ; steps/s = 1.64, tokens/s = 75144 (30703 source, 44441 target) ; Learning rate = 0.000252 ; Loss = 1.412790\n",
      "2024-12-06 06:50:52.335000: I runner.py:310] Step = 123000 ; steps/s = 1.64, tokens/s = 75157 (30742 source, 44415 target) ; Learning rate = 0.000252 ; Loss = 1.418509\n",
      "2024-12-06 06:51:52.811000: I runner.py:310] Step = 123100 ; steps/s = 1.65, tokens/s = 74353 (30424 source, 43929 target) ; Learning rate = 0.000252 ; Loss = 1.413643\n",
      "2024-12-06 06:52:53.745000: I runner.py:310] Step = 123200 ; steps/s = 1.64, tokens/s = 75096 (30709 source, 44387 target) ; Learning rate = 0.000252 ; Loss = 1.412360\n",
      "2024-12-06 06:53:54.196000: I runner.py:310] Step = 123300 ; steps/s = 1.65, tokens/s = 74368 (30423 source, 43945 target) ; Learning rate = 0.000252 ; Loss = 1.409779\n",
      "2024-12-06 06:54:55.098000: I runner.py:310] Step = 123400 ; steps/s = 1.64, tokens/s = 75116 (30703 source, 44413 target) ; Learning rate = 0.000252 ; Loss = 1.413630\n",
      "2024-12-06 06:55:56.036000: I runner.py:310] Step = 123500 ; steps/s = 1.64, tokens/s = 75090 (30703 source, 44387 target) ; Learning rate = 0.000252 ; Loss = 1.419644\n",
      "2024-12-06 06:56:56.418000: I runner.py:310] Step = 123600 ; steps/s = 1.66, tokens/s = 74457 (30460 source, 43997 target) ; Learning rate = 0.000251 ; Loss = 1.415556\n",
      "2024-12-06 06:57:57.299000: I runner.py:310] Step = 123700 ; steps/s = 1.64, tokens/s = 75136 (30714 source, 44422 target) ; Learning rate = 0.000251 ; Loss = 1.414439\n",
      "2024-12-06 06:58:57.770000: I runner.py:310] Step = 123800 ; steps/s = 1.65, tokens/s = 74387 (30453 source, 43934 target) ; Learning rate = 0.000251 ; Loss = 1.412511\n",
      "2024-12-06 06:59:58.677000: I runner.py:310] Step = 123900 ; steps/s = 1.64, tokens/s = 75107 (30697 source, 44410 target) ; Learning rate = 0.000251 ; Loss = 1.417269\n",
      "2024-12-06 07:00:59.571000: I runner.py:310] Step = 124000 ; steps/s = 1.64, tokens/s = 75135 (30722 source, 44413 target) ; Learning rate = 0.000251 ; Loss = 1.414685\n",
      "2024-12-06 07:02:00.051000: I runner.py:310] Step = 124100 ; steps/s = 1.65, tokens/s = 74318 (30395 source, 43923 target) ; Learning rate = 0.000251 ; Loss = 1.416600\n",
      "2024-12-06 07:03:00.934000: I runner.py:310] Step = 124200 ; steps/s = 1.64, tokens/s = 75155 (30729 source, 44426 target) ; Learning rate = 0.000251 ; Loss = 1.412344\n",
      "2024-12-06 07:04:01.406000: I runner.py:310] Step = 124300 ; steps/s = 1.65, tokens/s = 74368 (30437 source, 43931 target) ; Learning rate = 0.000251 ; Loss = 1.413035\n",
      "2024-12-06 07:05:02.336000: I runner.py:310] Step = 124400 ; steps/s = 1.64, tokens/s = 75088 (30697 source, 44391 target) ; Learning rate = 0.000251 ; Loss = 1.415485\n",
      "2024-12-06 07:06:03.177000: I runner.py:310] Step = 124500 ; steps/s = 1.64, tokens/s = 75200 (30741 source, 44459 target) ; Learning rate = 0.000251 ; Loss = 1.417815\n",
      "2024-12-06 07:07:03.707000: I runner.py:310] Step = 124600 ; steps/s = 1.65, tokens/s = 74282 (30388 source, 43894 target) ; Learning rate = 0.000250 ; Loss = 1.423283\n",
      "2024-12-06 07:08:04.570000: I runner.py:310] Step = 124700 ; steps/s = 1.64, tokens/s = 75176 (30739 source, 44437 target) ; Learning rate = 0.000250 ; Loss = 1.416480\n",
      "2024-12-06 07:09:05.084000: I runner.py:310] Step = 124800 ; steps/s = 1.65, tokens/s = 74318 (30415 source, 43903 target) ; Learning rate = 0.000250 ; Loss = 1.412233\n",
      "2024-12-06 07:10:05.923000: I runner.py:310] Step = 124900 ; steps/s = 1.64, tokens/s = 75178 (30721 source, 44457 target) ; Learning rate = 0.000250 ; Loss = 1.417825\n",
      "2024-12-06 07:11:06.775000: I runner.py:310] Step = 125000 ; steps/s = 1.64, tokens/s = 75174 (30731 source, 44443 target) ; Learning rate = 0.000250 ; Loss = 1.412709\n",
      "2024-12-06 07:11:06.776000: I training.py:192] Running evaluation for step 125000\n",
      "2024-12-06 07:11:53.192000: I training.py:192] Evaluation result for step 125000: loss = 1.018736 ; perplexity = 2.769692\n",
      "2024-12-06 07:12:53.524000: I runner.py:310] Step = 125100 ; steps/s = 1.66, tokens/s = 74566 (30522 source, 44044 target) ; Learning rate = 0.000250 ; Loss = 1.412972\n",
      "2024-12-06 07:13:54.375000: I runner.py:310] Step = 125200 ; steps/s = 1.64, tokens/s = 75164 (30720 source, 44444 target) ; Learning rate = 0.000250 ; Loss = 1.418064\n",
      "2024-12-06 07:14:55.274000: I runner.py:310] Step = 125300 ; steps/s = 1.64, tokens/s = 75135 (30722 source, 44413 target) ; Learning rate = 0.000250 ; Loss = 1.412827\n",
      "2024-12-06 07:15:55.716000: I runner.py:310] Step = 125400 ; steps/s = 1.65, tokens/s = 74384 (30429 source, 43955 target) ; Learning rate = 0.000250 ; Loss = 1.411519\n",
      "2024-12-06 07:16:56.577000: I runner.py:310] Step = 125500 ; steps/s = 1.64, tokens/s = 75171 (30736 source, 44435 target) ; Learning rate = 0.000250 ; Loss = 1.413673\n",
      "2024-12-06 07:17:57.117000: I runner.py:310] Step = 125600 ; steps/s = 1.65, tokens/s = 74287 (30399 source, 43888 target) ; Learning rate = 0.000249 ; Loss = 1.413700\n",
      "2024-12-06 07:18:58.044000: I runner.py:310] Step = 125700 ; steps/s = 1.64, tokens/s = 75092 (30693 source, 44399 target) ; Learning rate = 0.000249 ; Loss = 1.418436\n",
      "2024-12-06 07:19:58.901000: I runner.py:310] Step = 125800 ; steps/s = 1.64, tokens/s = 75170 (30733 source, 44437 target) ; Learning rate = 0.000249 ; Loss = 1.413973\n",
      "2024-12-06 07:20:59.414000: I runner.py:310] Step = 125900 ; steps/s = 1.65, tokens/s = 74295 (30394 source, 43901 target) ; Learning rate = 0.000249 ; Loss = 1.416272\n",
      "2024-12-06 07:22:00.285000: I runner.py:310] Step = 126000 ; steps/s = 1.64, tokens/s = 75163 (30728 source, 44435 target) ; Learning rate = 0.000249 ; Loss = 1.412717\n",
      "2024-12-06 07:23:00.779000: I runner.py:310] Step = 126100 ; steps/s = 1.65, tokens/s = 74342 (30429 source, 43913 target) ; Learning rate = 0.000249 ; Loss = 1.410558\n",
      "2024-12-06 07:24:01.643000: I runner.py:310] Step = 126200 ; steps/s = 1.64, tokens/s = 75165 (30728 source, 44437 target) ; Learning rate = 0.000249 ; Loss = 1.415859\n",
      "2024-12-06 07:25:02.540000: I runner.py:310] Step = 126300 ; steps/s = 1.64, tokens/s = 75126 (30707 source, 44419 target) ; Learning rate = 0.000249 ; Loss = 1.412840\n",
      "2024-12-06 07:26:03.033000: I runner.py:310] Step = 126400 ; steps/s = 1.65, tokens/s = 74309 (30395 source, 43914 target) ; Learning rate = 0.000249 ; Loss = 1.412837\n",
      "2024-12-06 07:27:03.922000: I runner.py:310] Step = 126500 ; steps/s = 1.64, tokens/s = 75167 (30745 source, 44422 target) ; Learning rate = 0.000249 ; Loss = 1.411100\n",
      "2024-12-06 07:28:04.378000: I runner.py:310] Step = 126600 ; steps/s = 1.65, tokens/s = 74368 (30424 source, 43944 target) ; Learning rate = 0.000248 ; Loss = 1.413328\n",
      "2024-12-06 07:29:05.197000: I runner.py:310] Step = 126700 ; steps/s = 1.64, tokens/s = 75229 (30760 source, 44469 target) ; Learning rate = 0.000248 ; Loss = 1.412180\n",
      "2024-12-06 07:30:06.075000: I runner.py:310] Step = 126800 ; steps/s = 1.64, tokens/s = 75154 (30725 source, 44429 target) ; Learning rate = 0.000248 ; Loss = 1.418641\n",
      "2024-12-06 07:31:06.464000: I runner.py:310] Step = 126900 ; steps/s = 1.66, tokens/s = 74441 (30445 source, 43996 target) ; Learning rate = 0.000248 ; Loss = 1.411747\n",
      "2024-12-06 07:32:07.408000: I runner.py:310] Step = 127000 ; steps/s = 1.64, tokens/s = 75070 (30693 source, 44377 target) ; Learning rate = 0.000248 ; Loss = 1.413173\n",
      "2024-12-06 07:33:07.933000: I runner.py:310] Step = 127100 ; steps/s = 1.65, tokens/s = 74302 (30409 source, 43893 target) ; Learning rate = 0.000248 ; Loss = 1.411543\n",
      "2024-12-06 07:34:08.824000: I runner.py:310] Step = 127200 ; steps/s = 1.64, tokens/s = 75125 (30704 source, 44421 target) ; Learning rate = 0.000248 ; Loss = 1.413581\n",
      "2024-12-06 07:35:09.742000: I runner.py:310] Step = 127300 ; steps/s = 1.64, tokens/s = 75121 (30725 source, 44396 target) ; Learning rate = 0.000248 ; Loss = 1.416641\n",
      "2024-12-06 07:36:10.262000: I runner.py:310] Step = 127400 ; steps/s = 1.65, tokens/s = 74277 (30380 source, 43897 target) ; Learning rate = 0.000248 ; Loss = 1.416038\n",
      "2024-12-06 07:37:11.157000: I runner.py:310] Step = 127500 ; steps/s = 1.64, tokens/s = 75142 (30729 source, 44413 target) ; Learning rate = 0.000248 ; Loss = 1.415982\n",
      "2024-12-06 07:38:11.647000: I runner.py:310] Step = 127600 ; steps/s = 1.65, tokens/s = 74332 (30412 source, 43920 target) ; Learning rate = 0.000247 ; Loss = 1.414577\n",
      "2024-12-06 07:39:12.528000: I runner.py:310] Step = 127700 ; steps/s = 1.64, tokens/s = 75141 (30716 source, 44425 target) ; Learning rate = 0.000247 ; Loss = 1.414542\n",
      "2024-12-06 07:40:13.473000: I runner.py:310] Step = 127800 ; steps/s = 1.64, tokens/s = 75078 (30693 source, 44385 target) ; Learning rate = 0.000247 ; Loss = 1.412880\n",
      "2024-12-06 07:41:13.969000: I runner.py:310] Step = 127900 ; steps/s = 1.65, tokens/s = 74345 (30429 source, 43916 target) ; Learning rate = 0.000247 ; Loss = 1.417553\n",
      "2024-12-06 07:42:14.822000: I runner.py:310] Step = 128000 ; steps/s = 1.64, tokens/s = 75184 (30741 source, 44443 target) ; Learning rate = 0.000247 ; Loss = 1.412389\n",
      "2024-12-06 07:43:15.303000: I runner.py:310] Step = 128100 ; steps/s = 1.65, tokens/s = 74331 (30405 source, 43926 target) ; Learning rate = 0.000247 ; Loss = 1.410666\n",
      "2024-12-06 07:44:16.168000: I runner.py:310] Step = 128200 ; steps/s = 1.64, tokens/s = 75163 (30731 source, 44432 target) ; Learning rate = 0.000247 ; Loss = 1.419644\n",
      "2024-12-06 07:45:17.062000: I runner.py:310] Step = 128300 ; steps/s = 1.64, tokens/s = 75122 (30702 source, 44420 target) ; Learning rate = 0.000247 ; Loss = 1.416260\n",
      "2024-12-06 07:46:17.546000: I runner.py:310] Step = 128400 ; steps/s = 1.65, tokens/s = 74336 (30413 source, 43923 target) ; Learning rate = 0.000247 ; Loss = 1.411877\n",
      "2024-12-06 07:47:18.441000: I runner.py:310] Step = 128500 ; steps/s = 1.64, tokens/s = 75134 (30710 source, 44424 target) ; Learning rate = 0.000247 ; Loss = 1.413270\n",
      "2024-12-06 07:48:18.979000: I runner.py:310] Step = 128600 ; steps/s = 1.65, tokens/s = 74287 (30415 source, 43872 target) ; Learning rate = 0.000246 ; Loss = 1.411379\n",
      "2024-12-06 07:49:19.857000: I runner.py:310] Step = 128700 ; steps/s = 1.64, tokens/s = 75137 (30713 source, 44424 target) ; Learning rate = 0.000246 ; Loss = 1.420122\n",
      "2024-12-06 07:50:20.745000: I runner.py:310] Step = 128800 ; steps/s = 1.64, tokens/s = 75135 (30710 source, 44425 target) ; Learning rate = 0.000246 ; Loss = 1.415737\n",
      "2024-12-06 07:51:21.195000: I runner.py:310] Step = 128900 ; steps/s = 1.65, tokens/s = 74392 (30446 source, 43946 target) ; Learning rate = 0.000246 ; Loss = 1.413765\n",
      "2024-12-06 07:52:22.076000: I runner.py:310] Step = 129000 ; steps/s = 1.64, tokens/s = 75146 (30720 source, 44426 target) ; Learning rate = 0.000246 ; Loss = 1.415744\n",
      "2024-12-06 07:53:22.973000: I runner.py:310] Step = 129100 ; steps/s = 1.64, tokens/s = 75127 (30716 source, 44411 target) ; Learning rate = 0.000246 ; Loss = 1.420971\n",
      "2024-12-06 07:54:23.407000: I runner.py:310] Step = 129200 ; steps/s = 1.65, tokens/s = 74404 (30441 source, 43963 target) ; Learning rate = 0.000246 ; Loss = 1.410369\n",
      "2024-12-06 07:55:24.288000: I runner.py:310] Step = 129300 ; steps/s = 1.64, tokens/s = 75146 (30710 source, 44436 target) ; Learning rate = 0.000246 ; Loss = 1.417528\n",
      "2024-12-06 07:56:24.735000: I runner.py:310] Step = 129400 ; steps/s = 1.65, tokens/s = 74387 (30445 source, 43942 target) ; Learning rate = 0.000246 ; Loss = 1.415633\n",
      "2024-12-06 07:57:25.568000: I runner.py:310] Step = 129500 ; steps/s = 1.64, tokens/s = 75204 (30749 source, 44455 target) ; Learning rate = 0.000246 ; Loss = 1.414109\n",
      "2024-12-06 07:58:26.427000: I runner.py:310] Step = 129600 ; steps/s = 1.64, tokens/s = 75180 (30735 source, 44445 target) ; Learning rate = 0.000246 ; Loss = 1.412989\n",
      "2024-12-06 07:59:26.882000: I runner.py:310] Step = 129700 ; steps/s = 1.65, tokens/s = 74370 (30424 source, 43946 target) ; Learning rate = 0.000245 ; Loss = 1.415580\n",
      "2024-12-06 08:00:27.778000: I runner.py:310] Step = 129800 ; steps/s = 1.64, tokens/s = 75134 (30721 source, 44413 target) ; Learning rate = 0.000245 ; Loss = 1.418514\n",
      "2024-12-06 08:01:28.245000: I runner.py:310] Step = 129900 ; steps/s = 1.65, tokens/s = 74366 (30428 source, 43938 target) ; Learning rate = 0.000245 ; Loss = 1.409626\n",
      "2024-12-06 08:02:29.130000: I runner.py:310] Step = 130000 ; steps/s = 1.64, tokens/s = 75131 (30712 source, 44419 target) ; Learning rate = 0.000245 ; Loss = 1.414392\n",
      "2024-12-06 08:02:31.151000: I training.py:176] Saved checkpoint TR-EN_std/ckpt-130000\n",
      "2024-12-06 08:02:31.151000: I training.py:192] Running evaluation for step 130000\n",
      "2024-12-06 08:03:17.850000: I training.py:192] Evaluation result for step 130000: loss = 1.022238 ; perplexity = 2.779409\n",
      "2024-12-06 08:04:18.606000: I runner.py:310] Step = 130100 ; steps/s = 1.65, tokens/s = 75326 (30802 source, 44524 target) ; Learning rate = 0.000245 ; Loss = 1.415971\n",
      "2024-12-06 08:05:19.145000: I runner.py:310] Step = 130200 ; steps/s = 1.65, tokens/s = 74271 (30387 source, 43884 target) ; Learning rate = 0.000245 ; Loss = 1.415004\n",
      "2024-12-06 08:06:20.081000: I runner.py:310] Step = 130300 ; steps/s = 1.64, tokens/s = 75096 (30706 source, 44390 target) ; Learning rate = 0.000245 ; Loss = 1.414390\n",
      "2024-12-06 08:07:20.588000: I runner.py:310] Step = 130400 ; steps/s = 1.65, tokens/s = 74298 (30396 source, 43902 target) ; Learning rate = 0.000245 ; Loss = 1.414358\n",
      "2024-12-06 08:08:21.486000: I runner.py:310] Step = 130500 ; steps/s = 1.64, tokens/s = 75115 (30698 source, 44417 target) ; Learning rate = 0.000245 ; Loss = 1.416841\n",
      "2024-12-06 08:09:22.373000: I runner.py:310] Step = 130600 ; steps/s = 1.64, tokens/s = 75157 (30736 source, 44421 target) ; Learning rate = 0.000245 ; Loss = 1.419872\n",
      "2024-12-06 08:10:22.894000: I runner.py:310] Step = 130700 ; steps/s = 1.65, tokens/s = 74289 (30396 source, 43893 target) ; Learning rate = 0.000244 ; Loss = 1.414836\n",
      "2024-12-06 08:11:23.744000: I runner.py:310] Step = 130800 ; steps/s = 1.64, tokens/s = 75180 (30732 source, 44448 target) ; Learning rate = 0.000244 ; Loss = 1.414841\n",
      "2024-12-06 08:12:24.302000: I runner.py:310] Step = 130900 ; steps/s = 1.65, tokens/s = 74246 (30377 source, 43869 target) ; Learning rate = 0.000244 ; Loss = 1.412942\n",
      "2024-12-06 08:13:25.202000: I runner.py:310] Step = 131000 ; steps/s = 1.64, tokens/s = 75133 (30722 source, 44411 target) ; Learning rate = 0.000244 ; Loss = 1.414853\n",
      "2024-12-06 08:14:26.120000: I runner.py:310] Step = 131100 ; steps/s = 1.64, tokens/s = 75096 (30698 source, 44398 target) ; Learning rate = 0.000244 ; Loss = 1.414774\n",
      "2024-12-06 08:15:26.533000: I runner.py:310] Step = 131200 ; steps/s = 1.66, tokens/s = 74426 (30448 source, 43978 target) ; Learning rate = 0.000244 ; Loss = 1.415010\n",
      "2024-12-06 08:16:27.426000: I runner.py:310] Step = 131300 ; steps/s = 1.64, tokens/s = 75134 (30715 source, 44419 target) ; Learning rate = 0.000244 ; Loss = 1.412894\n",
      "2024-12-06 08:17:27.846000: I runner.py:310] Step = 131400 ; steps/s = 1.66, tokens/s = 74422 (30457 source, 43965 target) ; Learning rate = 0.000244 ; Loss = 1.412144\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Tr-En base model\n",
    "!onmt-main --model kk-tr-en-shared.py --config tr-en.yml --auto_config train --with_eval --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e765433b-e7f2-4db7-8abf-709b4f9a86b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-06 08:18:28.973271: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-06 08:18:29.782963: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-06 08:18:29.783034: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-06 08:18:29.783042: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-06 08:18:30.777000: I main.py:308] Loading model description from TR-EN_std/model_description.py\n",
      "2024-12-06 08:18:30.976000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-06 08:18:30.976000: I main.py:315] Using model:\n",
      "(model): MyCustomTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): WordEmbedder()\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): WordEmbedder()\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-06 08:18:30.981000: I main.py:340] Using parameters:\n",
      "data:\n",
      "  eval_features_file: Tatoeba_tokens_dev\n",
      "  eval_labels_file: Tatoeba_dev_target_tokens.txt\n",
      "  source_vocabulary: tr_vocab.vocab\n",
      "  target_vocabulary: en_vocab.vocab\n",
      "  train_features_file: Tatoeba_tokens_train\n",
      "  train_labels_file: Tatoeba_train_target_tokens.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: TR-EN_std\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 250000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-06 08:18:31.186221: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-06 08:18:31.768853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-06 08:18:31.925000: I inputter.py:316] Initialized source input layer:\n",
      "2024-12-06 08:18:31.925000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-06 08:18:31.925000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-06 08:18:32.002000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-06 08:18:32.003000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-06 08:18:32.003000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-06 08:18:32.023000: I runner.py:462] Restored checkpoint TR-EN_std/ckpt-100000\n",
      "2024-12-06 08:18:32.062000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-06 08:18:32.600311: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-06 08:18:32.713000: I runner.py:471] Tracing and optimizing the inference graph...\n",
      "2024-12-06 08:18:46.946305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-06 08:18:47.829661: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-06 08:19:00.865000: I runner.py:471] 3013 predictions are buffered, but waiting for the prediction of queued line 29 to advance the output...\n",
      "2024-12-06 08:19:17.599000: I runner.py:471] 6828 predictions are buffered, but waiting for the prediction of queued line 54 to advance the output...\n",
      "2024-12-06 08:19:27.678000: I runner.py:471] 9196 predictions are buffered, but waiting for the prediction of queued line 54 to advance the output...\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 onmt-main --config tr-en.yml --auto_config --checkpoint_path TR-EN_std/ckpt-100000 infer --features_file Tatoeba_tokens_test --predictions_file output_tr_en_std.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f61e5d7-1d0c-426f-90ce-982ea824b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 MT-Preparation/subwording/3-desubword.py en_vocab.model output_tr_en_std.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b10b27d5-eb55-462b-b0e7-219febfb3d31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference first sentence: I won't stay there very long.\n",
      "Translated first sentence: I won't stay there for too long .\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "BLEU:  BLEU = 58.14 79.2/62.9/52.2/43.9 (BP = 1.000 ratio = 1.018 hyp_len = 78947 ref_len = 77587)\n",
      "CHRF:  chrF2 = 72.67\n"
     ]
    }
   ],
   "source": [
    "# BLEU and chrF scores\n",
    "!python3 compute-bleu.py Tatoeba.en-tr.en-filtered.en.test output_tr_en_std.txt.desubword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fb5b991-bcf5-4919-b433-a6c6a9f2773a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama METEOR Puan: 0.8038217751678343\n"
     ]
    }
   ],
   "source": [
    "# Average METEOR score (Ortalama METEOR Puan)\n",
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "def read_and_tokenize_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return [nltk.word_tokenize(line.strip()) for line in lines]\n",
    "\n",
    "def calculate_meteor(reference_file, hypothesis_file):\n",
    "    references = read_and_tokenize_file(reference_file)\n",
    "    hypotheses = read_and_tokenize_file(hypothesis_file)\n",
    "    \n",
    "    if len(references) != len(hypotheses):\n",
    "        raise ValueError(\"Dosyalarn satr saylar elemiyor\")\n",
    "\n",
    "    total_meteor_score = 0.0\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        total_meteor_score += meteor_score([ref], hyp)\n",
    "\n",
    "    average_meteor_score = total_meteor_score / len(references)\n",
    "    return average_meteor_score\n",
    "\n",
    "reference_file = 'Tatoeba.en-tr.en-filtered.en.test'\n",
    "hypothesis_file = 'output_tr_en_std.txt.desubword'\n",
    "\n",
    "score = calculate_meteor(reference_file, hypothesis_file)\n",
    "print(f\"Ortalama METEOR Puan: {score}\") #Average METEOR score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52a77c3b-467e-4e90-8bcc-cac37a8850fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-07 02:44:57.213104: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-07 02:44:58.011251: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-07 02:44:58.011331: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-07 02:44:58.011343: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-07 02:44:58.992000: I onmt-main:8] Creating model directory POS_TR_EN\n",
      "2024-12-07 02:44:59.189000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-07 02:44:59.189000: I main.py:315] Using model:\n",
      "(model): MultiFeaturesTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): ParallelInputter(\n",
      "      (inputters): ListWrapper(\n",
      "        (0): WordEmbedder()\n",
      "        (1): WordEmbedder()\n",
      "      )\n",
      "      (reducer): ConcatReducer()\n",
      "    )\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): ParallelInputter(\n",
      "        (inputters): ListWrapper(\n",
      "          (0): WordEmbedder()\n",
      "          (1): WordEmbedder()\n",
      "        )\n",
      "        (reducer): ConcatReducer()\n",
      "      )\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-07 02:44:59.192270: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-07 02:45:00.727649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-07 02:45:00.728380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7705 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-12-07 02:45:00.728878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 6099 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
      "2024-12-07 02:45:00.733000: I main.py:325] Using parameters:\n",
      "data:\n",
      "  eval_features_file:\n",
      "  - Tatoeba_tokens_dev\n",
      "  - Tatoeba_pos_tags_dev.txt\n",
      "  eval_labels_file: Tatoeba_dev_target_tokens.txt\n",
      "  source_1_vocabulary: tr_vocab.vocab\n",
      "  source_2_vocabulary: Zemberek_TR_unique_pos\n",
      "  target_vocabulary: en_vocab.vocab\n",
      "  train_features_file:\n",
      "  - Tatoeba_tokens_train\n",
      "  - Tatoeba_pos_tags_train.txt\n",
      "  train_labels_file: Tatoeba_train_target_tokens.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: POS_TR_EN\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 205000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-07 02:45:01.059000: I inputter.py:316] Initialized source_1 input layer:\n",
      "2024-12-07 02:45:01.059000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-07 02:45:01.059000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-07 02:45:01.062000: I inputter.py:316] Initialized source_2 input layer:\n",
      "2024-12-07 02:45:01.062000: I inputter.py:316]  - vocabulary size: 21\n",
      "2024-12-07 02:45:01.062000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-07 02:45:01.132000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-07 02:45:01.132000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-07 02:45:01.133000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-07 02:45:01.137000: W runner.py:269] No checkpoint to restore in POS_TR_EN\n",
      "2024-12-07 02:45:01.140000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "2024-12-07 02:45:01.194000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-07 02:45:02.288858: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-07 02:45:02.417000: I main.py:325] Accumulate gradients of 7 iterations to reach effective batch size of 25000\n",
      "2024-12-07 02:45:02.540000: I mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2024-12-07 02:45:02.683000: I dataset_ops.py:2542] Training on 647485 examples\n",
      "2024-12-07 02:46:09.982497: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-07 02:46:11.070147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-07 02:46:11.321186: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-07 02:46:20.928000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-07 02:46:20.953000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-07 02:46:22.535000: I cross_device_ops.py:897] batch_all_reduce: 261 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-07 02:46:27.584000: I cross_device_ops.py:897] batch_all_reduce: 261 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2024-12-07 02:46:34.200000: I runner.py:310] Number of model parameters: 93357261\n",
      "2024-12-07 02:46:34.204000: I runner.py:310] Number of model weights: 261 (trainable = 261, non trainable = 0)\n",
      "2024-12-07 02:46:34.237000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-07 02:46:36.349000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-1\n",
      "2024-12-07 02:46:37.073000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-07 02:46:37.097000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-07 02:46:37.760000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-07 02:46:37.784000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-07 02:46:38.399000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-07 02:46:38.422000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-07 02:46:39.004000: I cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-12-07 02:47:38.124000: I runner.py:310] Step = 100 ; steps/s = 1.61, tokens/s = 43541 (43541 target) ; Learning rate = 0.000009 ; Loss = 9.548411\n",
      "2024-12-07 02:48:40.071000: I runner.py:310] Step = 200 ; steps/s = 1.61, tokens/s = 43660 (43660 target) ; Learning rate = 0.000018 ; Loss = 8.487081\n",
      "2024-12-07 02:49:41.674000: I runner.py:310] Step = 300 ; steps/s = 1.62, tokens/s = 43123 (43123 target) ; Learning rate = 0.000027 ; Loss = 7.164573\n",
      "2024-12-07 02:50:42.580000: I runner.py:310] Step = 400 ; steps/s = 1.64, tokens/s = 44410 (44410 target) ; Learning rate = 0.000035 ; Loss = 6.184612\n",
      "2024-12-07 02:51:43.670000: I runner.py:310] Step = 500 ; steps/s = 1.64, tokens/s = 44270 (44270 target) ; Learning rate = 0.000044 ; Loss = 5.712969\n",
      "2024-12-07 02:52:44.792000: I runner.py:310] Step = 600 ; steps/s = 1.64, tokens/s = 43462 (43462 target) ; Learning rate = 0.000053 ; Loss = 5.366898\n",
      "2024-12-07 02:53:45.765000: I runner.py:310] Step = 700 ; steps/s = 1.64, tokens/s = 44353 (44353 target) ; Learning rate = 0.000062 ; Loss = 4.845012\n",
      "2024-12-07 02:54:46.401000: I runner.py:310] Step = 800 ; steps/s = 1.65, tokens/s = 43823 (43823 target) ; Learning rate = 0.000071 ; Loss = 4.579994\n",
      "2024-12-07 02:55:47.387000: I runner.py:310] Step = 900 ; steps/s = 1.64, tokens/s = 44351 (44351 target) ; Learning rate = 0.000080 ; Loss = 4.484880\n",
      "2024-12-07 02:56:48.376000: I runner.py:310] Step = 1000 ; steps/s = 1.64, tokens/s = 44342 (44342 target) ; Learning rate = 0.000088 ; Loss = 4.363109\n",
      "2024-12-07 02:57:48.870000: I runner.py:310] Step = 1100 ; steps/s = 1.65, tokens/s = 43912 (43912 target) ; Learning rate = 0.000097 ; Loss = 4.094810\n",
      "2024-12-07 02:58:49.808000: I runner.py:310] Step = 1200 ; steps/s = 1.64, tokens/s = 44392 (44392 target) ; Learning rate = 0.000106 ; Loss = 3.970687\n",
      "2024-12-07 02:59:50.320000: I runner.py:310] Step = 1300 ; steps/s = 1.65, tokens/s = 43903 (43903 target) ; Learning rate = 0.000115 ; Loss = 3.864270\n",
      "2024-12-07 03:00:51.225000: I runner.py:310] Step = 1400 ; steps/s = 1.64, tokens/s = 44410 (44410 target) ; Learning rate = 0.000124 ; Loss = 3.725603\n",
      "2024-12-07 03:01:52.169000: I runner.py:310] Step = 1500 ; steps/s = 1.64, tokens/s = 44377 (44377 target) ; Learning rate = 0.000133 ; Loss = 3.741377\n",
      "2024-12-07 03:02:52.593000: I runner.py:310] Step = 1600 ; steps/s = 1.66, tokens/s = 43960 (43960 target) ; Learning rate = 0.000142 ; Loss = 3.511639\n",
      "2024-12-07 03:03:53.447000: I runner.py:310] Step = 1700 ; steps/s = 1.64, tokens/s = 44450 (44450 target) ; Learning rate = 0.000150 ; Loss = 3.312742\n",
      "2024-12-07 03:04:53.840000: I runner.py:310] Step = 1800 ; steps/s = 1.66, tokens/s = 43983 (43983 target) ; Learning rate = 0.000159 ; Loss = 3.242912\n",
      "2024-12-07 03:05:54.650000: I runner.py:310] Step = 1900 ; steps/s = 1.64, tokens/s = 44475 (44475 target) ; Learning rate = 0.000168 ; Loss = 3.047717\n",
      "2024-12-07 03:06:55.525000: I runner.py:310] Step = 2000 ; steps/s = 1.64, tokens/s = 44428 (44428 target) ; Learning rate = 0.000177 ; Loss = 2.810247\n",
      "2024-12-07 03:07:55.941000: I runner.py:310] Step = 2100 ; steps/s = 1.66, tokens/s = 43979 (43979 target) ; Learning rate = 0.000186 ; Loss = 2.797007\n",
      "2024-12-07 03:08:56.817000: I runner.py:310] Step = 2200 ; steps/s = 1.64, tokens/s = 44434 (44434 target) ; Learning rate = 0.000195 ; Loss = 2.685316\n",
      "2024-12-07 03:09:57.252000: I runner.py:310] Step = 2300 ; steps/s = 1.65, tokens/s = 43949 (43949 target) ; Learning rate = 0.000203 ; Loss = 2.614548\n",
      "2024-12-07 03:10:58.104000: I runner.py:310] Step = 2400 ; steps/s = 1.64, tokens/s = 44448 (44448 target) ; Learning rate = 0.000212 ; Loss = 2.573115\n",
      "2024-12-07 03:11:58.981000: I runner.py:310] Step = 2500 ; steps/s = 1.64, tokens/s = 44430 (44430 target) ; Learning rate = 0.000221 ; Loss = 2.486439\n",
      "2024-12-07 03:12:59.447000: I runner.py:310] Step = 2600 ; steps/s = 1.65, tokens/s = 43935 (43935 target) ; Learning rate = 0.000230 ; Loss = 2.435133\n",
      "2024-12-07 03:14:00.307000: I runner.py:310] Step = 2700 ; steps/s = 1.64, tokens/s = 44443 (44443 target) ; Learning rate = 0.000239 ; Loss = 2.613997\n",
      "2024-12-07 03:15:00.718000: I runner.py:310] Step = 2800 ; steps/s = 1.66, tokens/s = 43973 (43973 target) ; Learning rate = 0.000248 ; Loss = 2.404722\n",
      "2024-12-07 03:16:01.604000: I runner.py:310] Step = 2900 ; steps/s = 1.64, tokens/s = 44419 (44419 target) ; Learning rate = 0.000256 ; Loss = 2.323974\n",
      "2024-12-07 03:17:02.574000: I runner.py:310] Step = 3000 ; steps/s = 1.64, tokens/s = 44363 (44363 target) ; Learning rate = 0.000265 ; Loss = 2.204730\n",
      "2024-12-07 03:18:03.005000: I runner.py:310] Step = 3100 ; steps/s = 1.65, tokens/s = 43958 (43958 target) ; Learning rate = 0.000274 ; Loss = 2.222831\n",
      "2024-12-07 03:19:03.884000: I runner.py:310] Step = 3200 ; steps/s = 1.64, tokens/s = 44431 (44431 target) ; Learning rate = 0.000283 ; Loss = 2.215441\n",
      "2024-12-07 03:20:04.329000: I runner.py:310] Step = 3300 ; steps/s = 1.65, tokens/s = 43952 (43952 target) ; Learning rate = 0.000292 ; Loss = 2.489416\n",
      "2024-12-07 03:21:05.210000: I runner.py:310] Step = 3400 ; steps/s = 1.64, tokens/s = 44424 (44424 target) ; Learning rate = 0.000301 ; Loss = 2.303798\n",
      "2024-12-07 03:22:06.121000: I runner.py:310] Step = 3500 ; steps/s = 1.64, tokens/s = 44405 (44405 target) ; Learning rate = 0.000309 ; Loss = 2.331311\n",
      "2024-12-07 03:23:06.599000: I runner.py:310] Step = 3600 ; steps/s = 1.65, tokens/s = 43926 (43926 target) ; Learning rate = 0.000318 ; Loss = 2.134645\n",
      "2024-12-07 03:24:07.471000: I runner.py:310] Step = 3700 ; steps/s = 1.64, tokens/s = 44436 (44436 target) ; Learning rate = 0.000327 ; Loss = 2.231144\n",
      "2024-12-07 03:25:07.950000: I runner.py:310] Step = 3800 ; steps/s = 1.65, tokens/s = 43923 (43923 target) ; Learning rate = 0.000336 ; Loss = 2.224808\n",
      "2024-12-07 03:26:08.841000: I runner.py:310] Step = 3900 ; steps/s = 1.64, tokens/s = 44413 (44413 target) ; Learning rate = 0.000345 ; Loss = 2.102027\n",
      "2024-12-07 03:27:09.786000: I runner.py:310] Step = 4000 ; steps/s = 1.64, tokens/s = 44386 (44386 target) ; Learning rate = 0.000354 ; Loss = 2.140390\n",
      "2024-12-07 03:28:10.301000: I runner.py:310] Step = 4100 ; steps/s = 1.65, tokens/s = 43896 (43896 target) ; Learning rate = 0.000362 ; Loss = 2.255045\n",
      "2024-12-07 03:29:11.275000: I runner.py:310] Step = 4200 ; steps/s = 1.64, tokens/s = 44356 (44356 target) ; Learning rate = 0.000371 ; Loss = 2.014371\n",
      "2024-12-07 03:30:12.162000: I runner.py:310] Step = 4300 ; steps/s = 1.64, tokens/s = 44426 (44426 target) ; Learning rate = 0.000380 ; Loss = 2.156765\n",
      "2024-12-07 03:31:12.672000: I runner.py:310] Step = 4400 ; steps/s = 1.65, tokens/s = 43905 (43905 target) ; Learning rate = 0.000389 ; Loss = 2.130712\n",
      "2024-12-07 03:32:13.619000: I runner.py:310] Step = 4500 ; steps/s = 1.64, tokens/s = 44370 (44370 target) ; Learning rate = 0.000398 ; Loss = 1.973172\n",
      "2024-12-07 03:33:14.108000: I runner.py:310] Step = 4600 ; steps/s = 1.65, tokens/s = 43925 (43925 target) ; Learning rate = 0.000407 ; Loss = 1.986924\n",
      "2024-12-07 03:34:15.042000: I runner.py:310] Step = 4700 ; steps/s = 1.64, tokens/s = 44384 (44384 target) ; Learning rate = 0.000416 ; Loss = 1.954011\n",
      "2024-12-07 03:35:15.989000: I runner.py:310] Step = 4800 ; steps/s = 1.64, tokens/s = 44383 (44383 target) ; Learning rate = 0.000424 ; Loss = 2.096673\n",
      "2024-12-07 03:36:16.377000: I runner.py:310] Step = 4900 ; steps/s = 1.66, tokens/s = 43992 (43992 target) ; Learning rate = 0.000433 ; Loss = 1.940438\n",
      "2024-12-07 03:37:17.277000: I runner.py:310] Step = 5000 ; steps/s = 1.64, tokens/s = 44414 (44414 target) ; Learning rate = 0.000442 ; Loss = 1.924156\n",
      "2024-12-07 03:37:17.278000: I training.py:192] Running evaluation for step 5000\n",
      "2024-12-07 03:38:28.461000: I training.py:192] Evaluation result for step 5000: loss = 0.831456 ; perplexity = 2.296659\n",
      "2024-12-07 03:39:28.943000: I runner.py:310] Step = 5100 ; steps/s = 1.65, tokens/s = 43927 (43927 target) ; Learning rate = 0.000451 ; Loss = 1.942672\n",
      "2024-12-07 03:40:29.953000: I runner.py:310] Step = 5200 ; steps/s = 1.64, tokens/s = 44329 (44329 target) ; Learning rate = 0.000460 ; Loss = 1.953081\n",
      "2024-12-07 03:41:31.079000: I runner.py:310] Step = 5300 ; steps/s = 1.64, tokens/s = 44250 (44250 target) ; Learning rate = 0.000469 ; Loss = 1.923987\n",
      "2024-12-07 03:42:31.592000: I runner.py:310] Step = 5400 ; steps/s = 1.65, tokens/s = 43903 (43903 target) ; Learning rate = 0.000477 ; Loss = 1.880113\n",
      "2024-12-07 03:43:32.640000: I runner.py:310] Step = 5500 ; steps/s = 1.64, tokens/s = 44303 (44303 target) ; Learning rate = 0.000486 ; Loss = 1.899025\n",
      "2024-12-07 03:44:33.249000: I runner.py:310] Step = 5600 ; steps/s = 1.65, tokens/s = 43832 (43832 target) ; Learning rate = 0.000495 ; Loss = 1.929312\n",
      "2024-12-07 03:45:34.329000: I runner.py:310] Step = 5700 ; steps/s = 1.64, tokens/s = 44279 (44279 target) ; Learning rate = 0.000504 ; Loss = 1.877858\n",
      "2024-12-07 03:46:35.396000: I runner.py:310] Step = 5800 ; steps/s = 1.64, tokens/s = 44289 (44289 target) ; Learning rate = 0.000513 ; Loss = 1.863207\n",
      "2024-12-07 03:47:35.997000: I runner.py:310] Step = 5900 ; steps/s = 1.65, tokens/s = 43844 (43844 target) ; Learning rate = 0.000522 ; Loss = 1.853042\n",
      "2024-12-07 03:48:37.034000: I runner.py:310] Step = 6000 ; steps/s = 1.64, tokens/s = 44307 (44307 target) ; Learning rate = 0.000530 ; Loss = 1.830571\n",
      "2024-12-07 03:49:37.616000: I runner.py:310] Step = 6100 ; steps/s = 1.65, tokens/s = 43848 (43848 target) ; Learning rate = 0.000539 ; Loss = 1.905239\n",
      "2024-12-07 03:50:38.639000: I runner.py:310] Step = 6200 ; steps/s = 1.64, tokens/s = 44322 (44322 target) ; Learning rate = 0.000548 ; Loss = 1.823018\n",
      "2024-12-07 03:51:39.642000: I runner.py:310] Step = 6300 ; steps/s = 1.64, tokens/s = 44340 (44340 target) ; Learning rate = 0.000557 ; Loss = 1.858741\n",
      "2024-12-07 03:52:40.234000: I runner.py:310] Step = 6400 ; steps/s = 1.65, tokens/s = 43847 (43847 target) ; Learning rate = 0.000566 ; Loss = 1.840164\n",
      "2024-12-07 03:53:41.222000: I runner.py:310] Step = 6500 ; steps/s = 1.64, tokens/s = 44348 (44348 target) ; Learning rate = 0.000575 ; Loss = 1.877994\n",
      "2024-12-07 03:54:41.773000: I runner.py:310] Step = 6600 ; steps/s = 1.65, tokens/s = 43867 (43867 target) ; Learning rate = 0.000583 ; Loss = 1.824842\n",
      "2024-12-07 03:55:42.747000: I runner.py:310] Step = 6700 ; steps/s = 1.64, tokens/s = 44359 (44359 target) ; Learning rate = 0.000592 ; Loss = 1.853491\n",
      "2024-12-07 03:56:43.743000: I runner.py:310] Step = 6800 ; steps/s = 1.64, tokens/s = 44336 (44336 target) ; Learning rate = 0.000601 ; Loss = 1.793125\n",
      "2024-12-07 03:57:44.211000: I runner.py:310] Step = 6900 ; steps/s = 1.65, tokens/s = 43938 (43938 target) ; Learning rate = 0.000610 ; Loss = 1.820062\n",
      "2024-12-07 03:58:45.171000: I runner.py:310] Step = 7000 ; steps/s = 1.64, tokens/s = 44372 (44372 target) ; Learning rate = 0.000619 ; Loss = 1.822394\n",
      "2024-12-07 03:59:45.660000: I runner.py:310] Step = 7100 ; steps/s = 1.65, tokens/s = 43918 (43918 target) ; Learning rate = 0.000628 ; Loss = 1.806646\n",
      "2024-12-07 04:00:46.628000: I runner.py:310] Step = 7200 ; steps/s = 1.64, tokens/s = 44361 (44361 target) ; Learning rate = 0.000636 ; Loss = 1.774103\n",
      "2024-12-07 04:01:47.568000: I runner.py:310] Step = 7300 ; steps/s = 1.64, tokens/s = 44384 (44384 target) ; Learning rate = 0.000645 ; Loss = 1.807626\n",
      "2024-12-07 04:02:48.005000: I runner.py:310] Step = 7400 ; steps/s = 1.65, tokens/s = 43961 (43961 target) ; Learning rate = 0.000654 ; Loss = 1.819354\n",
      "2024-12-07 04:03:48.995000: I runner.py:310] Step = 7500 ; steps/s = 1.64, tokens/s = 44344 (44344 target) ; Learning rate = 0.000663 ; Loss = 1.778793\n",
      "2024-12-07 04:04:49.570000: I runner.py:310] Step = 7600 ; steps/s = 1.65, tokens/s = 43857 (43857 target) ; Learning rate = 0.000672 ; Loss = 1.808207\n",
      "2024-12-07 04:05:50.497000: I runner.py:310] Step = 7700 ; steps/s = 1.64, tokens/s = 44393 (44393 target) ; Learning rate = 0.000681 ; Loss = 1.753528\n",
      "2024-12-07 04:06:51.485000: I runner.py:310] Step = 7800 ; steps/s = 1.64, tokens/s = 44340 (44340 target) ; Learning rate = 0.000690 ; Loss = 1.764559\n",
      "2024-12-07 04:07:52.024000: I runner.py:310] Step = 7900 ; steps/s = 1.65, tokens/s = 43885 (43885 target) ; Learning rate = 0.000698 ; Loss = 1.740610\n",
      "2024-12-07 04:08:52.946000: I runner.py:310] Step = 8000 ; steps/s = 1.64, tokens/s = 44396 (44396 target) ; Learning rate = 0.000707 ; Loss = 1.756165\n",
      "2024-12-07 04:09:53.647000: I runner.py:310] Step = 8100 ; steps/s = 1.65, tokens/s = 43993 (43993 target) ; Learning rate = 0.000716 ; Loss = 2.298369\n",
      "2024-12-07 04:10:54.495000: I runner.py:310] Step = 8200 ; steps/s = 1.64, tokens/s = 44219 (44219 target) ; Learning rate = 0.000725 ; Loss = 1.722699\n",
      "2024-12-07 04:11:55.447000: I runner.py:310] Step = 8300 ; steps/s = 1.64, tokens/s = 44375 (44375 target) ; Learning rate = 0.000734 ; Loss = 1.759254\n",
      "2024-12-07 04:12:55.914000: I runner.py:310] Step = 8400 ; steps/s = 1.65, tokens/s = 43934 (43934 target) ; Learning rate = 0.000743 ; Loss = 1.738459\n",
      "2024-12-07 04:13:56.895000: I runner.py:310] Step = 8500 ; steps/s = 1.64, tokens/s = 44357 (44357 target) ; Learning rate = 0.000751 ; Loss = 1.787768\n",
      "2024-12-07 04:14:57.884000: I runner.py:310] Step = 8600 ; steps/s = 1.64, tokens/s = 44342 (44342 target) ; Learning rate = 0.000760 ; Loss = 1.747761\n",
      "2024-12-07 04:15:58.434000: I runner.py:310] Step = 8700 ; steps/s = 1.65, tokens/s = 43878 (43878 target) ; Learning rate = 0.000769 ; Loss = 1.710824\n",
      "2024-12-07 04:16:59.416000: I runner.py:310] Step = 8800 ; steps/s = 1.64, tokens/s = 44349 (44349 target) ; Learning rate = 0.000778 ; Loss = 1.715389\n",
      "2024-12-07 04:17:59.930000: I runner.py:310] Step = 8900 ; steps/s = 1.65, tokens/s = 43902 (43902 target) ; Learning rate = 0.000787 ; Loss = 1.713502\n",
      "2024-12-07 04:19:00.844000: I runner.py:310] Step = 9000 ; steps/s = 1.64, tokens/s = 44408 (44408 target) ; Learning rate = 0.000796 ; Loss = 1.767131\n",
      "2024-12-07 04:20:01.805000: I runner.py:310] Step = 9100 ; steps/s = 1.64, tokens/s = 44362 (44362 target) ; Learning rate = 0.000804 ; Loss = 1.731928\n",
      "2024-12-07 04:21:02.264000: I runner.py:310] Step = 9200 ; steps/s = 1.65, tokens/s = 43940 (43940 target) ; Learning rate = 0.000813 ; Loss = 1.705150\n",
      "2024-12-07 04:22:03.194000: I runner.py:310] Step = 9300 ; steps/s = 1.64, tokens/s = 44390 (44390 target) ; Learning rate = 0.000822 ; Loss = 1.706136\n",
      "2024-12-07 04:23:03.728000: I runner.py:310] Step = 9400 ; steps/s = 1.65, tokens/s = 43887 (43887 target) ; Learning rate = 0.000831 ; Loss = 1.691143\n",
      "2024-12-07 04:24:04.652000: I runner.py:310] Step = 9500 ; steps/s = 1.64, tokens/s = 44396 (44396 target) ; Learning rate = 0.000840 ; Loss = 1.716554\n",
      "2024-12-07 04:25:05.603000: I runner.py:310] Step = 9600 ; steps/s = 1.64, tokens/s = 44371 (44371 target) ; Learning rate = 0.000849 ; Loss = 1.697188\n",
      "2024-12-07 04:26:06.102000: I runner.py:310] Step = 9700 ; steps/s = 1.65, tokens/s = 43907 (43907 target) ; Learning rate = 0.000857 ; Loss = 1.688580\n",
      "2024-12-07 04:27:07.044000: I runner.py:310] Step = 9800 ; steps/s = 1.64, tokens/s = 44384 (44384 target) ; Learning rate = 0.000866 ; Loss = 1.701146\n",
      "2024-12-07 04:28:07.534000: I runner.py:310] Step = 9900 ; steps/s = 1.65, tokens/s = 43921 (43921 target) ; Learning rate = 0.000875 ; Loss = 1.673265\n",
      "2024-12-07 04:29:08.475000: I runner.py:310] Step = 10000 ; steps/s = 1.64, tokens/s = 44387 (44387 target) ; Learning rate = 0.000884 ; Loss = 1.716534\n",
      "2024-12-07 04:29:10.655000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-10000\n",
      "2024-12-07 04:29:10.655000: I training.py:192] Running evaluation for step 10000\n",
      "2024-12-07 04:30:09.202000: I training.py:192] Evaluation result for step 10000: loss = 0.762668 ; perplexity = 2.143990\n",
      "2024-12-07 04:31:10.099000: I runner.py:310] Step = 10100 ; steps/s = 1.64, tokens/s = 44419 (44419 target) ; Learning rate = 0.000879 ; Loss = 1.687318\n",
      "2024-12-07 04:32:10.608000: I runner.py:310] Step = 10200 ; steps/s = 1.65, tokens/s = 43902 (43902 target) ; Learning rate = 0.000875 ; Loss = 1.661371\n",
      "2024-12-07 04:33:11.572000: I runner.py:310] Step = 10300 ; steps/s = 1.64, tokens/s = 44367 (44367 target) ; Learning rate = 0.000871 ; Loss = 1.671931\n",
      "2024-12-07 04:34:12.125000: I runner.py:310] Step = 10400 ; steps/s = 1.65, tokens/s = 43875 (43875 target) ; Learning rate = 0.000867 ; Loss = 1.666267\n",
      "2024-12-07 04:35:13.066000: I runner.py:310] Step = 10500 ; steps/s = 1.64, tokens/s = 44383 (44383 target) ; Learning rate = 0.000863 ; Loss = 1.680628\n",
      "2024-12-07 04:36:14.128000: I runner.py:310] Step = 10600 ; steps/s = 1.64, tokens/s = 44292 (44292 target) ; Learning rate = 0.000858 ; Loss = 1.680067\n",
      "2024-12-07 04:37:14.678000: I runner.py:310] Step = 10700 ; steps/s = 1.65, tokens/s = 43879 (43879 target) ; Learning rate = 0.000854 ; Loss = 1.665598\n",
      "2024-12-07 04:38:15.598000: I runner.py:310] Step = 10800 ; steps/s = 1.64, tokens/s = 44402 (44402 target) ; Learning rate = 0.000850 ; Loss = 1.658437\n",
      "2024-12-07 04:39:16.071000: I runner.py:310] Step = 10900 ; steps/s = 1.65, tokens/s = 43928 (43928 target) ; Learning rate = 0.000847 ; Loss = 1.638527\n",
      "2024-12-07 04:40:16.989000: I runner.py:310] Step = 11000 ; steps/s = 1.64, tokens/s = 44397 (44397 target) ; Learning rate = 0.000843 ; Loss = 1.657570\n",
      "2024-12-07 04:41:18.021000: I runner.py:310] Step = 11100 ; steps/s = 1.64, tokens/s = 44317 (44317 target) ; Learning rate = 0.000839 ; Loss = 1.660128\n",
      "2024-12-07 04:42:18.551000: I runner.py:310] Step = 11200 ; steps/s = 1.65, tokens/s = 43889 (43889 target) ; Learning rate = 0.000835 ; Loss = 1.645521\n",
      "2024-12-07 04:43:19.460000: I runner.py:310] Step = 11300 ; steps/s = 1.64, tokens/s = 44403 (44403 target) ; Learning rate = 0.000831 ; Loss = 1.651608\n",
      "2024-12-07 04:44:19.946000: I runner.py:310] Step = 11400 ; steps/s = 1.65, tokens/s = 43921 (43921 target) ; Learning rate = 0.000828 ; Loss = 1.651458\n",
      "2024-12-07 04:45:20.881000: I runner.py:310] Step = 11500 ; steps/s = 1.64, tokens/s = 44390 (44390 target) ; Learning rate = 0.000824 ; Loss = 1.625159\n",
      "2024-12-07 04:46:21.820000: I runner.py:310] Step = 11600 ; steps/s = 1.64, tokens/s = 44376 (44376 target) ; Learning rate = 0.000821 ; Loss = 1.655881\n",
      "2024-12-07 04:47:22.341000: I runner.py:310] Step = 11700 ; steps/s = 1.65, tokens/s = 43890 (43890 target) ; Learning rate = 0.000817 ; Loss = 1.639951\n",
      "2024-12-07 04:48:23.240000: I runner.py:310] Step = 11800 ; steps/s = 1.64, tokens/s = 44414 (44414 target) ; Learning rate = 0.000814 ; Loss = 1.636014\n",
      "2024-12-07 04:49:23.820000: I runner.py:310] Step = 11900 ; steps/s = 1.65, tokens/s = 43854 (43854 target) ; Learning rate = 0.000810 ; Loss = 1.623161\n",
      "2024-12-07 04:50:24.680000: I runner.py:310] Step = 12000 ; steps/s = 1.64, tokens/s = 44431 (44431 target) ; Learning rate = 0.000807 ; Loss = 1.612502\n",
      "2024-12-07 04:51:25.734000: I runner.py:310] Step = 12100 ; steps/s = 1.64, tokens/s = 44305 (44305 target) ; Learning rate = 0.000803 ; Loss = 1.634450\n",
      "2024-12-07 04:52:26.171000: I runner.py:310] Step = 12200 ; steps/s = 1.65, tokens/s = 43957 (43957 target) ; Learning rate = 0.000800 ; Loss = 1.614024\n",
      "2024-12-07 04:53:27.169000: I runner.py:310] Step = 12300 ; steps/s = 1.64, tokens/s = 44342 (44342 target) ; Learning rate = 0.000797 ; Loss = 1.629425\n",
      "2024-12-07 04:54:28.135000: I runner.py:310] Step = 12400 ; steps/s = 1.64, tokens/s = 44367 (44367 target) ; Learning rate = 0.000794 ; Loss = 1.636619\n",
      "2024-12-07 04:55:28.719000: I runner.py:310] Step = 12500 ; steps/s = 1.65, tokens/s = 43853 (43853 target) ; Learning rate = 0.000791 ; Loss = 1.627505\n",
      "2024-12-07 04:56:29.672000: I runner.py:310] Step = 12600 ; steps/s = 1.64, tokens/s = 44367 (44367 target) ; Learning rate = 0.000787 ; Loss = 1.623216\n",
      "2024-12-07 04:57:30.194000: I runner.py:310] Step = 12700 ; steps/s = 1.65, tokens/s = 43898 (43898 target) ; Learning rate = 0.000784 ; Loss = 1.595697\n",
      "2024-12-07 04:58:31.135000: I runner.py:310] Step = 12800 ; steps/s = 1.64, tokens/s = 44379 (44379 target) ; Learning rate = 0.000781 ; Loss = 1.606982\n",
      "2024-12-07 04:59:32.104000: I runner.py:310] Step = 12900 ; steps/s = 1.64, tokens/s = 44364 (44364 target) ; Learning rate = 0.000778 ; Loss = 1.625059\n",
      "2024-12-07 05:00:32.617000: I runner.py:310] Step = 13000 ; steps/s = 1.65, tokens/s = 43900 (43900 target) ; Learning rate = 0.000775 ; Loss = 1.594830\n",
      "2024-12-07 05:01:33.631000: I runner.py:310] Step = 13100 ; steps/s = 1.64, tokens/s = 44337 (44337 target) ; Learning rate = 0.000772 ; Loss = 1.606530\n",
      "2024-12-07 05:02:34.122000: I runner.py:310] Step = 13200 ; steps/s = 1.65, tokens/s = 43906 (43906 target) ; Learning rate = 0.000769 ; Loss = 1.591288\n",
      "2024-12-07 05:03:35.116000: I runner.py:310] Step = 13300 ; steps/s = 1.64, tokens/s = 44342 (44342 target) ; Learning rate = 0.000766 ; Loss = 1.603547\n",
      "2024-12-07 05:04:36.061000: I runner.py:310] Step = 13400 ; steps/s = 1.64, tokens/s = 44382 (44382 target) ; Learning rate = 0.000764 ; Loss = 1.601830\n",
      "2024-12-07 05:05:36.589000: I runner.py:310] Step = 13500 ; steps/s = 1.65, tokens/s = 43890 (43890 target) ; Learning rate = 0.000761 ; Loss = 1.590986\n",
      "2024-12-07 05:06:37.583000: I runner.py:310] Step = 13600 ; steps/s = 1.64, tokens/s = 44340 (44340 target) ; Learning rate = 0.000758 ; Loss = 1.599226\n",
      "2024-12-07 05:07:38.043000: I runner.py:310] Step = 13700 ; steps/s = 1.65, tokens/s = 43944 (43944 target) ; Learning rate = 0.000755 ; Loss = 1.581052\n",
      "2024-12-07 05:08:38.989000: I runner.py:310] Step = 13800 ; steps/s = 1.64, tokens/s = 44376 (44376 target) ; Learning rate = 0.000752 ; Loss = 1.590871\n",
      "2024-12-07 05:09:39.930000: I runner.py:310] Step = 13900 ; steps/s = 1.64, tokens/s = 44384 (44384 target) ; Learning rate = 0.000750 ; Loss = 1.606121\n",
      "2024-12-07 05:10:40.434000: I runner.py:310] Step = 14000 ; steps/s = 1.65, tokens/s = 43904 (43904 target) ; Learning rate = 0.000747 ; Loss = 1.588167\n",
      "2024-12-07 05:11:41.408000: I runner.py:310] Step = 14100 ; steps/s = 1.64, tokens/s = 44357 (44357 target) ; Learning rate = 0.000744 ; Loss = 1.587319\n",
      "2024-12-07 05:12:41.913000: I runner.py:310] Step = 14200 ; steps/s = 1.65, tokens/s = 43906 (43906 target) ; Learning rate = 0.000742 ; Loss = 1.569527\n",
      "2024-12-07 05:13:42.789000: I runner.py:310] Step = 14300 ; steps/s = 1.64, tokens/s = 44426 (44426 target) ; Learning rate = 0.000739 ; Loss = 1.579033\n",
      "2024-12-07 05:14:43.733000: I runner.py:310] Step = 14400 ; steps/s = 1.64, tokens/s = 44383 (44383 target) ; Learning rate = 0.000737 ; Loss = 1.582455\n",
      "2024-12-07 05:15:44.303000: I runner.py:310] Step = 14500 ; steps/s = 1.65, tokens/s = 43858 (43858 target) ; Learning rate = 0.000734 ; Loss = 1.580892\n",
      "2024-12-07 05:16:45.283000: I runner.py:310] Step = 14600 ; steps/s = 1.64, tokens/s = 44353 (44353 target) ; Learning rate = 0.000731 ; Loss = 1.602463\n",
      "2024-12-07 05:17:45.887000: I runner.py:310] Step = 14700 ; steps/s = 1.65, tokens/s = 43841 (43841 target) ; Learning rate = 0.000729 ; Loss = 1.562497\n",
      "2024-12-07 05:18:46.859000: I runner.py:310] Step = 14800 ; steps/s = 1.64, tokens/s = 44357 (44357 target) ; Learning rate = 0.000727 ; Loss = 1.576204\n",
      "2024-12-07 05:19:47.844000: I runner.py:310] Step = 14900 ; steps/s = 1.64, tokens/s = 44348 (44348 target) ; Learning rate = 0.000724 ; Loss = 1.581028\n",
      "2024-12-07 05:20:48.428000: I runner.py:310] Step = 15000 ; steps/s = 1.65, tokens/s = 43848 (43848 target) ; Learning rate = 0.000722 ; Loss = 1.563178\n",
      "2024-12-07 05:20:48.430000: I training.py:192] Running evaluation for step 15000\n",
      "2024-12-07 05:21:40.863000: I training.py:192] Evaluation result for step 15000: loss = 0.774865 ; perplexity = 2.170300\n",
      "2024-12-07 05:22:41.673000: I runner.py:310] Step = 15100 ; steps/s = 1.64, tokens/s = 44487 (44487 target) ; Learning rate = 0.000719 ; Loss = 1.574520\n",
      "2024-12-07 05:23:42.193000: I runner.py:310] Step = 15200 ; steps/s = 1.65, tokens/s = 43898 (43898 target) ; Learning rate = 0.000717 ; Loss = 1.564473\n",
      "2024-12-07 05:24:43.154000: I runner.py:310] Step = 15300 ; steps/s = 1.64, tokens/s = 44366 (44366 target) ; Learning rate = 0.000715 ; Loss = 1.563087\n",
      "2024-12-07 05:25:44.163000: I runner.py:310] Step = 15400 ; steps/s = 1.64, tokens/s = 44335 (44335 target) ; Learning rate = 0.000712 ; Loss = 1.570222\n",
      "2024-12-07 05:26:44.692000: I runner.py:310] Step = 15500 ; steps/s = 1.65, tokens/s = 43896 (43896 target) ; Learning rate = 0.000710 ; Loss = 1.566449\n",
      "2024-12-07 05:27:45.623000: I runner.py:310] Step = 15600 ; steps/s = 1.64, tokens/s = 44388 (44388 target) ; Learning rate = 0.000708 ; Loss = 1.565711\n",
      "2024-12-07 05:28:46.184000: I runner.py:310] Step = 15700 ; steps/s = 1.65, tokens/s = 43867 (43867 target) ; Learning rate = 0.000705 ; Loss = 1.546395\n",
      "2024-12-07 05:29:47.101000: I runner.py:310] Step = 15800 ; steps/s = 1.64, tokens/s = 44398 (44398 target) ; Learning rate = 0.000703 ; Loss = 1.551136\n",
      "2024-12-07 05:30:48.046000: I runner.py:310] Step = 15900 ; steps/s = 1.64, tokens/s = 44383 (44383 target) ; Learning rate = 0.000701 ; Loss = 1.576183\n",
      "2024-12-07 05:31:48.545000: I runner.py:310] Step = 16000 ; steps/s = 1.65, tokens/s = 43910 (43910 target) ; Learning rate = 0.000699 ; Loss = 1.554673\n",
      "2024-12-07 05:32:49.523000: I runner.py:310] Step = 16100 ; steps/s = 1.64, tokens/s = 44351 (44351 target) ; Learning rate = 0.000697 ; Loss = 1.560808\n",
      "2024-12-07 05:33:50.349000: I runner.py:310] Step = 16200 ; steps/s = 1.64, tokens/s = 44091 (44091 target) ; Learning rate = 0.000694 ; Loss = 1.668045\n",
      "2024-12-07 05:34:51.056000: I runner.py:310] Step = 16300 ; steps/s = 1.65, tokens/s = 44138 (44138 target) ; Learning rate = 0.000692 ; Loss = 1.554354\n",
      "2024-12-07 05:35:52.099000: I runner.py:310] Step = 16400 ; steps/s = 1.64, tokens/s = 44304 (44304 target) ; Learning rate = 0.000690 ; Loss = 1.563928\n",
      "2024-12-07 05:36:52.676000: I runner.py:310] Step = 16500 ; steps/s = 1.65, tokens/s = 43853 (43853 target) ; Learning rate = 0.000688 ; Loss = 1.550009\n",
      "2024-12-07 05:37:53.657000: I runner.py:310] Step = 16600 ; steps/s = 1.64, tokens/s = 44350 (44350 target) ; Learning rate = 0.000686 ; Loss = 1.555087\n",
      "2024-12-07 05:38:54.674000: I runner.py:310] Step = 16700 ; steps/s = 1.64, tokens/s = 44332 (44332 target) ; Learning rate = 0.000684 ; Loss = 1.557048\n",
      "2024-12-07 05:39:55.168000: I runner.py:310] Step = 16800 ; steps/s = 1.65, tokens/s = 43920 (43920 target) ; Learning rate = 0.000682 ; Loss = 1.539762\n",
      "2024-12-07 05:40:56.228000: I runner.py:310] Step = 16900 ; steps/s = 1.64, tokens/s = 44288 (44288 target) ; Learning rate = 0.000680 ; Loss = 1.551128\n",
      "2024-12-07 05:41:56.780000: I runner.py:310] Step = 17000 ; steps/s = 1.65, tokens/s = 43878 (43878 target) ; Learning rate = 0.000678 ; Loss = 1.549874\n",
      "2024-12-07 05:42:57.758000: I runner.py:310] Step = 17100 ; steps/s = 1.64, tokens/s = 44358 (44358 target) ; Learning rate = 0.000676 ; Loss = 1.546220\n",
      "2024-12-07 05:43:58.758000: I runner.py:310] Step = 17200 ; steps/s = 1.64, tokens/s = 44337 (44337 target) ; Learning rate = 0.000674 ; Loss = 1.550072\n",
      "2024-12-07 05:44:59.183000: I runner.py:310] Step = 17300 ; steps/s = 1.66, tokens/s = 43969 (43969 target) ; Learning rate = 0.000672 ; Loss = 1.539365\n",
      "2024-12-07 05:46:00.153000: I runner.py:310] Step = 17400 ; steps/s = 1.64, tokens/s = 44356 (44356 target) ; Learning rate = 0.000670 ; Loss = 1.554670\n",
      "2024-12-07 05:47:00.738000: I runner.py:310] Step = 17500 ; steps/s = 1.65, tokens/s = 43844 (43844 target) ; Learning rate = 0.000668 ; Loss = 1.535682\n",
      "2024-12-07 05:48:01.732000: I runner.py:310] Step = 17600 ; steps/s = 1.64, tokens/s = 44344 (44344 target) ; Learning rate = 0.000666 ; Loss = 1.541977\n",
      "2024-12-07 05:49:02.679000: I runner.py:310] Step = 17700 ; steps/s = 1.64, tokens/s = 44377 (44377 target) ; Learning rate = 0.000664 ; Loss = 1.557212\n",
      "2024-12-07 05:50:03.188000: I runner.py:310] Step = 17800 ; steps/s = 1.65, tokens/s = 43903 (43903 target) ; Learning rate = 0.000662 ; Loss = 1.534130\n",
      "2024-12-07 05:51:04.154000: I runner.py:310] Step = 17900 ; steps/s = 1.64, tokens/s = 44362 (44362 target) ; Learning rate = 0.000661 ; Loss = 1.542564\n",
      "2024-12-07 05:52:04.660000: I runner.py:310] Step = 18000 ; steps/s = 1.65, tokens/s = 43907 (43907 target) ; Learning rate = 0.000659 ; Loss = 1.537001\n",
      "2024-12-07 05:53:05.687000: I runner.py:310] Step = 18100 ; steps/s = 1.64, tokens/s = 44322 (44322 target) ; Learning rate = 0.000657 ; Loss = 1.542935\n",
      "2024-12-07 05:54:06.592000: I runner.py:310] Step = 18200 ; steps/s = 1.64, tokens/s = 44412 (44412 target) ; Learning rate = 0.000655 ; Loss = 1.545366\n",
      "2024-12-07 05:55:07.115000: I runner.py:310] Step = 18300 ; steps/s = 1.65, tokens/s = 43895 (43895 target) ; Learning rate = 0.000653 ; Loss = 1.537581\n",
      "2024-12-07 05:56:08.074000: I runner.py:310] Step = 18400 ; steps/s = 1.64, tokens/s = 44369 (44369 target) ; Learning rate = 0.000652 ; Loss = 1.534412\n",
      "2024-12-07 05:57:08.583000: I runner.py:310] Step = 18500 ; steps/s = 1.65, tokens/s = 43905 (43905 target) ; Learning rate = 0.000650 ; Loss = 1.518294\n",
      "2024-12-07 05:58:09.494000: I runner.py:310] Step = 18600 ; steps/s = 1.64, tokens/s = 44401 (44401 target) ; Learning rate = 0.000648 ; Loss = 1.540414\n",
      "2024-12-07 05:59:10.457000: I runner.py:310] Step = 18700 ; steps/s = 1.64, tokens/s = 44362 (44362 target) ; Learning rate = 0.000646 ; Loss = 1.537986\n",
      "2024-12-07 06:00:11.054000: I runner.py:310] Step = 18800 ; steps/s = 1.65, tokens/s = 43838 (43838 target) ; Learning rate = 0.000645 ; Loss = 1.542862\n",
      "2024-12-07 06:01:12.003000: I runner.py:310] Step = 18900 ; steps/s = 1.64, tokens/s = 44380 (44380 target) ; Learning rate = 0.000643 ; Loss = 1.533558\n",
      "2024-12-07 06:02:12.473000: I runner.py:310] Step = 19000 ; steps/s = 1.65, tokens/s = 43934 (43934 target) ; Learning rate = 0.000641 ; Loss = 1.517545\n",
      "2024-12-07 06:03:13.373000: I runner.py:310] Step = 19100 ; steps/s = 1.64, tokens/s = 44415 (44415 target) ; Learning rate = 0.000640 ; Loss = 1.531229\n",
      "2024-12-07 06:04:14.355000: I runner.py:310] Step = 19200 ; steps/s = 1.64, tokens/s = 44354 (44354 target) ; Learning rate = 0.000638 ; Loss = 1.541610\n",
      "2024-12-07 06:05:14.906000: I runner.py:310] Step = 19300 ; steps/s = 1.65, tokens/s = 43874 (43874 target) ; Learning rate = 0.000636 ; Loss = 1.526797\n",
      "2024-12-07 06:06:15.867000: I runner.py:310] Step = 19400 ; steps/s = 1.64, tokens/s = 44368 (44368 target) ; Learning rate = 0.000635 ; Loss = 1.533610\n",
      "2024-12-07 06:07:16.387000: I runner.py:310] Step = 19500 ; steps/s = 1.65, tokens/s = 43891 (43891 target) ; Learning rate = 0.000633 ; Loss = 1.527345\n",
      "2024-12-07 06:08:17.317000: I runner.py:310] Step = 19600 ; steps/s = 1.64, tokens/s = 44388 (44388 target) ; Learning rate = 0.000631 ; Loss = 1.514286\n",
      "2024-12-07 06:09:18.325000: I runner.py:310] Step = 19700 ; steps/s = 1.64, tokens/s = 44335 (44335 target) ; Learning rate = 0.000630 ; Loss = 1.547053\n",
      "2024-12-07 06:10:18.812000: I runner.py:310] Step = 19800 ; steps/s = 1.65, tokens/s = 43924 (43924 target) ; Learning rate = 0.000628 ; Loss = 1.527836\n",
      "2024-12-07 06:11:19.764000: I runner.py:310] Step = 19900 ; steps/s = 1.64, tokens/s = 44369 (44369 target) ; Learning rate = 0.000627 ; Loss = 1.531143\n",
      "2024-12-07 06:12:20.285000: I runner.py:310] Step = 20000 ; steps/s = 1.65, tokens/s = 43898 (43898 target) ; Learning rate = 0.000625 ; Loss = 1.515992\n",
      "2024-12-07 06:12:22.547000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-20000\n",
      "2024-12-07 06:12:22.547000: I training.py:192] Running evaluation for step 20000\n",
      "2024-12-07 06:13:15.314000: I training.py:192] Evaluation result for step 20000: loss = 0.810714 ; perplexity = 2.249513\n",
      "2024-12-07 06:14:16.217000: I runner.py:310] Step = 20100 ; steps/s = 1.64, tokens/s = 44416 (44416 target) ; Learning rate = 0.000623 ; Loss = 1.514414\n",
      "2024-12-07 06:15:17.230000: I runner.py:310] Step = 20200 ; steps/s = 1.64, tokens/s = 44329 (44329 target) ; Learning rate = 0.000622 ; Loss = 1.526820\n",
      "2024-12-07 06:16:17.789000: I runner.py:310] Step = 20300 ; steps/s = 1.65, tokens/s = 43869 (43869 target) ; Learning rate = 0.000620 ; Loss = 1.517934\n",
      "2024-12-07 06:17:18.753000: I runner.py:310] Step = 20400 ; steps/s = 1.64, tokens/s = 44366 (44366 target) ; Learning rate = 0.000619 ; Loss = 1.526768\n",
      "2024-12-07 06:18:19.787000: I runner.py:310] Step = 20500 ; steps/s = 1.64, tokens/s = 44316 (44316 target) ; Learning rate = 0.000617 ; Loss = 1.535649\n",
      "2024-12-07 06:19:20.321000: I runner.py:310] Step = 20600 ; steps/s = 1.65, tokens/s = 43883 (43883 target) ; Learning rate = 0.000616 ; Loss = 1.522316\n",
      "2024-12-07 06:20:21.295000: I runner.py:310] Step = 20700 ; steps/s = 1.64, tokens/s = 44359 (44359 target) ; Learning rate = 0.000614 ; Loss = 1.529372\n",
      "2024-12-07 06:21:21.803000: I runner.py:310] Step = 20800 ; steps/s = 1.65, tokens/s = 43906 (43906 target) ; Learning rate = 0.000613 ; Loss = 1.516731\n",
      "2024-12-07 06:22:22.781000: I runner.py:310] Step = 20900 ; steps/s = 1.64, tokens/s = 44349 (44349 target) ; Learning rate = 0.000611 ; Loss = 1.520075\n",
      "2024-12-07 06:23:23.764000: I runner.py:310] Step = 21000 ; steps/s = 1.64, tokens/s = 44359 (44359 target) ; Learning rate = 0.000610 ; Loss = 1.529206\n",
      "2024-12-07 06:24:24.268000: I runner.py:310] Step = 21100 ; steps/s = 1.65, tokens/s = 43906 (43906 target) ; Learning rate = 0.000608 ; Loss = 1.510142\n",
      "2024-12-07 06:25:25.265000: I runner.py:310] Step = 21200 ; steps/s = 1.64, tokens/s = 44339 (44339 target) ; Learning rate = 0.000607 ; Loss = 1.522603\n",
      "2024-12-07 06:26:25.807000: I runner.py:310] Step = 21300 ; steps/s = 1.65, tokens/s = 43882 (43882 target) ; Learning rate = 0.000606 ; Loss = 1.509184\n",
      "2024-12-07 06:27:26.835000: I runner.py:310] Step = 21400 ; steps/s = 1.64, tokens/s = 44317 (44317 target) ; Learning rate = 0.000604 ; Loss = 1.522265\n",
      "2024-12-07 06:28:27.826000: I runner.py:310] Step = 21500 ; steps/s = 1.64, tokens/s = 44341 (44341 target) ; Learning rate = 0.000603 ; Loss = 1.518988\n",
      "2024-12-07 06:29:28.290000: I runner.py:310] Step = 21600 ; steps/s = 1.65, tokens/s = 43939 (43939 target) ; Learning rate = 0.000601 ; Loss = 1.517713\n",
      "2024-12-07 06:30:29.247000: I runner.py:310] Step = 21700 ; steps/s = 1.64, tokens/s = 44371 (44371 target) ; Learning rate = 0.000600 ; Loss = 1.510437\n",
      "2024-12-07 06:31:29.806000: I runner.py:310] Step = 21800 ; steps/s = 1.65, tokens/s = 43871 (43871 target) ; Learning rate = 0.000599 ; Loss = 1.508888\n",
      "2024-12-07 06:32:30.806000: I runner.py:310] Step = 21900 ; steps/s = 1.64, tokens/s = 44330 (44330 target) ; Learning rate = 0.000597 ; Loss = 1.508729\n",
      "2024-12-07 06:33:31.872000: I runner.py:310] Step = 22000 ; steps/s = 1.64, tokens/s = 44295 (44295 target) ; Learning rate = 0.000596 ; Loss = 1.514101\n",
      "2024-12-07 06:34:32.461000: I runner.py:310] Step = 22100 ; steps/s = 1.65, tokens/s = 43847 (43847 target) ; Learning rate = 0.000595 ; Loss = 1.507083\n",
      "2024-12-07 06:35:33.475000: I runner.py:310] Step = 22200 ; steps/s = 1.64, tokens/s = 44333 (44333 target) ; Learning rate = 0.000593 ; Loss = 1.514552\n",
      "2024-12-07 06:36:34.016000: I runner.py:310] Step = 22300 ; steps/s = 1.65, tokens/s = 43879 (43879 target) ; Learning rate = 0.000592 ; Loss = 1.499710\n",
      "2024-12-07 06:37:34.972000: I runner.py:310] Step = 22400 ; steps/s = 1.64, tokens/s = 44371 (44371 target) ; Learning rate = 0.000591 ; Loss = 1.509178\n",
      "2024-12-07 06:38:35.972000: I runner.py:310] Step = 22500 ; steps/s = 1.64, tokens/s = 44336 (44336 target) ; Learning rate = 0.000589 ; Loss = 1.510805\n",
      "2024-12-07 06:39:36.461000: I runner.py:310] Step = 22600 ; steps/s = 1.65, tokens/s = 43922 (43922 target) ; Learning rate = 0.000588 ; Loss = 1.510711\n",
      "2024-12-07 06:40:37.425000: I runner.py:310] Step = 22700 ; steps/s = 1.64, tokens/s = 44369 (44369 target) ; Learning rate = 0.000587 ; Loss = 1.514607\n",
      "2024-12-07 06:41:38.034000: I runner.py:310] Step = 22800 ; steps/s = 1.65, tokens/s = 43827 (43827 target) ; Learning rate = 0.000585 ; Loss = 1.500327\n",
      "2024-12-07 06:42:38.986000: I runner.py:310] Step = 22900 ; steps/s = 1.64, tokens/s = 44372 (44372 target) ; Learning rate = 0.000584 ; Loss = 1.507582\n",
      "2024-12-07 06:43:39.980000: I runner.py:310] Step = 23000 ; steps/s = 1.64, tokens/s = 44344 (44344 target) ; Learning rate = 0.000583 ; Loss = 1.510831\n",
      "2024-12-07 06:44:40.582000: I runner.py:310] Step = 23100 ; steps/s = 1.65, tokens/s = 43835 (43835 target) ; Learning rate = 0.000582 ; Loss = 1.503866\n",
      "2024-12-07 06:45:41.524000: I runner.py:310] Step = 23200 ; steps/s = 1.64, tokens/s = 44379 (44379 target) ; Learning rate = 0.000580 ; Loss = 1.523168\n",
      "2024-12-07 06:46:42.051000: I runner.py:310] Step = 23300 ; steps/s = 1.65, tokens/s = 43894 (43894 target) ; Learning rate = 0.000579 ; Loss = 1.499717\n",
      "2024-12-07 06:47:43.025000: I runner.py:310] Step = 23400 ; steps/s = 1.64, tokens/s = 44357 (44357 target) ; Learning rate = 0.000578 ; Loss = 1.501057\n",
      "2024-12-07 06:48:44.033000: I runner.py:310] Step = 23500 ; steps/s = 1.64, tokens/s = 44333 (44333 target) ; Learning rate = 0.000577 ; Loss = 1.521551\n",
      "2024-12-07 06:49:44.566000: I runner.py:310] Step = 23600 ; steps/s = 1.65, tokens/s = 43884 (43884 target) ; Learning rate = 0.000575 ; Loss = 1.501876\n",
      "2024-12-07 06:50:45.517000: I runner.py:310] Step = 23700 ; steps/s = 1.64, tokens/s = 44376 (44376 target) ; Learning rate = 0.000574 ; Loss = 1.515377\n",
      "2024-12-07 06:51:46.057000: I runner.py:310] Step = 23800 ; steps/s = 1.65, tokens/s = 43884 (43884 target) ; Learning rate = 0.000573 ; Loss = 1.496161\n",
      "2024-12-07 06:52:47.018000: I runner.py:310] Step = 23900 ; steps/s = 1.64, tokens/s = 44365 (44365 target) ; Learning rate = 0.000572 ; Loss = 1.495121\n",
      "2024-12-07 06:53:48.031000: I runner.py:310] Step = 24000 ; steps/s = 1.64, tokens/s = 44328 (44328 target) ; Learning rate = 0.000571 ; Loss = 1.507016\n",
      "2024-12-07 06:54:48.619000: I runner.py:310] Step = 24100 ; steps/s = 1.65, tokens/s = 43849 (43849 target) ; Learning rate = 0.000569 ; Loss = 1.512488\n",
      "2024-12-07 06:55:49.604000: I runner.py:310] Step = 24200 ; steps/s = 1.64, tokens/s = 44354 (44354 target) ; Learning rate = 0.000568 ; Loss = 1.503045\n",
      "2024-12-07 06:56:50.511000: I runner.py:310] Step = 24300 ; steps/s = 1.64, tokens/s = 44131 (44131 target) ; Learning rate = 0.000567 ; Loss = 1.531097\n",
      "2024-12-07 06:57:51.158000: I runner.py:310] Step = 24400 ; steps/s = 1.65, tokens/s = 44078 (44078 target) ; Learning rate = 0.000566 ; Loss = 1.493904\n",
      "2024-12-07 06:58:52.184000: I runner.py:310] Step = 24500 ; steps/s = 1.64, tokens/s = 44321 (44321 target) ; Learning rate = 0.000565 ; Loss = 1.500097\n",
      "2024-12-07 06:59:52.772000: I runner.py:310] Step = 24600 ; steps/s = 1.65, tokens/s = 43842 (43842 target) ; Learning rate = 0.000564 ; Loss = 1.497030\n",
      "2024-12-07 07:00:53.762000: I runner.py:310] Step = 24700 ; steps/s = 1.64, tokens/s = 44345 (44345 target) ; Learning rate = 0.000562 ; Loss = 1.506974\n",
      "2024-12-07 07:01:54.698000: I runner.py:310] Step = 24800 ; steps/s = 1.64, tokens/s = 44390 (44390 target) ; Learning rate = 0.000561 ; Loss = 1.508550\n",
      "2024-12-07 07:02:55.192000: I runner.py:310] Step = 24900 ; steps/s = 1.65, tokens/s = 43917 (43917 target) ; Learning rate = 0.000560 ; Loss = 1.500748\n",
      "2024-12-07 07:03:56.194000: I runner.py:310] Step = 25000 ; steps/s = 1.64, tokens/s = 44335 (44335 target) ; Learning rate = 0.000559 ; Loss = 1.498945\n",
      "2024-12-07 07:03:56.195000: I training.py:192] Running evaluation for step 25000\n",
      "2024-12-07 07:04:46.379000: I training.py:192] Evaluation result for step 25000: loss = 0.816250 ; perplexity = 2.262002\n",
      "2024-12-07 07:05:46.839000: I runner.py:310] Step = 25100 ; steps/s = 1.65, tokens/s = 43954 (43954 target) ; Learning rate = 0.000558 ; Loss = 1.485540\n",
      "2024-12-07 07:06:47.872000: I runner.py:310] Step = 25200 ; steps/s = 1.64, tokens/s = 44315 (44315 target) ; Learning rate = 0.000557 ; Loss = 1.500774\n",
      "2024-12-07 07:07:48.901000: I runner.py:310] Step = 25300 ; steps/s = 1.64, tokens/s = 44305 (44305 target) ; Learning rate = 0.000556 ; Loss = 1.497717\n",
      "2024-12-07 07:08:49.487000: I runner.py:310] Step = 25400 ; steps/s = 1.65, tokens/s = 43853 (43853 target) ; Learning rate = 0.000555 ; Loss = 1.490806\n",
      "2024-12-07 07:09:50.568000: I runner.py:310] Step = 25500 ; steps/s = 1.64, tokens/s = 44280 (44280 target) ; Learning rate = 0.000553 ; Loss = 1.497775\n",
      "2024-12-07 07:10:51.090000: I runner.py:310] Step = 25600 ; steps/s = 1.65, tokens/s = 43894 (43894 target) ; Learning rate = 0.000552 ; Loss = 1.488534\n",
      "2024-12-07 07:11:52.094000: I runner.py:310] Step = 25700 ; steps/s = 1.64, tokens/s = 44342 (44342 target) ; Learning rate = 0.000551 ; Loss = 1.499718\n",
      "2024-12-07 07:12:53.098000: I runner.py:310] Step = 25800 ; steps/s = 1.64, tokens/s = 44332 (44332 target) ; Learning rate = 0.000550 ; Loss = 1.496669\n",
      "2024-12-07 07:13:53.618000: I runner.py:310] Step = 25900 ; steps/s = 1.65, tokens/s = 43889 (43889 target) ; Learning rate = 0.000549 ; Loss = 1.492612\n",
      "2024-12-07 07:14:54.614000: I runner.py:310] Step = 26000 ; steps/s = 1.64, tokens/s = 44340 (44340 target) ; Learning rate = 0.000548 ; Loss = 1.497518\n",
      "2024-12-07 07:15:55.209000: I runner.py:310] Step = 26100 ; steps/s = 1.65, tokens/s = 43853 (43853 target) ; Learning rate = 0.000547 ; Loss = 1.492971\n",
      "2024-12-07 07:16:56.221000: I runner.py:310] Step = 26200 ; steps/s = 1.64, tokens/s = 44320 (44320 target) ; Learning rate = 0.000546 ; Loss = 1.488342\n",
      "2024-12-07 07:17:57.263000: I runner.py:310] Step = 26300 ; steps/s = 1.64, tokens/s = 44310 (44310 target) ; Learning rate = 0.000545 ; Loss = 1.502259\n",
      "2024-12-07 07:18:57.811000: I runner.py:310] Step = 26400 ; steps/s = 1.65, tokens/s = 43877 (43877 target) ; Learning rate = 0.000544 ; Loss = 1.494702\n",
      "2024-12-07 07:19:58.763000: I runner.py:310] Step = 26500 ; steps/s = 1.64, tokens/s = 44373 (44373 target) ; Learning rate = 0.000543 ; Loss = 1.495013\n",
      "2024-12-07 07:20:59.268000: I runner.py:310] Step = 26600 ; steps/s = 1.65, tokens/s = 43909 (43909 target) ; Learning rate = 0.000542 ; Loss = 1.487498\n",
      "2024-12-07 07:22:00.198000: I runner.py:310] Step = 26700 ; steps/s = 1.64, tokens/s = 44388 (44388 target) ; Learning rate = 0.000541 ; Loss = 1.494539\n",
      "2024-12-07 07:23:01.267000: I runner.py:310] Step = 26800 ; steps/s = 1.64, tokens/s = 44292 (44292 target) ; Learning rate = 0.000540 ; Loss = 1.494719\n",
      "2024-12-07 07:24:01.826000: I runner.py:310] Step = 26900 ; steps/s = 1.65, tokens/s = 43866 (43866 target) ; Learning rate = 0.000539 ; Loss = 1.491573\n",
      "2024-12-07 07:25:02.820000: I runner.py:310] Step = 27000 ; steps/s = 1.64, tokens/s = 44337 (44337 target) ; Learning rate = 0.000538 ; Loss = 1.487540\n",
      "2024-12-07 07:26:03.371000: I runner.py:310] Step = 27100 ; steps/s = 1.65, tokens/s = 43880 (43880 target) ; Learning rate = 0.000537 ; Loss = 1.484807\n",
      "2024-12-07 07:27:04.315000: I runner.py:310] Step = 27200 ; steps/s = 1.64, tokens/s = 44381 (44381 target) ; Learning rate = 0.000536 ; Loss = 1.491148\n",
      "2024-12-07 07:28:05.349000: I runner.py:310] Step = 27300 ; steps/s = 1.64, tokens/s = 44313 (44313 target) ; Learning rate = 0.000535 ; Loss = 1.492170\n",
      "2024-12-07 07:29:05.872000: I runner.py:310] Step = 27400 ; steps/s = 1.65, tokens/s = 43890 (43890 target) ; Learning rate = 0.000534 ; Loss = 1.491398\n",
      "2024-12-07 07:30:06.861000: I runner.py:310] Step = 27500 ; steps/s = 1.64, tokens/s = 44339 (44339 target) ; Learning rate = 0.000533 ; Loss = 1.490729\n",
      "2024-12-07 07:31:07.360000: I runner.py:310] Step = 27600 ; steps/s = 1.65, tokens/s = 43915 (43915 target) ; Learning rate = 0.000532 ; Loss = 1.481919\n",
      "2024-12-07 07:32:08.320000: I runner.py:310] Step = 27700 ; steps/s = 1.64, tokens/s = 44366 (44366 target) ; Learning rate = 0.000531 ; Loss = 1.483438\n",
      "2024-12-07 07:33:09.314000: I runner.py:310] Step = 27800 ; steps/s = 1.64, tokens/s = 44349 (44349 target) ; Learning rate = 0.000530 ; Loss = 1.494170\n",
      "2024-12-07 07:34:09.825000: I runner.py:310] Step = 27900 ; steps/s = 1.65, tokens/s = 43899 (43899 target) ; Learning rate = 0.000529 ; Loss = 1.492305\n",
      "2024-12-07 07:35:10.854000: I runner.py:310] Step = 28000 ; steps/s = 1.64, tokens/s = 44317 (44317 target) ; Learning rate = 0.000528 ; Loss = 1.490679\n",
      "2024-12-07 07:36:11.488000: I runner.py:310] Step = 28100 ; steps/s = 1.65, tokens/s = 43825 (43825 target) ; Learning rate = 0.000527 ; Loss = 1.483511\n",
      "2024-12-07 07:37:12.497000: I runner.py:310] Step = 28200 ; steps/s = 1.64, tokens/s = 44326 (44326 target) ; Learning rate = 0.000526 ; Loss = 1.479353\n",
      "2024-12-07 07:38:13.465000: I runner.py:310] Step = 28300 ; steps/s = 1.64, tokens/s = 44358 (44358 target) ; Learning rate = 0.000525 ; Loss = 1.492348\n",
      "2024-12-07 07:39:14.081000: I runner.py:310] Step = 28400 ; steps/s = 1.65, tokens/s = 43835 (43835 target) ; Learning rate = 0.000524 ; Loss = 1.479192\n",
      "2024-12-07 07:40:15.090000: I runner.py:310] Step = 28500 ; steps/s = 1.64, tokens/s = 44331 (44331 target) ; Learning rate = 0.000524 ; Loss = 1.496130\n",
      "2024-12-07 07:41:16.065000: I runner.py:310] Step = 28600 ; steps/s = 1.64, tokens/s = 44357 (44357 target) ; Learning rate = 0.000523 ; Loss = 1.491428\n",
      "2024-12-07 07:42:16.569000: I runner.py:310] Step = 28700 ; steps/s = 1.65, tokens/s = 43909 (43909 target) ; Learning rate = 0.000522 ; Loss = 1.480044\n",
      "2024-12-07 07:43:17.601000: I runner.py:310] Step = 28800 ; steps/s = 1.64, tokens/s = 44304 (44304 target) ; Learning rate = 0.000521 ; Loss = 1.488727\n",
      "2024-12-07 07:44:18.141000: I runner.py:310] Step = 28900 ; steps/s = 1.65, tokens/s = 43889 (43889 target) ; Learning rate = 0.000520 ; Loss = 1.480001\n",
      "2024-12-07 07:45:19.161000: I runner.py:310] Step = 29000 ; steps/s = 1.64, tokens/s = 44330 (44330 target) ; Learning rate = 0.000519 ; Loss = 1.489553\n",
      "2024-12-07 07:46:20.148000: I runner.py:310] Step = 29100 ; steps/s = 1.64, tokens/s = 44343 (44343 target) ; Learning rate = 0.000518 ; Loss = 1.495783\n",
      "2024-12-07 07:47:20.736000: I runner.py:310] Step = 29200 ; steps/s = 1.65, tokens/s = 43848 (43848 target) ; Learning rate = 0.000517 ; Loss = 1.483862\n",
      "2024-12-07 07:48:21.709000: I runner.py:310] Step = 29300 ; steps/s = 1.64, tokens/s = 44355 (44355 target) ; Learning rate = 0.000516 ; Loss = 1.483011\n",
      "2024-12-07 07:49:22.372000: I runner.py:310] Step = 29400 ; steps/s = 1.65, tokens/s = 43794 (43794 target) ; Learning rate = 0.000515 ; Loss = 1.480116\n",
      "2024-12-07 07:50:23.372000: I runner.py:310] Step = 29500 ; steps/s = 1.64, tokens/s = 44332 (44332 target) ; Learning rate = 0.000515 ; Loss = 1.481396\n",
      "2024-12-07 07:51:24.406000: I runner.py:310] Step = 29600 ; steps/s = 1.64, tokens/s = 44318 (44318 target) ; Learning rate = 0.000514 ; Loss = 1.488858\n",
      "2024-12-07 07:52:24.914000: I runner.py:310] Step = 29700 ; steps/s = 1.65, tokens/s = 43906 (43906 target) ; Learning rate = 0.000513 ; Loss = 1.487795\n",
      "2024-12-07 07:53:25.963000: I runner.py:310] Step = 29800 ; steps/s = 1.64, tokens/s = 44300 (44300 target) ; Learning rate = 0.000512 ; Loss = 1.485273\n",
      "2024-12-07 07:54:26.500000: I runner.py:310] Step = 29900 ; steps/s = 1.65, tokens/s = 43892 (43892 target) ; Learning rate = 0.000511 ; Loss = 1.479051\n",
      "2024-12-07 07:55:27.405000: I runner.py:310] Step = 30000 ; steps/s = 1.64, tokens/s = 44402 (44402 target) ; Learning rate = 0.000510 ; Loss = 1.479578\n",
      "2024-12-07 07:55:29.542000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-30000\n",
      "2024-12-07 07:55:29.542000: I training.py:192] Running evaluation for step 30000\n",
      "2024-12-07 07:56:18.672000: I training.py:192] Evaluation result for step 30000: loss = 0.838089 ; perplexity = 2.311944\n",
      "2024-12-07 07:57:19.596000: I runner.py:310] Step = 30100 ; steps/s = 1.64, tokens/s = 44408 (44408 target) ; Learning rate = 0.000509 ; Loss = 1.487578\n",
      "2024-12-07 07:58:20.085000: I runner.py:310] Step = 30200 ; steps/s = 1.65, tokens/s = 43923 (43923 target) ; Learning rate = 0.000509 ; Loss = 1.477680\n",
      "2024-12-07 07:59:21.115000: I runner.py:310] Step = 30300 ; steps/s = 1.64, tokens/s = 44310 (44310 target) ; Learning rate = 0.000508 ; Loss = 1.481550\n",
      "2024-12-07 08:00:21.650000: I runner.py:310] Step = 30400 ; steps/s = 1.65, tokens/s = 43888 (43888 target) ; Learning rate = 0.000507 ; Loss = 1.474626\n",
      "2024-12-07 08:01:22.576000: I runner.py:310] Step = 30500 ; steps/s = 1.64, tokens/s = 44388 (44388 target) ; Learning rate = 0.000506 ; Loss = 1.485470\n",
      "2024-12-07 08:02:23.565000: I runner.py:310] Step = 30600 ; steps/s = 1.64, tokens/s = 44347 (44347 target) ; Learning rate = 0.000505 ; Loss = 1.478802\n",
      "2024-12-07 08:03:24.090000: I runner.py:310] Step = 30700 ; steps/s = 1.65, tokens/s = 43897 (43897 target) ; Learning rate = 0.000504 ; Loss = 1.475371\n",
      "2024-12-07 08:04:25.059000: I runner.py:310] Step = 30800 ; steps/s = 1.64, tokens/s = 44358 (44358 target) ; Learning rate = 0.000504 ; Loss = 1.484632\n",
      "2024-12-07 08:05:25.614000: I runner.py:310] Step = 30900 ; steps/s = 1.65, tokens/s = 43873 (43873 target) ; Learning rate = 0.000503 ; Loss = 1.471378\n",
      "2024-12-07 08:06:26.623000: I runner.py:310] Step = 31000 ; steps/s = 1.64, tokens/s = 44332 (44332 target) ; Learning rate = 0.000502 ; Loss = 1.478428\n",
      "2024-12-07 08:07:27.625000: I runner.py:310] Step = 31100 ; steps/s = 1.64, tokens/s = 44340 (44340 target) ; Learning rate = 0.000501 ; Loss = 1.483373\n",
      "2024-12-07 08:08:28.265000: I runner.py:310] Step = 31200 ; steps/s = 1.65, tokens/s = 43811 (43811 target) ; Learning rate = 0.000500 ; Loss = 1.480259\n",
      "2024-12-07 08:09:29.288000: I runner.py:310] Step = 31300 ; steps/s = 1.64, tokens/s = 44321 (44321 target) ; Learning rate = 0.000500 ; Loss = 1.479407\n",
      "2024-12-07 08:10:29.863000: I runner.py:310] Step = 31400 ; steps/s = 1.65, tokens/s = 43858 (43858 target) ; Learning rate = 0.000499 ; Loss = 1.471612\n",
      "2024-12-07 08:11:30.947000: I runner.py:310] Step = 31500 ; steps/s = 1.64, tokens/s = 44287 (44287 target) ; Learning rate = 0.000498 ; Loss = 1.483827\n",
      "2024-12-07 08:12:31.964000: I runner.py:310] Step = 31600 ; steps/s = 1.64, tokens/s = 44321 (44321 target) ; Learning rate = 0.000497 ; Loss = 1.495021\n",
      "2024-12-07 08:13:32.532000: I runner.py:310] Step = 31700 ; steps/s = 1.65, tokens/s = 43856 (43856 target) ; Learning rate = 0.000496 ; Loss = 1.479311\n",
      "2024-12-07 08:14:33.583000: I runner.py:310] Step = 31800 ; steps/s = 1.64, tokens/s = 44305 (44305 target) ; Learning rate = 0.000496 ; Loss = 1.493875\n",
      "2024-12-07 08:15:34.174000: I runner.py:310] Step = 31900 ; steps/s = 1.65, tokens/s = 43847 (43847 target) ; Learning rate = 0.000495 ; Loss = 1.469835\n",
      "2024-12-07 08:16:35.170000: I runner.py:310] Step = 32000 ; steps/s = 1.64, tokens/s = 44336 (44336 target) ; Learning rate = 0.000494 ; Loss = 1.477933\n",
      "2024-12-07 08:17:36.157000: I runner.py:310] Step = 32100 ; steps/s = 1.64, tokens/s = 44358 (44358 target) ; Learning rate = 0.000493 ; Loss = 1.482945\n",
      "2024-12-07 08:18:36.734000: I runner.py:310] Step = 32200 ; steps/s = 1.65, tokens/s = 43850 (43850 target) ; Learning rate = 0.000493 ; Loss = 1.472713\n",
      "2024-12-07 08:19:37.712000: I runner.py:310] Step = 32300 ; steps/s = 1.64, tokens/s = 44353 (44353 target) ; Learning rate = 0.000492 ; Loss = 1.480757\n",
      "2024-12-07 08:20:38.616000: I runner.py:310] Step = 32400 ; steps/s = 1.64, tokens/s = 44263 (44263 target) ; Learning rate = 0.000491 ; Loss = 1.497768\n",
      "2024-12-07 08:21:39.295000: I runner.py:310] Step = 32500 ; steps/s = 1.65, tokens/s = 43936 (43936 target) ; Learning rate = 0.000490 ; Loss = 1.471949\n",
      "2024-12-07 08:22:40.297000: I runner.py:310] Step = 32600 ; steps/s = 1.64, tokens/s = 44334 (44334 target) ; Learning rate = 0.000490 ; Loss = 1.481435\n",
      "2024-12-07 08:23:40.859000: I runner.py:310] Step = 32700 ; steps/s = 1.65, tokens/s = 43863 (43863 target) ; Learning rate = 0.000489 ; Loss = 1.475364\n",
      "2024-12-07 08:24:41.828000: I runner.py:310] Step = 32800 ; steps/s = 1.64, tokens/s = 44367 (44367 target) ; Learning rate = 0.000488 ; Loss = 1.481727\n",
      "2024-12-07 08:25:42.903000: I runner.py:310] Step = 32900 ; steps/s = 1.64, tokens/s = 44281 (44281 target) ; Learning rate = 0.000487 ; Loss = 1.475673\n",
      "2024-12-07 08:26:43.459000: I runner.py:310] Step = 33000 ; steps/s = 1.65, tokens/s = 43868 (43868 target) ; Learning rate = 0.000487 ; Loss = 1.472614\n",
      "2024-12-07 08:27:44.478000: I runner.py:310] Step = 33100 ; steps/s = 1.64, tokens/s = 44323 (44323 target) ; Learning rate = 0.000486 ; Loss = 1.477111\n",
      "2024-12-07 08:28:45.034000: I runner.py:310] Step = 33200 ; steps/s = 1.65, tokens/s = 43869 (43869 target) ; Learning rate = 0.000485 ; Loss = 1.469287\n",
      "2024-12-07 08:29:46.068000: I runner.py:310] Step = 33300 ; steps/s = 1.64, tokens/s = 44319 (44319 target) ; Learning rate = 0.000484 ; Loss = 1.470837\n",
      "2024-12-07 08:30:47.032000: I runner.py:310] Step = 33400 ; steps/s = 1.64, tokens/s = 44366 (44366 target) ; Learning rate = 0.000484 ; Loss = 1.475072\n",
      "2024-12-07 08:31:47.575000: I runner.py:310] Step = 33500 ; steps/s = 1.65, tokens/s = 43878 (43878 target) ; Learning rate = 0.000483 ; Loss = 1.474781\n",
      "2024-12-07 08:32:48.519000: I runner.py:310] Step = 33600 ; steps/s = 1.64, tokens/s = 44380 (44380 target) ; Learning rate = 0.000482 ; Loss = 1.479581\n",
      "2024-12-07 08:33:49.079000: I runner.py:310] Step = 33700 ; steps/s = 1.65, tokens/s = 43868 (43868 target) ; Learning rate = 0.000481 ; Loss = 1.470310\n",
      "2024-12-07 08:34:50.014000: I runner.py:310] Step = 33800 ; steps/s = 1.64, tokens/s = 44389 (44389 target) ; Learning rate = 0.000481 ; Loss = 1.475956\n",
      "2024-12-07 08:35:51.003000: I runner.py:310] Step = 33900 ; steps/s = 1.64, tokens/s = 44341 (44341 target) ; Learning rate = 0.000480 ; Loss = 1.473351\n",
      "2024-12-07 08:36:51.532000: I runner.py:310] Step = 34000 ; steps/s = 1.65, tokens/s = 43884 (43884 target) ; Learning rate = 0.000479 ; Loss = 1.473530\n",
      "2024-12-07 08:37:52.577000: I runner.py:310] Step = 34100 ; steps/s = 1.64, tokens/s = 44305 (44305 target) ; Learning rate = 0.000479 ; Loss = 1.477200\n",
      "2024-12-07 08:38:53.147000: I runner.py:310] Step = 34200 ; steps/s = 1.65, tokens/s = 43861 (43861 target) ; Learning rate = 0.000478 ; Loss = 1.465081\n",
      "2024-12-07 08:39:54.092000: I runner.py:310] Step = 34300 ; steps/s = 1.64, tokens/s = 44384 (44384 target) ; Learning rate = 0.000477 ; Loss = 1.474908\n",
      "2024-12-07 08:40:55.111000: I runner.py:310] Step = 34400 ; steps/s = 1.64, tokens/s = 44318 (44318 target) ; Learning rate = 0.000477 ; Loss = 1.473895\n",
      "2024-12-07 08:41:55.724000: I runner.py:310] Step = 34500 ; steps/s = 1.65, tokens/s = 43834 (43834 target) ; Learning rate = 0.000476 ; Loss = 1.475884\n",
      "2024-12-07 08:42:56.728000: I runner.py:310] Step = 34600 ; steps/s = 1.64, tokens/s = 44333 (44333 target) ; Learning rate = 0.000475 ; Loss = 1.474928\n",
      "2024-12-07 08:43:57.268000: I runner.py:310] Step = 34700 ; steps/s = 1.65, tokens/s = 43883 (43883 target) ; Learning rate = 0.000474 ; Loss = 1.468625\n",
      "2024-12-07 08:44:58.264000: I runner.py:310] Step = 34800 ; steps/s = 1.64, tokens/s = 44346 (44346 target) ; Learning rate = 0.000474 ; Loss = 1.465718\n",
      "2024-12-07 08:45:59.275000: I runner.py:310] Step = 34900 ; steps/s = 1.64, tokens/s = 44323 (44323 target) ; Learning rate = 0.000473 ; Loss = 1.476803\n",
      "2024-12-07 08:46:59.844000: I runner.py:310] Step = 35000 ; steps/s = 1.65, tokens/s = 43862 (43862 target) ; Learning rate = 0.000472 ; Loss = 1.468835\n",
      "2024-12-07 08:46:59.846000: I training.py:192] Running evaluation for step 35000\n",
      "2024-12-07 08:47:48.988000: I training.py:192] Evaluation result for step 35000: loss = 0.850821 ; perplexity = 2.341568\n",
      "2024-12-07 08:48:49.998000: I runner.py:310] Step = 35100 ; steps/s = 1.64, tokens/s = 44350 (44350 target) ; Learning rate = 0.000472 ; Loss = 1.475330\n",
      "2024-12-07 08:49:50.540000: I runner.py:310] Step = 35200 ; steps/s = 1.65, tokens/s = 43873 (43873 target) ; Learning rate = 0.000471 ; Loss = 1.462627\n",
      "2024-12-07 08:50:51.576000: I runner.py:310] Step = 35300 ; steps/s = 1.64, tokens/s = 44312 (44312 target) ; Learning rate = 0.000470 ; Loss = 1.474467\n",
      "2024-12-07 08:51:52.648000: I runner.py:310] Step = 35400 ; steps/s = 1.64, tokens/s = 44285 (44285 target) ; Learning rate = 0.000470 ; Loss = 1.475243\n",
      "2024-12-07 08:52:53.263000: I runner.py:310] Step = 35500 ; steps/s = 1.65, tokens/s = 43832 (43832 target) ; Learning rate = 0.000469 ; Loss = 1.470734\n",
      "2024-12-07 08:53:54.405000: I runner.py:310] Step = 35600 ; steps/s = 1.64, tokens/s = 44236 (44236 target) ; Learning rate = 0.000468 ; Loss = 1.474394\n",
      "2024-12-07 08:54:55.013000: I runner.py:310] Step = 35700 ; steps/s = 1.65, tokens/s = 43840 (43840 target) ; Learning rate = 0.000468 ; Loss = 1.467429\n",
      "2024-12-07 08:55:56.092000: I runner.py:310] Step = 35800 ; steps/s = 1.64, tokens/s = 44280 (44280 target) ; Learning rate = 0.000467 ; Loss = 1.467461\n",
      "2024-12-07 08:56:57.096000: I runner.py:310] Step = 35900 ; steps/s = 1.64, tokens/s = 44330 (44330 target) ; Learning rate = 0.000466 ; Loss = 1.485314\n",
      "2024-12-07 08:57:57.729000: I runner.py:310] Step = 36000 ; steps/s = 1.65, tokens/s = 43823 (43823 target) ; Learning rate = 0.000466 ; Loss = 1.468048\n",
      "2024-12-07 08:58:58.703000: I runner.py:310] Step = 36100 ; steps/s = 1.64, tokens/s = 44360 (44360 target) ; Learning rate = 0.000465 ; Loss = 1.466209\n",
      "2024-12-07 08:59:59.362000: I runner.py:310] Step = 36200 ; steps/s = 1.65, tokens/s = 43791 (43791 target) ; Learning rate = 0.000465 ; Loss = 1.459928\n",
      "2024-12-07 09:01:00.413000: I runner.py:310] Step = 36300 ; steps/s = 1.64, tokens/s = 44296 (44296 target) ; Learning rate = 0.000464 ; Loss = 1.470484\n",
      "2024-12-07 09:02:01.371000: I runner.py:310] Step = 36400 ; steps/s = 1.64, tokens/s = 44373 (44373 target) ; Learning rate = 0.000463 ; Loss = 1.470853\n",
      "2024-12-07 09:03:01.910000: I runner.py:310] Step = 36500 ; steps/s = 1.65, tokens/s = 43877 (43877 target) ; Learning rate = 0.000463 ; Loss = 1.462919\n",
      "2024-12-07 09:04:02.955000: I runner.py:310] Step = 36600 ; steps/s = 1.64, tokens/s = 44313 (44313 target) ; Learning rate = 0.000462 ; Loss = 1.474823\n",
      "2024-12-07 09:05:03.969000: I runner.py:310] Step = 36700 ; steps/s = 1.64, tokens/s = 44329 (44329 target) ; Learning rate = 0.000461 ; Loss = 1.470577\n",
      "2024-12-07 09:06:04.582000: I runner.py:310] Step = 36800 ; steps/s = 1.65, tokens/s = 43827 (43827 target) ; Learning rate = 0.000461 ; Loss = 1.465518\n",
      "2024-12-07 09:07:05.565000: I runner.py:310] Step = 36900 ; steps/s = 1.64, tokens/s = 44353 (44353 target) ; Learning rate = 0.000460 ; Loss = 1.465293\n",
      "2024-12-07 09:08:06.162000: I runner.py:310] Step = 37000 ; steps/s = 1.65, tokens/s = 43834 (43834 target) ; Learning rate = 0.000460 ; Loss = 1.468082\n",
      "2024-12-07 09:09:07.228000: I runner.py:310] Step = 37100 ; steps/s = 1.64, tokens/s = 44287 (44287 target) ; Learning rate = 0.000459 ; Loss = 1.470922\n",
      "2024-12-07 09:10:08.289000: I runner.py:310] Step = 37200 ; steps/s = 1.64, tokens/s = 44302 (44302 target) ; Learning rate = 0.000458 ; Loss = 1.468181\n",
      "2024-12-07 09:11:08.812000: I runner.py:310] Step = 37300 ; steps/s = 1.65, tokens/s = 43898 (43898 target) ; Learning rate = 0.000458 ; Loss = 1.461331\n",
      "2024-12-07 09:12:09.831000: I runner.py:310] Step = 37400 ; steps/s = 1.64, tokens/s = 44331 (44331 target) ; Learning rate = 0.000457 ; Loss = 1.465527\n",
      "2024-12-07 09:13:10.406000: I runner.py:310] Step = 37500 ; steps/s = 1.65, tokens/s = 43852 (43852 target) ; Learning rate = 0.000456 ; Loss = 1.462233\n",
      "2024-12-07 09:14:11.390000: I runner.py:310] Step = 37600 ; steps/s = 1.64, tokens/s = 44350 (44350 target) ; Learning rate = 0.000456 ; Loss = 1.468087\n",
      "2024-12-07 09:15:12.400000: I runner.py:310] Step = 37700 ; steps/s = 1.64, tokens/s = 44328 (44328 target) ; Learning rate = 0.000455 ; Loss = 1.468190\n",
      "2024-12-07 09:16:12.954000: I runner.py:310] Step = 37800 ; steps/s = 1.65, tokens/s = 43873 (43873 target) ; Learning rate = 0.000455 ; Loss = 1.464140\n",
      "2024-12-07 09:17:13.956000: I runner.py:310] Step = 37900 ; steps/s = 1.64, tokens/s = 44336 (44336 target) ; Learning rate = 0.000454 ; Loss = 1.465620\n",
      "2024-12-07 09:18:14.601000: I runner.py:310] Step = 38000 ; steps/s = 1.65, tokens/s = 43811 (43811 target) ; Learning rate = 0.000453 ; Loss = 1.459719\n",
      "2024-12-07 09:19:15.623000: I runner.py:310] Step = 38100 ; steps/s = 1.64, tokens/s = 44329 (44329 target) ; Learning rate = 0.000453 ; Loss = 1.472668\n",
      "2024-12-07 09:20:16.678000: I runner.py:310] Step = 38200 ; steps/s = 1.64, tokens/s = 44292 (44292 target) ; Learning rate = 0.000452 ; Loss = 1.473692\n",
      "2024-12-07 09:21:17.324000: I runner.py:310] Step = 38300 ; steps/s = 1.65, tokens/s = 43802 (43802 target) ; Learning rate = 0.000452 ; Loss = 1.464468\n",
      "2024-12-07 09:22:18.300000: I runner.py:310] Step = 38400 ; steps/s = 1.64, tokens/s = 44359 (44359 target) ; Learning rate = 0.000451 ; Loss = 1.472098\n",
      "2024-12-07 09:23:18.875000: I runner.py:310] Step = 38500 ; steps/s = 1.65, tokens/s = 43859 (43859 target) ; Learning rate = 0.000450 ; Loss = 1.454566\n",
      "2024-12-07 09:24:19.901000: I runner.py:310] Step = 38600 ; steps/s = 1.64, tokens/s = 44316 (44316 target) ; Learning rate = 0.000450 ; Loss = 1.477167\n",
      "2024-12-07 09:25:20.965000: I runner.py:310] Step = 38700 ; steps/s = 1.64, tokens/s = 44292 (44292 target) ; Learning rate = 0.000449 ; Loss = 1.467109\n",
      "2024-12-07 09:26:21.565000: I runner.py:310] Step = 38800 ; steps/s = 1.65, tokens/s = 43847 (43847 target) ; Learning rate = 0.000449 ; Loss = 1.466012\n",
      "2024-12-07 09:27:22.556000: I runner.py:310] Step = 38900 ; steps/s = 1.64, tokens/s = 44338 (44338 target) ; Learning rate = 0.000448 ; Loss = 1.460707\n",
      "2024-12-07 09:28:23.179000: I runner.py:310] Step = 39000 ; steps/s = 1.65, tokens/s = 43821 (43821 target) ; Learning rate = 0.000448 ; Loss = 1.455799\n",
      "2024-12-07 09:29:24.141000: I runner.py:310] Step = 39100 ; steps/s = 1.64, tokens/s = 44367 (44367 target) ; Learning rate = 0.000447 ; Loss = 1.469471\n",
      "2024-12-07 09:30:25.167000: I runner.py:310] Step = 39200 ; steps/s = 1.64, tokens/s = 44319 (44319 target) ; Learning rate = 0.000446 ; Loss = 1.467060\n",
      "2024-12-07 09:31:25.728000: I runner.py:310] Step = 39300 ; steps/s = 1.65, tokens/s = 43868 (43868 target) ; Learning rate = 0.000446 ; Loss = 1.460575\n",
      "2024-12-07 09:32:26.772000: I runner.py:310] Step = 39400 ; steps/s = 1.64, tokens/s = 44307 (44307 target) ; Learning rate = 0.000445 ; Loss = 1.469744\n",
      "2024-12-07 09:33:27.345000: I runner.py:310] Step = 39500 ; steps/s = 1.65, tokens/s = 43856 (43856 target) ; Learning rate = 0.000445 ; Loss = 1.466980\n",
      "2024-12-07 09:34:28.301000: I runner.py:310] Step = 39600 ; steps/s = 1.64, tokens/s = 44371 (44371 target) ; Learning rate = 0.000444 ; Loss = 1.461606\n",
      "2024-12-07 09:35:29.337000: I runner.py:310] Step = 39700 ; steps/s = 1.64, tokens/s = 44303 (44303 target) ; Learning rate = 0.000444 ; Loss = 1.463680\n",
      "2024-12-07 09:36:29.970000: I runner.py:310] Step = 39800 ; steps/s = 1.65, tokens/s = 43825 (43825 target) ; Learning rate = 0.000443 ; Loss = 1.457754\n",
      "2024-12-07 09:37:31.029000: I runner.py:310] Step = 39900 ; steps/s = 1.64, tokens/s = 44294 (44294 target) ; Learning rate = 0.000442 ; Loss = 1.464964\n",
      "2024-12-07 09:38:31.566000: I runner.py:310] Step = 40000 ; steps/s = 1.65, tokens/s = 43887 (43887 target) ; Learning rate = 0.000442 ; Loss = 1.457690\n",
      "2024-12-07 09:38:33.780000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-40000\n",
      "2024-12-07 09:38:33.780000: I training.py:192] Running evaluation for step 40000\n",
      "2024-12-07 09:39:27.409000: I training.py:192] Evaluation result for step 40000: loss = 0.860853 ; perplexity = 2.365178\n",
      "2024-12-07 09:40:28.323000: I runner.py:310] Step = 40100 ; steps/s = 1.64, tokens/s = 44408 (44408 target) ; Learning rate = 0.000441 ; Loss = 1.462043\n",
      "2024-12-07 09:41:29.350000: I runner.py:310] Step = 40200 ; steps/s = 1.64, tokens/s = 44312 (44312 target) ; Learning rate = 0.000441 ; Loss = 1.470870\n",
      "2024-12-07 09:42:29.903000: I runner.py:310] Step = 40300 ; steps/s = 1.65, tokens/s = 43878 (43878 target) ; Learning rate = 0.000440 ; Loss = 1.467801\n",
      "2024-12-07 09:43:30.904000: I runner.py:310] Step = 40400 ; steps/s = 1.64, tokens/s = 44338 (44338 target) ; Learning rate = 0.000440 ; Loss = 1.464660\n",
      "2024-12-07 09:44:31.916000: I runner.py:310] Step = 40500 ; steps/s = 1.64, tokens/s = 44325 (44325 target) ; Learning rate = 0.000439 ; Loss = 1.478114\n",
      "2024-12-07 09:45:32.520000: I runner.py:310] Step = 40600 ; steps/s = 1.65, tokens/s = 43837 (43837 target) ; Learning rate = 0.000439 ; Loss = 1.460311\n",
      "2024-12-07 09:46:33.528000: I runner.py:310] Step = 40700 ; steps/s = 1.64, tokens/s = 44338 (44338 target) ; Learning rate = 0.000438 ; Loss = 1.464773\n",
      "2024-12-07 09:47:34.085000: I runner.py:310] Step = 40800 ; steps/s = 1.65, tokens/s = 43867 (43867 target) ; Learning rate = 0.000438 ; Loss = 1.462664\n",
      "2024-12-07 09:48:35.098000: I runner.py:310] Step = 40900 ; steps/s = 1.64, tokens/s = 44331 (44331 target) ; Learning rate = 0.000437 ; Loss = 1.460065\n",
      "2024-12-07 09:49:36.095000: I runner.py:310] Step = 41000 ; steps/s = 1.64, tokens/s = 44340 (44340 target) ; Learning rate = 0.000437 ; Loss = 1.467496\n",
      "2024-12-07 09:50:36.679000: I runner.py:310] Step = 41100 ; steps/s = 1.65, tokens/s = 43851 (43851 target) ; Learning rate = 0.000436 ; Loss = 1.459527\n",
      "2024-12-07 09:51:37.665000: I runner.py:310] Step = 41200 ; steps/s = 1.64, tokens/s = 44346 (44346 target) ; Learning rate = 0.000435 ; Loss = 1.461142\n",
      "2024-12-07 09:52:38.281000: I runner.py:310] Step = 41300 ; steps/s = 1.65, tokens/s = 43827 (43827 target) ; Learning rate = 0.000435 ; Loss = 1.456106\n",
      "2024-12-07 09:53:39.300000: I runner.py:310] Step = 41400 ; steps/s = 1.64, tokens/s = 44330 (44330 target) ; Learning rate = 0.000434 ; Loss = 1.466427\n",
      "2024-12-07 09:54:40.317000: I runner.py:310] Step = 41500 ; steps/s = 1.64, tokens/s = 44324 (44324 target) ; Learning rate = 0.000434 ; Loss = 1.462566\n",
      "2024-12-07 09:55:40.903000: I runner.py:310] Step = 41600 ; steps/s = 1.65, tokens/s = 43852 (43852 target) ; Learning rate = 0.000433 ; Loss = 1.461705\n",
      "2024-12-07 09:56:41.871000: I runner.py:310] Step = 41700 ; steps/s = 1.64, tokens/s = 44357 (44357 target) ; Learning rate = 0.000433 ; Loss = 1.458801\n",
      "2024-12-07 09:57:42.481000: I runner.py:310] Step = 41800 ; steps/s = 1.65, tokens/s = 43835 (43835 target) ; Learning rate = 0.000432 ; Loss = 1.454453\n",
      "2024-12-07 09:58:43.472000: I runner.py:310] Step = 41900 ; steps/s = 1.64, tokens/s = 44345 (44345 target) ; Learning rate = 0.000432 ; Loss = 1.464329\n",
      "2024-12-07 09:59:44.518000: I runner.py:310] Step = 42000 ; steps/s = 1.64, tokens/s = 44306 (44306 target) ; Learning rate = 0.000431 ; Loss = 1.466641\n",
      "2024-12-07 10:00:45.099000: I runner.py:310] Step = 42100 ; steps/s = 1.65, tokens/s = 43858 (43858 target) ; Learning rate = 0.000431 ; Loss = 1.459886\n",
      "2024-12-07 10:01:46.054000: I runner.py:310] Step = 42200 ; steps/s = 1.64, tokens/s = 44373 (44373 target) ; Learning rate = 0.000430 ; Loss = 1.461095\n",
      "2024-12-07 10:02:46.608000: I runner.py:310] Step = 42300 ; steps/s = 1.65, tokens/s = 43865 (43865 target) ; Learning rate = 0.000430 ; Loss = 1.452262\n",
      "2024-12-07 10:03:47.608000: I runner.py:310] Step = 42400 ; steps/s = 1.64, tokens/s = 44348 (44348 target) ; Learning rate = 0.000429 ; Loss = 1.460707\n",
      "2024-12-07 10:04:48.630000: I runner.py:310] Step = 42500 ; steps/s = 1.64, tokens/s = 44309 (44309 target) ; Learning rate = 0.000429 ; Loss = 1.464087\n",
      "2024-12-07 10:05:49.199000: I runner.py:310] Step = 42600 ; steps/s = 1.65, tokens/s = 43866 (43866 target) ; Learning rate = 0.000428 ; Loss = 1.457934\n",
      "2024-12-07 10:06:50.246000: I runner.py:310] Step = 42700 ; steps/s = 1.64, tokens/s = 44301 (44301 target) ; Learning rate = 0.000428 ; Loss = 1.461182\n",
      "2024-12-07 10:07:50.823000: I runner.py:310] Step = 42800 ; steps/s = 1.65, tokens/s = 43857 (43857 target) ; Learning rate = 0.000427 ; Loss = 1.454400\n",
      "2024-12-07 10:08:51.804000: I runner.py:310] Step = 42900 ; steps/s = 1.64, tokens/s = 44348 (44348 target) ; Learning rate = 0.000427 ; Loss = 1.455834\n",
      "2024-12-07 10:09:52.830000: I runner.py:310] Step = 43000 ; steps/s = 1.64, tokens/s = 44328 (44328 target) ; Learning rate = 0.000426 ; Loss = 1.470872\n",
      "2024-12-07 10:10:53.384000: I runner.py:310] Step = 43100 ; steps/s = 1.65, tokens/s = 43875 (43875 target) ; Learning rate = 0.000426 ; Loss = 1.455818\n",
      "2024-12-07 10:11:54.395000: I runner.py:310] Step = 43200 ; steps/s = 1.64, tokens/s = 44333 (44333 target) ; Learning rate = 0.000425 ; Loss = 1.460470\n",
      "2024-12-07 10:12:55.016000: I runner.py:310] Step = 43300 ; steps/s = 1.65, tokens/s = 43817 (43817 target) ; Learning rate = 0.000425 ; Loss = 1.454419\n",
      "2024-12-07 10:13:56.041000: I runner.py:310] Step = 43400 ; steps/s = 1.64, tokens/s = 44324 (44324 target) ; Learning rate = 0.000424 ; Loss = 1.459210\n",
      "2024-12-07 10:14:57.072000: I runner.py:310] Step = 43500 ; steps/s = 1.64, tokens/s = 44312 (44312 target) ; Learning rate = 0.000424 ; Loss = 1.462505\n",
      "2024-12-07 10:15:57.672000: I runner.py:310] Step = 43600 ; steps/s = 1.65, tokens/s = 43838 (43838 target) ; Learning rate = 0.000423 ; Loss = 1.454655\n",
      "2024-12-07 10:16:58.659000: I runner.py:310] Step = 43700 ; steps/s = 1.64, tokens/s = 44349 (44349 target) ; Learning rate = 0.000423 ; Loss = 1.461483\n",
      "2024-12-07 10:17:59.266000: I runner.py:310] Step = 43800 ; steps/s = 1.65, tokens/s = 43832 (43832 target) ; Learning rate = 0.000422 ; Loss = 1.453108\n",
      "2024-12-07 10:19:00.287000: I runner.py:310] Step = 43900 ; steps/s = 1.64, tokens/s = 44318 (44318 target) ; Learning rate = 0.000422 ; Loss = 1.463556\n",
      "2024-12-07 10:20:01.318000: I runner.py:310] Step = 44000 ; steps/s = 1.64, tokens/s = 44318 (44318 target) ; Learning rate = 0.000421 ; Loss = 1.457397\n",
      "2024-12-07 10:21:01.904000: I runner.py:310] Step = 44100 ; steps/s = 1.65, tokens/s = 43848 (43848 target) ; Learning rate = 0.000421 ; Loss = 1.456289\n",
      "2024-12-07 10:22:02.914000: I runner.py:310] Step = 44200 ; steps/s = 1.64, tokens/s = 44337 (44337 target) ; Learning rate = 0.000420 ; Loss = 1.457183\n",
      "2024-12-07 10:23:03.453000: I runner.py:310] Step = 44300 ; steps/s = 1.65, tokens/s = 43881 (43881 target) ; Learning rate = 0.000420 ; Loss = 1.497631\n",
      "2024-12-07 10:24:04.460000: I runner.py:310] Step = 44400 ; steps/s = 1.64, tokens/s = 44332 (44332 target) ; Learning rate = 0.000419 ; Loss = 1.454432\n",
      "2024-12-07 10:25:05.478000: I runner.py:310] Step = 44500 ; steps/s = 1.64, tokens/s = 44325 (44325 target) ; Learning rate = 0.000419 ; Loss = 1.458264\n",
      "2024-12-07 10:26:06.086000: I runner.py:310] Step = 44600 ; steps/s = 1.65, tokens/s = 43827 (43827 target) ; Learning rate = 0.000419 ; Loss = 1.455463\n",
      "2024-12-07 10:27:07.131000: I runner.py:310] Step = 44700 ; steps/s = 1.64, tokens/s = 44315 (44315 target) ; Learning rate = 0.000418 ; Loss = 1.461315\n",
      "2024-12-07 10:28:08.105000: I runner.py:310] Step = 44800 ; steps/s = 1.64, tokens/s = 44354 (44354 target) ; Learning rate = 0.000418 ; Loss = 1.461767\n",
      "2024-12-07 10:29:08.619000: I runner.py:310] Step = 44900 ; steps/s = 1.65, tokens/s = 43909 (43909 target) ; Learning rate = 0.000417 ; Loss = 1.452261\n",
      "2024-12-07 10:30:09.650000: I runner.py:310] Step = 45000 ; steps/s = 1.64, tokens/s = 44311 (44311 target) ; Learning rate = 0.000417 ; Loss = 1.462876\n",
      "2024-12-07 10:30:09.651000: I training.py:192] Running evaluation for step 45000\n",
      "2024-12-07 10:30:59.006000: I training.py:192] Evaluation result for step 45000: loss = 0.870571 ; perplexity = 2.388275\n",
      "2024-12-07 10:31:59.511000: I runner.py:310] Step = 45100 ; steps/s = 1.65, tokens/s = 43916 (43916 target) ; Learning rate = 0.000416 ; Loss = 1.455915\n",
      "2024-12-07 10:33:00.618000: I runner.py:310] Step = 45200 ; steps/s = 1.64, tokens/s = 44259 (44259 target) ; Learning rate = 0.000416 ; Loss = 1.457880\n",
      "2024-12-07 10:34:01.703000: I runner.py:310] Step = 45300 ; steps/s = 1.64, tokens/s = 44277 (44277 target) ; Learning rate = 0.000415 ; Loss = 1.455675\n",
      "2024-12-07 10:35:02.309000: I runner.py:310] Step = 45400 ; steps/s = 1.65, tokens/s = 43835 (43835 target) ; Learning rate = 0.000415 ; Loss = 1.452716\n",
      "2024-12-07 10:36:03.388000: I runner.py:310] Step = 45500 ; steps/s = 1.64, tokens/s = 44283 (44283 target) ; Learning rate = 0.000414 ; Loss = 1.456147\n",
      "2024-12-07 10:37:04.000000: I runner.py:310] Step = 45600 ; steps/s = 1.65, tokens/s = 43825 (43825 target) ; Learning rate = 0.000414 ; Loss = 1.452849\n",
      "2024-12-07 10:38:04.985000: I runner.py:310] Step = 45700 ; steps/s = 1.64, tokens/s = 44352 (44352 target) ; Learning rate = 0.000413 ; Loss = 1.460318\n",
      "2024-12-07 10:39:06.047000: I runner.py:310] Step = 45800 ; steps/s = 1.64, tokens/s = 44298 (44298 target) ; Learning rate = 0.000413 ; Loss = 1.457529\n",
      "2024-12-07 10:40:06.605000: I runner.py:310] Step = 45900 ; steps/s = 1.65, tokens/s = 43876 (43876 target) ; Learning rate = 0.000413 ; Loss = 1.452194\n",
      "2024-12-07 10:41:07.625000: I runner.py:310] Step = 46000 ; steps/s = 1.64, tokens/s = 44320 (44320 target) ; Learning rate = 0.000412 ; Loss = 1.455415\n",
      "2024-12-07 10:42:08.229000: I runner.py:310] Step = 46100 ; steps/s = 1.65, tokens/s = 43835 (43835 target) ; Learning rate = 0.000412 ; Loss = 1.448589\n",
      "2024-12-07 10:43:09.253000: I runner.py:310] Step = 46200 ; steps/s = 1.64, tokens/s = 44313 (44313 target) ; Learning rate = 0.000411 ; Loss = 1.457653\n",
      "2024-12-07 10:44:10.248000: I runner.py:310] Step = 46300 ; steps/s = 1.64, tokens/s = 44349 (44349 target) ; Learning rate = 0.000411 ; Loss = 1.459551\n",
      "2024-12-07 10:45:10.852000: I runner.py:310] Step = 46400 ; steps/s = 1.65, tokens/s = 43836 (43836 target) ; Learning rate = 0.000410 ; Loss = 1.454251\n",
      "2024-12-07 10:46:11.888000: I runner.py:310] Step = 46500 ; steps/s = 1.64, tokens/s = 44313 (44313 target) ; Learning rate = 0.000410 ; Loss = 1.456394\n",
      "2024-12-07 10:47:12.485000: I runner.py:310] Step = 46600 ; steps/s = 1.65, tokens/s = 43836 (43836 target) ; Learning rate = 0.000409 ; Loss = 1.447398\n",
      "2024-12-07 10:48:13.527000: I runner.py:310] Step = 46700 ; steps/s = 1.64, tokens/s = 44305 (44305 target) ; Learning rate = 0.000409 ; Loss = 1.450203\n",
      "2024-12-07 10:49:14.548000: I runner.py:310] Step = 46800 ; steps/s = 1.64, tokens/s = 44326 (44326 target) ; Learning rate = 0.000409 ; Loss = 1.460218\n",
      "2024-12-07 10:50:15.207000: I runner.py:310] Step = 46900 ; steps/s = 1.65, tokens/s = 43797 (43797 target) ; Learning rate = 0.000408 ; Loss = 1.460924\n",
      "2024-12-07 10:51:16.269000: I runner.py:310] Step = 47000 ; steps/s = 1.64, tokens/s = 44296 (44296 target) ; Learning rate = 0.000408 ; Loss = 1.451095\n",
      "2024-12-07 10:52:16.861000: I runner.py:310] Step = 47100 ; steps/s = 1.65, tokens/s = 43840 (43840 target) ; Learning rate = 0.000407 ; Loss = 1.447365\n",
      "2024-12-07 10:53:17.906000: I runner.py:310] Step = 47200 ; steps/s = 1.64, tokens/s = 44307 (44307 target) ; Learning rate = 0.000407 ; Loss = 1.448221\n",
      "2024-12-07 10:54:18.930000: I runner.py:310] Step = 47300 ; steps/s = 1.64, tokens/s = 44329 (44329 target) ; Learning rate = 0.000406 ; Loss = 1.462749\n",
      "2024-12-07 10:55:19.537000: I runner.py:310] Step = 47400 ; steps/s = 1.65, tokens/s = 43828 (43828 target) ; Learning rate = 0.000406 ; Loss = 1.458328\n",
      "2024-12-07 10:56:20.558000: I runner.py:310] Step = 47500 ; steps/s = 1.64, tokens/s = 44325 (44325 target) ; Learning rate = 0.000406 ; Loss = 1.452882\n",
      "2024-12-07 10:57:21.166000: I runner.py:310] Step = 47600 ; steps/s = 1.65, tokens/s = 43834 (43834 target) ; Learning rate = 0.000405 ; Loss = 1.448954\n",
      "2024-12-07 10:58:22.193000: I runner.py:310] Step = 47700 ; steps/s = 1.64, tokens/s = 44321 (44321 target) ; Learning rate = 0.000405 ; Loss = 1.453826\n",
      "2024-12-07 10:59:23.266000: I runner.py:310] Step = 47800 ; steps/s = 1.64, tokens/s = 44286 (44286 target) ; Learning rate = 0.000404 ; Loss = 1.454926\n",
      "2024-12-07 11:00:23.870000: I runner.py:310] Step = 47900 ; steps/s = 1.65, tokens/s = 43829 (43829 target) ; Learning rate = 0.000404 ; Loss = 1.461910\n",
      "2024-12-07 11:01:24.866000: I runner.py:310] Step = 48000 ; steps/s = 1.64, tokens/s = 44345 (44345 target) ; Learning rate = 0.000403 ; Loss = 1.454751\n",
      "2024-12-07 11:02:25.448000: I runner.py:310] Step = 48100 ; steps/s = 1.65, tokens/s = 43855 (43855 target) ; Learning rate = 0.000403 ; Loss = 1.454522\n",
      "2024-12-07 11:03:26.515000: I runner.py:310] Step = 48200 ; steps/s = 1.64, tokens/s = 44295 (44295 target) ; Learning rate = 0.000403 ; Loss = 1.455026\n",
      "2024-12-07 11:04:27.583000: I runner.py:310] Step = 48300 ; steps/s = 1.64, tokens/s = 44285 (44285 target) ; Learning rate = 0.000402 ; Loss = 1.451196\n",
      "2024-12-07 11:05:28.133000: I runner.py:310] Step = 48400 ; steps/s = 1.65, tokens/s = 43873 (43873 target) ; Learning rate = 0.000402 ; Loss = 1.452249\n",
      "2024-12-07 11:06:29.214000: I runner.py:310] Step = 48500 ; steps/s = 1.64, tokens/s = 44278 (44278 target) ; Learning rate = 0.000401 ; Loss = 1.455411\n",
      "2024-12-07 11:07:30.229000: I runner.py:310] Step = 48600 ; steps/s = 1.64, tokens/s = 44330 (44330 target) ; Learning rate = 0.000401 ; Loss = 1.456150\n",
      "2024-12-07 11:08:30.809000: I runner.py:310] Step = 48700 ; steps/s = 1.65, tokens/s = 43857 (43857 target) ; Learning rate = 0.000401 ; Loss = 1.450828\n",
      "2024-12-07 11:09:31.828000: I runner.py:310] Step = 48800 ; steps/s = 1.64, tokens/s = 44323 (44323 target) ; Learning rate = 0.000400 ; Loss = 1.454773\n",
      "2024-12-07 11:10:32.452000: I runner.py:310] Step = 48900 ; steps/s = 1.65, tokens/s = 43817 (43817 target) ; Learning rate = 0.000400 ; Loss = 1.449755\n",
      "2024-12-07 11:11:33.464000: I runner.py:310] Step = 49000 ; steps/s = 1.64, tokens/s = 44333 (44333 target) ; Learning rate = 0.000399 ; Loss = 1.450254\n",
      "2024-12-07 11:12:34.485000: I runner.py:310] Step = 49100 ; steps/s = 1.64, tokens/s = 44321 (44321 target) ; Learning rate = 0.000399 ; Loss = 1.455404\n",
      "2024-12-07 11:13:35.138000: I runner.py:310] Step = 49200 ; steps/s = 1.65, tokens/s = 43803 (43803 target) ; Learning rate = 0.000398 ; Loss = 1.449201\n",
      "2024-12-07 11:14:36.208000: I runner.py:310] Step = 49300 ; steps/s = 1.64, tokens/s = 44296 (44296 target) ; Learning rate = 0.000398 ; Loss = 1.456038\n",
      "2024-12-07 11:15:36.861000: I runner.py:310] Step = 49400 ; steps/s = 1.65, tokens/s = 43790 (43790 target) ; Learning rate = 0.000398 ; Loss = 1.449207\n",
      "2024-12-07 11:16:37.907000: I runner.py:310] Step = 49500 ; steps/s = 1.64, tokens/s = 44308 (44308 target) ; Learning rate = 0.000397 ; Loss = 1.453776\n",
      "2024-12-07 11:17:38.994000: I runner.py:310] Step = 49600 ; steps/s = 1.64, tokens/s = 44273 (44273 target) ; Learning rate = 0.000397 ; Loss = 1.456649\n",
      "2024-12-07 11:18:39.601000: I runner.py:310] Step = 49700 ; steps/s = 1.65, tokens/s = 43835 (43835 target) ; Learning rate = 0.000396 ; Loss = 1.453173\n",
      "2024-12-07 11:19:40.642000: I runner.py:310] Step = 49800 ; steps/s = 1.64, tokens/s = 44309 (44309 target) ; Learning rate = 0.000396 ; Loss = 1.448146\n",
      "2024-12-07 11:20:41.247000: I runner.py:310] Step = 49900 ; steps/s = 1.65, tokens/s = 43827 (43827 target) ; Learning rate = 0.000396 ; Loss = 1.451513\n",
      "2024-12-07 11:21:42.270000: I runner.py:310] Step = 50000 ; steps/s = 1.64, tokens/s = 44325 (44325 target) ; Learning rate = 0.000395 ; Loss = 1.450787\n",
      "2024-12-07 11:21:44.342000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-50000\n",
      "2024-12-07 11:21:44.343000: I training.py:192] Running evaluation for step 50000\n",
      "2024-12-07 11:22:33.566000: I training.py:192] Evaluation result for step 50000: loss = 0.880897 ; perplexity = 2.413063\n",
      "2024-12-07 11:23:34.568000: I runner.py:310] Step = 50100 ; steps/s = 1.64, tokens/s = 44342 (44342 target) ; Learning rate = 0.000395 ; Loss = 1.455736\n",
      "2024-12-07 11:24:35.150000: I runner.py:310] Step = 50200 ; steps/s = 1.65, tokens/s = 43847 (43847 target) ; Learning rate = 0.000394 ; Loss = 1.450413\n",
      "2024-12-07 11:25:36.201000: I runner.py:310] Step = 50300 ; steps/s = 1.64, tokens/s = 44307 (44307 target) ; Learning rate = 0.000394 ; Loss = 1.450559\n",
      "2024-12-07 11:26:36.903000: I runner.py:310] Step = 50400 ; steps/s = 1.65, tokens/s = 43760 (43760 target) ; Learning rate = 0.000394 ; Loss = 1.446039\n",
      "2024-12-07 11:27:38.030000: I runner.py:310] Step = 50500 ; steps/s = 1.64, tokens/s = 44254 (44254 target) ; Learning rate = 0.000393 ; Loss = 1.451768\n",
      "2024-12-07 11:28:39.133000: I runner.py:310] Step = 50600 ; steps/s = 1.64, tokens/s = 44263 (44263 target) ; Learning rate = 0.000393 ; Loss = 1.454570\n",
      "2024-12-07 11:29:39.810000: I runner.py:310] Step = 50700 ; steps/s = 1.65, tokens/s = 43777 (43777 target) ; Learning rate = 0.000393 ; Loss = 1.459780\n",
      "2024-12-07 11:30:40.891000: I runner.py:310] Step = 50800 ; steps/s = 1.64, tokens/s = 44279 (44279 target) ; Learning rate = 0.000392 ; Loss = 1.451184\n",
      "2024-12-07 11:31:41.615000: I runner.py:310] Step = 50900 ; steps/s = 1.65, tokens/s = 43752 (43752 target) ; Learning rate = 0.000392 ; Loss = 1.452220\n",
      "2024-12-07 11:32:42.695000: I runner.py:310] Step = 51000 ; steps/s = 1.64, tokens/s = 44280 (44280 target) ; Learning rate = 0.000391 ; Loss = 1.453676\n",
      "2024-12-07 11:33:43.849000: I runner.py:310] Step = 51100 ; steps/s = 1.64, tokens/s = 44224 (44224 target) ; Learning rate = 0.000391 ; Loss = 1.451320\n",
      "2024-12-07 11:34:44.507000: I runner.py:310] Step = 51200 ; steps/s = 1.65, tokens/s = 43798 (43798 target) ; Learning rate = 0.000391 ; Loss = 1.451366\n",
      "2024-12-07 11:35:45.593000: I runner.py:310] Step = 51300 ; steps/s = 1.64, tokens/s = 44278 (44278 target) ; Learning rate = 0.000390 ; Loss = 1.454345\n",
      "2024-12-07 11:36:46.278000: I runner.py:310] Step = 51400 ; steps/s = 1.65, tokens/s = 43775 (43775 target) ; Learning rate = 0.000390 ; Loss = 1.444937\n",
      "2024-12-07 11:37:47.414000: I runner.py:310] Step = 51500 ; steps/s = 1.64, tokens/s = 44246 (44246 target) ; Learning rate = 0.000389 ; Loss = 1.448773\n",
      "2024-12-07 11:38:48.531000: I runner.py:310] Step = 51600 ; steps/s = 1.64, tokens/s = 44249 (44249 target) ; Learning rate = 0.000389 ; Loss = 1.453055\n",
      "2024-12-07 11:39:49.272000: I runner.py:310] Step = 51700 ; steps/s = 1.65, tokens/s = 43734 (43734 target) ; Learning rate = 0.000389 ; Loss = 1.452272\n",
      "2024-12-07 11:40:50.406000: I runner.py:310] Step = 51800 ; steps/s = 1.64, tokens/s = 44250 (44250 target) ; Learning rate = 0.000388 ; Loss = 1.452540\n",
      "2024-12-07 11:41:51.110000: I runner.py:310] Step = 51900 ; steps/s = 1.65, tokens/s = 43759 (43759 target) ; Learning rate = 0.000388 ; Loss = 1.442810\n",
      "2024-12-07 11:42:52.186000: I runner.py:310] Step = 52000 ; steps/s = 1.64, tokens/s = 44278 (44278 target) ; Learning rate = 0.000388 ; Loss = 1.446187\n",
      "2024-12-07 11:43:53.399000: I runner.py:310] Step = 52100 ; steps/s = 1.63, tokens/s = 44177 (44177 target) ; Learning rate = 0.000387 ; Loss = 1.452321\n",
      "2024-12-07 11:44:54.046000: I runner.py:310] Step = 52200 ; steps/s = 1.65, tokens/s = 43820 (43820 target) ; Learning rate = 0.000387 ; Loss = 1.447807\n",
      "2024-12-07 11:45:55.183000: I runner.py:310] Step = 52300 ; steps/s = 1.64, tokens/s = 44233 (44233 target) ; Learning rate = 0.000386 ; Loss = 1.453880\n",
      "2024-12-07 11:46:55.993000: I runner.py:310] Step = 52400 ; steps/s = 1.64, tokens/s = 43915 (43915 target) ; Learning rate = 0.000386 ; Loss = 1.483586\n",
      "2024-12-07 11:47:56.956000: I runner.py:310] Step = 52500 ; steps/s = 1.64, tokens/s = 44139 (44139 target) ; Learning rate = 0.000386 ; Loss = 1.448078\n",
      "2024-12-07 11:48:58.069000: I runner.py:310] Step = 52600 ; steps/s = 1.64, tokens/s = 44252 (44252 target) ; Learning rate = 0.000385 ; Loss = 1.450400\n",
      "2024-12-07 11:49:58.719000: I runner.py:310] Step = 52700 ; steps/s = 1.65, tokens/s = 43804 (43804 target) ; Learning rate = 0.000385 ; Loss = 1.448428\n",
      "2024-12-07 11:50:59.832000: I runner.py:310] Step = 52800 ; steps/s = 1.64, tokens/s = 44261 (44261 target) ; Learning rate = 0.000385 ; Loss = 1.448459\n",
      "2024-12-07 11:52:00.926000: I runner.py:310] Step = 52900 ; steps/s = 1.64, tokens/s = 44266 (44266 target) ; Learning rate = 0.000384 ; Loss = 1.452592\n",
      "2024-12-07 11:53:01.644000: I runner.py:310] Step = 53000 ; steps/s = 1.65, tokens/s = 43754 (43754 target) ; Learning rate = 0.000384 ; Loss = 1.452100\n",
      "2024-12-07 11:54:02.756000: I runner.py:310] Step = 53100 ; steps/s = 1.64, tokens/s = 44256 (44256 target) ; Learning rate = 0.000384 ; Loss = 1.448839\n",
      "2024-12-07 11:55:03.465000: I runner.py:310] Step = 53200 ; steps/s = 1.65, tokens/s = 43758 (43758 target) ; Learning rate = 0.000383 ; Loss = 1.449369\n",
      "2024-12-07 11:56:04.595000: I runner.py:310] Step = 53300 ; steps/s = 1.64, tokens/s = 44249 (44249 target) ; Learning rate = 0.000383 ; Loss = 1.451266\n",
      "2024-12-07 11:57:05.698000: I runner.py:310] Step = 53400 ; steps/s = 1.64, tokens/s = 44257 (44257 target) ; Learning rate = 0.000382 ; Loss = 1.449248\n",
      "2024-12-07 11:58:06.325000: I runner.py:310] Step = 53500 ; steps/s = 1.65, tokens/s = 43820 (43820 target) ; Learning rate = 0.000382 ; Loss = 1.449541\n",
      "2024-12-07 11:59:07.482000: I runner.py:310] Step = 53600 ; steps/s = 1.64, tokens/s = 44223 (44223 target) ; Learning rate = 0.000382 ; Loss = 1.454026\n",
      "2024-12-07 12:00:08.125000: I runner.py:310] Step = 53700 ; steps/s = 1.65, tokens/s = 43812 (43812 target) ; Learning rate = 0.000381 ; Loss = 1.444771\n",
      "2024-12-07 12:01:09.296000: I runner.py:310] Step = 53800 ; steps/s = 1.63, tokens/s = 44214 (44214 target) ; Learning rate = 0.000381 ; Loss = 1.449113\n",
      "2024-12-07 12:02:10.371000: I runner.py:310] Step = 53900 ; steps/s = 1.64, tokens/s = 44285 (44285 target) ; Learning rate = 0.000381 ; Loss = 1.448804\n",
      "2024-12-07 12:03:11.017000: I runner.py:310] Step = 54000 ; steps/s = 1.65, tokens/s = 43803 (43803 target) ; Learning rate = 0.000380 ; Loss = 1.448723\n",
      "2024-12-07 12:04:12.140000: I runner.py:310] Step = 54100 ; steps/s = 1.64, tokens/s = 44250 (44250 target) ; Learning rate = 0.000380 ; Loss = 1.450620\n",
      "2024-12-07 12:05:12.742000: I runner.py:310] Step = 54200 ; steps/s = 1.65, tokens/s = 43842 (43842 target) ; Learning rate = 0.000380 ; Loss = 1.445514\n",
      "2024-12-07 12:06:13.831000: I runner.py:310] Step = 54300 ; steps/s = 1.64, tokens/s = 44267 (44267 target) ; Learning rate = 0.000379 ; Loss = 1.449605\n",
      "2024-12-07 12:07:14.994000: I runner.py:310] Step = 54400 ; steps/s = 1.64, tokens/s = 44222 (44222 target) ; Learning rate = 0.000379 ; Loss = 1.447407\n",
      "2024-12-07 12:08:15.761000: I runner.py:310] Step = 54500 ; steps/s = 1.65, tokens/s = 43720 (43720 target) ; Learning rate = 0.000379 ; Loss = 1.444552\n",
      "2024-12-07 12:09:16.866000: I runner.py:310] Step = 54600 ; steps/s = 1.64, tokens/s = 44272 (44272 target) ; Learning rate = 0.000378 ; Loss = 1.447215\n",
      "2024-12-07 12:10:17.546000: I runner.py:310] Step = 54700 ; steps/s = 1.65, tokens/s = 43773 (43773 target) ; Learning rate = 0.000378 ; Loss = 1.444659\n",
      "2024-12-07 12:11:18.641000: I runner.py:310] Step = 54800 ; steps/s = 1.64, tokens/s = 44267 (44267 target) ; Learning rate = 0.000378 ; Loss = 1.449121\n",
      "2024-12-07 12:12:19.724000: I runner.py:310] Step = 54900 ; steps/s = 1.64, tokens/s = 44282 (44282 target) ; Learning rate = 0.000377 ; Loss = 1.448186\n",
      "2024-12-07 12:13:20.349000: I runner.py:310] Step = 55000 ; steps/s = 1.65, tokens/s = 43818 (43818 target) ; Learning rate = 0.000377 ; Loss = 1.447967\n",
      "2024-12-07 12:13:20.350000: I training.py:192] Running evaluation for step 55000\n",
      "2024-12-07 12:14:09.905000: I training.py:192] Evaluation result for step 55000: loss = 0.892404 ; perplexity = 2.440991\n",
      "2024-12-07 12:15:10.824000: I runner.py:310] Step = 55100 ; steps/s = 1.64, tokens/s = 44408 (44408 target) ; Learning rate = 0.000377 ; Loss = 1.448697\n",
      "2024-12-07 12:16:11.503000: I runner.py:310] Step = 55200 ; steps/s = 1.65, tokens/s = 43781 (43781 target) ; Learning rate = 0.000376 ; Loss = 1.446570\n",
      "2024-12-07 12:17:12.610000: I runner.py:310] Step = 55300 ; steps/s = 1.64, tokens/s = 44262 (44262 target) ; Learning rate = 0.000376 ; Loss = 1.449364\n",
      "2024-12-07 12:18:13.788000: I runner.py:310] Step = 55400 ; steps/s = 1.63, tokens/s = 44209 (44209 target) ; Learning rate = 0.000376 ; Loss = 1.451436\n",
      "2024-12-07 12:19:14.590000: I runner.py:310] Step = 55500 ; steps/s = 1.64, tokens/s = 43694 (43694 target) ; Learning rate = 0.000375 ; Loss = 1.448630\n",
      "2024-12-07 12:20:15.678000: I runner.py:310] Step = 55600 ; steps/s = 1.64, tokens/s = 44272 (44272 target) ; Learning rate = 0.000375 ; Loss = 1.450223\n",
      "2024-12-07 12:21:16.300000: I runner.py:310] Step = 55700 ; steps/s = 1.65, tokens/s = 43822 (43822 target) ; Learning rate = 0.000375 ; Loss = 1.444302\n",
      "2024-12-07 12:22:17.394000: I runner.py:310] Step = 55800 ; steps/s = 1.64, tokens/s = 44270 (44270 target) ; Learning rate = 0.000374 ; Loss = 1.448463\n",
      "2024-12-07 12:23:18.559000: I runner.py:310] Step = 55900 ; steps/s = 1.64, tokens/s = 44219 (44219 target) ; Learning rate = 0.000374 ; Loss = 1.452991\n",
      "2024-12-07 12:24:19.237000: I runner.py:310] Step = 56000 ; steps/s = 1.65, tokens/s = 43784 (43784 target) ; Learning rate = 0.000374 ; Loss = 1.446075\n",
      "2024-12-07 12:25:20.364000: I runner.py:310] Step = 56100 ; steps/s = 1.64, tokens/s = 44244 (44244 target) ; Learning rate = 0.000373 ; Loss = 1.447667\n",
      "2024-12-07 12:26:21.118000: I runner.py:310] Step = 56200 ; steps/s = 1.65, tokens/s = 43734 (43734 target) ; Learning rate = 0.000373 ; Loss = 1.440778\n",
      "2024-12-07 12:27:22.255000: I runner.py:310] Step = 56300 ; steps/s = 1.64, tokens/s = 44240 (44240 target) ; Learning rate = 0.000373 ; Loss = 1.441004\n",
      "2024-12-07 12:28:23.390000: I runner.py:310] Step = 56400 ; steps/s = 1.64, tokens/s = 44238 (44238 target) ; Learning rate = 0.000372 ; Loss = 1.448634\n",
      "2024-12-07 12:29:24.112000: I runner.py:310] Step = 56500 ; steps/s = 1.65, tokens/s = 43753 (43753 target) ; Learning rate = 0.000372 ; Loss = 1.441771\n",
      "2024-12-07 12:30:25.252000: I runner.py:310] Step = 56600 ; steps/s = 1.64, tokens/s = 44239 (44239 target) ; Learning rate = 0.000372 ; Loss = 1.447239\n",
      "2024-12-07 12:31:26.434000: I runner.py:310] Step = 56700 ; steps/s = 1.63, tokens/s = 44205 (44205 target) ; Learning rate = 0.000371 ; Loss = 1.450333\n",
      "2024-12-07 12:32:27.108000: I runner.py:310] Step = 56800 ; steps/s = 1.65, tokens/s = 43782 (43782 target) ; Learning rate = 0.000371 ; Loss = 1.443606\n",
      "2024-12-07 12:33:28.234000: I runner.py:310] Step = 56900 ; steps/s = 1.64, tokens/s = 44253 (44253 target) ; Learning rate = 0.000371 ; Loss = 1.450492\n",
      "2024-12-07 12:34:28.866000: I runner.py:310] Step = 57000 ; steps/s = 1.65, tokens/s = 43809 (43809 target) ; Learning rate = 0.000370 ; Loss = 1.449065\n",
      "2024-12-07 12:35:29.930000: I runner.py:310] Step = 57100 ; steps/s = 1.64, tokens/s = 44299 (44299 target) ; Learning rate = 0.000370 ; Loss = 1.448971\n",
      "2024-12-07 12:36:31.114000: I runner.py:310] Step = 57200 ; steps/s = 1.63, tokens/s = 44200 (44200 target) ; Learning rate = 0.000370 ; Loss = 1.447945\n",
      "2024-12-07 12:37:31.725000: I runner.py:310] Step = 57300 ; steps/s = 1.65, tokens/s = 43826 (43826 target) ; Learning rate = 0.000369 ; Loss = 1.447770\n",
      "2024-12-07 12:38:32.799000: I runner.py:310] Step = 57400 ; steps/s = 1.64, tokens/s = 44290 (44290 target) ; Learning rate = 0.000369 ; Loss = 1.448171\n",
      "2024-12-07 12:39:33.441000: I runner.py:310] Step = 57500 ; steps/s = 1.65, tokens/s = 43811 (43811 target) ; Learning rate = 0.000369 ; Loss = 1.447828\n",
      "2024-12-07 12:40:34.522000: I runner.py:310] Step = 57600 ; steps/s = 1.64, tokens/s = 44279 (44279 target) ; Learning rate = 0.000368 ; Loss = 1.445777\n",
      "2024-12-07 12:41:35.695000: I runner.py:310] Step = 57700 ; steps/s = 1.63, tokens/s = 44220 (44220 target) ; Learning rate = 0.000368 ; Loss = 1.448751\n",
      "2024-12-07 12:42:36.347000: I runner.py:310] Step = 57800 ; steps/s = 1.65, tokens/s = 43803 (43803 target) ; Learning rate = 0.000368 ; Loss = 1.445853\n",
      "2024-12-07 12:43:37.422000: I runner.py:310] Step = 57900 ; steps/s = 1.64, tokens/s = 44278 (44278 target) ; Learning rate = 0.000367 ; Loss = 1.444589\n",
      "2024-12-07 12:44:38.117000: I runner.py:310] Step = 58000 ; steps/s = 1.65, tokens/s = 43772 (43772 target) ; Learning rate = 0.000367 ; Loss = 1.443352\n",
      "2024-12-07 12:45:39.256000: I runner.py:310] Step = 58100 ; steps/s = 1.64, tokens/s = 44242 (44242 target) ; Learning rate = 0.000367 ; Loss = 1.446651\n",
      "2024-12-07 12:46:40.404000: I runner.py:310] Step = 58200 ; steps/s = 1.64, tokens/s = 44222 (44222 target) ; Learning rate = 0.000366 ; Loss = 1.448910\n",
      "2024-12-07 12:47:41.067000: I runner.py:310] Step = 58300 ; steps/s = 1.65, tokens/s = 43797 (43797 target) ; Learning rate = 0.000366 ; Loss = 1.443453\n",
      "2024-12-07 12:48:42.204000: I runner.py:310] Step = 58400 ; steps/s = 1.64, tokens/s = 44241 (44241 target) ; Learning rate = 0.000366 ; Loss = 1.445106\n",
      "2024-12-07 12:49:42.927000: I runner.py:310] Step = 58500 ; steps/s = 1.65, tokens/s = 43745 (43745 target) ; Learning rate = 0.000365 ; Loss = 1.444544\n",
      "2024-12-07 12:50:44.057000: I runner.py:310] Step = 58600 ; steps/s = 1.64, tokens/s = 44248 (44248 target) ; Learning rate = 0.000365 ; Loss = 1.442367\n",
      "2024-12-07 12:51:45.251000: I runner.py:310] Step = 58700 ; steps/s = 1.63, tokens/s = 44196 (44196 target) ; Learning rate = 0.000365 ; Loss = 1.446507\n",
      "2024-12-07 12:52:45.932000: I runner.py:310] Step = 58800 ; steps/s = 1.65, tokens/s = 43781 (43781 target) ; Learning rate = 0.000365 ; Loss = 1.445498\n",
      "2024-12-07 12:53:47.053000: I runner.py:310] Step = 58900 ; steps/s = 1.64, tokens/s = 44260 (44260 target) ; Learning rate = 0.000364 ; Loss = 1.444468\n",
      "2024-12-07 12:54:47.707000: I runner.py:310] Step = 59000 ; steps/s = 1.65, tokens/s = 43784 (43784 target) ; Learning rate = 0.000364 ; Loss = 1.440262\n",
      "2024-12-07 12:55:48.880000: I runner.py:310] Step = 59100 ; steps/s = 1.63, tokens/s = 44218 (44218 target) ; Learning rate = 0.000364 ; Loss = 1.449006\n",
      "2024-12-07 12:56:50.015000: I runner.py:310] Step = 59200 ; steps/s = 1.64, tokens/s = 44241 (44241 target) ; Learning rate = 0.000363 ; Loss = 1.446292\n",
      "2024-12-07 12:57:50.666000: I runner.py:310] Step = 59300 ; steps/s = 1.65, tokens/s = 43805 (43805 target) ; Learning rate = 0.000363 ; Loss = 1.444465\n",
      "2024-12-07 12:58:51.790000: I runner.py:310] Step = 59400 ; steps/s = 1.64, tokens/s = 44249 (44249 target) ; Learning rate = 0.000363 ; Loss = 1.443924\n",
      "2024-12-07 12:59:52.540000: I runner.py:310] Step = 59500 ; steps/s = 1.65, tokens/s = 43729 (43729 target) ; Learning rate = 0.000362 ; Loss = 1.436879\n",
      "2024-12-07 13:00:53.665000: I runner.py:310] Step = 59600 ; steps/s = 1.64, tokens/s = 44250 (44250 target) ; Learning rate = 0.000362 ; Loss = 1.445647\n",
      "2024-12-07 13:01:54.827000: I runner.py:310] Step = 59700 ; steps/s = 1.64, tokens/s = 44217 (44217 target) ; Learning rate = 0.000362 ; Loss = 1.455560\n",
      "2024-12-07 13:02:55.533000: I runner.py:310] Step = 59800 ; steps/s = 1.65, tokens/s = 43761 (43761 target) ; Learning rate = 0.000361 ; Loss = 1.446058\n",
      "2024-12-07 13:03:56.640000: I runner.py:310] Step = 59900 ; steps/s = 1.64, tokens/s = 44259 (44259 target) ; Learning rate = 0.000361 ; Loss = 1.445579\n",
      "2024-12-07 13:04:57.361000: I runner.py:310] Step = 60000 ; steps/s = 1.65, tokens/s = 43758 (43758 target) ; Learning rate = 0.000361 ; Loss = 1.441538\n",
      "2024-12-07 13:04:59.389000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-60000\n",
      "2024-12-07 13:04:59.389000: I training.py:192] Running evaluation for step 60000\n",
      "2024-12-07 13:05:48.808000: I training.py:192] Evaluation result for step 60000: loss = 0.895250 ; perplexity = 2.447948\n",
      "2024-12-07 13:06:49.746000: I runner.py:310] Step = 60100 ; steps/s = 1.64, tokens/s = 44392 (44392 target) ; Learning rate = 0.000361 ; Loss = 1.441612\n",
      "2024-12-07 13:07:50.886000: I runner.py:310] Step = 60200 ; steps/s = 1.64, tokens/s = 44235 (44235 target) ; Learning rate = 0.000360 ; Loss = 1.445235\n",
      "2024-12-07 13:08:51.594000: I runner.py:310] Step = 60300 ; steps/s = 1.65, tokens/s = 43761 (43761 target) ; Learning rate = 0.000360 ; Loss = 1.446759\n",
      "2024-12-07 13:09:52.694000: I runner.py:310] Step = 60400 ; steps/s = 1.64, tokens/s = 44265 (44265 target) ; Learning rate = 0.000360 ; Loss = 1.444537\n",
      "2024-12-07 13:10:53.621000: I runner.py:310] Step = 60500 ; steps/s = 1.64, tokens/s = 44018 (44018 target) ; Learning rate = 0.000359 ; Loss = 1.477730\n",
      "2024-12-07 13:11:54.501000: I runner.py:310] Step = 60600 ; steps/s = 1.64, tokens/s = 44011 (44011 target) ; Learning rate = 0.000359 ; Loss = 1.439136\n",
      "2024-12-07 13:12:55.635000: I runner.py:310] Step = 60700 ; steps/s = 1.64, tokens/s = 44238 (44238 target) ; Learning rate = 0.000359 ; Loss = 1.447227\n",
      "2024-12-07 13:13:56.313000: I runner.py:310] Step = 60800 ; steps/s = 1.65, tokens/s = 43784 (43784 target) ; Learning rate = 0.000358 ; Loss = 1.440016\n",
      "2024-12-07 13:14:57.471000: I runner.py:310] Step = 60900 ; steps/s = 1.64, tokens/s = 44230 (44230 target) ; Learning rate = 0.000358 ; Loss = 1.446503\n",
      "2024-12-07 13:15:58.616000: I runner.py:310] Step = 61000 ; steps/s = 1.64, tokens/s = 44229 (44229 target) ; Learning rate = 0.000358 ; Loss = 1.444638\n",
      "2024-12-07 13:16:59.298000: I runner.py:310] Step = 61100 ; steps/s = 1.65, tokens/s = 43777 (43777 target) ; Learning rate = 0.000358 ; Loss = 1.446232\n",
      "2024-12-07 13:18:00.385000: I runner.py:310] Step = 61200 ; steps/s = 1.64, tokens/s = 44279 (44279 target) ; Learning rate = 0.000357 ; Loss = 1.441664\n",
      "2024-12-07 13:19:01.089000: I runner.py:310] Step = 61300 ; steps/s = 1.65, tokens/s = 43757 (43757 target) ; Learning rate = 0.000357 ; Loss = 1.447580\n",
      "2024-12-07 13:20:02.219000: I runner.py:310] Step = 61400 ; steps/s = 1.64, tokens/s = 44251 (44251 target) ; Learning rate = 0.000357 ; Loss = 1.444620\n",
      "2024-12-07 13:21:03.366000: I runner.py:310] Step = 61500 ; steps/s = 1.64, tokens/s = 44227 (44227 target) ; Learning rate = 0.000356 ; Loss = 1.443192\n",
      "2024-12-07 13:22:04.029000: I runner.py:310] Step = 61600 ; steps/s = 1.65, tokens/s = 43794 (43794 target) ; Learning rate = 0.000356 ; Loss = 1.441890\n",
      "2024-12-07 13:23:05.187000: I runner.py:310] Step = 61700 ; steps/s = 1.64, tokens/s = 44216 (44216 target) ; Learning rate = 0.000356 ; Loss = 1.442820\n",
      "2024-12-07 13:24:05.857000: I runner.py:310] Step = 61800 ; steps/s = 1.65, tokens/s = 43793 (43793 target) ; Learning rate = 0.000356 ; Loss = 1.442411\n",
      "2024-12-07 13:25:06.975000: I runner.py:310] Step = 61900 ; steps/s = 1.64, tokens/s = 44254 (44254 target) ; Learning rate = 0.000355 ; Loss = 1.443192\n",
      "2024-12-07 13:26:08.095000: I runner.py:310] Step = 62000 ; steps/s = 1.64, tokens/s = 44250 (44250 target) ; Learning rate = 0.000355 ; Loss = 1.444096\n",
      "2024-12-07 13:27:08.747000: I runner.py:310] Step = 62100 ; steps/s = 1.65, tokens/s = 43796 (43796 target) ; Learning rate = 0.000355 ; Loss = 1.442472\n",
      "2024-12-07 13:28:09.760000: I runner.py:310] Step = 62200 ; steps/s = 1.64, tokens/s = 44328 (44328 target) ; Learning rate = 0.000354 ; Loss = 1.443142\n",
      "2024-12-07 13:29:10.408000: I runner.py:310] Step = 62300 ; steps/s = 1.65, tokens/s = 43810 (43810 target) ; Learning rate = 0.000354 ; Loss = 1.437330\n",
      "2024-12-07 13:30:11.526000: I runner.py:310] Step = 62400 ; steps/s = 1.64, tokens/s = 44243 (44243 target) ; Learning rate = 0.000354 ; Loss = 1.438782\n",
      "2024-12-07 13:31:12.662000: I runner.py:310] Step = 62500 ; steps/s = 1.64, tokens/s = 44243 (44243 target) ; Learning rate = 0.000354 ; Loss = 1.444404\n",
      "2024-12-07 13:32:13.353000: I runner.py:310] Step = 62600 ; steps/s = 1.65, tokens/s = 43783 (43783 target) ; Learning rate = 0.000353 ; Loss = 1.443349\n",
      "2024-12-07 13:33:14.513000: I runner.py:310] Step = 62700 ; steps/s = 1.64, tokens/s = 44220 (44220 target) ; Learning rate = 0.000353 ; Loss = 1.444843\n",
      "2024-12-07 13:34:15.177000: I runner.py:310] Step = 62800 ; steps/s = 1.65, tokens/s = 43789 (43789 target) ; Learning rate = 0.000353 ; Loss = 1.434113\n",
      "2024-12-07 13:35:16.264000: I runner.py:310] Step = 62900 ; steps/s = 1.64, tokens/s = 44276 (44276 target) ; Learning rate = 0.000352 ; Loss = 1.445683\n",
      "2024-12-07 13:36:17.382000: I runner.py:310] Step = 63000 ; steps/s = 1.64, tokens/s = 44251 (44251 target) ; Learning rate = 0.000352 ; Loss = 1.443851\n",
      "2024-12-07 13:37:18.078000: I runner.py:310] Step = 63100 ; steps/s = 1.65, tokens/s = 43773 (43773 target) ; Learning rate = 0.000352 ; Loss = 1.442024\n",
      "2024-12-07 13:38:19.221000: I runner.py:310] Step = 63200 ; steps/s = 1.64, tokens/s = 44233 (44233 target) ; Learning rate = 0.000352 ; Loss = 1.443839\n",
      "2024-12-07 13:39:19.829000: I runner.py:310] Step = 63300 ; steps/s = 1.65, tokens/s = 43829 (43829 target) ; Learning rate = 0.000351 ; Loss = 1.439505\n",
      "2024-12-07 13:40:20.948000: I runner.py:310] Step = 63400 ; steps/s = 1.64, tokens/s = 44252 (44252 target) ; Learning rate = 0.000351 ; Loss = 1.439854\n",
      "2024-12-07 13:41:22.025000: I runner.py:310] Step = 63500 ; steps/s = 1.64, tokens/s = 44282 (44282 target) ; Learning rate = 0.000351 ; Loss = 1.444996\n",
      "2024-12-07 13:42:22.726000: I runner.py:310] Step = 63600 ; steps/s = 1.65, tokens/s = 43760 (43760 target) ; Learning rate = 0.000350 ; Loss = 1.440440\n",
      "2024-12-07 13:43:23.839000: I runner.py:310] Step = 63700 ; steps/s = 1.64, tokens/s = 44264 (44264 target) ; Learning rate = 0.000350 ; Loss = 1.444107\n",
      "2024-12-07 13:44:24.499000: I runner.py:310] Step = 63800 ; steps/s = 1.65, tokens/s = 43795 (43795 target) ; Learning rate = 0.000350 ; Loss = 1.438219\n",
      "2024-12-07 13:45:25.609000: I runner.py:310] Step = 63900 ; steps/s = 1.64, tokens/s = 44260 (44260 target) ; Learning rate = 0.000350 ; Loss = 1.439568\n",
      "2024-12-07 13:46:26.707000: I runner.py:310] Step = 64000 ; steps/s = 1.64, tokens/s = 44266 (44266 target) ; Learning rate = 0.000349 ; Loss = 1.443316\n",
      "2024-12-07 13:47:27.350000: I runner.py:310] Step = 64100 ; steps/s = 1.65, tokens/s = 43809 (43809 target) ; Learning rate = 0.000349 ; Loss = 1.437554\n",
      "2024-12-07 13:48:28.434000: I runner.py:310] Step = 64200 ; steps/s = 1.64, tokens/s = 44278 (44278 target) ; Learning rate = 0.000349 ; Loss = 1.443173\n",
      "2024-12-07 13:49:29.068000: I runner.py:310] Step = 64300 ; steps/s = 1.65, tokens/s = 43819 (43819 target) ; Learning rate = 0.000349 ; Loss = 1.439017\n",
      "2024-12-07 13:50:30.138000: I runner.py:310] Step = 64400 ; steps/s = 1.64, tokens/s = 44287 (44287 target) ; Learning rate = 0.000348 ; Loss = 1.440168\n",
      "2024-12-07 13:51:31.285000: I runner.py:310] Step = 64500 ; steps/s = 1.64, tokens/s = 44230 (44230 target) ; Learning rate = 0.000348 ; Loss = 1.444073\n",
      "2024-12-07 13:52:31.992000: I runner.py:310] Step = 64600 ; steps/s = 1.65, tokens/s = 43764 (43764 target) ; Learning rate = 0.000348 ; Loss = 1.441318\n",
      "2024-12-07 13:53:33.112000: I runner.py:310] Step = 64700 ; steps/s = 1.64, tokens/s = 44248 (44248 target) ; Learning rate = 0.000347 ; Loss = 1.443193\n",
      "2024-12-07 13:54:34.255000: I runner.py:310] Step = 64800 ; steps/s = 1.64, tokens/s = 44236 (44236 target) ; Learning rate = 0.000347 ; Loss = 1.440025\n",
      "2024-12-07 13:55:34.961000: I runner.py:310] Step = 64900 ; steps/s = 1.65, tokens/s = 43762 (43762 target) ; Learning rate = 0.000347 ; Loss = 1.438553\n",
      "2024-12-07 13:56:36.089000: I runner.py:310] Step = 65000 ; steps/s = 1.64, tokens/s = 44243 (44243 target) ; Learning rate = 0.000347 ; Loss = 1.444565\n",
      "2024-12-07 13:56:36.091000: I training.py:192] Running evaluation for step 65000\n",
      "2024-12-07 13:57:25.581000: I training.py:192] Evaluation result for step 65000: loss = 0.902774 ; perplexity = 2.466436\n",
      "2024-12-07 13:58:26.119000: I runner.py:310] Step = 65100 ; steps/s = 1.65, tokens/s = 43894 (43894 target) ; Learning rate = 0.000346 ; Loss = 1.438559\n",
      "2024-12-07 13:59:27.212000: I runner.py:310] Step = 65200 ; steps/s = 1.64, tokens/s = 44269 (44269 target) ; Learning rate = 0.000346 ; Loss = 1.439363\n",
      "2024-12-07 14:00:28.343000: I runner.py:310] Step = 65300 ; steps/s = 1.64, tokens/s = 44247 (44247 target) ; Learning rate = 0.000346 ; Loss = 1.444932\n",
      "2024-12-07 14:01:29.050000: I runner.py:310] Step = 65400 ; steps/s = 1.65, tokens/s = 43762 (43762 target) ; Learning rate = 0.000346 ; Loss = 1.440963\n",
      "2024-12-07 14:02:30.176000: I runner.py:310] Step = 65500 ; steps/s = 1.64, tokens/s = 44252 (44252 target) ; Learning rate = 0.000345 ; Loss = 1.445315\n",
      "2024-12-07 14:03:30.938000: I runner.py:310] Step = 65600 ; steps/s = 1.65, tokens/s = 43718 (43718 target) ; Learning rate = 0.000345 ; Loss = 1.442891\n",
      "2024-12-07 14:04:31.968000: I runner.py:310] Step = 65700 ; steps/s = 1.64, tokens/s = 44309 (44309 target) ; Learning rate = 0.000345 ; Loss = 1.442895\n",
      "2024-12-07 14:05:33.093000: I runner.py:310] Step = 65800 ; steps/s = 1.64, tokens/s = 44253 (44253 target) ; Learning rate = 0.000345 ; Loss = 1.441175\n",
      "2024-12-07 14:06:33.771000: I runner.py:310] Step = 65900 ; steps/s = 1.65, tokens/s = 43779 (43779 target) ; Learning rate = 0.000344 ; Loss = 1.437698\n",
      "2024-12-07 14:07:34.847000: I runner.py:310] Step = 66000 ; steps/s = 1.64, tokens/s = 44293 (44293 target) ; Learning rate = 0.000344 ; Loss = 1.438553\n",
      "2024-12-07 14:08:35.516000: I runner.py:310] Step = 66100 ; steps/s = 1.65, tokens/s = 43783 (43783 target) ; Learning rate = 0.000344 ; Loss = 1.433036\n",
      "2024-12-07 14:09:36.612000: I runner.py:310] Step = 66200 ; steps/s = 1.64, tokens/s = 44272 (44272 target) ; Learning rate = 0.000344 ; Loss = 1.449207\n",
      "2024-12-07 14:10:37.778000: I runner.py:310] Step = 66300 ; steps/s = 1.64, tokens/s = 44215 (44215 target) ; Learning rate = 0.000343 ; Loss = 1.443754\n",
      "2024-12-07 14:11:38.429000: I runner.py:310] Step = 66400 ; steps/s = 1.65, tokens/s = 43805 (43805 target) ; Learning rate = 0.000343 ; Loss = 1.439012\n",
      "2024-12-07 14:12:39.532000: I runner.py:310] Step = 66500 ; steps/s = 1.64, tokens/s = 44272 (44272 target) ; Learning rate = 0.000343 ; Loss = 1.439775\n",
      "2024-12-07 14:13:40.224000: I runner.py:310] Step = 66600 ; steps/s = 1.65, tokens/s = 43764 (43764 target) ; Learning rate = 0.000342 ; Loss = 1.440655\n",
      "2024-12-07 14:14:41.328000: I runner.py:310] Step = 66700 ; steps/s = 1.64, tokens/s = 44262 (44262 target) ; Learning rate = 0.000342 ; Loss = 1.439914\n",
      "2024-12-07 14:15:42.454000: I runner.py:310] Step = 66800 ; steps/s = 1.64, tokens/s = 44246 (44246 target) ; Learning rate = 0.000342 ; Loss = 1.442483\n",
      "2024-12-07 14:16:43.080000: I runner.py:310] Step = 66900 ; steps/s = 1.65, tokens/s = 43824 (43824 target) ; Learning rate = 0.000342 ; Loss = 1.441918\n",
      "2024-12-07 14:17:44.212000: I runner.py:310] Step = 67000 ; steps/s = 1.64, tokens/s = 44245 (44245 target) ; Learning rate = 0.000341 ; Loss = 1.439282\n",
      "2024-12-07 14:18:44.919000: I runner.py:310] Step = 67100 ; steps/s = 1.65, tokens/s = 43757 (43757 target) ; Learning rate = 0.000341 ; Loss = 1.434684\n",
      "2024-12-07 14:19:46.011000: I runner.py:310] Step = 67200 ; steps/s = 1.64, tokens/s = 44268 (44268 target) ; Learning rate = 0.000341 ; Loss = 1.438475\n",
      "2024-12-07 14:20:47.144000: I runner.py:310] Step = 67300 ; steps/s = 1.64, tokens/s = 44246 (44246 target) ; Learning rate = 0.000341 ; Loss = 1.443693\n",
      "2024-12-07 14:21:47.809000: I runner.py:310] Step = 67400 ; steps/s = 1.65, tokens/s = 43789 (43789 target) ; Learning rate = 0.000340 ; Loss = 1.439747\n",
      "2024-12-07 14:22:48.894000: I runner.py:310] Step = 67500 ; steps/s = 1.64, tokens/s = 44278 (44278 target) ; Learning rate = 0.000340 ; Loss = 1.437308\n",
      "2024-12-07 14:23:49.538000: I runner.py:310] Step = 67600 ; steps/s = 1.65, tokens/s = 43803 (43803 target) ; Learning rate = 0.000340 ; Loss = 1.439245\n",
      "2024-12-07 14:24:50.636000: I runner.py:310] Step = 67700 ; steps/s = 1.64, tokens/s = 44272 (44272 target) ; Learning rate = 0.000340 ; Loss = 1.438587\n",
      "2024-12-07 14:25:51.726000: I runner.py:310] Step = 67800 ; steps/s = 1.64, tokens/s = 44273 (44273 target) ; Learning rate = 0.000339 ; Loss = 1.450947\n",
      "2024-12-07 14:26:52.330000: I runner.py:310] Step = 67900 ; steps/s = 1.65, tokens/s = 43836 (43836 target) ; Learning rate = 0.000339 ; Loss = 1.439900\n",
      "2024-12-07 14:27:53.398000: I runner.py:310] Step = 68000 ; steps/s = 1.64, tokens/s = 44288 (44288 target) ; Learning rate = 0.000339 ; Loss = 1.451206\n",
      "2024-12-07 14:28:54.145000: I runner.py:310] Step = 68100 ; steps/s = 1.65, tokens/s = 43732 (43732 target) ; Learning rate = 0.000339 ; Loss = 1.432730\n",
      "2024-12-07 14:29:55.237000: I runner.py:310] Step = 68200 ; steps/s = 1.64, tokens/s = 44273 (44273 target) ; Learning rate = 0.000338 ; Loss = 1.438310\n",
      "2024-12-07 14:30:56.394000: I runner.py:310] Step = 68300 ; steps/s = 1.64, tokens/s = 44226 (44226 target) ; Learning rate = 0.000338 ; Loss = 1.440457\n",
      "2024-12-07 14:31:57.125000: I runner.py:310] Step = 68400 ; steps/s = 1.65, tokens/s = 43742 (43742 target) ; Learning rate = 0.000338 ; Loss = 1.439450\n",
      "2024-12-07 14:32:58.229000: I runner.py:310] Step = 68500 ; steps/s = 1.64, tokens/s = 44262 (44262 target) ; Learning rate = 0.000338 ; Loss = 1.443202\n",
      "2024-12-07 14:33:59.189000: I runner.py:310] Step = 68600 ; steps/s = 1.64, tokens/s = 44099 (44099 target) ; Learning rate = 0.000337 ; Loss = 1.452006\n",
      "2024-12-07 14:34:59.986000: I runner.py:310] Step = 68700 ; steps/s = 1.64, tokens/s = 43966 (43966 target) ; Learning rate = 0.000337 ; Loss = 1.439078\n",
      "2024-12-07 14:36:01.086000: I runner.py:310] Step = 68800 ; steps/s = 1.64, tokens/s = 44263 (44263 target) ; Learning rate = 0.000337 ; Loss = 1.444952\n",
      "2024-12-07 14:37:01.783000: I runner.py:310] Step = 68900 ; steps/s = 1.65, tokens/s = 43776 (43776 target) ; Learning rate = 0.000337 ; Loss = 1.440218\n",
      "2024-12-07 14:38:02.904000: I runner.py:310] Step = 69000 ; steps/s = 1.64, tokens/s = 44252 (44252 target) ; Learning rate = 0.000336 ; Loss = 1.445654\n",
      "2024-12-07 14:39:04.039000: I runner.py:310] Step = 69100 ; steps/s = 1.64, tokens/s = 44239 (44239 target) ; Learning rate = 0.000336 ; Loss = 1.443208\n",
      "2024-12-07 14:40:04.732000: I runner.py:310] Step = 69200 ; steps/s = 1.65, tokens/s = 43760 (43760 target) ; Learning rate = 0.000336 ; Loss = 1.439704\n",
      "2024-12-07 14:41:05.901000: I runner.py:310] Step = 69300 ; steps/s = 1.63, tokens/s = 44221 (44221 target) ; Learning rate = 0.000336 ; Loss = 1.443071\n",
      "2024-12-07 14:42:06.558000: I runner.py:310] Step = 69400 ; steps/s = 1.65, tokens/s = 43801 (43801 target) ; Learning rate = 0.000336 ; Loss = 1.440014\n",
      "2024-12-07 14:43:07.674000: I runner.py:310] Step = 69500 ; steps/s = 1.64, tokens/s = 44256 (44256 target) ; Learning rate = 0.000335 ; Loss = 1.440290\n",
      "2024-12-07 14:44:08.816000: I runner.py:310] Step = 69600 ; steps/s = 1.64, tokens/s = 44234 (44234 target) ; Learning rate = 0.000335 ; Loss = 1.440162\n",
      "2024-12-07 14:45:09.528000: I runner.py:310] Step = 69700 ; steps/s = 1.65, tokens/s = 43756 (43756 target) ; Learning rate = 0.000335 ; Loss = 1.437680\n",
      "2024-12-07 14:46:10.669000: I runner.py:310] Step = 69800 ; steps/s = 1.64, tokens/s = 44238 (44238 target) ; Learning rate = 0.000335 ; Loss = 1.439953\n",
      "2024-12-07 14:47:11.311000: I runner.py:310] Step = 69900 ; steps/s = 1.65, tokens/s = 43808 (43808 target) ; Learning rate = 0.000334 ; Loss = 1.434625\n",
      "2024-12-07 14:48:12.439000: I runner.py:310] Step = 70000 ; steps/s = 1.64, tokens/s = 44246 (44246 target) ; Learning rate = 0.000334 ; Loss = 1.440229\n",
      "2024-12-07 14:48:14.881000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-70000\n",
      "2024-12-07 14:48:14.881000: I training.py:192] Running evaluation for step 70000\n",
      "2024-12-07 14:49:04.419000: I training.py:192] Evaluation result for step 70000: loss = 0.907125 ; perplexity = 2.477190\n",
      "2024-12-07 14:50:05.481000: I runner.py:310] Step = 70100 ; steps/s = 1.64, tokens/s = 44304 (44304 target) ; Learning rate = 0.000334 ; Loss = 1.441402\n",
      "2024-12-07 14:51:06.181000: I runner.py:310] Step = 70200 ; steps/s = 1.65, tokens/s = 43771 (43771 target) ; Learning rate = 0.000334 ; Loss = 1.436291\n",
      "2024-12-07 14:52:07.288000: I runner.py:310] Step = 70300 ; steps/s = 1.64, tokens/s = 44260 (44260 target) ; Learning rate = 0.000333 ; Loss = 1.441169\n",
      "2024-12-07 14:53:08.002000: I runner.py:310] Step = 70400 ; steps/s = 1.65, tokens/s = 43756 (43756 target) ; Learning rate = 0.000333 ; Loss = 1.436153\n",
      "2024-12-07 14:54:09.153000: I runner.py:310] Step = 70500 ; steps/s = 1.64, tokens/s = 44232 (44232 target) ; Learning rate = 0.000333 ; Loss = 1.437691\n",
      "2024-12-07 14:55:10.277000: I runner.py:310] Step = 70600 ; steps/s = 1.64, tokens/s = 44244 (44244 target) ; Learning rate = 0.000333 ; Loss = 1.439500\n",
      "2024-12-07 14:56:10.910000: I runner.py:310] Step = 70700 ; steps/s = 1.65, tokens/s = 43814 (43814 target) ; Learning rate = 0.000332 ; Loss = 1.436595\n",
      "2024-12-07 14:57:12.040000: I runner.py:310] Step = 70800 ; steps/s = 1.64, tokens/s = 44247 (44247 target) ; Learning rate = 0.000332 ; Loss = 1.434718\n",
      "2024-12-07 14:58:12.724000: I runner.py:310] Step = 70900 ; steps/s = 1.65, tokens/s = 43778 (43778 target) ; Learning rate = 0.000332 ; Loss = 1.432650\n",
      "2024-12-07 14:59:13.813000: I runner.py:310] Step = 71000 ; steps/s = 1.64, tokens/s = 44271 (44271 target) ; Learning rate = 0.000332 ; Loss = 1.436959\n",
      "2024-12-07 15:00:14.926000: I runner.py:310] Step = 71100 ; steps/s = 1.64, tokens/s = 44259 (44259 target) ; Learning rate = 0.000331 ; Loss = 1.441604\n",
      "2024-12-07 15:01:15.602000: I runner.py:310] Step = 71200 ; steps/s = 1.65, tokens/s = 43779 (43779 target) ; Learning rate = 0.000331 ; Loss = 1.441606\n",
      "2024-12-07 15:02:16.673000: I runner.py:310] Step = 71300 ; steps/s = 1.64, tokens/s = 44291 (44291 target) ; Learning rate = 0.000331 ; Loss = 1.440521\n",
      "2024-12-07 15:03:17.312000: I runner.py:310] Step = 71400 ; steps/s = 1.65, tokens/s = 43808 (43808 target) ; Learning rate = 0.000331 ; Loss = 1.432296\n",
      "2024-12-07 15:04:18.398000: I runner.py:310] Step = 71500 ; steps/s = 1.64, tokens/s = 44280 (44280 target) ; Learning rate = 0.000331 ; Loss = 1.440549\n",
      "2024-12-07 15:05:19.446000: I runner.py:310] Step = 71600 ; steps/s = 1.64, tokens/s = 44301 (44301 target) ; Learning rate = 0.000330 ; Loss = 1.440913\n",
      "2024-12-07 15:06:20.186000: I runner.py:310] Step = 71700 ; steps/s = 1.65, tokens/s = 43734 (43734 target) ; Learning rate = 0.000330 ; Loss = 1.438896\n",
      "2024-12-07 15:07:21.273000: I runner.py:310] Step = 71800 ; steps/s = 1.64, tokens/s = 44278 (44278 target) ; Learning rate = 0.000330 ; Loss = 1.443089\n",
      "2024-12-07 15:08:21.986000: I runner.py:310] Step = 71900 ; steps/s = 1.65, tokens/s = 43754 (43754 target) ; Learning rate = 0.000330 ; Loss = 1.436139\n",
      "2024-12-07 15:09:23.069000: I runner.py:310] Step = 72000 ; steps/s = 1.64, tokens/s = 44281 (44281 target) ; Learning rate = 0.000329 ; Loss = 1.439267\n",
      "2024-12-07 15:10:24.179000: I runner.py:310] Step = 72100 ; steps/s = 1.64, tokens/s = 44252 (44252 target) ; Learning rate = 0.000329 ; Loss = 1.443874\n",
      "2024-12-07 15:11:24.847000: I runner.py:310] Step = 72200 ; steps/s = 1.65, tokens/s = 43795 (43795 target) ; Learning rate = 0.000329 ; Loss = 1.433585\n",
      "2024-12-07 15:12:25.977000: I runner.py:310] Step = 72300 ; steps/s = 1.64, tokens/s = 44247 (44247 target) ; Learning rate = 0.000329 ; Loss = 1.439805\n",
      "2024-12-07 15:13:26.703000: I runner.py:310] Step = 72400 ; steps/s = 1.65, tokens/s = 43743 (43743 target) ; Learning rate = 0.000328 ; Loss = 1.433129\n",
      "2024-12-07 15:14:27.744000: I runner.py:310] Step = 72500 ; steps/s = 1.64, tokens/s = 44304 (44304 target) ; Learning rate = 0.000328 ; Loss = 1.435837\n",
      "2024-12-07 15:15:28.883000: I runner.py:310] Step = 72600 ; steps/s = 1.64, tokens/s = 44235 (44235 target) ; Learning rate = 0.000328 ; Loss = 1.440633\n",
      "2024-12-07 15:16:29.562000: I runner.py:310] Step = 72700 ; steps/s = 1.65, tokens/s = 43784 (43784 target) ; Learning rate = 0.000328 ; Loss = 1.434434\n",
      "2024-12-07 15:17:30.668000: I runner.py:310] Step = 72800 ; steps/s = 1.64, tokens/s = 44259 (44259 target) ; Learning rate = 0.000328 ; Loss = 1.437062\n",
      "2024-12-07 15:18:31.750000: I runner.py:310] Step = 72900 ; steps/s = 1.64, tokens/s = 44281 (44281 target) ; Learning rate = 0.000327 ; Loss = 1.440591\n",
      "2024-12-07 15:19:32.416000: I runner.py:310] Step = 73000 ; steps/s = 1.65, tokens/s = 43787 (43787 target) ; Learning rate = 0.000327 ; Loss = 1.432310\n",
      "2024-12-07 15:20:33.542000: I runner.py:310] Step = 73100 ; steps/s = 1.64, tokens/s = 44250 (44250 target) ; Learning rate = 0.000327 ; Loss = 1.442308\n",
      "2024-12-07 15:21:34.225000: I runner.py:310] Step = 73200 ; steps/s = 1.65, tokens/s = 43771 (43771 target) ; Learning rate = 0.000327 ; Loss = 1.435931\n",
      "2024-12-07 15:22:35.334000: I runner.py:310] Step = 73300 ; steps/s = 1.64, tokens/s = 44268 (44268 target) ; Learning rate = 0.000326 ; Loss = 1.438126\n",
      "2024-12-07 15:23:36.502000: I runner.py:310] Step = 73400 ; steps/s = 1.64, tokens/s = 44217 (44217 target) ; Learning rate = 0.000326 ; Loss = 1.439959\n",
      "2024-12-07 15:24:37.129000: I runner.py:310] Step = 73500 ; steps/s = 1.65, tokens/s = 43823 (43823 target) ; Learning rate = 0.000326 ; Loss = 1.433035\n",
      "2024-12-07 15:25:38.191000: I runner.py:310] Step = 73600 ; steps/s = 1.64, tokens/s = 44286 (44286 target) ; Learning rate = 0.000326 ; Loss = 1.437664\n",
      "2024-12-07 15:26:38.871000: I runner.py:310] Step = 73700 ; steps/s = 1.65, tokens/s = 43785 (43785 target) ; Learning rate = 0.000326 ; Loss = 1.435570\n",
      "2024-12-07 15:27:39.903000: I runner.py:310] Step = 73800 ; steps/s = 1.64, tokens/s = 44315 (44315 target) ; Learning rate = 0.000325 ; Loss = 1.438707\n",
      "2024-12-07 15:28:41.017000: I runner.py:310] Step = 73900 ; steps/s = 1.64, tokens/s = 44257 (44257 target) ; Learning rate = 0.000325 ; Loss = 1.438399\n",
      "2024-12-07 15:29:41.650000: I runner.py:310] Step = 74000 ; steps/s = 1.65, tokens/s = 43813 (43813 target) ; Learning rate = 0.000325 ; Loss = 1.432568\n",
      "2024-12-07 15:30:42.801000: I runner.py:310] Step = 74100 ; steps/s = 1.64, tokens/s = 44227 (44227 target) ; Learning rate = 0.000325 ; Loss = 1.439180\n",
      "2024-12-07 15:31:43.449000: I runner.py:310] Step = 74200 ; steps/s = 1.65, tokens/s = 43808 (43808 target) ; Learning rate = 0.000324 ; Loss = 1.436829\n",
      "2024-12-07 15:32:44.517000: I runner.py:310] Step = 74300 ; steps/s = 1.64, tokens/s = 44291 (44291 target) ; Learning rate = 0.000324 ; Loss = 1.439634\n",
      "2024-12-07 15:33:45.658000: I runner.py:310] Step = 74400 ; steps/s = 1.64, tokens/s = 44238 (44238 target) ; Learning rate = 0.000324 ; Loss = 1.439000\n",
      "2024-12-07 15:34:46.288000: I runner.py:310] Step = 74500 ; steps/s = 1.65, tokens/s = 43816 (43816 target) ; Learning rate = 0.000324 ; Loss = 1.435329\n",
      "2024-12-07 15:35:47.343000: I runner.py:310] Step = 74600 ; steps/s = 1.64, tokens/s = 44298 (44298 target) ; Learning rate = 0.000324 ; Loss = 1.433885\n",
      "2024-12-07 15:36:48.014000: I runner.py:310] Step = 74700 ; steps/s = 1.65, tokens/s = 43783 (43783 target) ; Learning rate = 0.000323 ; Loss = 1.432026\n",
      "2024-12-07 15:37:49.072000: I runner.py:310] Step = 74800 ; steps/s = 1.64, tokens/s = 44301 (44301 target) ; Learning rate = 0.000323 ; Loss = 1.435493\n",
      "2024-12-07 15:38:50.102000: I runner.py:310] Step = 74900 ; steps/s = 1.64, tokens/s = 44319 (44319 target) ; Learning rate = 0.000323 ; Loss = 1.436510\n",
      "2024-12-07 15:39:50.774000: I runner.py:310] Step = 75000 ; steps/s = 1.65, tokens/s = 43781 (43781 target) ; Learning rate = 0.000323 ; Loss = 1.439234\n",
      "2024-12-07 15:39:50.775000: I training.py:192] Running evaluation for step 75000\n",
      "2024-12-07 15:40:38.303000: I training.py:192] Evaluation result for step 75000: loss = 0.918595 ; perplexity = 2.505767\n",
      "2024-12-07 15:41:39.237000: I runner.py:310] Step = 75100 ; steps/s = 1.64, tokens/s = 44398 (44398 target) ; Learning rate = 0.000323 ; Loss = 1.436762\n",
      "2024-12-07 15:42:39.902000: I runner.py:310] Step = 75200 ; steps/s = 1.65, tokens/s = 43788 (43788 target) ; Learning rate = 0.000322 ; Loss = 1.439143\n",
      "2024-12-07 15:43:40.983000: I runner.py:310] Step = 75300 ; steps/s = 1.64, tokens/s = 44279 (44279 target) ; Learning rate = 0.000322 ; Loss = 1.434854\n",
      "2024-12-07 15:44:42.065000: I runner.py:310] Step = 75400 ; steps/s = 1.64, tokens/s = 44286 (44286 target) ; Learning rate = 0.000322 ; Loss = 1.437045\n",
      "2024-12-07 15:45:42.746000: I runner.py:310] Step = 75500 ; steps/s = 1.65, tokens/s = 43778 (43778 target) ; Learning rate = 0.000322 ; Loss = 1.435246\n",
      "2024-12-07 15:46:43.903000: I runner.py:310] Step = 75600 ; steps/s = 1.64, tokens/s = 44226 (44226 target) ; Learning rate = 0.000321 ; Loss = 1.441217\n",
      "2024-12-07 15:47:44.506000: I runner.py:310] Step = 75700 ; steps/s = 1.65, tokens/s = 43836 (43836 target) ; Learning rate = 0.000321 ; Loss = 1.432275\n",
      "2024-12-07 15:48:45.617000: I runner.py:310] Step = 75800 ; steps/s = 1.64, tokens/s = 44261 (44261 target) ; Learning rate = 0.000321 ; Loss = 1.437463\n",
      "2024-12-07 15:49:46.784000: I runner.py:310] Step = 75900 ; steps/s = 1.64, tokens/s = 44219 (44219 target) ; Learning rate = 0.000321 ; Loss = 1.440223\n",
      "2024-12-07 15:50:47.436000: I runner.py:310] Step = 76000 ; steps/s = 1.65, tokens/s = 43808 (43808 target) ; Learning rate = 0.000321 ; Loss = 1.436868\n",
      "2024-12-07 15:51:48.521000: I runner.py:310] Step = 76100 ; steps/s = 1.64, tokens/s = 44275 (44275 target) ; Learning rate = 0.000320 ; Loss = 1.435774\n",
      "2024-12-07 15:52:49.200000: I runner.py:310] Step = 76200 ; steps/s = 1.65, tokens/s = 43779 (43779 target) ; Learning rate = 0.000320 ; Loss = 1.434085\n",
      "2024-12-07 15:53:50.294000: I runner.py:310] Step = 76300 ; steps/s = 1.64, tokens/s = 44269 (44269 target) ; Learning rate = 0.000320 ; Loss = 1.435718\n",
      "2024-12-07 15:54:51.382000: I runner.py:310] Step = 76400 ; steps/s = 1.64, tokens/s = 44277 (44277 target) ; Learning rate = 0.000320 ; Loss = 1.437027\n",
      "2024-12-07 15:55:52.102000: I runner.py:310] Step = 76500 ; steps/s = 1.65, tokens/s = 43752 (43752 target) ; Learning rate = 0.000320 ; Loss = 1.434197\n",
      "2024-12-07 15:56:53.218000: I runner.py:310] Step = 76600 ; steps/s = 1.64, tokens/s = 44253 (44253 target) ; Learning rate = 0.000319 ; Loss = 1.441345\n",
      "2024-12-07 15:57:54.247000: I runner.py:310] Step = 76700 ; steps/s = 1.64, tokens/s = 44168 (44168 target) ; Learning rate = 0.000319 ; Loss = 1.451497\n",
      "2024-12-07 15:58:55.009000: I runner.py:310] Step = 76800 ; steps/s = 1.65, tokens/s = 43874 (43874 target) ; Learning rate = 0.000319 ; Loss = 1.433917\n",
      "2024-12-07 15:59:56.218000: I runner.py:310] Step = 76900 ; steps/s = 1.63, tokens/s = 44187 (44187 target) ; Learning rate = 0.000319 ; Loss = 1.434328\n",
      "2024-12-07 16:00:56.934000: I runner.py:310] Step = 77000 ; steps/s = 1.65, tokens/s = 43752 (43752 target) ; Learning rate = 0.000319 ; Loss = 1.436474\n",
      "2024-12-07 16:01:58.040000: I runner.py:310] Step = 77100 ; steps/s = 1.64, tokens/s = 44253 (44253 target) ; Learning rate = 0.000318 ; Loss = 1.435714\n",
      "2024-12-07 16:02:59.161000: I runner.py:310] Step = 77200 ; steps/s = 1.64, tokens/s = 44256 (44256 target) ; Learning rate = 0.000318 ; Loss = 1.439293\n",
      "2024-12-07 16:03:59.873000: I runner.py:310] Step = 77300 ; steps/s = 1.65, tokens/s = 43753 (43753 target) ; Learning rate = 0.000318 ; Loss = 1.438559\n",
      "2024-12-07 16:05:01.011000: I runner.py:310] Step = 77400 ; steps/s = 1.64, tokens/s = 44244 (44244 target) ; Learning rate = 0.000318 ; Loss = 1.436847\n",
      "2024-12-07 16:06:01.678000: I runner.py:310] Step = 77500 ; steps/s = 1.65, tokens/s = 43785 (43785 target) ; Learning rate = 0.000317 ; Loss = 1.436453\n",
      "2024-12-07 16:07:02.818000: I runner.py:310] Step = 77600 ; steps/s = 1.64, tokens/s = 44234 (44234 target) ; Learning rate = 0.000317 ; Loss = 1.437136\n",
      "2024-12-07 16:08:03.981000: I runner.py:310] Step = 77700 ; steps/s = 1.64, tokens/s = 44224 (44224 target) ; Learning rate = 0.000317 ; Loss = 1.433726\n",
      "2024-12-07 16:09:04.653000: I runner.py:310] Step = 77800 ; steps/s = 1.65, tokens/s = 43795 (43795 target) ; Learning rate = 0.000317 ; Loss = 1.432251\n",
      "2024-12-07 16:10:05.739000: I runner.py:310] Step = 77900 ; steps/s = 1.64, tokens/s = 44274 (44274 target) ; Learning rate = 0.000317 ; Loss = 1.439796\n",
      "2024-12-07 16:11:06.423000: I runner.py:310] Step = 78000 ; steps/s = 1.65, tokens/s = 43775 (43775 target) ; Learning rate = 0.000316 ; Loss = 1.434410\n",
      "2024-12-07 16:12:07.525000: I runner.py:310] Step = 78100 ; steps/s = 1.64, tokens/s = 44262 (44262 target) ; Learning rate = 0.000316 ; Loss = 1.435016\n",
      "2024-12-07 16:13:08.575000: I runner.py:310] Step = 78200 ; steps/s = 1.64, tokens/s = 44305 (44305 target) ; Learning rate = 0.000316 ; Loss = 1.437198\n",
      "2024-12-07 16:14:09.168000: I runner.py:310] Step = 78300 ; steps/s = 1.65, tokens/s = 43844 (43844 target) ; Learning rate = 0.000316 ; Loss = 1.440605\n",
      "2024-12-07 16:15:10.298000: I runner.py:310] Step = 78400 ; steps/s = 1.64, tokens/s = 44247 (44247 target) ; Learning rate = 0.000316 ; Loss = 1.435551\n",
      "2024-12-07 16:16:10.946000: I runner.py:310] Step = 78500 ; steps/s = 1.65, tokens/s = 43803 (43803 target) ; Learning rate = 0.000315 ; Loss = 1.438960\n",
      "2024-12-07 16:17:12.003000: I runner.py:310] Step = 78600 ; steps/s = 1.64, tokens/s = 44298 (44298 target) ; Learning rate = 0.000315 ; Loss = 1.436899\n",
      "2024-12-07 16:18:13.184000: I runner.py:310] Step = 78700 ; steps/s = 1.63, tokens/s = 44207 (44207 target) ; Learning rate = 0.000315 ; Loss = 1.436417\n",
      "2024-12-07 16:19:13.824000: I runner.py:310] Step = 78800 ; steps/s = 1.65, tokens/s = 43799 (43799 target) ; Learning rate = 0.000315 ; Loss = 1.435648\n",
      "2024-12-07 16:20:14.920000: I runner.py:310] Step = 78900 ; steps/s = 1.64, tokens/s = 44275 (44275 target) ; Learning rate = 0.000315 ; Loss = 1.437921\n",
      "2024-12-07 16:21:15.581000: I runner.py:310] Step = 79000 ; steps/s = 1.65, tokens/s = 43797 (43797 target) ; Learning rate = 0.000314 ; Loss = 1.435252\n",
      "2024-12-07 16:22:16.706000: I runner.py:310] Step = 79100 ; steps/s = 1.64, tokens/s = 44245 (44245 target) ; Learning rate = 0.000314 ; Loss = 1.437976\n",
      "2024-12-07 16:23:17.748000: I runner.py:310] Step = 79200 ; steps/s = 1.64, tokens/s = 44311 (44311 target) ; Learning rate = 0.000314 ; Loss = 1.436409\n",
      "2024-12-07 16:24:18.425000: I runner.py:310] Step = 79300 ; steps/s = 1.65, tokens/s = 43783 (43783 target) ; Learning rate = 0.000314 ; Loss = 1.431709\n",
      "2024-12-07 16:25:19.541000: I runner.py:310] Step = 79400 ; steps/s = 1.64, tokens/s = 44255 (44255 target) ; Learning rate = 0.000314 ; Loss = 1.436666\n",
      "2024-12-07 16:26:20.214000: I runner.py:310] Step = 79500 ; steps/s = 1.65, tokens/s = 43788 (43788 target) ; Learning rate = 0.000313 ; Loss = 1.436151\n",
      "2024-12-07 16:27:21.305000: I runner.py:310] Step = 79600 ; steps/s = 1.64, tokens/s = 44269 (44269 target) ; Learning rate = 0.000313 ; Loss = 1.433697\n",
      "2024-12-07 16:28:22.396000: I runner.py:310] Step = 79700 ; steps/s = 1.64, tokens/s = 44275 (44275 target) ; Learning rate = 0.000313 ; Loss = 1.436881\n",
      "2024-12-07 16:29:23.000000: I runner.py:310] Step = 79800 ; steps/s = 1.65, tokens/s = 43828 (43828 target) ; Learning rate = 0.000313 ; Loss = 1.439139\n",
      "2024-12-07 16:30:24.037000: I runner.py:310] Step = 79900 ; steps/s = 1.64, tokens/s = 44326 (44326 target) ; Learning rate = 0.000313 ; Loss = 1.436331\n",
      "2024-12-07 16:31:24.651000: I runner.py:310] Step = 80000 ; steps/s = 1.65, tokens/s = 43822 (43822 target) ; Learning rate = 0.000312 ; Loss = 1.434427\n",
      "2024-12-07 16:31:26.892000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-80000\n",
      "2024-12-07 16:31:26.892000: I training.py:192] Running evaluation for step 80000\n",
      "2024-12-07 16:32:14.112000: I training.py:192] Evaluation result for step 80000: loss = 0.922698 ; perplexity = 2.516070\n",
      "2024-12-07 16:33:15.069000: I runner.py:310] Step = 80100 ; steps/s = 1.64, tokens/s = 44385 (44385 target) ; Learning rate = 0.000312 ; Loss = 1.432789\n",
      "2024-12-07 16:34:16.225000: I runner.py:310] Step = 80200 ; steps/s = 1.64, tokens/s = 44218 (44218 target) ; Learning rate = 0.000312 ; Loss = 1.432643\n",
      "2024-12-07 16:35:16.895000: I runner.py:310] Step = 80300 ; steps/s = 1.65, tokens/s = 43786 (43786 target) ; Learning rate = 0.000312 ; Loss = 1.431392\n",
      "2024-12-07 16:36:18.015000: I runner.py:310] Step = 80400 ; steps/s = 1.64, tokens/s = 44255 (44255 target) ; Learning rate = 0.000312 ; Loss = 1.437902\n",
      "2024-12-07 16:37:18.732000: I runner.py:310] Step = 80500 ; steps/s = 1.65, tokens/s = 43755 (43755 target) ; Learning rate = 0.000312 ; Loss = 1.430184\n",
      "2024-12-07 16:38:19.787000: I runner.py:310] Step = 80600 ; steps/s = 1.64, tokens/s = 44294 (44294 target) ; Learning rate = 0.000311 ; Loss = 1.433866\n",
      "2024-12-07 16:39:20.890000: I runner.py:310] Step = 80700 ; steps/s = 1.64, tokens/s = 44264 (44264 target) ; Learning rate = 0.000311 ; Loss = 1.435345\n",
      "2024-12-07 16:40:21.570000: I runner.py:310] Step = 80800 ; steps/s = 1.65, tokens/s = 43775 (43775 target) ; Learning rate = 0.000311 ; Loss = 1.433822\n",
      "2024-12-07 16:41:22.649000: I runner.py:310] Step = 80900 ; steps/s = 1.64, tokens/s = 44286 (44286 target) ; Learning rate = 0.000311 ; Loss = 1.438296\n",
      "2024-12-07 16:42:23.729000: I runner.py:310] Step = 81000 ; steps/s = 1.64, tokens/s = 44279 (44279 target) ; Learning rate = 0.000311 ; Loss = 1.436840\n",
      "2024-12-07 16:43:24.380000: I runner.py:310] Step = 81100 ; steps/s = 1.65, tokens/s = 43792 (43792 target) ; Learning rate = 0.000310 ; Loss = 1.432914\n",
      "2024-12-07 16:44:25.469000: I runner.py:310] Step = 81200 ; steps/s = 1.64, tokens/s = 44280 (44280 target) ; Learning rate = 0.000310 ; Loss = 1.438397\n",
      "2024-12-07 16:45:26.167000: I runner.py:310] Step = 81300 ; steps/s = 1.65, tokens/s = 43776 (43776 target) ; Learning rate = 0.000310 ; Loss = 1.435484\n",
      "2024-12-07 16:46:27.239000: I runner.py:310] Step = 81400 ; steps/s = 1.64, tokens/s = 44285 (44285 target) ; Learning rate = 0.000310 ; Loss = 1.440946\n",
      "2024-12-07 16:47:28.336000: I runner.py:310] Step = 81500 ; steps/s = 1.64, tokens/s = 44268 (44268 target) ; Learning rate = 0.000310 ; Loss = 1.434044\n",
      "2024-12-07 16:48:29.003000: I runner.py:310] Step = 81600 ; steps/s = 1.65, tokens/s = 43791 (43791 target) ; Learning rate = 0.000309 ; Loss = 1.433122\n",
      "2024-12-07 16:49:30.147000: I runner.py:310] Step = 81700 ; steps/s = 1.64, tokens/s = 44236 (44236 target) ; Learning rate = 0.000309 ; Loss = 1.432640\n",
      "2024-12-07 16:50:30.777000: I runner.py:310] Step = 81800 ; steps/s = 1.65, tokens/s = 43817 (43817 target) ; Learning rate = 0.000309 ; Loss = 1.436724\n",
      "2024-12-07 16:51:31.884000: I runner.py:310] Step = 81900 ; steps/s = 1.64, tokens/s = 44252 (44252 target) ; Learning rate = 0.000309 ; Loss = 1.432383\n",
      "2024-12-07 16:52:32.985000: I runner.py:310] Step = 82000 ; steps/s = 1.64, tokens/s = 44270 (44270 target) ; Learning rate = 0.000309 ; Loss = 1.438977\n",
      "2024-12-07 16:53:33.637000: I runner.py:310] Step = 82100 ; steps/s = 1.65, tokens/s = 43801 (43801 target) ; Learning rate = 0.000308 ; Loss = 1.433410\n",
      "2024-12-07 16:54:34.764000: I runner.py:310] Step = 82200 ; steps/s = 1.64, tokens/s = 44247 (44247 target) ; Learning rate = 0.000308 ; Loss = 1.435065\n",
      "2024-12-07 16:55:35.395000: I runner.py:310] Step = 82300 ; steps/s = 1.65, tokens/s = 43817 (43817 target) ; Learning rate = 0.000308 ; Loss = 1.432322\n",
      "2024-12-07 16:56:36.470000: I runner.py:310] Step = 82400 ; steps/s = 1.64, tokens/s = 44289 (44289 target) ; Learning rate = 0.000308 ; Loss = 1.434695\n",
      "2024-12-07 16:57:37.546000: I runner.py:310] Step = 82500 ; steps/s = 1.64, tokens/s = 44280 (44280 target) ; Learning rate = 0.000308 ; Loss = 1.435661\n",
      "2024-12-07 16:58:38.216000: I runner.py:310] Step = 82600 ; steps/s = 1.65, tokens/s = 43781 (43781 target) ; Learning rate = 0.000308 ; Loss = 1.432075\n",
      "2024-12-07 16:59:39.329000: I runner.py:310] Step = 82700 ; steps/s = 1.64, tokens/s = 44264 (44264 target) ; Learning rate = 0.000307 ; Loss = 1.432636\n",
      "2024-12-07 17:00:39.995000: I runner.py:310] Step = 82800 ; steps/s = 1.65, tokens/s = 43790 (43790 target) ; Learning rate = 0.000307 ; Loss = 1.430393\n",
      "2024-12-07 17:01:41.102000: I runner.py:310] Step = 82900 ; steps/s = 1.64, tokens/s = 44259 (44259 target) ; Learning rate = 0.000307 ; Loss = 1.432335\n",
      "2024-12-07 17:02:42.223000: I runner.py:310] Step = 83000 ; steps/s = 1.64, tokens/s = 44254 (44254 target) ; Learning rate = 0.000307 ; Loss = 1.435694\n",
      "2024-12-07 17:03:42.820000: I runner.py:310] Step = 83100 ; steps/s = 1.65, tokens/s = 43834 (43834 target) ; Learning rate = 0.000307 ; Loss = 1.435907\n",
      "2024-12-07 17:04:43.867000: I runner.py:310] Step = 83200 ; steps/s = 1.64, tokens/s = 44306 (44306 target) ; Learning rate = 0.000306 ; Loss = 1.434320\n",
      "2024-12-07 17:05:44.509000: I runner.py:310] Step = 83300 ; steps/s = 1.65, tokens/s = 43814 (43814 target) ; Learning rate = 0.000306 ; Loss = 1.428606\n",
      "2024-12-07 17:06:45.638000: I runner.py:310] Step = 83400 ; steps/s = 1.64, tokens/s = 44246 (44246 target) ; Learning rate = 0.000306 ; Loss = 1.435278\n",
      "2024-12-07 17:07:46.736000: I runner.py:310] Step = 83500 ; steps/s = 1.64, tokens/s = 44265 (44265 target) ; Learning rate = 0.000306 ; Loss = 1.435723\n",
      "2024-12-07 17:08:47.294000: I runner.py:310] Step = 83600 ; steps/s = 1.65, tokens/s = 43875 (43875 target) ; Learning rate = 0.000306 ; Loss = 1.435196\n",
      "2024-12-07 17:09:48.330000: I runner.py:310] Step = 83700 ; steps/s = 1.64, tokens/s = 44315 (44315 target) ; Learning rate = 0.000306 ; Loss = 1.432875\n",
      "2024-12-07 17:10:48.945000: I runner.py:310] Step = 83800 ; steps/s = 1.65, tokens/s = 43825 (43825 target) ; Learning rate = 0.000305 ; Loss = 1.433073\n",
      "2024-12-07 17:11:49.992000: I runner.py:310] Step = 83900 ; steps/s = 1.64, tokens/s = 44303 (44303 target) ; Learning rate = 0.000305 ; Loss = 1.430362\n",
      "2024-12-07 17:12:51.039000: I runner.py:310] Step = 84000 ; steps/s = 1.64, tokens/s = 44299 (44299 target) ; Learning rate = 0.000305 ; Loss = 1.444493\n",
      "2024-12-07 17:13:51.652000: I runner.py:310] Step = 84100 ; steps/s = 1.65, tokens/s = 43829 (43829 target) ; Learning rate = 0.000305 ; Loss = 1.433330\n",
      "2024-12-07 17:14:52.728000: I runner.py:310] Step = 84200 ; steps/s = 1.64, tokens/s = 44287 (44287 target) ; Learning rate = 0.000305 ; Loss = 1.432840\n",
      "2024-12-07 17:15:53.450000: I runner.py:310] Step = 84300 ; steps/s = 1.65, tokens/s = 43754 (43754 target) ; Learning rate = 0.000304 ; Loss = 1.428831\n",
      "2024-12-07 17:16:54.514000: I runner.py:310] Step = 84400 ; steps/s = 1.64, tokens/s = 44288 (44288 target) ; Learning rate = 0.000304 ; Loss = 1.439970\n",
      "2024-12-07 17:17:55.622000: I runner.py:310] Step = 84500 ; steps/s = 1.64, tokens/s = 44272 (44272 target) ; Learning rate = 0.000304 ; Loss = 1.435711\n",
      "2024-12-07 17:18:56.237000: I runner.py:310] Step = 84600 ; steps/s = 1.65, tokens/s = 43819 (43819 target) ; Learning rate = 0.000304 ; Loss = 1.434259\n",
      "2024-12-07 17:19:57.311000: I runner.py:310] Step = 84700 ; steps/s = 1.64, tokens/s = 44284 (44284 target) ; Learning rate = 0.000304 ; Loss = 1.437434\n",
      "2024-12-07 17:20:58.385000: I runner.py:310] Step = 84800 ; steps/s = 1.64, tokens/s = 44280 (44280 target) ; Learning rate = 0.000304 ; Loss = 1.444506\n",
      "2024-12-07 17:21:59.061000: I runner.py:310] Step = 84900 ; steps/s = 1.65, tokens/s = 43785 (43785 target) ; Learning rate = 0.000303 ; Loss = 1.435238\n",
      "2024-12-07 17:23:00.109000: I runner.py:310] Step = 85000 ; steps/s = 1.64, tokens/s = 44305 (44305 target) ; Learning rate = 0.000303 ; Loss = 1.433885\n",
      "2024-12-07 17:23:00.110000: I training.py:192] Running evaluation for step 85000\n",
      "2024-12-07 17:23:47.481000: I training.py:192] Evaluation result for step 85000: loss = 0.924585 ; perplexity = 2.520822\n",
      "2024-12-07 17:24:48.012000: I runner.py:310] Step = 85100 ; steps/s = 1.65, tokens/s = 43898 (43898 target) ; Learning rate = 0.000303 ; Loss = 1.433233\n",
      "2024-12-07 17:25:49.085000: I runner.py:310] Step = 85200 ; steps/s = 1.64, tokens/s = 44291 (44291 target) ; Learning rate = 0.000303 ; Loss = 1.430713\n",
      "2024-12-07 17:26:50.170000: I runner.py:310] Step = 85300 ; steps/s = 1.64, tokens/s = 44276 (44276 target) ; Learning rate = 0.000303 ; Loss = 1.435847\n",
      "2024-12-07 17:27:50.770000: I runner.py:310] Step = 85400 ; steps/s = 1.65, tokens/s = 43838 (43838 target) ; Learning rate = 0.000302 ; Loss = 1.426066\n",
      "2024-12-07 17:28:51.848000: I runner.py:310] Step = 85500 ; steps/s = 1.64, tokens/s = 44282 (44282 target) ; Learning rate = 0.000302 ; Loss = 1.434052\n",
      "2024-12-07 17:29:52.507000: I runner.py:310] Step = 85600 ; steps/s = 1.65, tokens/s = 43794 (43794 target) ; Learning rate = 0.000302 ; Loss = 1.427424\n",
      "2024-12-07 17:30:53.630000: I runner.py:310] Step = 85700 ; steps/s = 1.64, tokens/s = 44246 (44246 target) ; Learning rate = 0.000302 ; Loss = 1.434116\n",
      "2024-12-07 17:31:54.706000: I runner.py:310] Step = 85800 ; steps/s = 1.64, tokens/s = 44285 (44285 target) ; Learning rate = 0.000302 ; Loss = 1.436061\n",
      "2024-12-07 17:32:55.422000: I runner.py:310] Step = 85900 ; steps/s = 1.65, tokens/s = 43754 (43754 target) ; Learning rate = 0.000302 ; Loss = 1.430997\n",
      "2024-12-07 17:33:56.467000: I runner.py:310] Step = 86000 ; steps/s = 1.64, tokens/s = 44309 (44309 target) ; Learning rate = 0.000301 ; Loss = 1.429773\n",
      "2024-12-07 17:34:57.086000: I runner.py:310] Step = 86100 ; steps/s = 1.65, tokens/s = 43820 (43820 target) ; Learning rate = 0.000301 ; Loss = 1.429418\n",
      "2024-12-07 17:35:58.143000: I runner.py:310] Step = 86200 ; steps/s = 1.64, tokens/s = 44297 (44297 target) ; Learning rate = 0.000301 ; Loss = 1.438178\n",
      "2024-12-07 17:36:59.188000: I runner.py:310] Step = 86300 ; steps/s = 1.64, tokens/s = 44310 (44310 target) ; Learning rate = 0.000301 ; Loss = 1.434947\n",
      "2024-12-07 17:37:59.870000: I runner.py:310] Step = 86400 ; steps/s = 1.65, tokens/s = 43774 (43774 target) ; Learning rate = 0.000301 ; Loss = 1.430882\n",
      "2024-12-07 17:39:00.954000: I runner.py:310] Step = 86500 ; steps/s = 1.64, tokens/s = 44283 (44283 target) ; Learning rate = 0.000301 ; Loss = 1.431206\n",
      "2024-12-07 17:40:01.589000: I runner.py:310] Step = 86600 ; steps/s = 1.65, tokens/s = 43808 (43808 target) ; Learning rate = 0.000300 ; Loss = 1.427909\n",
      "2024-12-07 17:41:02.607000: I runner.py:310] Step = 86700 ; steps/s = 1.64, tokens/s = 44329 (44329 target) ; Learning rate = 0.000300 ; Loss = 1.437444\n",
      "2024-12-07 17:42:03.679000: I runner.py:310] Step = 86800 ; steps/s = 1.64, tokens/s = 44285 (44285 target) ; Learning rate = 0.000300 ; Loss = 1.435117\n",
      "2024-12-07 17:43:04.339000: I runner.py:310] Step = 86900 ; steps/s = 1.65, tokens/s = 43801 (43801 target) ; Learning rate = 0.000300 ; Loss = 1.436628\n",
      "2024-12-07 17:44:05.380000: I runner.py:310] Step = 87000 ; steps/s = 1.64, tokens/s = 44308 (44308 target) ; Learning rate = 0.000300 ; Loss = 1.427655\n",
      "2024-12-07 17:45:06.006000: I runner.py:310] Step = 87100 ; steps/s = 1.65, tokens/s = 43818 (43818 target) ; Learning rate = 0.000299 ; Loss = 1.429935\n",
      "2024-12-07 17:46:07.057000: I runner.py:310] Step = 87200 ; steps/s = 1.64, tokens/s = 44302 (44302 target) ; Learning rate = 0.000299 ; Loss = 1.432490\n",
      "2024-12-07 17:47:08.135000: I runner.py:310] Step = 87300 ; steps/s = 1.64, tokens/s = 44277 (44277 target) ; Learning rate = 0.000299 ; Loss = 1.434674\n",
      "2024-12-07 17:48:08.808000: I runner.py:310] Step = 87400 ; steps/s = 1.65, tokens/s = 43788 (43788 target) ; Learning rate = 0.000299 ; Loss = 1.430840\n",
      "2024-12-07 17:49:09.945000: I runner.py:310] Step = 87500 ; steps/s = 1.64, tokens/s = 44243 (44243 target) ; Learning rate = 0.000299 ; Loss = 1.431123\n",
      "2024-12-07 17:50:10.628000: I runner.py:310] Step = 87600 ; steps/s = 1.65, tokens/s = 43775 (43775 target) ; Learning rate = 0.000299 ; Loss = 1.428416\n",
      "2024-12-07 17:51:11.702000: I runner.py:310] Step = 87700 ; steps/s = 1.64, tokens/s = 44286 (44286 target) ; Learning rate = 0.000298 ; Loss = 1.431749\n",
      "2024-12-07 17:52:12.794000: I runner.py:310] Step = 87800 ; steps/s = 1.64, tokens/s = 44268 (44268 target) ; Learning rate = 0.000298 ; Loss = 1.433366\n",
      "2024-12-07 17:53:13.416000: I runner.py:310] Step = 87900 ; steps/s = 1.65, tokens/s = 43817 (43817 target) ; Learning rate = 0.000298 ; Loss = 1.432816\n",
      "2024-12-07 17:54:14.516000: I runner.py:310] Step = 88000 ; steps/s = 1.64, tokens/s = 44267 (44267 target) ; Learning rate = 0.000298 ; Loss = 1.435062\n",
      "2024-12-07 17:55:15.183000: I runner.py:310] Step = 88100 ; steps/s = 1.65, tokens/s = 43795 (43795 target) ; Learning rate = 0.000298 ; Loss = 1.432543\n",
      "2024-12-07 17:56:16.205000: I runner.py:310] Step = 88200 ; steps/s = 1.64, tokens/s = 44314 (44314 target) ; Learning rate = 0.000298 ; Loss = 1.431140\n",
      "2024-12-07 17:57:17.305000: I runner.py:310] Step = 88300 ; steps/s = 1.64, tokens/s = 44271 (44271 target) ; Learning rate = 0.000297 ; Loss = 1.436033\n",
      "2024-12-07 17:58:17.901000: I runner.py:310] Step = 88400 ; steps/s = 1.65, tokens/s = 43843 (43843 target) ; Learning rate = 0.000297 ; Loss = 1.429650\n",
      "2024-12-07 17:59:19.026000: I runner.py:310] Step = 88500 ; steps/s = 1.64, tokens/s = 44248 (44248 target) ; Learning rate = 0.000297 ; Loss = 1.430141\n",
      "2024-12-07 18:00:19.684000: I runner.py:310] Step = 88600 ; steps/s = 1.65, tokens/s = 43801 (43801 target) ; Learning rate = 0.000297 ; Loss = 1.449246\n",
      "2024-12-07 18:01:20.710000: I runner.py:310] Step = 88700 ; steps/s = 1.64, tokens/s = 44315 (44315 target) ; Learning rate = 0.000297 ; Loss = 1.429801\n",
      "2024-12-07 18:02:21.757000: I runner.py:310] Step = 88800 ; steps/s = 1.64, tokens/s = 44304 (44304 target) ; Learning rate = 0.000297 ; Loss = 1.433676\n",
      "2024-12-07 18:03:22.337000: I runner.py:310] Step = 88900 ; steps/s = 1.65, tokens/s = 43858 (43858 target) ; Learning rate = 0.000296 ; Loss = 1.431739\n",
      "2024-12-07 18:04:23.414000: I runner.py:310] Step = 89000 ; steps/s = 1.64, tokens/s = 44280 (44280 target) ; Learning rate = 0.000296 ; Loss = 1.432535\n",
      "2024-12-07 18:05:24.573000: I runner.py:310] Step = 89100 ; steps/s = 1.64, tokens/s = 44220 (44220 target) ; Learning rate = 0.000296 ; Loss = 1.440424\n",
      "2024-12-07 18:06:25.278000: I runner.py:310] Step = 89200 ; steps/s = 1.65, tokens/s = 43765 (43765 target) ; Learning rate = 0.000296 ; Loss = 1.429347\n",
      "2024-12-07 18:07:26.333000: I runner.py:310] Step = 89300 ; steps/s = 1.64, tokens/s = 44299 (44299 target) ; Learning rate = 0.000296 ; Loss = 1.431432\n",
      "2024-12-07 18:08:26.945000: I runner.py:310] Step = 89400 ; steps/s = 1.65, tokens/s = 43828 (43828 target) ; Learning rate = 0.000296 ; Loss = 1.429364\n",
      "2024-12-07 18:09:28.043000: I runner.py:310] Step = 89500 ; steps/s = 1.64, tokens/s = 44259 (44259 target) ; Learning rate = 0.000295 ; Loss = 1.430704\n",
      "2024-12-07 18:10:29.062000: I runner.py:310] Step = 89600 ; steps/s = 1.64, tokens/s = 44330 (44330 target) ; Learning rate = 0.000295 ; Loss = 1.436127\n",
      "2024-12-07 18:11:29.701000: I runner.py:310] Step = 89700 ; steps/s = 1.65, tokens/s = 43812 (43812 target) ; Learning rate = 0.000295 ; Loss = 1.427830\n",
      "2024-12-07 18:12:30.786000: I runner.py:310] Step = 89800 ; steps/s = 1.64, tokens/s = 44273 (44273 target) ; Learning rate = 0.000295 ; Loss = 1.427103\n",
      "2024-12-07 18:13:31.427000: I runner.py:310] Step = 89900 ; steps/s = 1.65, tokens/s = 43810 (43810 target) ; Learning rate = 0.000295 ; Loss = 1.429242\n",
      "2024-12-07 18:14:32.475000: I runner.py:310] Step = 90000 ; steps/s = 1.64, tokens/s = 44305 (44305 target) ; Learning rate = 0.000295 ; Loss = 1.432941\n",
      "2024-12-07 18:14:34.798000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-90000\n",
      "2024-12-07 18:14:34.798000: I training.py:192] Running evaluation for step 90000\n",
      "2024-12-07 18:15:21.989000: I training.py:192] Evaluation result for step 90000: loss = 0.932450 ; perplexity = 2.540726\n",
      "2024-12-07 18:16:22.971000: I runner.py:310] Step = 90100 ; steps/s = 1.64, tokens/s = 44359 (44359 target) ; Learning rate = 0.000294 ; Loss = 1.431625\n",
      "2024-12-07 18:17:23.594000: I runner.py:310] Step = 90200 ; steps/s = 1.65, tokens/s = 43824 (43824 target) ; Learning rate = 0.000294 ; Loss = 1.435629\n",
      "2024-12-07 18:18:24.674000: I runner.py:310] Step = 90300 ; steps/s = 1.64, tokens/s = 44281 (44281 target) ; Learning rate = 0.000294 ; Loss = 1.430296\n",
      "2024-12-07 18:19:25.307000: I runner.py:310] Step = 90400 ; steps/s = 1.65, tokens/s = 43809 (43809 target) ; Learning rate = 0.000294 ; Loss = 1.427465\n",
      "2024-12-07 18:20:26.383000: I runner.py:310] Step = 90500 ; steps/s = 1.64, tokens/s = 44287 (44287 target) ; Learning rate = 0.000294 ; Loss = 1.432152\n",
      "2024-12-07 18:21:27.545000: I runner.py:310] Step = 90600 ; steps/s = 1.64, tokens/s = 44218 (44218 target) ; Learning rate = 0.000294 ; Loss = 1.433560\n",
      "2024-12-07 18:22:28.182000: I runner.py:310] Step = 90700 ; steps/s = 1.65, tokens/s = 43813 (43813 target) ; Learning rate = 0.000293 ; Loss = 1.430567\n",
      "2024-12-07 18:23:29.293000: I runner.py:310] Step = 90800 ; steps/s = 1.64, tokens/s = 44266 (44266 target) ; Learning rate = 0.000293 ; Loss = 1.432681\n",
      "2024-12-07 18:24:29.954000: I runner.py:310] Step = 90900 ; steps/s = 1.65, tokens/s = 43784 (43784 target) ; Learning rate = 0.000293 ; Loss = 1.431596\n",
      "2024-12-07 18:25:31.053000: I runner.py:310] Step = 91000 ; steps/s = 1.64, tokens/s = 44266 (44266 target) ; Learning rate = 0.000293 ; Loss = 1.426826\n",
      "2024-12-07 18:26:32.125000: I runner.py:310] Step = 91100 ; steps/s = 1.64, tokens/s = 44288 (44288 target) ; Learning rate = 0.000293 ; Loss = 1.431487\n",
      "2024-12-07 18:27:32.803000: I runner.py:310] Step = 91200 ; steps/s = 1.65, tokens/s = 43783 (43783 target) ; Learning rate = 0.000293 ; Loss = 1.433485\n",
      "2024-12-07 18:28:33.843000: I runner.py:310] Step = 91300 ; steps/s = 1.64, tokens/s = 44311 (44311 target) ; Learning rate = 0.000293 ; Loss = 1.435628\n",
      "2024-12-07 18:29:34.502000: I runner.py:310] Step = 91400 ; steps/s = 1.65, tokens/s = 43798 (43798 target) ; Learning rate = 0.000292 ; Loss = 1.428779\n",
      "2024-12-07 18:30:35.552000: I runner.py:310] Step = 91500 ; steps/s = 1.64, tokens/s = 44300 (44300 target) ; Learning rate = 0.000292 ; Loss = 1.431090\n",
      "2024-12-07 18:31:36.660000: I runner.py:310] Step = 91600 ; steps/s = 1.64, tokens/s = 44259 (44259 target) ; Learning rate = 0.000292 ; Loss = 1.431598\n",
      "2024-12-07 18:32:37.285000: I runner.py:310] Step = 91700 ; steps/s = 1.65, tokens/s = 43817 (43817 target) ; Learning rate = 0.000292 ; Loss = 1.433560\n",
      "2024-12-07 18:33:38.348000: I runner.py:310] Step = 91800 ; steps/s = 1.64, tokens/s = 44294 (44294 target) ; Learning rate = 0.000292 ; Loss = 1.429478\n",
      "2024-12-07 18:34:39.031000: I runner.py:310] Step = 91900 ; steps/s = 1.65, tokens/s = 43788 (43788 target) ; Learning rate = 0.000292 ; Loss = 1.429227\n",
      "2024-12-07 18:35:40.161000: I runner.py:310] Step = 92000 ; steps/s = 1.64, tokens/s = 44245 (44245 target) ; Learning rate = 0.000291 ; Loss = 1.432489\n",
      "2024-12-07 18:36:41.279000: I runner.py:310] Step = 92100 ; steps/s = 1.64, tokens/s = 44249 (44249 target) ; Learning rate = 0.000291 ; Loss = 1.432394\n",
      "2024-12-07 18:37:41.941000: I runner.py:310] Step = 92200 ; steps/s = 1.65, tokens/s = 43795 (43795 target) ; Learning rate = 0.000291 ; Loss = 1.427694\n",
      "2024-12-07 18:38:43.034000: I runner.py:310] Step = 92300 ; steps/s = 1.64, tokens/s = 44267 (44267 target) ; Learning rate = 0.000291 ; Loss = 1.432628\n",
      "2024-12-07 18:39:43.681000: I runner.py:310] Step = 92400 ; steps/s = 1.65, tokens/s = 43808 (43808 target) ; Learning rate = 0.000291 ; Loss = 1.429153\n",
      "2024-12-07 18:40:44.738000: I runner.py:310] Step = 92500 ; steps/s = 1.64, tokens/s = 44290 (44290 target) ; Learning rate = 0.000291 ; Loss = 1.428504\n",
      "2024-12-07 18:41:45.816000: I runner.py:310] Step = 92600 ; steps/s = 1.64, tokens/s = 44279 (44279 target) ; Learning rate = 0.000290 ; Loss = 1.432879\n",
      "2024-12-07 18:42:46.537000: I runner.py:310] Step = 92700 ; steps/s = 1.65, tokens/s = 43758 (43758 target) ; Learning rate = 0.000290 ; Loss = 1.426812\n",
      "2024-12-07 18:43:47.611000: I runner.py:310] Step = 92800 ; steps/s = 1.64, tokens/s = 44280 (44280 target) ; Learning rate = 0.000290 ; Loss = 1.429323\n",
      "2024-12-07 18:44:48.676000: I runner.py:310] Step = 92900 ; steps/s = 1.64, tokens/s = 44296 (44296 target) ; Learning rate = 0.000290 ; Loss = 1.437536\n",
      "2024-12-07 18:45:49.308000: I runner.py:310] Step = 93000 ; steps/s = 1.65, tokens/s = 43816 (43816 target) ; Learning rate = 0.000290 ; Loss = 1.430905\n",
      "2024-12-07 18:46:50.386000: I runner.py:310] Step = 93100 ; steps/s = 1.64, tokens/s = 44277 (44277 target) ; Learning rate = 0.000290 ; Loss = 1.429067\n",
      "2024-12-07 18:47:51.055000: I runner.py:310] Step = 93200 ; steps/s = 1.65, tokens/s = 43793 (43793 target) ; Learning rate = 0.000290 ; Loss = 1.427766\n",
      "2024-12-07 18:48:52.058000: I runner.py:310] Step = 93300 ; steps/s = 1.64, tokens/s = 44336 (44336 target) ; Learning rate = 0.000289 ; Loss = 1.432131\n",
      "2024-12-07 18:49:53.170000: I runner.py:310] Step = 93400 ; steps/s = 1.64, tokens/s = 44258 (44258 target) ; Learning rate = 0.000289 ; Loss = 1.431300\n",
      "2024-12-07 18:50:53.773000: I runner.py:310] Step = 93500 ; steps/s = 1.65, tokens/s = 43837 (43837 target) ; Learning rate = 0.000289 ; Loss = 1.427791\n",
      "2024-12-07 18:51:54.845000: I runner.py:310] Step = 93600 ; steps/s = 1.64, tokens/s = 44284 (44284 target) ; Learning rate = 0.000289 ; Loss = 1.429884\n",
      "2024-12-07 18:52:55.452000: I runner.py:310] Step = 93700 ; steps/s = 1.65, tokens/s = 43837 (43837 target) ; Learning rate = 0.000289 ; Loss = 1.426917\n",
      "2024-12-07 18:53:56.540000: I runner.py:310] Step = 93800 ; steps/s = 1.64, tokens/s = 44278 (44278 target) ; Learning rate = 0.000289 ; Loss = 1.428801\n",
      "2024-12-07 18:54:57.643000: I runner.py:310] Step = 93900 ; steps/s = 1.64, tokens/s = 44259 (44259 target) ; Learning rate = 0.000288 ; Loss = 1.429481\n",
      "2024-12-07 18:55:58.308000: I runner.py:310] Step = 94000 ; steps/s = 1.65, tokens/s = 43787 (43787 target) ; Learning rate = 0.000288 ; Loss = 1.430813\n",
      "2024-12-07 18:56:59.333000: I runner.py:310] Step = 94100 ; steps/s = 1.64, tokens/s = 44329 (44329 target) ; Learning rate = 0.000288 ; Loss = 1.431405\n",
      "2024-12-07 18:57:59.941000: I runner.py:310] Step = 94200 ; steps/s = 1.65, tokens/s = 43829 (43829 target) ; Learning rate = 0.000288 ; Loss = 1.426109\n",
      "2024-12-07 18:59:01.060000: I runner.py:310] Step = 94300 ; steps/s = 1.64, tokens/s = 44254 (44254 target) ; Learning rate = 0.000288 ; Loss = 1.433344\n",
      "2024-12-07 19:00:02.158000: I runner.py:310] Step = 94400 ; steps/s = 1.64, tokens/s = 44268 (44268 target) ; Learning rate = 0.000288 ; Loss = 1.434538\n",
      "2024-12-07 19:01:02.795000: I runner.py:310] Step = 94500 ; steps/s = 1.65, tokens/s = 43807 (43807 target) ; Learning rate = 0.000288 ; Loss = 1.433598\n",
      "2024-12-07 19:02:03.922000: I runner.py:310] Step = 94600 ; steps/s = 1.64, tokens/s = 44248 (44248 target) ; Learning rate = 0.000287 ; Loss = 1.430491\n",
      "2024-12-07 19:03:04.483000: I runner.py:310] Step = 94700 ; steps/s = 1.65, tokens/s = 43871 (43871 target) ; Learning rate = 0.000287 ; Loss = 1.424298\n",
      "2024-12-07 19:04:05.552000: I runner.py:310] Step = 94800 ; steps/s = 1.64, tokens/s = 44284 (44284 target) ; Learning rate = 0.000287 ; Loss = 1.430259\n",
      "2024-12-07 19:05:06.671000: I runner.py:310] Step = 94900 ; steps/s = 1.64, tokens/s = 44256 (44256 target) ; Learning rate = 0.000287 ; Loss = 1.431561\n",
      "2024-12-07 19:06:07.324000: I runner.py:310] Step = 95000 ; steps/s = 1.65, tokens/s = 43804 (43804 target) ; Learning rate = 0.000287 ; Loss = 1.431987\n",
      "2024-12-07 19:06:07.325000: I training.py:192] Running evaluation for step 95000\n",
      "2024-12-07 19:06:54.799000: I training.py:192] Evaluation result for step 95000: loss = 0.933073 ; perplexity = 2.542310\n",
      "2024-12-07 19:07:55.671000: I runner.py:310] Step = 95100 ; steps/s = 1.64, tokens/s = 44441 (44441 target) ; Learning rate = 0.000287 ; Loss = 1.431231\n",
      "2024-12-07 19:08:56.328000: I runner.py:310] Step = 95200 ; steps/s = 1.65, tokens/s = 43792 (43792 target) ; Learning rate = 0.000286 ; Loss = 1.426602\n",
      "2024-12-07 19:09:57.445000: I runner.py:310] Step = 95300 ; steps/s = 1.64, tokens/s = 44259 (44259 target) ; Learning rate = 0.000286 ; Loss = 1.431782\n",
      "2024-12-07 19:10:58.578000: I runner.py:310] Step = 95400 ; steps/s = 1.64, tokens/s = 44244 (44244 target) ; Learning rate = 0.000286 ; Loss = 1.430666\n",
      "2024-12-07 19:11:59.187000: I runner.py:310] Step = 95500 ; steps/s = 1.65, tokens/s = 43823 (43823 target) ; Learning rate = 0.000286 ; Loss = 1.432383\n",
      "2024-12-07 19:13:00.318000: I runner.py:310] Step = 95600 ; steps/s = 1.64, tokens/s = 44251 (44251 target) ; Learning rate = 0.000286 ; Loss = 1.428724\n",
      "2024-12-07 19:14:00.855000: I runner.py:310] Step = 95700 ; steps/s = 1.65, tokens/s = 43884 (43884 target) ; Learning rate = 0.000286 ; Loss = 1.425093\n",
      "2024-12-07 19:15:01.937000: I runner.py:310] Step = 95800 ; steps/s = 1.64, tokens/s = 44285 (44285 target) ; Learning rate = 0.000286 ; Loss = 1.431622\n",
      "2024-12-07 19:16:03.051000: I runner.py:310] Step = 95900 ; steps/s = 1.64, tokens/s = 44252 (44252 target) ; Learning rate = 0.000285 ; Loss = 1.430218\n",
      "2024-12-07 19:17:03.708000: I runner.py:310] Step = 96000 ; steps/s = 1.65, tokens/s = 43796 (43796 target) ; Learning rate = 0.000285 ; Loss = 1.431308\n",
      "2024-12-07 19:18:04.801000: I runner.py:310] Step = 96100 ; steps/s = 1.64, tokens/s = 44281 (44281 target) ; Learning rate = 0.000285 ; Loss = 1.439508\n",
      "2024-12-07 19:19:05.939000: I runner.py:310] Step = 96200 ; steps/s = 1.64, tokens/s = 43440 (43440 target) ; Learning rate = 0.000285 ; Loss = 1.426036\n",
      "2024-12-07 19:20:08.520000: I runner.py:310] Step = 96300 ; steps/s = 1.60, tokens/s = 43216 (43216 target) ; Learning rate = 0.000285 ; Loss = 1.429613\n",
      "2024-12-07 19:21:12.169000: I runner.py:310] Step = 96400 ; steps/s = 1.57, tokens/s = 42495 (42495 target) ; Learning rate = 0.000285 ; Loss = 1.433939\n",
      "2024-12-07 19:22:14.278000: I runner.py:310] Step = 96500 ; steps/s = 1.61, tokens/s = 42775 (42775 target) ; Learning rate = 0.000285 ; Loss = 1.433338\n",
      "2024-12-07 19:23:17.265000: I runner.py:310] Step = 96600 ; steps/s = 1.59, tokens/s = 42940 (42940 target) ; Learning rate = 0.000284 ; Loss = 1.428171\n",
      "2024-12-07 19:24:19.738000: I runner.py:310] Step = 96700 ; steps/s = 1.60, tokens/s = 42744 (42744 target) ; Learning rate = 0.000284 ; Loss = 1.460882\n",
      "2024-12-07 19:25:22.214000: I runner.py:310] Step = 96800 ; steps/s = 1.60, tokens/s = 43063 (43063 target) ; Learning rate = 0.000284 ; Loss = 1.425130\n",
      "2024-12-07 19:26:24.632000: I runner.py:310] Step = 96900 ; steps/s = 1.60, tokens/s = 43335 (43335 target) ; Learning rate = 0.000284 ; Loss = 1.432528\n",
      "2024-12-07 19:27:26.340000: I runner.py:310] Step = 97000 ; steps/s = 1.62, tokens/s = 43051 (43051 target) ; Learning rate = 0.000284 ; Loss = 1.430626\n",
      "2024-12-07 19:28:28.694000: I runner.py:310] Step = 97100 ; steps/s = 1.60, tokens/s = 43380 (43380 target) ; Learning rate = 0.000284 ; Loss = 1.432089\n",
      "2024-12-07 19:29:30.485000: I runner.py:310] Step = 97200 ; steps/s = 1.62, tokens/s = 43771 (43771 target) ; Learning rate = 0.000284 ; Loss = 1.431730\n",
      "2024-12-07 19:30:32.578000: I runner.py:310] Step = 97300 ; steps/s = 1.61, tokens/s = 42781 (42781 target) ; Learning rate = 0.000283 ; Loss = 1.428031\n",
      "2024-12-07 19:31:35.324000: I runner.py:310] Step = 97400 ; steps/s = 1.59, tokens/s = 43107 (43107 target) ; Learning rate = 0.000283 ; Loss = 1.430077\n",
      "2024-12-07 19:32:37.094000: I runner.py:310] Step = 97500 ; steps/s = 1.62, tokens/s = 43006 (43006 target) ; Learning rate = 0.000283 ; Loss = 1.429995\n",
      "2024-12-07 19:33:38.220000: I runner.py:310] Step = 97600 ; steps/s = 1.64, tokens/s = 44247 (44247 target) ; Learning rate = 0.000283 ; Loss = 1.431572\n",
      "2024-12-07 19:34:39.287000: I runner.py:310] Step = 97700 ; steps/s = 1.64, tokens/s = 44293 (44293 target) ; Learning rate = 0.000283 ; Loss = 1.433685\n",
      "2024-12-07 19:35:39.944000: I runner.py:310] Step = 97800 ; steps/s = 1.65, tokens/s = 43788 (43788 target) ; Learning rate = 0.000283 ; Loss = 1.429032\n",
      "2024-12-07 19:36:41.376000: I runner.py:310] Step = 97900 ; steps/s = 1.63, tokens/s = 44033 (44033 target) ; Learning rate = 0.000282 ; Loss = 1.429950\n",
      "2024-12-07 19:37:42.069000: I runner.py:310] Step = 98000 ; steps/s = 1.65, tokens/s = 43770 (43770 target) ; Learning rate = 0.000282 ; Loss = 1.426888\n",
      "2024-12-07 19:38:43.226000: I runner.py:310] Step = 98100 ; steps/s = 1.64, tokens/s = 44229 (44229 target) ; Learning rate = 0.000282 ; Loss = 1.430450\n",
      "2024-12-07 19:39:44.406000: I runner.py:310] Step = 98200 ; steps/s = 1.63, tokens/s = 44208 (44208 target) ; Learning rate = 0.000282 ; Loss = 1.432425\n",
      "2024-12-07 19:40:45.112000: I runner.py:310] Step = 98300 ; steps/s = 1.65, tokens/s = 43756 (43756 target) ; Learning rate = 0.000282 ; Loss = 1.425073\n",
      "2024-12-07 19:41:46.244000: I runner.py:310] Step = 98400 ; steps/s = 1.64, tokens/s = 44244 (44244 target) ; Learning rate = 0.000282 ; Loss = 1.434911\n",
      "2024-12-07 19:42:46.945000: I runner.py:310] Step = 98500 ; steps/s = 1.65, tokens/s = 43765 (43765 target) ; Learning rate = 0.000282 ; Loss = 1.424868\n",
      "2024-12-07 19:43:48.148000: I runner.py:310] Step = 98600 ; steps/s = 1.63, tokens/s = 44198 (44198 target) ; Learning rate = 0.000281 ; Loss = 1.431763\n",
      "2024-12-07 19:44:49.296000: I runner.py:310] Step = 98700 ; steps/s = 1.64, tokens/s = 44226 (44226 target) ; Learning rate = 0.000281 ; Loss = 1.432287\n",
      "2024-12-07 19:45:50.079000: I runner.py:310] Step = 98800 ; steps/s = 1.65, tokens/s = 43706 (43706 target) ; Learning rate = 0.000281 ; Loss = 1.430358\n",
      "2024-12-07 19:46:51.461000: I runner.py:310] Step = 98900 ; steps/s = 1.63, tokens/s = 44064 (44064 target) ; Learning rate = 0.000281 ; Loss = 1.430311\n",
      "2024-12-07 19:47:52.149000: I runner.py:310] Step = 99000 ; steps/s = 1.65, tokens/s = 43772 (43772 target) ; Learning rate = 0.000281 ; Loss = 1.427820\n",
      "2024-12-07 19:48:53.211000: I runner.py:310] Step = 99100 ; steps/s = 1.64, tokens/s = 44293 (44293 target) ; Learning rate = 0.000281 ; Loss = 1.430532\n",
      "2024-12-07 19:49:54.310000: I runner.py:310] Step = 99200 ; steps/s = 1.64, tokens/s = 44271 (44271 target) ; Learning rate = 0.000281 ; Loss = 1.431755\n",
      "2024-12-07 19:50:54.929000: I runner.py:310] Step = 99300 ; steps/s = 1.65, tokens/s = 43820 (43820 target) ; Learning rate = 0.000280 ; Loss = 1.428510\n",
      "2024-12-07 19:51:56.060000: I runner.py:310] Step = 99400 ; steps/s = 1.64, tokens/s = 44249 (44249 target) ; Learning rate = 0.000280 ; Loss = 1.430787\n",
      "2024-12-07 19:52:56.689000: I runner.py:310] Step = 99500 ; steps/s = 1.65, tokens/s = 43815 (43815 target) ; Learning rate = 0.000280 ; Loss = 1.425391\n",
      "2024-12-07 19:53:57.783000: I runner.py:310] Step = 99600 ; steps/s = 1.64, tokens/s = 44275 (44275 target) ; Learning rate = 0.000280 ; Loss = 1.429654\n",
      "2024-12-07 19:54:58.871000: I runner.py:310] Step = 99700 ; steps/s = 1.64, tokens/s = 44271 (44271 target) ; Learning rate = 0.000280 ; Loss = 1.428630\n",
      "2024-12-07 19:55:59.606000: I runner.py:310] Step = 99800 ; steps/s = 1.65, tokens/s = 43739 (43739 target) ; Learning rate = 0.000280 ; Loss = 1.428777\n",
      "2024-12-07 19:57:00.671000: I runner.py:310] Step = 99900 ; steps/s = 1.64, tokens/s = 44291 (44291 target) ; Learning rate = 0.000280 ; Loss = 1.428343\n",
      "2024-12-07 19:58:01.378000: I runner.py:310] Step = 100000 ; steps/s = 1.65, tokens/s = 43765 (43765 target) ; Learning rate = 0.000280 ; Loss = 1.426415\n",
      "2024-12-07 19:58:03.610000: I training.py:176] Saved checkpoint POS_TR_EN/ckpt-100000\n",
      "2024-12-07 19:58:03.610000: I training.py:192] Running evaluation for step 100000\n",
      "2024-12-07 19:58:51.801000: I training.py:192] Evaluation result for step 100000: loss = 0.943230 ; perplexity = 2.568263\n",
      "2024-12-07 19:59:53.106000: I runner.py:310] Step = 100100 ; steps/s = 1.63, tokens/s = 44126 (44126 target) ; Learning rate = 0.000279 ; Loss = 1.426396\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Tr-En (POS Tags)\n",
    "!onmt-main --model kk-tr-en-modelim.py --config tr-en-pos.yml --auto_config train --with_eval --num_gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bb7261c-d735-46dc-aa2f-73ae451a37b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-07 20:00:00.786407: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-07 20:00:01.569765: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-07 20:00:01.569832: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-07 20:00:01.569840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/omuceng/deneme/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-12-07 20:00:02.555000: I main.py:308] Loading model description from POS_TR_EN/model_description.py\n",
      "2024-12-07 20:00:02.750000: I main.py:315] Using OpenNMT-tf version 2.32.0\n",
      "2024-12-07 20:00:02.751000: I main.py:315] Using model:\n",
      "(model): MultiFeaturesTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): ParallelInputter(\n",
      "      (inputters): ListWrapper(\n",
      "        (0): WordEmbedder()\n",
      "        (1): WordEmbedder()\n",
      "      )\n",
      "      (reducer): ConcatReducer()\n",
      "    )\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): ParallelInputter(\n",
      "        (inputters): ListWrapper(\n",
      "          (0): WordEmbedder()\n",
      "          (1): WordEmbedder()\n",
      "        )\n",
      "        (reducer): ConcatReducer()\n",
      "      )\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(512)\n",
      "            (linear_keys): Dense(512)\n",
      "            (linear_values): Dense(512)\n",
      "            (linear_output): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(512)\n",
      "              (linear_keys): Dense(512)\n",
      "              (linear_values): Dense(512)\n",
      "              (linear_output): Dense(512)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(2048)\n",
      "            (outer): Dense(512)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "2024-12-07 20:00:02.756000: I main.py:340] Using parameters:\n",
      "data:\n",
      "  eval_features_file:\n",
      "  - Tatoeba_tokens_dev\n",
      "  - Tatoeba_pos_tags_dev.txt\n",
      "  eval_labels_file: Tatoeba_dev_target_tokens.txt\n",
      "  source_1_vocabulary: tr_vocab.vocab\n",
      "  source_2_vocabulary: Zemberek_TR_unique_pos\n",
      "  target_vocabulary: en_vocab.vocab\n",
      "  train_features_file:\n",
      "  - Tatoeba_tokens_train\n",
      "  - Tatoeba_pos_tags_train.txt\n",
      "  train_labels_file: Tatoeba_train_target_tokens.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: POS_TR_EN\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 8\n",
      "  coverage_penalty: 0.2\n",
      "  decay_params:\n",
      "    model_dim: 512\n",
      "    warmup_steps: 10000\n",
      "  decay_type: NoamDecay\n",
      "  dropout: 0.1\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  length_penalty: 0.2\n",
      "  num_hypotheses: 1\n",
      "  optimizer: Adam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.8\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 2048\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 2\n",
      "  length_bucket_width: 2\n",
      "  max_step: 205000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: 250000\n",
      "  save_checkpoints_steps: 10000\n",
      "  save_summary_steps: 100\n",
      "  scorers: bleu\n",
      "\n",
      "2024-12-07 20:00:02.937618: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-07 20:00:03.550329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-12-07 20:00:03.707000: I inputter.py:316] Initialized source_1 input layer:\n",
      "2024-12-07 20:00:03.707000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-07 20:00:03.707000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-07 20:00:03.711000: I inputter.py:316] Initialized source_2 input layer:\n",
      "2024-12-07 20:00:03.711000: I inputter.py:316]  - vocabulary size: 21\n",
      "2024-12-07 20:00:03.711000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n",
      "2024-12-07 20:00:03.782000: I inputter.py:316] Initialized target input layer:\n",
      "2024-12-07 20:00:03.782000: I inputter.py:316]  - vocabulary size: 32001\n",
      "2024-12-07 20:00:03.782000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n",
      "2024-12-07 20:00:03.802000: I runner.py:462] Restored checkpoint POS_TR_EN/ckpt-100000\n",
      "2024-12-07 20:00:03.846000: W deprecation.py:350] From /home/omuceng/deneme/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2024-12-07 20:00:04.602716: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-12-07 20:00:04.728000: I runner.py:471] Tracing and optimizing the inference graph...\n",
      "2024-12-07 20:00:18.128773: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8204\n",
      "2024-12-07 20:00:19.023547: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-12-07 20:00:32.058000: I runner.py:471] 2949 predictions are buffered, but waiting for the prediction of queued line 29 to advance the output...\n",
      "2024-12-07 20:00:49.238000: I runner.py:471] 6764 predictions are buffered, but waiting for the prediction of queued line 54 to advance the output...\n",
      "2024-12-07 20:00:59.342000: I runner.py:471] 9068 predictions are buffered, but waiting for the prediction of queued line 54 to advance the output...\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 onmt-main --config tr-en-pos.yml --auto_config --checkpoint_path POS_TR_EN/ckpt-100000 infer --features_file Tatoeba_tokens_test Tatoeba_pos_tags_test.txt --predictions_file output_tr_en_pos.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999dec7d-cd42-41b8-b0c6-54441901249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 MT-Preparation/subwording/3-desubword.py en_vocab.model output_tr_en_pos.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b4af643-69f2-4d85-82ad-91aabdd22062",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference first sentence: I won't stay there very long.\n",
      "Translated first sentence: I won't stay there very long .\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "BLEU:  BLEU = 59.70 80.5/64.4/53.8/45.6 (BP = 1.000 ratio = 1.006 hyp_len = 78064 ref_len = 77587)\n",
      "CHRF:  chrF2 = 73.20\n"
     ]
    }
   ],
   "source": [
    "# BLEU and chrF scores\n",
    "!python3 compute-bleu.py Tatoeba.en-tr.en-filtered.en.test output_tr_en_pos.txt.desubword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "066e467f-aca5-4cbf-a1b8-c2605ecefc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ortalama METEOR Puan: 0.8074712168919849\n"
     ]
    }
   ],
   "source": [
    "# Average METEOR score (Ortalama METEOR Puan)\n",
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# ki dosyay oku ve tokenletir\n",
    "def read_and_tokenize_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return [nltk.word_tokenize(line.strip()) for line in lines]\n",
    "\n",
    "def calculate_meteor(reference_file, hypothesis_file):\n",
    "    references = read_and_tokenize_file(reference_file)\n",
    "    hypotheses = read_and_tokenize_file(hypothesis_file)\n",
    "    \n",
    "    if len(references) != len(hypotheses):\n",
    "        raise ValueError(\"Dosyalarn satr saylar elemiyor\")\n",
    "\n",
    "    total_meteor_score = 0.0\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        total_meteor_score += meteor_score([ref], hyp)\n",
    "\n",
    "    average_meteor_score = total_meteor_score / len(references)\n",
    "    return average_meteor_score\n",
    "\n",
    "reference_file = 'Tatoeba.en-tr.en-filtered.en.test'\n",
    "hypothesis_file = 'output_tr_en_pos.txt.desubword'\n",
    "\n",
    "score = calculate_meteor(reference_file, hypothesis_file)\n",
    "print(f\"Ortalama METEOR Puan: {score}\") #Average METEOR score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd1c79-0f00-4119-b98c-dd8d9ac532ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
